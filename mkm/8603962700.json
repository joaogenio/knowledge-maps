[
    {
        "doc_title": "Making clinical simulation mannequins talk",
        "doc_scopus_id": "85064705139",
        "doc_doi": "10.5220/0007571404500455",
        "doc_eid": "2-s2.0-85064705139",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Consciousness",
            "Evaluation",
            "Glasgow coma scale",
            "Instructor",
            "Pre-recording",
            "Usability"
        ],
        "doc_abstract": "© 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.This paper advocates the interest of applying speech synthesis to clinical simulation mannequins, by focussing on a particular case study of recognised practical interest (evaluation of consciousness level based on the Glasgow Coma Scale), chosen as a proof of concept. A response repository comprising 109 sentences was recorded and an application was developed in Microsoft® Visual Basic® to allow configuration of the simulation scenario, control of response generation on a low-fidelity mannequin equipped with a loudspeaker and assessment of trainee performance. The system received very positive assessment in initial user tests on a typical training setting.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Immersive audio-guiding",
        "doc_scopus_id": "85081155527",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85081155527",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Music",
                "area_abbreviation": "ARTS",
                "area_code": "1210"
            }
        ],
        "doc_keywords": [
            "GPS Coordinates",
            "GPS receivers",
            "Immersive audio",
            "Position detection",
            "Route management",
            "Source position",
            "Virtual sound sources",
            "Virtual sources"
        ],
        "doc_abstract": "Copyright © 2018 DAFx.All rights reserved.An audio-guide prototype was developed which makes it possible to associate virtual sound sources to tourist route focal points. An augmented reality effect is created, as the (virtual) audio content presented through headphones seems to originate from the specified (real) points. A route management application allows specification of source positions (GPS coordinates), audio content (monophonic files) and route points where playback should be triggered. The binaural spatialisation effects depend on user pose relative to the focal points: position is detected by a GPS receiver; for head-tracking, an IMU is attached to the headphone strap. The main application, developed in C++, streams the audio content through a real-time auralisation engine. HRTF filters are selected according to the azimuth and elevation of the path from the virtual source, continuously updated based on user pose. Preliminary tests carried out with ten subjects confirmed the ability to provide the desired audio spatialisation effects and identified position detection accuracy as the main aspect to be improved in the future.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the validation of computerised lung auscultation",
        "doc_scopus_id": "84938822338",
        "doc_doi": "10.5220/0005293406540658",
        "doc_eid": "2-s2.0-84938822338",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Annotation",
            "Automatic detection algorithms",
            "Lung sounds",
            "Performance metrics",
            "Validation"
        ],
        "doc_abstract": "The development of computerised diagnosis tools based on lung auscultation necessitates appropriate validation. So far, this work front has received insufficient attention from researchers; validation studies found in the literature are largely flawed. We believe that building open-access crowd-sourced information systems based on large-scale repositories of respiratory sound files is an essential task and should be urgently addressed. Most diagnosis tools are based on automatic adventitious lung sound (ALS) detection algorithms. The gold standards required to assess their performance can only be obtained by human expert annotation of a statistically significant set of respiratory sound files; given the inevitable subjectivity of the process, statistical agreement criteria must be applied to multiple independent annotations obtained for each file. For these reasons, the information systems we propose should provide simple, efficient annotation tools; facilitate the formation of credible annotation panels; apply appropriate agreement criteria and metrics to generate goldstandard ALS annotation files and, based on them, allow easy quantitative assessment of detection algorithm performance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning Auditory Space: Generalization and Long-Term Effects",
        "doc_scopus_id": "84886047011",
        "doc_doi": "10.1371/journal.pone.0077900",
        "doc_eid": "2-s2.0-84886047011",
        "doc_date": "2013-10-22",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Adaptation, Physiological",
            "Adult",
            "Female",
            "Humans",
            "Learning",
            "Male",
            "Middle Aged",
            "Sound Localization",
            "Time Factors"
        ],
        "doc_abstract": "Background:Previous findings have shown that humans can learn to localize with altered auditory space cues. Here we analyze such learning processes and their effects up to one month on both localization accuracy and sound externalization. Subjects were trained and retested, focusing on the effects of stimulus type in learning, stimulus type in localization, stimulus position, previous experience, externalization levels, and time.Method:We trained listeners in azimuth and elevation discrimination in two experiments. Half participated in the azimuth experiment first and half in the elevation first. In each experiment, half were trained in speech sounds and half in white noise. Retests were performed at several time intervals: just after training and one hour, one day, one week and one month later. In a control condition, we tested the effect of systematic retesting over time with post-tests only after training and either one day, one week, or one month later.Results:With training all participants lowered their localization errors. This benefit was still present one month after training. Participants were more accurate in the second training phase, revealing an effect of previous experience on a different task. Training with white noise led to better results than training with speech sounds. Moreover, the training benefit generalized to untrained stimulus-position pairs. Throughout the post-tests externalization levels increased. In the control condition the long-term localization improvement was not lower without additional contact with the trained sounds, but externalization levels were lower.Conclusion:Our findings suggest that humans adapt easily to altered auditory space cues and that such adaptation spreads to untrained positions and sound types. We propose that such learning depends on all available cues, but each cue type might be learned and retrieved differently. The process of localization learning is global, not limited to stimulus-position pairs, and it differs from externalization processes. © 2013 Mendonça et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Reflection orders and auditory distance",
        "doc_scopus_id": "84878989549",
        "doc_doi": "10.1121/1.4800186",
        "doc_eid": "2-s2.0-84878989549",
        "doc_date": "2013-06-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Auditory distances",
            "Distant source",
            "Psychophysical",
            "Sound pressures",
            "Sound source",
            "Sound spectrum"
        ],
        "doc_abstract": "The perception of sound distance has been sparsely studied so far. It is assumed to depend on familiar loudness, reverberation, sound spectrum and parallax, but most of these factors have never been carefully addressed. Reverberation has been mostly analysed in terms of ratio between direct and indirect sound, and total duration. Here we were interested in assessing the impact of each reflection order on distance localization. We compared sound source discrimination at an intermediate and at a distant location with direct sound only, one, two, three, and four reflection orders in a 2AFC task. At the intermediate distances, normalized psychophysical curves reveal no differentiation between direct sound and up to three reflection orders, but sounds with four reflection orders have significantly lower thresholds. For the distant sources, sounds with four reflection orders yielded the best discrimination slopes, but there was also a clear benefit for sounds with three reflection orders. We conclude that at least three reflection orders are required so that reflection-related cues are accounted for in distance estimates. Also, these cues might interact differently with the direct sound pressure cues at different distances. © 2013 Acoustical Society of America.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-algorithm respiratory crackle detection",
        "doc_scopus_id": "84877993916",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84877993916",
        "doc_date": "2013-05-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Annotation",
            "Automatic detection algorithms",
            "Lung sounds",
            "Performance metrics",
            "Stethoscopy",
            "Validation"
        ],
        "doc_abstract": "Four crackle detection algorithms were implemented based on selected techniques proposed in the literature. The algorithms were tested on a set of lung sounds and their performance was assessed in terms of sensitivity (SE), accuracy (PPV) and their harmonic mean (F index). The reference annotation data for calculating these indices were obtained through agreement by majority between independent annotations made by three health professionals on the same set of lung sounds. Agreement by majority of the four algorithms afforded more than 7% performance improvement over the best individual algorithm.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic wheeze and respiratory phase detectors to evaluate respiratory physiotherapy in LRTI: A preliminary study",
        "doc_scopus_id": "84877967933",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84877967933",
        "doc_date": "2013-05-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Experimental groups",
            "Preliminary studies",
            "Reliable measurement",
            "Respiratory phasis",
            "Respiratory sounds",
            "Respiratory tract infections",
            "Statistical significance",
            "Wheezing"
        ],
        "doc_abstract": "Respiratory physiotherapy is a gold standard intervention for chronic respiratory conditions. However, its application in acute respiratory diseases (e.g., LRTI) is not well established. The objective and reliable measurement of adventitious lung sounds (ALS), such as wheezes, has the potential to contribute to respiratory physiotherapy evidence base. This paper reports on the implementation of reliable and published automatic wheeze and respiratory phase detectors to assess wheezing parameters pre/post respiratory physiotherapy treatment in patients with LRTI. Twenty patients with LRTI were randomly allocated to control group, which received standard medication treatment, or experimental group, which received standard medication plus respiratory physiotherapy treatment. Respiratory sounds were recorded in seven chest locations. Wheeze parameters, namely occupation rate, main frequency, duration and type were obtained per respiratory phase. Wheeze occupation rate was statistically significantly reduced in both groups following treatment (p<0.001). There was a greater reduction in wheeze occupation rate in the experimental group reaching statistical significance for the inspiratory phase (p=0.019). This promising result indicates the potential value of respiratory physiotherapy in LRTI. It also highlights the potential to use acoustic methods to establish respiratory physiotherapy efficacy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real-time dynamic image-source implementation for auralisation",
        "doc_scopus_id": "84901780958",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84901780958",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            },
            {
                "area_name": "Music",
                "area_abbreviation": "ARTS",
                "area_code": "1210"
            }
        ],
        "doc_keywords": [
            "Computational performance",
            "Dynamic calculations",
            "Head related transfer function",
            "Image-source method",
            "Interactive virtual reality",
            "Propagation direction",
            "Real-time application",
            "Visual representations"
        ],
        "doc_abstract": "This paper describes a software package for auralisation in interactive virtual reality environments. Its purpose is to reproduce, in real time, the 3D soundfield within a virtual room where listener and sound sources can be moved freely. Output sound is presented binaurally using headphones. Auralisation is based on geometric acoustic models combined with head-related transfer functions (HRTFs): the direct sound and reflections from each source are computed dynamically by the image-source method. Directional cues are obtained by filtering these incoming sounds by the HRTFs corresponding to their propagation directions relative to the listener, computed on the basis of the information provided by a head-tracking device. Two interactive real-time applications were developed to demonstrate the operation of this software package. Both provide a visual representation of listener (position and head orientation) and sources (including image sources). One focusses on the auralisation-visualisation synchrony and the other on the dynamic calculation of reflection paths. Computational performance results of the auralisation system are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the adaptation to non-individualised HRTF auralisations: A longitudinal study",
        "doc_scopus_id": "84883364337",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84883364337",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Acoustic spaces",
            "Active Learning",
            "Localization errors",
            "Long-term effects",
            "Longitudinal study",
            "Virtual sound"
        ],
        "doc_abstract": "Auralisations with HRTFs are an innovative tool for the reproduction of acoustic space. Their broad applicability depends on the use of non-individualised models, but little is known on how humans adapt to these sounds. Previous findings have shown that simple exposure to non-individualised virtual sounds did not provide a quick adaptation, but that training and feedback would boost this process. Here, we were interested in analyzing the long-term effect of such training-based adaptation. We trained listeners in azimuth and elevation discrimination in two separate experiments and retested them immediately, one hour, one day, one week and one month after. Results revealed that, with active learning and feedback, all participants lowered their localization errors. This benefit was still found one month after training. Interestingly, participants who had trained previously with elevations were better in azimuth localization and vice-versa. Our findings suggest that humans adapt easily to new anatomically shaped spectral cues and they are able to transfer that adaptation to non-trained sounds. Copyright© (2012) by the Audio Engineering Society.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real-time auralisation system for virtual microphone positioning",
        "doc_scopus_id": "84872691863",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84872691863",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Audio processing",
            "Audio-output",
            "Output data",
            "Overlap-add method",
            "Processing time",
            "Regular grids",
            "Sound source",
            "Virtual microphone"
        ],
        "doc_abstract": "A computer application was developed to simulate the process of microphone positioning in sound recording applications. A dense, regular grid of impulse responses pre-recorded on the region of the room under study allowed the sound captured by a virtual microphone to be auralised through real-time convolution with an anechoic stream representing the sound source. Convolution was performed using a block-based variation on the overlap-add method where the summation of many small sub-convolutions produced each block of output data samples. As the applied RIR filter varied on successive audio output blocks, a short cross fade was applied to avoid glitches in the audio. The maximum possible length of impulse response applied was governed by the size of audio processing block (hence latency) employed by the program. Larger blocks allowed a lower processing time per sample. At 23.2ms latency (1024 samples at 44.1kHz), it was possible to apply 9 second impulse responses on a standard laptop computer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the improvement of localization accuracy with non-individualized HRTF-based sounds",
        "doc_scopus_id": "84870349596",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84870349596",
        "doc_date": "2012-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Music",
                "area_abbreviation": "ARTS",
                "area_code": "1210"
            }
        ],
        "doc_keywords": [
            "Active Learning",
            "Auralizations",
            "Head related transfer function",
            "Localization accuracy",
            "Sound perception",
            "Sound source localization",
            "Virtual sound",
            "Virtual-reality environment"
        ],
        "doc_abstract": "Auralization is a powerful tool to increase the realism and sense of immersion in Virtual Reality environments. The Head Related Transfer Function (HRTF) filters commonly used for auralization are non-individualized, as obtaining individualized HRTFs poses very serious practical difficulties. It is therefore extremely important to understand to what extent this hinders sound perception. In this paper we address this issue from a learning perspective. In a set of experiments, we observed that mere exposure to virtual sounds processed with generic HRTF did not improve the subjects' performance in sound source localization, but short training periods involving active learning and feedback led to significantly better results. We propose that using auralization with non-individualized HRTF should always be preceded by a learning period.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Respiratory sound annotation software",
        "doc_scopus_id": "84861982798",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861982798",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Asthma",
            "COPD",
            "Crackles",
            "Cystic fibrosis",
            "Lung sounds",
            "Pneumonia",
            "Respiratory cycle",
            "Wheezes"
        ],
        "doc_abstract": "Significant research efforts have been dedicated to the automatic detection of adventitious lung sounds, using, for this purpose, different algorithms. The validation of these algorithms is based on the comparison of their results with reference annotations and therefore requires the development of user-friendly annotation software. This paper presents an application, developed in Matlab®, for the annotation of respiratory sounds. The user can identify respiratory cycles and adventitious sounds - crackles and wheezes - directly on the waveforms displayed on the screen, which may be simultaneously played back. The audio playback speed is user-adjustable and synchronised with the cursor display. Specific annotation file storage formats were defined. Preliminary usability tests performed by three health professionals using twenty respiratory sound files from six patients (with pneumonia and cystic fibrosis) indicate that the software is user-friendly and effective, allowing simple and quick annotations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and simulation of a rectangular meshotron unit prototype",
        "doc_scopus_id": "80055019718",
        "doc_doi": "10.1109/SAAHPC.2011.21",
        "doc_eid": "2-s2.0-80055019718",
        "doc_date": "2011-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Acoustic modelling",
            "Data partitioning",
            "Digital waveguides",
            "GALS",
            "Parallelisation",
            "VHDL"
        ],
        "doc_abstract": "A novel application-specific hardware (ASH) unit was designed to form the building block of the Meshotron - a parallelisation network for three-dimensional (3D) digital waveguide-mesh (DWM) room acoustic models. The rectangular mesh topology was elected. This ASH unit was tested using professional hardware simulation tools, assuming 32-bit integer data. Room impulse responses (RIR) were obtained for a set of small models under different test conditions, using both single-unit and multi-unit configurations. They proved exactly identical to those obtained using 3D DWM modelling software for the same models and test conditions, which validates the design. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the improvement of auditory accuracy with non-individualized HRTF-based sounds",
        "doc_scopus_id": "84866042620",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84866042620",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Active Learning",
            "Auralizations",
            "Head related transfer function",
            "Sound perception",
            "Sound source localization",
            "Virtual sound",
            "Virtual-reality environment"
        ],
        "doc_abstract": "Auralization is a powerful tool to increase the realism and sense of immersion in Virtual Reality environments. The Head Related Transfer Function (HRTF) filters commonly used for auralization are non-individualized, as obtaining individualized HRTFs poses very serious practical difficulties. It is therefore extremely important to understand to what extent this hinders sound perception. In this paper, we address this issue from a learning perspective. In a set of experiments, we observed that mere exposure to virtual sounds processed with generic HRTF did not improve the subjects' performance in sound source localization, but short training periods involving active learning and feedback led to significantly better results. We propose that using auralization with non-individualized HRTF should always be preceded by a learning period.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Finite difference room acoustic modelling on a general purpose graphics processing unit",
        "doc_scopus_id": "84866018700",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84866018700",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Auralizations",
            "Computational savings",
            "Data sets",
            "Finite difference",
            "General purpose",
            "Graphics Processing Unit",
            "Listening positions",
            "Parallelisation",
            "Processing capability",
            "Processing resources",
            "Processing time",
            "Room acoustics",
            "Room impulse response",
            "Virtual rooms",
            "Walkthroughs"
        ],
        "doc_abstract": "Detailed and convincing walkthrough auralizations of virtual rooms requires much processing capability. One method of reducing this requirement is to pre-calculate a data-set of room impulse responses (RIR) at locations throughout the space. Processing resources may then focus on RIR interpolation and convolution using the dataset as the virtual listening position changes in real-time. Recent work identified the suitability of wave-based models over traditional ray-based approaches for walkthrough auralization. Despite the computational saving of wave-based methods to generate the RIR dataset, processing times are still long. This paper presents a wave-based implementation for execution on a general purpose graphics processing unit. Results validate the approach and show that parallelisation provides a notable acceleration.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Meshotron: A network of specialised hardware units for 3D Digital Waveguide Mesh acoustic model parallelisation",
        "doc_scopus_id": "84866015236",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84866015236",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Acoustic model",
            "Application-specific hardware",
            "Boundary nodes",
            "Design and tests",
            "Digital waveguide mesh",
            "Hardware simulation",
            "Initial stages",
            "Parallelisation",
            "Rectangular mesh",
            "Room acoustics",
            "Software prototypes"
        ],
        "doc_abstract": "This paper presents the project of a computing network - the \"Meshotron\" - specifically designed for large-scale parallelisation of three-dimensional Digital Waveguide Mesh (3D DWM) room acoustic models. It discusses the motivation of the project, its advantages and the architecture envisaged for the application-specific hardware (ASH) units to form the network. The initial stages involve the development of a software prototype based on the rectangular mesh topology, using appropriate hardware simulation tools, and the design and test of FPGA-based scattering units for air and boundary nodes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "TheremUS: The ultrasonic Theremin",
        "doc_scopus_id": "84866050185",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84866050185",
        "doc_date": "2009-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Digital sound synthesis",
            "Hand movement",
            "Hand positions",
            "Multimedia device",
            "RGB LED",
            "Sound synthesis",
            "Ultrasonic sensing"
        ],
        "doc_abstract": "In the Theremin, the performer's hand movements, detected by two antennas, control the pitch and volume of the generated sound. The TheremUS builds on this concept by using ultrasonic sensing for hand position detection and processing all signals digitally, a distinct advantage in terms of versatility. Not only can different sound synthesis algorithms be programmed directly on the instrument but also it can be easily connected to other digital sound synthesis or multimedia devices; a MIDI interface was included for this purpose. The TheremUS also features translucent panels lit by controllable RGB LED devices. This makes it possible to specify sound-colour mappings in the spirit of the legendary Ocular Harpsichord by Castel.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D reconstruction and Auralisation of the \"painted dolmen\" of antelas",
        "doc_scopus_id": "47949110073",
        "doc_doi": "10.1117/12.766607",
        "doc_eid": "2-s2.0-47949110073",
        "doc_date": "2008-07-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D acquisition",
            "3D geometries",
            "3D reconstructions",
            "3d visual models",
            "Acoustic absorption coefficients",
            "Archaeological sites",
            "Audio sources",
            "Audio visuals",
            "Augmented reality",
            "Auralisation",
            "Dark rooms",
            "Geometric acoustics",
            "Geometric models",
            "Head Related Transfer Functions",
            "In-situ",
            "Irregular surfaces",
            "Iterative algorithms",
            "Iterative Closest points",
            "Laser range finder",
            "Orientation sensors",
            "Reconstruction softwares",
            "Reverberation times",
            "Software computes",
            "Sound waves",
            "Source localisation",
            "Stereo headphones",
            "Uniform grids",
            "Visual scenes",
            "Visualization toolkits"
        ],
        "doc_abstract": "This paper presents preliminary results on the development of a 3D audiovisual model of the Anta Pintada (painted dolmen) of Antelas, a Neolithic chamber tomb located in Oliveira de Frades and listed as Portuguese national monument. The final aim of the project is to create a highly accurate Virtual Reality (VR) model of this unique archaeological site, capable of providing not only visual but also acoustic immersion based on its actual geometry and physical properties. The project started in May 2006 with in situ data acquisition. The 3D geometry of the chamber was captured using a Laser Range Finder. In order to combine the different scans into a complete 3D visual model, reconstruction software based on the Iterative Closest Point (ICP) algorithm was developed using the Visualization Toolkit (VTK). This software computes the boundaries of the room on a 3D uniform grid and populates its interior with \"free-space nodes\", through an iterative algorithm operating like a torchlight illuminating a dark room. The envelope of the resulting set of \"free-space nodes\" is used to generate a 3D iso-surface approximating the interior shape of the chamber. Each polygon of this surface is then assigned the acoustic absorption coefficient of the corresponding boundary material. A 3D audiovisual model operating in real-time was developed for a VR Environment comprising head-mounted display (HMD) I-glasses SVGAPro, an orientation sensor (tracker) InterTrax 2 with 3 Degrees Of Freedom (3DOF) and stereo headphones. The auralisation software is based on a geometric model. This constitutes a first approach, since geometric acoustics have well-known limitations in rooms with irregular surfaces. The immediate advantage lies in their inherent computational efficiency, which allows real-time operation. The program computes the early reflections forming the initial part of the chamber's impulse response (IR), which carry the most significant cues for source localisation. These early reflections are processed through Head Related Transfer Functions (HRTF) updated in real-time according to the orientation of the user's head, so that sound waves appear to come from the correct location in space, in agreement with the visual scene. The late-reverberation tail of the IR is generated by an algorithm designed to match the reverberation time of the chamber, calculated from the actual acoustic absorption coefficients of its surfaces. The sound output to the headphones is obtained by convolving the IR with anechoic recordings of the virtual audio source. © 2008 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the computational efficiency of different waveguide mesh topologies for room acoustic simulation",
        "doc_scopus_id": "27644450855",
        "doc_doi": "10.1109/TSA.2005.852015",
        "doc_eid": "2-s2.0-27644450855",
        "doc_date": "2005-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Acoustic application",
            "Room acoustic simulations",
            "Signal synthesis",
            "Spatial resolution"
        ],
        "doc_abstract": "Room acoustic simulation using digital waveguide modeling requires three-dimensional waveguide meshes in order to represent fully the acoustic properties of the space. This paper presents a systematic analysis of four mesh topologies suggested in the literature: rectilinear, tetrahedral, cubic close-packed and octahedral. These mesh structures are compared from the standpoint of computational efficiency, bearing in mind specific issues that are important for room acoustic simulation. Each mesh topology offers a different compromise between spatial resolution, bandwidth, dispersion characteristics (including suitability for the application of dispersion-compensation techniques), computation time, memory requirements and implementation complexity. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the computation time of three-dimensional digital waveguide mesh acoustic models",
        "doc_scopus_id": "84889263662",
        "doc_doi": "10.1109/EURMIC.2000.874498",
        "doc_eid": "2-s2.0-84889263662",
        "doc_date": "2000-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Acoustic simulations",
            "Computation time",
            "Digital waveguide mesh",
            "ITS applications",
            "Multiprocessor computers",
            "Software implementation",
            "System applications",
            "Workstation clusters"
        ],
        "doc_abstract": "After briefly discussing the accuracy requirements of various auralisation system applications, the paper reviews the principles of digital-waveguide modelling and its application to 3D acoustic simulation for accurate auralisation. A formula is derived for the computation time of a regular rectilinear mesh model, assuming lossless propagation. A software implementation of this model is described along with the results from single-processor benchmarking tests. A parallelisation strategy applicable to both multiprocessor computers and workstation clusters is then presented and its impact on computation time analysed. © 2000 IEEE.",
        "available": false,
        "clean_text": ""
    }
]