[
    {
        "doc_title": "A graph neural network framework for causal inference in brain networks",
        "doc_scopus_id": "85104351359",
        "doc_doi": "10.1038/s41598-021-87411-8",
        "doc_eid": "2-s2.0-85104351359",
        "doc_date": "2021-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Brain",
            "Diffusion Tensor Imaging",
            "Humans",
            "Magnetic Resonance Imaging",
            "Neural Networks, Computer"
        ],
        "doc_abstract": "© 2021, The Author(s).A central question in neuroscience is how self-organizing dynamic interactions in the brain emerge on their relatively static structural backbone. Due to the complexity of spatial and temporal dependencies between different brain areas, fully comprehending the interplay between structure and function is still challenging and an area of intense research. In this paper we present a graph neural network (GNN) framework, to describe functional interactions based on the structural anatomical layout. A GNN allows us to process graph-structured spatio-temporal signals, providing a possibility to combine structural information derived from diffusion tensor imaging (DTI) with temporal neural activity profiles, like that observed in functional magnetic resonance imaging (fMRI). Moreover, dynamic interactions between different brain regions discovered by this data-driven approach can provide a multi-modal measure of causal connectivity strength. We assess the proposed model’s accuracy by evaluating its capabilities to replicate empirically observed neural activation profiles, and compare the performance to those of a vector auto regression (VAR), like that typically used in Granger causality. We show that GNNs are able to capture long-term dependencies in data and also computationally scale up to the analysis of large-scale networks. Finally we confirm that features learned by a GNN can generalize across MRI scanner types and acquisition protocols, by demonstrating that the performance on small datasets can be improved by pre-training the GNN on data from an earlier study. We conclude that the proposed multi-modal GNN framework can provide a novel perspective on the structure-function relationship in the brain. Accordingly this approach appears to be promising for the characterization of the information flow in brain networks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extracting control variables of casting processes with NMF and rule extraction",
        "doc_scopus_id": "85105593649",
        "doc_doi": "10.1016/j.eswa.2021.115118",
        "doc_eid": "2-s2.0-85105593649",
        "doc_date": "2021-10-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Casting process",
            "Control variable",
            "Industrial process monitoring",
            "Industrial settings",
            "Interpretability",
            "Large amounts of data",
            "Matrix factorizations",
            "Non-negative matrix factorization",
            "Optimisations",
            "Rules extraction"
        ],
        "doc_abstract": "© 2021 Elsevier LtdProcess monitoring and optimization in an industrial setting is a complex topic, a large amount of data is captured and its measurements are affected by many complex relations. There is, therefore, a growing necessity for methods to reduce its complexity and dimension such that it can be interpreted by humans. In this work we study the extraction of interpretable rules derived from components of a non-negative matrix factorization using real data.",
        "available": true,
        "clean_text": "serial JL 271506 291210 291817 291820 291862 291866 291870 291883 31 Expert Systems with Applications EXPERTSYSTEMSAPPLICATIONS 2021-04-28 2021-04-28 2021-05-10 2021-05-10 2021-12-09T14:55:51 S0957-4174(21)00559-5 S0957417421005595 10.1016/j.eswa.2021.115118 S300 S300.1 FULL-TEXT 2021-12-09T16:10:24.337229Z 0 0 20211015 2021 2021-04-28T15:27:08.582759Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body mmlmath affil appendices articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst pubtype ref 0957-4174 09574174 true 180 180 C Volume 180 30 115118 115118 115118 20211015 15 October 2021 2021-10-15 2021 article sco © 2021 Elsevier Ltd. All rights reserved. EXTRACTINGCONTROLVARIABLESCASTINGPROCESSESNMFRULEEXTRACTION XAREZ M 1 Introduction 2 Methods 2.1 Time series templates via Non-Negative Matrix Factorization 2.2 RuleFit algorithm 2.2.1 Rule extraction 2.2.2 Importance of the rules 2.2.3 Evaluation metrics 3 Data collection 3.1 Gravity casting temperature profiles 3.2 High-pressure casting temperature profiles 4 Feature extraction by NMF 4.1 Component temperature profiles 5 Rule extraction via a random forest 5.1 Supervised Learning and Rule extraction 5.1.1 Random Forest 5.1.2 RuleFit and Rule importance 5.1.3 RuleFit scores 6 Conclusion CRediT authorship contribution statement Supplementary data References CHEN 2004 24 C CHEN 2018 1 7 J IN2018IEEE15THINTERNATIONALCONFERENCENETWORKINGSENSINGCONTROLICNSC RANDOMIZEDLATENTFACTORMODELFORHIGHDIMENSIONALSPARSEMATRICESINDUSTRIALAPPLICATIONS CICHOCKI 2009 A COMON 1994 287 314 P ESCOBAR 2018 C FRIEDMAN 2008 916 954 J FU 2019 59 80 X JOLLIFFE 2016 2015202 I KOZLOWSKI 2019 J LADE 2017 74 79 P LEE 1999 788 791 D LUO 2020 1798 1809 X LUO 2020 X LUO 2016 579 592 X MASHAYEKHI 2017 1707 1727 M SHANG 2019 131 141 M SIRIKULVIRIYA 2011 N ININTERNATIONALCONFERENCEINFORMATIONELECTRONICSENGINEERING INTEGRATIONRULESARANDOMFOREST XAREZ 2019 M YIN 2014 6418 6428 S XAREZX2021X115118 XAREZX2021X115118XM 2023-05-10T00:00:00.000Z 2023-05-10T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 2021-06-03T23:29:07.066Z 0 item S0957-4174(21)00559-5 S0957417421005595 10.1016/j.eswa.2021.115118 271506 2021-12-09T16:10:24.337229Z 2021-10-15 true 3347038 MAIN 10 59054 849 656 IMAGE-WEB-PDF 1 gr7 73227 451 752 gr6 21313 661 538 gr3 27146 335 573 gr2 58831 716 669 gr5 58352 450 750 gr4 58936 824 665 gr1 12451 128 551 gr7 7733 131 219 gr6 1634 163 133 gr3 7800 128 219 gr2 6537 164 153 gr5 7027 131 219 gr4 3341 164 132 gr1 2842 51 219 gr7 438530 1995 3328 gr6 131195 2928 2382 gr3 263229 1481 2536 gr2 320275 3167 2961 gr5 339301 1992 3323 gr4 371296 3649 2944 gr1 78360 569 2440 mmc1 mmc1.xml xml 300 APPLICATION si1 2329 si10 6386 si11 4137 si12 8649 si13 6386 si14 19005 si15 18236 si16 17588 si17 17685 si18 18170 si19 17049 si2 3504 si20 25196 si21 19075 si22 25342 si23 18506 si24 19939 si25 19222 si26 6831 si27 4443 si28 7421 si29 7897 si3 2330 si30 1629 si31 1113 si32 1629 si33 1520 si34 11934 si35 5188 si36 4676 si37 1629 si38 4676 si39 1520 si4 8584 si40 12341 si41 4720 si42 4509 si43 11731 si44 11731 si45 4720 si46 1520 si47 1629 si48 3190 si49 26828 si5 6445 si50 3751 si51 4304 si52 4160 si53 4676 si54 9586 si55 7519 si56 7519 si57 17853 si58 7889 si59 5233 si6 10810 si60 8392 si61 3797 si62 11900 si63 10091 si64 11140 si65 10353 si66 8647 si67 2141 si68 12046 si69 2061 si7 3928 si70 28868 si71 14237 si72 30351 si73 2788 si74 15195 si75 4304 si76 9236 si77 11522 si78 8976 si79 5329 si8 3754 si80 1520 si81 1629 si82 1629 si83 1629 si84 1113 si85 11912 si86 19116 si87 19116 si88 17816 si9 8649 am false 2124047 ESWA 115118 115118 S0957-4174(21)00559-5 10.1016/j.eswa.2021.115118 Elsevier Ltd Fig. 1 A schematic illustration of the data analysis pipeline. Fig. 2 Temperature time series collected during one day using the gravity casting machine. The time series of sensor 8 was selected for further analysis. The ordinates give the sensor temperature in ° C and the abscissae present time in seconds. Fig. 3 Temperature time series T ( t ) collected during the pre-production phase of the high-pressure gravity casting machine. The ordinate provides sensor temperature in ° C , while the abscissa denotes time in seconds. Fig. 4 Comparison of normalized temperature profiles T / T max ( t ) for Top: gravity casting; Bottom: high pressure casting. The shown temperature profiles corrrespond to either a random (blue line) or a knowledge-based (red line) initialization. Note that despite different initializations, the respective profiles are very similar given the permutation indeterminacy of the NMF decomposition. On the ordinate the value of the value of the normalized temperature T / T max is given, and on the abscissa the time t in seconds is displayed. Fig. 5 The related coefficients w m ☆ , m = 1 , 2 , 3 of the temperature factors h * k from the NMF decomposition. The red dot points identify the processes classified as NOK. The strong oscillations of the weights of component m = 2 indicate the renewal of the release agent. The ordinates give the values of the coefficients w m 1 , w m 2 , w m 3 from top to bottom and the abscissa provides the process numbers m = 1 , … , M . Fig. 6 The importance of the rules: Top: gravity casting; Bottom: high-pressure casting. The ordinate gives the rule importance, while the abscissa numbers the rules. Fig. 7 Illustration of extracted coefficients w m ☆ , which correspond to different periods of data recording, hence the data follows different rules. The grey dots correspond to data points that don’t follow any of the two rules. The ordinates present the three coefficients w m 1 , w m 2 , w m 3 as function of the number m = 1 , … , M of processes displayed on the abscissa. Table 1 Gravity casting data: the 4 rules with highest importance value. Note that the rules in one line of the table are to be combined and apply simultaneously. rank Rule 1 397.1 ⩽ w 1 ⩽ 415.1 & 197.6 ⩽ w 2 ⩽ 251.5 & 226.1 < w 3 < ∞ 2 404.6 < w 1 < ∞ & 255.3 ⩽ w 2 ⩽ 344.1 & 0 < w 3 ⩽ 251.2 3 416.3 ⩽ w 1 ⩽ 419.7 & 94.2 < w 2 < ∞ & 242.6 < w 3 < ∞ 4 397.1 < w 1 < ∞ & 0 < w 2 ⩽ 226.6 & 230.9 < w 3 < ∞ Table 2 Gravity casting data: consensus rules or restrictive rules. rank Rule C 397.1 ⩽ w 1 ⩽ 419.7 & 94.2 < w 2 ⩽ 344.1 & 226.1 < w 3 ⩽ 251.2 R 416.3 ⩽ w 1 ⩽ 415.1 & 255.3 ⩽ w 2 ⩽ 251.5 & 242.6 < w 3 ⩽ 251.2 Table 3 High pressure casting data: the 4 rules with highest importance value. Again rules in one line apply simultaneously. rank Rule 1 unspecified & 172.6 < w 2 < ∞ & 25.3 ⩽ w 3 ⩽ 142.6 2 0 ⩽ w 1 ⩽ 364.9 & 352.7 ⩽ w 2 ⩽ 389.2 & 0 ⩽ w 3 ⩽ 126.2 3 364.9 ⩽ w 1 ⩽ 370.9 & 305.8 ⩽ w 2 ⩽ 352.6 & unspecified 4 341.4 ⩽ w 1 ⩽ 359.8 & 334.0 < w 2 < ∞ & 31.4 < w 3 < ∞ Table 4 High pressure casting data: the 4 rules with highest importance value. rank Rule C 341.4 ⩽ w 1 ⩽ 370.9 & 172.6 < w 2 ⩽ 389.2 & 25.3 ⩽ w 3 ⩽ 142.6 R 364.9 ⩽ w 1 ⩽ 359.8 & 352.7 ⩽ w 2 ⩽ 352.6 & 31.4 < w 3 ⩽ 126.2 Table 5 Scoring of the rules. Gravity Casting High Pressure Casting FDR (%) Recall (%) FDR (%) Recall (%) Rule 1 0.0 3.8 9.0 44.2 Rule 2 0.0 5.9 12.0 14.1 Rule 3 2.4 9.4 0.0 5.8 Rule 4 0.0 14.5 12.5 26.9 Short communication Extracting control variables of casting processes with NMF and rule extraction M. Xarez Conceptualization Software Investigation a P. Weiderer Conceptualization Methodology Software Investigation b A.M. Tomé Conceptualization Methodology Validation Writing - original draft Writing - review & editing Supervision c ⁎ Elmar W. Lang Methodology Validation Writing - original draft Writing - review & editing Supervision d a DETI-Universidade Aveiro, 3810-193 Aveiro, Portugal DETI-Universidade Aveiro 3810-193 Aveiro Portugal DETI-Universidade Aveiro,3810-193, Aveiro, Portugal b Biophysics, University of Regensburg, 93053 Regensburg, Germany Biophysics University of Regensburg 93053 Regensburg Germany Biophysics, University of Regensburg,93053 Regensburg, Germany c DETI IEETA, Universidade Aveiro, 3810-193 Aveiro, Portugal DETI IEETA Universidade Aveiro 3810-193 Aveiro Portugal DETI IEETA, Universidade Aveiro, 3810-193 Aveiro, Portugal d Institute Biophysics, University of Regensburg, 93053 Regensburg, Germany Institute Biophysics University of Regensburg 93053 Regensburg Germany Institute Biophysics, University of Regensburg,93053 Regensburg, Germany ⁎ Corresponding author. Process monitoring and optimization in an industrial setting is a complex topic, a large amount of data is captured and its measurements are affected by many complex relations. There is, therefore, a growing necessity for methods to reduce its complexity and dimension such that it can be interpreted by humans. In this work we study the extraction of interpretable rules derived from components of a non-negative matrix factorization using real data. Keywords Non-negative matrix factorization (NMF) Rule extraction Interpretability Industrial process monitoring 1 Introduction During the last decade manufacturing has evolved to be more intelligent and data driven (Lade, Ghosh, & Srinivasan, 2017). One major key pillar in this ongoing trend are recent advances in the use of machine learning techniques in an industrial production environment (Dogan & Birant, 2021). Such techniques helped in the early detection of defects and production failures, thus increasing productivity and quality (Tao, Qi, Liu, & Kusiak, 2018; Escobar et al., 2018). The casting industry has a thousand years of history and its roots trace back to the mining industry. This traditional background is one of the reasons why modern methods for process monitoring and data mining have not been employed to a significant extent in this domain compared to other industries like, for example, semiconductor manufacturing. From an economic view point, the main motivation behind the recent development of data mining solutions in the metal casting industry has always been the reduction of scrap rate and the optimization of the processing chains. Metal casting is a complex physical process in which a variety of defects can occur and the same defects can usually have multiple causes. Typical defects are related to solidification issues like cracks, cold runs or porosities. Apart from that, also an incorrect filling can cause defects like air entrapment or surface defects. The variety of defects and their possible causes fills books and very detailed documentations can be found in the related engineering literature. Because of that there is a definite need for tools and solutions to speed up the process of root cause analysis, and data mining methods appear to be promising candidates due to their ability to extract complex information and interdependencies from large amounts of process data collected during series production. One goal for machine learning techniques in casting processes is defect prediction. Many casting defects can only be detected by cumbersome investigation methods like X-ray computed tomography (CT) imaging. Those investigations are usually done hours after the actual casting process, and the casting part has already gone through additional processing steps. If casting defects can be predicted in an early step, or even right after the casting of the workpiece, these additional value chain steps can be saved and costs are reduced. The potential for saving costs will increase even further if processing steps are located across different plants. The use of machine learning tools to examine the impact of process parameter values on the final product quality in the casting processes has been studied (Kozłowski, Jakimiuk, Rogalewicz, Sika, & Hajkowski, 2019). It is now common that several process variables (like temperature, pressure and so on) are collected during component manufacturing. Such manufacturing time series have already been used in rule based systems to seek useful knowledge for explaining failures in these processes (de Pisón, Sanz, de Pisón, Jiménez, & Conti, 2012). One of the main applications of machine learning methods is for process monitoring tasks, i.e. the monitoring of a desired quantity (for example a quality index). A common approach for this task is the design of “soft sensors” or “virtual sensors,” which indirectly measure the desired quantity by modeling the process data and its variations. These approaches are typically based on the use of a matrix decomposition technique as a preprocessing step (Kano & Nakagawa, 2008; Keithley, Wightman, & Heien, 2009; Yin, Ding, Xie, & Luo, 2014). More generally speaking, the design of data-driven soft sensors combines an automatic feature extraction technique and the fitting of a regression model (Souza, Araùjo, & Mendes, 2016). The data driven feature extraction methods are based on the factorization of data matrices in order to reveal hidden or latent factors in the data. The three most widely used methods to factorize matrices are Singular Value Decomposition (SVD) or the closely related Principal Component Analysis (PCA) (Jolliffe & Cadima, 2016), Independent Component Analysis (ICA) (Comon, 1994) and Non-Negative Matrix Factorization (NMF) (Lee et al., 1999). All of them impose different constraints like orthogonality, statistical independence or non-negativity on the latent factors. In recent years further research, motivated by the availability of large and high-dimensional data sets as well as the occurrence of large and sparse matrices during data decomposition, lead to new algorithms related with latent factor models with large sparse factor matrices. Examples are collaborative filtering-based recommender systems (Luo et al., 2016), randomized latent factor models (Shang et al., 2019; Chen & Luo, 2018), position-transitional particle swarm optimization (P2SO) algorithms to implement self-adaptation of learning rates (Luo, Yuan, Chen, Zeng, & Wang, 2020) or a biased non-negative latent factorization of tensors model for temporal pattern-aware Quality-of-Service prediction (Luo, Wu, Yuan, & Zhou, 2020). Thus many types of real life data like images, text, audio spectra or multivariate sensor signal time series can be represented as non-negative data matrices, and decomposing them into non-negative factors often yields much simpler and physically intuitive interpretations. The advantage of non-negativity is the fact that the decompositions are strictly additive, i.e. the extracted components (factors) cannot cancel each other. This means that one component and the corresponding coefficient explain the relevance of the component in the original data (Fu, Huang, Sidiropoulos, & Ma, 2019). In a recent work on temperature time series data collected during the continuous production of metal parts with a gravity casting machine (Weiderer, Tomé, & Lang, 2020; Weiderer, Tomé, & Lang, 2019), NMF was employed to extract physically meaningful temperature profiles. It was discussed that each T-profile conveys a distinct information about the underlying heat flow processes. In this work we propose the application of rule extraction methods to interpret the results of an NMF analysis of temperature time series recorded during such thermal manufacturing processes like gravity or high-pressure casting. Rules are commonly considered easily understandable for humans and can therefore be used to design a tool that might help machine operators on their decisions concerning the quality of any processed workpiece. Several studies (Deng, 2019) report strategies to derive rules from tree ensembles, like random forest or boosted trees, as in general these models are accurate but represent black box systems, hence are difficult to understand or deploy. In such tree ensemble scenarios it is often impossible to look into the votes and the structures of the grown trees, even if small sized, to try to understand how a particular prediction is achieved. Therefore, several works have been proposing the generation of rules when working with these classifiers. Thus by producing a set of rules insight into classification decisions might be achieved (Mashayekhi et al., 2017; Wang et al., 2020). One such attempt is RuleFit (Friedman et al., 2008), which proposes a linear model that uses rules in its definition and also a framework to measure the importance of the extracted rules. The contribution of the presented work is an example of physics-guided explainable artificial intelligence (XAI). It combines several machine learning techniques like NMF, Random Forest and RuleFit and applies them for the first time to interpret sensor signals recorded during a casting process in the car industry. The study is composed of • an NMF analysis, which is pursued of temperature and pressure time series recorded during casting metal parts for the car industry. Data embedding is learnt applying the HALS algorithm. • a RuleFit algorithm, which is employed to define rules based on a linear discriminat function, which enters the argument of a logistic decision function. A sparse representation is encouraged resulting from the L1-norm constrained cost function, which is optimized by stochastic gradient descent. • a Rule extraction procedure, which is performed based on a random forest of decision trees. Rules are defined by logical conjunctions of all conditions valid along the path in the tree to the node in question. • Rule importance is evaluated by a combination of optimized coefficients and rule support measures. 2 Methods The search for hidden knowledge within temperature time series is the main goal of the approach discussed here. The proposed strategy is based on the combination of unsupervised matrix factorizations with supervised rule extraction methods as it is illustrated in Fig. 1 . Therefore, the temperature time series are first decomposed using a matrix decomposition approach in order to achieve time series profiles. In (Weiderer et al., 2020) it was shown that NMF decompositions extract component temperature profiles which can be related to thermal processes occurring during the gravity casting process. The coefficients that allow to reconstruct the original time series then constitute a new representation of the data. Building upon this preprocessing, rules will be extracted, following a supervised approach, which use the extracted coefficients and intend to result in an interpretable model which allows to understand the origin of defects. (See Fig. 2 ). 2.1 Time series templates via Non-Negative Matrix Factorization The sensed data can be organized as matrix T ∈ R M × N containing N consecutive recordings of a specific temperature sensor from M consecutively produced workpieces. Then N is the number of time points at which the sensor is sampled during the cast process of one piece. Approximate low-rank matrix factorizations are techniques that replace the original data by a lower dimensional representation. These low rank matrix factorization can be understood as any method that calculates an approximation of the original data matrix with two factor matrices (1) T ≈ WH where the rows of H ∈ R K × N are the characteristic component temperature profiles (time series templates) and the rows of W ∈ R M × K are the related coefficients to reconstruct the originally sensed data. The rows of W thus provide the new representation of each row of the original data matrix in terms of a weighted sum of the component profiles which are common to all recorded temperature time series T . The number of templates, e.g., the inner dimension K of the matrices is a user defined parameter. There are many algorithms available to solve the NMF decomposition (Cichocki, Zdunek, Phan, & Amari, 2009). However in recent years further research, motivated by the availability of large and high-dimensional data sets, lead to new algorithms related with sparse latent factor models Luo et al. (2016), Chen and Luo (2018),uo et al. (2020). In this work the Hierarchical Alternating Least Squares (HALS) algorithm was used to compute sequentially individual columns and rows of W and H , respectively. The key difference between HALS and regular alternating least squares (ALS) is the consecutive optimization of individual column vectors instead of the full matrices, which makes HALS more efficient and flexible than ALS. The HALS algorithm is based on the calculation of residuum matrices defined as E = T - WH = T ( k ) - w * k h k * where E = T - WH represents the error of the approximation and the last term represents a rank-one matrix, which is the outer product of the k - th column of W with the k - th row of H . As cost function the reconstruction error ‖ E ‖ F 2 = ‖ T ( k ) - w * k h k * ‖ F 2 is minimized by computing its gradients while alternatingly fixing w * k or h k * . Setting these gradients to zero and assuming strictly positive entries in all elements, the updating rules are (2) w * k ← T ( k ) h k * T + h k * h k * T (3) h k * ← w * k T T ( k ) + w * k T w * k The updating rules are applied sequentially for the K columns and rows of the two factor matrices, respectively. The procedure is iteratively repeated up to a stop criterion is met. Furthermore notice that to avoid growth of one factor and decay of the other, one of them needs to be normalized at each iteration. For example, the columns w * k can be scaled to norm one. Another important issue with NMF algorithms is related with the initialization of the factor matrices. Random initialization is almost never advisable, because the solutions will be different for each run and the number of iterations needed for a good fit will be larger. In practice thereby an algorithm is ran several times using different random initializations and the solution which offers the best approximation is finally picked. However, better strategies are discussed in literature to perform a more educated initialization (Langville, Meyer, Albright, Cox, & Duling, 2014). One of the solutions is based on the use of an SVD of the data matrix (Boutsidis & Gallopoulos, 2008) or by using domain knowledge (Weiderer et al., 2019). In the latter case, representing physics-guided explainable artificial intelligence (XAI), the rows of matrix H were initialized with exponential functions, which reflect simple physical heat transfer phenomena, like cooling and heating (Weiderer et al., 2020). Consequently the matrix W is here initialized using the definition of the factorization model (Eq. (1)) and the calculation of the pseudo-inverse H † . 2.2 RuleFit algorithm RuleFit (Friedman et al., 2008) is a linear model that besides the features also includes their interactions in form of decision rules. These decision rules represent new binary variables for the linear model. To turn the model more robust to outliers, the original feature values are also winsorized, i.e. their statistics is clipped (Hastings, Mosteller, Tukey, & Winsor, 1947; Tukey, 1962). The RuleFit discriminant function (Friedman et al., 2008) is written as (4) F ( w m * ) = a 0 + ∑ l = 1 L a l r l ( w m * ) + ∑ k = 1 K b k ℓ k ( w mk ) where r l ( . ) denotes the logical result of the l - th rule applied to the feature vector, ℓ k ( . ) represents the winsorised value (Tukey, 1962) of the k - th entry of the feature vector w m * ∈ R K . The adaptation of the model implies the extraction of L rules before the calculation of the coefficients of F ( w m * ) . For binary classification problems, the integration of the function F ( w m * ) into the logistic function framework allows to define a suitable cost function for the optimization procedure. With that purpose, the logistic function is defined as (5) g m ( w m * ) = 1 1 + exp ( - d m · F ( w m * ) ) where d m ∈ { - 1 , 1 } is the label of the feature vector w m * and g m ( w m * ) attains a positive value in the interval ( 0 , 1 ) . Note that if d m · F ( w m * ) ⩾ 0 , the logistic function can attain values g m ( w m * ) ≃ 1 while if d m · F ( w m * ) < 0 the logistic functions may attain values g m ( w m * ) ≃ 0 ) . Meaning that the model coefficients are or are not properly adapted to the training pair ( w m * , d m ) . Therefore the cost function L includes the logarithm log g m ( w m * ) estimated on the M examples of the training set to represent the fitting of the model to the data. Additionally a sparse representation is encouraged by an L 1 - regularization. This regularized cost function (6) L = - ∑ m = 1 M log g m ( w m * ) + λ ∑ l = 1 L | a l | + ∑ k = 1 K | b k | is optimized with respect to the coefficients a l , b k ; l = 1 , … , L , k = 1 , … , K by stochastic gradient decent (SGD) according to (7) min a l , b k L ( w m * , a l , b k ) ∀ l = 1 , … , L ; k = 1 , … , K 2.2.1 Rule extraction The random forest (RF) algorithm is an ensemble of binary decision trees, each performing a classification, being the final decision taken by majority voting. Decision trees represent machine learning models, which are interpretable, i.e., it is possible to understand how a prediction is achieved. However decisions in random forest are not so simple to follow as they depend on the ensemble. The RuleFit algorithm extracts all possible rules derived from the random forest to achieve interpretable decisions. For the described RuleFit algorithm, a rule is defined in every node (internal or leaf) in the ensemble. Such a rule is defined by the logical conjunction of all conditions found in the path between the root and this particular node. These conditions are the ones created during the training phase to create the nodes of the tree and can be considered logical tests to one of the features of feature vector. Finally the number of conditions of the rule is correlated with the length of the path between the root and the node. After generating all possible rules implicit in the tree ensemble, there are duplicate rules and rules with redundant conditions. In Sirikulviriya and Sinthupinyo (2011) a collection of heuristics is described to prune the set of rules created from the ensemble. 2.2.2 Importance of the rules After optimizing the cost function, most of the coefficients a l associated to each rule have declined to zero or close to zero. Therefore the coefficient values provide a direct measure of the importance of the rule for the decision. Another relevant indicator is the support of the rule, which is defined as s l = 1 M ∑ m = 1 M r l w m * related with the rate of compliance of the l - th out of M elements of the training set. Therefore (Friedman et al., 2008) also proposes that the importance of each rule might be defined as (8) I l = a l · s l 1 - s l The importance values allow ranking the rules created by the algorithm. 2.2.3 Evaluation metrics Each individual rule that is extracted from the RuleFit algorithm can be interpreted as a set of conditions under which the casting process is optimized for minimizing its NOK outcomes. Following this interpretation, the NOK recall score of a rule indicates the percentage of errors that would be reduced in the process if the conditions are followed. Nonetheless, even if the recall is 1.00 a rule may be too restrictive and therefore be nonviable or impossible to implement in the industrial process. Therefore we must also analyze the NOK precision score to understand if a rule is too restrictive, i.e., it delivers too many NOK mislabels. The following evaluation metrics have been used to score the extracted rules. • The false discovery rate (FDR), given as one minus the precision (Pr), measures the rate of type I error in null hypothesis testing with multiple comparisons. It is given by FDR = 1 - Pr = FP FP + TP , Pr = TP FP + TP where FP denotes the false positives and TP the true positives with respect to the null hypothesis. Hence it measures the percentage of false OK votes within all votes the model considers as OK. This is an important score because if it is high it means that many defects would be considered as non-defects, which would be a disaster for every car parts producer. • The recall, also called true positive rate (TPR), gives the percentage of non-defected, truly OK parts actually detected in the group of all real non-defect parts according to TPR = TP P = TP TP + FN Note that the FDR is very relevant in the production of critical car parts as it expresses the possibility of defects not being identified as such but labeled as non-defects. While the recall indicates the percentage of non-defects correctly identified. If this score is low it indicates that many non-defect parts are falsely identified as defect and therefore sorted out with a consequent waste of production costs. In an ideal scenario FDR should be very low while recall should have an high value. Anyway, in any production line the FDR score is the most relevant score indicating production quality. 3 Data collection This work deals with the analysis of temperature sensor recordings, collected during the casting of automobile parts. The collected data is related with two kinds of casting: gravity and high-pressure. Casting processes are not always stable and at the end of the processing chain a considerable percentage of workpieces is discarded. The quality control test is performed in our case based on a CT check, which sorts out all defect parts. Therefore, for every cast part the quality check tag o.k. (OK) or not o.k. (NOK) as well as the time series associated to the temperature sensors are associated with a unique identifier and stored. The defects are usually identified at the end of the fabrication pipeline or, if defects are visible, marked by the operator. But it is normally difficult to identify and understand the cause of the defect. This is why the data is analyzed with a physics - guided data decomposition model. The latter extracts features (component temperature profiles) used to generate rules, which minimize the number of defect parts produced. 3.1 Gravity casting temperature profiles Several sensors are placed inside the mold and record temperature time series during the duration of a casting process, which lasts for 160 s. During this process each sensor realizes a measurement every 5 s. Therefore each time series has 33 samples for every workpiece that passes through the casting process. The data set was collected during one month and 4428 parts were produced of which only 69 were marked NOK. Fig. 3 shows six out of the eight recorded times series. The temperature at any sensor quickly raises while the liquid metal approaches the locations where the sensors are placed. The peak of the temperature at any sensor should be close to when the liquid stops being poured and the cooling down starts. There is an exception to this behavior, which is seen by sensor 2 because it is placed near the place where the cooling channel wraps the casting form. This cooling channel contains cold water which, however, does not flow continuously but in a pulsed fashion. The data set analyzed in this study is the one recorded by sensor 8. 3.2 High-pressure casting temperature profiles This data set was collected during the pre-production phase, where the casting machine was still modified several times and “production” was interrupted frequently. This means that the machine was not continuously working and that several machine settings were often changed. In total 990 parts were processed, out of which 128 were defect and therefore labeled as NOK. The Fig. 3 shows one second of a registered temperature time series. In this case the sampling rate was 250 Hz . The high-pressure casting process is of much shorter duration and the complete time series only lasts for 12 s. For the study, the selected segment includes the filling of the cavity with the liquid metal, where the exponential growth is observed followed by a slow quasi-exponential cooling-down period. 4 Feature extraction by NMF 4.1 Component temperature profiles Recently such T-profiles were analyzed by extracting three component temperature profiles with meaningful information using a non-negative matrix decomposition. The work reported by Weiderer et al. (2020) and Weiderer et al. (2019) presents a detailed evaluation of the use of NMF for the extraction of component temperature profiles of the gravity casting process. The analysis shows that NMF captures a small number of physically meaningful component temperature profiles. In particular, the coefficients of one component captures the changing condition of the release agent (Weiderer et al., 2020), which is an important processing parameter that cannot be measured directly and is only obtainable through the NMF analysis. We follow the same methodology to analyze our data for both the gravity casting as well as for the high-pressure data set. Fig. 4 presents for both data sets these extracted component temperature profiles (or time series templates), which correspond to the rows of matrix H . Corresponding coefficients, collected in the weight matrix W are illustrated in Fig. 5 . The former figure shows the profiles obtained when the algorithm was initialized either randomly or with initial conditions which are related to the physical heat conduction processes of the casting process (Weiderer et al., 2020). The results show that the temperature profiles are stable and do not depend on the initialization. In the case of gravity casting, the algorithm converges to the same components for both initialization. This also holds for the high pressure casting case except for a permutation of the order of the 2nd and 3rd components. That is to say that the profile of component 2 with random initialization looks similar to the profile of component 3 with knowledge-based initialization and vice versa. This is most probably due to the permutation indeterminacy of all matrix factorization algorithms if no normalization is applied. Hence for both casting processes the resulting profiles are practically identical for both types of initialization. However, the component profiles for the gravity casting as compared to the high pressure casting look very different. As has been shown in Weiderer et al. (2020), the component profiles correspond to a cooling process (component 3) and a combination of a heating and subsequent cooling process (component 2), while component 1 reflects the average temperature behaviour of the gravity casting process. In case of high pressure casting components 2 and 3 both reflect a combination of a heating and a cooling process. As expected, both processes happen on much shorter time scales than in case of gravity casting. While one component (say component 2) shows a delayed second heating and cooling event with low amplitudes, the other component (component 3) exhibits a transition to a slowed down cooling process after an initial fast cooling down. It is interesting to see that this transition happens when the second heating/cooling cycle sets in at component 2. Only when the delayed cooling process in component 3 is finished, at the average temperature profile (component 1) a slow exponential cooling down process sets in. Considering the related weights of the component profiles, in the case of the gravity casting, the most interesting conclusion that can be drawn is the correlation between the coefficients for the second NMF component profile and the application of a coating release agent to the inner surface of the casting mold (Weiderer et al., 2020). Whenever this release agent was applied, the coefficient had a large sudden change on its value. From the point of view of the physical interpretation of the component profiles this makes sense: Applying the coating should slow down the transfer of heat. Note that in case of the high pressure casting no release agent needs to be applied.All further analysis referring to rule extraction has been performed with the results obtained with knowledge-based initialization(Xarez, 2019). 5 Rule extraction via a random forest At first glance the component temperature profiles appear very similar, only that those of the high pressure casting seem to be highly compressed on the time axis due to the much faster casting process. The physical interpretation of these components has been discussed in detail in Weiderer et al. (2020) and will not be repeated here. Rather we discuss rule extraction methods to ease the interpretation of the components extracted from the data. Extracting the rules is achieved via a random forest classifier. Commonly correlation analysis is applied as a pre-processing step when a classification analysis is intended. However, as the investigated dataset is structured (time series) and an NMF is applied for dimension reduction and feature extraction, any correlation analysis deemed not meaningful. 5.1 Supervised Learning and Rule extraction After the NMF decomposition, the coefficients collected in the weight matrix ( W ) can be considered the new data representation. The latter is employed to train a random forest (RF) ensemble tree classifier. Next rules are extracted from the individual trees and the RuleFit methodology is used to construct a sparse linear model. These rules are studied and the most relevant ones are used to classify the data into the two classes: OK and NOK. 5.1.1 Random Forest This work explores the use of the coefficients in each row of W in order to assign one of the two classes, OK and NOK, to the casting process represented by the T-profile of the corresponding row of the data matrix T . To this end, we apply an RF classifier to the feature vectors w m * = ( w m 1 w m 2 w m 3 ) . An experimental study was conducted using a 5-fold cross-validation to assign the depth of the trees and the number of trees. The recall measure was used for both classes as optimization criterion. Note that the data set is highly unbalanced. For this reason, the RF classifier used a cost sensitive technique to balance the weights for each class as proposed by Chen, Liaw, and Breiman (2004), and the Gini importance (Breiman, 2001) was the criterion to choose the split condition at the nodes. 5.1.2 RuleFit and Rule importance The number of rules extracted for the data sets differs for each set as is shown in Fig. 6 . However, as expected, the importance value is close to zero for most of the rules. Following all four sets of rules (see Table 1 ) consistently would impose the following consensus rules (C). Alternatively, a set of the most restrictive rules (R) can be formulated as well. It can easily be seen, that occasionally the rules contradict each other and cannot be fulfilled at the same time as is shown by the set of restrictive rules in Table 2 . A simple explanation to this apparent contradiction is that the rules do not always model the same subset (same time period or same machine configuration) of the data. This is illustrated in Fig. 7 , which shows different periods, where the data complies to different rules. On the other hand one might conclude that, given some variability of the machine settings or time intervals, a near optimal set of weights w 1 = 415.7 ± 0.7 & w 2 = 252.4 ± 3.8 & w 3 = 246.9 ± 4.3 might be extracted from the restrictive set of rules. This set of optimal rules, which needs to hold simultaneously, would be valid across all machine settings and time intervals. Remember that the rules can be interpreted as a set of conditions under which the casting process is optimized such that a minimal number of NOK labeled workpieces results. A similar consideration holds in case of high pressure casting as Tables 3 and 4 show. 5.1.3 RuleFit scores Considering the evaluation metrics, for the four most important rules extracted with the RuleFit methodology, see Eq. (8), their scores are collected for both casting methods in the following Table 5 . While gravity casting overall shows a good precision, i.e. a vanishing (or almost vanishing) false discovery rate, the recall is generally quite poor. This can be related to the extreme imbalance in the classes. If even a small percentage of OK workpieces is falsely misclassified as NOK, the precision becomes quite low. Given this observation, Rule 4 might be considered the most important rule to follow in case of gravity casting. Considering the case of high pressure casting, scores turn less reliable due to the fact that no consistent data set has been available due to the test phase conditions while the casting machine was operated. However, considering the utmost importance of a low FDR, Rule 3 seems to best define the conditions under which high pressure casting should be performed. These rules now need to be combined with the physical interpretation of the component temperature profiles, as given in (Weiderer et al., 2020), to deduce proper process handling by the line operator. Hence when the goal is to minimize the scrap rate, the following recommendation follows from the analysis: • In case of gravity casting, a near optimal set of weights for the three extracted temperature profiles might be: w 1 = 415.7 ± 0.7 & w 2 = 252.4 ± 3.8 & w 3 = 246.9 ± 4.3 • In case of high pressure casting, a simlar set might be: w 1 = 362.4 ± 2.5 & w 2 = 352.65 ± 0.05 & w 3 = 78.8 ± 47.4 6 Conclusion The study is based on a recent investigation in our lab of gravity and high pressure casting data collected at a leading car manufacturer during the casting of car work pieces. The preceding investigation deduced component temperature profiles from gravity casting processes (Weiderer et al., 2020). The current investigation applied a rule extraction methodology to these data as well as additional data from a high pressure casting process. The extracted rules were coupled with the physical meaning of each component profile. In that way we could conclude that the coefficient related with baseline temperature should be within a certain threshold; the coefficient of the heat conduction component should be lower than a certain threshold; and then the coefficient of the initial temperature of the mold should be high enough. If all these conditions are followed, one can expect less defects during production. Given the high variability of the data, the extracted rules were in most cases rather precise and helped to define the conditions for a parts production with minimal defect parts. CRediT authorship contribution statement M. Xarez: Conceptualization, Software, Investigation. P. Weiderer: Conceptualization, Methodology, Software, Investigation. A.M. Tomé: Conceptualization, Methodology, Validation, Writing - original draft, Writing - review & editing, Supervision. Elmar W. Lang: Methodology, Validation, Writing - original draft, Writing - review & editing, Supervision. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Appendix A Supplementary data Supplementary data associated with this article can be found, in the online version, Supplementary data The following are the Supplementary data to this article: Supplementary data References Boutsidis and Gallopoulos, 2008 Boutsidis, C. & Gallopoulos, E. (2008). SVD based initialization: A head start for nonnegative matrix factorization. Pattern Recognition, 41 (4): 1350–1362. ISSN 0031–3203. C. Boutsidis and E. Gallopoulos. SVD based initialization: A head start for nonnegative matrix factorization. Pattern Recognition, 41 (4): 1350 – 1362, 2008. ISSN 0031-3203. 10.1016/j.patcog.2007.09.010. URL Breiman, 2001 Breiman, L. (2001). Random forests. Machine Learning, 45 (1): 5–32. ISSN 1573–0565. doi: 10.1023/A:1010933404324. L. Breiman. Random forests. Machine Learning, 45 (1): 5–32, 2001. ISSN 1573-0565. Chen et al., 2004 C. Chen A. Liaw L. Breiman Using random forest to learn imbalanced data University of California, Berkeley 110 1–12 2004 24 C. Chen, A. Liaw, and L. Breiman. Using random forest to learn imbalanced data. University of California, Berkeley, 110 (1-12): 24, 2004. Chen and Luo, 2018 J. Chen X. Luo Randomized latent factor model for high-dimensional and sparse matrices from industrial applications In 2018 IEEE 15th International Conference on Networking, Sensing and Control (ICNSC) 2018 1 7 10.1109/ICNSC.2018.8361355 J. Chen and X. Luo. Randomized latent factor model for high-dimensional and sparse matrices from industrial applications. In 2018 IEEE 15th International Conference on Networking, Sensing and Control (ICNSC), pages 1–7, 2018. 10.1109/ICNSC.2018.8361355. Cichocki et al., 2009 A. Cichocki R. Zdunek A.H. Phan S. Amari Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-Way Data Analysis and Blind Source Separation Wiley and Sons 2009 10.1002/9780470747278 A. Cichocki, R. Zdunek, A.H. Phan, and S. Amari. Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-Way Data Analysis and Blind Source Separation. Wiley and Sons, 2009. 10.1002/9780470747278. Comon, 1994 P. Comon Independent component analysis, a new concept? Signal Processing 36 3 1994 287 314 P. Comon. Independent component analysis, a new concept? Signal Processing, 36 (3): 287–314, 1994. Deng, 2019 Deng, H. (Jun 2019). Interpreting tree ensembles with in trees. International Journal of Data Science and Analytics, 7 (4): 277–287. ISSN 2364–4168. H. Deng. Interpreting tree ensembles with in trees. International Journal of Data Science and Analytics, 7 (4): 277–287, Jun 2019. ISSN 2364-4168. 10.1007/s41060-018-0144-8. de Pisón et al., 2012 de Pisón, F.J.M., Sanz, A., de Pisón, E.M., Jiménez, E. & Conti, D. (2012) Mining association rules from time series to explain failures in a hot-dip galvanizing steel line. Computers & Industrial Engineering, 63 (1): 22–36. ISSN 0360–8352. F.J.M. de Pisón, A. Sanz, E.M. de Pisón, E. Jiménez, and D. Conti. Mining association rules from time series to explain failures in a hot-dip galvanizing steel line. Computers & Industrial Engineering, 63 (1): 22 – 36, 2012. ISSN 0360-8352. 10.1016/j.cie.2012.01.013. Dogan and Birant, 2021 Dogan, A. & Birant, D. (2021). Machine learning and data mining in manufacturing. Expert Systems with Applications, 166: 114060. ISSN 0957–4174. A. Dogan and D. Birant. Machine learning and data mining in manufacturing. Expert Systems with Applications, 166: 114060, 2021. ISSN 0957-4174. 10.1016/j.eswa.2020.114060. Escobar et al., 2018 C.A. Escobar R. Morales-Menendez Machine learning techniques for quality control in high conformance manufacturing environment Advances in Mechanical Engineering 10 2 2018 10.1177/1687814018755519 1687814018755519 C.A. Escobar and R. Morales-Menendez. Machine learning techniques for quality control in high conformance manufacturing environment. Advances in Mechanical Engineering, 10 (2): 1687814018755519, 2018. 10.1177/1687814018755519. Friedman et al., 2008 J.H. Friedman B.E. Popescu Predictive learning via rule ensembles Ann. Appl. Stat. 2 3 2008 916 954 10.1214/07-AOAS148 J.H. Friedman and B.E. Popescu. Predictive learning via rule ensembles. Ann. Appl. Stat., 2 (3): 916–954, 09 2008. 10.1214/07-AOAS148. Fu et al., 2019 X. Fu K. Huang N.D. Sidiropoulos W. Ma Nonnegative matrix factorization for signal and data analytics: Identifiability, algorithms, and applications IEEE Signal Processing Magazine 36 2 2019 59 80 X. Fu, K. Huang, N.D. Sidiropoulos, and W. Ma. Nonnegative matrix factorization for signal and data analytics: Identifiability, algorithms, and applications. IEEE Signal Processing Magazine, 36 (2): 59–80, 2019. Hastings et al., 1947 Hastings, C., Mosteller, F., Tukey, J.W. & Winsor, C.P. (1947) Low moments for small samples: A comparative study of order statistics. The Annals of Mathematical Statistics, 18 (3): 413–426. ISSN 00034851. C. Hastings, F. Mosteller, J.W. Tukey, and C.P. Winsor. Low moments for small samples: A comparative study of order statistics. The Annals of Mathematical Statistics, 18 (3): 413–426, 1947. ISSN 00034851. URL: Jolliffe and Cadima, 2016 I.T. Jolliffe J. Cadima Principal component analysis: a review and recent developments Phil. Trans. R. Soc. A 374 2016 2015202 10.1098/rsta.2015.0202 I.T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Phil. Trans. R. Soc. A, 374: 2015202, 2016. URL Kano and Nakagawa, 2008 Kano, M. & Nakagawa, Y. (2008). Data-based process monitoring, process control, and quality improvement: Recent developments and applications in steel industry. Computers and Chemical Engineering, 32 (1): 12–24. ISSN 0098–1354. M. Kano and Y. Nakagawa. Data-based process monitoring, process control, and quality improvement: Recent developments and applications in steel industry. Computers and Chemical Engineering, 32 (1): 12 – 24, 2008. ISSN 0098-1354. 10.1016/j.compchemeng.2007.07.005. URL Keithley et al., 2009 Keithley, R.B., Wightman, R.M., & Heien, M.L. (2009) Multivariate concentration determination using principal component regression with residual analysis. TrAC Trends in Analytical Chemistry, 28 (9): 1127–1136. ISSN 0165–9936. R.B. Keithley, R.M. Wightman, and M.L. Heien. Multivariate concentration determination using principal component regression with residual analysis. TrAC Trends in Analytical Chemistry, 28 (9): 1127 – 1136, 2009. ISSN 0165-9936. 10.1016/j.trac.2009.07.002. Kozłowski et al., 2019 J. Kozłowski M. Jakimiuk M. Rogalewicz R. Sika J. Hajkowski Analysis and control of high-pressure die-casting process parameters with use of data mining tools Lecture Notes in Mechanical Engineering 2019 10.1007/978-3-030-18789-7_22 J. Kozłowski, M. Jakimiuk, M. Rogalewicz, R. Sika, and J. Hajkowski. Analysis and control of high-pressure die-casting process parameters with use of data mining tools. In Lecture Notes in Mechanical Engineering, 2019. 10.1007/978-3-030-18789-7_22. Lade et al., 2017 P. Lade R. Ghosh S. Srinivasan Manufacturing analytics and industrial internet of things IEEE Intelligent Systems 32 3 2017 74 79 P. Lade, R. Ghosh, and S. Srinivasan. Manufacturing analytics and industrial internet of things. IEEE Intelligent Systems, 32 (3): 74–79, 2017. Langville et al., 2014 Langville, A., Meyer, C., Albright, R., Cox, J. & Duling, D. (2014) Algorithms, initializations, and convergence for the nonnegative matrix factorization. arXiv:1407.7299. A. Langville, C. Meyer, R. Albright, J. Cox, and D. Duling. Algorithms, initializations, and convergence for the nonnegative matrix factorization, 2014. arXiv:1407.7299. Lee et al., 1999 D. Lee H. Seung Learning the parts of objects by non-negative matrix factorization Nature 401 6755 1999 788 791 D. Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401 (6755): 788–791, 1999. Luo et al., 2020 X. Luo H. Wu H. Yuan M. Zhou Temporal pattern-aware qos prediction via biased non-negative latent factorization of tensors IEEE Transactions on Cybernetics 50 5 2020 1798 1809 10.1109/TCYB.2019.2903736 X. Luo, H. Wu, H. Yuan, and M. Zhou. Temporal pattern-aware qos prediction via biased non-negative latent factorization of tensors. IEEE Transactions on Cybernetics, 50 (5): 1798–1809, 2020a. 10.1109/TCYB.2019.2903736. Luo et al., 2020 X. Luo Y. Yuan S. Chen N. Zeng Z. Wang Position-transitional particle swarm optimization-incorporated latent factor analysis IEEE Transactions on Knowledge and Data Engineering 2020 10.1109/TKDE.2020.3033324 1–1 X. Luo, Y. Yuan, S. Chen, N. Zeng, and Z. Wang. Position-transitional particle swarm optimization-incorporated latent factor analysis. IEEE Transactions on Knowledge and Data Engineering, pages 1–1, 2020b. 10.1109/TKDE.2020.3033324. Luo et al., 2016 X. Luo M. Zhou S. Li Z. You Y. Xia Q. Zhu A nonnegative latent factor model for large-scale sparse matrices in recommender systems via alternating direction method IEEE Transactions on Neural Networks and Learning Systems 27 3 2016 579 592 10.1109/TNNLS.2015.2415257 X. Luo, M. Zhou, S. Li, Z. You, Y. Xia, and Q. Zhu. A nonnegative latent factor model for large-scale sparse matrices in recommender systems via alternating direction method. IEEE Transactions on Neural Networks and Learning Systems, 27 (3): 579–592, 2016. 10.1109/TNNLS.2015.2415257. Mashayekhi et al., 2017 M. Mashayekhi R. Gras Rule extraction from decision trees ensembles: new algorithms based on heuristic search and sparse group lasso methods International Journal of Information Technology and Decision Making 16 6 2017 1707 1727 10.1142/S0219622017500055 M. Mashayekhi and R. Gras. Rule extraction from decision trees ensembles: new algorithms based on heuristic search and sparse group lasso methods. International Journal of Information Technology and Decision Making, 16 (6): 1707–1727, 2017. 10.1142/S0219622017500055. Shang et al., 2019 M. Shang X. Luo Z. Liu J. Chen Y. Yuan M. Zhou Randomized latent factor model for high-dimensional and sparse matrices from industrial applications IEEE/CAA Journal of Automatica Sinica 6 1 2019 131 141 10.1109/JAS.2018.7511189 M. Shang, X. Luo, Z. Liu, J. Chen, Y. Yuan, and M. Zhou. Randomized latent factor model for high-dimensional and sparse matrices from industrial applications. IEEE/CAA Journal of Automatica Sinica, 6 (1): 131–141, 2019. 10.1109/JAS.2018.7511189. Sirikulviriya and Sinthupinyo, 2011 N. Sirikulviriya S. Sinthupinyo Integration of rules from a random forest In International Conference on Information and Electronics Engineering 2011 N. Sirikulviriya and S. Sinthupinyo. Integration of rules from a random forest. In International Conference on Information and Electronics Engineering, 2011. Souza et al., 2016 Souza, F.A., Araùjo, R. & Mendes, J. (2016) Review of soft sensor methods for regression applications. Chemometrics and Intelligent Laboratory Systems, 152: 69–79. ISSN 0169–7439. F.A. Souza, R. Araùjo, and J. Mendes. Review of soft sensor methods for regression applications. Chemometrics and Intelligent Laboratory Systems, 152: 69 – 79, 2016. ISSN 0169-7439. URL Tao et al., 2018 Tao, F., Qi, Q., Liu, A., & Kusiak, A. (2018) Data-driven smart manufacturing. Journal of Manufacturing Systems, 48: 157–169. ISSN 0278–6125. Special Issue on Smart Manufacturing. F. Tao, Q. Qi, A. Liu, and A. Kusiak. Data-driven smart manufacturing. Journal of Manufacturing Systems, 48: 157 – 169, 2018. ISSN 0278-6125. 10.1016/j.jmsy.2018.01.006. URL Special Issue on Smart Manufacturing. Tukey, 1962 Tukey, J.W. (1962) The future of data analysis. Ann. Math. Statist., 33 (1): 17–19. ISSN 0003–4851. URL: J.W. Tukey. The future of data analysis. Ann. Math. Statist., 33 (1): 17–19, 1962. ISSN 0003-4851. 10.1214/aoms/1177704711. URL: Wang et al., 2020 Wang, S., Wang, Y., Wang, D., Yin, Y., Wang, Y., & Jin, Y. (2020) An improved random forest-based rule extraction method for breast cancer diagnosis. Applied Soft Computing, 86: 105941. ISSN 1568–4946. URL: S. Wang, Y. Wang, D. Wang, Y. Yin, Y. Wang, and Y. Jin. An improved random forest-based rule extraction method for breast cancer diagnosis. Applied Soft Computing, 86: 105941, 2020. ISSN 1568-4946. 10.1016/j.asoc.2019.105941. URL: Weiderer et al., 2019 Weiderer, P., Tomé, A.M., & Lang, E.W. (2019) Decomposing temperature time series with non-negative matrix factorization. arXiv:1904.02217. P. Weiderer, A.M. Tomé, and E.W. Lang. Decomposing temperature time series with non-negative matrix factorization, 2019. arXiv:1904.02217. Weiderer et al., 2020 Weiderer, P., Tomé, A.M., & Lang, E.W. (2020) A NMF-based extraction of physically meaningful components from sensory data of metal casting processes. Journal of Manufacturing Systems, 54: 62–73. ISSN 0278–6125. P. Weiderer, A.M. Tomé, and E.W. Lang. A NMF-based extraction of physically meaningful components from sensory data of metal casting processes. Journal of Manufacturing Systems, 54: 62 – 73, 2020. ISSN 0278-6125. 10.1016/j.jmsy.2019.09.013. Xarez, 2019 M. Xarez Extracting control variables from manufacturing processes with machine learning techniques MsS University Aveiro 2019 Xarez, M. (2019). Extracting control variables from manufacturing processes with machine learning techniques. MsS University Aveiro. Yin et al., 2014 S. Yin S.X. Ding X. Xie H. Luo A review on basic data-driven approaches for industrial process monitoring IEEE Transactions on Industrial Electronics 61 11 2014 6418 6428 10.1109/TIE.2014.2301773 S. Yin, S.X. Ding, X. Xie, and H. Luo. A review on basic data-driven approaches for industrial process monitoring. IEEE Transactions on Industrial Electronics, 61 (11): 6418–6428, 2014. 10.1109/TIE.2014.2301773. "
    },
    {
        "doc_title": "Analysis of eyewitness testimony using electroencephalogram signals",
        "doc_scopus_id": "85122494850",
        "doc_doi": "10.1109/EMBC46164.2021.9630054",
        "doc_eid": "2-s2.0-85122494850",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Electroencephalogram signals",
            "Eyewitness testimony",
            "Face identification",
            "Features extraction",
            "Frequency domains",
            "Interface strategies",
            "Single trial",
            "Support vectors machine",
            "Time domain",
            "Time frequency",
            "Algorithms",
            "Brain-Computer Interfaces",
            "Electroencephalography",
            "Humans",
            "Support Vector Machine"
        ],
        "doc_abstract": "© 2021 IEEE.Face recognition and related psychological phenomenon have been the subject of neurocognitive studies during last decades. More recently the problem of face identification is also addressed to test the possibility of finding markers on the electroencephalogram signals. To this end, this work presents an experimental study where Brain Computer Interface strategies were implemented to find features on the signals that could discriminate between culprit and innocent. The feature extraction block comprises time domain and frequency domain characteristics of single-trial signals. The classification block is based on a support vector machine and its performance for the best ranked features. The data analysis comprises the signals of a cohort of 28 participants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Brain Connectivity Studies on Structure-Function Relationships: A Short Survey with an Emphasis on Machine Learning",
        "doc_scopus_id": "85107664035",
        "doc_doi": "10.1155/2021/5573740",
        "doc_eid": "2-s2.0-85107664035",
        "doc_date": "2021-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Neuroscience (all)",
                "area_abbreviation": "NEUR",
                "area_code": "2800"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Computational simulation",
            "Functional interaction",
            "Functional magnetic resonance imaging",
            "Machine learning techniques",
            "Neuronal activities",
            "Structural connectivity",
            "Structural networks",
            "Structure-function relationship",
            "Brain",
            "Brain Mapping",
            "Diffusion Tensor Imaging",
            "Machine Learning",
            "Magnetic Resonance Imaging",
            "Nerve Net",
            "Neural Pathways",
            "Structure-Activity Relationship"
        ],
        "doc_abstract": "© 2021 Simon Wein et al.This short survey reviews the recent literature on the relationship between the brain structure and its functional dynamics. Imaging techniques such as diffusion tensor imaging (DTI) make it possible to reconstruct axonal fiber tracks and describe the structural connectivity (SC) between brain regions. By measuring fluctuations in neuronal activity, functional magnetic resonance imaging (fMRI) provides insights into the dynamics within this structural network. One key for a better understanding of brain mechanisms is to investigate how these fast dynamics emerge on a relatively stable structural backbone. So far, computational simulations and methods from graph theory have been mainly used for modeling this relationship. Machine learning techniques have already been established in neuroimaging for identifying functionally independent brain networks and classifying pathological brain states. This survey focuses on methods from machine learning, which contribute to our understanding of functional interactions between brain regions and their relation to the underlying anatomical substrate.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Local-LDA: Open-Ended Learning of Latent Topics for 3D Object Recognition",
        "doc_scopus_id": "85090491146",
        "doc_doi": "10.1109/TPAMI.2019.2926459",
        "doc_eid": "2-s2.0-85090491146",
        "doc_date": "2020-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Classification performance",
            "Latent Dirichlet allocation",
            "Low-level features",
            "Object category recognition",
            "State-of-the-art approach",
            "Statistical features",
            "Structural semantics"
        ],
        "doc_abstract": "© 1979-2012 IEEE.Service robots are expected to be more autonomous and work effectively in human-centric environments. This implies that robots should have special capabilities, such as learning from past experiences and real-time object category recognition. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e., visual topics), from low-level feature co-occurrences, for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. In this way, the advantages of both the (hand-crafted) local features and the (learned) structural semantic features have been considered and combined in an efficient way. An extensive set of experiments has been performed to assess the performance of the proposed Local-LDA in terms of descriptiveness, scalability, and computation time. Experimental results show that the overall classification performance obtained with Local-LDA is clearly better than the best performances obtained with the state-of-the-art approaches. Moreover, the best scalability, in terms of number of learned categories, was obtained with the proposed Local-LDA approach, closely followed by a Bag-of-Words (BoW) approach. Concerning computation time, the best result was obtained with BoW, immediately followed by the Local-LDA approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Study on the usage feasibility of continuous-wave radar for emotion recognition",
        "doc_scopus_id": "85077493828",
        "doc_doi": "10.1016/j.bspc.2019.101835",
        "doc_eid": "2-s2.0-85077493828",
        "doc_date": "2020-04-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Classification algorithm",
            "Classifier performance",
            "Emotion classification",
            "Emotion recognition",
            "Emotional experiences",
            "Experimental set up",
            "K-nearest neighbours",
            "Vital signs monitoring"
        ],
        "doc_abstract": "© 2020 The AuthorsNon-contact vital signs monitoring has a wide range of applications, such as in safe drive and in health care. In mental health care, the use of non-invasive signs holds a great potential, as it would likely enhance the patient's adherence to the use of objective measures to assess their emotional experiences, hence allowing for more individualized and efficient diagnoses and treatment. In order to evaluate the possibility of emotion recognition using a non-contact system for vital signs monitoring, we herein present a continuous wave radar based on the respiratory signal acquisition. An experimental set up was designed to acquire the respiratory signal while participants were watching videos that elicited different emotions (fear, happiness and a neutral condition). Signal was registered using a radar-based system and a standard certified equipment. The experiment was conducted to validate the system at two levels: the signal acquisition and the emotion recognition levels. Vital sign was analysed and the three emotions were identified using different classification algorithms. Furthermore, the classifier performance was compared, having in mind the signal acquired by both systems. Three different classification algorithms were used: the support-vector machine, K-nearest neighbour and the Random Forest. The achieved accuracy rates, for the three-emotion classification, were within 60% and 70%, which indicates that it is indeed possible to evaluate the emotional state of an individual using vital signs detected remotely.",
        "available": true,
        "clean_text": "serial JL 273545 291210 291874 291880 291883 31 90 Biomedical Signal Processing and Control BIOMEDICALSIGNALPROCESSINGCONTROL 2020-01-09 2020-01-09 2020-01-09 2020-01-09 2021-02-08T15:55:36 S1746-8094(19)30416-1 S1746809419304161 10.1016/j.bspc.2019.101835 S300 S300.2 FULL-TEXT 2022-06-07T15:28:59.589146Z 0 0 20200401 20200430 2020 2020-01-09T18:36:51.212091Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor highlightsabst orcid primabst ref 1746-8094 17468094 UNLIMITED FCT true 58 58 C Volume 58 15 101835 101835 101835 202004 April 2020 2020-04-01 2020-04-30 2020 Research Papers article fla © 2020 The Authors. Published by Elsevier Ltd. STUDYUSAGEFEASIBILITYCONTINUOUSWAVERADARFOREMOTIONRECOGNITION GOUVEIA C 1 Introduction 2 Experimental procedure 2.1 Continuous wave radar operation principal 2.2 Vital signs acquisition 2.3 Bio-Radar signal processing algorithm 2.4 Bio-Radar signal validation 3 Classification procedure 3.1 Features extractions 3.2 Classifiers 4 Results 5 Discussion 5.1 Binary problem 5.2 Multiclass problem 5.3 Results after feature selection 6 Conclusion Authors’ contribution Acknowledgments References FRIJDA 2009 264 271 N SCHERER 2005 695 729 K BARRET 2012 413 429 L FISCHER 2008 456 465 A KOELSTRA 2012 18 31 S RAJA 2016 1 6 M GAO 2018 1468 1470 Q KATSIGIANNIS 2018 98 107 S ZHAO 2016 95 108 M ANISHCHENCKO 2018 73 L FERNANDEZ 2018 244 249 J GOUVEIA 2018 30 38 C GOUVEIA 2017 C BIORADAR GOUVEIA 2019 604 C GROOT 2015 684 700 J FERREIRA 2018 347 355 J YUAN 2013 23 25 G CLARK 1990 V GAL 2018 O ELLIPSEFITUSINGLEASTSQUARESCRITERION ALPAYDIN 2004 E INTRODUCTIONMACHINELEARNING TAN 2018 P INTRODUCTIONDATAMINING MATHWORKS 2019 TREEBAGGER PANKSEPP 2004 J AFFECTIVENEUROSCIENCEFOUNDATIONSHUMANANIMALEMOTIONS GIULIANI 2008 714 N MITCHELL 1997 T MACHINELEARNING PETROCHILOS 2007 1 333 N ZHANG 2013 362 364 H GOUVEIAX2020X101835 GOUVEIAX2020X101835XC Full 2020-01-03T12:13:22Z FundingBody Portugal Institutes 2022-01-09T00:00:00.000Z 2022-01-09T00:00:00.000Z This is an open access article under the CC BY-NC-ND license. © 2020 The Authors. Published by Elsevier Ltd. 2022-06-06T15:54:20.023Z FEDER UID/EEA/50008/2019 ERDF European Regional Development Fund Fundação para a Ciência e Tecnologia FCT SFRH/BD/139847/2018 FCT Fundação para a Ciência e a Tecnologia MEC MEC Ministério da Educação e Ciência This work is funded by FCT/MEC through national funds and when applicable co-funded by FEDER – PT2020 partnership agreement under the project UID/EEA/50008/2019 and by National Portuguese Funds through FCT – Fundação para a Ciência e Tecnologia under the Ph.D. grant SFRH/BD/139847/2018. item S1746-8094(19)30416-1 S1746809419304161 10.1016/j.bspc.2019.101835 273545 2021-02-08T16:05:50.250618Z 2020-04-01 2020-04-30 UNLIMITED FCT true 2037998 MAIN 10 59662 849 656 IMAGE-WEB-PDF 1 gr1 47806 512 339 gr10 11819 278 339 gr2 22183 331 489 gr3 9353 55 489 gr4 39195 599 282 gr5 39143 512 565 gr6 35488 324 565 gr7 28016 325 565 gr8 28710 208 489 gr9 13083 268 339 gr1 13358 163 108 gr10 4503 164 200 gr2 6347 148 219 gr3 2666 25 219 gr4 3944 163 77 gr5 5051 164 181 gr6 6372 126 219 gr7 4814 126 219 gr8 6476 93 219 gr9 4711 164 207 gr1 627403 2264 1500 gr10 90023 1228 1500 gr2 215215 1465 2167 gr3 64349 243 2167 gr4 309995 2654 1250 gr5 307489 2267 2500 gr6 322085 1435 2500 gr7 216268 1438 2500 gr8 196131 920 2167 gr9 102137 1188 1500 si1 11028 si2 3643 si3 9564 si4 2107 si5 3355 si6 3616 am false 3444090 BSPC 101835 101835 S1746-8094(19)30416-1 10.1016/j.bspc.2019.101835 The Authors Fig. 1 Bio-Radar prototype: (a) hardware set-up composed by a SDR front-end and two array antennas, (b) signal acquisition using the LabVIEW software. Fig. 1 Fig. 2 Effect due to the phase correction procedure in real-time. ‘W’ signs a wrap occurrence and ‘S1’, ‘S2’ and ‘S3’ represent the signal segments after phase adjustment. Fig. 2 Fig. 3 Block diagram of the Bio-Radar signal pre-processing before the classification stage. Fig. 3 Fig. 4 Flow chart of the algorithm implemented to remove the DC component. Fig. 4 Fig. 5 Ellipse fitting method: (a) considering the arc formed in the complex plane, the ellipse that best fits to the arc is estimated along with its centre, (b) signal after phase demodulation with DC component, (c) the arc is re-centred by subtracting the centre coordinates from the signal, (d) signal after phase demodulation without DC component. Fig. 5 Fig. 6 Respiratory signal acquired during the Fear state: by the Bio-Radar system (top) and by the BIOPAC system (bottom). Peaks with red dots were the moments where subject got frightened. Segment ‘A’ contains an increase of the heartbeat rate and the remain segments (‘B’ and ‘C’) sign the body motion of the subject. (For interpretation of the references to color in this figure citation, the reader is referred to the web version of this article.) Fig. 6 Fig. 7 Zoom-in of the respiratory signal in the Fear state, during the first fright experience: by the Bio-Radar system (top) and by the BIOPAC system (bottom). Fig. 7 Fig. 8 Specification of the segmentation process for the different feature categories. Fig. 8 Fig. 9 Out-of-bag error curve according to the number of trees. Fig. 9 Fig. 10 Classification accuracy for different classifiers considering the different signals, Bio-Radar ‘bR’ and BIOPAC ‘bP’, respectively. Fig. 10 Table 1 Features specification: statistical and waveform features. Table 1 Statistical and waveform features FT1 Mean value FT2 Variance FT3 Waveform width FT4 Time between peaks FT5 Respiratory rate Table 2 Features specification: spectral features Table 2 Spectral features FT6 Power Spectral density in the range 0—0.1 Hz FT7 Power Spectral density in the range 0.1–0.2 Hz FT8 Power Spectral density in the range 0.2–0.3 Hz FT9 Power Spectral density in the range 0.3–0.4 Hz FT10 Power Spectral density in the range 0.4–0.9 Hz FT11 Power Spectral density in the range 0.9–1.5 Hz FT12 Ratio of the power spectral density values between low-frequency and high-frequency range Table 3 Accuracy rate results for the binary classification. Table 3 Happy vs. neutral Happy vs. fear Neutral vs. fear SVM 70.8% 68.8% 72.2% Bio-Radar 79.2% 73.5% 81.7% BIOPAC KNN 72.6% 72.4% 72.4% Bio-Radar 75.8% 71.0% 79.2% BIOPAC Random Forest 77.4% 73.8% 77.8% Bio-Radar 83.9% 73.3% 83.0% BIOPAC Table 4 Accuracy rate results for multiclass classification. Table 4 SVM KNN Random Forest Bio-Radar 58.1% 58.2% 65.2% BIOPAC 65.2% 61.4% 69.1% Table 5 Accuracy rate results for multiclass classification with a limited number of features. Table 5 SVM KNN Random Forest Bio-Radar 63.3% 66.2% 67.1% BIOPAC 63.3% 76.5% 75.1% Study on the usage feasibility of continuous-wave radar for emotion recognition Carolina Gouveia a b ⁎ Ana Tomé a c Filipa Barros d e Sandra C. Soares d e f José Vieira a b c Pedro Pinho b g a Departamento de Eletrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Eletrónica, Telecomunicações e Informática, Universidade de Aveiro Portugal Departamento de Eletrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal b Instituto de Telecomunicações, Aveiro, Portugal Instituto de Telecomunicações Aveiro Portugal Instituto de Telecomunicações, Aveiro, Portugal c Instituto de Engenharia Eletrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal Instituto de Engenharia Eletrónica e Telemática de Aveiro, Universidade de Aveiro Portugal Instituto de Engenharia Eletrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal d Center for Health Technology and Services Research (CINTESIS.UA), Department of Education and Psychology, University of Aveiro, Aveiro, Portugal Center for Health Technology and Services Research (CINTESIS.UA), Department of Education and Psychology, University of Aveiro Aveiro Portugal Center for Health Technology and Services Research (CINTESIS.UA), Department of Education and Psychology, University of Aveiro, Aveiro, Portugal e William James Center for Research (WJCR), Department of Education and Psychology, University of Aveiro, Aveiro, Portugal William James Center for Research (WJCR), Department of Education and Psychology, University of Aveiro Aveiro Portugal William James Center for Research (WJCR), Department of Education and Psychology, University of Aveiro, Aveiro, Portugal f Department of Clinical Neuroscience, Division of Psychology, Karolinska Institute, Stockholm, Sweden Department of Clinical Neuroscience, Division of Psychology, Karolinska Institute Stockholm Sweden Department of Clinical Neuroscience, Division of Psychology, Karolinska Institute, Stockholm, Sweden g Instituto Superior de Engenharia de Lisboa, Portugal Instituto Superior de Engenharia de Lisboa Portugal Instituto Superior de Engenharia de Lisboa, Portugal ⁎ Corresponding author at: Departamento de Eletrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal. Departamento de Eletrónica, Telecomunicações e Informática, Universidade de Aveiro Portugal Non-contact vital signs monitoring has a wide range of applications, such as in safe drive and in health care. In mental health care, the use of non-invasive signs holds a great potential, as it would likely enhance the patient's adherence to the use of objective measures to assess their emotional experiences, hence allowing for more individualized and efficient diagnoses and treatment. In order to evaluate the possibility of emotion recognition using a non-contact system for vital signs monitoring, we herein present a continuous wave radar based on the respiratory signal acquisition. An experimental set up was designed to acquire the respiratory signal while participants were watching videos that elicited different emotions (fear, happiness and a neutral condition). Signal was registered using a radar-based system and a standard certified equipment. The experiment was conducted to validate the system at two levels: the signal acquisition and the emotion recognition levels. Vital sign was analysed and the three emotions were identified using different classification algorithms. Furthermore, the classifier performance was compared, having in mind the signal acquired by both systems. Three different classification algorithms were used: the support-vector machine, K-nearest neighbour and the Random Forest. The achieved accuracy rates, for the three-emotion classification, were within 60% and 70%, which indicates that it is indeed possible to evaluate the emotional state of an individual using vital signs detected remotely. Keywords Continuous wave radar Emotion recognition Pattern recognition Support-vector machine K-nearest neighbour Random Forest 1 Introduction Emotions are adaptive and multidimensional responses triggered by meaningful events and/or stimuli, that affect the way we think, feel, behave and interact with others in our daily life [1,2]. These responses encompass changes in different systems including cognition, physiology, motivation, motor expression and subjective sensations, hence allowing for effective self-regulation processes and providing crucial resources to a successful adaptation to the environment [3,4]. The changes can then be expressed by different means, such as via verbal communication, facial expressions and body motion [5], as well as by more subtle cues, including biological signs (increased heart rate and respiration) [6]. Although automatic emotion recognition represents a challenging process, it can provide useful information regarding the individuals’ well-being and, generally speaking, contribute to systems’ enhancement, according to the user's needs. For example, it can be useful to adjust medical and psychological treatments or even aid in detecting the veracity of information in forensic investigations. Additionally, emotion identification can be a crucial asset in the Human–Computer Interaction (HCI) field, once it enables the adjustment of the design and its functional features to the users’ needs [5,7–9]. Emotion recognition through physiological signs has already been reported, namely by using breathing rate, blood flow, skin conductance, electroencephalogram (EEG), and electrocardiogram (ECG) [6], [8]. However, most of these physiological-based systems still rely on the direct contact with sensors and it is recognized that the individual's awareness of the monitoring process may influence the results [6]. Moreover, the use in real life contexts, which is of great value to allow for the establishment of individualized profiles, can also interfere with the signal. Therefore, non-contact systems can be advantageous in these applications, as they can remotely monitor vital signs. Recently, emotion recognition algorithms using vital signs acquired by radar systems, was done in [7,9], with a continuous-wave (CW) radar and a frequency-modulated continuous radar (FMCW) radar, respectively. In [9], the heartbeat signal was tracked and used to identify positive and negative emotions (joy, pleasure, sadness and anger), that resulted from memories triggered by music or photos. The support-vector machine (SVM) classification algorithm was implemented, and 72.3% of accuracy rate was achieved, considering the dataset of 12 volunteers. Additionally, [7] presented an application using the K-nearest neighbour (KNN) as classifier, using the respiratory signal detected by a heterodyne CW radar, with an accuracy rate of 67.4%. In this case, the classifier was trained also to identify four emotions, now including an emotionally neutral condition (joy, sadness, fear and neutral). A dataset from 5 subjects was used, with waveform features (such as mean or standard deviation) and spectral features obtained through the power spectral density computation in several frequency intervals, from 0 to 0.4 Hz. Similar studies using CW radars were carried out in [10,11], but focused on the mental stress response. In [10], the SVM classifier was used to identify if a subject was under mental stress, physical stress or in steady state. The used dataset encompassed waveform and spectral features, extracted from the breathing signal, similarly to [7]. The stress state was identified with a binary strategy, with an accuracy of 77.5% and 77.9%, for the steady state/mental stress case and the steady state/physical stress case, respectively. The binary strategy was also tested for the steady/stress state in [11], but using neural networks (NN). In this work, features were extracted from the respiratory signal. However, authors had to limit the amount of data, which led to a limited number of features used on the input of the NN (a single feature and combinations of two features were used). In this work, a radar system based on a software defined radio (SDR) technology was employed to acquire the respiratory signals resulting from the induction of three different emotions: fear, happiness and an emotionally neutral state to serve as a control condition. Along with the respiratory signal, we also aimed at capturing other types of body motion that occur naturally due to the individual's reaction to specific emotions. These body movements, that have been seen as a source of distortion in similar works, hold important information regarding human responsiveness to specific emotions. As in previous works, in which the authors induced emotions [6,8], in the present study we used film clips to elicit specific emotions. Each participant was exposed to a set of videos inducing the three specific emotions, which were presented in three different sessions, separated by at least one week, in order to isolate the specific emotions. Volunteers did not have prior knowledge of video's content that were going to watch in that day. For the emotion recognition using a set of features extracted from these signals, three different classification algorithms were used, in order to evaluate which provided results with the best accuracy: SVM, KNN and the Random Forest. Furthermore, a feature selection was performed to increase the accuracy. The respiratory signals of the volunteers were acquired using simultaneously, the CW radar system, from now on called Bio-Radar, and a certified measurement equipment (BIOPAC), with a chest-band transducer. By comparing the performance of classifiers with signals acquired using both systems, we aimed at investigating if signals remotely measured can be used for emotion identification. This was possible to prove indeed, since the classification results concerning Bio-Radar and BIOPAC signals were in the same order of magnitude. The bio-radar system was calibrated using a chest-wall simulator developed for this purpose and already validated using the same certified measurement equipment, in [12]. The best result, regarding the Bio-Radar signal, was achieved for the Random Forest classifier, being 65.2% and 67.1%, before and after the feature selection. This work is divided as follows: in Section 2, the Bio-Radar operation mode is explained and the experimental procedure is described. The signal processing algorithm performed before the classification stage is explained in detail. This section ends with the validation of the Bio-Radar signal acquisition, where some useful signal characteristics are identified. Thereafter, the classifier implementation is explained in Section 3, which starts with the features’ extraction procedure and explains the classifier implementation. The cross-validation methods are presented in Section 4 along with the results. Results are later discussed in Section 5, through a performance comparison between different classifiers. We intend to focus specially in the classifier performance considering both measuring systems and inspect if their accuracy differs or if it is equivalent. Finally, conclusions are presented in Section 6. 2 Experimental procedure 2.1 Continuous wave radar operation principal Doppler radars for vital signs measurement, also defined as the Bio-Radar systems, consist in a technology capable to acquire the respiratory and the cardiac signals, without interfering directly with the individual. For this purpose, it uses electromagnetic waves, which are transmitted towards the chest-wall of the subject under monitoring, and the reflected echo is received. From the Doppler effect, it is possible to relate the received signal properties with the distance change between the radar antennas and the subject's chest-wall, which moves according to the cardiopulmonary function. The CW radar continuously transmits a sinusoidal carrier, generated digitally, and receives the echo from the reflecting target. Due to the Doppler effect, there is a phase change as the subject's chest-wall moves towards or away from the radar and, hence, a phase modulation in the received signal is created [13]. The mathematical model of the Bio-Radar channel response and its MATLAB simulation are detailed in [14]. The Bio-Radar implemented in this work is depicted in Fig. 1 and it was developed in [13]. It consists in a real-time measuring system implemented with the LabVIEW software. Signals were acquired using two antennas, one for transmission and the other for reception, and a front-end based in a SDR system. These radars allow the digital configuration of its input and output (receiver and transmitter), regarding the required frequency and sampling rate of the current application. In the framework of this application, the SDR used was an USRP B210. It operates in a limited range of carriers (70 MHz to 6 GHz) and, therefore, we selected 5.8 GHz as the carrier frequency. 2.2 Vital signs acquisition The experiment was conducted with nine volunteers, in three different days, spaced by at least two days (hence, a within-subject's experimental design). The experiment consisted in measuring the respiratory signal of each subject and other motion characteristics related to his emotional state, while they were watching different sets of short videos that elicited different emotions (fear, happiness and neutral emotion), similarly to the works presented in [15,16]. For example, happiness was induced by comedy videos, the neutral state was induced by documentary videos and, finally, the fear condition was induced by scary videos. The videos used in this experiment were previously selected in pilot studies, where for each video it was verified how intense was the triggered emotion. On each testing session one emotion was induced by using these different types of videos. During the experiment, videos were shown according to an increasing order of emotion intensification, in order to allow for a continuous induction and an effective engagement of the subject throughout the session. Participants did not have prior knowledge about the type of videos that they were about to watch, to avoid for potential confounds. Each testing session took around 30 minutes. Since the main goal of this work is to validate the usage of a Bio-Radar system in emotion recognition, respiratory signal was acquired using Bio-Radar and a certified measuring equipment, the BIOPAC MP160 Data Acquisition System with Acknowledgment 5 Software (from BIOPAC Systems, Inc.). The BIOPAC system is connected to an acquisition board, which have several modules for different types of signal acquisition, such as ECG, breathing and blood pressure. The module RSP100C is focused in the processing of data acquired from a respiration transducer chest band, placed around the chest cavity of the subject at test. This transducer measures the respiratory effort by analysing the instantaneous thoracic perimeter [12]. The Bio-Radar signal is a lowpass signal with a bandwidth equal to 0.2–2 Hz, which comprises the respiratory bandwidth for a healthy adult (0.2–0.4 Hz) [17], the hyperpnea cases (0.4–0.8 Hz) and the cardiac signature along with other random motions from the body (0.8–2 Hz) [18]. The received signal had to be visually inspected during the real-time acquisition, once it was required to do a phase adjustment in real time, for several reasons. First, the local oscillator in the USRP board stabilizes in a different phase value every time the system restarts, which changes the mean value of the phase and hence the arc position in the complex plane. The distance between the subject and the radar also determines the arc position. Furthermore, the subject is not totally still during the monitoring period and cannot keep the same amplitude of chest-wall motion (in fact, changes in amplitude as the result of different emotions, is exactly what we want to capture), hence we expect that the mean value of the phase changes during the experiment. If the nominal phase is near π or −π in the beginning of the experiment, it can reach easily those values and wraps will occur. In those cases, the phase should be corrected by changing the phase of the transmitted signal, digitally and in real time. The effect of this phase correction procedure is presented in Fig. 2 . This figure has a wrap marked as ‘W’ and three different segments (‘S1’, ‘S2’ and ‘S3’), which result from the phase adjustment in that time, to avoid new wrap occurrences. Regarding the acquisition details, the Bio-Radar signal is received with a sampling rate equal to f s1 = 100 kHz and is downsampled in real-time to f s2 = 1 kHz. This is performed in LabVIEW and it was necessary to ease the visualization in real-time and to decrease the amount of data to be stored, which can be processed in MATLAB with low computational effort. The BIOPAC signal is acquired with a sampling rate equal to f s2. 2.3 Bio-Radar signal processing algorithm Considering the phase adjustment that was necessary to perform during the signal acquisition, the signal processing algorithm implemented in the Bio-Radar signal should be performed with care. In this sense, the algorithm executes the following steps (see block diagram from Fig. 3 ). Signals are acquired using a SDR, which has an In-phase and Quadrature (IQ) demodulator and delivers on its output the complex baseband signal, g(n), with a sampling frequency equal to f s2. Thus, the first step is to do a second stage of downsampling, once the signals that we are dealing are lowpass signals. Complex signal g d (n) has a new sampling rate equal to f s3 = 100 Hz. Also, the BIOPAC signal is downsampled to this new sample rate. The next step is to remove the DC component of the complex signal. In order to do this, an algorithm was developed, having in mind that the signals extraction results from the arctangent computation. In other words, it is necessary to guarantee that the arc in the complex plane does not cross the π value. Furthermore, the phase transitions due to the phase correction should also be removed. Fig. 4 shows an algorithm to perform the DC removal, that avoids the wrap occurrence and removes phase transitions. The algorithm started to inspect if there are phase transitions due to our adjustments. In those cases, the signal is divided in sub-segments (the correspondent sub-segment prior the transition and after the transition) and the phases are aligned. Then, with a more uniformed signal (regarding the mean phase value), the ellipse fitting method [19] is applied to remove the DC component of the signal. Fig. 5 depicts the ellipse fitting method and its importance in the extracted signal. First, the ellipse that best fits to the arc is determined and its centre is computed. Then, the centre coordinates are subtracted from the signal in order to re-centre it back to the origin. Fig. 5(b) shows that if the received signal has high DC component, the phase demodulated result is distorted and attenuated. On the other hand, the signal can recover its normal form after the DC removal. In this stage, it is required to inspect if the arc is located around the π value. In this case, the arc should be rotated to avoid the wrap occurrence after the arctangent computation. Even with the arc located far from the π value, mean value fluctuations due to the subject random motion can imply that the arc crosses the origin of the complex plane. In these cases, a small offset is added just to avoid wraps, but it should be small enough the maintain the signal information unchanged. After the DC removal, the resulting complex signal is p(n). The respiratory signal is recovered from p(n), by performing phase demodulation with the arctangent computation, resulting in the real signal b(n). Later, the respiratory signal b(n) is divided in sub-segments with one minute duration for the feature extraction for classification. 2.4 Bio-Radar signal validation In the introductory section, we referred to the numerous advantages of the vital signs acquisition using non-contact methods, considering the emotion recognition framework. Nonetheless, one needs to verify that the same results are achieved by both conventional sensors and contactless means, and that this can also contribute for the classifiers performance enhancement. For this purpose, vital signs were measured using two measurement set-ups simultaneously: the Bio-Radar prototype for the remote signals acquisition and the BIOPAC system, as the conventional measurement equipment. Fig. 6 shows two signal samples obtained during the Fear state test, acquired simultaneously with these two systems. In this sample, it is possible to observe that the subject was frightened twice and these events are represented by two peaks with higher amplitudes, when compared with the rest of the signal (marked with a red point in the figure). Immediately after each moment of fright, it is possible to identify an increase in the heartbeat rate in the Bio-Radar signal, during the exhale periods. This event is signaled in green with the capital letter ’A’ and is not captured in the BIOPAC signal. Fig. 7 shows a zoom-in of the first moment of fright, considering the signal presented in Fig. 6. In this figure, the increase of heartbeat rate is more clear in the Bio-Radar signal rather in the BIOPAC. The radar system can also detect the random motion of the subject, that occurs naturally due to the sudden reactions of videos (such as laugh or fright), or for comforting purposes. In fact, this random motion along with other signal signatures that arise from the emotional induction, serve as tools for the proper emotion recognition. For example, the subject's discomfort can be observed in the Bio-Radar signal from Fig. 6. If he/she moves slightly his/her position, a mean value change will occur, as showed by the signal slots marked as ‘B’ and ‘C’. On the other hand, this type of event is not evident in the BIOPAC signal. 3 Classification procedure 3.1 Features extractions Features were extracted from one minute observations. For this purpose, the signal was divided in one minute segments, performed by Segmentation block, as depicted in Fig. 8 . The “raw signal” at the block input is the result of phase demodulation. Inside the block, this signal passes through different data processing, accordingly with the features’ categories that are going to be used. During the monitoring period, changes to the respiration pattern are expected according to the natural reaction of the body due to emotions, and other body motions. Thus, statistical and waveform characteristics can be identified. In addition, the spectral evaluation is also carried out, not only for the respiratory bandwidth 0.2–0.4 Hz, but also for higher frequencies that can reveal heartbeat detection or sudden motions from the subject. For spectral features, the input data is only divided in 1 min sub-segments, without any prior processing. On the other hand, for statistical and waveform features, a filter was applied in the full signal before the sub-segmentation process. The filter used was a 2nd order Butterworth with a bandpass equal to 0.05–1.5 Hz. The usage of this filter is important to centre the respiratory signal in zero, and to avoid biased results on the mean value due to different phases, as showed in Fig. 2. A total of 12 features were extracted from the segmented signals and they are presented in Tables 1 and 2 . Statistical features encompasses: • FT1 – the mean value of the segment; • FT2 – the variance considering the average of all segment samples. Waveform features includes: • FT3 – the waveform width, which is the mean value of all peak widths in that segment. The width is defined distance between the points where signal intercepts a reference line. In this case, the reference line is located beneath the peak at half of the peak prominence. • FT4 – the time between peaks, where the number of samples between two peaks is counted and its mean value is computed • FT5 – the respiratory rate of the full segment, which can be obtained by through the estimation of the maximum component in power spectral density. Finally, spectral features FT6-FT11 are the amplitudes of spectral components in the different frequency bands. This is also obtained through the power spectral density, using a sliding Hamming window with 50% of overlap. The last feature FT12 relates the spectral components within the low band (0.1–0.5 Hz) and the high band (0.6–1.5 Hz). It is expected that the low band expresses only the respiratory presence and the high band expresses a restless behaviour. Since features have different natures and their range of values are different from each other, a normalization is required. In this case, the z-score normalization was applied to all features, as described by (1) x ˜ i = x i − m i σ i where x ˜ i is the normalized feature, x i is the feature to be normalized, m i is its mean value and σ i is its standard deviation. 3.2 Classifiers In this work, three different classifiers were implemented and their performance was compared. The same classification procedure was applied for both the BIOPAC and the Bio-Radar signals, in order to validate the usage of the developed prototype for this scope. The classification was performed defining different problems: binary and multiclass, both using a dataset with 221 observations for each emotion. For the binary problem, only pairs of emotions were considered, therefore the dataset length had 442 observations. On the other hand, the multiclass case considered three elicited emotions and a final dataset length equal to 663 observations. Notice that these observations were chosen after visual inspection of the signal, when noisy segments are discarded. This resulted in an imbalanced dataset, since it was not possible to have the same number of observations for each person, at each emotion. Thus, the number of observations was limited to the minimum duration time of all experiments, in order to balance the dataset. Three classification algorithms were chosen – SVM, KNN and Random Forest, considering their good performance with short datasets [20]. Moreover, SVM and KNN were also used in literature (in [9,7], respectively) and since the goal of this work is to reinforce the possibility to use radar systems in emotion recognition, that same algorithms as the ones implemented in literature should be used. In addition, Random Forest is presented as an alternate option. These algorithms were implemented for both binary and multiclass classification. The accuracy results are shown in Tables 3 and 4 , respectively, and are discussed in Section 5. Their implementation considered the following functional principles: • SVM was applied with a Radial Basis Function (RBF) kernel [20], in both binary and multiclass problems. – Regarding the multiclass case, since SVM algorithms are based in binary classification only, a decision strategy based in voting was applied in order to reduce the multiclass problem in a set of binary problems. In this sense, one-versus-one (OVO) strategy selects the class through the major vote, within C(C − 1)/2 binary classifiers (SVMs) that can be to deal with the C classes. • The KNN algorithm uses the Euclidean distance as measure of proximity and the closest neighbour (with k = 1) for the decision rule [21]. The same classifier characteristics were used for both binary and multiclass problems. • Finally, the Random Forest algorithm consists in an ensemble of decision trees trained in parallel to give a contribute to the final decision. This ensemble classifier was implemented with the Bagging mode, i.e., each tree was trained with a bootstrap sample of the dataset, without normalization [20,22]. In order to prevent the deep grow of trees, a minimum leaf size was assigned to five observations. The out-of-bag score was then used to choose the number of trees (see Fig. 9 ) and 70 trees were selected. – The feature impact can also be studied from the Random Forest method. For this purpose, the relevance of the feature selection on each node was analysed and the six more relevant features were chosen to create a new dataset. The classifiers performance, with the limited number of features, is presented in Table 5 . 4 Results SVM and KNN were trained and tested using the Leave-one-out cross-validation strategy. Note that the dataset is small. Leave-One-Out is an extreme case of K-fold cross-validation. Considering that the dataset has N observations, this strategy uses N − 1 observations for training and the left out one instance for testing [20]. With this method we have N combinations of training sets with different single instances for testing. The final result shows how many times the prediction was right on identifying the correct label. Therefore, the binomial distribution can be used to model the accuracy of the classifier. In the case of Random Forest, the out-of-bag observations (samples not included in the bootstrap sample) can be used to estimate the performance of the classifier [22]. Considering the procedure described in the previous section, the achieved results for each classifier are presented in the tables bellow. First, Table 3 presents the results regarding the binary classification problem. Then, Table 4 presents the results achieved for multiclass problem. Finally, Random Forest classifier allowed the execution of a preliminary study regarding the feature importance. This study revealed which features had more influence on the achieved results and this information can be used to presumably increase the accuracy results, by decreasing the number of features used in dataset. Table 5 shows the obtained results after implementing the classification algorithms with a smaller number of features. 5 Discussion In this section, the results achieved for both binary and multiclass problems are discussed. 5.1 Binary problem In this case, by analysing the obtained accuracies, it is possible to verify that results from both Bio-Radar and BIOPAC signals were similar. Generally, the Happy vs. fear case is the one that presents lower accuracy, with a difference of less than 10%, if we compare with the other binary cases. This can be explained by the similar responses for these two emotions, leading to an approximated feature result, which hinders the class identification. The algorithm with better performance was the Random Forest and the worst accuracy results were obtained with the SVM classifier, which also presented the biggest deviation between Bio-Radar and BIOPAC signals, with a maximum drift of 10%, approximately. Aside from these results and in order to properly validate the Bio-Radar system usage, the McNemar test was performed [20], with the mid-p-value test. This test helps to understand which type of signal could be used to identify more true labels accurately, or if the classifier accuracy is similar using both signals. It verifies if the null hypothesis is accepted or rejected. In this case, the null hypothesis stands that the classifier with the different signals have the same accuracy of predicting correctly the provided class labels. The McNemar test was performed for every classification algorithm (SVM, KNN and Random Forest) and for every binary case. To apply the test, the predicted values of the test set of the leave-one-out strategy loop, are stored for all classifiers. Then, the null hypothesis is tested, e.g., if the predicted labels of two classifiers have the equal accuracy for predicting the true labels. The test was applied with 5% significance level to compare the results of classifiers, with input the BIOPAC signals and radar signals, respectively. Since both positive and negative emotions serve motivational purposes (reward or withdrawal, respectively), similar autonomic nervous system responses favour environmental adaptation purposes, as to prepare the organism for appropriate behavioural responses [23]. Although this remains a controversial topic in the literature, several studies have corroborated this view, by showing an increased physiological arousal (e.g., increased heart rate) not only in negative emotional states (which trigger “fight or flight” responses) but also in positive emotional states [24]. The current results support such view for respiratory signals captured using the Bio-Radar. 5.2 Multiclass problem The results for multiclass classification (presented in Table 4), were again similar within the different classifiers and the different acquisition systems. With a detailed analysis it is possible to observe that the results for the KNN and Random Forest classifier were approximately the same for Bio-Radar and BIOPAC signals, with only 3.2% and 3.9% of difference, respectively. The difference was slightly more evident for the SVM, with 7.1% of difference. For the multiclass case, the best classification algorithm was the Random Forest, with 65.2% of accuracy for the Bio-Radar signal and 69.1% for the BIOPAC signal. Consistently with the previous results, the BIOPAC signal resulted in better accuracy results than the Bio-Radar. However, the McNemar test performed for every classification algorithm, did not rejected the null hypothesis at the 5% significance level, which sustain that differences between accuracy results with Bio-Radar and BIOPAC signals are not relevant. An alternate way to evaluate the results is by computing the confidence interval of the accuracy results. For this purpose, the normal approximation of binomial confidence interval was computed through by: (2) p ˆ ± z p ˆ ( 1 − p ˆ ) N where p ˆ is the probability of correct decision, N is the total number of observations and z is the equivalent 1 − α 2 quantile of the standard deviation. In this case, α = 0.05 was considered, which results in z = 1.96. In fact, p ˆ is also an estimate of accuracy, it can be obtain through the relation N s N , where N s is the number of correct decisions [25]. Fig. 10 shows the performance of each classifier using the Bio-Radar and BIOPAC signals, and their distribution through the computed intervals. Results are disposed in consecutive pairs of ‘name/signal’, with the respective classifier name and signal type, where ‘bR’ stands for Bio-Radar and ‘bP’ stands for BIOPAC. By analysing this graph it is possible to observe that results under the same classifier deviate slightly between the different signals, by keeping an interval portion in common. 5.3 Results after feature selection As mentioned previously, the Random Forest method also allows to check the relevance of the features in the decision. Table 5 shows the obtained results, for each classifier implemented, with the most relevant features. In the Bio-Radar case the selected features were FT6, FT7, FT12, FT4, FT9 and FT2 (disposed in decreasing order of importance). In the BIOPAC case, the selected features were FT9, FT7, FT2, FT3, FT10 and FT8. There were some common features used by both systems, such as the variance (FT2) and the power spectral density in the range of 0.1–0.2 Hz, (FT7) and in the range of 0.3–0.4 Hz (FT9). These features can depict modifications on the breathing signal characteristics, i.e., the change of the waveform over time and the dynamic range variation of the breathing rate of a healthy person, which can occur due to random motions or to sudden change of emotional state (caused by laughs or frights). It should be also highlighted the features that were selected differently in the different signal cases. For example, FT4, that measures the time between peaks and FT12, that represents the ratio between low frequency components and high frequency components, were only selected on the Bio-Radar based system. These features can encompass either the heartbeat (mostly detected in apnea periods), and random body motions that are not easily detected by the BIOPAC, as seen previously. With the limited number of features, the results increased in general (beside SVM for the BIOPAC case). This determines the importance of the selected features and proves their relation with the current emotional state of the subject. 6 Conclusion This work aimed to validate the usage of the Bio-Radar system for the emotion recognition through the remote detection of vital signs, in this case the respiratory signal. Three different algorithms for emotions identification were applied and compared. Emotional responses were extracted from vital signs acquired simultaneously by a non-contact system based on a CW Doppler radar and a certified measuring equipment (BIOPAC herein used as a conventional method). By analysing both signals visually, it is possible to observe that the Bio-Radar system was more successful on identifying the actual emotional state of the individual, than the BIOPAC system, since it can detect more subtle characteristics. The executed classifier algorithms were the SVM, KNN and the Random Forest, which were applied to both signals. Regarding binary classification, it was possible to observe a difficulty to distinguish between the Happy and the Fear emotional states, when compared with the other binary cases. In binary and multiclass classification problems, the BIOPAC signal presented better results. Importantly, and despite the marginal differences, both Bio-Radar and BIOPAC signals presented similar performances, between 65% and 80%. Within a 5% of significance level, the same classifier using these different signals have the same accuracy, according to McNemar test result. Generally, the obtained accuracy results increased when the number of features was limited to the six more important ones, after the feature importance evaluation. In sum, the current study contributes to show that the Bio-Radar can indeed be used in the emotion recognition, with a similar performance to those conventional methods used for vital signal detection. We believe that it is possible to enhance the achieved results, by improving the set-up in two levels. Firstly, regarding the monitoring scenario, the presence of metallic surfaces should be minimized, once they corrupt the reception of the electromagnetic signals. Secondly, in terms of signal processing, the development of strategies to remove the DC component in real-time and also to track the optimal detection point (i.e., the centre of the subject's chest-wall), during the experiment, can contribute for the accuracy of signals’ reception. Moreover, it is known that the cardiac signal could also be separated from the respiratory signal [26,27]. Since their spectral components are closed to each other and the amplitude of respiratory signal is around 10 times bigger than the cardiac signal, their separation is a challenging task. The cardiac signal has useful information that could help in the emotional state identification, therefore it should be explored methods for an efficient signal decomposition. These results represent an important step towards the use of emotion recognition systems with non-invasive equipment and reinforces the notion that these systems represent a promising tool in several contexts, with mental health care representing a prime example, given the pivotal role of emotion dysregulation in psychiatric disorders. This type of system may act to provide individualized profiles and, therefore, more tailored-based interventions, hence reducing the burden of such disorders. Authors’ contribution Carolina Gouveia: conceptualization; software; investigation; formal analysis; data curation; writing – original draft. Ana Tomé: conceptualization; methodology; validation; writing - review & editing; supervision. Filipa Barros: conceptualization; methodology; investigation; resources; writing – review & editing. Sandra C. Soares: conceptualization; methodology; resources; validation; writing – review & editing; supervision. José Vieira: methodology; validation; writing – review & editing; supervision. Pedro Pinho: validation; writing – review & editing; supervision. Acknowledgments This work is funded by FCT/MEC through national funds and when applicable co-funded by FEDER – PT2020 partnership agreement under the project UID/EEA/50008/2019 and by National Portuguese Funds through FCT – Fundação para a Ciência e Tecnologia under the Ph.D. grant SFRH/BD/139847/2018. Conflict of interest: None declared. References [1] N.H. Frijda Emotion experience and its varieties Emot. Rev. 1 3 2009 264 271 Frijda N.H., Emotion experience and its varieties, Emot. Rev., 2009, 1, 3, 264–271. [2] K.R. Scherer What are emotions? and how can they be measured? Soc. Sci. Inform. 44 4 2005 695 729 Scherer K.R., What are emotions? and how can they be measured?, Soc. Sci. Inform., 2005, 44, 4, 695–729. [3] L.F. Barret Emotions are real Emotion (Washington, DC) 12 2012 413 429 10.1037/a0027555 Barret L.F., Emotions are real, Emotion (Washington, DC), 2012, 12, 413–429, 10.1037/a0027555. [4] A.H. Fischer A.S.R. Manstead Social functions of emotion Handb. Emot. 3 2008 456 465 Fischer A.H., Manstead A.S.R., Social functions of emotion, Handb. Emot., 2008, 3, 456–465. [5] S. Koelstra Deap: a database for emotion analysis using physiological signals IEEE Trans. Affect. Comput. 3 1 2012 18 31 Koelstra S., et al., Deap: a database for emotion analysis using physiological signals, IEEE Trans. Affect. Comput., 2012, 3, 1, 18–31. [6] M. Raja S. Sigg Applicability of RF-based methods for emotion recognition: a survey 2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops) 2016 1 6 10.1109/PERCOMW.2016.7457119 Raja M., Sigg S., Applicability of RF-based methods for emotion recognition: a survey, 2016, 2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops), 1–6, 10.1109/PERCOMW.2016.7457119. [7] Q. Gao J. Yan H. Zhao C. Ding H. Hong X. Zhu Non-contact emotion recognition via CW Doppler radar IEEE Asia-Pacific Microwave Conference (APMC) 2018 1468 1470 Gao Q., Yan J., Zhao H., Ding C., Hong H., Zhu X., Non-contact emotion recognition via CW Doppler radar, 2018, IEEE Asia-Pacific Microwave Conference (APMC), 1468–1470. [8] S. Katsigiannis N. Ramzan Dreamer: a database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices IEEE J. Biomed. Health Informatics 22 1 2018 98 107 Katsigiannis S., Ramzan N., Dreamer: a database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices, IEEE J. Biomed. Health Informatics, 2018, 22, 1, 98–107. [9] M. Zhao F. Adib D. Katabi Emotion recognition using wireless signals Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking 3 1 2016 95 108 Zhao M., Adib F., Katabi D., Emotion recognition using wireless signals, Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking, 2016, 3, 1, 95–108. [10] L. Anishchencko Challenges and potential solutions of psychophysiological state monitoring with bioradar technology Diagnostics 8 4 2018 73 Anishchencko L., Challenges and potential solutions of psychophysiological state monitoring with bioradar technology, Diagnostics, 2018, 8, 4, 73. [11] J. Fernandez L. Anishchencko Mental stress detection using bioradar respiratory signals Biomed. Signal Process. Control 43 2018 244 249 Fernandez J., Anishchencko L., Mental stress detection using bioradar respiratory signals, Biomed. Signal Process. Control, 2018, 43, 244–249. [12] C. Gouveia D. Malafaia J.N. Vieira P. Pinho Bio-radar performance evaluation for different antenna designs URSI Radio Sci. Bull. 2018 364 2018 30 38 Gouveia C., Malafaia D., Vieira J.N., Pinho P., Bio-radar performance evaluation for different antenna designs, URSI Radio Sci. Bull., 2018, 2018, 364, 30–38. [13] C. Gouveia Bio-Radar 2017 12 Universidade de Aveiro Aveiro, Portugal (Master's thesis) Gouveia C., Bio-Radar, 2017 12, Universidade de Aveiro, Aveiro, Portugal, (Master's thesis). [14] C. Gouveia J.N. Vieira P. Pinho A review on methods for random motion detection and compensation in bio-radar systems Sensors 19 3 2019 604 Gouveia C., Vieira J.N., Pinho P., A review on methods for random motion detection and compensation in bio-radar systems, Sensors, 2019, 19, 3, 604. [15] J. Groot A sniff of happiness Psychol. Sci. 26 6 2015 684 700 Groot J., et al., A sniff of happiness, Psychol. Sci., 2015, 26, 6, 684–700. [16] J. Ferreira V. Parma L. Alho C.F. Silva S. Soares Emotional body odors as context: effects on cardiac and subjective responses Chem. Senses 43 5 2018 347 355 Ferreira J., Parma V., Alho L., Silva C.F., Soares S., Emotional body odors as context: effects on cardiac and subjective responses, Chem. Senses, 2018, 43, 5, 347–355. [17] G. Yuan N.A. Drost R.A. McIvor Respiratory rate and breathing pattern McMaster Univ. Med. J. 10 1 2013 23 25 Yuan G., Drost N.A., McIvor R.A., Respiratory rate and breathing pattern, McMaster Univ. Med. J., 2013, 10, 1, 23–25. [18] V.L. Clark J.A. Kruse Clinical methods: The history, physical, and laboratory examinations JAMA 264 21 1990 V.L. Clark, J.A. Kruse, 1990, Clinical methods: The history, physical, and laboratory examinations, JAMA, 264, 21. [19] O. Gal Ellipse Fit Using Least Squares Criterion 2018 (Accessed) Gal O., Ellipse Fit Using Least Squares Criterion, 2018, (Accessed). [20] E. Alpaydin Introduction to Machine Learning 2004 The MIT Press, Massachusetts Institute of Technology Cambridge, MA Alpaydin E., Introduction to Machine Learning, 2004, The MIT Press, Massachusetts Institute of Technology, Cambridge, MA. [21] P.-N. Tan M. Steinbach A. Karpatne V. Kumar Introduction to Data Mining 2nd ed. 2018 Pearson Tan P.-N., Steinbach M., Karpatne A., Kumar V., Introduction to Data Mining, 2018, 2nd ed., Pearson. [22] Mathworks Treebagger 2019 (Accessed) Mathworks, Treebagger, 2019, (Accessed). [23] J. Panksepp Affective Neuroscience: The Foundations of Human and Animal Emotions 2004 New York Oxford University Press J. Panksepp, 2004, Affective Neuroscience: The Foundations of Human and Animal Emotions, New York Oxford University Press. [24] N.R. Giuliani K. McRae J.J. Gross The up-and down-regulation of amusement: experiential, behavioral, and autonomic consequences Emotion 8 5 2008 714 Giuliani N.R., McRae K., Gross J.J., The up-and down-regulation of amusement: experiential, behavioral, and autonomic consequences, Emotion, 2008, 8, 5, 714. [25] T.M. Mitchell Machine Learning 1997 McGraw Hill Mitchell T.M., Machine Learning, 1997, McGraw Hill. [26] N. Petrochilos M. Rezk A. Høst-Madsen V. Lubecke O. Boric-Lubecke Blind separation of human heartbeats and breathing by the use of a doppler radar remote sensing IEEE International Conference on Acoustics, Speech and Signal Processing 1 2007 1 333 Petrochilos N., Rezk M., Hst-Madsen A., Lubecke V., Boric-Lubecke O., Blind separation of human heartbeats and breathing by the use of a doppler radar remote sensing, IEEE International Conference on Acoustics, Speech and Signal Processing, 2007, 1, 1–333. [27] H. Zhang The separation of the heartbeat and respiratory signal of a Doppler radar based on the LMS adaptive harmonic cancellation algorithm 6th International Symposium on Computational Intelligence and Design 1 2013 362 364 Zhang H., et al., The separation of the heartbeat and respiratory signal of a Doppler radar based on the LMS adaptive harmonic cancellation algorithm, 6th International Symposium on Computational Intelligence and Design, 2013, 1, 362–364. "
    },
    {
        "doc_title": "A recognition–verification system for noisy faces based on an empirical mode decomposition with Green’s functions",
        "doc_scopus_id": "85068166484",
        "doc_doi": "10.1007/s00500-019-04150-9",
        "doc_eid": "2-s2.0-85068166484",
        "doc_date": "2020-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Geometry and Topology",
                "area_abbreviation": "MATH",
                "area_code": "2608"
            }
        ],
        "doc_keywords": [
            "Bi-dimensional empirical mode decompositions",
            "Classification results",
            "Empirical Mode Decomposition",
            "Environmental conditions",
            "Face Verification",
            "K-nearest neighbor classifier",
            "Parameter adjustments",
            "S function"
        ],
        "doc_abstract": "© 2019, Springer-Verlag GmbH Germany, part of Springer Nature.Face recognition or verification remains a real challenge in the area of pattern recognition and image processing. The image acquisition process is a crucial step in which noise will inevitably be introduced, and in most cases this noise drastically decreases the accuracy of the classification rate of recognition systems, making them ineffective. This paper presents a novel approach to face recognition or verification, which increases the recognition rate in noisy environmental conditions. The latter is achieved by using the intrinsic face mode functions that result from applying a bi-dimensional empirical mode decomposition with Green’s functions in tension to noisy images. Each image is individually decomposed, and noisy modes are discarded or filtered during reconstruction. Then, the extracted modes are used for classification purposes with canonical classifiers such as vector support machines or k-nearest neighbor classifiers. Experimental results show that this method achieves very stable results, almost independently of the amount of noise added to the image, due to the ability of decomposition to capture the noise in the first mode. Classification results using noisy images are at the same level as other algorithms proposed for the same databases but working on clean images and therefore are better than those obtained using classic image filters in noisy images. Moreover, unlike most of the available algorithms, the algorithm proposed in this paper is based on the input data (without the need to adjust parameters), making it transparent to the user. Finally, the proposed new approach achieves good results independently of the type of noise, the level of noise and the type of the database, which is not possible with other classical methods requiring parameter adjustment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A constrained ICA-EMD model for group level fMRI analysis",
        "doc_scopus_id": "85084394244",
        "doc_doi": "10.3389/fnins.2020.00221",
        "doc_eid": "2-s2.0-85084394244",
        "doc_date": "2020-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Neuroscience (all)",
                "area_abbreviation": "NEUR",
                "area_code": "2800"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 Wein, Tomé, Goldhacker, Greenlee and Lang.Independent component analysis (ICA), being a data-driven method, has been shown to be a powerful tool for functional magnetic resonance imaging (fMRI) data analysis. One drawback of this multivariate approach is that it is not, in general, compatible with the analysis of group data. Various techniques have been proposed to overcome this limitation of ICA. In this paper, a novel ICA-based workflow for extracting resting-state networks from fMRI group studies is proposed. An empirical mode decomposition (EMD) is used, in a data-driven manner, to generate reference signals that can be incorporated into a constrained version of ICA (cICA), thereby eliminating the inherent ambiguities of ICA. The results of the proposed workflow are then compared to those obtained by a widely used group ICA approach for fMRI analysis. In this study, we demonstrate that intrinsic modes, extracted by EMD, are suitable to serve as references for cICA. This approach yields typical resting-state patterns that are consistent over subjects. By introducing these reference signals into the ICA, our processing pipeline yields comparable activity patterns across subjects in a mathematically transparent manner. Our approach provides a user-friendly tool to adjust the trade-off between a high similarity across subjects and preserving individual subject features of the independent components.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Machine Learning Methods for Radar-Based People Detection and Tracking by Mobile Robots",
        "doc_scopus_id": "85079096158",
        "doc_doi": "10.1007/978-3-030-36150-1_31",
        "doc_eid": "2-s2.0-85079096158",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Indoor environment",
            "Learning models",
            "Machine learning approaches",
            "Machine learning methods",
            "Mobile robotic",
            "Moving objects",
            "Non-persons",
            "People detection"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.This paper reports a machine learning approach for people detection and tracking in indoor environments using a compact radar system deployed by a mobile robot. The set-up described in the paper includes a series of experiments carried out in an indoor scenario involving walking people and dummies representative of other moving objects. In these experiments, distinct learning models (a neural network and a random forest) were explored with different combinations of radar features to achieve person versus non-person classification.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Stroke Thrombus Segmentation on SWAN with Multi-Directional U-Nets",
        "doc_scopus_id": "85077955842",
        "doc_doi": "10.1109/IPTA.2019.8936074",
        "doc_eid": "2-s2.0-85077955842",
        "doc_date": "2019-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Acute stroke",
            "Automatic Detection",
            "Automatic segmentations",
            "Human expert",
            "Sagittal plane",
            "Stroke",
            "Thrombus",
            "Thrombus segmentation"
        ],
        "doc_abstract": "© 2019 IEEE.The thrombus causing a stroke can be seen on the susceptibility weighted angiography (SWAN) magnetic resonance imaging (MRI) sequence. But it is very small and hard to detect by humans. Up to date the thrombus is identified by trained human experts. But as stroke needs quick treatment, an automatic detection of the thrombus would be useful to speed up the diagnosis of acute stroke. We propose a method for automatic thrombus detection from SWAN using three separate U-Nets which work on the axial, coronal and sagittal planes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mining EEG scalp maps of independent components related to HCT tasks",
        "doc_scopus_id": "85077903042",
        "doc_doi": "10.1109/EMBC.2019.8857600",
        "doc_eid": "2-s2.0-85077903042",
        "doc_date": "2019-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Ensemble averages",
            "Independent component analysis(ICA)",
            "Independent components",
            "Signal components",
            "Signal dynamics",
            "Visual feedback"
        ],
        "doc_abstract": "© 2019 IEEE.This work presents an unsupervised mining strategy, applied to an independent component analysis (ICA) of segments of data collected while participants are answering to the items of the Halstead Category Test (HCT). This new methodology was developed to achieve signal components at trial level and therefore to study signal dynamics which are not available within participants' ensemble average signals. The study will be focused on the signal component that can be elicited by the binary visual feedback which is part of the HCT protocol. The experimental study is conducted using a cohort of 58 participants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hybridizing EMD with cICA for fMRI Analysis of Patient Groups",
        "doc_scopus_id": "85077879588",
        "doc_doi": "10.1109/EMBC.2019.8856355",
        "doc_eid": "2-s2.0-85077879588",
        "doc_date": "2019-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Activity patterns",
            "Data-driven methods",
            "Empirical Mode Decomposition",
            "Functional magnetic resonance imaging",
            "Independent component analysis(ICA)",
            "Individual features",
            "Multivariate approach",
            "Reference signals",
            "Algorithms",
            "Brain",
            "Brain Mapping",
            "Humans",
            "Magnetic Resonance Imaging",
            "Principal Component Analysis"
        ],
        "doc_abstract": "© 2019 IEEE.Independent component analysis (ICA), as a data driven method, has shown to be a powerful tool for functional magnetic resonance imaging (fMRI) data analysis. One drawback of this multivariate approach is, that it is naturally not convenient for analysis of group studies. Therefore various techniques have been proposed in order to overcome this limitation of ICA. In this paper a novel ICA based work-flow for extracting resting state networks from fMRI group studies is proposed. An empirical mode decomposition (EMD) is used to generate reference signals in a data driven manner, which can be incorporated into a constrained version of ICA (cICA), what helps to overcome the inherent ambiguities. The results of the proposed workflow are then compared to those obtained by a widely used group ICA approach. It is demonstrated that intrinsic modes, extracted by EMD, are suitable to serve as references for cICA to obtain typical resting state patterns, which are consistent over subjects. This novel processing pipeline makes it transparent for the user, how comparable activity patterns across subjects emerge, and also the trade-off between similarity across subjects and preserving individual features can be well adjusted and adapted for different requirements in the new work-flow.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive open-ended object, affordance and grasp learning for robotic manipulation",
        "doc_scopus_id": "85071516057",
        "doc_doi": "10.1109/ICRA.2019.8794184",
        "doc_eid": "2-s2.0-85071516057",
        "doc_date": "2019-05-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2019 IEEE.Service robots are expected to autonomously and efficiently work in human-centric environments. For this type of robots, object perception and manipulation are challenging tasks due to need for accurate and real-time response. This paper presents an interactive open-ended learning approach to recognize multiple objects and their grasp affordances concurrently. This is an important contribution in the field of service robots since no matter how extensive the training data used for batch learning, a robot might always be confronted with an unknown object when operating in human-centric environments. The paper describes the system architecture and the learning and recognition capabilities. Grasp learning associates grasp configurations (i.e., end-effector positions and orientations) to grasp affordance categories. The grasp affordance category and the grasp configuration are taught through verbal and kinesthetic teaching, respectively. A Bayesian approach is adopted for learning and recognition of object categories and an instance-based approach is used for learning and recognition of affordance categories. An extensive set of experiments has been performed to assess the performance of the proposed approach regarding recognition accuracy, scalability and grasp success rate on challenging datasets and real-world scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Technical Note: A comparison of point set registration methods for electromagnetic tracking",
        "doc_scopus_id": "85062939399",
        "doc_doi": "10.1002/mp.13443",
        "doc_eid": "2-s2.0-85062939399",
        "doc_date": "2019-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biophysics",
                "area_abbreviation": "BIOC",
                "area_code": "1304"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Brachytherapy",
            "Coherent point drifts",
            "Electromagnetic tracking",
            "High-dose rate brachytherapy",
            "Multi-dimensional scaling",
            "Multidimensional scaling techniques",
            "Point-set registrations",
            "Probabilistic methods",
            "Brachytherapy",
            "Breast Neoplasms",
            "Electromagnetic Phenomena",
            "Equipment Design",
            "Female",
            "Humans",
            "Image Processing, Computer-Assisted",
            "Organs at Risk",
            "Radiotherapy Dosage",
            "Radiotherapy Planning, Computer-Assisted",
            "Tomography, X-Ray Computed"
        ],
        "doc_abstract": "© 2019 American Association of Physicists in MedicinePurpose: High dose rate brachytherapy applies intense and destructive radiation. A treatment plan defines radiation source dwell positions to avoid irradiating healthy tissue. The study discusses methods to quantify any positional changes of source locations along the various treatment sessions. Methods: Electromagnetic tracking (EMT) localizes the radiation source during the treatment sessions. But in each session the relative position of the patient relative to the filed generator is changed. Hence, the measured dwell point sets need to be registered onto each other to render them comparable. Two point set registration techniques are compared: a probabilistic method called coherent point drift (CPD) and a multidimensional scaling (MDS) technique. Results: Both enable using EMT without external registration and achieve very similar results with respect to dwell position determination of the radiation source. Still MDS achieves smaller grand average deviations (CPD-rPSR: MD = 2.55 mm, MDS-PSR: MD = 2.15 mm) between subsequent dwell position determinations, which also show less variance (CPD-rPSR: IQR = 4 mm, MDS-PSR: IQR = 3 mm). Furthermore, MDS is not based on approximations and does not need an iterative procedure to track sensor positions inside the implanted catheters. Conclusion: Although both methods achieve similar results, MDS is to be preferred over rigid CPD while nonrigid CPD is unsuitable as it does not preserve topology.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Does music help to be more attentive while performing a task? A brain activity analysis.",
        "doc_scopus_id": "85062534062",
        "doc_doi": "10.1109/BIBM.2018.8621388",
        "doc_eid": "2-s2.0-85062534062",
        "doc_date": "2019-01-21",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Attention level",
            "Biomedical signal",
            "Brain activity analysis",
            "Concentration levels",
            "Electroencephalographic signals",
            "Mindwave",
            "Physiological systems",
            "Strong correlation"
        ],
        "doc_abstract": "© 2018 IEEE.The human brain is one of the most complex physiological systems, involving billions of interacting physiological and chemical processes giving rise to the experimentally observed neuroelectric activity, called electroencephalographic (EEG) signals. A biomedical signal, electroencephalography (EEG), reflects the state of mind and is often used to verify the influence of music on human brain activity. In this paper we present a brief study of various effects of music on Attention and Concentration state of the brain using the Neurosky Mindwave Mobile Headset and EEG Analyser application. The aim of the study is to identify which type of music can help the brain activities to be more attentive while performing a cognitive task (playing a game). In contrast the effects of music in the Concentration state is also studied in order to understand how the music can help the brain activities to be more focused. Although the number of participants was reduced, the results were interesting. The music acts in an individualized way in the performance of each participant. It was found that for certain tasks there is a strong correlation between Attention and Concentration levels. In addition, a distinction between genders of participants can be made according to the results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Attention and concentration in normal and deaf gamers",
        "doc_scopus_id": "85062526459",
        "doc_doi": "10.1109/BIBM.2018.8621513",
        "doc_eid": "2-s2.0-85062526459",
        "doc_date": "2019-01-21",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Attention",
            "Blinking",
            "Concentration levels",
            "Hearing impaired",
            "Impaired hearings",
            "Mindwave",
            "Normal hearing"
        ],
        "doc_abstract": "© 2018 IEEE.In this research the performance of individuals with normal hearing and impaired hearing while playing a computer game was evaluated. The aim was to study and understand if impaired hearing gamers are at disadvantage when playing games without being able to hear the music. Three levels (attention, concentration and blinking) were measured to compare and understand how sound can influence players' attention and concentration performance. The data was recorded using Mindwave equipment during the game Outlast considering two scenarios: game with sound and game without sound. The results show that hearing impaired individuals have the same standard of attention and level of concentration as individuals with normal hearing when there is sound in the game. In the case of the blinking level, this is quite different between the scenarios and the analyzed groups. For this particular study the results suggest that sound is not an important level in the attention and concentration performance of impaired hearing players. Although much work still needs to be done, there is evidence of a relation between the attention and the concentration levels between normal hearing and impaired hearing individuals in the presence and absence of sound.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Machine learning methods for radar-based people detection and tracking",
        "doc_scopus_id": "85072888214",
        "doc_doi": "10.1007/978-3-030-30241-2_35",
        "doc_eid": "2-s2.0-85072888214",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Applied machine learning",
            "High potential",
            "Indoor environment",
            "Locomotion patterns",
            "Machine learning methods",
            "Machine learning techniques",
            "People detection",
            "Random forest classifier"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.This paper describes the work developed towards the implementation of a radar-based system for people detection and tracking in indoor environments using machine learning techniques. For such, a series of experiments were carried out in an indoor scenario involving walking people and dummies representative of other moving objects. The applied machine learning methods included a neural network and a random forest classifier. The success rates (accuracies) obtained with both methods using the experimental data sets evidence the high potential of the proposed approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Identifying evoked potential response patterns using independent component analysis and unsupervised learning",
        "doc_scopus_id": "85062858858",
        "doc_doi": "10.1088/2057-1976/aaeeed",
        "doc_eid": "2-s2.0-85062858858",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Nursing (all)",
                "area_abbreviation": "NURS",
                "area_code": "2900"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IOP Publishing Ltd.Independent Component Analysis (ICA) is a pre-processing step widely used in brain studies. One of the most common problems in artifact elimination or brain activity related studies is the ordering and identification of the independent components (ICs). In this work, a novel procedure is proposed which combines ICA decomposition at trial level with an unsupervised learning algorithm (K-means) at participant level in order to enhance the related signal patterns which might represent interesting brain waves. The feasibility of this methodology is evaluated with EEG data acquired with participants performing on the Halstead Category Test. The analysis shows that it is possible to find the Feedback Error Negativity (FRN) Potential at single-trial level and relate its characteristics with the performance of the participant based on their knowledge of the abstract principle underlying the task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "MDSLAB: A toolbox for the analysis of point sets using multi-dimensional scaling, hartigan dip test and α-stable distributions",
        "doc_scopus_id": "85056374675",
        "doc_doi": "10.1088/2057-1976/aac19c",
        "doc_eid": "2-s2.0-85056374675",
        "doc_date": "2018-10-25",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biophysics",
                "area_abbreviation": "BIOC",
                "area_code": "1304"
            },
            {
                "area_name": "Bioengineering",
                "area_abbreviation": "CENG",
                "area_code": "1502"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Physiology",
                "area_abbreviation": "BIOC",
                "area_code": "1314"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IOP Publishing Ltd.Background and objective: Mapping different point sets onto each other is a frequent problem in many fields of science. In radiation therapy, for example, electromagnetic tracking allows to measure the spatial location of radiation sources inside the treatment volume without external registration. The various source loci form point sets, with concomitant subsets, which refer to different coordinate systems when data is collected during different treatment sessions. We present a toolbox, called MDSLAB, which allows to unify a given point set and its reference point set. Methods: The toolbox relies on multi-dimensional scaling and the estimation of principal coordinates only from observed distances between pairs of points in each set. Deviations between the principal coordinates, i.e. the data projections, are quantified and their histograms analyzed employing a Hartigan dip test. Observed uni- or bimodal, asymmetric distributions of distance deviations are approximated by α-stable distributions. Results: We illustrate the working of the toolbox with an application to data collected during high dose rate brachytherapy. Sensor dwell positions inside catheters, implanted into a female breast, are collected, their distance matrices estimated and their underlying principal coordinates computed by diagonalizing the related kernel matrix. Considering the projections of the data onto their principal axes, distance deviations are quantified, their underlying distributions determined and approximated by heavy-tailed distributions. Tools, either images or whole animations, are developed to visualize the spatial dwell positions of the sensors, which map out the catheter shapes before any radiation treatment is started. Distance deviation histograms are also visualized and fit to α-stable distributions. Conclusion: MDSLAB provides a convenient tool to register point sets which are collected in different coordinate systems but whose relative distances should be identical, ideally. In practice, however, they mostly differ and MDSLAB allows to quantify such deviations and analyze their statistics as well as conveniently visualize them in images or movies.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Frequency-resolved dynamic functional connectivity reveals scale-stable features of connectivity-states",
        "doc_scopus_id": "85055114002",
        "doc_doi": "10.3389/fnhum.2018.00253",
        "doc_eid": "2-s2.0-85055114002",
        "doc_date": "2018-06-26",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Neuropsychology and Physiological Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3206"
            },
            {
                "area_name": "Neurology",
                "area_abbreviation": "NEUR",
                "area_code": "2808"
            },
            {
                "area_name": "Psychiatry and Mental Health",
                "area_abbreviation": "MEDI",
                "area_code": "2738"
            },
            {
                "area_name": "Biological Psychiatry",
                "area_abbreviation": "NEUR",
                "area_code": "2803"
            },
            {
                "area_name": "Behavioral Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2802"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 Goldhacker, Tomé, Greenlee and Lang.Investigating temporal variability of functional connectivity is an emerging field in connectomics. Entering dynamic functional connectivity by applying sliding window techniques on resting-state fMRI (rs-fMRI) time courses emerged from this topic. We introduce frequency-resolved dynamic functional connectivity (frdFC) by means of multivariate empirical mode decomposition (MEMD) followed up by filter-bank investigations. In general, we find that MEMD is capable of generating time courses to perform frdFC and we discover that the structure of connectivity-states is robust over frequency scales and even becomes more evident with decreasing frequency. This scale-stability varies with the number of extracted clusters when applying k-means. We find a scale-stability drop-off from k = 4 to k = 5 extracted connectivity-states, which is corroborated by null-models, simulations, theoretical considerations, filter-banks, and scale-adjusted windows. Our filter-bank studies show that filter design is more delicate in the rs-fMRI than in the simulated case. Besides offering a baseline for further frdFC research, we suggest and demonstrate the use of scale-stability as a possible quality criterion for connectivity-state and model selection. We present first evidence showing that connectivity-states are both a multivariate, and a multiscale phenomenon. A data repository of our frequency-resolved time-series is provided.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards lifelong assistive robotics: A tight coupling between object perception and manipulation",
        "doc_scopus_id": "85043759771",
        "doc_doi": "10.1016/j.neucom.2018.02.066",
        "doc_eid": "2-s2.0-85043759771",
        "doc_date": "2018-05-24",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Cognitive Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2805"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3D object",
            "Assistive robots",
            "Interactive learning",
            "Object manipulation",
            "Open-ended learning"
        ],
        "doc_abstract": "© 2018 Elsevier B.V.This paper presents an artificial cognitive system tightly integrating object perception and manipulation for assistive robotics. This is necessary for assistive robots, not only to perform manipulation tasks in a reasonable amount of time and in an appropriate manner, but also to robustly adapt to new environments by handling new objects. In particular, this system includes perception capabilities that allow robots to incrementally learn object categories from the set of accumulated experiences and reason about how to perform complex tasks. To achieve these goals, it is critical to detect, track and recognize objects in the environment as well as to conceptualize experiences and learn novel object categories in an open-ended manner, based on human–robot interaction. Interaction capabilities were developed to enable human users to teach new object categories and instruct the robot to perform complex tasks. A naive Bayes learning approach with a Bag-of-Words object representation are used to acquire and refine object category models. Perceptual memory is used to store object experiences, feature dictionary and object category models. Working memory is employed to support communication purposes between the different modules of the architecture. A reactive planning approach is used to carry out complex tasks. To examine the performance of the proposed architecture, a quantitative evaluation and a qualitative analysis are carried out. Experimental results show that the proposed system is able to interact with human users, learn new object categories over time, as well as perform complex tasks.",
        "available": true,
        "clean_text": "serial JL 271597 291210 291735 291866 31 Neurocomputing NEUROCOMPUTING 2018-03-07 2018-03-07 2018-03-22 2018-03-22 2018-03-22T15:20:58 S0925-2312(18)30232-7 S0925231218302327 10.1016/j.neucom.2018.02.066 S300 S300.1 FULL-TEXT 2018-03-22T16:18:06.833994Z 0 0 20180524 2018 2018-03-07T17:38:33.811543Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid misctext orcid primabst ref vitae 0925-2312 09252312 true 291 291 C Volume 291 14 151 166 151 166 20180524 24 May 2018 2018-05-24 2018 article fla © 2018 Elsevier B.V. All rights reserved. TOWARDSLIFELONGASSISTIVEROBOTICSATIGHTCOUPLINGBETWEENOBJECTPERCEPTIONMANIPULATION HAMIDREZAKASAEI S 1 Introduction 2 Related work 2.1 Assistive and service robots 2.2 Object manipulation 2.3 Object perception and learning 3 Overall system architecture 4 Environment exploration and dictionary construction 5 Object detection and representation 5.1 Object detection and tracking 5.2 Feature extraction and object representation 6 Interactive object category learning and recognition 6.1 User interaction 6.2 Object conceptualizer 6.3 Object category recognition 7 Planning and execution 8 Experimental results 8.1 Off-line evaluation of the perceptual learning approach 8.2 Open-ended evaluation 8.3 A real life use-case: clear table 9 Conclusions Acknowledgments References CIOCARLIE 2014 241 252 M EXPERIMENTALROBOTICS TOWARDSRELIABLEGRASPINGMANIPULATIONINHOUSEHOLDENVIRONMENTS KIM 2012 2 14 D JAIN 2010 45 64 A LEROUX 2013 101 107 C BEETZ 2011 529 536 M 11THIEEERASINTERNATIONALCONFERENCEHUMANOIDROBOTSHUMANOIDS2011 ROBOTICROOMMATESMAKINGPANCAKES VAHRENKAMP 2010 2883 2888 N 2010IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA INTEGRATEDGRASPMOTIONPLANNING JEONG 2012 130 140 S SMITH 2005 13 29 L CHAUHAN 2011 341 354 A HE 2008 1727 1738 H KASAEI 2015 537 553 S OLIVEIRA 2014 2216 2223 M PROCEEDINGSIEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS APERCEPTUALMEMORYSYSTEMFORGROUNDINGSEMANTICREPRESENTATIONSININTELLIGENTSERVICEROBOTS OLIVEIRA 2015 2488 2495 M PROCEEDINGSIEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS CONCURRENTLEARNINGVISUALCODEBOOKSOBJECTCATEGORIESINOPENENDEDDOMAINS OLIVEIRA 2016 614 626 M SRINIVASA 2008 2155 2162 S INTERNATIONALCONFERENCEINTELLIGENTAUTONOMOUSSYSTEMS ROBOTICBUSBOYSTEPSTOWARDSDEVELOPINGAMOBILEROBOTICHOMEASSISTANT HERTZBERG 2014 297 304 J ROCKEL 2013 52 57 S DESIGNINGINTELLIGENTROBOTSREINTEGRATINGAIIIAAAISPRINGSYMPOSIUMSTANFORDUSA ONTOLOGYBASEDMULTILEVELROBOTARCHITECTUREFORLEARNINGEXPERIENCES MOKHTARI 2016 509 517 V TWENTYSIXTHINTERNATIONALCONFERENCEAUTOMATEDPLANNINGSCHEDULING EXPERIENCEBASEDROBOTTASKLEARNINGPLANNINGGOALINFERENCE SRINIVASA 2010 5 20 S BOHG 2014 289 309 J SAHBANI 2012 326 336 A CHINELLATO 2009 223 254 E MONACO 2015 454 465 S CASTIELLO 2005 726 736 U CULHAM 2006 2668 2684 J SHAFII 2016 2895 2900 N 2016IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS LEARNINGGRASPFAMILIAROBJECTSUSINGOBJECTVIEWRECOGNITIONTEMPLATEMATCHING STUCKLER 2013 1106 1115 J ALDOMA 2012 80 91 A MARTINEZTORRES 2010 2043 2049 M IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA2010 MOPEDASCALABLELOWLATENCYOBJECTRECOGNITIONPOSEESTIMATIONSYSTEM ISLAM 2011 1398 1401 M 8THASIANCONTROLCONFERENCEASCC2011 OBJECTCLASSIFICATIONBASEDVISUALEXTENDEDFEATURESFORVIDEOSURVEILLANCEAPPLICATION YEH 2009 280 287 T IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION2009CVPR2009 FASTCONCURRENTOBJECTLOCALIZATIONRECOGNITION YEH 2008 1 8 T IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITIONCVPR2008 DYNAMICVISUALCATEGORYLEARNING KIRSTEIN 2012 90 105 S COLLET 2015 3 25 A QUIGLEY 2009 5 11 M ICRAWORKSHOPOPENSOURCESOFTWARE ROSOPENSOURCEROBOTOPERATINGSYSTEM HAMIDREZAKASAEI 2014 47 52 S 2014IEEEINTERNATIONALCONFERENCEAUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC INTERACTIVEOPENENDEDLEARNINGAPPROACHFOR3DOBJECTRECOGNITION LIM 2014 153 160 G 23RDIEEEINTERNATIONALSYMPOSIUMROBOTHUMANINTERACTIVECOMMUNICATION2014ROMAN INTERACTIVETEACHINGEXPERIENCEEXTRACTIONFORLEARNINGABOUTOBJECTSROBOTACTIVITIES FISCHLER 1981 381 395 M HARTIGAN 1979 100 108 J SEABRALOPES 2007 53 81 L LAI 2011 1817 1824 K 2011IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATIONICRA ALARGESCALEHIERARCHICALMULTIVIEWRGBDOBJECTDATASET BADDELEY 1997 A HUMANMEMORYTHEORYPRACTICE KASAEI 2016 312 320 S RUSU 2010 2155 2162 R 2010IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS FAST3DRECOGNITIONPOSEUSINGVIEWPOINTFEATUREHISTOGRAM KASAEI 2016 1948 1956 S ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMSNIPS HIERARCHICALOBJECTREPRESENTATIONFOROPENENDEDOBJECTCATEGORYLEARNINGRECOGNITION HAMIDREZAKASAEIX2018X151 HAMIDREZAKASAEIX2018X151X166 HAMIDREZAKASAEIX2018X151XS HAMIDREZAKASAEIX2018X151X166XS 2020-03-22T00:00:00.000Z UnderEmbargo © 2018 Elsevier B.V. All rights reserved. item S0925-2312(18)30232-7 S0925231218302327 10.1016/j.neucom.2018.02.066 271597 2018-03-22T15:36:32.186592Z 2018-05-24 true 4881073 MAIN 16 54620 849 656 IMAGE-WEB-PDF 1 gr1 10651 115 219 gr10 11599 164 154 gr11 9351 82 219 gr12 26962 155 219 gr13 10036 87 219 gr14 30351 164 192 gr2 16717 128 219 gr3 17641 163 189 gr4 7005 67 219 gr5 15703 164 190 gr6 10047 120 219 gr7 26213 145 219 gr8 12701 164 211 gr9 9512 121 219 fx1 16407 163 121 fx2 20096 164 123 fx3 18022 164 123 fx4 20147 164 123 fx5 13277 164 123 gr1 47886 271 518 gr10 175841 860 809 gr11 81706 304 809 gr12 35822 274 386 gr13 50601 301 756 gr14 123104 564 662 gr2 124400 457 784 gr3 67649 456 527 gr4 24552 185 602 gr5 64347 520 602 gr6 30416 211 386 gr7 77632 397 602 gr8 155709 628 809 gr9 78168 445 809 fx1 8590 150 111 fx2 8116 151 113 fx3 8034 151 113 fx4 8724 151 113 fx5 9747 151 113 gr1 448813 1441 2752 gr10 1668752 4569 4300 gr11 693215 1614 4300 gr12 373823 1454 2050 gr13 374749 1333 3346 gr14 1106550 2500 2933 gr2 1452536 2428 4164 gr3 683503 2017 2333 gr4 215965 821 2667 gr5 583371 2303 2668 gr6 254415 1122 2053 gr7 709493 1760 2667 gr8 1791270 3340 4300 gr9 673821 2367 4300 fx1 54703 667 494 fx2 57038 667 500 fx3 55795 667 500 fx4 82756 667 500 fx5 86154 667 500 si10 526 16 143 si20 496 16 106 si30 610 18 149 si5 312 17 72 si6 284 13 63 si7 170 11 19 si8 1704 18 510 si9 184 16 20 si11 129 13 13 si12 2064 46 390 si13 443 16 73 si14 396 16 66 si15 633 16 114 si16 1203 16 360 si17 3195 110 354 si18 418 16 71 si19 1128 49 220 si2 396 17 62 si21 185 12 24 si22 394 14 111 si23 611 39 102 si24 511 16 143 si25 1983 43 432 si26 1588 52 298 si27 1391 52 265 si28 166 13 16 si29 1436 31 261 si3 1671 19 527 si4 456 16 95 si1 396 17 63 NEUCOM 19370 S0925-2312(18)30232-7 10.1016/j.neucom.2018.02.066 Elsevier B.V. Fig. 1 Overall architecture of the proposed system. Fig. 1 Fig. 2 Dictionary construction: (left) the robot moves through an office to extract tabletop objects; (center) the captured scenes are processed to produce a pool of object candidates; (right) a pool of local shape features is obtained by computing spin-images from the pool of object candidates; the dictionary is subsequently constructed by clustering the features using the k-means algorithm; finally, a dictionary with 20 visual words is built. Fig. 2 Fig. 3 An example of preprocessing and Object Detection: (a) experiment setup; the JACO robotic arm performs manipulation tasks to clear the table; (b) distance filtering; (c) result of the second preprocessing step and table detection; (d) the position of the arm joints are used to filter out the points corresponding to robot’s body from the original point cloud. The object candidates are shown by different bounding boxes and colors. The red, green and blue lines represent the local reference frame of the objects. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article). Fig. 3 Fig. 4 Object representation for a flask: (a) keypoint extraction; (b) surface normal estimation for the keypoints; (c) a schematic of how spin-image is computed for a keypoint p; (d) histogram of visual words that represents the object view. Fig. 4 Fig. 5 A 3D visualization of an object labelling event: (a) pointing to object by the instructor; (b) associating a label to the object that is currently being pointed; (c) labelling object categories by associating a label to a TrackID; (d) instructing the robot to perform the clear_table task. Fig. 5 Fig. 6 Schematic representation of task planning, grasp planning and execution manager. Fig. 6 Fig. 7 Sequence of snapshots showing the JACO robotic arm performing a constrained pick and place task to clean the table; In this task, the orientation of the grasped object must be kept consistent throughout the plan; (a) the JACO robotic arm goes to the initial pose and extracts object (i.e. ’PlasticCup’) pose and shape properties; (b) a side grasp is selected and the robot goes to pre-grasp position; (c) the robot approaches and grasps the PlasticCup; (d) picking up the PlasticCup and moving it to the side; (e) placing the object and (f) going back to the initial position. Fig. 7 Fig. 8 Object recognition performance for different values of four parameters of the system; the system parameters are represented as a tuple (VS, DS, IW, SL). Fig. 8 Fig. 9 (top) Evolution of teaching protocol accuracy versus number of question/correction iterations in the first 200 iterations of the simulated teacher experiment 1 with the protocol accuracy threshold set to 0.67; (bottom) protocol accuracy versus the number of learned categories, for the same experiment. Fig. 9 Fig. 10 Evolution of teaching protocol accuracy versus number of question/correction iterations in simulated teacher experiments #3, 5, 7 and 9 with the protocol accuracy threshold set to 0.67. Fig. 10 Fig. 11 System performance during simulated user experiments: (left) global accuracy versus number of learned categories, a measure of how well the system learns; (right) number of learned categories versus number of question/correction iterations, represents how fast the system learned object categories. Fig. 11 Fig. 12 Our experimental setup consists of a computer for human–robot interaction purposes, a Kinect sensor and a JACO robotic-arm as the primary sensory-motor embodiments for perceiving and acting upon its environment. Fig. 12 Fig. 13 System performance during the clear_table use-case; (left): Initially, the system starts with no knowledge of any object. The position of the arm joints are retrieved from Working Memory and visualized by grey spheres and black lines. The table is then detected as shown by the green rectangle. Afterwards, the object candidates are detected and highlighted by different colors. The grey bounding boxes and the local reference frames represent the pose of the objects as estimated by object tracking module. (center): A user then teaches all the active objects to the system and all objects are correctly recognized, i.e., the output of object recognition is shown in blue on top of each object. (right): When grasping and manipulating an object, the shape of the object is partially changed and, as a consequence, a misclassification might happen. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article). Fig. 13 Fig. 14 The sequence of snapshots showing the JACO robotic arm performing a clear_table task; (First row): PlasticCup is the closest object to the arm’s base. Therefore, the robot picks it up first from the table, transports it into the first predefined area and then, places the PlasticCup down. (Second row): CoffeeJug is selected as the second closest object. The robot goes to the pre-grasp area and then grasps the CoffeeJug. The robot moves the object into the second placing area and places it down. (Third row): Similarly, Bottle object is picked-up, moved and placed. Fig. 14 Table 1 List of used constraints with a short description for each one. Table 1 Constraints Description Section C table: “is this candidate on a table?” The target object candidate is placed on top of a table. 4 C track: “is this candidate being tracked?” Storing all object views while the object is static would lead to unnecessary accumulation of highly redundant data. This constraint is used to infer that the segmented object is already being tracked or not. 4 C size: “is this candidate manipulatable?” Reject large object candidate 5 C instructor: “is this candidate part of the instructor’s body? Reject candidates that are belong to the user’s body 4 C robot: “is this candidate part of the robot’s body?” Reject candidates that are belong to the robot’s body 4 C edge: “is this candidate near to the edge of the table?” Reject candidates that are near to the edge of the table 5 C key _ view : “is this candidate a key view?” For representing an object, only object views that are marked as key-views are stored in the database. An object view is selected as a key view whenever the tracking of an object is initialized, or when it becomes static again after being moved. In case the hands are detected near the object, storing key views is postponed until the hands are withdrawn. 5 Table 2 Average object recognition performance for different parameters. Table 2 Parameters VS DS IW SL Values 0.01 0.02 0.03 50 60 70 80 90 4 8 0.02 0.03 0.04 0.05 Average accuracy 0.76 0.74 0.71 0.72 0.73 0.74 0.74 0.75 0.75 0.72 0.63 0.74 0.78 0.79 Table 3 Summary of experiments(1). Table 3 EXP# #QCI #LC #AIC GCA (%) APA (%) 1 1257 49 8.16 79 83 2 1228 49 7.83 80 84 3 1227 49 7.65 81 84 4 1240 49 9.08 75 78 5 1236 49 7.95 80 83 6 1346 49 9.46 76 79 7 1293 49 9.02 77 81 8 1330 49 9.79 74 79 9 1336 49 9.55 75 78 10 1225 49 8.30 78 82 EXP#: experiment number; QCI: Question/Correction Iterations; LC: Learned Categories; AIC: Average Instances per Category; GCA: Global Classification Accuracy; APA: Average Protocol Accuracy. Towards lifelong assistive robotics: A tight coupling between object perception and manipulation S. Hamidreza Kasaei ⁎ a Miguel Oliveira a b Gi Hyun Lim a Luís Seabra Lopes a c Ana Maria Tomé a c a IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro Universidade de Aveiro Portugal b Instituto de Engenharia de Sistemas e Computadores, Tecnologia Ciência, R. Dr. Roberto Frias, 465, Porto 4200, Portugal Instituto de Engenharia de Sistemas e Computadores, Tecnologia Ciência R. Dr. Roberto Frias, 465 Porto 4200 Portugal c Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Electrónica, Telecomunicações e Informática Universidade de Aveiro Portugal ⁎ Corresponding author. Communicated by Prof. Zidong Wang This paper presents an artificial cognitive system tightly integrating object perception and manipulation for assistive robotics. This is necessary for assistive robots, not only to perform manipulation tasks in a reasonable amount of time and in an appropriate manner, but also to robustly adapt to new environments by handling new objects. In particular, this system includes perception capabilities that allow robots to incrementally learn object categories from the set of accumulated experiences and reason about how to perform complex tasks. To achieve these goals, it is critical to detect, track and recognize objects in the environment as well as to conceptualize experiences and learn novel object categories in an open-ended manner, based on human–robot interaction. Interaction capabilities were developed to enable human users to teach new object categories and instruct the robot to perform complex tasks. A naive Bayes learning approach with a Bag-of-Words object representation are used to acquire and refine object category models. Perceptual memory is used to store object experiences, feature dictionary and object category models. Working memory is employed to support communication purposes between the different modules of the architecture. A reactive planning approach is used to carry out complex tasks. To examine the performance of the proposed architecture, a quantitative evaluation and a qualitative analysis are carried out. Experimental results show that the proposed system is able to interact with human users, learn new object categories over time, as well as perform complex tasks. Keywords Assistive robots 3D object perception Open-ended learning Interactive learning Object manipulation 1 Introduction Assistive robots are extremely useful because they can help elders or people with motor impairments to achieve independence in everyday tasks [1,2]. Elderly, injured, and disabled people have consistently attributed a high priority to object manipulation tasks [3]. Object manipulation tasks consist of two phases: the first is the perception of the object and the second is the planning and execution of arm or body motions which grasp the object and carry out the manipulation task. These two phases are closely related: object perception provides information to update the model of the environment, while planning uses this world model information to generate sequences of arm movements and grasp actions for the robot. In addition, assistive robots must perform the tasks in reasonable time. It is also expected that the competence of the robot increases over time, that is, robots must robustly adapt to new environments by being capable of handling new objects. However, it is not reasonable to assume that one can pre-program all necessary object categories for assistive robots. Instead, robots should learn autonomously from novel experiences, supported in the feedback from human teachers. In order to incrementally adapt to new environments, an autonomous assistive robot must have the ability to process visual information and conduct learning and recognition tasks in a concurrent and interleaved fashion. Several state-of-the-art assistive robots use traditional object category learning and recognition approaches [4–6]. These classical approaches are often designed for static environments in which it is viable to separate the training (off-line) and testing (on-line) phases. In these cases, the world model is static, in the sense that the representation of the known categories does not change after the training stage. Therefore, these robots are unable to adapt to dynamic environments [7]. This leads to several shortcomings such as the inability to detect/recognize new or unknown categories. To cope with these issues, several cognitive robotics groups have started to explore how robots could learn incrementally from their own experiences as well as from interaction with humans [8–10]. In this paper, a cognitive framework for assistive robots is presented which provides a tight coupling between object perception and manipulation. The approach is designed to be used by an assistive robot working in a domestic environment. In particular, we present an adaptive object perception system based on environment exploration and Bayesian learning. The objective is that the robotic system is capable of continuously learning new object categories while carrying out manipulation tasks in the environment. This work focuses on learning, recognizing and manipulating table-top objects. The contributions proposed in this work are the following: (i) an integrated framework for object manipulation incorporating perception and planning capabilities for manipulation tasks; (ii) unsupervised object exploration methodology that produces a dictionary of visual words used for representing objects (Bag-of-Words model); (iii) interactive categorization (labelling) of physical objects, in which a human user playing the role of tutor provides category labels for objects under shared attention; (iv) open-ended learning of object category models from experiences. The fourth contribution follows our previous works on open-ended learning for object recognition [11–14]. These previous approaches are instance-based, i.e. a set of features is stored for each object view. In contrast, the present work uses a Naive Bayes learning method to compute category models from the observed views of instances of the categories. Furthermore, manipulation experiments are carried out for validating the approach. The remainder of the paper is organized as follows: Section 2 describes the related work; an overview of the developed system is presented in Section 3; Sections 4–7 describe in detail the proposed methodologies. Finally, results are presented and discussed in Section 8 and conclusions are presented in Section 9. 2 Related work Although an exhaustive survey of assistive robotics as well as object perception and manipulation techniques is beyond the scope of this paper, representative works will be reviewed in this section. 2.1 Assistive and service robots Daily tasks such as setting a table for a meal or cleaning a table are difficult for disabled or elder people [2]. Over the past decade, several researches have been conducted to develop robots to assist those people in order to enable them to maintain an active life less dependent on others [1]. In the ARMEN project, Leroux et al. [4] proposed a mobile assistive robotics approach providing advanced functions to help maintaining elderly or disabled people at home. Similar to our system, this project involves object manipulation, knowledge representation and object recognition. The authors also developed an interface to facilitate the communication between the user and the robot. Jain et al. [3] presented an assistive mobile manipulator named EL-E that can autonomously pick objects from a flat surface and deliver them to the users. They used a multi-step control policy that is not suitable to achieve real time performance. In our approach we can achieve real-time performance through the use of ROS nodelets and multiplexing mechanisms [12]. Furthermore, in [3], the user provides the location of the object to be grasped by the robot by briefly illuminating a location with a laser pointer. In this work, objects are detected and recognized autonomously. Therefore it is enough for the user to specify the category of the object to be picked up. In another work [15], a multi-robot assistive system, consisting of a Segway mobile robot with a tray and a stationary Barrett WAM robotic arm, was developed. The Segway robot navigates through the environment and collects empty mugs from people. Then, it delivers the mugs to a predefined position near the Barrett arm. Afterwards, the arm detects and manipulates the mugs from the tray and loads them into a dishwasher rack. This work is similar to ours in that it integrates perception and motion planning for pick and place operations. However there are some differences: their vision system is designed for detecting a single object type (mugs), while our perception system not only tracks the pose of different types of objects but also recognizes their categories. Furthermore, because there is a single object type (i. e. mug), they computed the set of grasp points off-line. In our approach, grasping must handle a variety of objects never seen before. In the RACE project (Robustness by Autonomous Competence Enhancement), a PR2 robot demonstrated effective capabilities in a restaurant scenario including the ability to serve a coffee, set a table for a meal and clear a table [16–18]. The aim of RACE was to develop a cognitive system, embodied by a service robot, which enabled the robot to build a high-level understanding of the world by storing and exploiting appropriate memories of its experiences. Other examples of assistive robot platforms that have demonstrated perception and action coupling include TUM Rosie robot [5], HERB [19] and ARMAR-III [6]. 2.2 Object manipulation In most cases, prior works on object manipulation requires a complete geometric description of the objects [20,21]. However, in real scenarios, it is not possible to have complete knowledge of the geometric properties of all possible objects in advance. That information has to be extracted online from the experiences of the robot. In neuroscience and neurocomputing literature, it has been demonstrated that visual processing in the ventral and dorsal pathways is based on classifying the grasped objects into three groups: known, familiar and unknown objects [1,22–25]. This classification has been adopted in robotics [20]. The underlying reason for this classification is that prior knowledge about objects determines how grasp candidates are generated and ranked. For known objects, i.e., when there is complete knowledge of the geometric properties of objects, grasping is limited to solving the problems of recognition and pose estimation. In the case of familiar objects, an object comparison procedure may be used to compare the given object with known objects, and to define grasping strategies based on that [26]. For unknown objects, heuristic methods are used to extract grasps in run-time from 3D sensor data. Commonly, the heuristic methods work based on both the overall shape of the object and its features. For more details on grasp synthesis, we refer the reader to the surveys of Bohg et al. [20] and Sahbani [21]. Similar to our grasping approach, Ciocarlie et al. [1] and Stuckler et al. [27] have considered grasps on objects either from above or from the side based on the overall shape of the object and the global characteristics such as center of mass and bounding box obtained from RGBD data. The intuition behind this approach is that many domestic objects are graspable by aligning the grippers with the (estimated) principal axes of the object. They follow a standard train and test procedure for object recognition, while our approach can incrementally update its knowledge based on new observations. 2.3 Object perception and learning Interactive open-ended object category learning and recognition are key capabilities in assistive and service robotics. This means that a robot should be capable of continuously learning new objects in order to perform different tasks in domestic domains. Aldoma et al. [28] reviewed properties, advantages and disadvantages of several state-of-the-art 3D shape descriptors available from the Point Cloud Library (PCL) to develop 3D object recognition and pose estimation system. They also proposed two pipelines for object recognition systems using local and global 3D shape descriptors from PCL. Martinez et al. [29] described a fast and scalable perception system for object recognition and pose estimation. The authors employed the RANSAC and Levenberg Marquardt algorithms to segment objects and represented them based on SIFT descriptors. In [30], an object classification approach was proposed, in which the object representation was based on SIFT, SURF and color histograms. All these features were compacted into a histogram of visual words for optimizing the recognition process, as well as memory usage. In this case, authors used a naive Bayes classifier in the recognition stage. Yeh et al. [31] integrated the bag-of-words methodology to propose an efficient method for concurrent object localization and recognition. In most of the proposed systems described above, training and testing are separate processes, i.e., they do not occur simultaneously. However, in open-ended applications, data is continuously available and the target object categories are not known in advance. In these cases, traditional object recognition approaches are not well suited, because those systems are limited to using off-line data for training and are therefore unable to adapt to new environments / objects. There are some approaches which support incremental learning of object categories. In these approaches, the set of classes is predefined and the models of known object categories are enhanced (e.g., augmented, improved) over time, while in open-ended approaches the set of categories is also continuously growing. Haibo et al. [10] proposed an incremental multiple-object recognition and localization (IMORL) framework using a multilayer perceptron (MLP) structure as the base learning model. The authors claimed that the proposed framework can incrementally learn from accumulated experiences and use such knowledge for object recognition. Yeh and Darrell [32] developed novel methods for efficient incremental learning of SVM-based visual category classifiers, and showed that, using their framework, it is possible to adapt the classifiers incrementally. Kirstein et al. [33] proposed a lifelong learning approach for interactive learning of multiple categories based on vector quantization and a user interface. Collet et al. [34] proposed a graph-based approach for lifelong robotic object discovery. Similar to our approach, they used a set of constraints to explore the environment and to detect object candidates from raw RGB-D data streams. In contrast, their system does not interactively acquire more data to learn and recognize the object. Seabra Lopes and Chauhan [9] approached the problem of object experience gathering and category learning with a focus on open-ended learning and human-robot interaction. In their approach, learning is based on multiple representations as well as combinations of classifiers. They showed a system that starts with an empty vocabulary and can incrementally acquire object categories through the interaction with a human user. They used RGB data whereas we used depth data. Moreover, their object detection, learning and recognition approaches are completely different from our approach. 3 Overall system architecture The overall system architecture is depicted in Fig. 1 . It is a reusable framework, with all modules developed in Robot Operating System (ROS) [35]. The current architecture is an evolution of the architecture developed in previous work for object perception and open-ended perceptual learning [14,36]. Information exchange is performed using standard ROS mechanisms (i.e. either publish / subscribe or server/client). Therefore, any new module can be easily added to the system. The architecture includes two memory systems, namely the Working Memory and the Perceptual Memory. Both memory systems have been implemented using a lightweight NoSQL database called LevelDB 1 1 LevelDB has been developed by Google: . LevelDB is a fast key-value storage database that provides an ordered mapping from string keys to string values. The Working Memory is used for temporarily storing information as well as for communication among different modules. It keeps track of the evolution of both the internal state of the robot and the events observed in the environment (i.e. world model). The object features, dictionary of visual words, object representation data and object category models are stored into the Perceptual Memory. The goal of Grasp Planning is to extract a grasp pose (i.e. a gripper pose relative to the object) either from above or from the side of the object, using global characteristics of the object. The Execution Manager works based on a Finite-State-Machine (FSM) paradigm. It retrieves the task plan and the world model information from Working Memory and computes the next action (i.e. a primitive operator) based on the current context. Then, it dispatches the action to the robot platform as well as records success or failure information in the Working Memory. Whenever the robot captures a scene, the first step is preprocessing which includes three filtering procedures, namely distance filtering, a filter to remove the robot’s body from sensor data, and a downsampling filter for reducing the size of the data. Object Detection, responsible for detecting objects in the scene, launches a new perception pipeline for each detected object. Each pipeline includes Object Tracking, Feature Extraction, Object Representation and Object Recognition modules. The Object Tracking module estimates the current pose of the object based on a particle filter, which uses shape and color data [12]. The Feature Extraction module extracts features of the current object view and stores them in the Perceptual Memory. Based on the extracted features and on a visual dictionary, the Object Representation module describes objects as histograms of visual words and stores them into the Perceptual Memory. A user can provide category labels for these objects via the User Interaction module [37]. User Interaction is essential for supervised experience gathering. A graphical user interface has been developed to teach the robot new object categories or to instruct the robot to perform a complex task. The developed architecture, shown in Fig. 1, includes two perceptual learning modules. One of them, the Dictionary Builder, is concerned with building a dictionary of visual words for object representation. The dictionary plays a prominent role because it is used for category learning as well as recognition. The second learning module is the Object Conceptualizer. Whenever the instructor provides a category label for an object, the Conceptualizer retrieves the probabilistic models of the current object categories as well as the representation of the labeled object in order to improve an existing object category model or to create a new category model. In recognition situations, a probabilistic classification rule is used to assign a category label to the detected object. The system is run in two stages. The first stage is dedicated to environment exploration. In this stage, unsupervised object discovery is carried out in the environment while the robot operates. The robot seeks to segment the world into “object” and “non-object”. Afterwards, a pool of shape features is created by computing local shape features for the extracted objects. The pool of features is then clustered by the Dictionary Builder leading to a set of visual words (dictionary). Only the modules directly involved in object discovery and dictionary building are active in this stage. The second stage corresponds to the normal operation of the robot, with object category learning, recognition, planning and execution. In the following sections, the characteristics of each module are explained in detail. 4 Environment exploration and dictionary construction Comparing 3D objects by their local features would be computationally expensive. To address this problem, a Bag-of-Word (BoW) approach is adopted for object representation, i.e. objects are described by histograms of local shape features. This approach requires a dictionary of visual words. Usually, this dictionary is created off-line through clustering of a given training set. In open-ended learning scenarios, there is no predefined set of training data available at the beginning of the learning process. To cope with this limitation, we look at human cognition, in particular at the fact that human babies explore their environment in a playful (arbitrary) way [8]. Therefore, we propose that the robot freely explores several scenes and collects several object experiences. Gathering object experiences by exploration has the advantage of not requiring any human annotation of individual objects. This (non goal-directed) exploration provides chances to discover new objects. In general, object exploration is a challenging task because of the dynamic nature of the world and ill-definition of the objects [34]. Since a system of boolean equations can represent any expression or any algorithm, it is particularly well suited for encoding the world and object candidates. Similar to Collet’s work [34], we use boolean algebra 2 2 , using three logical operators, namely AND (∧), OR (∨) and NOT (¬). A set of boolean constraints, C, was then defined based on which boolean expressions, ψ, were established to encode object candidates for the process of constructing the dictionary of visual words as well as for interactive object category learning and recognition (see Table 1 ). The definition of “object” in the exploration stage is more general than in the normal operation stage (see Eqs. 1 and 4). In both cases, we assume that interesting objects are on tables and the robot seeks to detect tabletop objects (i.e. C table). Due to memory size concerns, a representation of an object should only contain distinctive views. A view which is different from the current view may appear after the object is moved (i.e. the pose of the object relative to the sensor changes). An object view is selected as a key view (i.e. C k e y _ v i e w ) whenever the tracking of an object is initialized (C track), or when it becomes static again after being moved. Therefore, the C k e y _ v i e w constraint is used to optimize memory usage and computation while keeping potentially relevant and distinctive information. Moreover, C instructor and C robot are used to filter out object candidates which are part of the instructor’s body or robot’s body. Accordingly, the resulting object candidates are less noisy and include only data corresponding to the environment: (1) ψ exploration = C table ∧ C track ∧ C key _ view ∧ ¬ ( C instructor ∨ C robot ) , In our current setup, a table is detected by finding the dominant plane in the point cloud. This is done using the RANSAC algorithm [38]. Extraction of polygonal prisms is used for collecting the points which lie directly above the table. Afterwards, an Euclidean Cluster Extraction 3 3 algorithm is used to segment each scene into individual clusters. Every cluster that satisfies the exploration expression, ψ exploration, is selected. The output of object exploration is a pool of object candidates. It should be noted that to balance computational efficiency and robustness, a downsampling filter is applied to obtain a smaller set of points distributed over the surface of the object. Subsequently, to construct a pool of features, spin-images 4 4 The default spin-image parameters are the following: S L = 5 mm , A = π / 2 and I W = 4 . are computed for the selected points extracted from the pool of object candidates. We use a PCL function to compute spin-images 5 5 In this work, we computed around 32,000 spin-images from the point cloud of the 194 objects. . These capabilities are implemented in the Object Detection, Object Representation and Feature Extraction modules (see Fig. 1). Finally, the dictionary is constructed by clustering the features using the k-means algorithm [39]. The centers of the N generated clusters are treated as visual words, w i (1 ≤ i ≤ N). Fig. 2 shows a dictionary containing 20 words. In the implementation, we tested different dictionary sizes (see Section 8.1). In the context of the RACE project [16], the University of Osnabruck provided us with a rosbag collected by one of their robots while exploring an office environment. A video of this exploration is available at: The exploration stage was run on this rosbag. 5 Object detection and representation This section presents the Object Detection, Feature Extraction and Object Representation modules as they are used in the normal operation stage. 5.1 Object detection and tracking A common way for fast processing of massive point clouds is to use some mechanisms for removing unnecessary or irrelevant data. For this purpose, two filters are used that discard large quantities of 3D points from the original point cloud. The first step is to define a cubic volume in 3D (distance filtering), which defines the region of interest. The second filter reduces the spatial resolution of points (downsampling) using a voxelized grid approach 6 6 . Furthermore, the points corresponding to the body of the robot are filtered out from the original point cloud by retrieving the knowledge of the positions of the arm joints relative to the camera pose from the working memory. After preprocessing, the next step is to find objects in the scene using the preprocessed point cloud. The object detection module implements the following specification: (2) ψ detection = C table ∧ C track ∧ C size ∧ ¬ ( C instructor ∨ C robot ∨ C edge ) , The object detection uses a size constraint, C size, to detect objects which can be manipulated by the robot. Moreover, a C edge constraint is considered to filter out the segmented point clouds that are too close to the edge of the table. The Object Detection module then assigns a new TrackID to each newly detected object and launches an object perception pipeline for the object. Finally, the object detection module pushes the segmented object candidate into the respective pipeline for subsequent processing steps. An example of the proposed detection approach is shown in Fig. 3 . The Object Tracking module is responsible for keeping track of the target object over time while it remains visible. It receives the point cloud of the detected object and computes an oriented bounding box aligned with the point cloud’s principal axes. The center of the bounding box is considered as the pose of the object. The module sends out the tracked object information to the Feature Extraction module. 5.2 Feature extraction and object representation Object representation is critical to any object recognition system. In the present work, we adopt an approach to object representation in which object views (instances) are described by histograms of frequencies of visual words. The input is the set of features of an object candidate, O , computed by the Feature Extraction module. The Feature Extraction module involves keypoint extraction and computation of a spin image for each keypoint. Finally, the Object Representation module represents these features as a histogram of visual words. For keypoint extraction, first a voxelized grid approach is used to obtain a smaller set of points. The nearest neighbor point to each voxel center is selected as a keypoint [11]. Afterwards, the spin-image descriptor is used to encode the surrounding shape in each keypoint using the original point cloud. By searching for the nearest neighbor in the dictionary, each spin image is assigned to a visual word. Finally, each object is represented as a histogram of occurrences of visual words: (3) h = [ h 1 h 2 . . . h n ] , where the ith element of h is the count of the number of features assigned to a visual word, w i and n is the size of the dictionary. Fig. 4 illustrates the Feature Extraction and Object Representation processes for an object. The obtained histogram is dispatched to the Object Recognition module and is recorded in Perceptual Memory. To optimize the Perceptual Memory, some object views are marked as key views and only these are recorded into the memory. Key object views are selected by the Object Tracking module when the object is not moving and the user’s hands are far away from the object [37]. In other words, key views are defined as follows: (4) ψ key _ view = C table ∧ C track ∧ C size ∧ C key _ view ∧ ¬ ( C instructor ∨ C robot ∨ C edge ) . 6 Interactive object category learning and recognition The key idea for fast 3D object recognition is to use mechanisms for representing objects in a uniform and compact format. Estimating a robust model for each object category is more promising than template matching. In this section, first, a user interface for supervised experience gathering is presented. The interface is used not only for teaching new object categories in situations where the robot encounters with new objects but also for providing corrective feedback in the case there is a misclassification. The Bag-of-Words representation combined with the Naive Bayes approach are used to incrementally learn probabilistic models of object categories. 6.1 User interaction Human–robot interaction is essential for supervised experience gathering i.e. for instructing the robot how to perform different tasks. Particularly, an open-ended object category learning and recognition system will be more flexible if it is able to learn new categories using the feedback of a human user. The User Interaction module provides a graphical menu to facilitate the collection of supervised object experiences and to instruct the robot to perform a task. In the case of supervised object experiences, two alternative interactions with an instructor are supported: gesture recognition or the usage of a graphical menu interface. In the first case, the instructor points to an object and then selects the desired label from a menu. In the second case the instructor can select the category label for an object based on its TrackID. Further details on supervised object experience gathering are available in [37]. An example of object labelling is depicted in Fig. 5 . The instructor puts a ‘Vase’ on the table. Tracking is initialized with TrackID 1. The gray bounding box signals the pose of the object as estimated by the tracker. TrackID 1 is classified as ‘Unknown’ because vases are not yet known to the system; the instructor points at TrackID 1. The system recognizes the pointing gesture and the corresponding menu is activated. The instructor labels the object as ‘Vase’. The Object Conceptualizer (category learning) module is activated when the instructor provides a category label for the object. In addition, the User Interaction module provides a menu to request the robot to perform a task or to abort the current task. 6.2 Object conceptualizer Learning methods used in most of the classical object recognition systems are not designed for open-ended domain, since those methods do not support an incremental update of the internal robot’s knowledge based on new experiences. On the contrary, open-ended learning approaches can incrementally update the acquired knowledge (category models) and extend the set of categories over time, which is suitable for real-world scenarios. For example, if the robot does not know how a ‘Mug’ looks like, it may ask the user to show one. Such situation provides an opportunity to collect training instances from actual experiences of the robot and the system can incrementally update it’s knowledge rather than retraining from scratch when a new instance is added or a new category is defined. In this section, we propose an open-ended 3D object category learning approach, which considers category learning as a process of updating a probabilistic model for each object category using the Naive Bayes approach. There are two reasons why Bayesian learning is useful for open-ended learning. One of them is the computational efficiency of the Naive Bayes approach. In fact, this model can be easily updated when new information is available, rather than retrained from scratch. Second, instance-based open-ended systems have continuously growing memory since they are constantly storing new object view representations (instances). Therefore, these systems must resort to experience management methodologies to discard some instances and thus prevent the accumulation of a too large set of experiences. In Bayesian learning, new experiences are used to update category models and then the experiences are forgotten immediately. The category model encodes the information collected so far. Therefore, this approach consumes a much smaller amount of memory when compared to any instance-based approach. The probabilistic category model requires calculating the likelihoods of the object given the category k, P ( O | C k ) , and it is also parametrized by the prior probabilities P(Ck ). It should be noted that the parameters of the likelihood are the probabilities of each visual word given the object category P ( w t | C ) . In this work, we consider the probability of each visual word occurring in the object independently, regardless of any possible correlations with the other visual words (Naive Bayes approach). The P ( C k ) P ( O | C k ) is equivalent to the joint probability model P ( C k , w 1 , ⋯ , w n ) = P ( C k ) P ( w 1 , ⋯ , w n | C k ) . The joint model can be rewritten using conditional independence assumptions: (5) P ( C k | w 1 , ⋯ , w n ) ∝ P ( C k , w 1 , ⋯ , w n ) ∝ p ( C k ) P ( w 1 | C k ) P ( w 2 | C k ) ⋯ P ( w n | C k ) ∝ P ( C k ) ∏ i = 1 n P ( w i | C k ) , where n is the size of the dictionary and P ( w i | C k ) is the probability of the visual word w i occurring in an object of category k. (6) P ( w i | C k ) = s i k + 1 ∑ j = 1 n ( s j k + 1 ) , where sik is the number of times that word w i was seen in objects from category Ck . Note, the probabilities are estimated with Laplace smoothing, by adding one to every counter, in order to prevent P ( w i | C k ) = 0 . On each newly seen object of this category with xi features of type w i , the following update is carried out: (7) s i k ← s i k + x i . The prior probability of category Ck is estimated as follows: (8) P ( C k ) = N k N , where N is the total number of seen objects of all categories and Nk is the number of seen objects from category k. 6.3 Object category recognition The last step in object perception is object category recognition. To classify an object O, which is represented as a histogram of occurrences of visual words h = [ h 1 h 2 . . . h n ] , the posterior probability for each object category is approximated using the Bayes theorem as: (9) P ( C k | O ) = P ( C k | h ) = P ( h | C k ) P ( C k ) P ( h ) ≈ P ( h | C k ) P ( C k ) . Because the denominator does not depend on Ck , and the values of the features are given as a histogram of occurrences of visual words, the denominator is constant. Eq. 9 is re-expressed based on Eq. 5 and multinomial distribution assumption: (10) P ( h | C k ) P ( C k ) ≈ P ( C k ) ∏ i = 1 n P ( w i | C k ) h i , In addition, to avoid underflow problems, the logarithm of the likelihood is computed: (11) ≈ log P ( C k ) + ∑ i = 1 n h i log P ( w i | C k ) , The category of the target object O is the one with highest likelihood: (12) C a t e g o r y ( O ) = argmax C k ∈ C P ( C k | O ) . 7 Planning and execution Fig. 6 shows a schematic representation of the planning and execution framework. In this framework, task planning is triggered when a user instructs the robot to achieve a task (e.x. clear_table). This is handled by the User Interaction module. The current state of the system, including world model information, global characteristics of the object of interest (i.e. overall shape, main axis, center of bounding box) and robot pose is retrieved from the working memory. Then, a task plan would be generated. A plan is a sequence of primitive operators to be performed to achieve the given goal. It should be noted that Task Planning is not in the scope of this paper. Previously, we showed how to conceptualize successfully executed task plans and how to use these conceptualized experiences for task planning [18]. In the present work, a predefined task plan is used. In order to be executed, a task plan must be complemented with end-effector poses. A pose is represented as a tuple G = (x , y , z , roll , pitch , yaw), specified relative to the base reference frame of the robot. The Grasp Planning module receives the task plan and chooses a grasp point either from above or from the side as well as a pre-grasp pose using the world model information and global characteristics of the object. In the current setup, the pre-grasp pose is placed at a fixed distance ( d p r e − g r a s p = 0.15 m ) behind or above the center of bounding box of the object. The intuition behind this assumption is that many domestic objects are graspable by aligning grippers with the principal axes of the object [1,27]. In another work, we proposed an advanced grasping approach to learn how to grasp familiar objects using interactive object view labeling and kinesthetic grasp teaching [26]. Afterwards, the Execution Manager retrieves the plan and grasp information from the Working Memory. The Execution Manager uses a Fine State Machine to reactively execute the plan. The actions are dispatched to the Robot Capabilities module. Inverse kinematics and safe controller, integrated from the JACO arm driver 7 7 , are used to transform a given end-effector pose goal into joint-space goals. Whenever the object is grasped, the height of the robot’s end-effector relative to the robot’s base is recorded into Working Memory and it is used as the desired height for placing the grasped object. The Execution Manager computes a new trajectory to navigate the robot’s end-effector to the placing area and sends out the action. After executing each action, the current state of the robot is updated in the Working Memory. Since world model information is updated by different modules (i.e. Object Detection, Execution Manager and etc.), the Execution Manager can abort execution when an unpredictable situation happens along expected execution path such as new obstacles move into the planned path of the robot arm. It should be noted that an orientation constraint on the end-effector is used to grasp and move an object parallel to the support plane. In addition, objects outside of the arm’s workspace are not considered. Fig. 7 illustrates the result of a constrained pick and place plan executed on the robot. 8 Experimental results Three types of experiments were performed to evaluate the proposed approach. First, an off-line quantitative evaluation for the object recognition system is presented (Section 8.1). Second, in Section 8.2, a “simulated teacher” was developed to assess the performance and scalability of the proposed object perception system. Finally, a qualitative analysis of the complete interactive open-ended object recognition system is shown in the context of a real-life use case (Section 8.3). In this case, a seven-minute demonstration session is described, where a user interacts with the system by teaching several objects to the robot and instructing the robot to perform a “clear_table” task. 8.1 Off-line evaluation of the perceptual learning approach An object dataset has been acquired for off-line evaluations, which contains 339 views of 10 categories of objects [11]. The system has four different parameters that must be tunned to provide a good balance between recognition performance, memory usage and computation time. To examine the accuracy of different configurations of the proposed approach, 10-fold cross validation was carried out. A total of 120 experiments were performed for different values of the four system parameters namely the voxel size (VS), which is related to number of keypoints extracted from each object view, the dictionary size (DS), the image width (IW) and support length (SL) of spin images. Results are presented in Table 2 . The object recognition performance for each system configuration is depicted in Fig. 8 where the system parameters are represented as a tuple (VS, DS, IW, SL). The parameters that obtained the best average accuracy were selected as the default system parameters. They are the following: VS = 0.01, DS = 90, IW = 4 and SL = 0.05. The accuracy of the system with the default parameters was 79%. Results show that the overall performance of the recognition system is promising. Spin images are capable of collecting distinctive traits of the local surface patches of each object. The results presented in Sections 8.3 and 8.2 are computed using this configuration. 8.2 Open-ended evaluation The off-line evaluation methodologies are not well suited to evaluate open-ended learning systems, because they do not abide to the simultaneous nature of learning and recognition and also those methodologies imply that the set of categories must be predefined. Therefore, an open-ended teaching protocol [9,11,40] is adopted in this evaluation. A simulated teacher was developed to assess the performance and scalability of the proposed object perception system by following the teaching protocol. The simulated teacher autonomously interacts with the learning system using teach, ask and correct actions. For each newly taught category, the simulated teacher repeatedly picks unseen object views of the currently known categories from a dataset and presents them to the system for checking whether the system can recognize them. The simulated teacher also provides corrective feedback in case of misclassification. Experiments were run on the largest publicly available 3D object dataset namely Washington RGB-D Object Dataset consisting of 250,000 views of 300 common household objects [41]. In the experiments that will be presented, the system begins with zero knowledge and the training instances become gradually available according to the teaching protocol. Therefore, the system learns new object categories as well as incrementally updates the existing object category models. Average Protocol Accuracy (APA) is computed using a sliding window of size 3n, where n is the number of categories that have already been introduced. If the number of iterations k, since the last time a new category was introduced, is less than 3n, all results are used. APA is used to determine if a new category can be taught. According to the protocol, the system is ready to learn a new object category when APA is higher than a certain threshold (marked by the horizontal line in Fig. 9 ), and at least one instance of every known category has been tested (k ≥ n). When an experiment is carried out, learning performance is evaluated using several measures, including: • The number of learned categories at the end of an experiment (LC), an indicator of How much does it learn?. • The number of question / correction iterations (QCI) required to learn those categories and the average number of stored instances per category (AIC), indicators of time and memory resources required for learning; i.e. How fast does it learn? • Global classification accuracy (GCA), computed using all predictions in a complete experiment, and the Average Protocol Accuracy (APA), indicators of How well does it learn?. Since the order of introduction of new categories may have an effect on the performance of the system, ten experiments were carried out in which categories were introduced in random sequences. Fig. 9 (top) shows the performance of the system in the initial 200 iterations of the first experiment. The introduced categories are signaled by vertical red lines and category labels in the plot. In the additional nine experiments, these categories were used again with different introduction sequences, the results of which are reported in Table 3 . By comparing all experiments, it is visible that in the third experiment, the system learned all categories faster than other experiments. In the case of experiment 9, the number of iterations required to learn 49 object categories was greater than other experiments. The underlying reason for different performances of these experiments is that categories were introduced to the system in a different order, which has a significant influence on the evolution of the learning performance. Figs. 9(top) and 10 show the evolution of the teaching protocol accuracy in experiments 1, 3, 5, 7 and 9. Fig. 9 (bottom) shows the protocol accuracy as a function of the number of learned categories. Fig. 11 (left) shows the global classification accuracy (i.e. the accuracy since the beginning of the experiment) as a function of the number of learned categories. In this figure we can see that the global classification accuracy decreases as more categories are learned. This is expected since the number of categories known by the system makes the classification task more difficult. To cope with this issue, memory management mechanisms [42], including salience and forgetting, can be considered. Finally, Fig. 11 (right) shows the number of learned categories as a function of the protocol iterations. This gives a measure of how fast the learning occurred in each of the experiments. 8.3 A real life use-case: clear table In this section, we present and discuss a “Clear Table” use-case to show all the functionalities of the system. In this use-case, the system works in a scenario where a table is in front of the robot, and a user interacts with the system. In this task, the robot must be able to detect and recognize different objects and transport all objects except decorative table-top objects (e.g., Vase) to predefined areas. The experimental setup is shown in Fig. 12 . It consists of a computer for human–robot interactions, a Kinect sensor for perceiving users and environment and a JACO robotic arm. The JACO arm has six degrees of freedom and a three fingers gripper. Since the JACO arm can carry up to 1.5 kg 8 8 , it is ideal for manipulating everyday objects. Moreover, infinite rotation around the wrist joints allows for flexible and effective interaction in a domestic environment. At the beginning of the session, there is a Vase object on top of the table. Later, a user places three more objects including Bottle, CoffeeJug and PlasticCup on the table. Note that, at the start of the experiment, the set of categories known to the system is empty and therefore, the system recognizes all table-top objects as Unknown (see Fig. 13 left). Afterwards, the user labels TrackID1 as a Vase. The system conceptualizes the Vase category and the category of TrackID1 is correctly recognized. Similarly, the user teaches all the other objects to the robot by providing the respective category labels. As depicted in Fig. 13 (center), the system could recognize all objects properly. Afterwards, the user instructs the robot to perform a clear_table task (i.e. puts the table back into a clear state). While there are active objects on the table, the robot retrieves the world model information from the Working Memory, including label and position of all active objects. The robot then selects the object closer to the arm’s base and clears it from the table (see Fig. 14 ). As it is shown in Fig. 13 (right), whenever the robot grasps an object, the shape of the object is partially changed and therefore a misclassification might happen. This real life use-case shows that the developed system is capable of detecting new objects, tracking and recognizing them, as well as manipulating objects in various positions. In other words, it shows the important role of robust object recognition and manipulation in performing tasks in human environments. Moreover, it shows how human–robot interaction is currently supported. A video of this session is available online at: 9 Conclusions In this paper, we have presented a cognitive architecture designed to support a tight coupling between perception and manipulation for assistive robots. In particular, an interactive open-ended learning approach for grounding 3D object categories has been presented, which enables robots to adapt to different environments and reason out how to behave in response to the request of a complex task such as clear_table. Unsupervised object exploration is used to construct a feature dictionary based on which objects are represented and object categories are learned. A Bayesian approach to category learning is proposed. We have assumed that the set of object categories to be learned is not known in advance and the training instances are extracted from actual experiences of a robot rather than being available at the beginning of the learning process. The proposed approach starts with the construction of a local 3D shape dictionary (visual words); each object is represented as a histogram of visual words and then the system creates or updates the probabilistic object category models based on Bayesian learning. For recognition, a probabilistic classification rule was used to assign a category label to the detected object. Results showed that the system can incrementally learn new object categories and perform manipulation tasks in reasonable time and appropriate manner. We have also tried to make the proposed architecture easy to integrate on other robotic systems. Our approach to object perception has been successfully tested on a JACO arm, showing the importance of having a tight coupling between perception and manipulation. In the continuation of this work, we are investigating the possibility of improving performance by topic modelling based on Latent Dirichlet Allocation (LDA) and also using other 3D shape descriptors (e.g. GOOD [43] and VFH [44]). Some results obtained with LDA have already been published [45]. Moreover, we would like to integrate compliance into the arm to provide comfortable interaction with the arm. Acknowledgments This work was funded by the EC 7th FP theme FP7-ICT-2011-7, grant agreement no. 287752 (project RACE - Robustness by Autonomous Competence Enhancement) and by National Funds through FCT project PEst-OE/EEI/UI0127/2014 and FCT scholarship SFRH/BD/94183/2013. We would like to thank the other RACE project partners for their efforts in the integration and the demonstrations, and especially to the Knowledge-Based Systems Group, Institute of Computer Science, University of Osnabruck for providing the ROS bag used for dictionary building in the exploration stage. References [1] M. Ciocarlie K. Hsiao E.G. Jones S. Chitta R.B. Rusu I.A. Şucan Towards reliable grasping and manipulation in household environments Experimental Robotics 2014 Springer 241 252 [2] Kim D.-J. R. Hazlett-Knudsen H. Culver-Godfrey G. Rucks T. Cunningham D. Portee J. Bricout Wang Z. A. Behal How autonomy impacts performance and satisfaction: results from a study with spinal cord injured subjects using an assistive robot IEEE Trans. Syst., Man Cybern., Part A: Syst. Hum. 42 1 2012 2 14 [3] A. Jain C.C. Kemp El-e: an assistive mobile manipulator that autonomously fetches objects from flat surfaces Auton. Robots 28 1 2010 45 64 [4] C. Leroux O. Lebec M.B. Ghezala Y. Mezouar L. Devillers C. Chastagnol J.-C. Martin V. Leynaert C. Fattal Armen: Assistive robotics to maintain elderly people in natural environment IRBM 34 2 2013 101 107 [5] M. Beetz U. Klank I. Kresse A. Maldonado L. Mosenlechner D. Pangercic T. Ruhr M. Tenorth Robotic roommates making pancakes 11th IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2011 2011 IEEE 529 536 [6] N. Vahrenkamp M. Do T. Asfour R. Dillmann Integrated grasp and motion planning 2010 IEEE International Conference on Robotics and Automation (ICRA) 2010 IEEE 2883 2888 [7] S. Jeong M. Lee Adaptive object recognition model using incremental feature representation and hierarchical classification Neural Netw. 25 0 2012 130 140 [8] L. Smith M. Gasser The development of embodied cognition: six lessons from babies Artif. Life 11 1–2 2005 13 29 [9] A. Chauhan L. Seabra Lopes Using spoken words to guide open-ended category formation Cogn. Process. 12 4 2011 341 354 [10] He H. Chen S. Imorl: Incremental multiple-object recognition and localization IEEE Trans. Neural Netw. 19 10 2008 1727 1738 [11] S.H. Kasaei M. Oliveira Lim G.H. L. Seabra Lopes A.M. Tomé Interactive open-ended learning for 3D object recognition: an approach and experiments J. Intell. Robot. Syst. 80 3 2015 537 553 [12] M. Oliveira Lim G.H. L. Seabra Lopes H. Kasaei A. Tomé A. Chauhan A perceptual memory system for grounding semantic representations in intelligent service robots Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2014 IEEE 2216 2223 [13] M. Oliveira L. Seabra Lopes Lim G.H. H. Kasaei A. Sappa A. Tomé Concurrent learning of visual codebooks and object categories in open-ended domains Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2015 IEEE 2488 2495 [14] M. Oliveira L. Seabra Lopes Lim G.H. S.H. Kasaei A.M. Tomé A. Chauhan 3D object perception and perceptual learning in the RACE project Robot. Auton. Syst. 75, Part B 2016 614 626 [15] S. Srinivasa D.I. Ferguson M. Vande Weghe R. Diankov D. Berenson C. Helfrich H. Strasdat The robotic busboy: steps towards developing a mobile robotic home assistant International Conference on Intelligent Autonomous Systems 2008 2155 2162 [16] J. Hertzberg Zhang J. Zhang L. S. Rockel B. Neumann J. Lehmann K. Dubba A. Cohn A. Saffiotti F. Pecora M. Mansouri Š. Konĕcný M. Günther S. Stock L. Seabra Lopes M. Oliveira Lim G. H. Kasaei V. Mokhtari L. Hotz W. Bohlken The race project KI - Künstliche Intelligenz 28 4 2014 297 304 [17] S. Rockel et al. An ontology-based multi-level robot architecture for learning from experiences Designing Intelligent Robots: Reintegrating AI II, AAAI Spring Symposium, Stanford (USA) 2013 52 57 [18] V. Mokhtari L. Seabra Lopes A.J. Pinho Experience-based robot task learning and planning with goal inference Twenty-Sixth International Conference on Automated Planning and Scheduling 2016 509 517 [19] S.S. Srinivasa D. Ferguson C.J. Helfrich D. Berenson A. Collet R. Diankov G. Gallagher G. Hollinger J. Kuffner M.V. Weghe Herb: a home exploring robotic butler Auton. Robots 28 1 2010 5 20 [20] J. Bohg A. Morales T. Asfour D. Kragic Data-driven grasp synthesis-a survey IEEE Trans. Robot. 30 2 2014 289 309 [21] A. Sahbani S. El-Khoury P. Bidaud An overview of 3D object grasp synthesis algorithms Robot. Auton. Syst. 60 3 2012 326 336 [22] E. Chinellato A.P. Del Pobil The neuroscience of vision-based grasping: a functional review for computational modeling and bio-inspired robotics J. Integr. Neurosci. 8 02 2009 223 254 [23] S. Monaco A. Sedda C. Cavina-Pratesi J.C. Culham Neural correlates of object size and object location during grasping actions Eur. J. Neurosci. 41 4 2015 454 465 [24] U. Castiello The neuroscience of grasping Nat. Rev. Neurosci. 6 9 2005 726 736 [25] J.C. Culham C. Cavina-Pratesi A. Singhal The role of parietal cortex in visuomotor control: what have we learned from neuroimaging? Neuropsychologia 44 13 2006 2668 2684 [26] N. Shafii S.H. Kasaei L. Seabra Lopes Learning to grasp familiar objects using object view recognition and template matching 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2016 IEEE 2895 2900 [27] J. Stückler R. Steffens D. Holz S. Behnke Efficient 3D object perception and grasp planning for mobile manipulation in domestic environments Robot. Auton. Syst. 61 10 2013 1106 1115 [28] A. Aldoma Z.-C. Marton F. Tombari W. Wohlkinger C. Potthast B. Zeisl R.B. Rusu S. Gedikli M. Vincze Point cloud library: three-dimensional object recognition and 6 DoF pose estimation IEEE Robot. Autom. Mag. 19 3 2012 80 91 [29] M. Martinez Torres A. Collet Romea S. Srinivasa Moped: a scalable and low latency object recognition and pose estimation system IEEE International Conference on Robotics and Automation, (ICRA 2010) 2010 2043 2049 [30] M. Islam F. Jahan J.-H. Min J. hwan Baek Object classification based on visual and extended features for video surveillance application 8th Asian Control Conference (ASCC 2011) 2011 1398 1401 [31] Yeh T. Lee J.J. T. Darrell Fast concurrent object localization and recognition IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009 2009 IEEE 280 287 [32] Yeh T. T. Darrell Dynamic visual category learning IEEE Conference on Computer Vision and Pattern Recognition, (CVPR 2008) 2008 1 8 [33] S. Kirstein H. Wersing H.-M. Gross E. Körner A life-long learning vector quantization approach for interactive learning of multiple categories Neural Netw. 28 2012 90 105 [34] A. Collet Xiong B. C. Gurau M. Hebert S.S. Srinivasa Herbdisc: towards lifelong robotic object discovery Int. J. Robot. Res. 34 1 2015 3 25 [35] M. Quigley K. Conley B. Gerkey J. Faust T. Foote J. Leibs R. Wheeler A.Y. Ng ROS: an open-source robot operating system ICRA Workshop on Open Source Software vol. 3 2009 5 11 [36] S. Hamidreza Kasaei M. Oliveira Lim G.H. L. Seabra Lopes A. Tomé An interactive open-ended learning approach for 3D object recognition 2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC) 2014 47 52 [37] Lim G.H. M. Oliveira V. Mokhtari S. Hamidreza Kasaei A. Chauhan L. Seabra Lopes A. Tomé Interactive teaching and experience extraction for learning about objects and robot activities The 23rd IEEE International Symposium on Robot and Human Interactive Communication, 2014 RO-MAN 2014 153 160 [38] M.A. Fischler R.C. Bolles Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography Commun. ACM 24 6 1981 381 395 [39] J.A. Hartigan Wong M.A. Algorithm AS 136: A k-means clustering algorithm J. Royal Stat. Soc. Series C (Appl. Stat.) 28 1 1979 100 108 [40] L. Seabra Lopes A. Chauhan How many words can my robot learn?: an approach and experiments with one-class learning Interact. Stud. 8 1 2007 53 81 [41] Lai K. Bo L. X. Ren D. Fox A large-scale hierarchical multi-view rgb-d object dataset 2011 IEEE International Conference on Robotics and Automation (ICRA) 2011 1817 1824 [42] A.D. Baddeley Human Memory: Theory and Practice 1997 Psychology Press [43] S.H. Kasaei A.M. Tomé L. Seabra Lopes M. Oliveira Good: a global orthographic object descriptor for 3d object recognition and manipulation Pattern Recognit. Lett. 83 2016 312 320 [44] R.B. Rusu G. Bradski R. Thibaux Hsu J. Fast 3d recognition and pose using the viewpoint feature histogram 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2010 IEEE 2155 2162 [45] S.H. Kasaei A.M. Tomé L. Seabra Lopes Hierarchical object representation for open-ended object category learning and recognition Advances in Neural Information Processing Systems (NIPS) 2016 1948 1956 Hamidreza Kasaei is a Ph.D. student at the University of Porto (MAP-i), Portugal. Currently, he is a researcher at the IEETA, University of Aveiro, Portugal, where he works on 3D object category learning and recognition in open-ended domains. His main research interests focus on the intersection of robotics, machine learning, and machine vision. He is interested in developing algorithms for an adaptive perception system based on interactive environment exploration and open-ended learning, which enables robots to learn from past experiences and interact with human users. He investigates active perception, where robots use their mobility and manipulation capabilities not only to gain the most useful perceptual information to model the world, also to predict the next best view for improving object detection and manipulation performances. Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Gi Hyun Lim received the B.S. degree in metallurgical engineering and the M.S. and Ph.D degrees in electronics and computer engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a post-doctoral researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal. His research interests lie in the area of intelligence and learning for robots, including perception and semantics. Luís Seabra Lopes is Associate Professor of Informatics in the Department of Electronics, Telecommunications and Informatics of the University of Aveiro, Portugal. He received a PhD in Robotics and Integrated Manufacturing from the New University of Lisbon, Portugal, in 1998. Lus Seabra Lopes has long standing interests in robot learning, cognitive robotic architectures, and human-robot interaction. Ana Maria Tomé is an Associate Professor of electrical engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as machine learning applications. "
    },
    {
        "doc_title": "Non-negative sub-tensor ensemble factorization (NsTEF) algorithm. A new incremental tensor factorization for large data sets.",
        "doc_scopus_id": "85030835061",
        "doc_doi": "10.1016/j.sigpro.2017.09.012",
        "doc_eid": "2-s2.0-85030835061",
        "doc_date": "2018-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Candecomp/parafac decompositions",
            "Incremental algorithm",
            "Learning methods",
            "Matrix factorizations",
            "Non-negative tensor decompositions"
        ],
        "doc_abstract": "© 2017 Elsevier B.V.In this work we present a novel algorithm for nonnegative tensor factorization (NTF). Standard NTF algorithms are very restricted in the size of tensors that can be decomposed. Our algorithm overcomes this size restriction by interpreting the tensor as a set of sub-tensors and by proceeding the decomposition of sub-tensor by sub-tensor. This approach requires only one sub-tensor at once to be available in memory.",
        "available": true,
        "clean_text": "serial JL 271605 291210 291718 291874 31 Signal Processing SIGNALPROCESSING 2017-09-22 2017-09-22 2017-10-11 2017-10-11 2019-11-28T12:29:24 S0165-1684(17)30334-1 S0165168417303341 10.1016/j.sigpro.2017.09.012 S300 S300.2 FULL-TEXT 2019-11-29T04:12:05.823358Z 0 0 20180301 20180331 2018 2017-09-22T16:25:09.23401Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid highlightsabst primabst ref 0165-1684 01651684 true 144 144 C Volume 144 8 77 86 77 86 201803 March 2018 2018-03-01 2018-03-31 2018 Regular Articles article fla © 2017 Elsevier B.V. All rights reserved. NONNEGATIVESUBTENSORENSEMBLEFACTORIZATIONNSTEFALGORITHMANEWINCREMENTALTENSORFACTORIZATIONFORLARGEDATASETS VIGNERON V 1 Introduction Notation 2 Tensor models 2.1 CANDECOMP/PARAFAC (CP) model 2.2 Tucker model 2.3 Advantages of tensor models 2.4 Non-negative factorization in the tensor domain 3 A fast NTF algorithm with additional non-negativity constraints 3.1 The principle 3.2 Update of the matrix factors 3.3 Implementation 3.4 Factor normalization 4 Numerical experiments with two image databases 4.1 Discussion 5 Algorithm cost 6 Conclusion Acknowledgment References BERRY 2007 155 173 M BOUSSE 2016 M PROC24THEUROPEANSIGNALPROCESSINGCONFERENCE ATENSORBASEDMETHODFORLARGESCALEBLINDSYSTEMIDENTIFICATIONUSINGSEGMENTATION BRO 1998 R MULTIWAYANALYSISINFOODINDUSTRYMODELSALGORITHMSAPPLICATIONS BUCAK 2009 788 797 S CARROLL 1970 283 319 J CHEN 2014 J CICHOCKI 2006 548 562 A ARTIFICIALINTELLIGENCESOFTCOMPUTINGICAISC2006VOLUME4029LECTURENOTESINCOMPUTERSCIENCE EXTENDEDSMARTALGORITHMSFORNONNEGATIVEMATRIXFACTORIZATION CICHOCKI 2014 145 163 A CICHOCKI 2006 32 39 A INDEPENDENTCOMPONENTANALYSISBLINDSIGNALSEPARATION CSISZARSDIVERGENCESFORNONNEGATIVEMATRIXFACTORIZATIONFAMILYNEWALGORITHMS CICHOCKI 2007 169 176 A INDEPENDENTCOMPONENTANALYSISSIGNALSEPARATION HIERARCHICALALSALGORITHMSFORNONNEGATIVEMATRIX3DTENSORFACTORIZATION CICHOCKI 2009 A NONNEGATIVEMATRIXTENSORFACTORIZATIONSAPPLICATIONSEXPLORATORYMULTIWAYDATAANALYSISBLINDSOURCESEPARATION CICHOCKI 2009 1 14 A COLINEAU 2008 1050 1061 J ADVANCEDCONCEPTSFORINTELLIGENTVISIONSYSTEMS 3DFACERECOGNITIONEVALUATIONEXPRESSIVEFACESUSINGIV2DATABASE COMON 1996 93 108 P COMON 2009 2997 3007 P COMON 2010 P HANDBOOKBLINDSOURCESEPARATIONINDEPENDENTCOMPONENTANALYSISAPPLICATIONS DELATHAUWER 2006 642 666 L DELATHAUWER 2004 295 327 L DESILVA 2008 1084 1127 V FRIEDLANDER 2008 631 647 M GUILLAMET 2002 116 119 D PROCEEDINGS16THINTERNATIONALCONFERENCEPATTERNRECOGNITIONVOLUME2 ANALYZINGNONNEGATIVEMATRIXFACTORIZATIONFORIMAGECLASSIFICATION GUO 2012 111 129 X HARSHMAN 1970 1 84 R HARSHMAN 1972 111 117 R HAZAN 2005 T INTERNATIONALCONFERENCEMACHINELEARNINGBONNGERMANY NONNEGATIVETENSORFACTORIZATIONAPPLICATIONSSTATISTICSCOMPUTERVISION HAZAN 2005 50 57 T TENTHIEEEINTERNATIONALCONFERENCECOMPUTERVISIONICCV05 SPARSEIMAGECODINGUSINGA3DNONNEGATIVETENSORFACTORIZATION HITCHCOCK 1927 164 189 F HOYER 2002 557 565 P PROCEEDINGSIEEEWORKSHOPNEURALNETWORKSFORSIGNALPROCESSING NONNEGATIVESPARSECODING HOYER 2004 1457 1469 P HOYER 2004 1457 1469 P HU 2015 956 964 L ILLAN 2009 I ANALISISENCOMPONENTESDEIMAGENESFUNCIONALESPARALAAYUDAALDIAGNASTICODELAENFERMEDADDEALZHEIMER ILLAN 2011 903 916 I KIM 2011 3261 3281 J KIM 2008 1829 1832 Y PROCEEDINGSINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSING NONNEGATIVETUCKERDECOMPOSITIONALPHADIVERGENCE KODEWITZ 2013 42 56 A KRUSKAL 1977 111 117 J LATHAUWER 1997 L SIGNALPROCESSINGBASEDMULTILINEARALGEBRA LAWSON 1995 C SOLVINGLEASTSQUARESPROBLEMS LEE 1999 788 791 D LEE 2001 556 562 D ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS13 ALGORITHMSFORNONNEGATIVEMATRIXFACTORIZATION LEE 2007 305 317 H LI 2001 207 212 S PROCIEEECONFCOMPUTERVISIONPATTERNRECOGNITIONCVPRHAWAIIUSA LEARNINGSPATIALLYLOCALIZEDPARTSBASEDREPRESENTATION LIM 2009 432 441 L LIM 2014 1260 1280 L LIN 2007 C LIN 2007 2756 2779 C MORUP 2008 2112 2131 M NDACOSTA 2016 M PROC24THEUROPEANSIGNALPROCESSINGCONFERENCE RANDOMIZEDMETHODSFORHIGHERORDERSUBSPACESPERATION PAATERO 1994 111 126 P PAUCA 2006 29 47 V QI 2016 2170 2183 Y RAIMONDI 2016 75 88 F ROYER 2011 2159 2171 J SCHACHTNER 2014 251 256 R STEGEMAN 2010 2498 2516 A STEGEMAN 2007 540 552 A TENBERGE 2002 399 409 J TUCKER 1966 279 311 L VEGANZONES 2016 2577 2588 M WELLING 2001 1255 1261 M YOKOTA 2015 234 249 T YONGMIN 2004 1509 1518 L ZDUNEK 2012 226 235 R PROCJOINT34THDAGM36THOAGMSYMPOSIUMPATTERNRECOGNITIONDAGMOAGM TRUSTREGIONALGORITHMFORNONNEGATIVEMATRIXFACTORIZATIONALPHABETADIVERGENCES ZHANG 2005 350 363 D INPROCICCV05WORKSHOPANALYSISMODELINGFACESGESTURESAMFG05 TWODIMENSIONALNONNEGATIVEMATRIXFACTORIZATIONFORFACEREPRESENTATIONRECOGNITION ZHOU 2012 2928 2940 G ZHOU 2014 54 65 G VIGNERONX2018X77 VIGNERONX2018X77X86 VIGNERONX2018X77XV VIGNERONX2018X77X86XV 2019-10-11T00:00:00.000Z © 2017 Elsevier B.V. All rights reserved. 2019-03-02T00:41:20.583Z S0165168417303341 FAPESP 2014/23936-4 Fundação do Estado de São Paulo This work was supported by Fundação do Estado de São Paulo (FAPESP), Brazil, under Grant 2014/23936-4 . item S0165-1684(17)30334-1 S0165168417303341 10.1016/j.sigpro.2017.09.012 271605 2019-11-29T04:12:05.823358Z 2018-03-01 2018-03-31 true 1090528 MAIN 10 52726 849 656 IMAGE-WEB-PDF 1 gr1 5071 82 219 gr2 20530 153 219 gr3 7868 164 190 gr4 10749 108 219 gr5 9772 64 219 gr6 7574 143 219 gr1 12338 147 393 gr2 48317 380 543 gr3 21858 328 381 gr4 36590 266 539 gr5 18620 152 523 gr6 14219 200 306 gr1 70106 653 1741 gr2 365279 1684 2404 gr3 154714 1454 1688 gr4 220450 1180 2389 gr5 154063 672 2315 gr6 123498 886 1355 si1 285 13 58 si10 421 17 103 si100 427 19 103 si101 1550 38 286 si102 1282 38 204 si103 1599 72 307 si104 273 25 30 si105 308 25 32 si106 1635 28 393 si107 3335 79 451 si108 954 61 147 si109 176 20 15 si11 270 17 50 si110 275 22 32 si111 247 22 36 si112 905 24 262 si113 494 23 105 si114 823 16 280 si115 1700 55 345 si116 231 17 28 si117 479 22 94 si118 233 22 30 si119 259 16 55 si12 314 17 63 si120 1196 19 339 si121 514 15 148 si122 255 15 47 si123 1171 56 189 si124 1703 56 302 si125 694 16 161 si126 985 16 238 si127 369 16 74 si128 400 16 78 si129 419 16 68 si13 469 21 104 si130 327 16 49 si14 477 21 104 si15 454 21 98 si16 490 19 108 si17 710 27 201 si18 764 27 196 si19 753 27 196 si2 1983 41 417 si20 562 18 145 si21 401 16 82 si22 790 15 292 si23 2115 57 532 si24 894 18 268 si25 161 13 17 si26 162 15 13 si27 2963 91 498 si28 242 20 27 si29 475 15 98 si3 2288 51 627 si30 481 16 135 si31 145 13 15 si32 163 15 15 si33 1947 52 418 si34 508 18 126 si35 2960 94 500 si36 2240 55 478 si37 257 22 27 si38 488 15 103 si39 428 16 120 si4 318 17 69 si40 434 16 98 si41 505 18 111 si42 729 19 212 si43 382 16 73 si44 449 21 94 si45 754 19 214 si46 307 13 64 si47 240 13 51 si48 1655 34 375 si49 1932 46 420 si5 280 16 67 si50 3142 84 481 si51 175 16 19 si52 179 16 22 si53 603 23 147 si54 293 16 62 si55 818 24 211 si56 673 15 233 si57 502 23 100 si58 216 15 52 si59 678 26 159 si6 244 16 61 si60 494 17 126 si61 237 17 35 si62 2514 72 447 si63 268 25 27 si64 516 23 98 si65 780 26 158 si66 1992 28 503 si67 600 24 115 si68 401 16 124 si69 288 25 34 si7 252 16 59 si70 415 16 117 si71 2690 89 343 si72 925 25 202 si73 539 23 109 si74 809 25 174 si75 248 13 48 si76 541 17 162 si77 3099 124 414 si78 557 23 123 si79 187 13 45 si8 162 15 17 si80 2227 57 382 si81 541 23 115 si82 4530 164 412 si83 387 16 111 si84 1188 30 261 si85 1200 26 255 si86 1145 26 260 si87 1211 38 236 si88 3135 111 358 si89 301 16 98 si9 291 12 67 si90 284 25 30 si91 247 22 27 si92 1563 45 254 si93 360 26 45 si94 368 28 45 si95 866 28 177 si96 629 28 124 si97 218 15 50 si98 1814 63 304 si99 1042 28 244 SIGPRO 6604 S0165-1684(17)30334-1 10.1016/j.sigpro.2017.09.012 Elsevier B.V. Fig. 1 Scheme of the sub-tensor decomposition for a third-order input tensor as described in (12). Fig. 1 Fig. 2 Reconstruction after decomposition into 32 basis images; relative error is given as error  ±  standard deviation. Computation time raising from left to right. Fig. 2 Fig. 3 Evolution of relative error over time for several NTF algorithms when decomposing in third-order CBCL face image tensor. Basis images R = 32 . Time in seconds. Fig. 3 Fig. 4 32 basis faces obtained by nonnegative sub-tensor ensemble factorization (NsTEF) decomposing 2429 CBCL face images. Images corresponding to nose (3rd row, 7th column), mouth (1st row, 4th column), chin (3rd row, 1st column), cheeks (3rd row, 5th column), eyes, eye-brows or combinations can be found. Others are harder to interpret. Fig. 4 Fig. 5 Reconstruction after decomposition into 32 basis images. Fig. 5 Algorithm 1 NsTEF algorithm. Algorithm 1 Non-negative sub-tensor ensemble factorization (NsTEF) algorithm. A new incremental tensor factorization for large data sets. Vincent Vigneron ⁎ a Andreas Kodewitz a Michele Nazareth da Costa b Ana Maria Tome c Elmar Langlang d a IBISC, Univ Evry, Université Paris-Saclay, 91025, Evry, France IBISC Univ Evry, Université Paris-Saclay 91025, Evry France b DSPCom Laboratory, University of Campinas (UNICAMP), PO Box 6101, 13083-852, Campinas/SP, Brazil DSPCom Laboratory University of Campinas (UNICAMP) PO Box 6101, 13083-852, Campinas/SP Brazil c Departamento de Electronica, Telecomunicacoes e Informática, Universidade de Aveiro, Portugal Departamento de Electronica, Telecomunicacoes e Informática Universidade de Aveiro Portugal d Institut für Biophysik und physikalische Biochemie, University of Regensburg, Universitätsstrasse 31, D-93040 Regensburg, Germany Institut für Biophysik und physikalische Biochemie University of Regensburg Universitätsstrasse 31, D-93040 Regensburg Germany ⁎ Corresponding author. In this work we present a novel algorithm for nonnegative tensor factorization (NTF). Standard NTF algorithms are very restricted in the size of tensors that can be decomposed. Our algorithm overcomes this size restriction by interpreting the tensor as a set of sub-tensors and by proceeding the decomposition of sub-tensor by sub-tensor. This approach requires only one sub-tensor at once to be available in memory. Keywords Non-negative tensor decomposition CANDECOMP/PARAFAC Decomposition Matrix factorization NTF Incremental algorithm Learning method 1 Introduction Since the pioneering works [42,52], non-negative matrix factorization (NMF) has attracted much interest in the context of several applications such as image and signal processing, computer vision, data analysis, blind source separation [12,22,27,30,32,42,45,53,69]. Particularly in image processing, the associated constraints are desirable to retain the non-negative characteristics of the original data, since the pixel values of the basis images essentially share this feature, leading to a natural meaning regarding the underlying components. As a result, we can, for instance, better represent a face as a linear combination of basis images by NMF in contrast with classical methods such as principal component analysis (PCA) [42,45]. Furthermore, NMF can be viewed as an implicit sparse representation of the input data [27,30,42], which allows representing local features of distributed parts over a human face such as eyes, nose and mouth, and, consequently, learning features of images in face recognition applications. Broadly speaking, a subspace representation by non-negative factorizations makes possible to determinate hidden structures and characteristics inherent to an object class of the input data set, which is helpful in object recognition, detection of semantic features of text documents and of spectral characteristics of hyperspectral images, among others [1,12,22,27,42,46,53,62,69]. Tensorial approaches naturally arise from multilinear structures or multidimensional data, and NMF has been extended to higher-order tensors by the non-negative tensor factorizations (NTFs). The NTF was firstly introduced [6] by imposing non-negative constraints over the matrix factors of the well-known decomposition called CANDECOMP/PARAFAC (or, shortly CP) [5,24]. Analogously, a non-negative version of the Tucker decomposition [61] has also been presented and is referred to here as non-negative Tucker decomposition (NTD) [3], representing a more complex model, as the core tensor could be dense and the matrix factors not necessarily have the same number of columns. An interesting advantage of the NTF/NTD is that, in general, tensor decompositions are essentially unique under mild conditions, as opposed to NMF. More precisely, the uniqueness issue associated with the NTD/NTF happens when the factors are not sufficiently sparse [70]. Almost all NMF algorithms can be generalized or extended to non-negative tensor factorizations by the use of unfolding matrices of the higher-order tensor or by a multi-layer strategy (multi-factor model) [12,17]. A very popular multiplicative update (MU) method [42] can be derived, regarding gradient descent methods, by solving the following optimization problem (1) ( A * , B * ) = arg max A , B f ( A , B ) = arg max A , B 1 2 ∥ Y − A B ∥ F 2 . The MU rule with a simple projection to the non-negative space at each updating step is given by (2) b n , p ← b n , p [ A T Y A T A B ] n , p , + , a m , n ← a m , n [ Y B T A B B T ] m , n , + , which has a simple and easy implementation despite presenting slow convergence [49]. A basic approach to NMF, called alternating non-negative least squares (ANLS), is driven by alternating least squares (ALS) techniques [12] based on the alternating minimization of the cost function (1) with respect to the nonnegativity-constrained matrices A and B separately. However, it does not necessarily lead to global minimization. Several algorithms have been proposed based on the ANLS framework with the purpose of accelerating and overcoming the unstable convergence properties of the standard ANLS and, also, becoming more robust to noise [12] by including penalty terms on the cost function (1) to add supplementary or to preserve constraints on A and B as nonnegativity, sparsity and smoothness, leading to generalized NMF methods [11,29,30,53,64]. The hierarchical ALS (HALS) algorithm [11] is an alternative method to ALS based on an optimization of a set of local cost functions, which updates each column of A and B instead of directly computing the whole matrices at each iterative step. This method is simple and often used for multi-layer models to improve performance; furthermore, it is efficient for large-scale NMF [12,17]. Another fast algorithm in the ANLS framework, referred to as ANLS-BPP, was proposed in [35], and employs the block principal pivoting (BPP) and active set methods [41]. The ANLS-BPP technique can outperform the HALS mainly when the matrix factors are sparse. It is interesting to remark that the optimization problem given by (1) can also be formulated in terms of the Kullback–Leibler divergence [43,45] or other divergences [8,10,67] instead of the Frobenius norm. A problem that arises from the processing of large-scale or ill-conditioned data is the slow convergence, mainly for the MU methods, and the increase of computational complexity and memory requirements. An efficient way to reduce the complexity and to improve the performance of the NTF/NTD is to include a pre-processing step based on low-rank approximation techniques, as proposed in [69,70]. We present in this paper a novel algorithm for NTF and sparse NTF adapted to higher-order tensor decomposition with one large dimension. Algorithms for NTF presented in the past were often restricted in the size of tensors that can be decomposed. Algorithms designed to overcome this size restriction, for example based on block wise decomposition, require a frequent access to partitions of the whole data of the tensor. The presented algorithm in contrary requires only a minimum of data access and is even capable to start a decomposition before the whole tensor is known. In comparative tests the algorithm has proved to be competitive with state of the art algorithms. NsTEF is an incremental algorithm as incremental PCA [65] or INMF proposed by Bucak and Gunsel [4] but it deals with tensors, not with matrices. An important advantage of tensor decompositions over standard matrix approach is the model uniqueness; if it exists, is unique [9], which leads to an interesting benefit of our method. The rest of the paper is organized as follows: related works are presented in Section 2 where problems encountered using standard NTF algorithms are detailed; Section 3 introduces the proposed algorithm, named NsTEF; we present the experimental results in Section 4; finally, we conclude this paper in Section 6. Notation N-th order tensors (for N ≥ 3), matrices (second-order tensors), vectors (first-order tensors), and scalars (zero-order tensors) are respectively denoted by calligraphic ( A , B , … ), boldface upper-case ( A , B , … ), boldface lower-case ( a , b , … ), and lower-case letters ( a , b , … ). Each element of an N-order tensor A is denoted by a i 1 , i 2 , ⋯ , i N . A tensor A is called non-negative if all its elements are non-negative, i.e. a i 1 , i 2 , ⋯ , i N ≥ 0 . For non-negative real tensors we use the short hand notation A ≥ 0 and A ∈ R + . A i 1 · · ∈ R I 2 × I 3 , A · i 2 · ∈ R I 1 × I 3 , A · · i 3 ∈ R I 1 × I 2 represent the slices of a third-order tensor A constructed by fixing the mode 1, 2, and 3, respectively. Any higher-order tensor can be represented by matrix unfoldings from the rearrangement of its elements into a matrix from the matrix slicings by fixing one mode. Consider for example a third-order tensor A ∈ R I 1 × I 2 × I 3 , we can define three different matrix unfoldings: A I 1 × I 2 I 3 Δ = [ A · 1 · ⋯ A · I 2 · ] , A I 2 × I 3 I 1 Δ = [ A · · 1 T ⋯ A · · I 3 T ] and A I 3 × I 1 I 2 Δ = [ A 1 · · T ⋯ A I 1 · · T ] to represent the same tensor A . By convention, the indexes placed more to the left vary slower and the ones placed more to the right vary faster. The Kronecker, Khatri–Rao, Hadamard and outer products are denoted by ⊗,⋄, • and ○ respectively. The trace of A is denoted by Tr(A). Definition 1 The n-mode product of a tensor G ∈ R I 1 × ⋯ × I n × ⋯ × I N and a matrix A ∈ R J n × I n is an ( I 1 × ⋯ × I n − 1 × J n × I n + 1 × ⋯ × I N )-tensor given by (3) [ G × n A ] i 1 , ⋯ , i n − 1 , j n , i n + 1 , ⋯ , i N Δ = ∑ i n = 1 I n g i 1 , ⋯ , i n , ⋯ , i N a j n , i n , for all index values. The n-mode product is a compact form to represent linear transformations involving tensors and (3) can be rewritten in terms of matrix unfoldings by fixing the n-th mode as follows (4) X = G × n A ⇔ X ( n ) = A G ( n ) , where X (n) and G (n) denote the matrix unfolding of X and G associated with the n-th mode. 2 Tensor models Tensor decompositions were first discussed in 1927 by Hitchcock [28]. In the late 1960s tensor decompositions were rediscovered by Tucker [61], Carroll and Chang [5], and Harshman [24] respectively named Tucker decomposition, canonical decomposition (CANDECOMP), and parallel factors analysis (PARAFAC). The two last models, referred to herein as CP, were independently developed in psychometrics and phonetics, however both correspond to the same decomposition and the names report to different features of this model. Tucker model is a general version of the well-known CP model and was also applied in psychometrics. A particular case of this decomposition can be viewed as a multilinear generalization of the singular value decomposition (SVD) for higher-order tensors later introduced by Lathauwer [40]. Tensor decompositions appear today in various fields including image and signal processing, clustering analysis, data compression, blind source separation, direction of arrival estimation, hyperspectral imaging and others [2,51,55,62]. 2.1 CANDECOMP/PARAFAC (CP) model The CP model decomposes a tensor as a minimal sum of rank-one tensors, which can be defined in a concise form and denoted by a sum of outer products of vectors or by the n-mode product according to (5) Y Δ = [ [ A ( 1 ) , A ( 2 ) , … , A ( N ) ] ] ∈ R I 1 × I 2 × ⋯ × I N = ∑ r = 1 R a · r ( 1 ) ∘ a · r ( 2 ) ∘ ⋯ ∘ a · r ( N ) = I × 1 A ( 1 ) × 2 A ( 2 ) ⋯ × N A ( N ) , where a · r ( n ) denotes the r-th column of A ( n ) ∈ R I n × R for all n ∈ { 1 , 2 , … , N } and the tensor core I is the identity tensor with ones on the superdiagonal and zeros elsewhere. Despite the usual definition of tensor rank [38] being a generalization of the definition of matrix rank, the best rank-R approximation problem is ill-posed [20] and can only be achieved in some special cases [15,16,19]. From the matrix representation in (4), the N-th order tensor Y in (5) can be rewritten regarding the matrix unfolding associated with the n-th mode and using the Khatri–Rao product as (6) Y ( n ) = A ( n ) ( A ( N ) ⋄ ⋯ ⋄ A ( n + 1 ) ⋄ A ( n − 1 ) ⋄ ⋯ ⋄ A ( 1 ) ) T ∈ R I n × I N … I n + 1 I n − 1 … I 1 . 2.2 Tucker model In contrast to CP, Tucker decomposition incorporates interacting dimensions and does not require the same number of columns for the factor matrices { A ( 1 ) , … , A ( N ) } . It can be expressed analogously to (5) and (6) as follows (7) Y Δ = [ [ G ; A ( 1 ) , … , A ( N ) ] ] ∈ R I 1 × ⋯ × I N = ∑ r 1 = 1 R 1 ⋯ ∑ r N = 1 R N g r 1 , … , r N a · r 1 ( 1 ) ∘ ⋯ ∘ a · r N ( N ) = G × 1 A ( 1 ) ⋯ × N A ( N ) and (8) Y ( n ) = A ( n ) G ( n ) ( A ( N ) ⊗ ⋯ ⊗ A ( n + 1 ) ⊗ A ( n − 1 ) ⊗ ⋯ ⊗ A ( 1 ) ) T ∈ R I n × I N … I n + 1 I n − 1 … I 1 , where a · r n ( n ) denotes the rn -th column of A ( n ) ∈ R I n × R n for all n ∈ { 1 , … , N } , and G (n) represents a matrix unfolding of G . Notice that the core tensor I ∈ R R × ⋯ × R in (5) is replaced by a general core tensor G ∈ R R 1 × ⋯ × R N . This allows every interaction between the factor matrices. 2.3 Advantages of tensor models To understand the motivation for a tensor decomposition model let us first recall the linear mixing model widely-used in independent component analysis (ICA) [17], a successful spin-off of PCA. The task of ICA is to learn the basis (mixing) matrix A = [ a 1 a 2 … a J ] ∈ R m × J and the encoding variable matrix S ∈ R J × p which minimizes the Frobenious norm ∥ Y − A S ∥ F 2 , given a data matrix Y = [ y 1 y 2 … y p ] ∈ R m × p . The linear data model X = A S is learned such that row vectors of the encoding variable matrix are as statistically independent as possible. This model is adapted to one-dimensional signals, e.g. a time series signal. But if we intend to search for underlying sources of a two-dimensional image, a three-dimensional brain scan or a set of images or of measurements for this kind of data model is no longer directly applicable. To make the data fit into the model, a common practice is to vectorize the measured data, which means to concatenate the data into a vector. This vectorization is justified by the assumption that the x i are independent which might not be strictly true. Consider for example a human face image, the calculation of Eigen-faces is a quite common application of ICA, PCA, and other subspace analysis techniques, or scan brain images acquired by positron emission tomography (PET). By evidence the pixels of the images are related to their neighboring pixels, thus the pixels are not independent as required to justify vectorization. Nevertheless, many real data applications show good results using vectorization on face decomposition, brain imaging and other applications [33,34,37,66]. Literature provides to our knowledge no insight why the vectorization approach is successful even with the pixels being dependent. In a tensor model, on the contrary, it is possible to maintain the data in its natural shape [9]. The vectorization is not necessary and in the case of image data, pixel neighborhoods remain intact as in a tensorial representation since we can associate one index with each dimension or measurement. In the case of a series of images, for example, we can reserve two indices for the image dimensions and one index for the different images which naturally leads to a third-order tensor. A time-series of PET for different patients would form a fifth-order tensor. In addition, there is also an interesting advantage of tensor decompositions over the standard matrix approach: the decomposition, if it exists, is unique [9]. The uniqueness issues for the CP model have been extensively investigated [18,20,23,25,38,39,47,58–60], but the most general sufficient condition and well-known result on uniqueness is attributed to Kruskal [38]. Lim and Comon [47] have recently recovered the uniqueness results given in [38] providing a more practical solution in terms of the coherence measure instead of Kruskal rank, which was employed in array processing context. Standard NTF algorithms, like presented in [13,35,56], usually require considerable time to decompose a tensor, especially if the tensor to be decomposed presents a large size. The processing time especially rises if the dimensions of the input tensor are imbalanced, i.e. one dimension of the tensor is much larger than the other dimensions. 2.4 Non-negative factorization in the tensor domain non-negative CP decomposition, also known as NTF, is a generalization of NMF to higher-order tensors. A nonnegativity constraint is incorporated to the CP model in (5), i.e. the input tensor is decomposed into a sum of outer products of non-negative vectors, which leads to a constrained CP model [70]. In several application problems [26,27,36,62,63], this non-negativity constraint facilitates the interpretation of the decomposition of input data as well as all components are constrained to be non-negative. The decomposition is therefore strictly additive and a graphical display of non-negative data is more convenient. But even more important, the nonnegativity constraint guarantees the existence of a rank reducing solution to the non-negative CP approximation problem as proven by Lim and Comon in [46] and for this reason, the minimum value of R in (5) is called non-negative rank. This proof holds as well for the matrix case (second-order, i.e. N = 2 ) as for the higher-order tensor case (i.e. for N ≥ 3). Additionally, a recent result [54] establishes that a best non-negative rank-R approximation is almost always unique. Analogously to NTF, NTD adds to the Tucker model, given in (7), a nonnegativity constraint. Implementations of NTF as direct multi-way generalization to the Lee and Seung NMF algorithm were presented by Kim and Choi [36], Lee et al. [44], Mørup et al. [50], Welling and Weber [63]. These implementations use the same MU rule and the Euclidean norm or Kulback-Leibler divergence as cost-function as Lee and Seung. More specifically, an alternating minimization of the set of nonnegativity constrained least squares (ANLS) [12] can be expressed in terms of the Euclidean norm as (9) A ( n ) = arg min A ( n ) ∥ Y ( n ) − A ( n ) Z ( n ) ∥ 2 2 s.t. A ( n ) ≥ 0 or, in case of the Kulback–Leibler divergence [43,45], as (10) A ( n ) = arg min A ( n ) ∑ i 1 , … , i N y i 1 , … , i N log y i 1 , … , i N x i 1 , … , i N s.t. A ( n ) ≥ 0 where (11) X ( n ) = A ( n ) Z ( n ) , Z ( n ) = G ( n ) ( A ( N ) ⊗ … ⊗ A ( n + 1 ) ⊗ A ( n − 1 ) ⊗ … ⊗ A ( 1 ) ) T ⟺ X = G × 1 A ( 1 ) … × N A ( N ) ∈ R I 1 × I 2 × … × I N and Y (n), X (n), and G (n) denoting the n-th matrix unfolding of Y , X , and G respectively, is performed using the MU rule proposed by Lee and Seung [43]. Several other approaches were proposed to perform NTF: gradient-based descent [27,49], fixed point ALS and alternating interior-point gradient [44], ALS [21], HALS [11,13], pre-conditioned nonlinear conjugate gradient [56] and a block principal pivoting method (ANLS-BPP) [35]. 3 A fast NTF algorithm with additional non-negativity constraints 3.1 The principle In the search for faster NTF algorithms, various optimization strategies have been pursued. In this paper we consider the non-negative CP model (see Section 2.4). Then we develop MU algorithms for learning a non-negative CP decomposition of a non-negative input tensor. The MU algorithms is iteratively applied to a matrix representation of the input tensor associated with each mode and then solve the related NMF problem. In the tensor decomposition for very large-scale problems, memory becomes a major factor. Block wise processing of the data is a common approach to implement parallel processing, but it has the drawback that the blocks of data have to be accessed multiple times. This creates a memory overhead and we want to avoid with our novel algorithm designed for the decomposition of large-scale tensors with imbalanced dimensions. Consider an N-th order tensor, Y ∈ R + I 1 × ⋯ × I n × ⋯ × I N and an ensemble of ( N − 1 ) -th order sub-tensors, defined as Y ( i n ) ∈ R + I 1 × ⋯ × I n − 1 × I n + 1 × ⋯ × I N and constructed by fixing the index associated with the n-th mode. Our proposal is to approach the tensor decomposition for large-scale problems by decomposing each sub-tensor at a time and by proceeding until to obtain a decomposition for the entire set of sub-tensors. We introduce our algorithm, named as non-negative sub-tensor ensemble factorization (NsTEF), and especially derived by considering the input tensor with unbalanced dimensions, which means that one dimension is much larger than the others i.e. I n ≫ I 1 , … , I n − 1 , I n + 1 , … , I N . The proposed algorithm aims to circumvent the memory problems encountered with standard NTF algorithms. For simplicity, we consider a third-order input tensor Y ∈ R + I 1 × I 2 × I 3 assuming that the third mode is the largest dimension, i.e. I 3 ≫ I 1, I 2, and the identity tensor with unitary elements, i.e. λ r = 1 . In this way, we treat the input tensor from a set of its matrix slicings Y ( i 3 ) Δ = Y · · i 3 ∈ R + I 1 × I 2 i.e. { Y ( 1 ) , … , Y ( I 3 ) } . Each matrix Y ( i 3 ) is decomposed as the non-negative CP model obtaining a set of cost-functions given by (12) D ( Y ( i 3 ) , Y ^ ( i 3 ) ) = ∥ Y ( i 3 ) − [ [ A ( 1 ) , A ( 2 ) , a i 3 · ( 3 ) ] ] ∥ F 2 s.t. A ( 1 ) , A ( 2 ) , a i 3 · ( 3 ) ≥ 0 , where a i 3 · ( 3 ) denotes the i 3-th row of A ( 3 ) ∈ R + I 3 × R . Fig. 1 depicts this procedure associated with each row i 3. The optimization of each cost function in (12) will lead to the optimization of ∑ i 3 = 1 I 3 D ( Y ( i 3 ) , Y ^ ( i 3 ) ) i.e. (13) D ( Y , Y ^ ) = ∥ Y − [ [ A ( 1 ) , A ( 2 ) , A ( 3 ) ] ] ∥ F 2 s.t. A ( 1 ) , A ( 2 ) , A ( 3 ) ≥ 0 . We perform the optimization of the set of cost-functions D ( Y ( i 3 ) , Y ^ ( i 3 ) ) , for i 3 = { 1 , … , I 3 } , by converting the nonlinear problem (13) into three independent linear least squares problems. So we can update each row of the third factor, a i 3 · ( 3 ) , and both matrix factors A (1) and A (2) in an alternating way. These updating steps are repeated until all sub-tensors are treated, i.e. for all i 3 ∈ { 1 , … , I 3 } . Our proposed algorithm is depicted in Algorithm 1. In the following, we will describe the details of each of these steps. 3.2 Update of the matrix factors In the first step, each row of the third factor a i 3 · ( 3 ) is updated by fixing the matrix factors A (1) and A (2). From the matrix representation (6), the optimization problem takes the form (17) a i 3 · ( 3 ) = arg min a i 3 · ( 3 ) 1 2 ∥ y i 3 · ( 3 ) − a i 3 · ( 3 ) ( A ( 2 ) ⋄ A ( 1 ) ) T ∥ F 2 + λ ∥ a i 3 · ( 3 ) ∥ 1 s.t. a i 3 · ( 3 ) ≥ 0 , where y i 3 · ( 3 ) = vec ( Y ( i 3 ) ) ∈ R + 1 × I 2 I 1 denotes the i 3-th row of the matrix unfolding of Y associated with the third mode i.e. Y ( 3 ) ∈ R + I 3 × I 2 I 1 . The expression (17) can be optimized by various methods but, for sake of simplicity, we will implement this step using the MU rule given in [43]. Using the regularization term λ ∥ a i 3 · ( 3 ) ∥ 1 = λ ∑ r | a i 3 , r ( 3 ) | in (17), a sparse coding is enforced by λ, which λ ∈ R is the regularization parameter controlling the amount of sparseness. This means that a solution with many small or zero entries is favored over a dense solution. The sparse coding vector, a i 3 · ( 3 ) , will influence the update of the basis factor, A (1) and A (2), insofar as the changes in the factor matrices will be concentrated on the part of the factors that was relevant in the coding of the corresponding example. So far we have only updated a i 3 · ( 3 ) , it remains to update the basis factors A (1) and A (2). In order to perform these updates, knowing only one sub-tensor Y ( i 3 ) at a time, we will load the information from the already considered sub-tensors. As both matrices A (1) and A (2) are treated in exactly the same way, we will derive the method to store this information on the example of A (1). Assuming that we store the information of the already treated sub-tensors, we are able to calculate the cost-function for the decomposition of the set of sub-tensors { Y ( i 3 ) ; i 3 = 1 , … , t } , where t is associated with the t-th slice of Y being treated in the current moment. Thus, the cost-function of the decomposition can be written as (18) D ( Y , Y ^ ) I 3 ≈ 1 2 t ∑ i 3 = 1 t D ( Y ( i 3 ) , Y ^ ( i 3 ) ) = 1 2 t ∑ i 3 = 1 t ∥ Y ( i 3 ) − [ [ A ( 1 ) , A ( 2 ) , a i 3 · ( 3 ) ] ] ∥ F 2 . A direct calculation of the sum in this cost function at each iteration for the known { a 1 · ( 3 ) , … , a t · ( 3 ) } , A (1) and A (2) would be too expensive to create a fast algorithm. Therefore let us rewrite (18) using (6) with n = 1 as (19) D ( Y , Y ^ ) I 3 ≈ 1 2 t ∑ i 3 = 1 t ∥ Y ( i 3 ) − A ( 1 ) ( a i 3 · ( 3 ) ⋄ A ( 2 ) ) T ∥ F 2 . From (19) and by fixing { a 1 · ( 3 ) , … , a t · ( 3 ) } and A (2), we can obtain an estimate of A (1) (20) A ( 1 ) = arg min A ( 1 ) 1 2 t ∑ i 3 = 1 t [ Tr ( Y ( i 3 ) Y ( i 3 ) T ) − − 2 Tr ( A ( 1 ) ( a i 3 · ( 3 ) ⋄ A ( 2 ) ) T Y ( i 3 ) T ) + Tr ( A ( 1 ) ( a i 3 · ( 3 ) ⋄ A ( 2 ) ) T ( a i 3 · ( 3 ) ⋄ A ( 2 ) ) A ( 1 ) T ) ] for t ∈ { 1 , … , I 3 } . After rewriting the optimization problem in this way, we can now replace the sum over i 3 in (20) by the following recursion relations (21a) Γ ( t ) ( 1 ) = Γ ( t − 1 ) ( 1 ) + Z ( t ) ( 1 ) Z ( t ) ( 1 ) T ∈ R + R × R (21b) Ω ( t ) ( 1 ) = Ω ( t − 1 ) ( 1 ) + Z ( t ) ( 1 ) Y ( t ) T ∈ R + R × I 1 (21c) Θ ( t ) ( 1 ) = Θ ( t − 1 ) ( 1 ) + Y ( t ) Y ( t ) T ∈ R + I 1 × I 1 with (22) Z ( t ) ( 1 ) = ( a t · ( 3 ) ⋄ A ( 2 ) ) T ∈ R + R × I 2 . Thus the optimization problem given by (20) is simplified to the form (23) A ( 1 ) = arg min A ( 1 ) 1 2 t [ Tr ( Θ ( t ) ( 1 ) ) − 2 Tr ( A ( 1 ) Ω ( t ) ( 1 ) ) + Tr ( A ( 1 ) Γ ( t ) ( 1 ) A ( 1 ) T ) ] . s.t. A ( 1 ) ≥ 0 Updating them at each step for t = 1 , … , I 3 we can save the information of the past sub-tensors and avoid the calculation of the sum in (20). However the recursive estimation of these variables (21a)–(21b) leads to an approximation to the cost-function since Z ( t ) ( 1 ) depends on the knowledge of A (2) and a t · ( 3 ) . 3.3 Implementation To implement our algorithm, we use a MU for positivity preserving as proposed by Lee and Seung [43] with additional sparseness constraint and factor normalization. We chose this kind of implementation for its simplicity in order to obtain quickly a proof-of-concept of our approach. Considering a cost function C( θ ) of non-negative variables θi , the MU has the form (24) θ i ← θ i • ( ∂ C − ( θ ) ∂ θ i / ∂ C + ( θ ) ∂ θ i ) , with ∂ C − ( θ ) ∂ θ i the positive and ∂ C ( θ ) + ∂ θ i the negative part of the derivative with respect to θi . The complete derivative of the cost function C( θ ) is ∂ C ( θ ) ∂ θ i = ∂ C ( θ ) + ∂ θ i + ∂ C ( θ ) − ∂ θ i . This MU rule can be explained as follows: In the case that the gradient is zero, i.e. ∂ C ( θ ) + ∂ θ i = ∂ C ( θ ) − ∂ θ i , θi remains unchanged. In the case that the gradient is positive the update rule will decrease the entries of θi and vice versa in the case of a negative gradient. For an interpretation in terms of a variational Bayes approach see [57]. From (17) with i 3 = t , the resulting update rule for the coding factors a t · ( 3 ) (the first step) is given by (25) a t · ( 3 ) ← a t · ( 3 ) • y t · ( 3 ) Z ( 3 ) T a t · ( 3 ) Z ( 3 ) Z ( 3 ) T + λ ∥ a t · ( 3 ) ∥ 1 , with (26) Z ( 3 ) = ( A ( 2 ) ⋄ A ( 1 ) ) T ∈ R + R × I 2 I 1 . The sparse non-negative updating of the coding vector has to respect a stopping criterion that is adjusted to the factors’ needed precision of the sparse coding vs. avoidance of over-fitting and time needed for the sparse coding. For simplicity, it was chosen to stop the updates when the change of the cost function is such that Δ error < 10 − 5 . This value was empirically obtained to perform well for various input tensors. The derivatives of (23) for n ∈ {1, 2} are (27) ∂ ∂ A ( n ) Tr ( A ( n ) Γ ( t ) ( n ) A ( n ) T ) = 2 A ( n ) Γ ( t ) ( n ) (28) ∂ ∂ A ( n ) Tr ( A ( n ) Ω ( t ) ( n ) ) = Ω ( t ) ( n ) and the update rule for the basis factor matrices A (n) results in (29) A ( n ) ← A ( n ) • Ω ( t ) ( n ) T A ( n ) Γ ( t ) ( n ) , for n ∈ { 1 , 2 } . The factor matrices A (1) and A (2) are randomly initialized with each entry in the range [0, 1]. The iterative variables Γ ( t ) ( n ) and Ω ( t ) ( n ) are initialized with 0. We also require all entries of the input tensor to be in the same range [0, 1]. 3.4 Factor normalization Applying a sparseness constraint to only one factor matrix but not all, the normalization of the remaining factors is essential to avoid growing of the non constrained factors. A cost function of the form (30) D ( Y , Y ^ ) = ∥ Y − [ [ A ( 1 ) , A ( 2 ) , A ( 3 ) ] ] ∥ F 2 + λ ∥ A ( 3 ) ∥ 1 could be decreased by simply shrinking the entries of A (3) and growing the factors A (1) and A (2) that do not obey the sparseness constraint. To avoid this effect different kinds of normalization can be applied to the remaining factors. Using a MU rule like Lee and Seung [43], a simple min-max normalization, i.e.re-normalization which maps the maximum value to 1 and the minimum value to 0, would create values that are exactly zero. These values would not change any more in consequence of the multiplicative nature of the update. Common solutions are normalization of all columns of the factors to a sum of one (31) A ( n ) ← A ( n ) • ( Ω ( t ) ( n ) T A ( n ) Γ ( t ) ( n ) ) • 1 ( 1 A ( n ) Γ ( t ) ( n ) • ( A ( n ) Γ ( t ) ( n ) Ω ( t ) ( n ) T ) ) These are the normalizing update rules to be applied in the coding step. 4 Numerical experiments with two image databases The usefulness of the NTF algorithm in the extraction of facial features for classification has been demonstrated in [26,50]. In [26] the NTF has been applied for feature extraction for face detection. The feature vector representing an image was the inner-product between the factors. Those measurement vectors over positive (faces) and negative (non-faces) examples were fed into various classifiers, such as Support Vector Machines (SVM) and Adaboost. The MIT CBCL face repository has been used and it has been shown that the features extracted by the NTF factors generated the higher classification accuracy when compared to NMF and PCA. To examine the convergence behavior of the algorithm and the achieved decomposition accuracy, we performed decompositions of a set of example tensors. In order to allow the comparison of decomposition errors achieved in decompositions of different tensors, we use the relative error given by (32) C rel. = ∥ Y − Y ^ ∥ F ∥ Y ∥ F , where Y ^ is the reconstruction tensor of Y from the estimated matrix factors A (1),A (2),and A (3). For comparison a selection of four algorithms will be employed: The original NMF algorithm by [43], the direct extension of the same algorithm to NTD by [50] i.e. higher-order nonnegative matrix factorization (HONMF), a conjugate gradient (cgP) algorithm [56] and an ANLS algorithm with BPP [35] (referred to as ANLS-BPP). All algorithms are used in a pure Matlab®implementation. NMF is a 2-way method, i.e. designed for second-order tensors (matrices), therefore the input tensors have to be vectorized. The preconditioned nonlinear cgP is an implementation specialized to third-order tensors, consequently a vectorization becomes necessary if the order of the input tensor exceeds 3. For the evaluation and comparison of these algorithms, we use two sets of face images: The MIT center for biological and computational learning (CBCL) face database and IV2 face database [14]. The MIT CBCL face database contains 2429 gray scale face images with a resolution of 19 × 19 pixels. Forming a tensor of dimensions 19 × 19 × 2429 it is a good example of an input tensor of moderate size and heavily imbalanced dimensions. The face images of the CBCL database present a typical decomposition task and have been used in the evaluation of several publications dealing with matrix and tensor factorizations [21,27,31,42]. As the data set contains several images of the same person we have to assure by shuffling their order that they are not grouped together in the data tensor. To also perform tests on a fourth order tensor we selected 211 persons with at least 6 images available and formed a fourth order tensor of dimensions 19 × 19 × 6 × 211,where the first two ways correspond to the pixels, the third indexes the different images of one person and the fourth way corresponds to the different persons. The IV2 face database contains 4721 face gray scale face images with a resolution of 128 × 128 pixels. These images form a tensor of dimensions 128 × 128 × 4721 which presents an example of much larger size. On the machine with 4 Gigabytes of RAM which we used to perform our experiments, this tensor uses up almost half of the available memory. In Fig. 2 we display examples of reconstructed CBCL face images, as well as processing time and relative error, obtained with our algorithm and the 4 algorithms to compare with. The cgP is only used with 3D arrays and HONMF is the tensor extension of NMF. Both visual inspection as well as the reconstruction error and the processing time show that the NMF algorithm is superior to all NTF algorithms. This is due to the fact that NMF is especially designed for 2D-way arrays. From all NTF algorithms, NsTEF performs best. Both HONMF and cgP show poorer reconstruction and a slower processing time. ANLS-BPP is adapted to 3 way arrays but it is very slow (10 times as much as NsTEF) and can not reasonably be used in its actual form in the case of very large data set such as in big data context. The three algorithms converges similarly fast toward nearly the same relative error level. In Fig. 3 we plotted the relative decomposition error over time. These plots show that NsTEF algorithm exhibits a good convergence behaviour, the latter having being recently analyzed in more details in, e.g. [7,48]. These plots also show that the stopping criterion of the ANLS-BPP algorithm has stopped quite late, and that the processing time is much longer than with NsTEF. In Fig. 4 we show the basis images obtained by the decomposition with the NsTEF algorithm. These images build the basis for the reconstruction. As can be seen, the basis images are sparse, i.e. a large number of entries is zero, because of the purely additive nature of non-negative decompositions. The obtained basis images show a similarity to the basis images obtained by other NTF and NMF algorithms, see [21,31,68]. The tensor decomposition formed from IV2 image database of dimensions 128 × 128 × 4721 is much more demanding. In fact, the HONMF and the cgP algorithms did not converge within 24 hours. However, the NMF, NsTEF and ANLS-BPP algorithms converged within 1 h. The exact processing time as well as the relative error achieved with each of the algorithms and the reconstructed images are shown in Fig. 5 . NMF achieved by far the best decomposition. The reconstruction error is almost one magnitude lower, and the reconstructed image shows much more similarity with the original than with both other algorithms. With almost identical processing time, NsTEF and ANLS-BPP have achieved a decomposition that lacks detail. Recognizing the person in the original image is still possible, and the reconstruction error is not higher than for the CBCL face images. 4.1 Discussion One shortcoming of our algorithm is that an approximation is necessary to derive the basis update step. The numeric examples showed that our algorithm is comparable with the state-of-the-art algorithms in terms of the error reconstruction and also the processing time. We ascribe this to the coding step which does not need an approximation and thus can equilibrate the committed error. Another way to approach large-scale decompositions are block wise approaches, e.g. Kim and Park [35] and Cichocki et al. [13, see. 1.3.1]. In these approaches parts of the input tensor are selected randomly to be processed. NsTEF, on the contrary, does process parts of the tensor in a regular way. One might suspect that this leads to a better representation of the information processed in later steps in the decomposition but as the past information is stored in the basis update step we did not observe any dependence of the reconstruction error on the location of the data in the input tensor whatsoever. Additionally, the NsTEF avoids the need to access the same part of the tensor multiple times, which considerably reduces memory overhead and contributes to the algorithm speed. Furthermore, block wise algorithms require that parts of the tensor be selected according to an uniform distribution in order to obtain a decomposition with uniform reconstruction error over the whole tensor. In consequence, additional precautions are necessary to be able to start the decomposition of a tensor before all entries of the tensor are known. With our NsTEF algorithm, it is possible to add new examples at any time, and the algorithm will incorporate the new information available. After finishing the presented implementation, we also investigated whether it is possible to transfer our decomposition approach directly to an implementation using the Kullback-Leibler divergence as cost criterion. The use of this divergence, however, did not allow us to derive the iterative variables Ω ( n ) t and Γ ( n ) t , which are essential to circumvent the complete re-calculation of the sum in the cost function (18) in every update. 5 Algorithm cost The approach to decompose an N-dimensional data tensor as a series of ( N − 1 ) -dimensional data tensors allows to keep less data in memory. Intending to decompose a set of tensors { Y ( t ) ∈ R + I 1 × … × I N − 1 , t = 1 , … , I N } which is equivalent to the decomposition of a tensor Y ∈ R + I 1 × ⋯ × I N with I N ≥ I N − 1 ≥ … ≥ I 1 ≥ R ≥ N ≥ 2 , where R is the reduced dimension, the proposed algorithm has a memory usage of (33) M = 2 ∏ i = 1 N − 1 I i + R ∏ i = 1 N − 2 I i + ∑ i = 1 N − 1 I i + ( N − 1 ) R 2 matrix entries. The first summand is the space required to keep the example Y ( t ) and its reconstruction in memory, the second for a Z ( n ) t , n ≠ N , the third for the Ω ( n ) t and the last for the Γ ( n ) t . In the case of a third-order tensor, i.e. N = 3 , this is: M = 2 R 2 + R ( I 1 I 2 + 2 I 1 + 2 I 2 + I 3 ) + 2 I 1 I 2 . The memory consumption of the NsTEF algorithm is quadratic in R. Supposing cubic tensors to be decomposed, i.e. I n = I m = I , ∀ n , m , we can also determine the memory consumption of the algorithm to be proportional to I ( N − 1 ) and exponential in N. The numerical complexity can help to compare the performance of different algorithms. For the proposed algorithm, the computational cost for the two steps are different. The coding step, i.e. for updating each row of A (3),has a computational cost of (34) O ( ( 2 R + 1 ) ∏ i = 1 N − 1 I i + R ) and the basis update steps, i.e. for updating A (1) and A (2), (35) O ( R ∑ i = 1 N − 1 I i + ( N − 1 ) ( 2 R + 1 ) ∏ i = 1 N − 1 I i ) . For a third-order tensor, this cost is: O ( ( 2 R + 1 ) I 1 I 2 + J ) for the coding step and O ( 2 ( 2 R + 1 ) I 1 I 2 + R ( I 1 + I 2 ) ) for the basis update step. The cost of the ordinary matrix product AB,with A ∈ R I × J and B ∈ R J × K is assumed to be O ( I J K ) , i.e. in the case of a dense matrix. The computational cost of the element-wise product A•B is O ( I J ) . The algorithmic cost of both basis update and coding step are linear in R. Supposing the tensor to decompose be cubic, the algorithmic cost of both steps is of the order I ( N − 1 ) in I and exponential in N. With this decomposition approach we have the possibility to start the decomposition of a tensor before all of its sub-tensors Y ( t ) are known. In cases where the whole tensor, which we want to decompose, is known from the beginning we can present the sub-tensors in arbitrary order. This simple change of the order makes the algorithm more robust to decomposition problems originating from a strong order in the sub-tensors, e.g. the sub-tensors belong to several different classes of examples. Our algorithm decomposes the data tensor along the largest dimension. By re-indexing the tensor we can decompose also along every other direction using our algorithm. In terms of computational cost and memory consumption it might be of no advantage to decompose along any other dimension but the largest one. There might be circumstances that make it necessary to choose this route. 6 Conclusion We have presented a novel approach for fast decomposition of large non-negative tensors with imbalanced dimensions. We avoided the slowdown due to excessive memory use encountered with standard algorithms by accessing parts of the tensor, sub-tensors in a sequential way and storing the information obtained by already accessed sub-tensors. Tests of the proposed algorithm on examples of moderate to large size data repositories demonstrated a highly competitive convergence speed and comparable decomposition quality of the NsTEF algorithm. Because of the special structure, tensor factorization always contains, implicitly or explicitly, a core tensor, which does not exist in matrix factorization. How to efficiently and effectively deal with it is one key problem in NTF. We can either fix it as an identity, or incorporate it into the optimization procedure. The latter one is computationally very expensive, which makes it unsuitable for large tensor data. Acknowledgment This work was supported by Fundação do Estado de São Paulo (FAPESP), Brazil, under Grant 2014/23936-4. References [1] M.W. Berry M. Browne A.N. Langville V.P. Pauca R.J. Plemmons Algorithms and applications for approximate nonnegative matrix factorization Comput. Stat. Data Anal. 52 1 2007 155 173 [2] M. Boussé O. Debals L. De Lathauwer A tensor-based method for large-scale blind system identification using segmentation Proc. 24th European Signal Processing Conference 2016 accepted [3] R. Bro Multi-way analysis in the food industry. Models, algorithms and applications. 1998 University of Amsterdam Netherlands Ph.D. thesis [4] S.S. Bucak B. Gunsel Incremental subspace learning via non-negative matrix factorization Pattern Recognit. 42 5 2009 788 797 [5] J.D. Carroll J.J. Chang Analysis of individual differences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition Psychometrika 35 1970 283 319 [6] J.D. Carroll, G. De Soete, S. Pruzansky, Multiway data analysis (1989) 463–472. [7] J. Chen C. Richard J.-C.M. Bermudezz P. Honeine Variants of non-negative least means square algorihtm and convergence analysis IEEE Trans. Signal Process. 62 15 2014 [8] A. Cichocki S. Amari R. Zdunek R. Kompass G. Hori Z. He Extended smart algorithms for nonnegative matrix factorization Artificial Intelligence and Soft Computing - ICAISC 2006, volume 4029 of Lecture Notes in Computer Science 2006 548 562 [9] A. Cichocki D. Mandic A.-H. Phan C. Caiafa G. Zhou Q. Zhao L.D. Lathauwer Tensor decompositions for signal processing applications from two-way to multiway component analysis IEEE Signal Process. Mag. 32 2 2014 145 163 [10] A. Cichocki R. Zdunek S.-I. Amari Csiszar’s divergences for nonnegative matrix factorization: family of new algorithms Independent Component Analysis and Blind Signal Separation Vol. 3889 2006 Springer Berlin / Heidelberg 32 39 [11] A. Cichocki R. Zdunek S.-I. Amari Hierarchical ALS algorithms for nonnegative matrix and 3D tensor factorization Independent Component Analysis and Signal Separation Vol. 4666 2007 Springer Berlin / Heidelberg 169 176 ICA 2007 [12] A. Cichocki R. Zdunek A.-H. Phan S. Amari Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation 2009 John Wiley & Sons, Ltd [13] A. Cichocki A.-H. Phan Fast local algorithms for large scale nonnegative matrix and tensor factorizations IEICE Trans. Fundam. Electron. Commun. Comput. Sci. E92-A 3 2009 1 14 [14] J. Colineau J. D’Hose B. Ben Amor M. Ardabilian L. Chen B. Dorizzi 3D face recognition evaluation on expressive faces using the IV 2 database J. Blanc-Talon S. Bourennane W. Philips D. Popescu P. Scheunders Advanced Concepts for Intelligent Vision Systems Lecture Notes in Computer Science Vol. 5259 2008 Springer Berlin Heidelberg 1050 1061 [15] P. Comon B. Mourrain Decomposition of quantics in sums of powers of linear forms Signal Process. 53 1996 93 108 Special Issue on Higher Order Statistics. [16] P. Comon J.M.F. Ten Berge L. de Lathauwer J. Castaing Generic and typical ranks of multi-way arrays Linear Algebra Appl. 430 11 2009 2997 3007 [17] P. Comon C. Jutten Handbook of Blind Source Separation: Independent Component Analysis and Applications 1st 2010 Academic Press [18] L. de Lathauwer A link between the canonical decomposition in multilinear algebra and simultaneous matrix diagonalization SIAM J. Matrix Analy. Appl. 28 3 2006 642 666 [19] L. de Lathauwer B. de Moor J. Vandewalle Computation of the canonical decomposition by means of a simultaneous generalized Schur decomposition SIAM J. Matrix Anal. Appl. 26 2 2004 295 327 [20] V. de Silva L. Lim Tensor rank and the ill-posedness of the best low-rank approximation problem SIAM J. Matrix Anal. Appl. 30 3 2008 1084 1127 [21] M. Friedlander K. Hatz Computing non-negative tensor factorizations Optim. Methods Softw. 23 4 2008 631 647 [22] D. Guillamet B. Schiele J. Vitriá Analyzing nonnegative Matrix factorization for image classification Proceedings of the 16th International Conference on Pattern Recognition, Volume 2 2002 IEEE Computer Society 116 119 [23] X. Guo S. Miron D. Brie A. Stegeman Uni-mode and partial uniquennes conditions for CANDECOMP/PARAFAC of three-way arrays with linearly dependent loadings. IEEE Trans. Signal Process. 33 1 2012 111 129 [24] R.A. Harshman Foundations of the PARAFAC procedure: models and conditions for an “explanatory” multi-modal factor analysis UCLA Work. Pap. Phonetics 16 1970 1 84 [25] R.A. Harshman Determination and proof of minimum uniqueness conditions for PARAFAC1 UCLA Work. Pap. Phonetics 22 1972 111 117 [26] T. Hazan S. Polak A. Shashua Non-negative tensor factorization with applications to statistics and computer vision International Conference of Machine Learning, Bonn, Germany 2005 [27] T. Hazan S. Polak A. Shashua Sparse image coding using a 3D nonnegative tensor factorization Tenth IEEE International Conference on Computer Vision (ICCV’05) Vol. 1 2005 50 57 [28] F.L. Hitchcock The expression of a tensor or a polyadic as a sum of products J. Math. Phys. 6 1 1927 164 189 [29] P.O. Hoyer Nonnegative sparse coding Proceedings of IEEE Workshop Neural networks for signal processing 2002 557 565 [30] P.O. Hoyer Nonnegative matrix factorization with sparseness constraints J. Mach. Learn. Res. 2004 1457 1469 [31] P.O. Hoyer P. Dayan Non-negative matrix factorization with sparseness constraints J. Mach. Learn. Res. 5 2004 1457 1469 [32] L.-Y. Hu G.-D. Guo C.-F. Ma Image processing using Newton-based algorithm of nonnegative matrix factorization Appl. Math. Comput. 269 2015 956 964 [33] I.A. Illán Análisis en Componentes de Imágenes Funcionales para la Ayuda al Diagnástico de la Enfermedad de Alzheimer 2009 Departamento de Arquitectura y Tecnología de Computadores Ph.D. thesis [34] I.A. Illán J.M. Górriz J. Ramírez D. Salas-Gonzalez M. López F. Segovia C.G. Puntonet M. Gómez-Rio 18F-FDG PET imaging for computer aided Alzheimer’s diagnosis 181 4 2011 903 916 [35] J. Kim H. Park Fast nonnegative matrix factorization: an active-set-like method and comparisons SIAM J. Sci. Comput. 33 6 2011 3261 3281 [36] Y.-D. Kim A. Cichocki S. Choi Nonnegative Tucker decomposition with alpha-divergence Proceedings of International Conference on Acoustics, Speech and Signal Processing 2008 1829 1832 [37] A. Kodewitz S. Lelandais C. Montagne V. Vigneron Learning and using brain maps for Alzheimer’s disease detection Electron. Lett. Comput. Vision Image Anal. 12 1 2013 42 56 [38] J.B. Kruskal Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics Linear Algebra Appl. 18 1977 111 117 [39] J.B. Kruskal, Multiway Data Analysis, vol. 33 of Biometrial Journal, chapter Rank, decomposition, and uniqueness for 3-way and N-way arrays, Elsevier Science Publishers B.V., North-Holland, 1989. [40] L.D. Lathauwer Signal processing based on multilinear algebra. 1997 Université Catholique de Leuven Netherlands Ph.D. thesis [41] C. Lawson R. Hanson Solving Least Squares Problems 1995 Society for Industrial and Applied Mathematics [42] D.D. Lee H.S. Seung Learning the parts of objects by nonnegative matrix factorization Nature 401 1999 788 791 [43] D.D. Lee H.S. Seung Algorithms for nonnegative matrix factorization T.K. Leen T.G. Dietterich V. Tresp Advances in Neural Information Processing Systems 13 2001 MIT Press 556 562 [44] H. Lee Y.-D. Kim A. Cichocki S. Choi Non-negative tensor factorization for continuous eeg classification Int. J. Neural Syst. 17 4 2007 305 317 [45] S.Z. Li X. Hou H. Zhang Q. Cheng Learning spatially localized, parts-based representation Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Hawaii, USA Vol. I 2001 207 212 [46] L.-H. Lim P. Comon Nonnegative approximations of nonnegative tensors J. Chemom. 23 7–8 2009 432 441 [47] L.-H. Lim P. Comon Blind multilinear identification IEEE Trans. Inf. Theory 60 2 2014 1260 1280 [48] C.-J. Lin On the convergence of multiplicative update algorithms for non negative matrix factorization IEEE Trans. on Neural Networks 16 6 2007 [49] C.-J. Lin Projected gradient methods for nonnegative matrix factorization Neural Comput. 19 10 2007 2756 2779 [50] M. Mørup L.K. Hansen S.M. Arnfred Algorithms for sparse non-negative Tucker decompositions Neural Comput. 20 8 2008 2112 2131 [51] M. N. da Costa R. R. Lopes J.M. T. Romano Randomized methods for higher-order subspace speration Proc. 24th European Signal Processing Conference 2016 accepted. [52] P. Paatero U. Tapper Positive matrix factorization: a nonnegative factor model with optimal utilization of error estimates of data values Environmetrics 5 2 1994 111 126 [53] V.P. Pauca J. Piper R.J. Plemmons Nonnegative matrix factorization for spectral data analysis Linear Algebra Appl. 416 1 2006 29 47 [54] Y. Qi P. Comon L.-H. Lim Uniqueness of nonnegative tensor approximations IEEE Trans. Inf. Theory 62 4 2016 2170 2183 [55] F. Raimondi P. Comon O. Michel S. Sahnoun A. Helmstetter Tensor decomposition exploiting diversity of propagation velocities; application to localization of icequake events Signal Process. 118 2016 75 88 [56] J.-P. Royer N. Thirion-Moreau P. Comon Computing the polyadic decomposition of nonnegative third order tensors Signal Process. 91 9 2011 2159 2171 [57] R. Schachtner G. Pöppel A.M. Tomé E.W. Lang A Bayesian approach to the Lee–Seung update rules for NMF Pattern Recognit. Lett. 45 2014 251 256 [58] A. Stegeman On uniqueness of the n-th order tensor decomposition into rank-1 terms with linear independence in one mode SIAM J. Matrix Anal. Appl. 31 5 2010 2498 2516 [59] A. Stegeman N.D. Sidiropoulos On Kruskal’s uniqueness condition for the CANDECOMP/PARAFAC decomposition Linear Algebra Appl. 420 2007 540 552 [60] J.M.F. Ten Berge N.D. Sidiropoulos On uniqueness in CANDECOMP/PARAFAC Psychometrika 67 3 2002 399 409 [61] L. Tucker Some mathematical notes of three-mode factor analysis Psychometrika 31 3 1966 279 311 [62] M.A. Veganzones J.E. Cohen R.C. Farias J. Chanussot P. Comon Nonnegative tensor CP decomposition of hyperspectral data IEEE Trans. Geosci. Remote Sens. 54 5 2016 2577 2588 [63] M. Welling M. Weber Positive tensor factorization Pattern Recognit. Lett. 22 2001 1255 1261 [64] T. Yokota R. Zdunek A. Cichocki Y. Yamashita Smooth non-negative matrix and tensor factorizations for robust multi-way data analysis Signal Process. 113 2015 234 249 [65] L. Yongmin On incremental and robust subspace learning Pattern Recognit. 37 2004 1509 1518 [66] S. Zafeiriou, Tensors in Image Processing and Computer Vision, Advances in Pattern Recognition, Springer, 2009, pp. 105–124. [67] R. Zdunek Trust-region algorithm for nonnegative matrix factorization with alpha- and beta-divergences Proc. Joint 34th DAGM and 36th OAGM Symposium on Pattern Recognition (DAGM/OAGM) Vol. 7476 2012 Springer Berlin / Heidelberg 226 235 [68] D. Zhang S. Chen Z.-H. Zhou Two-dimensional non-negative matrix factorization for face representation and recognition in Proc. ICCV’05 Workshop on Analysis and Modeling of Faces and Gestures (AMFG’05) Vol. 3723 2005 Springer 350 363 [69] G. Zhou A. Cichocki S. Xie Fast nonnegative matrix/tensor factorization based on low-Rank approximation IEEE Trans. Signal Process. 60 6 2012 2928 2940 [70] G. Zhou A. Cichocki Q. Zhao S. Xie Nonnegative matrix and tensor factorizations: an algorithmic perspective IEEE Signal Process. Mag. 31 3 2014 54 65 "
    },
    {
        "doc_title": "Perceiving, learning, and recognizing 3D objects: An approach to cognitive service robots",
        "doc_scopus_id": "85060470647",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85060470647",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Cognitive architectures",
            "Next best view",
            "Object categories",
            "Perception capability",
            "Robotic applications",
            "Service robots",
            "Training data",
            "Unknown objects"
        ],
        "doc_abstract": "Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.There is growing need for robots that can interact with people in everyday situations. For service robots, it is not reasonable to assume that one can pre-program all object categories. Instead, apart from learning from a batch of labelled training data, robots should continuously update and learn new object categories while working in the environment. This paper proposes a cognitive architecture designed to create a concurrent 3D object category learning and recognition in an interactive and open-ended manner. In particular, this cognitive architecture provides automatic perception capabilities that will allow robots to detect objects in highly crowded scenes and learn new object categories from the set of accumulated experiences in an incremental and open-ended way. Moreover, it supports constructing the full model of an unknown object in an on-line manner and predicting next best view for improving object detection and manipulation performance. We provide extensive experimental results demonstrating system performance in terms of recognition, scalability, next-best-view prediction and real-world robotic applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A multi-variate blind source separation algorithm",
        "doc_scopus_id": "85028328076",
        "doc_doi": "10.1016/j.cmpb.2017.08.019",
        "doc_eid": "2-s2.0-85028328076",
        "doc_date": "2017-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Algebraic approaches",
            "Decomposition approach",
            "fMRI",
            "Functional connectivity",
            "Resting state",
            "Retinotopy",
            "Singular value decomposition technique",
            "Spatio temporal",
            "Algorithms",
            "Brain",
            "Connectome",
            "Humans",
            "Image Processing, Computer-Assisted",
            "Magnetic Resonance Imaging"
        ],
        "doc_abstract": "© 2017 Elsevier B.V.Background and objective The study follows the proposal of decomposing a given data matrix into a product of independent spatial and temporal component matrices. A multi-variate decomposition approach is presented, based on an approximate diagonalization of a set of matrices computed using a latent space representation. Methods The proposed methodology follows an algebraic approach, which is common to space, temporal or spatiotemporal blind source separation algorithms. More specifically, the algebraic approach relies on singular value decomposition techniques, which avoids computationally costly and numerically instable matrix inversion. The method is equally applicable to correlation matrices determined from second order correlations or by considering fourth order correlations. Results The resulting algorithms are applied to fMRI data sets either to extract the underlying fMRI components or to extract connectivity maps from resting state fMRI data collected for a dynamic functional connectivity analysis. Intriguingly, our algorithm shows increased spatial specificity compared to common approaches, while temporal precision stays similar. Conclusion The study presents a novel spatiotemporal blind source separation algorithm, which is both robust and avoids parameters that are difficult to fine tune. Applied on experimental data sets, the new method yields highly confined and focused areas with least spatial extent in the retinotopy case, and similar results in the dynamic functional connectivity analyses compared to other blind source separation algorithms. Therefore, we conclude that our novel algorithm is highly competitive and yields results, which are superior or at least similar to existing approaches.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2017-08-31 2017-08-31 2017-09-01 2017-09-01 2017-09-22T18:26:12 S0169-2607(16)30609-5 S0169260716306095 10.1016/j.cmpb.2017.08.019 S300 S300.1 FULL-TEXT 2017-09-22T16:27:55.783865-04:00 0 0 20171101 20171130 2017 2017-08-31T18:06:31.18233Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref 0169-2607 01692607 true 151 151 C Volume 151 8 91 99 91 99 201711 November 2017 2017-11-01 2017-11-30 2017 Section I: Methodology article fla © 2017 Elsevier B.V. All rights reserved. AMULTIVARIATEBLINDSOURCESEPARATIONALGORITHM GOLDHACKER M 1 Introduction 2 Spatiotemporal decomposition 2.1 Motivation 2.2 Singular value decomposition 2.3 Independent components 2.3.1 Estimation of the matrices C l 2.3.1.1 Second-order statistics 2.3.1.2 Higher order statistics 3 Algorithm 4 Results 4.1 Retinotopy fMRI 4.2 Dynamic functional network connectivity 4.2.1 Group-ICA 5 Discussion 5.1 Retinotopy fMRI 5.1.1 Dimension of the latent space 5.1.2 Spatial and temporal components 5.2 Dynamic functional connectivity 5.2.1 Correlation structure of the resting states 5.2.2 Temporal evolution of the correlation structure 6 Conclusion 7 Conflict of interest References JOLLIFFE 2006 I PRINCIPALCOMPONENTANALYSIS HYVARINEN 2001 A INDEPENDENTCOMPONENTANALYSIS CICHOCKI 2009 A NONNEGATIVEMATRIXTENSORFACTORIZATIONSAPPLICATIONSEXPLORATORYMULTIWAYDATAANALYSISBLINDSOURCESEPARATION COMON 2010 P HANDBOOKBLINDSOURCESEPARATIONINDEPENDENTCOMPONENTANALYSISAPPLICATIONS POLINE 2012 871 880 J CALHOUN 2012 60 73 V CALHOUN 2001 43 53 V SHI 2015 362 371 Y CALHOUN 2009 S163 S172 V SEIFRITZ 2002 1706 1708 E STONE 2002 407 421 J THEIS 2008 2209 2216 F SMITH 2012 3131 3136 S VANESSEN 2012 2222 2231 D MOELLER 2010 1144 1153 S FEINBERG 2010 e15710 D SETSOMPOP 2012 1210 1224 K XU 2012 2306 J PROCEEDINGS20THANNUALMEETINGISMRM HIGHLYACCELERATEDWHOLEBRAINIMAGINGUSINGALIGNEDBLIPPEDCONTROLLEDALIASINGMULTIBANDEPI GLASSER 2013 105 124 M JENKINSON 2002 825 841 M JENKINSON 2012 782 790 M FISCHL 2012 774 781 B SMITH 2013 144 168 S SALIMIKHORSHIDI 2014 449 468 G GRIFFANTI 2014 232 247 L CALHOUN 2012 255 256 V ALLEN 2012 E THEIS 2004 726 733 F SECONDORDERBLINDSOURCESEPARATIONBASEDMULTIDIMENSIONALAUTOCOVARIANCES EGUILUZ 2005 018102 V LANG 2012 21 E BULLMORE 2009 186 198 E STAM 2007 92 99 C MA 2014 196 206 S GEERLIGS 2014 1 13 L FRAIMAN 2009 061922 D CHANG 2010 81 98 C CRIBBEN 2012 907 920 I CALHOUN 2014 262 274 V DECO 2011 43 56 G KOPELL 2014 1319 1328 N JING 2015 9 16 Y EKLUND 2012 145 161 A GOLDHACKERX2017X91 GOLDHACKERX2017X91X99 GOLDHACKERX2017X91XM GOLDHACKERX2017X91X99XM 2018-09-01T00:00:00.000Z UnderEmbargo © 2017 Elsevier B.V. All rights reserved. item S0169-2607(16)30609-5 S0169260716306095 10.1016/j.cmpb.2017.08.019 271322 2017-09-22T16:27:55.783865-04:00 2017-11-01 2017-11-30 true 2372232 MAIN 9 54889 849 656 IMAGE-WEB-PDF 1 gr1 21425 160 219 gr2 15840 106 219 gr3 7607 164 208 gr4 18238 164 103 gr5 13954 164 207 gr6 8090 163 219 gr7 9838 164 215 gr8 11666 164 208 gr9 7539 53 219 gr1 24654 194 265 gr2 44703 242 499 gr3 27194 298 379 gr4 107573 625 393 gr5 44173 300 379 gr6 27025 281 378 gr7 29846 286 375 gr8 37555 294 373 gr9 30830 122 500 gr1 170734 857 1173 gr2 396530 1070 2210 gr3 195261 1320 1677 gr4 800861 2769 1741 gr5 280661 1329 1680 gr6 183687 1246 1676 gr7 225994 1267 1660 gr8 281411 1304 1653 gr9 280471 539 2213 si10 1143 69 192 si20 914 41 170 si30 904 19 249 si40 605 16 152 si50 172 21 18 si7 350 21 65 si8 170 18 15 si9 416 17 91 si11 436 15 95 si12 331 21 65 si13 323 15 70 si14 445 21 90 si15 531 24 105 si16 522 24 100 si17 262 16 58 si18 363 19 76 si19 475 16 127 si2 415 17 91 si21 468 16 135 si22 788 54 95 si23 819 24 176 si24 482 16 148 si1 478 14 132 si25 245 14 59 si26 831 23 181 si27 443 16 133 si28 711 23 160 si29 549 17 137 si3 312 15 66 si31 854 17 244 si32 312 13 70 si33 576 13 139 si34 385 13 89 si35 314 13 65 si36 548 16 120 si37 538 16 127 si38 383 14 82 si39 454 16 107 si4 603 17 175 si41 298 13 77 si42 361 13 73 si43 315 13 66 si44 257 13 58 si45 293 15 64 si46 259 13 58 si47 446 21 94 si48 225 13 46 si49 166 18 15 si5 1215 21 385 si51 353 13 71 si52 247 13 51 si53 261 16 65 si54 237 13 51 si55 243 21 25 si56 335 13 74 si6 1567 80 250 COMM 4482 S0169-2607(16)30609-5 10.1016/j.cmpb.2017.08.019 Elsevier B.V. Fig. 1 Illustration of the dFNC approach applied to the correlation time series. Fig. 1 Fig. 2 Comparison of best fitting temporal and spatial components obtained with stBSS-FO or stBSS-SO (green) vs SPM (blue/yellow). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Fig. 3 Normalized eigenvalues of the data X. Fig. 3 Fig. 4 Five Brain States estimated using 3 algorithms: INFOMAX, JADE and stBSS-FO. Fig. 4 Fig. 5 Measurement of the cosine similarity between the original dFCN matrices and the linear combination of the five extracted states. Fig. 5 Fig. 6 Averaged cosine similarity, over all 400 sessions, between the original dFCN matrices and the linear combination of the five extracted states. The gray shadding display the error bars. Fig. 6 Fig. 7 Normalized eigenvalues of dFCN times series for different number of components of gICA step. Fig. 7 Fig. 8 Mean of the average cosine similarity for various numbers of BRS computed with different number of gICA components. Fig. 8 Algorithm 1 stBSS. Algorithm 1 Table 1 Assignments of the ICs to the RSN for the data set with 24 extracted ICs. Table 1 network name abbreviation number of components auditory AUD 2 sensorymotor SM 5 visual VIS 6 cognitive control CC 6 default mode DM 5 Table 2 Dimension of the space of latent variables. Average correlation coefficients of the correlation of temporal components with the design vector for different K. Averages are taken over all 8 subjects. Table 2 K stBSS-SO stBSS-FO 4 0.81 ± 0.26 0.85 ± 0.24 6 0.87 ± 0.10 0.93 ± 0.04 8 0.83 ± 0.10 0.91 ± 0.08 10 0.82 ± 0.13 0.93 ± 0.04 15 0.78 ± 0.17 0.92 ± 0.05 Table 3 Cosine similarity score of the extracted connectivity states resulting from the algorithms: (1) INFOMAX, (2) JADE and (3) stBSS-FO. Table 3 states (1) vs (2) (1) vs (3) (2) vs (3) 1 0.99 0.98 0.97 2 0.98 0.94 0.97 3 0.98 0.92 0.95 4 0.99 0.98 0.97 5 0.99 0.68 0.72 Table 4 Explained variance with five PCs of dFCN correlation matrices computed with different number of gICA components. Table 4 Nr components variance 11 0.4440 24 0.3868 43 0.2735 59 0.2236 A multi-variate blind source separation algorithm M. Goldhacker ⁎ c b P. Keck c A. Igel c E.W. Lang c A.M. Tomé a a DETI- IEETA -Universidade Aveiro, 3810-193 Aveiro, Portugal DETI- IEETA -Universidade Aveiro 3810-193 Aveiro Portugal b Experimental Psychology, University of Regensburg, 93040 Regensburg, Germany Experimental Psychology University of Regensburg 93040 Regensburg Germany c CIML, Biophysics, University of Regensburg, 93040 Regensburg, Germany CIML, Biophysics University of Regensburg 93040 Regensburg Germany ⁎ Corresponding author. Background and objective The study follows the proposal of decomposing a given data matrix into a product of independent spatial and temporal component matrices. A multi-variate decomposition approach is presented, based on an approximate diagonalization of a set of matrices computed using a latent space representation. Methods The proposed methodology follows an algebraic approach, which is common to space, temporal or spatiotemporal blind source separation algorithms. More specifically, the algebraic approach relies on singular value decomposition techniques, which avoids computationally costly and numerically instable matrix inversion. The method is equally applicable to correlation matrices determined from second order correlations or by considering fourth order correlations. Results The resulting algorithms are applied to fMRI data sets either to extract the underlying fMRI components or to extract connectivity maps from resting state fMRI data collected for a dynamic functional connectivity analysis. Intriguingly, our algorithm shows increased spatial specificity compared to common approaches, while temporal precision stays similar. Conclusion The study presents a novel spatiotemporal blind source separation algorithm, which is both robust and avoids parameters that are difficult to fine tune. Applied on experimental data sets, the new method yields highly confined and focused areas with least spatial extent in the retinotopy case, and similar results in the dynamic functional connectivity analyses compared to other blind source separation algorithms. Therefore, we conclude that our novel algorithm is highly competitive and yields results, which are superior or at least similar to existing approaches. Keywords Blind source separation Independent component analysis fMRI Resting state Retinotopy Spatio temporal 1 Introduction Exploratory matrix factorization (EMF) techniques are unsupervised, data-driven approaches which are used to discover latent factors, also called sources or features, in the data [1–4]. Such techniques often provide alternative representations of large data sets such as, for example, functional imaging data. This transformed data often reveals underlying characteristics of the data sets under study. Hence these features are generally considered indicative of underlying processes or networks, and often serve classification purposes for discriminating different types of data. Especially functional magnetic resonance imaging (fMRI) experiments produce sequences of images/volumes. Such data is then organized into a data matrix X where each row contains a volume, i.e., a complete 3D scan of the brain. The intensities I(x, y, z) of the voxels within the volume are concatenated into a row vector by associating the spatial voxel coordinates (x, y, z) with an index n. Each row of X with a total of N = s 1 × s 2 × s 3 voxels thus represents one volume scan, or a region of interest (ROI) in the total volume, and is associated with a time index m. A session involves a total of M scans, therefore a session is represented by an M × N data matrix where M is related with the time domain and N is associated with the spatial domain. If instead of analyzing intensity distributions, we are interested in dynamic functional connectivity aspects, the data matrix X is formed with Pearson correlation coefficients, which represent two-point correlations between time series resulting from pre-processing the fMRI data. Therefore, the raw data is often decomposed into functional networks using a group level spatial ICA. The referred pre-processing step results in L independent spatial maps and the related time courses. These time courses are then subdivided into short, partially overlapping segments, and L ( L − 1 ) / 2 correlation coefficients are computed between all time windows. These correlations vary over time, thus represent what is called dynamic functional connectivity networks (dFCN). The rows of the data matrix are then formed by concatenating N = S P , e.g. S sessions and P time points per session, correlation coefficients ρ m m ′ [ n ] , n = 1 , … , N . The goal of exploratory matrix factorization techniques is to approximate the original data matrix by the product of two matrices, which can be expressed as an outer product of K column W *k and row H k* vectors according to (1) X ^ = W H = W * 1 H 1 * + W * 2 H 2 * + … + W * K H K * with (2) W * k H k * = ( w 1 k ⋮ w M k ) ( h k 1 ⋯ h k N ) . The original matrix is thus approximated by K outer products of factors related with the information contained in the space spanned by the column vectors W *k and the space spanned by the row vectors H k*. Moreover, each row of W and each column of H , respectively, form the coordinates of a new representation by which the information is encoded in a latent (hidden) space of dimension K. Note that in general no a priori knowledge about the factor matrices is available. The mathematical model described by Eq. (1) is also followed by the general linear model (GLM) widely used in fMRI data analysis. The main difference is that the matrix W, now called design matrix, represents the experimental manipulations and conditions which are assumed to be known. Therefore, GLM represents a semi-knowledge-based approach [5], where the unknown matrix H can be estimated by minimizing the Frobenius norm of the error matrix ( X − X ^ ) and X ^ represents the estimate WH. In data-driven methods, applied to fMRI data sets X, both factor matrices (W, H) are estimated given only the measured data [6]. To achieve this goal, assumptions like orthogonality (singular value decomposition (SVD) or principal component analysis (PCA)), statistical independence (independent component analysis (ICA) or blind source separation (BSS)), nonnegativity (non-negative matrix factorization (NMF)) and so on need to be applied. Independent Component Analysis (ICA) has been widely applied to fMRI data by imposing the independence constraint either to the spatial (rows of H) domain (sICA) or the time (columns of W) domain (tICA).Note that with sICA the matrix W is considered the mixing matrix, while with tICA this becomes the role of the matrix H. The sICA is more common due to the large dimensionality of the spatial domain when compared with the temporal domain. Moreover, if it comes to compare ICs across a group of subjects, group ICA [7–9] has to be applied to get around the inherent permutation and scaling indeterminacies of ICA. And the latter leads to a set of independent spatial maps, which are visually analyzed to identify artifacts like head movement or known structures in the brain (major blood vessels or the ventricles). Finally, the time courses, corresponding to valid independent spatial components, are selected to compute the dFCN data matrix. 2 Spatiotemporal decomposition Early attempts to combine sICA and tICA can be found in [10] where sICA is used to select the regions of interest (ROIs), and then tICA is applied to the time series of those voxels to find temporal components. More generally, spatiotemporal approaches were also suggested [11] and [12]. In both works, a singular value decomposition (SVD) pre-processing step determines the dimension of the latent space. Then a new transformation matrix is computed using both time and space components of the SVD decompostion. In [11], the independent components in the new latent space are computed by optimizing a contrast function via gradient descent. In [12], an algebraic joint approximative diagonalization (JAD) approach is considered after computing a set of matrices derived from time and space components. More recently [13], a cascaded approach was proposed to compute independent components. Thereby, after applying sICA, a tICA is applied to the mixing matrix W of the spatial mixing model. In this proposal, there is an additional stage to visually identify and remove artifact related independent spatial components. Then, the corresponding time components are regressed out of the non-artifact-related time components of the mixing model. Notice that the final model to approximate the original data is presented as a product of three matrices, originating from the two step procedure. 2.1 Motivation The Motivation of this study is to combine the work of [11] with an algebraic approach as proposed in [12]. Therefore, the transformation matrix, estimated in the latent space, is an orthogonal matrix and both, temporal and spatial components, undergo a similar rotation. Both works have shortcomings, which are corrected for by our proposed combination. The algorithm in [11] employs many additional parameters resulting in a less robust behavior, whereas the approach in [12] is an exact analytical method. Nevertheless, the work of [12] uses pseudoinverses, which introduce a computationally costly and a random component to the algorithm. Furthermore, the pseudoinverses render the algorithm computationally instable because of possible degeneracy of the matrix columns. These shortcomings are accounted for in our proposed method, which is based on the well known and robust eigenvalue decomposition. The performance of the method is illustrated with fMRI data sets to extract relevant information in different stages of the processing chain: a processing and a post-processing step. Nevertheless, the algorithm is generally suitable for spatiotemporal datasets irrespective of their origin and may have applications in many fields of science. 2.2 Singular value decomposition Singular Value Decomposition (SVD) of the original data matrix X is the most widely used factorization technique that serves as a pre-processing step in most of the decomposition techniques. The matrix decomposition reads (3) X = U Σ V T If N >  > M, the maximal number of non-zero singular values is M, and those form the entries of the M × M diagonal matrix Σ. The related eigenvector matrices, i.e. the M × M matrix U and the N × M matrix V, have M orthogonal columns. The SVD decomposition may be re-written using the factorization model described in Eq. (1) by using only the K < M largest singular values and corresponding eigenvectors. This decomposition will lead to a low-rank approximation of the original data. The Frobenius norm of the error of the approximation is related with the discarded singular values (4) ∥ X − X ^ ∥ F = ∑ i = K + 1 M Σ i i 2 when the diagonal entries of Σ are assumed to be arranged in decreasing order of magnitude. Now considering the decomposition given in Eq. (1): • If a spatial decomposition (sBSS) is intended, then we identify W = U K Σ K and H = V K T . Note that here H is related to the underlying sources while W forms part of the mixing matrix of the model. • If, however, one is interested in a temporal decomposition (tBSS), then one has instead W = U K and H = Σ K V K T . Now W corresponds to the underlying sources and H forms part of the mixing matrix. • If, finally, we consider a spatio - temporal decomposition (stBSS), then it is most convenient to identify W = U K Σ K 1 / 2 and H = Σ K 1 / 2 V K T . Both components have the same energy, which equals the singular values of the original data matrix. Note that the transformation, described in the first two hypothesis, is called whitening in most of the independent components algorithms (ICA) and achieves source-related components with an energy normalized to unity. So far, only decorrelated components have been achieved, and the next step provides the independent components by applying a new transformation. In summary, the spatial and temporal components in the latent space of dimension K are written as S s = H and S t = W T , respectively, where S s is a K × N matrix and S t is a K × M matrix. 2.3 Independent components As the goal is to have all components of the decomposition statistically independent, an additional rotation matrix needs to be determined. Several options are available to achieve this goal. One of the most widely used is an algebraic method called Joint Approximative Diagonalization (JAD). Therefore, after computing a set of real-valued K × K matrices ( C l , l = 1 … D ) JAD aims at estimating a single K × K matrix A, called joint diagonalizer, which satisfies the condition ∑ l o f f ( A T C l A ) ≈ 0 where the off - operator selects the off-diagonal elements of the argument. The most widely used algorithm to implement this condition is based on Given’s rotations and estimates an orthogonal matrix A A T = A T A = I . The application of this matrix to the SVD outputs results in (5) S ^ t = A T W T S ^ s = A T H Note that thereby the original data matrix X can be approximated by (6) X ^ = S ^ s T S ^ t = U K Σ K U K T as the matrix A is an orthogonal matrix. Therefore, the reconstruction error is not altered by the rotation matrix computed in this step. Notice that a similar conclusion might be drawn for most of the sBSS or tBSS algorithms, which apply the whitening procedure as described before, and compute an orthogonal matrix on the second step. 2.3.1 Estimation of the matrices C l Two common strategies can be found to estimate the set of matrices C l , l = 1 , 2 , … , D : 2.3.1.1 Second-order statistics One way is to use second order statistics (SOS) only. This is possible if further knowledge about correlations, usually time correlations in time series data, is available in addition and can be exploited. Therefore, denoting the latent signals as s *[n], where * = s , t denotes either the temporal or spatial components, second order statistics is based on time-delayed correlation matrices R ^ l = E { s * [ n ] s * T [ n + l ] } Note that the expectation operator E usually deals with a different number of samples when considering spatial n ∈ [1, N] or temporal m ∈ [1, M] components, whereby N ≫ M. The index l = { 1 , 2 , … , D } counts the number of delays which determine the number of matrices to be diagonalized. The value D is an user assigned parameter. To assure symmetry, the matrices to be diagonalized are defined as C l = 0.5 R ^ l + 0.5 R ^ l T 2.3.1.2 Higher order statistics An alternative way is to consider the data as samples from a stochastic process. Then higher order statistics (HOS) of the distribution of random variables needs to be exploited. Often fourth-order statistics are considered only. They rely on the estimation of cumulant matrices. The number of cumulant matrices is D = K ( K + 1 ) / 2 and the entries to each cumulant matrix are 4-th order cumulants computed with the latent space variables. Considering either a spatial * ≡ s or temporal * ≡ t component in latent space, i.e. s * [ n ] = [ s 1 [ n ] s 2 [ n ] … s K [ n ] ] T , the (p, q)th element of the l cumulant matrix is E{si [n]sj [n]sp [n]sq [n]]}. Note that the different cumulant matrices result from indexing ( i , j ) , i ≥ j , ( i , j ) ∈ { 1 , 2 … K } . From now on algorithms employing the second-order time-delayed correlation matrices are denoted stBSS-SO, while algorithms using fourth-order cumulant matrices are called stBSS-FO. 3 Algorithm In this section we introduce a step-by-step formulation of our algorithm (see Algorithm. 1 ). The information of Section 2 is condensed in this algorithmic description. It was implemented on an INTEL i7 Quad-core 3.2  GHz with 16 GB RAM. The used programming language was MATLAB (v2016a). 4 Results The spatiotemporal methodology was applied to two fMRI data sets: a retinotopy data set and a resting-state fMRI data set. We chose the data sets, because both types of data are inherently spatiotemporal. Therefore, they render suitable for applying our stBSS method. Furthermore, the used data sets represent recent fields in the neuroscientific community. Additionally, the number of dimensions of those data sets are relatively high serving as a good benchmark for the proposed algorithm. The following evaluation methods were chosen very specifically for each data set, since we dealt with complex data and experiments. For the retinotopy data set, the spatiotemporal approach was used to extract the underlying components of the fMRI data, and to select the component related with the design vector. In case of resting-state data, the methodology of a spatiotemporal approach was applied to interpret the correlation coefficient time series after the group-ICA analysis. Hence, we apply a stBSS algorithm to find for the two domains, i.e. the correlation domain and the temporal domain in our application, a statistically independent representation. 4.1 Retinotopy fMRI A total of 8 participants were scanned during a visual retinotopic mapping experiment. The visual stimulus was a flickering black-and-white, bow-tie shaped checkerboard at either a horizontal (stimulus 1) or vertical (stimulus 2) orientation and flickering at 8 Hz. The task of every subject was to focus a white cross shown in the center of the bow-tie. Each stimulus was displayed for 15 seconds and was alternated during data acquisition corresponding to a total of N = 140 scans. The time courses of both stimuli were convolved with a typical hemodynamic response function (HRF) as provided in the Statistical Parametric Mapping (SPM) package [14]. In this experiment, the difference between the two design vectors, reflecting the phase-shifted stimulus designs, was considered the proper reference time course for correlating temporal components. This was done because the stimuli are highly correlated thus violating the assumption of independent underlying source signals when an ICA analysis is performed. The data set was pre-processed with the SPM toolbox: space and time realignment, normalization to a standard brain and Gaussian smoothing ( F W H W = 4 mm ). The occipital lobe region was extracted by using an appropriate mask which extracts N = 21515 voxels of each 79 × 95 × 69 complete brain scan. Illustrations showing activity distributions in the brain were generated using the software tool CARET (see Fig. 2). 4.2 Dynamic functional network connectivity We based our analysis on volumetric data from the preselected bundle of 100 unrelated human subjects from the S500 release of the Human Connectome Project [15], in which each subject went through four rs-fMRI sessions lasting 873 [s] resulting in 1200 volumes per session and n = 400 sessions in total. Data was acquired at customized 3T MRI scanners at Washington University using multi-band (factor 8) acquisition techniques [16–19]. From the different versions of the data we chose the most preprocessed data set with motion-correction, structural preprocessing, and ICA-FIX denoising [20–26]. The rs-fMRI data has T R = 720 [ m s ] and T E = 33.1 [ m s ] acquired by a Gradient-echo EPI sequence. Flip angle was 52° and the FOV 208 [mm] × 180 [mm] with a slice thickness of 2 [mm], 72 slices, and isotropic 2 [mm] voxel size. As additional preprocessing, we applied a Gaussian smoothing kernel with a FWHM of 5 [mm] using SPM8 software package and we discarded the first five scans of each session from our analysis. Therefore, each individual session data is formed with M = 1195 scans with 193965 voxels each. The principle of a dFCN is shown in Fig. 1 . Assuming that highly correlated time series share highly similar information contents, such correlation measures could help identifying networks of neural information processing. The temporal dynamics of such connectivity patterns can be estimated by computing correlations between different time series within specified time segments which are shifted along the time axis. Hence, time windows with Δ t = 57.6 [ s ] were chosen and were shifted in steps of δ t = T r = 720 [ m s ] along the time series. Finally this resulted in a total of P = 1115 correlation matrices per session. Each 24 × 24 - dimensional correlation matrix is symmetric and contains M = 276 different Pearson coefficients. The latter correlation values are concatenated into a column of the data matrix X which encompassed a total of S = 400 sessions. 4.2.1 Group-ICA To apply gICA [27] on raw data sets, we used the GIFT toolbox 1 1 . The main processing steps comprise: a principal component analysis (PCA), at a level of individual sessions, followed by a spatial ICA at group level. The PCA step reduces the time dimension of the data to T = 45 principal components related with the largest eigenvalues. Then these T components of all S individual sessions are concatenated in the time domain, and a subsequent PCA reduces the dimension from T · S to L 1 = 30 components. Finally, group spatial independent components are computed using the INFOMAX algorithm. These group spatial components are analyzed to eliminate possible artifacts. The components are selected according to their spatial localization as well as the power spectral information of its corresponding time course. This selection process finally yields L = 24 ICs which are back-projected to the individual session space using the matrices computed in the three steps described before. The selected components, corresponding to different brain regions, can be assigned to five networks as described in Table. 1 In particular, the time courses related with the ICs of individual sessions [28] are described by (7) T ^ i = F i G i A ^ where F i is the 1195 × T PCA transformation matrix computed at the level of individuals; G i is the T × L 1 - dimensional i − t h block of a subsequent PCA transformation matrix computed at the group level, and A ^ is the L 1 × L - dimensional matrix resulting from the INFOMAX algorithm after the elimination of artifact-related components. Therefore, each column of T ^ i is a time series, and the correlation coefficients are computed using sliding windows with T R = 80 times points, with a shift of 1 time step between consecutive windows. Furthermore, the correlation coefficients are computed after point-wise multiplication of the time series with a Gaussian window with variance equal to 5. The pairwise correlation matrix resulting from each window has L ( L − 1 ) / 2 unique entries, and those are concatenated to form a column of the new data set. So far, at session level, the correlation data matrix X i has dimensions 276 × 1195. In this study, the matrix to be analyzed by a “spatiotemporal” algorithm, is the result of the concatenation of all individual matrices. 5 Discussion 5.1 Retinotopy fMRI 5.1.1 Dimension of the latent space The SVD decomposition determines the approximation error, i.e., how well the decomposition approximates the original data. A scree plot of the singular values offers a good estimate about the intrinsic dimension of the observations, i.e. the number K of informative components. Applying the knee rule , K = 6 turned out to be appropriate for all subjects. This is corroborated by Pearson correlation coefficients between the design vector and the column vectors carrying temporal information, which were computed for different numbers of extracted components (see Table 2 ). The largest absolute value was used as an indicator of the most relevant vector of latent components. 5.1.2 Spatial and temporal components After the dimension reduction step, the spatial and temporal components are estimated according the Eq. (5). The temporal components are correlated with the reference time course estimated using the SPM tool. The corresponding spatial component was normalized to zero mean and unit standard deviation. Activation maps were then generated by including only activation values that lay outside the range of [ − σ , σ ] , where σ denotes the standard deviation of the corresponding activity distribution. Two groups of results could be found (see Table. 2): • For one group, both algorithms have correlation coefficients with temporal components always larger than ρ > 0.80. Then the spatial activation maps point to the same areas and have a large overlap. • For a second group, the activation maps computed with 4-th order cumulant matrices were always broader than those computed with 2-nd order correlation matrices. In that case, the correlation coefficients of temporal components with the design vector dropped to less than ρ < 0.80 whatever was the dimension of the latent space. However, in all cases correlation coefficients were larger than ρ > 0.7. Fig. 2 shows, for the former group, overlay graphs of the design vector and the ”best-fitting” temporal component and illustrates the spatial extension of the activations resulting from either the stBSS-SO or the stBSS-FO algorithm (green) in relation to a corresponding evaluation using the SPM toolbox (blue - positive, yellow negative activity). The best fitting component is indeed highly correlated with the design vector, hence follows the experimental paradigm almost perfectly. Concerning spatial maps, areas colored green indicate congruent activated regions obtained with one of the newly proposed algorithms as well as with the SPM tool. However, the new algorithms clearly result in much more focused and spatially localized activity distributions. Similar results have been obtained with the algorithms stJADE [12] and stSOBI [29] (not shown here). Results of the second group show that the correlation of the temporal component with the design vector is still high in case of stBSS-FO but drops in case of stBSS-SO. Spatial maps again turn out to be much more localized. By and large, across all subjects studied, activity distributions are congruent between subjects showing prominent activations in area V1 and V2, and to a lesser extent also in higher visual areas as can be seen from Fig. 2. 5.2 Dynamic functional connectivity Functional connectivity is a key aspect in the analysis of resting-state functional magnetic resonance imaging (rs-fMRI). It is based on calculating association measures – mostly Pearson correlation – between distinct regions in the brain. First attempts focused on the static case, for which whole time courses of resting-state-related brain regions were used for evaluating correlation coefficients representing the strength of their functional connections [30,31]. This approach resulted in many insights ranging from a small-world organization of brain graphs that are constructed from this so-called connectome [32] over deviations in functional connectivity between pathological and healthy brains [33,34] to developmental changes of functional connectivity [35]. It was also possible to identify similarities between physical systems like the Ising-model of a spin glass and functional connectivity brain networks [36]. However, using whole time courses integrates out all temporal dependences within the connectome, resulting in static average connectivities. Recently a paradigm shift occurred towards a so-called dFCN, which takes into account the temporal variability of functional connections in the brain. Investigating temporal fluctuations of functional connectivity thus has received considerable attention in the last few years [28,37–41]. DFCN can be investigated with a sliding-window technique [28] applied to the time courses of independent components (ICs) resulting from a group independent component analysis (gICA) [7] on a very large set of subjects undergoing rs-fMRI. This common technique is also employed in the present study. Note that recent development in the field offers promising new methods for doing gICA efficiently in a parallelized manner [42,43]. The idea is to track the variability of correlation matrices formed from segments of the time courses of all ICs. Shifting the window one time step further, results in a new correlation matrix for the next time step with slight changes in correlation coefficients and so on. The vast set of correlation matrices resulting from such a sliding-window approach can be condensed into several representative correlation patterns by applying k-means clustering [28]. The stable patterns, which robustly showed up as cluster-centroids, can be considered connectivity-states given the fact that these centroids represent very robust and almost discrete correlation patterns, reflecting characteristic connectivities that the brain goes through over time, while simultaneously remaining similar between subjects. When using the k-means algorithm the number of extracted clusters has to be predefined. Usually it is deduced from a silhouette score which compares the variance within the extracted clusters to the variance between them while varying the number of clusters. 5.2.1 Correlation structure of the resting states Considering all correlation data collected in matrix X, only the stBSS-FO variant of the stBSS algorithm is explored. There, as a first analysis step, an SVD of the data matrix is considered where the number of significant components, i.e. resting states to be considered, can be chosen by a knee criterion applied to a scree plot of the normalized eigenvalues (see 3 ). As a result, we concluded that 5 resting states deem most appropriate for further analysis. As described in Section 2.3, the K = 5 latent components of both spaces are used to estimate cumulant matrices. The latter serve to find a joint diagonalizer which finally helps to estimate the rotation matrix. Then the components in both spaces are estimated as described by Eq. (5). The result is compared in Fig. 4 with the mixing matrix resulting from an application of the INFOMAX or the JADE algorithm, respectively. Note that in these algorithms a similar rotation matrix is estimated with pre-whitened data, i.e. using the eigenvector matrix V K T resulting from an SVD. As can be seen, all algorithms produce rather similar states, except the fifth state of the stBSS-FO, which looks slightly different when compared to the result of the other algorithms. The Table 3 collects the values of a pairwise cosine similarity score. 5.2.2 Temporal evolution of the correlation structure Finally, the quality of the extracted resting states has to be assessed. This is achieved by measuring the reconstruction quality of a weighted linear combination of resting states with the original centered dFCN matrices. For this purpose, the five resting states were multiplied with their time course weights and then added up to reconstruct the dFCN matrix of a certain time point. Similarity was measured with a cosine criterion. Fig. 5 illustrates the similarity measures for a whole session. At the single subject level this similarity score fluctuates considerably, which renders it difficult to assess the quality of the states. Therefore an average similarity across all 400 sessions was calculated separately for the three different ICA algorithms. The result of these computations is shown in Fig. 6 . All three ICA algorithms yield similar temporal evolutions for each session, though the states are not completely identical. As referred before, all algorithms share a common pre-processing step which explains the reconstruction error in terms of the discarded singular values of the SVD decomposition (see Eq. (4)). The second step does not change the value of this error as it follows from Eq. (6). Therefore, the components, estimated from all three algorithms, show a similar behavior when compared with the original. Finally we need to test the robustness of the resulting brain states against any variation of the number of states to be extracted as well as the number of dFCN correlation matrices to be used in the analysis. Obviously a larger number of ICs used in the gICA analysis would lead to a higher resolution of the human connectome, thus resulting in an improved data representation via the low-rank approximation of the data matrix. However, the computation time and working memory requirements for the calculation of independent spatial maps via gICA increase strongly with a rising number of brain components. To test the robustness of the extracted resting states, next to the 24 brain component correlation matrices, also a smaller set with 11 components and two larger sets with 43 and 59 components were used. First, the number of underlying brain states was estimated for the new data with the elbow criterion applied to the eigenvalue spectrum. As can be seen in Fig. 7 , the resulting curves of the normalized and ordered eigenvalues of the correlation matrix are quite similar. It appears that by applying an elbow criterion, five brain states seems a good choice for the different brain component numbers tested. The amount of variance, which can be explained with five principal components, is given in the following table (see Table. 4 ) for different sizes of the correlation matrices. It appears that 24 components result in clearly structured brain states, which yield a homogeneous activity distribution across the related brain areas involved. Considering a proper number of brain states, the cosine similarity score has been employed to estimate the closeness between the original dFCNs and the reconstructed correlation maps while varying the number of extracted brain states. Not surprisingly, the similarity score increases with decreasing number of ICs and an increasing number of brain states (see Fig. 8 ). No optimum could be observed, however, which would have helped to identify a best number of ICs used for the gICA preprocessing. 6 Conclusion The present study treats spatiotemporal ICA based on second order or higher order correlations on a similar footing. It starts from a SVD of any given data array and discusses the form of the factor matrices of a generic ICA model, i.e. X = W H in relation to the eigenvector and singular values matrices of the SVD decomposition. A subsequent ICA only needs to find an orthogonal rotation matrix A, which multiplies the factor matrices of SVD decomposition to simultaneously yield independent spatial and temporal factor matrices of an ICA decomposition. Furthermore, the approximation X ^ to the original data set X does not depend on the rotation matrix. This is a common characteristic of all methods that apply SVD on centered data, followed by the estimation of an orthogonal matrix. Two applications are discussed to illustrate the new spatiotemporal BSS algorithm. A rather trivial retinotopic fMRI experiment is discussed first, which compares stBSS-SO with stBSS-FO, and both with an SPM-GLM analysis. It is shown that temporal components agree surprisingly well, while spatial maps differ somewhat though showing substantial overlap. Importantly, the new method yields most confined and focused areas with the least spatial extension. As a second application, dynamic functional connectivity networks are studied with respect to resting state dynamics. The identification of a proper number of resting states is discussed, and the dynamical evolution of the correlations between their voxel time courses are compared as they resulted from different ICA algorithms, including the new stBSS. Algorithms like INFOMAX, JADE and the new stBSS resulted in rather similar resting states and their temporal evolution. 7 Conflict of interest The authors declare no conflict of interest. References [1] I. Jolliffe Principal Component Analysis 2006 Springer Series in Statistics [2] A. Hyvärinen J. Karhunen E. Oja Independent Component Analysis 2001 John Wiley & Sons [3] A. Cichocki R. Zdunek A.H. Pham S. Amari Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation 2009 Wiley and Sons [4] P. Comon C. Jutten Handbook of Blind Source Separation: Independent Component Analysis and its Applications 2010 Academic Press [5] J.-B. Poline M. Brett The general linear model and fMRI: does love last forever? Neuroimage 62 2 2012 871 880 [6] V.D. Calhoun T. Adali Multi-subject independent component analysis of fMRI: a decade of intrinsic networks, default mode, and neurodiagnostic discovery IEEE Rev. Biomed. Eng. 5 2012 60 73 10.1109/RBME.2012.2211076 [7] V.D. Calhoun T. Adali G.D. Pearlson J.J. Pekar Spatial and temporal independent component analysis of functional MRI data containing a pair of task-related waveforms. Hum. Brain Mapp. 13 1 2001 43 53 [8] Y. Shi W. Zeng N. Wang D. Chen A novel fMRI group data analysis method based on data-driven reference extracting from group subjects Comput. Methods Programs Biomed. 122 3 2015 362 371 10.1016/j.cmpb.2015.09.002 [9] V.D. Calhoun J. Liu T. Adali A review of group ICA for fMRI data and ICA for joint inference of imaging, genetic, and ERP data Neuroimage 45 1, Supplement 2009 S163 S172 10.1016/j.neuroimage.2008.10.057 [10] E. Seifritz F. Esposito F. Hennel H. Mustovic J.G. Neuhoff D. Bilecen G. Tedeschi K. Scheffler F.D. Salle Spatiotemporal pattern of neural processing in the human auditory cortex. Science (New York, N.Y.) 297 5587 2002 1706 1708 [11] J.V. Stone J. Porrill N.R. Porter I.D. Wilkinson Spatiotemporal independent component analysis of event-related fMRI data using skewed probability density functions Neuroimage 15 2 2002 407 421 [12] F.J. Theis P. Gruber I.R. Keck E.W. Lang A robust model for spatiotemporal dependencies Neurocomputing 71 10 2008 2209 2216 [13] S.M. Smith K.L. Miller S. Moeller J. Xu E.J. Auerbach M.W. Woolrich C.F. Beckmann M. Jenkinson J. Andersson M.F. Glasser D.C.V. Essen D.A. Feinberg E.S. Yacoub K. Ugurbil Temporally-independent functional modes of spontaneous brain activity Proc. Natl. Acad. Sci. 109 8 2012 3131 3136 [14] W.T.C. of Neuroimaging, 2009. [15] D.C. Van Essen K. Ugurbil E. Auerbach D. Barch T.E.J. Behrens R. Bucholz A. Chang L. Chen M. Corbetta S.W. Curtiss S. Della Penna D. Feinberg M.F. Glasser N. Harel A.C. Heath L. Larson-Prior D. Marcus G. Michalareas S. Moeller R. Oostenveld S.E. Petersen F. Prior B.L. Schlaggar S.M. Smith A.Z. Snyder J. Xu E. Yacoub The human connectome project: a data acquisition perspective. Neuroimage 62 4 2012 2222 2231 10.1016/j.neuroimage.2012.02.018 [16] S. Moeller E. Yacoub C.A. Olman E. Auerbach J. Strupp N. Harel K. Uğrbil Multiband multislice GE-EPI at 7 tesla, with 16-fold acceleration using partial parallel imaging with application to high spatial and temporal whole-brain fMRI. Magn. Reson. Med. 63 5 2010 1144 1153 10.1002/mrm.22361 [17] D.A. Feinberg S. Moeller S.M. Smith E. Auerbach S. Ramanna M. Gunther M.F. Glasser K.L. Miller K. Ugurbil E. Yacoub Multiplexed echo planar imaging for sub-second whole brain FMRI and fast diffusion imaging. PLoS ONE 5 12 2010 e15710 10.1371/journal.pone.0015710 [18] K. Setsompop B.A. Gagoski J.R. Polimeni T. Witzel V.J. Wedeen L.L. Wald Blipped-controlled aliasing in parallel imaging for simultaneous multislice echo planar imaging with reduced g-factor penalty. Magnet. Reson. Med. 67 5 2012 1210 1224 10.1002/mrm.23097 [19] J. Xu S. Moeller J. Strupp E.J. Auerbach L. Chen D.A. Feinberg K. Ugurbil E. Yacoub Highly accelerated whole brain imaging using aligned-blipped-controlled-aliasing multiband EPI Proceedings of the 20th Annual Meeting of ISMRM 2012 2306 [20] M.F. Glasser S.N. Sotiropoulos J.A. Wilson T.S. Coalson B. Fischl J.L. Andersson J. Xu S. Jbabdi M. Webster J.R. Polimeni D.C. Van Essen M. Jenkinson The minimal preprocessing pipelines for the human connectome project. Neuroimage 80 2013 105 124 10.1016/j.neuroimage.2013.04.127 [21] M. Jenkinson P. Bannister M. Brady S. Smith Improved optimization for the robust and accurate linear registration and motion correction of brain images Neuroimage 17 2 2002 825 841 10.1006/nimg.2002.1132 [22] M. Jenkinson C.F. Beckmann T.E.J. Behrens M.W. Woolrich S.M. Smith FSL Neuroimage 62 2 2012 782 790 10.1016/j.neuroimage.2011.09.015 [23] B. Fischl FreeSurfer Neuroimage 62 2 2012 774 781 10.1016/j.neuroimage.2012.01.021 [24] S.M. Smith C.F. Beckmann J. Andersson E.J. Auerbach J. Bijsterbosch G. Douaud E. Duff D.A. Feinberg L. Griffanti M.P. Harms M. Kelly T. Laumann K.L. Miller S. Moeller S. Petersen J. Power G. Salimi-Khorshidi A.Z. Snyder A.T. Vu M.W. Woolrich J. Xu E. Yacoub K. Uğrbil D.C. Van Essen M.F. Glasser Resting-state fMRI in the human connectome project. Neuroimage 80 2013 144 168 10.1016/j.neuroimage.2013.05.039 [25] G. Salimi-Khorshidi G. Douaud C.F. Beckmann M.F. Glasser L. Griffanti S.M. Smith Automatic denoising of functional MRI data: combining independent component analysis and hierarchical fusion of classifiers. Neuroimage 90 2014 449 468 10.1016/j.neuroimage.2013.11.046 [26] L. Griffanti G. Salimi-Khorshidi C.F. Beckmann E.J. Auerbach G. Douaud C.E. Sexton E. Zsoldos K.P. Ebmeier N. Filippini C.E. Mackay S. Moeller J. Xu E. Yacoub G. Baselli K. Ugurbil K.L. Miller S.M. Smith ICA-based artefact removal and accelerated fMRI acquisition for improved resting state network imaging. Neuroimage 95 2014 232 247 10.1016/j.neuroimage.2014.03.034 [27] V.D. Calhoun T. Eichele T. Adali E.A. Allen Decomposing the brain: components and modes, networks and nodes Trends Cogn. Sci. 16 5 2012 255 256 10.1016/j.tics.2012.03.008 [28] E.A. Allen E. Damaraju S.M. Plis E.B. Erhardt T. Eichele V.D. Calhoun Tracking whole-brain connectivity dynamics in the resting state Cereb. Cortex 2012 10.1093/cercor/bhs352 [29] F. Theis A. Meyer-Bäse E.W. Lang Second-Order Blind Source Separation Based on Multi-Dimensional Autocovariances 2004 Springer Berlin Proc. ICA 2004, Granada, Spain 726 733 [30] V.M. Eguíluz D.R. Chialvo G.a. Cecchi M. Baliki a.V. Apkarian Scale-free brain functional networks Phys. Rev. Lett. 94 1 2005 018102 10.1103/PhysRevLett.94.018102 [31] E.W. Lang A.M. Tomé I.R. Keck J.M. Górriz-Sáez C.G. Puntonet Brain connectivity analysis: a short survey Comput. Intell. Neurosci. 2012 2012 21 10.1155/2012/412512 [32] E. Bullmore O. Sporns Complex brain networks: graph theoretical analysis of structural and functional systems Nat. Rev. Neurosci. 10 3 2009 186 198 10.1038/nrn2575 [33] C.J. Stam B.F. Jones G. Nolte M. Breakspear P. Scheltens Small-world networks and functional connectivity in Alzheimer’s disease. Cerebral Cortex (New York, N.Y. : 1991) 17 1 2007 92 99 10.1093/cercor/bhj127 [34] S. Ma V.D. Calhoun R. Phlypo T. Adali Dynamic changes of spatial functional network connectivity in healthy individuals and schizophrenia patients using independent vector analysis. Neuroimage 90 2014 196 206 10.1016/j.neuroimage.2013.12.063 [35] L. Geerligs R.J. Renken E. Saliasi N.M. Maurits M.M. Lorist A brain-wide study of age-related changes in functional connectivity. Cereb. Cortex 2 2014 1 13 10.1093/cercor/bhu012 [36] D. Fraiman P. Balenzuela J. Foss D. Chialvo Ising-like dynamics in large-scale functional brain networks Phys. Rev. E 79 6 2009 061922 10.1103/PhysRevE.79.061922 [37] C. Chang G.H. Glover Time-frequency dynamics of resting-state brain connectivity measured with fMRI. Neuroimage 50 1 2010 81 98 10.1016/j.neuroimage.2009.12.011 [38] I. Cribben R. Haraldsdottir L.Y. Atlas T.D. Wager M.A. Lindquist Dynamic connectivity regression: determining state-related changes in brain connectivity. Neuroimage 61 4 2012 907 920 10.1016/j.neuroimage.2012.03.070 [39] V.D. Calhoun R. Miller G. Pearlson T. Adali The chronnectome: time-varying connectivity networks as the next frontier in fMRI data discovery Neuron 84 2 2014 262 274 10.1016/j.neuron.2014.10.015 [40] G. Deco V.K. Jirsa A.R. McIntosh Emerging concepts for the dynamical organization of resting-state activity in the brain Nat. Rev. Neurosci. 12 1 2011 43 56 10.1038/nrn2961 [41] N.J. Kopell H.J. Gritton M.A. Whittington M.A. Kramer Beyond the connectome: the dyome Neuron 83 6 2014 1319 1328 10.1016/j.neuron.2014.08.016 [42] Y. Jing W. Zeng N. Wang T. Ren Y. Shi J. Yin Q. Xu GPU-Based parallel group ICA for functional magnetic resonance data Comput. Methods Programs Biomed. 119 1 2015 9 16 10.1016/j.cmpb.2015.02.002 [43] A. Eklund M. Andersson H. Knutsson fMRI analysis on the GPU-Possibilities and challenges Comput. Methods Programs Biomed. 105 2 2012 145 161 10.1016/j.cmpb.2011.07.007 "
    },
    {
        "doc_title": "On the use of multi-dimensional scaling and electromagnetic tracking in high dose rate brachytherapy",
        "doc_scopus_id": "85029896044",
        "doc_doi": "10.1088/1361-6560/aa8944",
        "doc_eid": "2-s2.0-85029896044",
        "doc_date": "2017-10-03",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Brachytherapy",
            "Dip tests",
            "Electromagnetic tracking",
            "Levy distribution",
            "Multi-dimensional scaling",
            "Brachytherapy",
            "Catheters",
            "Electromagnetic Phenomena",
            "Humans",
            "Neoplasms",
            "Phantoms, Imaging",
            "Radiotherapy Dosage",
            "Radiotherapy Planning, Computer-Assisted"
        ],
        "doc_abstract": "© 2017 Institute of Physics and Engineering in Medicine.High dose rate brachytherapy affords a frequent reassurance of the precise dwell positions of the radiation source. The current investigation proposes a multi-dimensional scaling transformation of both data sets to estimate dwell positions without any external reference. Furthermore, the related distributions of dwell positions are characterized by uni-or bi-modal heavy-tailed distributions. The latter are well represented by α-stable distributions. The newly proposed data analysis provides dwell position deviations with high accuracy, and, furthermore, offers a convenient visualization of the actual shapes of the catheters which guide the radiation source during the treatment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of particle filters for electromagnetic tracking in high dose rate brachytherapy",
        "doc_scopus_id": "85029724960",
        "doc_doi": "10.1088/1361-6560/aa8591",
        "doc_eid": "2-s2.0-85029724960",
        "doc_date": "2017-09-12",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Brachytherapy",
            "Computed X-ray tomographies",
            "Electromagnetic tracking",
            "Electromagnetic tracking systems",
            "Empirical Mode Decomposition",
            "High-dose rate brachytherapy",
            "Multi-dimensional scaling",
            "Particle filter",
            "Aged",
            "Artifacts",
            "Brachytherapy",
            "Breast Neoplasms",
            "Catheters",
            "Electromagnetic Phenomena",
            "Female",
            "Humans",
            "Male",
            "Middle Aged",
            "Phantoms, Imaging",
            "Radiotherapy Dosage",
            "Radiotherapy Planning, Computer-Assisted",
            "Tomography, X-Ray Computed"
        ],
        "doc_abstract": "© 2017 Institute of Physics and Engineering in Medicine.Modern radiotherapy of female breast cancers often employs high dose rate brachytherapy, where a radioactive source is moved inside catheters, implanted in the female breast, according to a prescribed treatment plan. Source localization relative to the patient's anatomy is determined with solenoid sensors whose spatial positions are measured with an electromagnetic tracking system. Precise sensor dwell position determination is of utmost importance to assure irradiation of the cancerous tissue according to the treatment plan. We present a hybrid data analysis system which combines multi-dimensional scaling with particle filters to precisely determine sensor dwell positions in the catheters during subsequent radiation treatment sessions. Both techniques are complemented with empirical mode decomposition for the removal of superimposed breathing artifacts. We show that the hybrid model robustly and reliably determines the spatial positions of all catheters used during the treatment and precisely determines any deviations of actual sensor dwell positions from the treatment plan. The hybrid system only relies on sensor positions measured with an EMT system and relates them to the spatial positions of the implanted catheters as initially determined with a computed x-ray tomography.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A tool to automatically analyze electromagnetic tracking data from high dose rate brachytherapy of breast cancer patients",
        "doc_scopus_id": "85029716962",
        "doc_doi": "10.1371/journal.pone.0183608",
        "doc_eid": "2-s2.0-85029716962",
        "doc_date": "2017-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Aged",
            "Automation",
            "Brachytherapy",
            "Breast Neoplasms",
            "Catheters",
            "Electromagnetic Phenomena",
            "Female",
            "Humans",
            "Image Processing, Computer-Assisted",
            "Male",
            "Middle Aged",
            "Motion",
            "Phantoms, Imaging",
            "Radiation Dosage",
            "Radiotherapy Dosage",
            "Radiotherapy Planning, Computer-Assisted",
            "Tomography, X-Ray Computed"
        ],
        "doc_abstract": "© 2017 Götz et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.During High Dose Rate Brachytherapy (HDR-BT) the spatial position of the radiation source inside catheters implanted into a female breast is determined via electromagnetic tracking (EMT). Dwell positions and dwell times of the radiation source are established, relative to the patient’s anatomy, from an initial X-ray-CT-image. During the irradiation treatment, catheter displacements can occur due to patient movements. The current study develops an automatic analysis tool of EMT data sets recorded with a solenoid sensor to assure concordance of the source movement with the treatment plan. The tool combines machine learning techniques such as multi-dimensional scaling (MDS), ensemble empirical mode decomposition (EEMD), singular spectrum analysis (SSA) and particle filter (PF) to precisely detect and quantify any mismatch between the treatment plan and actual EMT measurements. We demonstrate that movement artifacts as well as technical signal distortions can be removed automatically and reliably, resulting in artifact-free reconstructed signals. This is a prerequisite for a highly accurate determination of any deviations of dwell positions from the treatment plan.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A combined cICA-EEMD analysis of EEG recordings from depressed or schizophrenic patients during olfactory stimulation",
        "doc_scopus_id": "85010471609",
        "doc_doi": "10.1088/1741-2552/14/1/016011",
        "doc_eid": "2-s2.0-85010471609",
        "doc_date": "2017-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Cellular and Molecular Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2804"
            }
        ],
        "doc_keywords": [
            "Constrained independent component analysis (cICA)",
            "depression",
            "Ensemble empirical mode decomposition",
            "Ensemble empirical mode decompositions (EEMD)",
            "Independent components",
            "Independent mode",
            "schizophrenia",
            "Schizophrenic patients",
            "Adult",
            "Brain",
            "Depression",
            "Electroencephalography",
            "Female",
            "Humans",
            "Male",
            "Olfaction Disorders",
            "Olfactory Perception",
            "Principal Component Analysis",
            "Reproducibility of Results",
            "Schizophrenia",
            "Sensitivity and Specificity",
            "Young Adult"
        ],
        "doc_abstract": "© 2016 IOP Publishing Ltd.Objective. We propose a combination of a constrained independent component analysis (cICA) with an ensemble empirical mode decomposition (EEMD) to analyze electroencephalographic recordings from depressed or schizophrenic subjects during olfactory stimulation. Approach. EEMD serves to extract intrinsic modes (IMFs) underlying the recorded EEG time. The latter then serve as reference signals to extract the most similar underlying independent component within a constrained ICA. The extracted modes are further analyzed considering their power spectra. Main results. The analysis of the extracted modes reveals clear differences in the related power spectra between the disease characteristics of depressed and schizophrenic patients. Such differences appear in the high frequency γ-band in the intrinsic modes, but also in much more detail in the low frequency range in the α-, θ- and δ-bands. Significance. The proposed method provides various means to discriminate both disease pictures in a clinical environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cognitive radio and spectrum sensing",
        "doc_scopus_id": "85052704335",
        "doc_doi": "10.1201/b21234",
        "doc_eid": "2-s2.0-85052704335",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Analysis techniques",
            "Directional transmission",
            "Flexible transceivers",
            "Noise model analysis",
            "Opportunistic transmission",
            "Radio frequency spectrum",
            "Software-defined radios",
            "Spectrum sensing"
        ],
        "doc_abstract": "© 2018 by Taylor & Francis Group, LLC.A cognitive radio is a flexible transceiver based on a software-defined radio that is able to sense the radio frequency spectrum (RFS) and dynamically determine what spectrum bands are unoccupied and can be used for opportunistic transmission. This chapter starts by introducing the topics of software-defined radio and the consequent cognitive radio with an overall of these architectures. The key element for cognitive radio is its ability to detect unused RFS; this technique is called spectrum sensing. In this chapter, spectrum sensing is divided in three areas; first, spectrum analysis, followed by signal detection, and then noise model. Different spectrum analysis techniques are explored and explained. In signal detection, the binary decision of determining a channel occupation is explained. The noise model analyses the determination of the threshold for that decision. The chapter ends by exploring the application of spectrum sensing in the recent trend of multiple-input, multiple-output (MIMO) communications with highly directional transmissions, using spatial spectrum sensing to allow an even more efficient reuse of spectrum resources.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Energy based clustering method to estimate channel occupation of LTE in unlicensed spectrum",
        "doc_scopus_id": "85027183330",
        "doc_doi": "10.1007/978-3-319-59773-7_33",
        "doc_eid": "2-s2.0-85027183330",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "4G mobile communication",
            "Detection algorithm",
            "Different distributions",
            "Histogram clustering",
            "Priori information",
            "Probability density distribution",
            "Radio spectrum management",
            "Unlicensed spectrum"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.In this article we propose a subtractive histogram clustering method to estimate the number of LTE users based on the uplink energy probability density distribution sensed by an RF front-end. The energy of the signal is estimated and its histogram is analyzed to determinate the number of different distributions. As the energy estimation of the sensed LTE uplink can be modeled by a Gaussian mixture, this allow us to have a priori information that help us to determine the number of users. The lowest value Gaussian distribution can be used to accurately estimate the noise floor and the remaining distributions allow us to estimate the number of LTE users and their received power.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Z-images",
        "doc_scopus_id": "85021189400",
        "doc_doi": "10.1007/978-3-319-58838-4_20",
        "doc_eid": "2-s2.0-85021189400",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Coding techniques",
            "Computer vision applications",
            "Generative model",
            "Image Descriptor",
            "Local patterns",
            "Pixel intensities",
            "Statistical information",
            "Zeckendorf theorem"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Local patterns and other patch based features have been an integral part of various computer vision applications as they encode local structural and statistical information. In this paper, we propose an image coding technique that utilizes Zeckendorf representation of pixel intensities and basic mathematical operators such as intersection, set difference, maximum, summation etc. for summarization of image regions. The algorithm produces a Z-coded image that tells about the homogeneity or the contrast in image regions with all codes in a range of 0 to 255.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Object learning and grasping capabilities for robotic home assistants",
        "doc_scopus_id": "85006399952",
        "doc_doi": "10.1007/978-3-319-68792-6_23",
        "doc_eid": "2-s2.0-85006399952",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Assistive robots",
            "Manipulation task",
            "Object categories",
            "Object grasping",
            "Object learning",
            "Perception capability",
            "Proposed architectures",
            "Qualitative evaluations"
        ],
        "doc_abstract": "© 2017, Springer International Publishing AG.This paper proposes an architecture designed to create a proper coupling between perception and manipulation for assistive robots. This is necessary for assistive robots, not only to perform manipulation tasks in reasonable amounts of time, but also to robustly adapt to new environments by handling new objects. In particular, this architecture provides automatic perception capabilities that will allow robots to, (i) incrementally learn object categories from the set of accumulated experiences and (ii) infer how to grasp household objects in different situations. To examine the performance of the proposed architecture, quantitative and qualitative evaluations have been carried out. Experimental results show that the proposed system is able to interact with human users, learn new object categories over time, as well as perform object grasping tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Concurrent 3D Object Category Learning and Recognition Based on Topic Modelling and Human Feedback",
        "doc_scopus_id": "85010297513",
        "doc_doi": "10.1109/ICARSC.2016.28",
        "doc_eid": "2-s2.0-85010297513",
        "doc_date": "2016-12-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Learning objects",
            "Limited training data",
            "Object categories",
            "Object exploration",
            "Object recognition systems",
            "On-line evaluation",
            "Service robots",
            "Visual information"
        ],
        "doc_abstract": "© 2016 IEEE.In open-ended domains, autonomous robots must have the ability to continuously process visual information, and execute learning and recognition in a concurrent and interleaved fashion. Because the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by service robots. Topic modelling approaches usually construct the topics from a training set to recognize objects. However, in open-ended domains, the data available for training increases continuously. If limited training data is used, this might lead to non-discriminative topics and, as a consequence, to poor object recognition performance. This paper proposes an object recognition system capable of learning object categories as well as the topics used to encode objects concurrently and in an open-ended manner. This system provides a robot with the capabilities to, (i) use unsupervised object exploration to construct a dictionary of visual words for representing objects and (ii) conceptualize object experiences and learn new object categories using topic modelling and human feedback. To examine the performance of the system, an on-line evaluation protocol is used to assess the performance of the system in an open-ended setting. The experimental results show the fulfilling performance of this approach on different types of objects.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Combined EMD-sLORETA analysis of EEG data Collected during a Contour integration task",
        "doc_scopus_id": "85004154185",
        "doc_doi": "10.1371/journal.pone.0167957",
        "doc_eid": "2-s2.0-85004154185",
        "doc_date": "2016-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Brain",
            "Brain Mapping",
            "Data Collection",
            "Electroencephalography",
            "Evoked Potentials",
            "Female",
            "Humans",
            "Male",
            "Task Performance and Analysis",
            "Tomography",
            "Young Adult"
        ],
        "doc_abstract": "© 2016 Al-Subari et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Lately, Ensemble Empirical Mode Decomposition (EEMD) techniques receive growing interest in biomedical data analysis. Event-Related Modes (ERMs) represent features extracted by an EEMD from electroencephalographic (EEG) recordings. We present a new approach for source localization of EEG data based on combining ERMs with inverse models. As the first step, 64 channel EEG recordings are pooled according to six brain areas and decomposed, by applying an EEMD, into their underlying ERMs. Then, based upon the problem at hand, the most closely related ERM, in terms of frequency and amplitude, is combined with inverse modeling techniques for source localization. More specifically, the standardized low resolution brain electromagnetic tomography (sLORETA) procedure is employed in this work. Accuracy and robustness of the results indicate that this approach deems highly promising in source localization techniques for EEG data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Adaptive threshold spectrum sensing based on Expectation Maximization algorithm",
        "doc_scopus_id": "84995743831",
        "doc_doi": "10.1016/j.phycom.2016.10.004",
        "doc_eid": "2-s2.0-84995743831",
        "doc_date": "2016-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Constant false alarm rate detectors",
            "Expectation - maximizations",
            "Expectation-maximization algorithms",
            "Noise estimation",
            "Number of active users",
            "Software-defined radios",
            "Spectrum sensing",
            "Statistical properties"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.In this article we address a novel method for spectrum sensing, based on the Expectation Maximization algorithm applied to the histogram of the moving average signal power. The method enables the estimation of the number of active users in a given frequency band, the power received from each user, the occupied time slots and the front-end noise floor. The proposed approach takes advantage of the statistical properties of the averaging estimator output, which allows to model the received estimated power as a Gaussian mixture. This model represents the distributions of the users transmitted signal power as well as the system noise floor. Moreover, the Gaussian with the lowest mean that is related with the noise floor, can be used to estimate an adaptive threshold for a constant false alarm rate detector. Finally, the method was validated in a Wi-Fi experimental setup, where real-world data was acquired with a software defined radio.",
        "available": true,
        "clean_text": "serial JL 276924 291210 291716 291869 291870 291871 31 Physical Communication PHYSICALCOMMUNICATION 2016-11-03 2016-11-03 2016-11-15 2016-11-15 2016-12-01T09:04:10 S1874-4907(16)30185-9 S1874490716301859 10.1016/j.phycom.2016.10.004 S300 S300.1 FULL-TEXT 2016-12-01T04:12:14.014137-05:00 0 0 20161201 20161231 2016 2016-11-03T08:27:16.908954Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid orcid primabst pubtype ref vitae 1874-4907 18744907 true 21 21 C Volume 21 8 60 69 60 69 201612 December 2016 2016-12-01 2016-12-31 2016 article fla © 2016 Elsevier B.V. All rights reserved. ADAPTIVETHRESHOLDSPECTRUMSENSINGBASEDEXPECTATIONMAXIMIZATIONALGORITHM MALAFAIA D 1 Introduction 2 Spectrum sensing with Gaussian mixture models 2.1 Signal model 2.2 Signal energy estimation 2.2.1 Energy estimator 2.2.2 Complex absolute square 2.2.3 Moving average 2.3 Analyzing the output distribution of the energy estimator 2.3.1 The PDF of the output 2.3.2 Mean and variance estimates 2.3.3 Results in order of the input signal statistics 3 EM algorithm applied to the time domain signal 3.1 EM algorithm 3.2 Illustrative example 4 Histogram based EM algorithm 5 Estimating the number of distributions on the model 5.1 Elbow method 5.2 Overfitting method 6 Application to real-world WiFi data 7 Conclusions Acknowledgments References ANDREWS 2014 1065 1082 J CISCO 2015 CISCOVISUALNETWORKINGINDEXFORECASTMETHODOLOGY20142019TECHREP PANWAR 2015 64 84 N MITOLA 2014 77 85 I UMAR 2013 148 170 R MOON 1996 47 60 T FIGUEIREDO 2003 906 916 M ZHANG 2001 45 57 Y ZHELEVA 2015 94 105 M ASSRA 2016 1229 1243 A LINDEBERG 1922 211 225 J HAZEWINKEL 1990 M ENCYCLOPAEDIAMATHEMATICS GARCIA 2008 A PROBABILITYSTATISTICSRANDOMPROCESSESFORELECTRICALENGINEERING THERRIEN 2011 C PROBABILITYRANDOMPROCESSESFORELECTRICALCOMPUTERENGINEERS BENAROYA 2005 H PROBABILITYMODELSINENGINEERINGSCIENCE HASTIE 2009 T SPRINGERSERIESINSTATISTICS ELEMENTSSTATISTICALLEARNINGDATAMININGINFERENCEPREDICTION OCHIAI 2001 282 289 H KETCHEN 1996 441 458 D LEHMANN 2014 1117 1130 R IBRAHIM 2016 2159 2170 R MALAFAIAX2016X60 MALAFAIAX2016X60X69 MALAFAIAX2016X60XD MALAFAIAX2016X60X69XD 2018-11-15T00:00:00.000Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S1874-4907(16)30185-9 S1874490716301859 10.1016/j.phycom.2016.10.004 276924 2016-12-01T04:12:14.014137-05:00 2016-12-01 2016-12-31 true 1361076 MAIN 10 56654 849 656 IMAGE-WEB-PDF 1 gr4 21081 164 205 gr2 21714 164 200 gr14 16664 164 199 pic2 33404 164 141 gr15 18442 164 204 gr8 15044 114 219 gr12 25370 163 200 gr1 12592 39 219 gr5 27829 164 199 gr13 20124 164 206 pic3 31100 164 140 gr7 18694 164 204 gr6 22915 164 199 gr9 16984 164 215 gr11 21817 162 219 pic1 26168 163 140 gr3 19354 164 204 gr10 17702 71 219 gr4 57525 278 348 gr2 67736 275 336 gr14 49608 282 343 pic2 42340 131 113 gr15 55106 282 351 gr8 41861 197 379 gr12 84261 278 340 gr1 33906 68 376 gr5 84474 284 345 gr13 58112 270 339 pic3 39551 131 112 gr7 54400 281 350 gr6 67072 281 341 gr9 52292 279 367 gr11 65924 270 364 pic1 40117 132 113 gr3 58203 281 351 gr10 48254 122 376 si25 123 11 9 si123 170 11 16 si56 1901 83 255 si21 112 11 6 si96 166 12 18 si111 693 45 103 si143 223 11 53 si71 468 19 112 si107 1291 63 186 si95 455 19 92 si166 186 21 16 si31 444 17 93 si81 2108 87 256 si53 947 21 244 si75 1242 80 134 si37 471 17 97 si7 669 15 167 si155 364 14 80 si134 795 90 100 si39 748 22 154 si4 124 8 10 si68 820 15 207 si14 170 14 17 si114 259 11 55 si10 386 15 133 si124 149 13 13 si152 977 35 245 si26 133 11 11 si15 125 8 12 si122 453 16 126 si66 249 15 30 si11 256 15 33 si119 308 12 67 si8 236 15 29 si110 1041 86 153 si99 161 12 14 si12 146 11 13 si132 1077 88 157 si9 146 11 14 si57 2421 118 348 si73 1360 20 304 si51 138 11 13 si69 1810 82 254 si28 191 11 40 si125 164 16 14 si80 452 26 70 si148 283 14 52 si151 191 19 19 si89 145 8 14 si34 352 19 68 si1 207 11 43 si30 1080 44 246 si101 182 15 19 si164 272 14 55 si94 189 14 19 si133 808 65 108 si74 603 16 122 si157 131 8 11 si72 1039 37 201 si161 242 14 35 si165 181 11 38 si120 397 12 88 si153 446 21 80 si91 896 45 172 si47 1315 90 196 si70 399 14 97 si6 499 15 114 si65 294 12 65 si77 1130 42 173 si27 142 11 12 si40 114 8 9 si13 165 14 15 si36 353 17 74 si118 136 11 14 si42 520 19 104 si105 416 14 123 si38 618 21 131 si162 594 14 136 si49 532 26 101 si3 142 11 12 si130 135 12 10 si50 1191 42 226 si24 171 13 18 si131 959 90 113 si35 431 19 84 si104 208 15 27 si52 163 14 19 si102 203 19 19 si22 123 8 10 si103 179 15 18 si55 599 24 123 si98 334 17 70 si58 126 9 10 si45 1268 74 169 si92 684 15 198 si33 190 19 20 si76 1382 42 246 si128 1406 63 198 si32 359 19 73 si158 114 10 8 si113 314 11 67 si82 1501 48 257 si106 260 15 35 si109 820 86 96 si23 144 11 14 si167 318 11 70 PHYCOM 346 S1874-4907(16)30185-9 10.1016/j.phycom.2016.10.004 Elsevier B.V. Fig. 1 Functional diagram of the used energy estimator. Fig. 2 Effect of the logarithm function in the output distribution compared to a Gaussian distribution. Fig. 3 Energy estimation of the generated data. Fig. 4 Analysis of the convergence of each distribution mean. Fig. 5 Responsibility of each distribution in discrete-time. Fig. 6 Comparison of the data histogram and the EM estimated distribution. The data was generated according with the parameters of Table 1. Fig. 7 ROC curve comparison between a fixed noise floor estimation with variable uncertainty and the EM noise floor estimation. Fig. 8 Elbow method used to determine the number of distributions present on the data. Fig. 9 MSE evolution (full line) for an increasing number of distributions. The inflection point is clearly visible in derivative (dashed line) of the MSE for K = 4 . Fig. 10 Diagram of the WiFi setup. Fig. 11 On the top the used data segment. On the bottom the MSE derivative of the elbow method where a clear inflection point is not present for K = 4 . Fig. 12 On top the mean power of the identified distributions. On bottom the number of detected distributions and the number obtained by visual inspection. Fig. 13 Responsibilities of each Gaussian distribution on a signal segment. Fig. 14 Calculation of the dynamic threshold based on minimum variance estimation. Fig. 15 802.11 signal in time and threshold. Table 1 Generated test signal components. Signal Power (dB) Estimated power (dB) Appearance order Noise floor −100 −100.08 3rd Low power −98 −98.04 1st Medium power −96 −95.94 2nd and 5th High power −94 −93.89 4th Table 2 Comparative of FLOP by EM implementation. Number of points Histogram (MFLOPS) Time (MFLOPS) 100 2.4 2.2 150 2.4 3.3 200 2.4 4.4 Full length article Adaptive threshold spectrum sensing based on Expectation Maximization algorithm Daniel Malafaia ⁎ José Vieira Ana Tomé IEETA/DETI-Universidade de Aveiro, Portugal IEETA/DETI-Universidade de Aveiro Portugal ⁎ Corresponding author. In this article we address a novel method for spectrum sensing, based on the Expectation Maximization algorithm applied to the histogram of the moving average signal power. The method enables the estimation of the number of active users in a given frequency band, the power received from each user, the occupied time slots and the front-end noise floor. The proposed approach takes advantage of the statistical properties of the averaging estimator output, which allows to model the received estimated power as a Gaussian mixture. This model represents the distributions of the users transmitted signal power as well as the system noise floor. Moreover, the Gaussian with the lowest mean that is related with the noise floor, can be used to estimate an adaptive threshold for a constant false alarm rate detector. Finally, the method was validated in a Wi-Fi experimental setup, where real-world data was acquired with a software defined radio. Keywords Spectrum sensing Noise estimation Expectation Maximization Signal detection 1 Introduction The Long-Term Evolution (LTE) is the current standard for high-speed wireless communication systems and has reached its maturity. The new releases only consider incremental improvements to the standard without any additional spectral bands [1]. In the recent Visual Network Index (VNI) report [2], it is foreseen that an incremental approach will be unable to meet the future demands of the mobile data by 2019 when an increase of traffic is expected compared to 2014. Therefore, with a limited amount of available spectrum and the necessity of increasingly higher data-rates, spectrum sharing became a relevant research topic in 5G mobile communication systems [3]. This approach allows the dynamic assignment of spectrum resources to RF devices in an opportunistic way, even for frequency bands that may be already assigned to primary users. This is especially true provided that it is possible to prevent collisions between opportunistic users and the primary users. However, this strategy presents a huge challenge to spectrum regulators in order to control interference. The future spectrum sharing implementations can be used in licensed spectrum if the opportunist users sense any incumbent signals, being constantly aware of the medium in order to avoid conflict with the primary users that have priority in that frequency band [4]. It can also be applied to an unlicensed spectrum where users need to sense the occupation of these unlicensed bands in order to allocate their communication data on channels without interfering with other users. For sharing unlicensed and licensed spectra, the key element will be the need to sense the occupation of the spectral bands by opportunistic RF devices. Spectrum sensing is thus the key mechanism for any multiple access communication medium. It avoids collisions between users that share the access to the medium, and reduces the contention delay experienced in dense user environments. Even in a scenario where a central database assigns spectrum resources to the users taking into consideration frequency, time and space, the spectrum sensing will be mandatory in order to supervise the behavior of the RF systems. This option is currently tested in pilot project by the Microsoft Spectrum Observatory [5]. The main goal of spectrum sensing is to determine when a certain frequency band is being used [6]. In this work the proposed channel utilization analysis is based on the Expectation Maximization (EM) algorithm. The EM algorithm is usually employed for the extraction of unknown parameters where the observed data set has a known distribution function. The most common estimation problem is usually to obtain the mean and variance of a given set of signals in the presence of noise [7]. For this reason, EM has typically been associated with reconstruction and segmentation of data, with a clear emphasis on image processing [8,9]. More recently, the application of EM has been studied for Spectrum Sensing. In [10] the instantaneously received power of a single transmitter is detected. The received signal suffers multi-path propagation that can be modeled approximately by a gamma distribution and the parameters estimated with the EM. The noise-floor is also approximated to a gamma distribution to account for non-Gaussian sources of noise. In [11] measured spectra are acquired by a spectrum analyzer and evaluated using EM. In this article the authors state that the amplitude of the transmitted wireless signals follows a Rayleigh distribution while the noise follows a Gaussian distribution. This statement allows the use of a Rayleigh–Gaussian mixture as a model for the analyzed data with the EM algorithm. In [12] EM is used for a multi-antenna Spectrum Sensing network to detect the primary user transmissions. In the article a Rayleigh fading channel is assumed but the noise and the signal are both modeled by Gaussian distributions due to power averaging. The discrete Fourier Transform of the data is then analyzed by the EM algorithm using a complex Gaussian Distribution Mixture model. The method expects a single transmission in each channel and assumes perfect knowledge of the frequency responses of each receiver channel and the noise floor variances. In the method proposed in this paper as the signal analysis is performed after a moving average of the signal energy, the central-limit theorem ensures that the distribution of the signals and noise will be close to the Gaussian mixture model. Allowing to sense both the signal received from a predominant direct path as well as those scattered by a heavy multi-path channel. The proposed method works in multi-transmitter scenario and does not need to know the number of transmitters in a given frequency channel due to a priori knowledge of the energy estimation variance. This method also makes a novel use of the EM algorithm on the RF data energy estimation, by using a histogram to identify the spectrum occupation through time allowing for a higher algorithmic efficiency. This method is also able to determine the number of active users on a given channel and the time slot that each one occupies based on the analysis of the signal received energy. The noise-floor is also dynamically estimated from the EM and is used for a Constant False Alarm Rate (CFAR) threshold calculation for signal detection. 2 Spectrum sensing with Gaussian mixture models 2.1 Signal model Consider a spectrum sensing front-end that is constantly analyzing the RF spectrum. The sensed spectrum is divided into multiple sub-bands, where each sub-band may be vacant (only noise-floor is present) or it is occupied by one of U possible users. We assume that each possible user will be sensed by the RF front-end with a unique power level that is due to the uniqueness of each path from the transmitting user to the sensing unit. At a particular instance of time n , the received signal will then be one of the U users or, if no one is transmitting, the received signal will be the background noise floor. This scenario is modeled by the following hypothesis: H o : x ( n ) = w ( n ) (1) H k : x ( n ) = s k ( n ) + w ( n ) where x ( n ) is the discrete received time signal in the front-end, s k denotes the signal transmitted by the user k = 1 , 2 , … , K − 1 and w ( n ) the additive zero-mean white Gaussian noise. A sensing interval, encompassing N samples, will be a sequence of received user signals, represented by H i , and also a noise floor, described by H o . Let x denote the signal vector with a sample size of N , this vector is given by the concatenation of a set of H i and H o states. The vector x contains a mixture of various transmitted users signals and the noise floor. As we assume that each sensed user has a unique power level, an energy estimation analysis of the x data vector will be able to differentiate the different users of the spectrum. 2.2 Signal energy estimation 2.2.1 Energy estimator In order to analyze the occupation, the spectrum can be divided into individual sub-bands. This spectrum division can be performed, for example, by a band-pass filter-bank, with the necessary bandwidth for the required specifications. For each individual sub-band, the filtered signal can be analyzed to detect occupation. On the other hand, each i user of the shared medium will transmit different data at a different power level. Then, to characterize the energy in the sub-band, the processing steps illustrated in Fig. 1 are proposed. The input x is the acquired complex signal with component in phase ( x I ) and quadrature ( x Q ). The complex absolute square of the signal is calculated in order to obtain its instantaneous energy. The instantaneous energy is filtered by an L order moving average. The last step ensures that the output of the filter has an approximate Gaussian distribution [13]. Finally, the smoothed energy estimation is converted to decibels. Considering the transmission scenario described in Eq. (1) the smoothed energy can correspond to either user or noise. The probability density function of the output y can be modeled as a mixture of K Gaussian distributions corresponding to K − 1 active users and the noise-floor. The output can be modeled by a Gaussian Mixture Model, where each Gaussian has an unknown mean, which corresponds to its received energy, and a variance that will only depend on the order L of the moving average as will be proven latter. 2.2.2 Complex absolute square For convenience of calculation, let us assume the front-end input as zero-mean Gaussian complex data, then the absolute square of the input is given by the sum of the squares of the phase and quadrature part of the signal. (2) c = | x | 2 = ( x I 2 + x Q 2 ) 2 = x I 2 + x Q 2 . By definition the Chi-Squared distribution is the result of squaring a standard normal random variable [14]. So, if x ∼ N ( 0 , σ 2 ) , x 2 ∼ σ 2 χ 1 2 , where χ 1 2 is the Chi-Squared distribution with 1 degree of freedom. The mean and the variance of Chi-Squared are E [ χ 1 2 ] = 1 and Var [ χ 1 2 ] = 2 , respectively. The mean of the squared signal is given by E [ x 2 ] = σ 2 and the variance by Var [ x 2 ] = 2 σ 4 [15]. By assuming that the two components of the complex value are normally distributed independent variables x I , x Q ∼ N ( 0 , 1 2 σ x 2 ) , i.e. both with zero mean and the same variance. Then, the square of the quadrature and phase components of the signal will follow the Chi-Squared distribution x I 2 , x Q 2 ∼ χ 1 2 ( 1 2 σ x 2 , 1 2 σ x 4 ) with the mean and variance calculated as previously explained. The output of the first block, c , is the sum of the squared phase and quadrature components. Therefore, the c signal is the sum of two independent components which have identical Chi-Squared distributions. The distribution of the signal at the output of the first block of Fig. 1 is also a Chi-Squared distribution c ∼ χ 1 2 ( σ x 2 , σ x 4 ) . Therefore the mean is equal to the variance of the input signal, while the variance is the square of the input variance. 2.2.3 Moving average By averaging the signal with a high L order filter the central-limit theorem is applicable and the filter output distribution can be modeled by a Gaussian distribution [13]. Assuming that the input corresponds to L independently repeated measurements of the same user transmission process, i.e., the data points are independent and identically distributed (i.i.d.), then the expected value of the sample mean, calculated in the moving average, is given by [16], (3) E [ a ( n ) ] = 1 L E [ ∑ n = 0 L − 1 c ( n ) ] = σ x 2 . Thus the expected value is the variance of the original complex input signal x . The variance given by the moving average is given by [16], (4) Var [ a ( n ) ] = 1 L 2 ∑ n = 0 L − 1 Var [ c ( n ) ] = σ x 4 L thus allowing to reduce the variance of the input signal by a factor L . Finally, we get the distribution for the moving average filter as the following normal distribution, a ∼ N ( σ x 2 , σ x 4 L ) . 2.3 Analyzing the output distribution of the energy estimator The estimated energy value, from the moving average is converted to decibels. The distribution function of this new variable can be computed but the mean and the variance can only be approximated. For this purpose a Taylor series approximation of the energy estimator distribution function is proposed. 2.3.1 The PDF of the output As shown before, the moving average filter output signal follows a normal distribution given by, (5) f ( a ) = 1 σ 2 π exp ( − ( a − μ ) 2 2 σ 2 ) where the mean is μ and variance is σ 2 . Then applying the following logarithmic transformation we obtain (6) g ( a ) = y = 10 log 10 ( a ) ⇔ a = 1 0 y 10 . The distribution function of the estimated energy output, y , is computed as f ( y ) = f ( a ( y ) ) | d a d y | [17]. The two terms of this equation are the following: (7) { f ( a ( y ) ) = 1 σ 2 π exp ( − ( a − μ ) 2 2 σ 2 ) d a d y = ln ( 10 ) 1 0 y 10 − 1 . The probability distribution function of the output of the energy estimation system is then, (8) f ( y ) = f ( a ( y ) ) | d a d y | = ln ( 10 ) 1 0 y 10 − 1 1 σ 2 π exp ( − ( 1 0 y 10 − μ ) 2 2 σ 2 ) this distribution function will be analyzed next. 2.3.2 Mean and variance estimates The random variable a has a Gaussian distribution with a positive mean and, for a high-order ( L ) filter, the a values will never be zero. Note that the variance of a reduces with L , but the mean stays constant. So the concentration of a around its mean is proportional to the order L of the moving average. This allows to replace the logarithm in a short interval by a linear approximation with a small error. To evaluate the approximation, we consider the signal with mean 0.5 (i.e., ≈ − 3 dB ) and define two different ratios between the signal mean and variance. Fig. 2 shows the results for two different ratios, 50 and 1000. For a ratio of 50 the distribution has a Kurtosis value of 4.3 and for the second ratio distribution the Kurtosis value is 3.02. This demonstrates that for a high ratio between the mean and the variance the logarithm’s output can also be approximated by the Gaussian distribution. This effect is also illustrated in Fig. 2, where the distribution of the largest ratio is closer to the Gaussian fit. The distribution is concentrated around its positive mean, avoiding an approximation to the value of zero and the consequent impact in the derivatives of the logarithmic function g ( a ) . This allows the use of the Taylor series to approximate the moments of the logarithm of a . Considering an approximation with the Taylor expansion, using three terms, the first two moments are estimated as follows: • First moment: (9) E [ g ( a ) ] = E [ g ( μ a + ( a − μ a ) ) ] (10) E [ g ( a ) ] ≈ E [ g ( μ a ) + g ′ ( μ a ) ( a − μ a ) + 1 2 g ″ ( μ a ) ( a − μ a ) 2 ] . As E [ a − μ a ] = 0 and E [ a − μ a ] 2 = σ a 2 we can then write that, (11) E [ g ( a ) ] ≈ g ( μ a ) + g ″ ( μ a ) 2 σ a 2 . • Second moment: Analogously to the first moment we can write [18], (12) Var [ g ( a ) ] ≈ ( g ′ ( E [ a ] ) ) 2 Var [ a ] = ( g ′ ( μ a ) ) 2 σ a 2 . The first and second moments are proportional to second and first order derivatives. As g ( a ) = 10 log 10 ( a ) then the first and second derivatives can be obtained. (13) { g ′ ( a ) = 10 ln ( 10 ) a g ″ ( a ) = 10 ln ( 10 ) a 2 . Substituting the moments by their Taylor expansion, we can finally obtain an estimate of the expected value and variance. (14) E [ g ( a ) ] ≈ 10 log 10 ( μ a 2 ) − 10 σ a 2 2 μ a 2 ln ( 10 ) (15) Var [ g ( a ) ] ≈ 100 σ a 2 μ a 2 ( ln ( 10 ) ) 2 . 2.3.3 Results in order of the input signal statistics As the process g ( a ) that generates y , has in the input the values from the output of the moving average of the system under study, with a random value that can be approximated by N ( σ x 2 , σ x 4 L ) we have, (16) E [ g ( a ) ] ≈ 10 log 10 ( σ x 2 ) − 10 σ x 4 L 2 σ x 4 ln ( 10 ) 2 = 10 log 10 ( σ x 2 ) − 10 2 L ln ( 10 ) (17) Var [ g ( a ) ] ≈ 100 σ x 4 L σ x 4 ( ln ( 10 ) ) 2 = 100 L ln ( 10 ) 2 . Note that for large L , the mean of y is the value in decibels of the expected value of the energy estimator input x . The variance of the output y is only dependent on the order L of the moving average. The a priori knowledge of a fixed variance in the energy estimation output will be later used for improving the results of the EM algorithm. 3 EM algorithm applied to the time domain signal In the spectrum sensing model as described by Eq. (1) it is implicitly assumed that a long segment of data samples is formed by the concatenation of sub-segments where either one user is using the channel or noise is present. Therefore, as the energy estimator y can be approximated by a Gaussian distribution, the suitable model for the probability distribution function is the Gaussian Mixture Model (GMM). The mean of each Gaussian would then represent a user and the noise floor can be related with the lowest Gaussian mean estimated. Assuming this model the EM algorithm is used to estimate unknown parameters of the GMM. The EM algorithm was modified to allow histogram-based estimation of the GMM unknown parameters. 3.1 EM algorithm Given a set of observations, the Expectation Maximization (EM) algorithm aims to compute the maximum likelihood estimation of the unknown parameters of a statistical model. Hence, the available data is assumed to be a set of observations drawn from a mixture of different probability distributions. For the energy estimator we consider the data modeled by a GMM m explaining each data value y as follows, (18) m ( y | Θ ) = ∑ k = 1 K π k Φ k ( y | θ k ) where Θ = { θ 1 , θ 2 … θ K , π 1 , … π K } denotes the set of parameters of the K normal distributions, with normal density given by Φ k that is characterized by the parameters θ k = { μ k , σ k 2 } and π k is the weight of each distribution for the data y . The weights are constrained to sum to one, e.g ∑ k π k = 1 . The algorithm starts with initial guesses for each unknown parameter of the Θ set. A common way to set up the initial values is to choose K random samples from the data points for the means μ ˆ k and the overall data variance as the initial value for σ ˆ k 2 . For the parameters π ˆ k an equal proportion, 1 / K , can be initially attributed to each distribution. After the initial guesses, the algorithm proceeds by recursively applying the following two steps: • Expectation step: For each data value, given the parameters of the mixture model, compute the a posteriori probabilities of belonging to each distribution. Using the Bayes Rule, the probability that each data value, y i , i = 1 , 2 , … , N , belongs to a Φ k ( . ) , also called responsibilities, is given by (19) γ ˆ k , i ( Φ k | y i ) = π ˆ k Φ k ( y i | θ k ) ∑ l = 1 K π ˆ l Φ l ( y i | θ l ) . • Maximization step: The estimated probabilities on the Expectation step are integrated in the maximum likelihood estimation of parameters of the normal distributions [19]. Therefore the updates depend on the weighted values. The aim of the maximum likelihood estimation is to compute the parameters Θ that give the best fit to the given set of independent data observations. (20) μ ˆ k = ∑ i = 1 N γ ˆ k , i y i ∑ i = 1 N γ ˆ k , i (21) σ ˆ k 2 = ∑ i = 1 N γ ˆ k , i ( y i − μ ˆ k ) 2 ∑ i = 1 N γ ˆ k , i (22) π ˆ k = ∑ i = 1 N γ ˆ ( k , i ) N . The Expectation step and Maximization step are iterated until the convergence criterion is met (e.g., a maximum number of iterations). The algorithm’s output will then give us the estimation of the mean value, variance and mixture proportion of each normal distribution on the analyzed model. 3.2 Illustrative example To test the described method we have generated an artificial signal, encompassing three users where each one is being received with a different power. Moreover a period where only noise floor is present, which has the lowest power level, was also defined. The users and the noise floor are interleaved in order to create the test signal, as described in Table 1 . The test signal x is divided into 5 time slots with N = 1000 samples. Each user is present in different time slots, one of the users occupies two time slots, and one of the time slots has no user transmitting (noise floor). The data is generated as a complex Gaussian random variable. Due to its simplicity, it is used as an approximate model for both the noise floor and the received signal. Notice that orthogonal frequency-division multiplexing (OFDM) signals, as in 802.11n, are expected to have Gaussian distributions [20]. The power of the generated signals is given in Table 1, and is then evaluated with the energy estimator as described in Fig. 1 with L = 100 . The output y of the estimator is shown in Fig. 3 . The data is then analyzed with the Expectation Maximization algorithm in order to obtain the unknown parameters of the normal distributions present in the model. The stopping criteria in the EM are 100 iterations. Fig. 4 shows the evolution of the estimated mean for each of the K = 4 distributions. After convergence, the mean values of the Gaussian distributions are −100.08, −98.04, −95.94 and −93.89 dB. After the EM convergence, the responsibility parameter for each sample is also informative. This parameter gives us the channel occupation through time, and also identifies the individual user within a specific time slot. The responsibilities were smoothed with a moving average filter or order L = 100 , and are illustrated in Fig. 5 . The EM output can be analyzed by using the estimated parameters from the method to generate the probability density function of the mixture. These distributions are then compared with the input data histogram. Fig. 6 compares the two approaches. To analyze the performance of the EM algorithm, the receiver operating characteristic (ROC) curve of the energy detection (ED) was calculated from the previously generated simulation. In this evaluation a threshold with a variable probability of false alarm is applied to different uncertainty values Δ of the noise power estimation. The calculated curves are compared with the performance obtained when using the estimated noise floor with the Expectation Maximization method. In Fig. 7 the various ROC curves are compared. The energy detection that used the noise power estimation with the EM algorithm has, as expected, a worse performance than the ideal ED at Δ = 0 dB , but shows a similar performance to an ED with Δ = 0.25 dB . 4 Histogram based EM algorithm In the previous section the standard EM algorithm in the time domain was described. The parameters of the Gaussian mixture model were estimated directly from the energy estimation output. Therefore the method allows not only to determine these parameters, but also to identify the Gaussian component to which a data point belongs. To obtain a more efficient approach an algorithm version based on the data histogram was developed. The EM algorithm will then be applied to the bin values and not to all energy values in a segment of data. That way, as the number of bins is smaller than the number of data points in the energy estimation output, the complexity, either in terms of memory and operations, is reduced significantly. The disadvantages are the loss of precision in identifying the exact sample where the distributions are present and the quantization error induced by the histogram intervals. The EM algorithm applied to the time domain can be easily adapted to work with histogram data. Let y i , i = 1 , 2 , … , N be the input data, then y ̄ j , j = 1 , 2 , … , M denotes the center of the bins of the histogram, and M gives the number of intervals. The number of occurrences in each interval is given by o j . • Expectation step: For each histogram bin, compute the a posteriori probabilities of belonging to each distribution given the parameters of the mixture model. Using the Bayes rule, the probability that a bin y ̄ j , with o j occurrences, belongs to a Φ k ( . ) , also called responsibilities, is given by (23) γ ˆ k , i ( Φ k | y ̄ j ) = π ˆ k Φ k ( y ̄ j | θ k ) o j ∑ l = 1 K π ˆ l Φ l ( y ̄ j | θ l ) o j . • Maximization step: The estimated probabilities on the Expectation step are integrated in the maximum likelihood estimation of parameters of the normal distributions. Therefore the updates depend on the weighted values. The aim of the maximum likelihood estimation is to compute the parameters Θ that give the best fit to the given set of independent data observations. The calculated mean for k will then be the weighted mean of the histogram values using the calculated responsibilities for that distribution. (24) μ ˆ k = ∑ j = 1 M γ ˆ k , j y ̄ j o j ∑ j = 1 M γ ˆ k , j o j . The variance will be a weighted sum of the variances within each bin (25) σ ˆ k 2 = ∑ i = 1 N w k , j ( y ̄ j − μ ˆ k ) 2 ∑ j = 1 N w k , j where these weights are given by (26) w k , j = γ ˆ k , j o j ∑ j = 1 N γ ˆ k , j o j . The mixing coefficients of GMM are calculated by (27) π ˆ k = ∑ j = 1 N γ ˆ k , j o j ∑ j = 1 N o j . Once again the expectation and maximization steps are iterated until the convergence criterion is met. The algorithm’s output will then give us the estimation of the mean value, variance and mixture proportion of each normal distribution on the analyzed model from the histogram data. The performance improvement in the EM can be observed by comparing the number of FLoating-point OPerations (FLOP) using both methods. The complexity, in terms of FLOP, of the algorithm is measured using the data described in Section 3.2 but with a variable number of samples N . The number of bins used on the EM based histogram is one hundred. Table 2 presents the results. As expected for the histogram implementation the number of FLOP remains constant with increasing number of points, as the histogram will always have a fixed number of outputs. Using the time dependent data directly, the number of FLOPS is smaller but quickly matches the value for the histogram implementation. The time implementation complexity will increase linearly with the analyzed number of points. 5 Estimating the number of distributions on the model The EM algorithm assumes that the number K of distributions is fixed and known a priori. If there is no information on the number of distributions present on the received signal, then an alternative solution needs to be addressed. The next sections present two different strategies to estimate the GMM model order, i.e., the number K of distributions. 5.1 Elbow method The elbow method is based on the comparison of two Cumulative Distribution Functions (CDF) [21]. One of the CDFs is estimated using the GMM model, for a variable order K , obtained by the EM algorithm and the second is obtained directly from the complex-valued input data. The Mean Squared Error (MSE) between the two CDFs is then estimated for increasing values of K , that are part of the mixture model used in the EM algorithm. The optimal value for K corresponds to the largest slope variation in the MSE for the different values of K . A block diagram describing the elbow method process is described in Fig. 8 . In this method the input of the EM algorithm is the output of the energy estimator, as described in the block diagram of Fig. 1. The EM algorithm is repeated for K varying from 1 → 10 Gaussian distributions in the GMM model until the error reaches an elbow point. The CDF of the GMM, estimated by the EM, is then compared to the cumulative histogram of the energy estimator output. Fig. 9 illustrates the MSE between the CDFs for the different values of K using the simulated data described before (Section 3.2). Furthermore, the largest value of the derivative of the MSE error is also for K = 4 . However, as will be seen in Section 6, the elbow method does not perform well with real world data, as the latter follows the GMM model only approximately. 5.2 Overfitting method In this section we propose a more robust method to estimate the number K of Gaussian mixtures even when the data does not follow a low order GMM. From Eq. (17) we can verify that the variance of the Gaussians that compose the mixture is a function of the order L of the moving average. We can take advantage of this a priori information by considering a large number of Gaussians to overfit the number of components of the GMM. The distributions with a variance largely different from the theoretical value given by Eq. (17) are then discarded. In this overfitting methodology, the GMM will have an initial order of K o ≪ K which corresponds to the number of transmitting users and noise floor. Using (17) we can distinguish between Gaussian distributions, from noise-floor and transmitting users, from other noise sources. This method can be compared to a model selection based on hypotheses testing [22], where the testing hypotheses are the known variance of each Gaussian distribution used on the overfitting. For a robust fit of the analyzed data histogram, every Φ k of the GMM whose variance is not close to the theoretically expected value or its contribution to the overall model is negligible, is discarded. Therefore after the overfitted model is obtained from the EM, the Φ k of the GMM is chosen according to the following criteria. 1. The distribution’s variance σ k 2 should be in the range defined by, (28) 1 t S y 2 < σ k 2 < t S y 2 , k ∈ { 1 , 2 , … , K } where Var [ y ] = S y 2 is the calculated theoretical variance of y calculated by using Eq. (17), t ∈ ] 1 , + ∞ [ is a tolerance factor to accommodate the model imprecisions and the overfitting initialization. 2. Additionally, the distribution’s responsibility, π k , should be larger than a threshold α . Both t and α are two user defined parameters according to the experimental conditions. 6 Application to real-world WiFi data A setup for a real-world application of the EM algorithm for spectrum sensing was assembled with two smartphones connected through WiFi to an Access Point as seen in Fig. 10 . The sensing device is a USRP B200 software defined radio platform controlled by a personal computer. The test was conducted indoors in a dense urban environment. In this setup the USRP is in the same room as the first smartphone. The router is further away in an adjacent room and the second mobile phone, that is the farthest from the USRP, is in a third separated room. The equipment does not change position during the setup and there are no moving elements in the setup. Due to the different distances of these three devices to the USRP, the received signals from each equipment will have different power levels. The USRP B200 acquired 107 samples during 2.5 s using a sampling frequency of 4 MHz, centered at 2432 MHz (802.11—Channel 5). This data was analyzed by the energy estimator described in Fig. 1 with a moving average of L = 100 coefficients, which corresponds to an integration time of 25 μ s . Since the length of the impulse response in an indoor environment is generally of a few hundred nanoseconds [23], this integration time allows us to ignore the multi-path fading effect. We can approximate the variance for the energy estimator output by using (17), which for our case is approximately given by S L = Var [ e ] ≃ 0.189 . For detection purposes, the histogram version of the EM algorithm presented in Section 4 was used. In this method, the output of the energy estimator is evaluated by forming a total of 1000 histograms, each encompassing 105 samples. Each histogram has 1000 bins, uniformly distributed in amplitude. Each histogram was analyzed with the EM algorithm with a stopping criterion of 100 iterations. In Fig. 11 , we can observe the application of the elbow method to one of the data segments. It is not possible to obtain a conclusive answer for the number K of distributions, as the elbow point is not present on the MSE plot. A similar behavior is obtained for the other data segments. The absence of a clear inflection point is due to an acquisition in a non controlled urban environment, where interference from other Wi-Fi devices is to be expected. For comparison we applied the overfitting method, as explained in Section 5.2, to the same signal. Each data segment is evaluated with a histogram of 1000 equally distributed bins. In spite of knowing the number of users in the experimental set-up, we consider to initially overfit the data with K o = 10 Gaussians. Taking advantage of a priori knowledge of the variance, the nuisance parameter t = 5 was set. This means that distributions with a variance five times lower or five times higher than the expected S y 2 will be discarded. The other nuisance parameter was chosen as α = 0.001 to discard Gaussian distributions with a small contribution (responsibility) to the GMM. By repeating this process for all the segments, we obtained the results shown in Fig. 12 . In the experiment three WiFi devices were transmitting information in a non-controlled way. Due to this, for each segment a different utilization of the channel occurs for each WiFi device. To overcome this indeterminacy, each segment was visually analyzed thus determining the number of expected users for each segment. As both estimations are noisy, a median filter of order ten was used to obtain a more stable result. The results show a visible tendency for the detected Gaussian distributions to concentrate around four different power values. The three highest concentration values can be attributed to the two smartphones and the access point transmissions. The lowest value is the contribution of the noise floor to the GMM. The observed large dispersion near the noise floor is explained by the nature of the test. This test was performed in a real urban environment and other remote access points are being received, with a low power, during the experiment. The mean of the number of detected distributions is 4.16, which is very close to the true value of K = 4 . The number of estimated distributions in each segment is shown on the bottom of Fig. 12. While the curves of the number of detected distributions are not exactly identical, they show a similar behavior, reliably determining the expected number of distributions on the experiment. The responsibilities of each distribution of the GMM in the data are calculated by the EM algorithm. These responsibilities can be used to determine which time slots are being occupied by each user. They can be seen in Fig. 13 . The WiFi channel is not constantly occupied. In an analyzed segment of data, there may be a time interval with no transmissions. In this situation, the distribution with the smallest power models the background noise. This allows to have a robust estimation of the background noise power level and to determine the detection threshold to implement a Constant False Alarm Rate (CFAR) detector. For this application, the threshold is defined to have a CFAR of 0.1%. The instantaneous noise floor value is estimated using the lowest Gaussian distribution given by the overfitting method in the last ten segments. The distribution mean and variance are then used to extract the threshold for the defined CFAR. Fig. 14 illustrates the obtained estimation of the noise floor level and also the corresponding threshold. For reference the noise level of the USRP front-end was measured without any transmission and the measured mean value for the noise floor was of −95.5 dB, consistent with the estimation obtained by the proposed method. For each new segment of data a new threshold is calculated based on the instantaneous noise floor estimation. This value is used to detect white spaces on the analyzed frequency band as illustrated in the data segment presented in Fig. 15 . The calculated CFAR threshold, based on the noise floor estimation, allows to quickly detect the occupancy of the transmission band. 7 Conclusions In this paper, we used the Expectation Maximization algorithm to automatically extract useful information from energy spectrum sensing. The algorithm analyzes the logarithm of the averaged signal energy estimation, allowing a better dynamic range in the calculated histogram. It was shown that for a sufficiently long integration time, a GMM can be assumed for the RF signal acquisition and the EM algorithm can be used. This method dynamically estimates the noise floor from the input signal, provided that there are regular periods with no transmissions on the channel. It was also shown that the EM algorithm is able to obtain the number of transmitters, their relative power and an estimation of the amount of time each one is occupying the channel. The classical EM algorithm was also adapted to analyze histogram data instead of raw data samples, making the algorithm more amenable to real time implementations. The method allows the implementation of a CFAR detector to quickly determine white spaces on the spectrum. The results from the real-world environment WiFi test, acquired with a USRP B200 software defined radio, attest the validity of the proposed methodology. Acknowledgments The authors would like to acknowledge the support of this work by the Portuguese Foundation of Science and Technology (FCT) under the projects EXCL/EEI-TEL/0067/2012 (CREaTION) and UID/CEC/00127/2013. References [1] J.G. Andrews S. Buzzi W. Choi S.V. Hanly A. Lozano A.C.K. Soong J.C. Zhang What will 5G be? IEEE J. Sel. Areas Commun. 32 6 2014 1065 1082 10.1109/JSAC.2014.2328098 arXiv:arXiv:1405.2957v1 [2] Cisco Cisco Visual Networking Index: Forecast and Methodology, 2014-2019, Tech. Rep 2015 URL [3] N. Panwar S. Sharma A.K. Singh A survey on 5G: The next generation of mobile communication Phys. Commun. 18 2015 64 84 10.1016/j.phycom.2015.10.006 arXiv:1511.01643 [4] I. Mitola J. ~ J. Guerci J. Reed Y.-D. Yao Y. Chen T. Clancy J. Dwyer H. Li H. Man R. McGwier Y. Guo Accelerating 5G QoE via public-private spectrum sharing IEEE Commun. Mag. 52 May 2014 77 85 10.1109/MCOM.2014.6815896 [5] Microsoft Spectrum Observatory. URL [6] R. Umar A.U.H. Sheikh A comparative study of spectrum awareness techniques for cognitive radio oriented wireless networks Phys. Commun. 9 2013 148 170 10.1016/j.phycom.2012.07.005 [7] T.K. Moon The expectation-maximization algorithm IEEE Signal Process. Mag. 13 1996 47 60 10.1109/79.543975 [8] M.a.T. Figueiredo R.D. Nowak An EM algorithm for wavelet-based image restoration IEEE Trans. Image Process. 12 8 2003 906 916 10.1109/TIP.2003.814255 a publication of the IEEE Signal Processing Society [9] Y. Zhang M. Brady S. Smith Segmentation of brain MR images through a hidden markov random field model and the expectation-maximization algorithm IEEE Trans. Med. Imaging 20 1 2001 45 57 [10] K. Wanuga, N. Gulati, H. Saarnisaari, K.R. Dandekar, Online learning for spectrum sensing and reconfigurable antenna control, in: 9th. International ICST Conference on Cognitive Radio Oriented Wireless Networks and Communications, CROWNCOM, 2014, pp. 508–513. [11] M. Zheleva R. Chandra A. Chowdhery A. Kapoor P. Garnett TxMiner: Identifying transmitters in real-world spectrum measurements Dyn. Spectrum Access Netw. (DySPAN) i 2015 94 105 [12] A. Assra J. Yang B. Champagne An EM approach for cooperative spectrum sensing in multi-antenna CR networks IEEE Trans. Veh. Technol. 65 3 2016 1229 1243 10.1109/TVT.2015.2408369 [13] J.W. Lindeberg Eine neue Herleitung des Exponentialgesetzes in der Wahrscheinlichkeitsrechnung Math. Z. 15 1 1922 211 225 URL [14] M. Hazewinkel Encyclopaedia of Mathematics 1990 Springer Science [15] D. Williams, V. Madisetti, Digital signal processing Handbook. URL [16] A. Garcia Probability, Statistics, and Random Processes for Electrical Engineering 2008 [17] C. Therrien M. Tummala Probability and Random Processes for Electrical and Computer Engineers second ed. 2011 CRC Press [18] H. Benaroya M.H. Seon M. Nagurka Probability Models in Engineering and Science 2005 10.1198/tech.2006.s441 [19] T. Hastie J. Friedman R. Tibshirani The elements of statistical learning: Data mining, inference and prediction Springer Series in Statistics 2009 [20] H. Ochiai H. Imai On the distribution of the peak-to-average power ratio in OFDM signals IEEE Trans. Commun. 49 2 2001 282 289 10.1109/26.905885 [21] D. Ketchen Jr. C. Shook The application of cluster analysis in strategic management research: An analysis and critique Strateg. Manage. J. 17 6 1996 441 458 [22] R. Lehmann Transformation model selection by multiple hypotheses testing J. Geod. 88 12 2014 1117 1130 10.1007/s00190-014-0747-3 [23] R. Ibrahim D. Voyer A. Bréard J. Huillery C. Vollaire B. Allard S. Member Y. Zaatar Experiments of time-reversed pulse waves for wireless power transmission in an indoor environment IEEE Trans. Microw. Theory Tech. 64 7 2016 2159 2170 Daniel Malafaia received his M.Sc. from Aveiro University, Portugal in 2012. He is currently pursuing his Ph.D. in the MAP-tele joint Doctoral Programme. His research interests include cognitive radio, statistical signal processing, spectrum sensing and data clustering. José Vieira received the Ph.D. degree in Electrical Engineering from the University of Aveiro in 2000. He has been a professor of electrical engineering at the University of Aveiro since 1991, and is also a researcher at the IEETA Institute. He has been the President of the Portuguese chapter of the AES since 2003. His major research interests are in the fields of digital audio, signal reconstruction, digital fountains and compressed sensing. Ana Tomé received the Ph.D. degree in electrical engineering from the University of Aveiro, Portugal, in 1990. Currently, she is an Associate Professor of electrical engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as classification and pattern recognition applications. "
    },
    {
        "doc_title": "An orthographic descriptor for 3D object learning and recognition",
        "doc_scopus_id": "85006365227",
        "doc_doi": "10.1109/IROS.2016.7759612",
        "doc_eid": "2-s2.0-85006365227",
        "doc_date": "2016-11-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D object",
            "Classification performance",
            "Computation time",
            "Descriptors",
            "Object reference",
            "Object representations",
            "Real-time application",
            "State of the art"
        ],
        "doc_abstract": "© 2016 IEEE.Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure reliability, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. The performance of the proposed object descriptor is compared with the main state-of-the-art descriptors. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-oftheart descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GOOD: A global orthographic object descriptor for 3D object recognition and manipulation",
        "doc_scopus_id": "84994232513",
        "doc_doi": "10.1016/j.patrec.2016.07.006",
        "doc_eid": "2-s2.0-84994232513",
        "doc_date": "2016-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Classification performance",
            "Disambiguation method",
            "Distribution matrices",
            "Object perception",
            "Object representations",
            "Orthographic projections",
            "Real-time application"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure robustness, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. We propose a novel sign disambiguation method, for computing a unique reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the target object view captured by a 3D sensor. Three principal orthographic projections and their distribution matrices are computed by exploiting the object reference frame. The descriptor is finally obtained by concatenating the distribution matrices in a sequence determined by entropy and variance features of the projections. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications. The estimated object's pose is precise enough for real-time object manipulation tasks.",
        "available": true,
        "clean_text": "serial JL 271524 291210 291718 291872 291874 31 Pattern Recognition Letters PATTERNRECOGNITIONLETTERS 2016-07-20 2016-07-20 2016-11-03 2016-11-03 2017-03-08T23:40:18 S0167-8655(16)30168-4 S0167865516301684 10.1016/j.patrec.2016.07.006 S300 S300.3 FULL-TEXT 2017-03-08T21:25:26.072496-05:00 0 0 20161101 2016 2016-07-20T22:11:34.444637Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb vol volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref 0167-8655 01678655 true 83 83 P3 Volume 83, Part 3 11 312 320 312 320 20161101 1 November 2016 2016-11-01 2016 Efficient Shape Representation, Matching, Ranking, and its Applications Xiang Bai Michael Donoser Hairong Liu Longin Jan Latecki article sco © 2016 Elsevier B.V. All rights reserved. GOODAGLOBALORTHOGRAPHICOBJECTDESCRIPTORFOR3DOBJECTRECOGNITIONMANIPULATION KASAEI S 1 Introduction 2 Related work 3 Local reference frame 4 Object descriptor 5 Experimental results 5.1 Descriptiveness 5.2 Scalability 5.3 Robustness 5.3.1 Gaussian noise 5.3.2 Varying point cloud density 5.4 Efficiency 5.4.1 Memory footprint 5.4.2 Computation time 5.5 System demonstration 6 Conclusion Acknowledgments References ALDOMA 2012 A ANDREOPOULOS 2013 827 891 A BO 2011 1729 1736 L IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITIONCVPR2011 OBJECTRECOGNITIONHIERARCHICALKERNELDESCRIPTORS BRO 2008 135 140 R CHEN 2007 1252 1262 H COVER 2012 T ELEMENTSINFORMATIONTHEORY DENG 2009 248 255 J IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION2009CVPR2009 IMAGENETALARGESCALEHIERARCHICALIMAGEDATABASE DINH 2006 863 870 H IEEECOMPUTERSOCIETYCONFERENCECOMPUTERVISIONPATTERNRECOGNITION2006 MULTIRESOLUTIONSPINIMAGES FROME 2004 224 237 A COMPUTERVISIONECCV2004 RECOGNIZINGOBJECTSINRANGEDATAUSINGREGIONALPOINTDESCRIPTORS GUO 2013 86 93 Y GRAPPIVAPP TRISIADISTINCTIVELOCALSURFACEDESCRIPTORFOR3DMODELINGOBJECTRECOGNITION HORN 1984 1671 1686 B JOHNSON 1999 433 449 A KASAEI 2015 537 553 S LAI 2011 1817 1824 K ROBOTICSAUTOMATIONICRA2011IEEEINTERNATIONALCONFERENCE ALARGESCALEHIERARCHICALMULTIVIEWRGBDOBJECTDATASET MARTON 2010 365 370 Z 10THIEEERASINTERNATIONALCONFERENCEHUMANOIDROBOTSHUMANOIDS2010 HIERARCHICALOBJECTGEOMETRICCATEGORIZATIONAPPEARANCECLASSIFICATIONFORMOBILEMANIPULATION MIAN 2010 348 361 A MILLER 1995 39 41 G OLIVEIRA 2014 2216 2223 M IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS20142014 APERCEPTUALMEMORYSYSTEMFORGROUNDINGSEMANTICREPRESENTATIONSININTELLIGENTSERVICEROBOTS OLIVEIRA 2015 M OSADA 2002 807 832 R PANG 2015 171 179 G INTERNATIONALCONFERENCE3DVISION3DV2015 FASTROBUSTMULTIVIEW3DOBJECTRECOGNITIONINPOINTCLOUDS PASQUALOTTO 2013 608 623 G REGAZZONI 2014 719 728 D RUSU 2009 3212 3217 R IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATION2009ICRA09 FASTPOINTFEATUREHISTOGRAMSFPFHFOR3DREGISTRATION RUSU 2010 2155 2162 R IEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS2010 FAST3DRECOGNITIONPOSEUSINGVIEWPOINTFEATUREHISTOGRAM RUSU 2009 47 54 R IEEE12THINTERNATIONALCONFERENCECOMPUTERVISIONWORKSHOPSICCVWORKSHOPS2009 DETECTINGSEGMENTINGOBJECTSFORMOBILEMANIPULATION RUSU 2008 927 941 R SU 2015 945 953 H PROCEEDINGSIEEEINTERNATIONALCONFERENCECOMPUTERVISION MULTIVIEWCONVOLUTIONALNEURALNETWORKSFOR3DSHAPERECOGNITION TOMBARI 2010 356 369 F COMPUTERVISIONECCV2010 UNIQUESIGNATURESHISTOGRAMSFORLOCALSURFACEDESCRIPTION WOHLKINGER 2011 2987 2992 W IEEEINTERNATIONALCONFERENCEROBOTICSBIOMIMETICSROBIO2011 ENSEMBLESHAPEFUNCTIONSFOR3DOBJECTCLASSIFICATION WU 2015 1912 1920 Z PROCEEDINGSIEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION 3DSHAPENETSADEEPREPRESENTATIONFORVOLUMETRICSHAPES ZHONG 2009 689 696 Y IEEE12THINTERNATIONALCONFERENCECOMPUTERVISIONWORKSHOPSICCVWORKSHOPS2009 INTRINSICSHAPESIGNATURESASHAPEDESCRIPTORFOR3DOBJECTRECOGNITION KASAEIX2016X312 KASAEIX2016X312X320 KASAEIX2016X312XS KASAEIX2016X312X320XS 2018-11-03T00:00:00.000Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0167-8655(16)30168-4 S0167865516301684 10.1016/j.patrec.2016.07.006 271524 2017-03-08T21:25:26.072496-05:00 2016-11-01 true 2164112 MAIN 9 55193 849 656 IMAGE-WEB-PDF 1 gr1 15364 164 203 gr2 9775 83 219 gr3 14154 141 219 gr4 7518 52 219 gr5 12316 91 219 gr6 8243 54 219 gr7 11732 78 219 gr8 7807 76 219 gr9 15100 86 219 gr1 43088 311 385 gr2 59290 305 809 gr3 33529 246 383 gr4 57188 194 808 gr5 17361 131 316 gr6 66223 200 810 gr7 17689 119 334 gr8 20773 134 385 gr9 24998 150 382 gr1 307699 1377 1704 gr2 563764 1351 3583 gr3 301966 1090 1697 gr4 501105 857 3576 gr5 276100 581 1402 gr6 558076 885 3587 gr7 271243 527 1478 gr8 165674 595 1705 gr9 226874 667 1693 si1 224 14 46 si10 171 12 22 si11 174 12 22 si12 265 11 58 si13 184 16 20 si14 200 20 28 si15 186 17 28 si16 901 22 315 si17 409 13 90 si18 1178 56 198 si19 341 18 83 si2 507 17 143 si20 127 13 23 si21 141 15 24 si22 198 16 63 si23 601 16 145 si24 272 15 44 si25 562 20 132 si26 581 17 173 si27 580 17 173 si28 1151 53 244 si29 1183 53 245 si3 623 52 106 si30 149 24 10 si31 245 24 46 si32 272 24 46 si33 176 13 20 si34 1150 16 322 si35 962 52 206 si36 168 11 22 si37 460 17 93 si38 271 16 59 si39 972 52 207 si4 162 13 17 si40 189 12 26 si41 596 58 112 si42 340 19 57 si43 219 13 44 si44 853 16 261 si5 1066 52 228 si6 382 16 84 si7 461 16 122 si8 699 17 162 si9 146 11 16 PATREC 6593 S0167-8655(16)30168-4 10.1016/j.patrec.2016.07.006 Elsevier B.V. Fig. 1 Visualization of sign disambiguation procedure: (a) orthographic projection of the object on the XoZ and XoY planes; (b) XoY plane is used to determine the sign of Y axis; (c) XoZ plane is used to determine the sign of X axis. The red, green and blue lines represent the unambiguous X, Y, Z axes respectively. Fig. 1 Fig. 2 An illustrative example of the producing a GOOD shape description for a mug object (i.e. d = 5 ): (a) The mug object and its bounding box, reference frame and three projected views; the object’s points are then projected onto three planes; therefore, XoZ (b), YoZ (c) and XoY projections (d) are created. Each plane is partitioned into bins and the number of point falling into each bin is counted. Accordingly, three distribution matrices are obtained for the projections; afterwards, each distribution matrix is converted to a distribution vector, (i.e. (e), (f) and (g)) and two statistic features including entropy and variance are then calculated for each distribution vector; (h) the distribution vectors are consequently concatenated together using the statistics features, to form a single description for the given object. The ordering of the three distribution vectors is first by decreasing values of entropy. Afterwards the second and third vectors are sorted again by increasing values of variance. Fig. 2 Fig. 3 Example of how the projections used to build GOOD can also be used for extracting features relevant for object manipulation (see text): (a) Local reference frame and projections; (b) Projections in multi-view layout. Fig. 3 Fig. 4 Object recognition performance in descriptiveness and scalability experiments; (left) effect of number of bins on performance; (center) scalability of the selected descriptors with respect to varying numbers of categories in the dataset as a function of accuracy vs. Number of categories; (right) scalability experiment time vs. Number of categories. Fig. 4 Fig. 5 An illustration of a Vase object with different levels of Gaussian noise. Fig. 5 Fig. 6 The robustness of the selected descriptors to different level of Gaussian noise and varying point cloud density: (left) different levels of Gaussian noise applied to the test; (center) different levels of downsampling applied to the test data; (right) different levels of downsampling applied to the train data. Fig. 6 Fig. 7 An illustration of a Flask object with different levels of downsampling. Fig. 7 Fig. 8 Average computation time of the selected descriptors on 20 randomly selected objects from the RGB-D dataset. Fig. 8 Fig. 9 Two snapshots showing the object perception system performing object recognition and pose estimation using the GOOD descriptor; (left) the instructor puts a Mug and a Vase on the table. The gray bonding boxes and red, green and blue lines signal the pose of the object and the GOOD descriptions are visualized and computed; this frame shows that the system is able to compute the GOOD description and estimate pose of objects in the scene. Moreover, it demonstrates that the Vase and the Mug are properly recognized; (right) A Plate enters the scene. Its shape description and pose are computed and visualized. Because there is no prior knowledge about plates, it is classified as Unknown [13]. Fig. 9 Table 1 Summary of descriptiveness experiments. Table 1 Number of bins Descriptor size Memory (Kb) Accuracy 5 75 0.3 0.92 10 300 1.2 0.93 15 675 2.7 0.94 20 1200 4.8 0.93 25 1875 7.5 0.94 30 2700 10.8 0.93 35 3675 14.7 0.93 40 4800 19.2 0.93 45 6075 24.3 0.93 50 7500 30.0 0.93 Table 2 Summary of scalability experiments. Table 2 Number of categories Accuracy GOOD(5bins) GOOD(15bins) VFH ESF GFPFH GRSD 5 0.96 0.98 0.98 0.97 0.90 0.97 10 0.94 0.95 0.96 0.96 0.71 0.92 15 0.95 0.95 0.97 0.98 0.75 0.80 20 0.95 0.95 0.97 0.96 0.71 0.81 25 0.95 0.95 0.97 0.97 0.61 0.79 30 0.94 0.94 0.96 0.94 0.61 0.77 35 0.94 0.94 0.94 0.96 0.59 0.76 40 0.93 0.94 0.94 0.93 0.59 0.72 45 0.92 0.94 0.94 0.93 0.60 0.69 50 0.92 0.94 0.94 0.93 0.59 0.68 Table 3 Summary of robustness to Gaussian noise experiments. Table 3 Gaussian noise(mm) Accuracy GOOD 5bins VFH ESF GFPFH GRSD 1 0.94 0.95 0.90 0.40 0.66 2 0.93 0.93 0.74 0.17 0.61 3 0.92 0.93 0.49 0.10 0.51 4 0.91 0.94 0.32 0.09 0.35 5 0.89 0.91 0.24 0.09 0.26 6 0.85 0.85 0.23 0.09 0.19 7 0.83 0.78 0.23 0.09 0.12 8 0.78 0.57 0.22 0.09 0.08 9 0.69 0.42 0.23 0.09 0.10 10 0.67 0.36 0.22 0.09 0.09 Table 4 Length of selected 3D shape descriptors. Table 4 No. Descriptor Feature length (float) Adjustable length Implementation 1 GFPFH 16 No PCL 1.7 2 GRSD 21 No PCL 1.8 3 GOOD 75 Yes – 4 VFH 308 No PCL 1.7 5 ESF 640 No PCL 1.7 ☆ This paper has been recommended for acceptance by Gabriella Sanniti di Baja. GOOD: A global orthographic object descriptor for 3D object recognition and manipulation S. Hamidreza Kasaei ⁎ a Ana Maria Tomé a b Luís Seabra Lopes a b Miguel Oliveira c a IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Aveiro, 3810-193, Portugal IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro Universidade de Aveiro Aveiro 3810-193 Portugal b Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Electrónica, Telecomunicações e Informática Universidade de Aveiro Portugal c Instituto de Engenharia de Sistemas e Computadores, Tecnologia e Ciência R. Dr. Roberto Frias, 465, Porto 4200, Portugal Instituto de Engenharia de Sistemas e Computadores Tecnologia e Ciência R. Dr. Roberto Frias 465 Porto 4200 Portugal ⁎ Corresponding author. Tel.: +351 234 370 500; fax: +351 234 370 545. Object representation is one of the most challenging tasks in robotics because it must provide reliable information in real-time to enable the robot to physically interact with the objects in its environment. To ensure robustness, a global object descriptor must be computed based on a unique and repeatable object reference frame. Moreover, the descriptor should contain enough information enabling to recognize the same or similar objects seen from different perspectives. This paper presents a new object descriptor named Global Orthographic Object Descriptor (GOOD) designed to be robust, descriptive and efficient to compute and use. We propose a novel sign disambiguation method, for computing a unique reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the target object view captured by a 3D sensor. Three principal orthographic projections and their distribution matrices are computed by exploiting the object reference frame. The descriptor is finally obtained by concatenating the distribution matrices in a sequence determined by entropy and variance features of the projections. Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. Concerning memory and computation time, GOOD clearly outperforms the other descriptors. Therefore, GOOD is especially suited for real-time applications. The estimated object’s pose is precise enough for real-time object manipulation tasks. Keywords 3D object recognition Object Perception Orthographic projection 1 Introduction Following the advent of inexpensive depth sensing devices such as Microsoft Kinect or the ASUS Xtion, which record RGB and depth information, the use of 3D data is becoming increasingly popular. One of the primary goals in service robotics is to develop reliable capabilities in the area of perception that will allow robots to robustly recognize objects and interact with the environment by manipulating those objects. For this purpose, a robot must reliably recognize the object. Furthermore, in order to interact with human users, this process of object recognition cannot take more than a fraction of a second. Although many object recognition methods for both 2D and 3D data have been proposed [2], recognizing 3D objects in the presence of noise and variable point cloud resolution is still a challenging task. However, 3D data contains more information about the spatial positioning of objects, which in turn eases the process of object segmentation. Moreover, depth data is more robust than RGB data to the effects of illumination and shadows [25]. Therefore, 3D data can be employed to describe the surface of the objects based on geometric properties 1 1 2D data can also be used to distinguish objects that have same geometric properties with different texture (e.x. a Coke can from a Diet Coke can). . A 3D object recognition system is composed of several software modules such as Object Detection, Object Representation, Object Recognition and Perceptual Memory. Object Detection is responsible for detecting all objects in a scene. Object representation is concerned with the calculation of a set of features for the detected object, which are send to the Object Recognition. Objects are recognized by comparing their description against the descriptions of known objects (stored in the Perceptual Memory). Object Representation plays a prominent role because the output of this module is used for learning as well as recognition. Existing 3D object representation approaches are based on either global or local descriptors. Global descriptors encode the entire 3D object, while local descriptors represent a small area of an object around a specific keypoint. Generally, global descriptors are increasingly used in the context of 3D object recognition, object manipulation, as well as geometric categorization [1]. These must be efficient in terms of computation time as well as memory, to facilitate real-time performance. For example, Ensemble of Shape Functions (ESF) [32], Global Fast Point Feature Histogram (GFPFH) [28], Viewpoint Feature Histogram (VFH) [27] and Global Radius-based Surface Descriptor (GRSD) [16], are global descriptors. Local descriptors tend to handle occlusion and clutter better when compared to global descriptor. However, comparing 3D object views based on their local features tends to be computationally more expensive [1]. Examples in this category include Spin Images (SI) [12], Signature of Histograms of Orientations (SHOT) [31], Fast Point Feature Histogram (FPFH) [26] and Hierarchical Kernel Descriptors [3]. Invariance to the pose of an object is a critical property of any 3D shape descriptor. A number of 3D shape descriptors achieve pose invariance using either a reference axis only (e.x. Spin-Images [12]) or a complete object reference frame (e.x. Intrinsic Shape Signatures [34]). In this paper, a new global 3D shape descriptor named GOOD (i.e. Global Orthographic Object Descriptor) is presented. GOOD provides an appropriate trade-off between descriptiveness, computation time and memory usage. The descriptor is designed to be scale and pose invariant, informative and stable, with the objective of supporting accurate 3D object recognition. A novel sign disambiguation method is proposed to compute a unique and repeatable reference frame from the eigenvectors obtained through Principal Component Analysis of the point cloud of the object. Using this reference frame, three three principal projections, namely XoZ, XoY and YoZ, are created based on orthographical projection. The space of each projection is partitioned into bins and the number of points falling into each bin is counted. From this, three distribution matrices are obtained for the projected views. Two statistic features, namely entropy and variance are then calculated for each distribution matrix. The distribution matrices are consequently concatenated together using the entropy and variance features to form a single description for the given object view. In this paper, we assume that an object has already been segmented from the point cloud of the scene, and we will focus on detailing the 3D object descriptor. This descriptor works directly on 3D point clouds and requires neither triangulation of the object’s points nor surface meshing. For additional details on the object detection and object recognition methodologies, we refer the reader to our previous works on interactive open-ended learning for 3D object recognitions [13,19,20]. The contributions presented in this paper are the following: (i) design a new sign disambiguation method to compute a unique and unambiguous complete local reference frame, from the eigenvectors obtained through Principal Component Analysis of the segmented point cloud of the object and (ii) a novel global object descriptor computed using that local reference frame, that provides a good trade-off between descriptiveness, computation time and memory usage. The remainder of this paper is organized as follows. In Section 2, we discuss related works. The methodology for computing the local reference frame is presented in Section 3. Section 4 describes the proposed global object descriptor. Evaluation of the proposed shape descriptor is presented in Section 5. Finally, in Section 6, conclusions are presented and future research is discussed. 2 Related work Three-dimensional shape description has been under investigation for a long time in various research fields, such as pattern recognition, computer graphics and robotics. Although an exhaustive survey of shape descriptor is beyond the scope of this paper, we will review a few recent efforts. As previously mentioned, some descriptors use LRF to compute a pose invariant description. Therefore, this property can be used to categorize 3D shape descriptors into two categories including (i) shape descriptors without LRF; (ii) shape descriptors with LRF. Most of the shape descriptors of the first category use certain statistic features or geometric properties of the points on the surface like depth value, curvature and surface normal to generate a description. For instance, Shape Distributions descriptor [21] represents an object as a shape distribution sampled from a shape function measuring global geometric properties of the object. Extended Gaussian Images (EGI) descriptor [11] is based on the distribution of surface normals on the Gaussian sphere. Since descriptiveness of the EGI depends on the shape of the object and it is not suitable for non-convex object. Chen and Bhanu [5] proposed a local surface patch (LSP) descriptor that encodes the shape of objects by accumulating points in particular bins along the two dimensions that are the shape index value and the cosine of the angle between the surface normals. Wohlkinger and Vincze [32] introduced a global shape descriptor called Ensemble of Shape Function (ESF) that does not require the use of normals to describe the object and the characteristic properties of an object is represented using an ensemble of ten 64-bin histograms of angle, point distance, and area shape functions. Point Feature Histogram (PFH) [29] can be used as local or global shape descriptor. The PFH represents the relative orientation of normals, as well as distances, between point pairs. For each point p, k-neighborhood points are selected based on a sphere centered at p with radius r. Afterwards, a surface normal for each point is estimated. Subsequently, four features are calculated for every pair of points using their surface normals, positions and angular variations. In a later work [26], in order to improve the robustness of PFH in case of point densities variations, the distance between point pairs is excluded from the histogram of PFH. The computation complexity of a PFH is O(n 2), where n is the number points in the point cloud. Fast Point Feature Histogram (FPFH) [26] is an extension version of PFH. The FPFH estimates the sets of values only between every point and its k nearest neighbors. This is different from PFH, where all pairs of points in the support region are considered. Therefore, the computational complexity is reduced to O(k.n). The FPFH is a scale and pose invariant descriptor which is not suitable for grasping. Viewpoint Feature Histogram (VFH) [27] is another extension of PFH descriptor. The VFH descriptor computes the same angular features as the PFH. Additionally, it computes another statistics between the central viewpoint direction and the normals estimated at each point. The VFH shape descriptor produces a single histogram that encodes the geometry of the whole object and its viewpoint. Because of the global nature of VFH, the computational complexity of VFH is O(n). The descriptiveness of the above shape descriptors are limited because the 3D spatial information either is not taken into account or it is discarded during the description process. Unlike the above approaches, some researchers have recently adopted deep learning algorithms for 3D object representation, learning and recognition [15,30,33]. These works use a collection of 2D images rendered from different view points to learn a shape representation that aggregates information from input views and provides a compact shape descriptor. As it was pointed out in [33], training a deep artificial neural network for 3D object representation requires a large collection of 3D objects to provide accurate representations and typically involves long training times. In contrast, the shape descriptors in the second category encode the spatial information of the objects’ points using a Local Reference Frame (LRF). Some descriptors provide a description using only a Reference Axis. For example, Spin-Images [12] uses surface normal of a vertex as a reference axis and proposed a spin image representation by projecting the surface points to the tangent plane of the vertex. Then, each projected point is represented by a pair (α, β), where α is the distance to the surface normal, i.e., the radius, and β is the perpendicular distance from the point to the tangent plane. Consequently, a histogram is formed by counting the occurrences of different discretized distance pairs. Spin images descriptor has been successfully used in many applications, but one limitation of this descriptor is that it is not scale invariant. Dinh and Kropac [8] proposed multi-resolution pyramids of spin images in order to improve the discrimination of the original spin image and speed up the matching process. Some variants of the spin image shape descriptor also presented such as Tri-Spin-Image descriptor (TriSI) [10] and color spin image [23]. Similar to the SI, 3D Shape Context (3DSC) [9] uses the surface normal of an basis point as its LRF. The 3DSC descriptor is calculated by counting the weighted number of points falling into each bin of an sphere grid centered on the basis point and its north pole oriented with the surface normal. The sphere grid is constructed based on dividing the support area into bins by logarithmically spaced boundaries along the radial dimension and equally spaced boundaries in the azimuth and elevation dimensions. Whenever only an axis is used as a reference frame, there is an uncertainty in the rotation around the axis that should be handled for generating a robust and repeatable description. In order to eliminate this issue, several descriptors (e.g. 3D Shape Context) proposed to compute multiple descriptions for different possible rotations of the object. Since this kind of solutions are caused increasing the computational cost in terms of both execution time as well as memory usage, they are not optimum and real solutions. Furthermore, the recognition process becomes not only significantly slow, but also more laborious. Differently, Zhong [34] proposed a shape descriptor namely Intrinsic Shape Signatures (ISS) using defining a LRF based on the eigenvectors of the scatter matrix of the point cloud of the object and describing the point distribution in the spherical angular space. Similar to Zhong work, Mian et al. [17] introduced LRF computed with eigenvectors of the covariance matrix of the object’s points. However, in both cases the eigenvectors define the principal directions of the data, their sign is not defined unambiguously. Accordingly, different descriptors can be generated for the object. As highlighted before, they are neither computationally efficient nor repeatable. [22] proposed a multi-view 3D object recognition approach. In this approach, each object is projected into 46 projection planes distributed on a sphere, whereas we just compute three principal orthographic projections. Their object representation is clearly not efficient for real time application like robotics. In order to achieve true rotation invariant descriptor, Tombari et al. [31] proposed a 3D shape descriptor namely Signature of Histograms of OrienTations (SHOT). To generate the description for the object, they first applied a sign disambiguation technique to the eigenvectors of the scatter matrix of the object and constructed a unique and unambiguous LRF. The object’s points are then aligned with the LRF. Consequently, similar to 3D Shape Context, a spherical coordinate based approach is used to generate a SHOT description for the given object. 3D object descriptors that use spherical coordinate system suffer from the singularity issue at the poles, because bins at the poles are significantly smaller than bins around the equator. Our shape descriptor differ from all of the listed descriptors above as it is simultaneously unique, unambiguous, and robust to noise and varying low-level point cloud density. Besides, our approach can be used not only for object recognition but also for object manipulation. 3 Local reference frame A Local Reference Frame (LRF), invariant to translations and rotations and robust to noise is important for object recognition as well as object manipulation. Since the repeatability of a LRF directly affects the descriptiveness of the object representation [17], the LRF should be as repeatable and robust as possible to improve the performance of object recognition. In this section, we propose a method to compute a LRF. For this purpose, the three principal axes of a given object are firstly determined based on Principal Component Analysis (PCA). Given a point cloud of an object that contains m points, O = { p 1 , ⋯ , p m } , the geometric center of the object is defined as: (1) c = 1 m ∑ i = 1 m p i , where p i is a three dimensional point in the object’s point cloud. The normalized covariance matrix, C, of the object is constructed: (2) C = 1 m ∑ i = 1 m ( p i − c ) ( p i − c ) T . Then, eigenvalue decomposition is performed on C: (3) C V = E V , where V = [ v 1 , v 2 , v 3 ] contains the three eigenvectors, E = d i a g ( λ 1 , λ 2 , λ 3 ) is a diagonal matrix of the corresponding eigenvalues and λ 1 ≥ λ 2 ≥ λ 3. Since the covariance matrix is symmetric positive, its eigenvalues are positive and the eigenvectors are orthogonal. Eigenvectors define directions which are not unique, i.e. not repeatable across different PCA trials. This is known as the sign ambiguity problem, for which there is no mathematical solution [4]. Since there are two possible directions for each eigenvector, a total of eight reference frames can be created from the same set of eigenvectors. A mechanism is needed to transform this reference frame into a unique object reference frame, which will be always the same across multiple trials. We start with a provisional reference frame, in which the first two axes, X and Y, are defined by the eigenvectors v 1 and v 2 , respectively. However, regarding the third axis, Z, instead of defining it based on v 3 , we define it based on the cross product v 1 × v 2 . This way, because the result of the cross product follows the right-hand rule, the number of alternatives is reduced to four. It is now enough to disambiguate the directions of the X and Y axes. So either the directions of X and Y are both changed or both remain unchanged. To complete the disambiguation, the object’s point cloud, O , is transformed to be placed in the provisional reference frame. Then, the number of points that have positive x, S x + , and the number of points that have negative x, S x − , are counted as follows: (4) S x + = { i : x p i > t } , S x − = { i : x p i < − t } , where t is a threshold (e.g. t = 0.015 m ) that is used to deal with the special case when a point is close to the YoZ plane, and therefore can change from negative to positive X in different trials. Afterwards, the variable Sx is defined as: (5) S x = { + 1 , | S x + | ≥ | S x − | − 1 , o t h e r w i s e , where |.| denotes the number of points of the argument. A similar indication, Sy , is computed for the Y axis. Finally, the sign of the axes is determined as: (6) s = S x . S y , where s can be either − 1 or + 1 . In case of s = − 1 , the directions of X and Y must be changed, otherwise not. Therefore, the final LRF (X, Y, Z) will be defined by ( s v 1 , s v 2 , v 1 × v 2 ) . An illustrative example of the sign disambiguation procedure is provided in Fig. 1 . 4 Object descriptor This section describes the computation of the proposed object descriptor, GOOD, in the obtained LRF centered in the geometric center of the object. The descriptor consists of a concatenation of the orthographic projections of the object on the three orthogonal planes, XoY, YoZ and XoZ. Each projection is described by a distribution matrix. To ensure correct comparison between different object shapes, the number of bins in the distribution matrices must be the same and the bins should be of equal size. Therefore, each distribution matrix must be computed from a square area in the projection plane centered on the object’s center, and this square area must have the same dimensions for the three projections. The side length of these square areas, l, is determined by the largest edge length of a tight-fitting axis-aligned bounding box (AABB) of the object. The dimensions of the AABB are obtained by computing the minimum and maximum coordinate values along each axis. With this setup, the number of bins, n, is the only parameter that must be specified to compute GOOD. For each projection, the l × l projection area is divided into n × n square bins. Finally, a distribution matrix M n × n is obtained by counting the number of points falling into each bin. For each projected point ρ = ( α , β ) ∈ R 2 , where α is the perpendicular distance to the horizontal axis and β is the perpendicular distance to the vertical axis, a row, r ( ρ ) ∈ { 0 , ⋯ , n − 1 } , and a column, c ( ρ ) ∈ { 0 , ⋯ , n − 1 } , are associated as follows: (7) r ( ρ ) = ⌊ α + l 2 l + ϵ n ⌋ = ⌊ n α + l 2 l + ϵ ⌋ , (8) c ( ρ ) = ⌊ β + l 2 l + ϵ n ⌋ = ⌊ n β + l 2 l + ϵ ⌋ , where ϵ is a very small value used to deal with the special cases when a point is projected onto the upper bound of the projection area, and ⌊x⌋ returns the largest integer not greater than x. Note that the projected view is shifted to right and top by l 2 (i.e. α + l 2 and β + l 2 ). Furthermore, to achieve invariance to point cloud density, M is normalized such that the sum of all bins is equal to one (see Fig. 2 ). The matrix M is called distribution matrix, because it represents the 2D spatial distribution of the object’s points. According to standard practice, this matrix is converted to a vector m 1 × n 2 = [ M ( 1 , 1 ) , M ( 1 , 2 ) , ⋯ , M ( n , n ) ] . The three projection vectors will be concatenated producing a vector of dimension 3 × n 2 which is the final object descriptor, GOOD. Statistical features are used to decide the order in which the projection vectors will be concatenated. For the first projection in the descriptor, the one with largest area is preferred. The number of points is not a good indicator of area because all points of the object are represented in the three projections. The number of occupied bins (the ones with a mass greater than 0) could be used as a measure of area. However, this measure tends to be brittle when the boundary of the object is close to boundaries between bins. Therefore, in this work the entropy of the projection is used. Entropy, a measure from Information Theory [6], nicely takes into account both the number of occupied bins and their density. In this work, the entropy of a projection is computed as follows: (9) H ( m ) = − ∑ i = 1 n m i log 2 m i , where m i is the mass in bin i. The logarithm is taken in base 2 and 0 log 2 0 = 0 . The projection with highest entropy is the one that will appear in the first n 2 positions of the descriptor. The next step is to select, from the remaining two projections, which one should appear in the second part of the descriptor (positions n 2 to 2 n 2 − 1 ). It is common that these two projections have similar areas, and therefore similar entropies, leading to instability of the decision if it is made based on entropy. Therefore, instead of entropy, we use variance to make this decision. Since the projection matrices are probability mass functions (pmf), the variance is defined as follows: (10) σ 2 ( m ) = ∑ i = 1 n ( i − μ m ) 2 m i , where μ m is the expected value (i.e. a weighted average of the possible values of i, corresponding to the geometric center of the projection), which is computed as follows: (11) μ m = ∑ i = 1 n 2 i m i , unlike the simple mean, which gives each projection equal weight, the mean of a projection weights each bin, i, according to its probability distribution, m i . The variance measure, σ 2 ( m ) , is used to measure the spread or variability of the spatial distribution of the object’s points in the projection vector. A small variance indicates that the projected points tend to be very close to each other and to the mean of the vector, i.e. the shape of distribution is small and compact. A high variance indicates that the data points in the projection vector are very spread out from the mean. An illustrative example of the proposed shape descriptor is depicted in Fig. 2. In this example, after determining the local reference frame, a mug is projected onto the three orthogonal planes. Based on the entropy criterion, the XoZ projection (Fig. 2b) is selected to appear in the first part of the descriptor. Based on the variance criterion, the YoZ projection (Fig. 2c) is selected to appear in the second part of the descriptor. The remaining projection, XoY (Fig. 2d), appears in the last part of the descriptor. In order to grasp an object, it is necessary to know true dimensions of different parts of the object. Such information is not adequately represented in most shape descriptors (e.g. Viewpoint Feature Histogram [27]). Because GOOD is composed of three orthogonal projections, it is especially rich in terms of information suited for manipulation tasks. In Fig. 3 , again we consider the projections of a mug. Here, we adopt a multi-view orthographic projection layout in which there is a central or front view, a top view and a side view. The central view is the one selected based on the entropy criterion and appearing in the first part of the descriptor. The top view contains the projection in the orthogonal plane formed by the horizontal axis of the central projection and the third axis. The side view contains the projection in the orthogonal plane formed by the vertical axis of the central projection and the third axis. The figure shows that projections can be further processed for object manipulation purposes. In the top view, the gray symbols C, W, D and T represent how the projection can be further processed and some features for manipulation task are extracted, namely inner radius (C), thickness (T), handle length (W) and handle thickness (D). 5 Experimental results Several experiments were carried out to evaluate the performance of the proposed object descriptor concerning descriptiveness, scalability, robustness and efficiency characteristics. The proposed descriptor has a parameter namely number of bins (i.e. d) that must be well selected to provide a good balance between recognition accuracy, memory usage and computation time. For this purpose, 10 experiments were performed for different values of the descriptor’s parameter. The performance of the shape descriptor and its scalability were examined on the Washington RGB-D Object Dataset [14]. Afterwards, various tests were executed to measure the robustness of the proposed shape descriptor concerning different levels of noise and varying mesh resolutions on the Restaurant Object Dataset [13]. Next, two efficiency evaluations relating to computational efficiency and memory usage were performed. Furthermore, a real demonstration was performed to show all the characteristics of the proposed descriptor. The largest publicly available dataset, namely Washington RGB-D Object Dataset [14], consisting of 250,000 views of 300 common household objects. The objects are categorized into 51 categories arranged using WordNet [18] hypernym-hyponym relationships (similar to ImageNet [7]). The Restaurant Object Dataset contains 241 views of one instance of each category (Bottle, Bowl, Flask, Fork, Knife, Mug, Plate, Spoon, Teapot, and Vase) [13]. In all experiments, an instance-based learning approach is used, i.e. object categories are represented by sets of known instances. The instance-based approach is a baseline method for evaluating representations. However, more advanced approaches like SVM and object Bayesian approaches can be easily adapted. Similarly, a simple baseline recognition mechanism in the form of a Euclidean nearest neighbor classifier is used. Moreover, the proposed descriptor was compared with four state-of-the-art object descriptors that are available in the Point-Cloud Library 2 2 (PCL version 1.7 and 1.8) including VFH [27], ESF [32], GFPFH [28] and GRSD [16]. The selected descriptors were evaluated based on a 10-fold cross validation algorithm in terms of Accuracy [24]. In each iteration, a single fold is used for testing, and the remaining data are used as training data. The cross-validation process is then repeated 10 times, which each of the 10 folds used exactly once as the test data. For all selected shape descriptors, the default parameters in the respective PCL implementations were used. All tests were performed with an i7, 2.40GHz processor and 16GB RAM. 5.1 Descriptiveness As mentioned above, GOOD has a parameter called number of bins that has effect on descriptiveness, efficiency and robustness. Therefore, it must be well selected to provide a good balance between recognition performance, memory usage and computation time. The descriptiveness of the proposed descriptor with respect to varying number of bins was evaluated using Washington dataset. Results are presented in Fig. 4 (left) and Table 1 . In these experiments, the configurations that obtained the best precision and recall figures were number 3 and 5. Although, a large number of bins provides more details about the point distribution, it increases computation time, memory usage and sensitivity to noise. Therefore, since the difference to other configurations is not very large, we prefer to use configuration number 1 (i.e. b = 5 ) which displays a good balance between recognition performance, memory usage, and processing speed. The accuracy of the proposed system with this configuration was 92%. It shows that the overall performance of the recognition system is promising and the proposed descriptor is capable of providing distinctive global feature for the given object. The following results are computed using this the default value, unless otherwise noted. 5.2 Scalability A set of experiments was carried out to evaluate the performance of the proposed descriptor on the Washington dataset, concerning its scalability with respect to varying numbers of categories. Results are depicted in Fig. 4 (center) and (right). One important observation is that the accuracy decreases in all approaches as more categories are used (Fig. 4 (center)). This is expected since the number of categories known by the system makes the classification task more difficult and the difference in performance between descriptors becomes smaller. Moreover, it can be concluded from Table 2 that when the number of object categories increases (i.e. more than 35 categories), VFH and GOOD descriptors achieve the best accuracy and stable performance regarding varying numbers of categories Table 3 . It is clear from Fig. 4 (right) that the computation time of our approach is significantly smaller than VFH, GRSD and GFPFH. However, GOOD, VFH and ESF descriptors obtain an acceptable scalability regarding varying numbers of categories, the scalability of GRSD and GFPFH are very low and their performance drops aggressively when the number of categories increases. Although ESF descriptor achieves better performance than our approach with 5 bins (i.e. GOOD 5bins), the length of ESF descriptor (i.e. compactness) is around 8.5 times more than our descriptor (see Table 4 ). It is notable that whenever the size of dataset is larger than 35 object categories, the difference between EFS performance and our approach with 5 bins, is equal or less than 1% and in the similar situation our approach with 15 bins (i.e. GOOD 15bins) works better than ESF descriptor. 5.3 Robustness The robustness of the proposed object descriptor with respect to different levels of Gaussian noise and varying mesh resolutions was evaluated and compared with other global object descriptors. These experiments were run on the mentioned Restaurant Object Dataset. 5.3.1 Gaussian noise Ten levels of Gaussian noise with standard deviations from 1 to 10 mm were added to the test data. For a given test object, Gaussian noise is independently added to the X, Y and Z-axes. As an example, a Vase object with three levels of standard deviation of Gaussian noise ( σ = 3 mm , σ = 6 mm , σ = 9 mm ) is depicted in Fig. 5 . The results are presented in Fig. 6 (left) and Table 3. An important observation can be made from Figs. 6 and 4. Although GOOD, ESF and VFH descriptors achieved a really good performance on noise free data, GOOD outperformed ESF, GFPFH and GRSD descriptors by a large margin under all levels of Gaussian noise. While the performance of VFH descriptor was similar to our approach under a low-level noise (i.e. σ ≤ 6 mm), our shape descriptor outperformed all descriptors under high levels of noise. It can be concluded from this observation that GOOD descriptor is robust to noise due to use an stable, unique and unambiguous object reference frame. In contrast, since the VFH and GFPFH descriptors are rely on surface normals to calculate their shape descriptions, they are highly sensitive to the noise. GRSD employs radial relationships to describe the geometry of points at each voxel cell and ESF uses distances and angels between randomly sampled points to generate a shape description; therefore, GRSD and ESF are also sensitive to the noise and its performance decrease rapidly when the standard deviation of the Gaussian noise increases. In addition, GOOD descriptor uses three distribution matrices that are constructed based on orthographical projection, therefore less affected by noise (i.e. in each orthographic projection one dimension is discarded). 5.3.2 Varying point cloud density Two sets of experiments were carried out to examine the robustness of the proposed descriptor with respect to varying point cloud density. In the first set of experiments, the original density of training objects was kept and the density of testing objects was reduced (downsampling) using a voxelized grid approach 3 3 In the second set of experiments, the original density was kept in testing objects and reduced in training objects. This is initiated with a root volume element (voxel) and the eight children voxels in which each internal node has exactly eight children nodes. These are recursively subdivided until all voxels contain at most one point or the minimum voxel size is reached (i.e. The cloud is divided in multiple voxels with the desired resolution). Afterwards all the points that fall into the same voxel will be downsampled with their centroid. In this evaluation, each object (either test or train) is downsampled using five different voxel sizes including {1, 5, 10, 15, 20} millimetre. An illustration example of a Flask object with four level of downsampling is depicted in Fig. 7 . The robustness results regarding varying point cloud density in test and train data are presented in Fig. 6. From experiments of reducing density of test data (i.e. Fig. 6(left)), it was found that our approach is more robust than the other descriptors concerning low-level downsampling (i.e. DS ≤ 3.5 mm) and works slightly better than the other in high-level downsampling resolution (i.e. DS ≥ 18 mm). In contrast, the performance of VFH, ESF and GRSD were better than GOOD descriptor in mid-level downsampling resolution (i.e. 3.5 mm < DS < 18). The performance of GFPFH was very low under all level levels of point cloud resolution. Besides, it can be concluded from Fig 6 (right) that when the level of up-sampling increases, VFH, ESF and GRSD descriptors achieve better performance than GOOD and GFPFH descriptors. 5.4 Efficiency In this subsection, evaluations regarding computational efficiency and memory footprint (i.e. the amount of main memory that a program uses or references while running) are presented and discussed. 5.4.1 Memory footprint The length or size of a descriptor, has direct influence on memory usage and computation time in object recognition process (see Fig. 4). The length of all descriptors used in this evaluation is given in Table 4. Although GFPFH and GRSD are the tow most compact descriptors in this evaluation (see Table 4), their computation time and are not good as depicted in Figs. 4 and 8 . Our approach is the third compact descriptor that provides good balance between computation time and descriptiveness with 75 floats. However, VFH and ESF descriptors achieve a good description power, their feature length is around 4.1 and 8.5 times larger than our approach and 20 and 40 times larger than GFPFH descriptor respectively. ESF is the lowest compact descriptor compared to all the other descriptors. 5.4.2 Computation time Several experiments were performed to measure computational time of all descriptors used in this evaluation. Since the number of object’s points directly affects the computational time, we calculate the average time required to generate a description for 20 randomly selected objects from the RGB-D dataset. Fig. 8 compares the average computation time of the selected object descriptors in which several observations can be made; first, GOOD descriptor is the most computation time efficient descriptor. In contrast, GFPFH descriptor is the computationally most expensive descriptor. ESF, VFH and GRSD descriptors achieve a medium performance in terms of computation time. Overall, GOOD descriptor achieves the best performance, which is around 10 times better performance than ESF and 44, 50 and 254 times better performance than VFH, GRSD and GFPFH descriptors. VFH, GRSD and GFPFH descriptors are extremely time consuming descriptors. The underlying reason is that GOOD descriptor works directly on 3D point clouds and requires neither triangulation of the object’s points nor surface meshing. According to the evaluations our approach is competent for robotic applications with strict limits on the memory footprint and computation time requirements. 5.5 System demonstration To show all the described functionalities and properties of the proposed GOOD descriptor, a real demonstration was performed. For this purpose, GOOD has been integrated in the object perception system presented in [13,20] and [19] (see Fig. 9 ). In this demonstration a table is in front of a robot and two users interact with the system. During the demonstration, users presented objects to the system and provided the respective category labels. Therefore, throughout this session, the system must be able to detect, conceptualize and recognize unknown (i.e. new) objects. It should be noted that a constraint has been set on the Z axis that the initial direction of Z axis of objects’ LRF should be similar to direction of Z axis of the table. It is assumed that there is no learned categories in the memory at the beginning of the demonstration. It was observed that the proposed object descriptor is capable to provide distinctive global feature for recognizing different type of objects. It also estimates pose of objects and build orthographic projections for object manipulation purposes. A video of this demonstration is available in: 6 Conclusion This paper presented a global object descriptor named GOOD (i.e. Global Orthographic Object Descriptor) that provides a good trade-off between descriptiveness, computation time and memory usage, allowing concurrent object recognition and pose estimation. For an object, GOOD is computed on a unique and repeatable local reference frame. It is calculated with the discretization of the three orthographic projections and their concatenation to form a single description for the given object. A set of experiments were carried out to assess the performance of GOOD and compare it with other state-of-art descriptors with respect to several characteristics including descriptiveness, scalability, robustness (Gaussian noise and varying low-level point cloud density) and efficiency (memory footprint and computation time). Experimental results show that the overall classification performance obtained with GOOD is comparable to the best performances obtained with the state-of-the-art descriptors. GOOD outperformed the selected state-of-the-art descriptors (i.e. VFH, ESF, GRSD and GFPFH descriptors), achieving appropriate descriptiveness and significant robustness to Gaussian noise. GOOD was robust to varying low-level point cloud density too. The accuracy of VFH, ESF and GRSD was better than GOOD in the case of varying medium and high point cloud density. In addition, GOOD obtained the best computation time performance. Besides, GOOD demonstrates the capability of estimating objects’ poses and building orthographic projections for object manipulation purposes. We are currently working on integrating and using the GOOD descriptor for manipulation purposes and we would like to put the source code of the GOOD descriptor available to the research community in the ROS 4 4 repository and Point Cloud Library 5 5 in the near future. Acknowledgments This work was funded by National Funds through FCT project PEst-OE/EEI/UI0127/2014 and FCT scholarship SFRH/BD/94183/2013. References [1] A. Aldoma Z.-C. Marton F. Tombari W. Wohlkinger C. Potthast B. Zeisl R.B. Rusu S. Gedikli M. Vincze Point cloud library IEEE Robot. Autom. Mag. 1070 9932/12 2012 [2] A. Andreopoulos J.K. Tsotsos 50 years of object recognition: Directions forward Comput. Vis. Image Underst. 117 8 2013 827 891 [3] Bo L. Lai K. Ren X. D. Fox Object recognition with hierarchical kernel descriptors IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011 2011 IEEE 1729 1736 [4] R. Bro E. Acar T.G. Kolda Resolving the sign ambiguity in the singular value decomposition J. Chemometr. 22 2 2008 135 140 [5] Chen H. B. Bhanu 3d free-form object recognition in range images using local surface patches Pattern Recogn. Lett. 28 10 2007 1252 1262 [6] T.M. Cover J.A. Thomas Elements of Information Theory 2012 John Wiley & Sons [7] Deng J. Dong W. R. Socher Li L.-J. Li K. Fei-Fei L. Imagenet: A large-scale hierarchical image database IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009 2009 IEEE 248 255 [8] H.Q. Dinh S. Kropac Multi-resolution spin-images IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2006 1 2006 IEEE 863 870 [9] A. Frome D. Huber R. Kolluri T. Bulow J. Malik Recognizing objects in range data using regional point descriptors Computer Vision-ECCV 2004 2004 Springer 224 237 [10] Guo Y. F.A. Sohel M. Bennamoun Lu M. Wan J. Trisi: A distinctive local surface descriptor for 3d modeling and object recognition. GRAPP-IVAPP 2013 86 93 [11] B.K. Horn Extended gaussian images Proc. IEEE 72 12 1984 1671 1686 [12] A.E. Johnson M. Hebert Using spin images for efficient object recognition in cluttered 3d scenes IEEE Trans. Pattern Anal. Mach. Intell. 21 5 1999 433 449 [13] S. Kasaei M. Oliveira G. Lim L. Seabra Lopes A.M. Tome Interactive open-ended learning for 3d object recognition: an approach and experiments J. Intell. Robot. Syst. 80 3–4 2015 537 553 [14] Lai K. Bo L. Ren X. D. Fox A large-scale hierarchical multi-view rgb-d object dataset Robotics and Automation (ICRA), 2011 IEEE International Conference on 2011 IEEE 1817 1824 [15] Y. Li, S. Pirk, H. Su, C.R. Qi, L.J. Guibas, Fpnn: field probing neural networks for 3d data, arXiv preprint arXiv:1605.06240 (2016). [16] Z.-C. Marton D. Pangercic R.B. Rusu A. Holzbach M. Beetz Hierarchical object geometric categorization and appearance classification for mobile manipulation 10th IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2010 2010 IEEE 365 370 [17] A. Mian M. Bennamoun R. Owens On the repeatability and quality of keypoints for local feature-based 3d object retrieval from cluttered scenes Int. J. Comput. Vis. 89 2–3 2010 348 361 [18] G.A. Miller Wordnet: a lexical database for english Commun. ACM 38 11 1995 39 41 [19] M. Oliveira Lim G.H. L. Seabra Lopes S. Hamidreza Kasaei A.M. Tome A. Chauhan A perceptual memory system for grounding semantic representations in intelligent service robots IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014), 2014 2014 IEEE 2216 2223 [20] M. Oliveira L.S. Lopes Lim G.H. S.H. Kasaei A.M. Tomé A. Chauhan 3d object perception and perceptual learning in the race project Robot. Auton. Syst. 2015 [21] R. Osada T. Funkhouser B. Chazelle D. Dobkin Shape distributions ACM Trans. Graph. 21 4 2002 807 832 [22] Pang G. U. Neumann Fast and robust multi-view 3d object recognition in point clouds International Conference on 3D Vision (3DV), 2015 2015 IEEE 171 179 [23] G. Pasqualotto P. Zanuttigh G.M. Cortelazzo Combining color and shape descriptors for 3d model retrieval Signal Process.: Image Commun. 28 6 2013 608 623 [24] D.M. Powers, Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation (2011). [25] D. Regazzoni G. de Vecchi C. Rizzi Rgb cams vs rgb-d sensors: low cost motion capture technologies performances and limitations J. Manuf. Syst. 33 4 2014 719 728 [26] R.B. Rusu N. Blodow M. Beetz Fast point feature histograms (fpfh) for 3d registration IEEE International Conference on Robotics and Automation, 2009. ICRA’09. 2009 IEEE 3212 3217 [27] R.B. Rusu G. Bradski R. Thibaux Hsu J. Fast 3d recognition and pose using the viewpoint feature histogram IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2010 2010 IEEE 2155 2162 [28] R.B. Rusu A. Holzbach M. Beetz G. Bradski Detecting and segmenting objects for mobile manipulation IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops), 2009 2009 IEEE 47 54 [29] R.B. Rusu Z.C. Marton N. Blodow M. Dolha M. Beetz Towards 3d point cloud based object maps for household environments Robot. Auton. Syst. 56 11 2008 927 941 [30] H. Su S. Maji E. Kalogerakis E. Learned-Miller Multi-view convolutional neural networks for 3d shape recognition Proceedings of the IEEE International Conference on Computer Vision 2015 945 953 [31] F. Tombari S. Salti L. Di Stefano Unique signatures of histograms for local surface description Computer Vision–ECCV 2010 2010 Springer 356 369 [32] W. Wohlkinger M. Vincze Ensemble of shape functions for 3d object classification IEEE International Conference on Robotics and Biomimetics (ROBIO), 2011 2011 IEEE 2987 2992 [33] Wu Z. Song S. A. Khosla Yu F. Zhang L. Tang X. Xiao J. 3d shapenets: a deep representation for volumetric shapes Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2015 1912 1920 [34] Zhong Y. Intrinsic shape signatures: A shape descriptor for 3d object recognition IEEE 12th International Conference on Computer Vision Workshops (ICCV Workshops), 2009 2009 IEEE 689 696 "
    },
    {
        "doc_title": "Cognitive bio-radar: The natural evolution of bio-signals measurement",
        "doc_scopus_id": "84984911350",
        "doc_doi": "10.1007/s10916-016-0572-8",
        "doc_eid": "2-s2.0-84984911350",
        "doc_date": "2016-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Artificial Intelligence",
            "Feasibility Studies",
            "Monitoring, Physiologic",
            "Radar",
            "Respiratory Rate",
            "Signal Processing, Computer-Assisted"
        ],
        "doc_abstract": "© 2016, Springer Science+Business Media New York.In this article we discuss a novel approach to Bio-Radar, contactless measurement of bio-signals, called Cognitive Bio-Radar. This new approach implements the Bio-Radar in a Software Defined Radio (SDR) platform in order to obtain awareness of the environment where it operates. Due to this, the Cognitive Bio-Radar can adapt to its surroundings in order to have an intelligent usage of the radio frequency spectrum to improve its performance. In order to study the feasibility of such implementation, a SDR based Bio-Radar testbench was developed and evaluated. The prototype is shown to be able to acquire the heartbeat activity and the respiratory effort. The acquired data is compared with the acquisitions from a Biopac research data acquisition system, showing coherent results for both heartbeat and breathing rate.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Analysis of fMRI images with bi-dimensional empirical mode decomposition based-on Green's functions",
        "doc_scopus_id": "84978174432",
        "doc_doi": "10.1016/j.bspc.2016.06.019",
        "doc_eid": "2-s2.0-84978174432",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Bi-dimensional empirical mode decompositions",
            "Classification accuracy",
            "Contour integration",
            "Empirical Mode Decomposition",
            "FMRI",
            "Intrinsic Mode functions",
            "Mode decomposition method",
            "VIMFs"
        ],
        "doc_abstract": "© 2016 Elsevier LtdWe present a new method for decomposing two-dimensional data arrays with empirical mode decomposition (EMD). It performs envelope surface interpolation based on Green's functions in tension (GiT) to extract bi-dimensional intrinsic mode functions (BIMFs). The new method is called GiT-BEMD and outperforms existing bi-dimensional ensemble EMD (BEEMD) variants in terms of computational costs and quality of extracted intrinsic modes. More specifically, it is easy to implement, much faster than BEEMD, very robust and free from processing artifacts. GiT-BEMD is applied to fMRI data recorded during a contour integration task. Features extracted from resulting volume intrinsic mode functions (VIMFs) achieve higher classification accuracy compared to the canonical BEEMD. The new method thus provides a valuable alternative to existing mode decomposition methods for analyzing images.",
        "available": true,
        "clean_text": "serial JL 273545 291210 291874 291880 291883 31 Biomedical Signal Processing and Control BIOMEDICALSIGNALPROCESSINGCONTROL 2016-07-05 2016-07-05 2016-07-05 2016-07-05 2016-08-24T18:01:58 S1746-8094(16)30079-9 S1746809416300799 10.1016/j.bspc.2016.06.019 S300 S300.2 FULL-TEXT 2016-08-24T13:21:36.976923-04:00 0 0 20160901 20160930 2016 2016-07-05T05:27:04.383834Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor highlightsabst primabst ref specialabst 1746-8094 17468094 true 30 30 C Volume 30 6 53 63 53 63 201609 September 2016 2016-09-01 2016-09-30 2016 article fla © 2016 Elsevier Ltd. All rights reserved. ANALYSISFMRIIMAGESBIDIMENSIONALEMPIRICALMODEDECOMPOSITIONBASEDONGREENSFUNCTIONS ALBADDAI S 1 Introduction 2 Materials 2.1 Experimental protocol 2.2 Data set 3 Methods 3.1 A special noise-assisted BEMD 3.2 BEMD based on Green's function 3.3 Features generation and selection 3.3.1 Principal components analysis 3.3.2 t-Test 3.4 Support vector machine classifier 4 Results and discussion 4.1 GiT-BEMD results 4.1.1 First level analysis 4.1.2 Second level analysis 4.2 Classification results 4.2.1 Reducing data dimension 5 Conclusion Acknowledgement References ALBADDAI 2014 218 236 S ALSUBARI 2015 193 205 K ALSUBARI 2015 e0119489 K BHUIYAN 2008 1313 1316 S IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP ANOVELAPPROACHFASTADAPTIVEBIDIMENSIONALEMPIRICALMODEDECOMPOSITION BHUIYAN 2009 18 24 S 2009IEEEINTERNATIONALCONFERENCEMACHINELEARNINGAPPLICATIONS STUDYBIDIMENSIONALEMPIRICALMODEDECOMPOSITIONMETHODFORVARIOUSRADIALBASISFUNCTIONSURFACEINTERPOLATORS CALHOUN 2003 281 288 V PROCEEDINGSINTERNATIONALWORKSHOPINDEPENDENTCOMPONENTANALYSISBLINDSIGNALSEPARATION ICAFUNCTIONALMRIDATAOVERVIEW CASTELLANO 2014 M CHANG 2001 C ALIBRARYFORSUPPORTVECTORMACHINES CICHOCKI 2007 A ICALABTOOLBOX CICHOCKI 2009 A NONNEGATIVEMATRIXTENSORFACTORIZATIONSAPPLICATIONSEXPLORATORYMULTIWAYDATAANALYSISBLINDSOURCESEPARATION DAMERVAL 2005 701 704 C DEVORE 1997 J STATISTICSEXPLORATIONANALYSISDATA HUANG 1998 903 995 N HUMEAUHEURTIER 2015 A HYVARINEN 2001 A INDEPENDENTCOMPONENTANALYSIS KOURTZI 2005 440 452 Z LINDERHED 2002 1 8 A WAVELETINDEPENDENTCOMPONENTANALYSISAPPLICATIONSIXPROCEEDINGSSPIEVOL4738 2DEMPIRICALMODEDECOMPOSITIONSINSPIRITIMAGECOMPRESSION LIU 2005 33 36 Z LIU 2004 803 806 Z PROCEEDINGS17THIEEEINTERNATIONALCONFERENCEPATTERNRECOGNITIONICPR04 TEXTURECLASSIFICATIONTHROUGHDIRECTIONALEMPIRICALMODEDECOMPOSITION MATLAB 2013 STOOLBOXRELEASE2013 MINSUNG 2010 13 16 K 2010IEEE10THINTERNATIONALCONFERENCESIGNALPROCESSINGICSP ANEWTWODIMENSIONALEMPIRICALMODEDECOMPOSITIONFORIMAGESUSINGINPAINTING NUNES 2003 1019 1026 J NUNES 2009 125 175 J NUNES 2005 177 188 J RILLING 2007 936 939 G RODEN 2012 C MRICRO SCHOLKOPF 2002 B LEARNINGKERNELS WESSEL 2009 1247 1254 P WESSEL 1998 77 93 P WU 2009 339 372 Z XIONG 2006 1516 1521 C ALBADDAIX2016X53 ALBADDAIX2016X53X63 ALBADDAIX2016X53XS ALBADDAIX2016X53X63XS 2018-07-05T00:00:00Z UnderEmbargo © 2016 Elsevier Ltd. All rights reserved. item S1746-8094(16)30079-9 S1746809416300799 10.1016/j.bspc.2016.06.019 273545 2016-08-24T13:21:36.976923-04:00 2016-09-01 2016-09-30 true 2068346 MAIN 11 60213 849 656 IMAGE-WEB-PDF 1 fx1 true 3267 88 219 gr1 16482 92 219 gr10 6024 86 219 gr2 9298 164 165 gr3 2621 72 219 gr4 3922 112 219 gr5 19179 94 219 gr6 9819 80 219 gr7 10671 69 219 gr8 12287 70 219 gr9 5027 98 219 gr9 18150 269 602 fx1 true 9182 200 500 gr1 49692 277 660 gr10 46721 317 811 gr2 5426 176 178 gr3 11469 216 659 gr4 22070 289 565 gr5 75006 257 602 gr6 23946 206 565 gr7 31121 191 604 gr8 38081 193 604 si1 558 14 176 si10 287 17 49 si11 488 18 121 si12 937 15 243 si13 655 15 186 si14 489 16 121 si15 962 37 223 si16 841 47 164 si17 160 11 19 si18 317 15 86 si19 1096 15 296 si2 1166 40 315 si20 491 15 173 si21 252 11 57 si22 294 15 75 si23 306 16 72 si24 1002 18 299 si25 1399 98 169 si26 947 64 131 si27 1784 49 331 si28 941 15 301 si29 651 47 111 si3 538 15 169 si30 871 20 207 si31 1188 49 259 si32 834 14 330 si33 514 18 153 si4 374 15 140 si5 349 19 87 si6 274 18 51 si7 498 18 146 si8 472 18 146 si9 950 47 213 BSPC 830 S1746-8094(16)30079-9 10.1016/j.bspc.2016.06.019 Elsevier Ltd Fig. 1 Left: Stimuli are built with Gabor patches either forming no contour line (NCT) or a contour line (CT). The latter is highlighted just for visualization. Middle: shows a subject being prepared for the scanner. An EEG cap is put on the subject's head and he/she is laid on a movable bed to take him/her inside the scanner. Right: shows two recorded slices during experiment responding to the two stimuli. Fig. 2 One slice of an fMRI brain volume. Fig. 3 Graphical representation of SVD. Fig. 4 Illustration of the framework of this study. Fig. 5 Comparative decomposition of one slice (shown in Fig. 2) of an fMRI brain volume: BIMF 1, 2 and 3. Left to right: canonical BEEMD (E =20), GiT-BEMD (without noise), GiT-BEMD (assisted-noise, E =1), GiT-BEMD (E =2), GiT-BEMD (E =10), GiT-BEMD (E =20). Fig. 6 An activity distribution resulting from first level (SPM 1) and a second level (SPM 2) analysis, respectively. Fig. 7 Illustration of activity differences, averaged over all subjects, for modes VIMF 1, VIMF 2 and VIMF 3, respectively. Only above threshold activity differences (range 0.7–1) are shown. Fig. 8 Illustration of the t values resulting from the statistical voxel-wise test of the activity distributions seen in VIMF 1, VIMF 2 and VIMF 3 for the two stimulus conditions: CT and NCT. Only t-values corresponding to a significant level p =0.001 are highlighted. Fig. 9 Illustration of the classification and a leave-one-out cross-validation framework based on BIMFs. Dotted lines represent the tuning parameters proper for the process. Fig. 10 Illustration of the normalized amount of total variance explained by PCA, and the most discriminant principal components ranked by a t-test for all VIMFs. Left: represents the extracted features of VIMF1 and Right: VIMF3, respectively. Table 1 MNI coordinates of the activity distributions according to first level analysis highlighted in Fig. 6 extracted by SPM 1 and in Fig. 7 extracted from GiT-BEMD. Modes x y z Anatomical structure VIMF 1 −51 −70 4 Left middle temporal gyrus −51 −73 16 left middle occipital gyrus −9 65 28 Left superior medial gyrus −18 −46 79 Left superior parietal lobule −24 64 7 Left superior frontal gyrus VIMF 2 −42 −52 58 Left inferior parietal lobule 0 53 37 Left superior medial gyrus 12 56 37 Right superior medial gyrus VIMF 3 9 29 49 Right superior medial gyrus −6 41 49 Left superior medial gyrus 18 −79 28 Right superior occipital gyrus 15 −61 61 Right precuneus SPM 1 45 −25 64 Right postcentral gyrus 36 −19 70 Right precentral gyrus −42 −22 64 Left precentral gyrus Table 2 MNI coordinates of the activity distributions according to second level analysis highlighted in Fig. 6 extracted by SPM 2 and in Fig. 8 extracted from GiT-BEMD. Modes x y z Anatomical structure Condition VIMF 1 −30 −85 10 Left middle occipital gyrus CT −39 53 25 Left middle frontal gyrus CT 12 −43 13 Unknown area NCT 18 47 28 Right superior frontal gyrus NCT VIMF 2 −33 26 14 Unknown area CT 6 −76 43 Unknown area, right precuneus CT 60 −13 34 Right postcentral gyrus CT −60 −19 46 Left inferior parietal lobule CT −42 −56 −11 Left middle orbital gyrus CT −60 5 −29 Left middle temporal gyrus CT 0 56 1 Left superior medial gyrus CT −12 −34 37 Unknown area map NCT 21 −40 43 Unknown area map NCT VIMF 3 39 50 19 Right middle frontal gyrus CT −45 −22 −5 Unknown area map CT 51 −64 19 Right middle temporal gyrus CT −48 −70 10 Left middle temporal gyrus CT −21 −85 10 Left middle occipital gyrus CT −18 −73 49 Left superior parietal lobule CT −54 8 10 Left IFG (p. opercularis) CT 21 −37 67 Right postcenteral gyrus CT −3 −34 25 Unknown area map NCT SPM 2 0 50 1 Left anterior cingulate cortex CT 0 32 58 Left superior medial gyrus CT −36 −13 67 Left precentral gyrus CT 51 20 −11 Right inferior frontal gyrus CT 30 8 −20 Right temporal pole CT −6 2 20 Left caudate nucleus CT Table 3 Results of the baseline classification VAF. Method Acc ± std Spec ± std Sens ± std SVM-SMO 0.50±0.04 0.54±0.08 0.40±0.04 Table 4 Shows the optimal parameters of sigmoid kernel, chosen by a grid search approach in a sensible ranges, and the default parameters as well. Parameter Default Optimal Cost (c) 1 16 Gamma (γ) 1 8 Coef (r) −1 −1 Table 5 Statistical measures evaluating classification results obtained by PCA and SVM classifier to the VIMFs extracted by GiT-BEMD. The optimal parameters showed in Table 4 and features are used with LOOCV and t-test feature selection. SVM Modes Default parameters Best tuning parameters Acc Spec Sens Acc Spec Sens VIMF1 0.92[16] 0.89 0.94 0.92[23] 0.89 0.94 VIMF2 0.87[13] 0.84 0.89 0.92[13] 0.89 0.94 VIMF3 0.82[16] 0.84 0.79 0.84[21] 0.89 0.79 VIMF4 0.79[30] 0.79 0.79 0.79[11] 0.74 0.84 VIMF5 0.63[18] 0.68 0.57 0.68[18] 0.68 0.68 VRes 0.68[19] 0.68 0.68 0.71[5] 0.63 0.78 Analysis of fMRI images with bi-dimensional empirical mode decomposition based-on Green's functions Saad Al-Baddai a b ⁎ Karema Al-Subari a b Ana Maria Tomé c Bernd Ludwig b Diego Salas-Gonzales a Elmar Wolfgang Lang a a CIML Lab, Department of Biophysics, University of Regensbug, 93040 Regensburg, Germany CIML Lab, Department of Biophysics, University of Regensbug Regensburg 93040 Germany b Department of Information Sciences, University of Regensburg, 93040 Regensburg, Germany Department of Information Sciences, University of Regensburg Regensburg 93040 Germany c DETI-IEETA, Universidade de Aveiro, 3810-193 Aveiro, Portugal DETI-IEETA, Universidade de Aveiro Aveiro 3810-193 Portugal ⁎ Corresponding author. Graphical abstract We present a new method for decomposing two-dimensional data arrays with empirical mode decomposition (EMD). It performs envelope surface interpolation based on Green's functions in tension (GiT) to extract bi-dimensional intrinsic mode functions (BIMFs). The new method is called GiT-BEMD and outperforms existing bi-dimensional ensemble EMD (BEEMD) variants in terms of computational costs and quality of extracted intrinsic modes. More specifically, it is easy to implement, much faster than BEEMD, very robust and free from processing artifacts. GiT-BEMD is applied to fMRI data recorded during a contour integration task. Features extracted from resulting volume intrinsic mode functions (VIMFs) achieve higher classification accuracy compared to the canonical BEEMD. The new method thus provides a valuable alternative to existing mode decomposition methods for analyzing images. Keywords Empirical mode decomposition Green's function FMRI SVM VIMFs 1 Introduction Analyzing functional magnetic resonance imaging (fMRI) data is still a challenging task. It includes identifying spatial regions of the brain activated by a specific task or even in the resting state, mapping out related functional brain networks and developing a robust classification system for different brain states. Modern data mining and machine learning methods offer a plethora of tools satisfying the specific requirements of analyzing functional brain images and unraveling underlying neuronal processing networks. With the emergence of new and more advanced experimental designs and equipments, big data analysts try keeping pace with technological progress by developing new tools. Traditionally, mapping out cognitive brain activities from fMRI recordings is achieved by a voxel-wise statistical hypothesis testing. Such tests are done deepening on β-images resulting from a linear regression analysis. This kind of analysis is performed by employing powerful tools like the widely used statistical parametric mapping (SPM). The main disadvantage of such univariate techniques is their ignorance of existing spatial correlations. Alternatively, exploratory analysis techniques like independent/principal component analysis (ICA/PCA) [6,9] and nonnegative matrix and tensor factorization (NMF/NTF) [10] offer a more global rather than only local analysis. The latter methods, however, are built upon assumptions and constraints (orthogonality, statistical independence) which might not always be obeyed. Generally, spatial variations and corresponding voxel time courses of activity distributions in fMRI images represent non-linear and non-stationary signals while the techniques mentioned require at least wide-sense stationary signals. A decade ago, Huang et al. [13] proposed an empirical method to analyze such data sets called empirical mode decomposition (EMD). Together with a Hilbert transform, this method allows to study the instantaneous frequency content of signals. Its noise-assisted variant, called ensemble empirical mode decomposition (EEMD) represents a powerful tool for analyzing biomedical time series. By the way, recently, different variants of one-dimensional EMD algorithms were embedded in a toolbox which is called EMDLAB [2]. Concerning fMRI data, obviously, two-dimensional data arrays were of special interest [25]. First attempts in applying EMD to bi-dimensional images considered an approach called pseudo-two-dimensional EMD where data was treated as a collection of one-dimensional data arrays. Although this approach is fast, it needs a coherence structure associated with the spatial scales in a specific direction. Hence, the use of pseudo EMD is limited to specific applications. Alternative approaches to deal with bi-dimensional EMD (BEMD) have been proposed as well. There it is generally required finding local maxima and minima and subsequently interpolating these points in each iteration of the process [11,17,18,23,24,31]. In [11] interpolation methods based on a Delaunay triangulation with subsequent cubic interpolation on triangles has been studied. Instead of considering envelope surface interpolation, Bhuiyan et al. [4,5] studied both a direct envelope surface estimation method and radial basis function interpolators. Similarly, Xiong et al. [31] implement a FastRBF algorithm for the estimation of the envelope surfaces. However, detection and interpolation of local extrema render the process complicated and time consuming. A major breakthrough has been achieved by Wu et al. [30], who recently proposed a multi-dimensional ensemble empirical mode decomposition (MEEMD) for multidimensional data arrays. MEEMD turned out to be the best choice in practical applications, for example, medical image analysis [1], image analysis [22], texture analysis [19], laser speckle contrast images [14], etc. In this study, we consider the BEEMD approach developed by Wu et al. [30] as the canonical BEMD variant, and contrast it with a novel and computationally highly efficient bi-dimensional EMD approach which replaces the direct cubic spline envelope surface interpolation step by an interpolation based on Green's function with tension [29]. The technique is borrowed from geophysics where it is in use already since more than a decade [28]. This new approach is called Green's-function-in-tension BEMD (GiT-BEMD). This new technique is applied in this study to fMRI images to extract bi-dimensional intrinsic mode functions (BIMFs) on various spatial scales. The results are compared with corresponding results of a recent study which used canonical BEEMD on the same data [1]. We show the benefits of the new method in terms of reducing the computational load while simultaneously enhancing image quality and information content of the extracted BIMFs. The latter, finally, are re-arranged into 3D volume intrinsic mode function (VIMFs). We discuss an application of the new technique to fMRI data collected during a contour-integration task and employ features generated from the VIMFs to discriminate between two stimulus conditions, contour true (CT) and non-contour true (NCT) (for details see [1]). This paper is outlined as follows: Section 2 presents a short outline of the experimental setup and the data sets. Section 3 explains the new BEMD technique using Green's function in tension. Also, common feature extraction and selection techniques, namely PCA and t-test, are briefly mentioned in Section 3 as well. Then, in Section 4 results are discussed in detail. Finally, Section 5 concludes on the highlights of the proposed technique. 2 Materials 2.1 Experimental protocol In this study, a 3-Tesla head scanner (Siemens Allegra, Erlangen, Germany) was used to record fMRI images, see Fig. 1 . Hence, the functional series volumes were continuously obtained with 46 interleaved axial slices employing a standard T 2*-weighted echo-planar imaging sequence. Further parameters were set as follows: repetition time TR=2000[ms]; echo time TE=30[ms]; flip angle θ =90[°]; 64×64 matrices; in-plane resolution: 3[mm]×3[mm]; slice thickness: 3[mm]. Besides the functional scans, a 3D high resolution structural scan was acquired by employing the following parameters: TR=2250[ms]; TE=2.6[ms]; 1[mm] isotropic voxel size. Such sequence is required to enhance distinguishing between white and gray matter. To avoid head movement artifacts and for safety reasons, subjects (participants) were laid down inside the scanner and their heads were fixed in the head coil. Gabor stimuli either forming a contour (CT) or no contour (NCT) were presented as visual stimuli. The resolution of the projector was 800×600 pixels and the refresh rate was 72[Hz]. The viewing distance to the stimulus screen was 64[cm]. Each stimulus array encompassed 90–100 Gabor patterns, subtended by 16.6 by 16.5 degrees of visual angle. A cohort of 19 subjects has been investigated during 3 sessions, each session conducted (≲150) trials with Gabor stimuli, organized in 5 blocks. Thus, a perceptual detection task was initiated, see Fig. 1. Then, in each trial, the subject had to decide, in a fixed time 194[ms], whether or not he/she could recognize a contour when the stimulus was presented by pressing an associated response button with either the left or the right hand. The contour and non-contour stimuli were presented interchangeably followed by a blank screen. Consequently, this resulted in 2 conditions: here was a contour and the subject recognized it correctly (decoded as CT) and there was no contour and the subject recognized its absence correctly (decoded as NCT). Hence, this study concentrates on analyzing these two conditions: CT and NCT. 2.2 Data set The number of subjects participating in the study was 19 volunteers, 6 male and 13 female. They are, on the average (22.79±2.7)[years] years old. Subjects were treated according to the principles laid down in the Helsinki declaration. FMRI recordings were obtained by scanning the whole brain in a number of slices, here (N s =46), which together form a 3D activity distribution called a 3D volume/scan, henceforth called a volume of activations. Each such brain slice, which represents a 2D data array, is decomposed by GiT-BEMD into K =6 BIMFs per slice. Then, the extracted BIMFs are combined to form a VIMF. Fig. 5 illustrates the three interesting modes of a single brain slice decomposed by different BEMD methods. In summary, N c =790 scans per session were recorded, each one comprising N s =46 brain slices. A complete scan is further-on called volume scan. The latter is equivalent to a data volume V c (x, y, z) where x, y, z denote the spatial coordinates of the voxels in the brain volume. Here index c represents the number of scans, thus forming an index set { C | c ∈ ℕ , c = 1 , … , 790 } . Because the hemodynamic response (HR) usually happens roughly τ ≈5[s] after the stimulus onset, the subset of indices corresponding to volume scans acquired during this time were selected. Thus, for any given stimulus/response condition, the corresponding volumes were registered while the hemodynamic response was active. For each of the stimuli, contour true (CT) and non-contour true (NCT), there were N t ≈90–120 trials across all three sessions. To reduce the computational cost per stimulus condition, these volume scans were averaged. Thus, one volume of average activations per condition and session 〈V sc (x, y, z)〉 was analyzed according to V ¯ ( x , y , z ) ≡ 〈 V sc ( x , y , z ) 〉 = 1 N t ∑ sc ∈ C V sc ∈ C ( x , y , z ) where sc ∈ C denotes indices of volumes belonging to stimulus condition sc ∈{CT, NCT}. Thus 〈 V sc ∈ C ( x , y , z = n s ) 〉 ≡ X n s represents an average brain slice to be decomposed by GiT-BEMD into K BIMFs. If repeated for all brain slices X n s , n s = 1 , … , N s , this decomposition results in K volume intrinsic mode functions VIMFk, k =1, …, K. For further processing such as in classification, each VIMF is concatenated into a column vector. Right on the beginning, the collected data were preprocessed with the software package SPM 8 (Wellcome Department of Imaging Neuroscience, London, UK), running under MATLAB 7.0 (Mathworks, Natick, MA). This preprocessing included slice-time correction, motion-correction, spatial normalization and spatial Gaussian smoothing. Data without any pre-processing or/and decomposition has been considered as “raw” images and has been analyzed, too. For further details about the data acquisition and experiment, see [1] and [3]. 3 Methods Each fMRI brain slice is encoded as a two-dimensional array of intensity values f(x, y)= f(x) at the 2D Cartesian coordinates x = [ x y ] T . Before image decomposition, pixel values are normalized to unit variance. 3.1 A special noise-assisted BEMD Here we propose a special variant of a noise-assisted EMD starting with two noisy versions of every image. First white noise with zero mean and standard deviation σ =0.2 is added and subtracted, thus forming the two versions f ˜ * ( x , y ) : f ˜ + ( x , y ) = f ( x , y ) + ɛ and f ˜ − ( x , y ) = f ( x , y ) − ɛ . Therefore, the EMD decomposition of each noisy version yields (1) f ˜ * ( x , y ) = ∑ i = 1 I c i * ( x , y ) + r * ( x , y ) where r *(x, y) denotes the residuum, and the c i * ( x , y ) designate the noise-enhanced bi-dimensional intrinsic modes. Note that by averaging the two noisy versions 0.5(f +(x, y)+ f −(x, y)), the original image f(x, y) is obtained. Therefore performing the BEEMD with a fixed number I of BIMFs, each BIMF of f(x, y) is naturally obtained by averaging the corresponding BIMFs of the noisy versions. The main steps of this noise-assisted BEMD [22,21] are as follows: 1. Assign the number of intrinsic modes (BIMFs) I =5 Assign the number of sifting steps J =5 Initialize r * ( x , y ) = f ˜ * ( x , y ) . 2. Extract the i =1, 2, …, I BIMFs (a) Do the sifting by repeating the following steps i Extract all local 2D-maxima and 2D-minima of r *(x, y). ii Build 2D surface interpolation by employing the extrema from the previous step to obtain a 2-D maxima envelope surface s max (x, y) and 2-D minima envelope surface s min (x, y). Calculate the average of the envelope surfaces m(x, y) m ( x , y ) = 0.5 ( s max ( x , y ) + s min ( x , y ) ) iii Update function r *(x, y) by subtracting the average surface m(x, y) from the r *(x, y); r * ( x , y ) ← r * ( x , y ) − m ( x , y ) (b) If the sifting loop does not finish, go to step 2(a)i. Otherwise the BIMF is c i * ( x , y ) = r * ( x , y ) 3. Calculate the new r *(x, y) by subtracting all the computed BIMFs from the noise-enhanced image. r * ( x , y ) = f ˜ * ( x , y ) − ∑ i < ( i + 1 ) c i * ( x , y ) 4. If (i +1)< I, go to step 2 and estimate the next (i +1) BIMF, otherwise r *(x, y) is the residuum. Common ensemble decomposition algorithms apply this procedure to an ensemble of E noisy versions of the original image with concomitant increase in computational costs. Employing GiT-BEMD, only two noisy versions of each image are needed, reducing the computational load dramatically. 3.2 BEMD based on Green's function This work proposes a novel method to compute the 2D envelopes by using Green's functions. Therefore after finding the local extrema (minima or maxima) by using an 8-connected neighborhood strategy, these extremal points are considered the known points of the surface envelopes. Just for the sake of simplicity, let's assume that N is the number of maxima (or minima), therefore the surface envelopes are expressed as a weighted sum of Green's functions as follows (2) s ( x u ) = ∑ n = 1 N w n Φ ( x u , x n ) where w n are the weights, Φ(..) denote the Green's functions and the x u = [ x y ] represent the Cartesian coordinates of the interpolated intensity values (pixel value) s(x u ). Moreover x n refer to coordinates of either the local maxima or the minima. The Green's function, expressed in 2D Cartesian coordinates, is defined as follows [28]: (3) Φ ( x u , x n ) = log ( p | x u − x n | ) + K 0 ( p | x u − x n | ) where |.| denotes the Euclidian distance and K 0(0) represents a modified Bessel function of second kind. Furthermore, p 2 = T/D is related with the tension T applied at the boundaries, and D denotes the flexural rigidity of the surface [28]. This parameter ranges between 0< p ≤1. The envelope estimation procedure is based on two steps: the first step estimates the weights w = [ w 1 w 2 ⋯ w N ] , and the second step estimates the surface envelope for all the values of the coordinates as follows: • Taking the values of local maxima (or minima) as the values s(x n )=[s(x 1), …, s(x N )] T ≡ c =[c 1, c 2, …, c N ] T in a total of N locations x n =[x n , y n ] T and solving Eq. (2) for all locations, a linear system with N equations is obtained which can conveniently be summarized in matrix notation as G w = c where the nth row of matrix G provides the evaluation of the Green's function Φ(x n , x m ), m =1, 2, …, N. Therefore, corresponding weights can be estimated via w = G − 1 c . • Using these weights w, the value s(x u )≡ c u of the envelope surface can be estimated at any point x u by solving Eq. (2). The latter can be re-written as (4) c u = w T Φ where the vector Φ = [ Φ ( x u , x 1 ) Φ ( x u , x 2 ) , … , Φ ( x u , x N ) ] T contains the Green's function values of all distances between the N data constraints and the considered location. In the implementation, a uniform grid with the number of coordinates equal to the size of the image was considered, and all distances are divided by the maximum possible distance of the grid. The GiT-BEMD decomposition was applied to every slice (see Fig. 2 ) of the whole volume independently. Next the extracted BIMFs (see Fig. 5) were de-normalized for each slice by multiplying by the standard deviation computed before the decomposition. Finally, all BIMFs have been organized into 3D volume intrinsic mode function (VIMFs). 3.3 Features generation and selection In this paper, a contour detection system consists of a series of steps: texture extraction, feature generation and classification are the most crucial for its overall performance. Feature generation reduces the dimensionality of the sample space by extracting the most discriminatory information. The feature generation step may also alleviate the worst effects of the so-called curse of dimensionality. Hence, the performance of the feature generation stage has a significant impact on the performance of the next stages, feature selection and classification. PCA is a powerful feature generation technique for high-dimensional spaces like fMRI data. PCA is employed to reduce the dimensionality of the extracted fMRI textures by GiT-BEMD. Afterwards, a common feature selection technique like t-test is used to identify the most relevant components (features). The latter are used as input vectors to a support vector machine (SVM) classifier. 3.3.1 Principal components analysis PCA is a rather general statistical feature generation procedure used in many applications, such as statistical data analysis, dimensionality reduction and data decomposition. The goal of PCA is to find a smaller informative set of variables with less redundancy which represents the original signal as accurately as possible [15]. In PCA, the redundancy is measured in terms of correlations between the observed data sets. The aim of PCA is to decrease the dimensionality of the data by retaining as much data variance as possible. On the other hand, dimensionality reduction implies some information loss. PCA provides the best solution to this approximation in a least mean squares sense. Thus, PCA is implemented on the extracted modes (VIMFs) from all N sj =19 subjects and for the two stimulus conditions, CT and NCT. • First, the data are collected into a data matrix M (5) M = m 11 … m 1 R m 21 … m 2 R ⋮ ⋱ ⋮ m S 1 … m SR • Next, PCA is employed to extract the eigenvolumes U of the co-variance, or, for centered data (see Fig. 3 ), correlation matrix R which span the space of all voxels. Projecting all VIMFs onto these eigenvolumes reveals the contribution of each eigenvolume to every VIMF. Thus, the principle components (features) are extracted from the VIMFs by projecting them onto eigenvolumes resulting from a PCA. These features then are used as input to a classifier. PCA eigenvolumes can be obtained by decomposing the R × S dimensional matrix M of VIMFs via a singular value decomposition (SVD), see Fig. 3, where, D denotes a diagonal matrix, which contains the singular values sorted in descending order along the diagonal. Furthermore, U and V are the eigenvolume matrices of the non-normalized correlation matrix R = MM T and the related kernel matrix K = M T M. Their corresponding column vectors u, v form orthogonal eigenvectors in spaces of dimension R × R and S × S, respectively. A key property of the derived vectors is that all directions are orthogonal (i.e. linearly independent) to each other, so that each eigenvolume is uncorrelated to the others. Very briefly, PCA or SVD thus performs a second order decorrelation of a dataset. 3.3.2 t-Test The Student's t-test is historically a statistical hypothesis test, which used to compare two normally distributed samples or populations [12]. It candidates features with the highest difference of intra-class mean values and the lowest inter-class variability for further analysis. But it could statistically collapse when there is an insufficient number of samples, often less than 30 samples. As enough data is available here, we use the t-test as an univariate feature selection algorithm where features are ranked, and only a subset of most discriminant features z l is selected. First, the data are divided into two sets CT and NCT. Thus, let μ CT ; σ CT denote the average and variance of the data set CT, and μ NCT and σ NCT denote the average and variance of the data set NCT. Also, n 1 and n 2 represent the number of samples for each stimulus condition CT and NCT respectively. Then we define the T-score for a feature i in the data as: T ( i ) = μ CT ( i ) − μ NCT ( i ) σ CT ( i ) n 1 + σ NCT ( i ) n 2 . Finally, features with the highest T-scores (most discriminating features) are selected. 3.4 Support vector machine classifier A support vector machine (SVM) is a classifier, which can deal with linear as well as nonlinear data. The main idea behind an SVM is to separate a given set of training data from different classes by a hyperplane. SVM is a supervised learning technique and was first developed for binary classification problems. It is applied by initially generating an optimal hyperplane maximizing the distance between the two given classes. Representing the normal to the hyperplane by the training data set, the optimization problem can be represented and solved in its dual form as (cf. [27]): (6) maximize α ∈ ℝ m W ( α ) = ∑ i = 1 S − 1 α i − 1 2 ∑ i , j = 1 S − 1 α i α j y i y j k ( z i , z j ) (7) subject to 0 ≤ α i ≤ C for all i = 1 , … , S − 1 (8) and ∑ i = 1 S − 1 α i y i = 0 . where k(z i , z j ) performs the kernel dot product of the non-linearly mapped training data. The parameter C allows controlling the number of margin errors and support vectors (cf. [27]). Here, the mapping is constituted by a sigmoidal kernel, (9) k ( z ( i ) , z ( j ) ) = tanh ( γ z ( i ) T z ( j ) + r ) with parameters γ >0, r <0. The latter parameters have to be set in advance. The constrained optimization problem can be solved via optimizing its Lagragian with Lagrange parameters 0≤ α i ≤ C. During the training phase, the samples with α ≠0 represent the support vectors and determine the hyperplane. In the recall mode, a new test vector z test is classified by employing the following decision function: (10) f ( z test ) = sgn ∑ i = 1 S − 1 α i y i k ( z test , z i ) + b where b denotes the distance from the origin to the considered hyperplane. Note that only the training samples z i with α i ≠0 (support vectors) are required for testing. 4 Results and discussion In this section, the results of a statistical analysis, visualization and classification of the GiT-BEMD-related intrinsic modes are presented and discussed according to the framework shown in Fig. 4 . Results achieved by applying, in a previous study [1], an SPM analysis or a canonical BEMD analysis can be consulted for comparison. Let us start by presenting the general appearance of the extracted intrinsic modes. Fig. 5 exemplifies some intrinsic modes obtained by different variants of a BEMD. There the first column shows modes BIMF1, BIMF2 and BIMF3 as obtained with the canonical BEEMD using an ensemble size E =20. The second column (from left to right) illustrates the same modes resulting from a GiT-BEMD without noise assistance. The remaining four columns show these intrinsic modes as obtained from a GiT-BEMD analysis with noise-assistance and different ensemble sizes E =1, 2, 10, 20, respectively. Substantial differences are clearly visible: • First of all, the modes extracted with the canonical BEEMD (E =20) show streak artifacts, stemming from the decomposition process, which do not show up anymore if GiT-BEMD is applied instead. Also, the mode with the highest spatial frequency does show some structure contrary to the canonical BEEMD case. • Second, noise assistance has a strong impact on the resulting modes. Most importantly, mode mixing is avoided by adding noise during the decomposition process. Further, increasing the size of the ensemble results in smoother and better defined textures in the modes. However, the latter can be achieved only at the expense of a strongly increasing computational load (Run Time: canonical BEEMD (E =20)=2880[s], GiT-BEMD (E =1)=32 [s], GiT-BEMD (E =20)=640[s]). Hence, in practice one has to choose a compromise between a maximally smooth and clearly structured texture within the modes and a reasonably short computation time with an almost equally good structure of the textures. Let us proceed by focusing our discussion on differences of normalized VIMFs obtained from both stimulus conditions. After the GiT-BEMD analysis, the activity distributions within the resulting VIMFs have been normalized across both conditions. Given this normalization, the analysis is called first level analysis. Results are illustrated as difference images (11) Δ i = | VIMFi CT − VIMFi NCT | , i = 1 , 2 , … , I = 5 The various Δ i have been normalized independently, thus distinctly exhibiting the sometimes small differences. Only activity differences above a threshold Δth=0.7Δmax are shown in the images. This threshold has been chosen to correspond to a level of significance p =0, 001 of a first level SPM (SPM 1) and a second level (SPM 2) analysis rendering both, SPM generated and GiT-BEMD related, activity distributions comparable quantitatively. Note that an SPM analysis deals with raw data exclusively while any BEMD analysis first considers a decomposition into intrinsic modes with characteristic spatial frequency scales before any feature selection and classification follows. Let us first summarize shortly the procedures involved in a first and second level analysis: • Generally, a first level analysis is done with each of the 19 members of the cohort separately. Within SPM 1, data is subjected to a GLM analysis according to both stimulus conditions. Subsequently, a Student's t-test, applied to average β-images, identifies significant differences in mean local activations, i.e. averaged over trials and sessions, between both stimulus responses. The result is presented as a map of average t-values per subject. The result of this analysis is a set of 19 t-value maps. Fig. 6 illustrates such a t-value map, obtained by averaging over all 19 participants. Employing a first level BEMD analysis, raw data is averaged over all trials and sessions before a decomposition into intrinsic modes is effected applying any of the variants of BEMD. Because of the averaging, any t-testing cannot be done at this stage. Rather we translate significance levels of β-images, related with the resulting t-value thresholds of the significant activations resulting from the SPM analysis, to choose proper thresholds for the activation difference images. • In a second level analysis, the β-images of all 19 subjects, resulting from an SPM 1 analysis, is subjected to a second t-test producing a single map of t-values indicating brain areas of significant variation between subjects across both stimulus conditions. So the result is a single map of t-values and a corresponding β-image. Applying a BEMD analysis, t-tests are performed for all 19 subjects and for each intrinsic mode across both stimulus conditions. Thus, the difference to an SPM2 analysis only relates to using intrinsic modes instead of β-images as input. In summary, the goal of any BEMD analysis compared to a classical SPM-GLM analysis thus is a precise localization of significant activity differences between both stimulus conditions. As we will show in the following, indeed the new approach provides superior details of significantly different activity distributions related to the stimulus paradigm. 4.1 GiT-BEMD results In general, similar local activity distributions were seen in intrinsic modes when the latter have been extracted from this data set either via the canonical BEEMD or via the GiT-BEMD algorithm, respectively. Consistent with the canonical BEEMD analysis, activation textures in the VIMFs were significantly different for the two stimulus conditions in VIMF3 and VIMF4, despite a high-intersubject variability. However, the current GiT-BEMD analysis also revealed significant differences with the intrinsic modes VIMF1 and VIMF2. For example, the GiT-BEMD-related mode VIMF1 exhibits significant activation in frontal and central brain regions. Similar activation patterns are also exhibited by VIMF2 and VIMF4, respectively, when extracted with BEEMD. Whatever is the mode where such activity blobs emerge, across all 19 subjects, both analysis methods consistently localize activity in the area of the temporal, frontal and occipital gyrus, though to a varying extent. 4.1.1 First level analysis For visualization purposes, the MRIcro analyze viewer [26] has been used. Fig. 7 illustrates mode activation differences Δ i , i =1, …, 3 as averages over all subjects. It robustly highlights activation loci obtained with a GiT-BEMD analysis. The differences corresponding to i =1 to i =3 clearly show highly focused and spatially localized activities. Such results can be compared to results obtained with the canonical BEEMD analysis to demonstrate the superior performance of the new approach. In case of a canonical BEEMD analysis, the following observations can be summarized [1]: • VIMF2 exhibited significant activity differences almost exclusively in the left rectal gyrus. • VIMF3, instead, showed activity differences mainly in the temporal gyrus. However, activity is more pronounced in the left temporal gyrus compared to the right temporal gyrus, especially in the left inferior and middle temporal gyrus. • VIMF4, finally, exhibited a pronounced activity difference in the (left) paracentral lobule. In contrast, with the new technique (GiT-BEMD) the following observations can be made: • VIMF1 exhibits significant activity differences exclusively in the left hemisphere, but varying between occipital, temporal, medial and frontal gyrus. • VIMF2 shows significant activity differences in the left and right medial gyrus and in the left parietal lobule. • VIMF3 also shows significant activity differences in the left and right medial gyrus, but they extend into the precuneus and occipital gyrus. Coordinates of these localized activities according to the Montreal Neurological Institute and Hospital(MNI) coordinate system are collected in Table 1 . These results are in agreement with previous functional neuroimaging results on contour integration [1,7,16] which highlighted similar brain regions. Note that results obtained in the literature cited were obtained with a classical SPM analysis while [1] also applied a canonical BEEMD procedure. In summary, a first level analysis of the data set achieved with either SPM 1, a canonical BEMD analysis or a GiT-BEMD analysis, localized similar significant activity differences in the modes extracted. Compared with an SPM analysis of the raw data (see Fig. 6, left), a decomposition into intrinsic modes helps to unravel activities at various characteristic spatial frequencies and allows to clearly identify significant activity differences (see Fig. 7). In summary, spatial localization is already at this level more focused and precise with the new approach which allows for a better identification of the related brain areas. 4.1.2 Second level analysis While VIMFs 1, 2 and 3, extracted with a first level GiT-BEMD analysis, exhibit strictly localized activity distributions, a first level SPM analysis (SPM 1) of the raw data exhibits activity blobs which most closely resemble activities seen in VIMF1 only. This discrepancy indicates the need for a second level analysis which allows to estimate the significance of these activity distributions. It can be done between subjects by using the results of a first level analysis as input to a Student's t-test. The latter identifies those spatial locations where the activity distribution is significantly different between the two stimulus conditions: CT and NCT. As with a second level SPM analysis (SPM 2) of the raw data, additional activity blobs emerge, in this study such an analysis has been performed also with the newly proposed GiT-BEMD algorithm. Fig. 8 illustrates voxel-wise t-values resulting from activity distributions which are significantly different between the two stimulus conditions: CT and NCT. For this analysis, an uncorrected significance level p <0.001 was used together with a one-sample t-test. Results thus indicate significant activities specific to each stimulus condition separately, i.e. the localized activity is significant for the stimulus condition CT but not NCT and vice versa. As can be seen in Fig. 8, VIMF1 exhibits activity mainly on the left hemisphere at the occipital and frontal gyrus in case of a CT-stimulus, and on the right hemisphere at the superior frontal gyrus and unknown area map for an NCT-stimulus condition. VIMF2 shows, for a CT stimulus, activities in various brain areas including the right precuneus, postcentral gyrus and the left temporal, medial and orbital gyri in addition to the parietal lobule. The VIMF3 reveals activity which is distributed mainly in frontal, temporal, occipital and postcentral areas. Corresponding MNI coordinates of these localized activity blobs, identified with a second level analysis using GiT-BEMD, are collected in Table 2 . In summary, GiT-BEMD clearly comes up with superior detail compared to an SPM 2 analysis. Moreover, the extracted modes exhibit localized activations for a NCT-stimulus condition which an SPM 2 analysis completely fails to show. 4.2 Classification results A support vector machine (SVM) is a classifier, which can deal with linear or nonlinear data. For a binary classification problem, a soft margin SVM classifier, more specifically the C-SVM [8] algorithm as described in the MATLAB statistics toolbox [20], has been employed in this study. The non-linear SVM classifier has been used by selecting a hyperbolic tangent as a sigmoidal kernel (SVM-SMO). As baseline for measuring classification performance, the Voxels-As-Features (VAF) approximation is taken. It uses all voxels of each raw data volume as a feature vector. Corresponding results are summarized in Table 3 . These results are used as input to an SVM classifier. The original feature vector encompasses 153,594 features. A leave-one-out cross-validation (LOOCV) strategy has been applied, and several statistics were computed to quantity the performance of the classifiers, namely accuracy (acc), sensitivity (sens) and specificity (spec). 4.2.1 Reducing data dimension The main purpose of PCA in this study is to reduce the size of the high-dimensional data sets, thus providing a properly reduced set of features for classification while avoiding overfitting. Indeed, proper features are obtained as projections of data onto the leading principal components (PCs). These projections (PCs) reveal the most informative directions underlying functional image textures corresponding to the two stimulus conditions of oriented contours being present (CT) and absent (NCT). These PCs are sorted in descending order according to their amount of variance. Thus, the first PC explains the most variance of the matrix; the second PC captures the second most important dimension in terms of a mount the variance (orthogonal to the first one) and so on (see Fig. 10). Such ordering is not useful for response discrimination in many cases, especially when the inter-class variance is higher than the intra-class variance. Thus, a feature selection and t-test is applied to re-order the projections in terms of their discriminating power (see Fig. 10). Fig. 9 presents the proposed classification framework based on the intrinsic modes extracted with GiT-BEMD. The training data related to each intrinsic mode is represented by a matrix whose rows are formed by the value of a given voxel across all subjects and conditions, resulting in N =38− q column vectors c n , n =1, …, N. The curse of dimensionality is handled by employing a PCA decomposition of the intrinsic modes, resulting in N eigenvectors e n and concomitant eigenvalues λ n (see Fig. 10 ). As the number of voxels is much larger than the number of examples N, the dual PCA approach based on the dot product matrix (kernel matrix) should be used to estimate the N pairs (e n , λ n ). Next, each column vector c n is projected onto the first l eigenvectors e l T c n , l = 1 , … , L ≪ N . An optimal subspace dimension L can be obtained from applying a t-test to select the most relevant projections (features) into PCA feature subspace. These selected features of all images formed the training data set except q =1 image at a time. The latter was used for testing during a leave-one-out cross-validation (LOOCV), i.e. q =1. The scored projections are used as input to the classifier. Hence, the LOOCV is repeated 38 times. Fig. 10 shows the importance of features in terms of explained variance and feature discrimination. An SVM classifier has been used to classify the projections of the extracted intrinsic modes onto the principal directions. The soft margin parameter C and the tangent hyperbolic kernel (γ, r) parameters of the SVM classifier have been tuned for improved performance. Therefore the optimization of the classifier performance could be achieved by employing a grid search in the ranges C ∈{2−5, 2−4, 2−3, …, 23, 24}; γ ∈{2−7, 2−6, 2−5, …, 21, 22}; and r ∈{−1, −0.9, −0.8, …, -0.01, -0.001} to find the best parameters of the sigmoidal (hyperbolic tangent) kernel. Table 5 shows the performance which has been achieved by using the optimal parameters as collected in Table 4 . The number of optimal features is shown in parenthesis. These results clearly indicate a considerable improvement of the classification performance (as maximum 92%) that could be achieved by employing the new GiT-BEMD compared with the classification accuracy (roughly 82%) which was produced by a canonical BEEMD in previous study [1]. 5 Conclusion In summary, this study proposed a new and fully data-driven method to adaptively decompose fMRI images into different textures on characteristic spatial scales based on their inherent characteristics. The new method offers some advantages compared to a canonical BEMD analysis: (a) GiT-BEMD replaces the standard cubic-spline-based envelope estimation step by a Green's function-based envelope estimation which adapts its functional shape to the given amplitude distribution of the data set considered. (b) GiT-BEMD only employs two noise-assisted versions of the original data set while a canonical BEMD needs large ensembles of noisy data sets to achieve a similar quality of the component images. These large ensembles cause an exponentially growing computational load. (c) Even with a large ensemble of noise-assisted data streak-like artifacts, caused by the way data is processed with multi-dimensional EMD, cannot be avoided completely while GiT-BEMD achieves results free of such artifacts. (d) The analysis clearly demonstrated that GiT-BEMD revealed richer textures (VIMFs) which contributed to improve the discriminating power of intrinsic modes compared to a canonical BEMD analysis as in the previous study. Hence, due to its data-driven merit, the proposed GiT-BEMD analysis offers a better performing alternative for functional imaging data sets and yields component images of superior quality. Acknowledgement Financial support by the DAAD, Acciones Integradas Hispano-Alemanas is gratefully acknowledged. References [1] S. Al-Baddai K. Al-Subari A. Tomé G. Volberg S. Hanslmayr R. Hammwöhner E. Lang Bidimensional ensemble empirical mode decomposition of functional biomedical images taken during a contour integration task Biomed. Signal Process. Control 13 2014 218 236 [2] K. Al-Subari S. Al-Baddai A. Tomé M. Goldhacker R. Faltermeier E.W. Lang EMDLAB: a toolbox for analysis of single-trial EEG dynamics using empirical mode decomposition J. Neurosci. Methods 253 2015 193 205 [3] K. Al-Subari S. Al-Baddai A. Tomé G. Volberg R. Hammwöhner E. Lang Ensemble empirical mode decomposition analysis of EEG data collected during a contour integration task PLOS ONE 10 4 2015 e0119489 [4] S. Bhuiyan R. Adhami J. Khan A novel approach of fast and adaptive bidimensional empirical mode decomposition IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2008 1313 1316 [5] S. Bhuiyan J. Khan N.A.-O.R.R. Adhami Study of bidimensional empirical mode decomposition method for various radial basis function surface interpolators 2009 IEEE International Conference on Machine Learning and Applications 2009 18 24 [6] V.D. Calhoun T. Adali L.K. Hansen J. Larsen J.J. Pekar ICA of functional MRI data: an overview Proceedings of the International Workshop on Independent Component Analysis and Blind Signal Separation 2003 281 288 [7] M. Castellano M. Plöchl R. Vicente G. Pipa Neuronal oscillations during contour integration of dynamic visual stimuli form parietal/frontal networks Front. Integr. Neurosci. 8 2014 [8] C.-C. Chang C.-J. Lin A Library for Support Vector Machines 2001 [9] A. Cichocki S. Amari K. Siwek T. Tanaka A.H. Phan ICALAB Toolbox 2007 [10] A. Cichocki R. Zdunek A.H. Pham S. Amari Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation 2009 Wiley & Sons [11] C. Damerval S. Meignen V. Perrier A fast algorithm for bidimensional EMD IEEE Signal Process. Lett. 12 10 2005 701 704 [12] J. Devore R. Peck Statistics: The Exploration and Analysis of Data 1997 Duxbury Press Pacific Grove, USA [13] N.E. Huang Z. Shen S.R. Long M.L. Wu H.H. Shih Q. Zheng N.C. Yen C.C. Tung H.H. Liu The empirical mode decomposition and Hilbert spectrum for nonlinear and non-stationary time series analysis Proc. R. Soc. Lond. A 454 1998 903 995 [14] A. Humeau-Heurtier G. Mahe P. Abraham Multi-dimensional complete ensemble empirical mode decomposition with adaptive noise applied to laser speckle contrast images IEEE Trans. Med. Imaging 34 10 2015 [15] A. Hyvärinen J. Karhunen E. Oja Independent Component Analysis 2001 John Wiley New York [16] Z. Kourtzi E. Huberle Spatiotemporal characteristics of form analysis in the human visual cortex revealed by rapid event-related fMRI adaptation Neuroimage 28 2 2005 440 452 [17] A. Linderhed 2-D empirical mode decompositions in the spirit of image compression Wavelet and Independent Component Analysis Applications IX, Proceedings of SPIE, vol. 4738 2002 1 8 [18] Z. Liu S. Peng Boundary processing of bidimensional EMD using texture synthesis IEEE Signal Process. Lett. 12 2005 33 36 [19] Z. Liu H. Wang S. Peng Texture classification through directional empirical mode decomposition Proceedings of the 17th IEEE International Conference on Pattern Recognition (ICPR '04) 2004 803 806 [20] MATLAB S. Toolbox, Release 2013 2013 [21] K. Min-Sung E. Rodriguez-Marek T. Fischer A new two dimensional empirical mode decomposition for images using inpainting 2010 IEEE 10th International Conference on Signal Processing (ICSP) 2010 13 16 [22] J. Nunes Y. Bouaoune E. Delechelle O. Niang P. Bunel Image analysis by bidimensional empirical mode decomposition Image Vis. Comput. 21 12 2003 1019 1026 [23] J. Nunes E. Deléchelle Empirical mode decomposition: applications on signal and image processing Adv. Adapt. Data Anal. 1 2009 125 175 [24] J. Nunes S. Guyot E. Deléchelle Texture analysis based on local analysis of the bidimensional empirical mode decomposition Mach. Vis. Appl. 16 2005 177 188 [25] G. Rilling P. F1andrin P. Goncalves J.M. Lilly Bivariate empirical mode decomposition IEEE Signal Process. Lett. 14 2007 936 939 [26] C. Roden MRIcro 2012 [27] B. Schölkopf A.J. Smola Learning with Kernels 2002 The MIT Press [28] P. Wessel A general-purpose Green's function-based interpolator Comput. Geosci. 35 6 2009 1247 1254 [29] P. Wessel D. Bercovici Interpolation with splines in tension: a Green's function approach Math. Geol. 30 1 1998 77 93 [30] Z. Wu N.E. Huang X. Chen The multidimensional ensemble empirical mode decomposition method Adv. Adapt. Data Anal. 1 2009 339 372 [31] C.-Z. Xiong J.Y. Xu J.-C. Zou D.-X. Qi Texture classification based on EMD and FFT J. Zhejiang Univ. Sci. A 7 2006 1516 1521 10.1631/jzus.2006.A1516 "
    },
    {
        "doc_title": "ERP correlates of error processing during performance on the Halstead Category Test",
        "doc_scopus_id": "84976524087",
        "doc_doi": "10.1016/j.ijpsycho.2016.06.010",
        "doc_eid": "2-s2.0-84976524087",
        "doc_date": "2016-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Neuroscience (all)",
                "area_abbreviation": "NEUR",
                "area_code": "2800"
            },
            {
                "area_name": "Neuropsychology and Physiological Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3206"
            },
            {
                "area_name": "Physiology (medical)",
                "area_abbreviation": "MEDI",
                "area_code": "2737"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Cerebral Cortex",
            "Electroencephalography",
            "Evoked Potentials",
            "Executive Function",
            "Female",
            "Humans",
            "Male",
            "Middle Aged",
            "Neurofeedback",
            "Young Adult"
        ],
        "doc_abstract": "© 2016The Halstead Category Test (HCT) is a neuropsychological test that measures a person's ability to formulate and apply abstract principles. Performance must be adjusted based on feedback after each trial and errors are common until the underlying rules are discovered. Event-related potential (ERP) studies associated with the HCT are lacking. This paper demonstrates the use of a methodology inspired on Singular Spectrum Analysis (SSA) applied to EEG signals, to remove high amplitude ocular and movement artifacts during performance on the test. This filtering technique introduces no phase or latency distortions, with minimum loss of relevant EEG information. Importantly, the test was applied in its original clinical format, without introducing adaptations to ERP recordings. After signal treatment, the feedback-related negativity (FRN) wave, which is related to error-processing, was identified. This component peaked around 250ms, after feedback, in fronto-central electrodes. As expected, errors elicited more negative amplitudes than correct responses. Results are discussed in terms of the increased clinical potential that coupling ERP information with behavioral performance data can bring to the specificity of the HCT in diagnosing different types of impairment in frontal brain function.",
        "available": true,
        "clean_text": "serial JL 271907 291210 291726 291738 291782 291833 31 International Journal of Psychophysiology INTERNATIONALJOURNALPSYCHOPHYSIOLOGY 2016-06-19 2016-06-19 2016-06-30 2016-06-30 2016-07-20T10:02:51 S0167-8760(16)30113-1 S0167876016301131 10.1016/j.ijpsycho.2016.06.010 S300 S300.1 FULL-TEXT 2016-07-20T09:23:14.031267-04:00 0 0 20160801 20160831 2016 2016-06-19T09:08:20.734821Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid orcid primabst ref 0167-8760 01678760 true 106 106 C Volume 106 14 97 105 97 105 201608 August 2016 2016-08-01 2016-08-31 2016 Original Articles article fla © 2016 Elsevier B.V. All rights reserved. ERPCORRELATESERRORPROCESSINGDURINGPERFORMANCEHALSTEADCATEGORYTEST SANTOS I 1 Introduction 2 Methods 2.1 Participants 2.2 Materials and procedure 2.3 EEG recordings 2.4 EEG/ERP data analysis 2.4.1 Filtering using Singular Spectrum Analysis 2.4.2 Filter coefficients and filter design 3 Results 3.1 P100 3.2 FRN analysis 4 Discussion and conclusions Acknowledgments References ADAMS 1995 275 280 K ALEXANDER 2011 1338 1344 W ALEXANDROV 2009 1 22 T ALLEN 1999 237 244 D ALLEN 2007 638 652 D ANDERSON 1991 909 922 S BARCELO 2003 27 37 F BARCELO 1998 747 751 F BARCELO 1997 399 408 F BELLEBAUM 2008 1823 1835 C BERNAT 2011 352 364 E CHOCA 1997 61 75 J DEFILIPPIS 2002 N CATEGORYTESTCOMPUTERVERSION DEFILIPPIS 1997 N BOOKLETCATEGORYTEST FALKENSTEIN 2000 87 107 M FERDINAND 2012 12087 12092 N GEHRING 2002 2279 2282 W GOLYANDINA 2001 N ANALYSISTIMESERIESSTRUCTURESSARELATEDTECHNIQUES HAJCAK 2006 148 154 G HAUSER 2014 159 168 T HEATON 1993 R WISCONSINCARDSORTINGTESTMANUALREVISEDEXPANDED HOFFMANN 2012 208 212 S HOLROYD 2004 211 218 C ERRORSCONFLICTSBRAINCURRENTOPINIONSPERFORMANCEMONITORING ANOTEODDBALLN200FEEDBACKERNLEARNINGCOGNITIVECONTROLLABORATORY HOLROYD 2002 679 709 C KRAUS 2014 e100486 D LUCK 2005 S INTRODUCTIONEVENTRELATEDPOTENTIALTECHNIQUE MCNALLY 2015 1 9 S MILTNER 1997 788 798 W MINASSIAN 2003 213 221 A PRATT 2011 89 114 H OXFORDHANDBOOKEVENTRELATEDPOTENTIALCOMPONENTS SENSORYERPCOMPONENTS ROUSSELET 2012 G TEIXEIRA 2006 125 138 A TOME 2010 345 355 A VILABALLO 2015 98 109 A WALSH 2012 1870 1884 M WIDMANN 2012 A WIERSEMA 2005 1417 1430 J SANTOSX2016X97 SANTOSX2016X97X105 SANTOSX2016X97XI SANTOSX2016X97X105XI 2017-06-30T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0167-8760(16)30113-1 S0167876016301131 10.1016/j.ijpsycho.2016.06.010 271907 2016-07-20T09:23:14.031267-04:00 2016-08-01 2016-08-31 true 1164435 MAIN 9 58219 849 656 IMAGE-WEB-PDF 1 gr1 10620 67 219 gr2 8694 68 219 gr3 8946 76 219 gr4 29938 158 219 gr5 12110 104 219 gr6 9814 110 219 gr7 19457 115 219 gr1 23305 159 522 gr2 16369 160 514 gr3 27917 214 619 gr4 141950 448 620 gr5 48231 338 713 gr6 28919 268 534 gr7 52012 264 501 si1 1567 13 182 si2 982 13 28 si3 3181 75 226 si4 1127 29 61 si5 1691 13 115 si6 1265 13 95 INTPSY 11122 S0167-8760(16)30113-1 10.1016/j.ijpsycho.2016.06.010 Fig. 1 Stimuli examples used in HCT test. More than one abstract principle can be formulated for each item, in order to correspond a number ranging from 1 to 4 to each stimulus. Participants must find out which is the correct abstract principle used in each subtest. Correct responses to the stimuli examples, from left to right: Top row 4, 2, 3; Bottom row 1, 2, 1. Fig. 1 Fig. 2 Schematic representation of a test trial in the HCT. Fig. 2 Fig. 3 Illustration of filter design for an ERPr segment. Left - Frequency response of the system (0< f <50Hz); right - Impulse response c[n]. Fig. 3 Fig. 4 Average signal of one participant, when epochs were time-locked to the onset of the visual stimuli, to reflect the filtering process in time: (a) Original signal; (b) Corrected signal. Fig. 4 Fig. 5 (a) Grand-average waveform in channel O1 time-locked to the onset of the visual stimuli. (b) and (c) Head topography of the grand-average waveforms considering a visualization window between [100120]ms: (b) Original signal and (c) Corrected signal. Fig. 5 Fig. 6 Grand-average waveforms of individual ERPs for FCz and Cz channels considering two subsets of trials: Wrong responses (dash line) and Right responses (solid line) Fig. 6 Fig. 7 Head topography of the grand-average waveforms considering a visualization window centered at 250ms: (a) Wrong trials and (b) Right trials Fig. 7 ERP correlates of error processing during performance on the Halstead Category Test I.M. Santos a ⁎ A.R. Teixeira b c Tome A.M. Tomé b e A.T. Pereira a d P. Rodrigues f g P. Vagos h J. Costa f M.L. Carrito a d B. Oliveira a d N.A. DeFilippis i C.F. Silva a a Center for Health Technology and Services Research (CINTESIS), Department of Education and Psychology, University of Aveiro, Portugal Center for Health Technology and Services Research (CINTESIS) Department of Education and Psychology University of Aveiro Portugal b IEETA, University of Aveiro, Portugal IEETA University of Aveiro Portugal c ESEC, Polytechnic Institute of Coimbra, Portugal ESEC , Polytechnic Institute of Coimbra Portugal d IBILI, Faculty of Medicine, University of Coimbra, Portugal IBILI Faculty of Medicine University of Coimbra Portugal e DETI, University of Aveiro, Portugal DETI University of Aveiro Portugal f University of Beira Interior, Portugal University of Beira Interior Portugal g CICS, Research Center in Health Sciences, Portugal CICS Research Center in Health Sciences Portugal h CINEICC, Cognitive and Behavioural Center for Research and Intervention, Faculty of Psychology and Educational Sciences, University of Coimbra, Portugal CINEICC Cognitive and Behavioural Center for Research and Intervention Faculty of Psychology and Educational Sciences University of Coimbra Portugal i Georgia School of Professional Psychology at Argosy University, Atlanta, USA Georgia School of Professional Psychology at Argosy University Atlanta USA ⁎ Corresponding author at: University of Aveiro, Department of Education and Psychology, Campus Universitario de Santiago, 3810–193 Aveiro, Portugal. University of Aveiro Department of Education and Psychology Campus Universitario de Santiago Aveiro 3810–193 Portugal The Halstead Category Test (HCT) is a neuropsychological test that measures a person's ability to formulate and apply abstract principles. Performance must be adjusted based on feedback after each trial and errors are common until the underlying rules are discovered. Event-related potential (ERP) studies associated with the HCT are lacking. This paper demonstrates the use of a methodology inspired on Singular Spectrum Analysis (SSA) applied to EEG signals, to remove high amplitude ocular and movement artifacts during performance on the test. This filtering technique introduces no phase or latency distortions, with minimum loss of relevant EEG information. Importantly, the test was applied in its original clinical format, without introducing adaptations to ERP recordings. After signal treatment, the feedback-related negativity (FRN) wave, which is related to error-processing, was identified. This component peaked around 250ms, after feedback, in fronto-central electrodes. As expected, errors elicited more negative amplitudes than correct responses. Results are discussed in terms of the increased clinical potential that coupling ERP information with behavioral performance data can bring to the specificity of the HCT in diagnosing different types of impairment in frontal brain function. Keywords feedback-related negativity (FRN) Halstead Category Test Singular Spectrum Analysis (SSA) event-related potentials (ERP) feedback processing 1 Introduction The Halstead Category Test (HCT) (DeFilippis and McCampbell, 1997; DeFilippis, 2002), which is part of the Halstead-Reitan Neuropsychological Battery, is a neuropsychological test routinely used to assess abstract reasoning, concept formation and problem solving abilities in a variety of clinical contexts and populations (Allen et al., 2007, 1999; Choca et al., 1997). The test consists of 208 items, divided into 7 subtests, and the total number of errors is the score most commonly used to assess performance, which has proven to be highly sensitive for identifying brain dysfunction (Choca et al., 1997). However, as studies seem to demonstrate that the HCT is a multidimensional instrument which assesses diverse cognitive executive abilities (Allen et al., 1999), the total error score has also received some criticism, due to its lack of specificity in identifying which particular abilities are impaired (Allen et al., 2007). For this reason, a number of studies have attempted to develop more elaborate approaches to scoring the HCT and created scales based on individual subtests (e.g., McNally et al., 2015; Minassian et al., 2003). Results suggest that by looking at the scores on the various subscales, the HCT is able to provide information about a variety of cognitive functions that are usually assessed by different instruments, which is useful in informing clinicians about different domains where further evaluation should be focused in case of impairment. Nonetheless, studies demonstrating that the scales are able to discriminate neurologically impaired patients and healthy individuals are still lacking (McNally et al., 2015). The informative potential of this test regarding brain dysfunction could be considerably increased if brain activity was registered concomitantly to test performance. To the best of our knowledge, no studies so far have examined brain activity through event-related potentials (ERPs) during performance on the HCT. ERPs are a high temporal resolution electrophysiological technique, which is particularly suited for studying the time course of brain processes. Therefore, ERP recordings during performance on the HCT could be helpful in increasing the diagnostic value of the test, providing a more direct link between test scores and underlying brain processes, and helping to disentangle the multitude of cognitive operations that are in play during HCT performance. This information could be used to better understand the brain processes and cognitive abilities that are impaired in association with a particular behavioral pattern, thus increasing the specificity of the test, an aspect of the HCT which has been pointed out as a limitation (Choca et al., 1997). Although the electroencephalogram (EEG) can be easily recorded while participants perform on the HCT, the conditions are not ideal for the study of ERPs. Several characteristics of the test items differ from what is a classical ERP paradigm, namely the length of stimulus presentation, which in the HCT has no time limit and is typically long. This originates a high number of artifacts, both ocular and due to movement, since it is more difficult for the participant to exert self-control of these for the time of exposure. Another issue is the number of test items, which in total is 208, but which can be significantly reduced if specific analysis need to be carried out, namely analyzing only certain subtests, or separating correct from incorrect responses. Thus, it is necessary to employ a signal processing method that will allow for preservation of a high signal-to-noise ratio. The main aim of the present work was to demonstrate that it is possible to use the HCT in its current clinical format and simultaneously record an EEG for ERP analysis, in order to increase the informative value of the test in terms of identifying the underlying brain processes. Another test commonly used in neuropsychological practice to assess executive function is the Wisconsin Card Sorting Test (WCST) (Heaton et al., 1993). Similarly to the HCT, the WCST is commonly regarded as a test of abstract reasoning and concept formation, and it is also generally adopted as an indicator of frontal lobe dysfunction. However, a lack of specificity to frontal damage has also been reported (e.g., Anderson et al., 1991), which has led to attempts to use ERPs as a brain activity measure to probe in detail the relation between brain dynamics and the cognitive processes underlying WCST performance (Barceló et al., 1997; Barceló, 2003). These studies have been successful in identifying frontal ERP components related to task performance, but also components that have a non-frontal origin (such as the P3b wave), which help clarify some reports of non-specificity of the WCST to prefrontal damage (Barceló and Rubia, 1998). However, these ERP studies have used a modified version of the WCST in order to adapt it to the recording of ERPs and to specifically allow the investigation of the electrophysiological dynamics related to attention set-shifting (Barceló, 2003). Our aim was to be able to explore the ERP correlates of performance on the HCT in its current clinical format, without introducing any adaptation to the task. In this way, we would be able to provide an additional tool that could be used in conjunction with behavioral performance. This would allow the use of standard norms and cut-off points for neuropsychological diagnosis, whilst simultaneously informing on the underlying brain dynamics, thus increasing the clinical potential of the test. In the present work, we focused our analysis on the posterior P100 visual ERP component, which is elicited by visual stimuli independently of the task that the subject is doing (Luck, 2005) and is generally related to activity in extrastriate areas of the visual cortex (Pratt, 2011) and on the feedback-related negativity (FRN) wave, a frontocentral ERP component related to error processing (Miltner et al., 1997), which should be expected during performance on the HCT. This test consists of 208 items, divided into 7 subtests. The items consist of nonverbal stimuli representing geometric figures or designs, and the participant is asked to indicate the number between 1 and 4 that each stimulus suggests. After each response, visual feedback on whether the response was correct or incorrect is provided, which helps participants to adjust their strategy. The participant is informed that all items in a particular subtest have the same underlying abstract principle, and that this abstract principle may or may not change between subtests. Thus, an incorrect feedback after a response indicates the need to search for a different abstract principle. On the other hand, a correct feedback indicates that the same abstract principle should be maintained for the following items within that subtest. Every time a new subtest begins, the participant is informed that the underlying principle may be the same as in the last subtest or that it may change (McNally et al., 2015). Thus, this test measures concept learning, flexibility of thinking and ability to learn and apply new rules. Also, the test directly taps the ability to learn from experience, monitor the errors that are committed, and adjust one's response strategy as a function of feedback on the accuracy of a previous response, until a correct rule has been successfully identified which can be followed in the subsequent trials. Given the nature of the task, committing errors is common until the new rule is discovered. An ERP component has been described, which follows the display of negative feedback, in tasks where errors are due to uncertainty regarding the correct response, and participants only become aware of the accuracy of their response after a feedback signal has been provided (Walsh and Anderson, 2012). This wave has been called feedback-related negativity (FRN). This component was first discovered in a time estimation task where participants had to push a button a second after a signal. A feedback stimulus told the participants whether the estimation was accurate or wrong, and a negative deflection appeared after negative feedback (Miltner et al., 1997). The FRN is measured maximally at midline fronto-central electrodes and is typically larger for erroneous responses than for correct responses, peaking between 200 and 250ms after feedback. This component is believed to originate in a general purpose neural system for dealing with errors in different types of task, which contributes to the adjustment of ongoing behavior (Gehring and Willoughby, 2002; Miltner et al., 1997). Research has suggested that the FRN reflects the evaluation function of a neural system that determines whether an outcome was correct or incorrect relative to one's expectation (Gehring and Willoughby, 2002; Hajcak et al., 2006; Holroyd, 2004). Importantly, the FRN seems to reflect the processing of external cues about performance (Bernat et al., 2011). The most likely neural generator of the FRN has been localized in the dorsal area of the anterior circulate cortex (ACC), a brain region known to be involved in cognitive control and behavior regulation, and which is important for the ability to adapt behavior to different task demands and circumstances (Hauser et al., 2014; Walsh and Anderson, 2012). As mentioned above, the Wisconsin Card Sorting Test (WCST) is another neuropsychological test commonly used to assess frontal lobe executive functions. Like the HCT, it also requires participants to infer sorting rules associated with simple geometrical stimuli (based on their color, shape, or number). Participants must adapt their responses based on the provided feedback regarding the accuracy of their responses (correct or incorrect feedback). A new discovered rule will be maintained for a number of trials, but after a while it will change again, requiring participants to discover the new sorting rule (Vilà-Balló et al., 2015). Studies using a modified version of the WCST, adapted to the recording of ERPs, have demonstrated the occurrence of the FRN component during performance on the test. In particular, (Kraus and Horowitz-Kraus, 2014) have shown that individuals with dyslexia exhibit decreased FRN amplitudes in the early phases of the task compared to normal readers, consistent with their difficulty in learning from previous mistakes. Another study, also using a modified ERP version of the WCST, has found larger FRN amplitude to positive feedback in a violent juvenile offender group compared to a control group, which suggested difficulty in using previous external feedback to accurately predict the negative outcomes of their behavior (Vilà-Balló et al., 2015). Thus, the FRN seems to be a reliable neurophysiological correlate of feedback-processing during performance on the WCST, a test that requires various executive abilities which are common to the HCT, as outlined above. Also consistent with these findings is evidence that performance on the WCST is positively correlated with metabolic activity in the cingulate region (Adams et al., 1995). So, since performance on the HCT also depends on the ability to monitor errors and adapt behavioral responses based on the success of previous performance, it is expected that the feedback stimuli in the HCT should elicit an FRN. The main objective of this work was to demonstrate the possibility of meaningfully recording, analyzing and interpreting event-related potentials associated with performance on the HCT. To the best of our knowledge, this has never been done before. The main question addressed in this paper is whether it is possible to identify specific ERP waveforms (in this case, the FRN component after feedback, which should differentiate between right and wrong answers) without introducing changes in the structure of the test, and maintaining its clinical format and validity. As mentioned before, this is important because it allows the possibility of relating brain activity with clinical performance according to established norms in a clinical context. However, the identification of specific ERP components without modifying the structure of the HCT was a challenge, since the low number of trials and long stimulus duration are not typical from an ERP paradigm and originate a low signal-to-noise ratio and a large number of artifacts. The fact that participants had unlimited time to examine each stimulus, before providing a response, originated a large number of high amplitude artifacts, both due to blinks and other ocular movements, and also body movements. This compromises the analyzes of ERP components through more traditional methods, given that the elimination of artifact contaminated segments would lead to a very low signal-to-noise ratio, due to the relatively small number of trials available in this test, in its original format. Importantly, we demonstrate that the application of a specific filtering and signal processing technique successfully cleans the data, allowing the identification of the relevant ERP components. This paper details the data processing methodology used to filter the signal, which guaranteed an artifact free signal with minimal loss of relevant information and no phase or latency distortions, which are common after the application of various types of filters (Rousselet, 2012). However it should be noticed that filtering manipulations follow the distributive property (Luck, 2005). Thus, averaging filtered signals is equivalent to averaging the raw signals and filtering the resulting average. Therefore, in this study, average ERP waves for each subject were computed first and then the filtering operation was applied. We demonstrate that, by extracting a high amplitude component of the average signal, it is possible to visualize characteristic transient events after the stimulus or the feedback. First, we show a clear P100 in response to the visual stimuli, which demonstrates the efficiency of the method (importantly showing that the filtering technique does not introduce distortions in the signal). Secondly, we also demonstrate that after extracting a high amplitude component of the signal, it is possible to identify the FRN component related to error processing, which should be elicited by feedback stimuli during the HCT. The extraction of the high-amplitude artifact component follows a methodology known as Singular Spectrum Analysis (SSA) that allows, with a filtering mechanism, the decomposition of a signal time-series into components that are in phase with the original signals (Tomé et al., 2010). Moreover, the SSA strategy was modified in order to compute the coefficients of the filter with a template of the signal to be extracted. This template is the average of the electro-oculogram signal. The advantage over the standard SSA methodology is that only one filter is computed and applied to all channels. After filtering the signal following this methodology, results show, as expected, larger negative amplitudes for negative feedback (incorrect responses) compared to positive feedback (correct responses) in the FRN component during performance on the HCT. 2 Methods 2.1 Participants Fifty-eight graduate and undergraduate students were recruited from the University of Aveiro (39 females and 19 males). The participants mean age was 22.5years (SD=4.9; range: 19–48). Consent forms were signed prior to the experimental task and participants were paid for their participation. This study was conducted in accordance with the Declaration of Helsinki. 2.2 Materials and procedure A computerized version of the Halstead Category Test (HCT) was used to assess cognitive executive frontal lobe function. This is a nonverbal test that measures a person's ability to formulate abstract principles. It consists of 208 items divided into 7 subtests: the first two are the training subtests (8 items in subtest I and 20 items in subtest II), the third and fourth measure spatial/positional reasoning, the fifth and sixth assess proportional reasoning (40 items in each of the subtests III to VI) and the last one is a memory subtest (20 items in subtest VII). Each test item shows one or more figures that, altogether, suggest a number ranging from one to four. In subtests I to VI, participants are instructed to determine or guess the correct number based on their conceptualization of the abstract principle represented by the stimulus, as shown in Fig. 1 . Visual feedback is provided after each response, to indicate if the participant's response was right or wrong. Based on this feedback, the participant must maintain or change their response strategy accordingly, keeping the same abstract principle if the response was correct, or trying to guess a new one, in case of wrong feedback. The participant is informed that the same abstract principle is kept throughout each subtest and may change or not between subtests. Subtest VII consists in a memory task, where there is no unifying abstract principle underlying the correct responses to all the items. Instead, participants are informed that they will see items that they have seen before and that they must recall which was the correct answer the first time they saw each particular item and give that same answer again. All 20 items in subtest VII were presented before, distributed by the other 6 subtests. The scoring of the test consists of the total number of errors made. Participants were seated in a sound-attenuated cabin during performance on the HCT. Specific instructions were given at the beginning of each subtest on the computer screen. The response consisted of pressing one of four keys numbered 1 to 4 on the computer keyboard, on each test trial. All stimuli remained on the computer screen until the participant responded. Visual feedback was provided 1500 ms after the response, indicating if the response was right (written in green) or wrong (written in red). The feedback remained on the screen for 750 ms, and there was an interval of 2000 ms before the next stimulus was displayed on the screen (see Fig. 2 for a schematic representation of a test trial). Participants were instructed to rest at the end of each subtest. Stimuli presentation, response registration and synchronization with the EEG recording system were controlled by E-Prime software (Psychology Software Tools, Pittsburgh, PA). 2.3 EEG recordings EEG signals were collected with a Neuroscan SynAmps2 amplifier through an Easy-Cap with 26 channels and recorded with the software Scan 4.3 (Neuroscan Systems). EEG was continuously recorded with Ag-AgCl sintered electrodes which were located according to the 10–10 system (FP1, FPz, FP2,F7, F3, Fz, F4, F8, FC3, FCz, FC4, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, P8, PO7, PO8, O1, Oz and O2). The reference electrode was placed at the tip of the nose. Vertical EOG (VEOG) was recorded by two electrodes placed above and below the left eye and horizontal EOG (HEOG) was recorded from the outer canthi of both eyes. The impedance was kept below 5KΩ. A notch filter for 50Hz was used during recordings. 2.4 EEG/ERP data analysis In this work, we considered exclusively the EEG data associated with subtests III, IV, V and VI of the HCT, which tap directly spatial/positional and proportional reasoning. Subtests I and II were not included because they are training subtests, where participants are still learning the task. Subtest VII was also excluded because it consists of a memory subtest which relies more heavily on conceptual shifting and short term memory. Considering those subtests, the average number of right and wrong answers for each individual was 117±24 and 34±24, respectively. We will here describe the signal processing steps that allowed us to obtain an artifact free signal. This signal was then firstly analyzed to identify the posterior P100 component in response to all the visual stimuli. Secondly, we analyzed the fronto-central FRN component as an index of feedback processing that differed between right and wrong responses. 2.4.1 Filtering using Singular Spectrum Analysis Singular Spectrum Analysis (SSA) (Golyandina et al., 2001; Alexandrov, 2009) provides means to decompose a time series (signal) into components that are in-phase with the original signal (Tomé et al., 2010). It was shown that signal enhancement can be achieved by a bank of finite impulse response filters arranged as parallel pairs of analysis-synthesis filters (Tomé et al., 2010). It is particularly suitable to extract high-amplitude components by designing the analysis-synthesis pair related to the highest energy component y[n]. Then, the input signal x[n] can be considered the sum of two components (1) x n = y n + x ˜ n n = 0 , 1 … N − 1 where the x ~ n represents the component of interest (Teixeira et al., 2006) which can be computed by subtracting the high- amplitude artifact y[n] to x[n]. Assuming that the non-causal analysis-synthesis pair have an impulse response c[n], with 2M−1 amplitude values, the output signal can be expressed by the following convolution summation (2) y n = ∑ k = − M − 1 M − 1 c k x n − k = ∑ k = 0 M − 1 c k x n − k + ∑ k = 1 M − 1 c − k x n + k where c[k]= c[− k] assures that the output signal is in-phase with the input. The first summation is called the causal contribution (depends on the past of the sample n-th sample) and the second summation is the anti-causal contribution (depends on the future of n-th sample) (Widmann and Schröger, 2012). The global strategy is called non-causal filtering scheme (Rousselet, 2012) and this strategy leads to a frequency response with zero-phase (Tomé et al., 2010). The output signal y[n] can have the same time duration as the input. Then, assuming that input time indexes are n =0,1,… N −1 the following issues need to be considered • for the samples at n =0,…, M −2 the causal contribution depends on samples x[n] that are unknown, for instance {x[−1], x[−2],…}. Usually assumed as having value zero. • for the samples at n = N − M +1,…, N −1 the anti-causal contribution depends on samples of x[n] that are unknown, for instance {x[N], x[N +1],…}. As before usually assumed as zero. • for the samples at n = M −1,… N − M are computed by applying Eq. (2) without any constraint. Those samples are usually called the stationary response of the filter while previous are called transient responses (Tomé et al., 2010). The stationary response should then correspond to segments of signals to be further processed or analyzed. To get a further insight of the filtering operation the difference Eq. (2) can be transformed into a polynomial expression by substituting time-delayed sequence x[n ± k] by its equivalent z-transform z ± k X(z), assuming that the transform z-transform of the input sequence x[n] is X(z). Applying a similar operation to the output sequence the transfer function C z = Y z X z is easily obtained. The frequency response of the system is then obtained by substituting z =exp(j2πf/F s ) into the transfer function, where f ≤ F s and F s is the sampling rate in Hertz (number of samples per second). It can be shown that C(e j2πf/F s ) is a weighted sum of cosine functions and therefore having only real values (Tomé et al., 2010). 2.4.2 Filter coefficients and filter design The analysis-synthesis pair of SSA is formed by a causal filter (the analysis) whose input is the sequence x[n] and the output is fed into the synthesis filter which is an anti-causal filter. The analysis- synthesis filter pairs of SSA (Tomé et al., 2010) are data-driven filters, e.g., the filter coefficients are computed based on the autocorrelation function of the signal to be filtered. The autocorrelation function is the correlation of the signal with itself as a function of shifts in time, r m = E x n x n + m . The values of the autocorrelation function are used to estimate the correlation matrix whose eigen-decomposition gives the necessary information to calculate the filter coefficients. The eigenvectors of the matrix are used to estimate the coefficients c[n] and the corresponding eigenvalues are related with the energy of the filters output. The original signals suffered from a strong interference of high amplitude artifacts, such as participant movements or ocular movements. The EOG signal from each ERP was used to calculate the filter coefficients ensuring a custom filtering. Therefore, for each participant, the EOG average was used to compute the correlation matrix with entries of the autocorrelation function computed with M=101 time shifts. The eigenvector corresponding to the largest eigenvalue, of the eigendecomposition of the correlation matrix, was used to compute the analysis-synthesis pair as described before, originating a filter c[n] with 201 coefficients. Thus, only the frequencies associated with high amplitude movements present in each EOG channel that usually propagate to the remaining channels will be eliminated. Fig. 3 represents the frequency response and impulse response, respectively, of one filter designed to process the EEG segments. The frequency response shows that the pass-band is f <3Hz and the first zero around f =10Hz which corresponds to ≃ F s /M. The filter stop-band shows that the frequency zone around ≈10Hz and their multiples are strongly attenuated. Therefore the frequency contents of the filter output y[n] must have most of their energy concentrated up to 3Hz and in particular no energy around the 10Hz band (Fig. 3 - left). The same filter coefficients c[n] were applied to all channels in order to always have the same gain in amplitude. Finally, the corrected ERP signal was obtained by subtracting the extracted components to the original ERP x ~ n = x n − y n . Note that the corrected signal will preserve the alpha and beta bands of the original signal x[n]. 3 Results In this work segments of signals time-locked either with visual stimulus (t v ) or with response (t r ) onsets were considered. Therefore the segments, with duration of 4s, were taken from t ⁎ −1000ms to t ⁎ +3000ms, where t ⁎ ={t v , t r }, are the visual stimulus and the response onsets. Then the filter was applied to the average signal and the corrected version was obtained subtracting to the original signal. The filtering technique that was applied in the present work allowed to center the data. Therefore, no baseline removal was necessary. Note that the length of the segments ensures that the transient response of the filter does not include the time span of the ERP events under study. 3.1 P100 The segments centered in the visual stimulus, i.e., the first analysis window, were used to demonstrate the impact of the application of this method and its ability to clean the high amplitude artifacts in the signal. After filtering, in this time window it is possible to clearly identify the posterior P100 visual ERP component, which is associated with visual processing and is elicited by all visual stimuli, independently of the task (Luck, 2005). The segments were epoched time-locked to the onset of the visual stimuli, [01000] ms. Fig. 4 shows (a) the original average signal x[n] of one participant and (b) the corresponding corrected version x ~ n . As can be observed, all high amplitude artifacts were adequately removed and the peak around 100 ms is clearly visible in occipital regions. The grand average of the original and the corrected signal of all participants was also computed. Fig. 5 (a) presents the grand-average in channel O1 in the clean signal. To confirm the effect of the filtering operation described above, the head topography of the grand average waveforms considering a visualization window between [100 120] ms after the visual stimuli onset is presented, Fig. 5 (b) and (c), for original and corrected signal, respectively. As can be observed, whereas in the original signals (b), there is no clear evidence of the posterior positivity that characterizes the P100, this positivity is evident in the corrected signals (c). 3.2 FRN analysis As before segments, with 4s, time locked to the response onset, were averaged according to the condition (right or wrong) and then filtered to obtain y[n] and then subtracted to the original in order to have a corrected version x ~ n . For the FRN analysis, the EEG data segments centered on response onset were epoched between 100 ms prior to feedback onset to 400 ms after it. The more conspicuous FRN effects were observed in channels FCz and Cz, in agreement with the existing literature. Fig. 6 displays the grand-average ERP waveforms for these channels considering the two subsets of trials corresponding to the Right and Wrong answers, time-locked to the feedback. Thus, the value 0 ms corresponds to the moment when the feedback occurred. In the displayed waveforms, we can observe large negative components after the feedback, peaking around 250 ms, which are consistent with the feedback-related negativity. As can be observed, the FRN wave corresponding to the wrong answers (dashed line) is more negative than for the right answers (solid line). Fig. 7 represents the topography of the grand-average waveforms considering a visualization time window centered at 250 ms. The results show that this negative deflection was strongest at the electrode FCz, and exhibited a fronto-central scalp distribution. Furthermore, an apparent difference between the Wrong and Right conditions is observed, with more negative amplitudes for the Wrong trials. To assess the FRN differences between right and wrong trials, the average amplitudes and peak latencies in the window [220260] ms after the feedback were analyzed. FRN latencies were estimated as the minimum peak value found in the window [220260] ms after the feedback onset. Following other authors, we opted to run our analyses with the average amplitudes because mean amplitude measures are less affected by differences in the number of trials between conditions (Bellebaum and Daum, 2008; Luck, 2005), which was the case in the present study. For the analysis, the channels FC3, FCz, FC4 and Cz were considered, as they were the channels where the FRN was more evident after visual inspection. Two 2×4 repeated measures ANOVAs were carried out, with error condition (right and wrong) and electrode (FC3, FCz, FC4 and Cz) as within-subjects factors, considering both average amplitude and peak latency as the dependent variables. The Greenhouse-Geisser correction was applied for violations of sphericity and the Bonferroni adjustment was applied to multiple comparisons. Regarding amplitude, there was a significant main effect of error condition, F(1,57)=16.29, p <.001, partial η 2 =.222, where the wrong trials elicited significantly more negative amplitudes (M amp =−1.67μV) than the right trials (M amp =−0.76μV). There was also a significant main effect of electrode, F(2.44,138.86)=37.17, p <.001, partial η 2 =.40. The midline electrodes FCz (M amp =−1.46μV) and Cz (M amp =−1.58μV) elicited significantly more negative amplitudes than FC3 (M amp =−1.04μV) and FC4 (M =−0.77μV). The interaction between error condition and electrode was not significant, F(2.68,152.99)=1.004, p =.392, partial η 2 =.017. Regarding peak latencies, there was a significant main effect of electrode, F(3,171)=6.31, p <.001, partial η 2 =.10, where Cz exhibited significantly longer latencies (M lat =253ms) than FC4 (M lat =250ms). Latencies in FC3 and FCz (both M lat =250ms) did not differ significantly from the other locations. Neither the main effect of error condition, F(1,57)=1.82, p =.182, partial η 2 =.031, nor the interaction between error condition and electrode, F(2.32,132.51)=1.57, p =.209, partial η 2 =.027, were significant for the peak latency. Thus, results indicate, as expected, that errors elicited significantly more negative FRN amplitudes than correct responses. Furthermore, there were no significant differences in peak latency. 4 Discussion and conclusions The present work aimed to demonstrate that it is possible to study event-related potentials during the performance on a non-modified version of the Halstead Category Test (HCT) (DeFilippis, 2002), a neuropsychological instrument clinically used to explore cognitive executive function associated with the ability to formulate and apply abstract principles. The possibility of coupling neuropsychological data with information about brain dynamic activity associated with performance on the test can usefully increase the clinical potential of the test, allowing for a higher specificity in detecting brain dysfunction, an issue that has received some criticism in the past (Allen et al., 2007; Choca et al., 1997). It is important to use a non-modified version of the HCT to be able to use normative data and cut-off scores for clinical diagnosis. Since the clinical version of the test is not a classical paradigm for ERP recordings (being a time unconstrained task, where participants are free to visually explore the stimuli before making a response), we aimed to demonstrate that it is possible to use a SSA based filtering technique (Tomé et al., 2010) to process contaminated signal with high amplitude ocular and movement artifacts. As shown in this paper, this technique allows obtaining a clean signal with minimal loss of relevant information and no phase or latency distortions. More traditional approaches to processing ERP data, where segments containing artifacts are typically eliminated, would implicate the loss of a considerable amount of trials (Luck, 2005). This would result in a very low signal-to-noise ratio, since the clinical version of the test also contains a limited number of trials. The principle underlying each subtest of the HCT (except the seventh, which is a memory subtest) is the same throughout the entire subtest (but it may change from one subtest to another), and participants receive feedback on their performance after each trial. Thus, based on that feedback, they should opt for maintaining the same rule for the next stimulus, if performance was correct, or they should change their reasoning and try to extract a different abstract principle, if performance was incorrect, until they discover the general principle underlying that particular subtest. Due to the nature and difficulty of the task, there is a high likelihood of committing errors and the degree of uncertainty in the outcome of each trial is considerably high until the moment when the participant is confident that he/she has determined the underlying abstract principle of that subtest. Thus, performance on such a task requires that participants monitor the outcome of their behavior for errors and change their course of action accordingly, in an effort to avoid making another error in the subsequent trial. Hence, we expected that a feedback-related negativity (FRN) wave would be elicited in response to feedback stimuli, which would be more negative for error trials (when feedback was negative) than for correct trials (when feedback was positive). This ERP pattern is consistent with an error monitoring process (Hajcak et al., 2006; Miltner et al., 1997). After data processing and artifact removal through SSA filtering, a P100 visual component associated with the processing of all visual stimuli (Pratt, 2011) was clearly identified in epochs time-locked with the onset of the stimulus, which confirms the adequacy of the filtering procedure. Importantly, a FRN component was also identified in epochs time-locked with the feedback onset. This component had a fronto-central topographical distribution and peaked around 250ms after feedback onset. As predicted, error trials elicited significantly more negative amplitudes than correct trials, particularly at midline fronto-central electrode sites. The FRN is normally observed in tasks difficult enough that the subjects do not know the accuracy of their judgments until the feedback occurs, i.e., tasks with a certain degree of uncertainty (e.g., Gehring and Willoughby, 2002), which is consistent with performance on the HCT. Thus, this result supports the idea that performance on the HCT involves error monitoring processes, which allow participants to adjust their response strategy in face of negative feedback, in order to find the correct underlying rule/abstract principle. The present finding is consistent with previous studies using another executive function test, the Wisconsin Card Sorting Test (WCST), which requires partially similar cognitive processes (Kraus and Horowitz-Kraus, 2014; Vilà-Balló et al., 2015). In summary, the present study demonstrated that the application of a nonzero phase filtering technique successfully removed high-amplitude ocular and movement artifacts from EEG signals. In order to guarantee the same gain distortion, the SSA filter coefficients were computed only once by using the corresponding EOG average signal. The HCT is a well-established neuropsychological measure of non-verbal reasoning, abstract concept formation and cognitive flexibility, which are aspects of executive function. The identification of the FRN component associated with feedback processing during performance on the HCT is consistent with the role of the anterior cingulate cortex (ACC), the most likely neural generator of the FRN (Hauser et al., 2014; Walsh and Anderson, 2012), in error detection and conflict monitoring. The present results are compatible with views of ACC function that link this structure to performance-monitoring processes and predict increased ACC activity and, consequently, larger FRN amplitudes, when performance is poor (Miltner et al., 1997; Holroyd and Coles, 2002). However, alternative views of ACC function, such as the predicted response-outcome model (Alexander and Brown, 2011), propose that the main function of the ACC is to predict the likely outcomes of actions and to signal when an outcome is unexpected, independently of that outcome being good or bad. Thus, according to this theory, an increased FRN should be elicited after unexpected outcomes regardless of their valence. Evidence that similar FRN amplitudes for negative and positive unexpected feedback occur when the unexpectedness of both types of feedback is equivalent has provided support for this theory (Ferdinand et al., 2012). In the present study, negative feedback is less frequent than positive feedback and therefore it is not possible to test these different theories. This was indeed not the aim of this work, as we intended to maintain the original format of the HCT. Future studies with different objectives may introduce an adaptation of the HCT structure in order to balance the frequencies of positive and negative feedback, and thus be able to specifically explore the different predictions made by theories about ACC function regarding the FRN component as an electrophysiological correlate of performance-monitoring. At the clinical level, the present results may contribute to a better understanding of the HCT as a neuropsychological assessment instrument, as well as to its clinical utility. The possibility of coupling behavioral performance on the test with the recording of ERPs will potentially increase the specific information that can be extracted from the test when applying it to individuals who may suffer from altered feedback processing. This might affect general performance on the HCT, but a global error score might not be sensitive enough to pinpoint the specific cognitive ability that is impaired. Using the FRN as an electrophysiological marker of feedback processing might help to identify specific deficits in those processes (e.g., Vilà-Balló et al. 2015). To this end, future research should explore the possibility of analyzing single-subject ERP correlates of performance on the HCT, since there is evidence that error-related potentials can be measured on a single-trial basis (e.g., Falkenstein et al., 2000; Wiersema et al., 2005). In future studies, it would also be interesting to explore how the FRN and other error-related components vary as the learning of the new rules proceeds during performance on the HCT. It is possible that the FRN is larger at the beginning of the task, when the correct response is yet unknown, whereas other error-related components, such as the ERN, which are initially absent, may become larger when the participant has learned the adequate response (Hoffmann and Falkenstein, 2012). A trial-by-trial analysis would also be critical to understanding these dynamic changes in the correlates of the error-monitoring neural system as learning progresses. Acknowledgments This research was supported by Bial Foundation with Grant ref. 136/08 to Isabel M. Santos and by funding from Foundation for Science and Technology (FCT) and the POPH/FSE Program (grant reference SFRH/BPD/101112/2014) to Ana R. Teixeira. The funding agency had no role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript. References Adams et al., 1995 K.M. Adams S. Gilman R. Koeppe K. Kluin L. Junck M. Lohman D. Johnson-Greene S. Berent D. Dede P. Kroll Correlation of neuropsychological function with cerebral metabolic rate in subdivisions of frontal lobes of older alcoholic patients measured with [18F] fluorodeoxyglucose and positron emission tomography Neuropsychology 9 1995 275 280 10.1037/0894-4105.9.3.275 Alexander and Brown, 2011 W.H. Alexander J.W. Brown Medial prefrontal cortex as an action-outcome predictor Nat. Neurosci. 14 2011 1338 1344 10.1038/nn.2921 Alexandrov, 2009 T. Alexandrov A method of trend extraction using singular spectrum analysis REVSTAT 7 2009 1 22 Allen et al., 1999 D.N. Allen G. Goldstein E. Mariano Is the Halstead Category Test a multidimensional instrument? J. Clin. Exp. Neuropsychol. 21 1999 237 244 10.1076/jcen.21.2.237.926 Allen et al., 2007 D.N. Allen J.E. Caron L.A. Duke G. Goldstein Sensitivity of the Halstead Category Test factor scores to brain damage Clin. Neuropsychol. 21 2007 638 652 10.1080/13854040600744821 Anderson et al., 1991 S.W. Anderson H. Damasio R.D. Jones D. Tranel Wisconsin Card Sorting Test performance as a measure of frontal lobe damage J. Clin. Exp. Neuropsychol. 13 1991 909 922 10.1080/01688639108405107 Barceló, 2003 F. Barceló The Madrid Card Sorting Test (MCST): a task switching paradigm to study executive attention with event-related potentials Brain Res. Protocol. 11 2003 27 37 10.1016/S1385-299X(03)00013-8 Barceló and Rubia, 1998 F. Barceló F.J. Rubia Non-frontal P3b-like activity evoked by the Wisconsin Card Sorting Test Neuroreport 9 1998 747 751 10.1097/00001756-199803090-00034 Barceló et al., 1997 F. Barceló M. Sanz V. Molina F.J. Rubia The Wisconsin Card Sorting Test and the assessment of frontal function: a validation study with event-related potentials Neuropsychologia 35 1997 399 408 Bellebaum and Daum, 2008 C. Bellebaum I. Daum Learning-related changes in reward expectancy are reflected in the feedback-related negativity Eur. J. Neurosci. 27 2008 1823 1835 10.1111/j.1460-9568.2008.06138.x Bernat et al., 2011 E.M. Bernat L.D. Nelson V.R. Steele W.J. Gehring C.J. Patrick Externalizing psychopathology and gain/loss feedback in a simulated gambling task: dissociable components of brain response revealed by time-frequency analysis J. Abnorm. Psychol. 120 2011 352 364 10.1037/a0022124 Choca et al., 1997 J.P. Choca L. Laatsch L. Wetzel A. Agresti The Halstead Category Test: a fifty year perspective Neuropsychol. Rev. 7 1997 61 75 10.1023/b:nerv.0000005944.98635.16 DeFilippis, 2002 N.A. DeFilippis The Category Test, Computer Version 2002 Psychological Assessment Resources DeFilippis and McCampbell, 1997 N.A. DeFilippis E. McCampbell The Booklet Category Test 1997 Psychological Assessment Resources Falkenstein et al., 2000 M. Falkenstein J. Hoormann S. Christ J. Hohnsbein ERP components on reaction errors and their functional significance: a tutorial Biol. Psychol. 51 2000 87 107 10.1016/S0301-0511(99)00031-9 Ferdinand et al., 2012 N.K. Ferdinand A. Mecklinger J. Kray W.J. Gehring The processing of unexpected positive response outcomes in the mediofrontal cortex J. Neurosci. 32 2012 12087 12092 10.1523/JNEUROSCI.1410-12.2012 Gehring and Willoughby, 2002 W.J. Gehring A.R. Willoughby The medial frontal cortex and the rapid processing of monetary gains and losses Science 295 2002 2279 2282 10.1126/science.1066893 Golyandina et al., 2001 N. Golyandina V. Nekrutkin A. Zhigljavsky Analysis of Time Series Structure: SSA and Related Techniques 2001 Chapman & HALL/CRC Hajcak et al., 2006 G. Hajcak J.S. Moser C.B. Holroyd R.F. Simons The feedback-related negativity reflects the binary evaluation of good versus bad outcomes Biol. Psychol. 71 2006 148 154 10.1016/j.biopsycho.2005.04.001 Hauser et al., 2014 T.U. Hauser R. Iannaccone P. Stämpfli R. Drechsler D. Brandeis S. Walitza S. Brem The feedback-related negativity (FRN) revisited: new insights into the localization, meaning and network organization NeuroImage 84 2014 159 168 10.1016/j.neuroimage.2013.08.028 Heaton et al., 1993 R.K. Heaton G.J. Chelune J.L. Talley G.G. Kay G. Curtiss Wisconsin Card Sorting Test Manual: Revised and Expanded 1993 Psychological Assessment Resources Odessa Hoffmann and Falkenstein, 2012 S. Hoffmann M. Falkenstein Predictive information processing in the brain: errors and response monitoring Int. J. Psychophysiol. 83 2012 208 212 10.1016/j.ijpsycho.2011.11.015 Holroyd, 2004 C.B. Holroyd A note on the oddball N200 and the feedback ERN. Learning and cognitive control laboratory M. Ullsperge M. Falkenstein Errors, Conflicts, and the Brain: Current Opinions on Performance Monitoring 2004 Max Planck Institute for Human Cognitive and Brain Sciences Leipzig 211 218 Holroyd and Coles, 2002 C.B. Holroyd M.G. Coles The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity Psychol. Rev. 109 2002 679 709 10.1037/0033-295X.109.4.679 Kraus and Horowitz-Kraus, 2014 D. Kraus T. Horowitz-Kraus The effect of learning on feedback-related potentials in adolescents with dyslexia: an EEG-ERP study PLoS One 9 2014 e100486 10.1371/journal.pone.0100486 Luck, 2005 S.J. Luck An Introduction to the Event-related Potential Technique 2005 MIT Press McNally et al., 2015 S. McNally J. Dsurney J. McGovern N. DeFilippis L. Chan Concurrent validity of new subscale scores for the Booklet Category Test Assessment 2015 1 9 10.1177/1073191115588783 Miltner et al., 1997 W.H.R. Miltner C.H. Braun M.G.H. Coles Event-related brain potentials following incorrect feedback in a time-estimation task: evidence for a generic neural system for error detection J. Cogn. Neurosci. 9 1997 788 798 10.1162/jocn.1997.9.6.788 Minassian et al., 2003 A. Minassian W. Perry M. Carlson M. Pelham N. DeFilippis The category test perseveration, loss of set, and memory scales: three new scales and their relationship to executive functioning measures Assessment 10 2003 213 221 10.1177/1073191103253498 Pratt, 2011 H. Pratt Sensory ERP components E.S. Kappenman S.J. Luck The Oxford Handbook of Event-related Potential Components 2011 Oxford University Press New York 89 114 10.1093/oxfordhb/9780195374148.013.0050 Rousselet, 2012 G.A. Rousselet Does filtering preclude us from studying ERP time-courses? Front. Psychol. 3 2012 10.3389/fpsyg.2012.00131 Teixeira et al., 2006 A.R. Teixeira A.M. Tomé E.W. Lang P. Gruber A.M. da Silva Automatic removal of high-amplitude artifacts from single-channnel electroencephalograms Comput. Methods Prog. Biomed. 83 2006 125 138 10.1016/j.cmpb.2006.06.003 Tomé et al., 2010 A.M. Tomé A.R. Teixeira N. Figueiredo I.M. Santos P. Georgieva E.W. Lang SSA of biomedical signals: a linear invariant systems approach Stat. Interface 3 2010 345 355 10.4310/SII.2010.v3.n3.a Vilà-Balló et al., 2015 A. Vilà-Balló T. Cunillera C. Rostan P. Hdez-Lafuente L. Fuentemilla A. Rodríguez-Fornells Neurophysiological correlates of cognitive flexibility and feedback processing in violent juvenile offenders Brain Res. 1610 2015 98 109 10.1016/j.brainres.2015.03.040 Walsh and Anderson, 2012 M.M. Walsh J.R. Anderson Learning from experience: event-related potential correlates of reward processing, neural adaptation, and behavioral choice Neurosci. Biobehav. Rev. 36 2012 1870 1884 10.1016/j.neubiorev.2012.05.008 Widmann and Schröger, 2012 A. Widmann E. Schröger Filter effects and filter artifacts in the analysis of electrophysiological data Front. Psychol. 3 2012 10.3389/fpsyg.2012.00233 Wiersema et al., 2015 J.R. Wiersema J.J. van der Meere H. Roeyers ERP correlates of impaired error monitoring in children with ADHD J. Neural Transm. 112 2005 1417 1430 10.1007/s00702-005-0276-6 "
    },
    {
        "doc_title": "A green's function-based Bi-dimensional empirical mode decomposition",
        "doc_scopus_id": "84959519803",
        "doc_doi": "10.1016/j.ins.2016.01.089",
        "doc_eid": "2-s2.0-84959519803",
        "doc_date": "2016-06-20",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Bi dimensional empirical mode decomposition (BEMD)",
            "Bi-dimensional empirical mode decompositions",
            "Empirical Mode Decomposition",
            "Envelope surface",
            "Facial images",
            "Intrinsic Mode functions",
            "Surface interpolation",
            "Two-dimensional surface"
        ],
        "doc_abstract": "© 2016 Elsevier Inc. All rights reserved.Bidimensional Empirical Mode Decomposition(BEMD) interprets an image as a superposition of Bidimensional Intrinsic Mode Functions (BIMFs). They are extracted by a process called sifting, which encompasses two-dimensional surface interpolations connecting a set of local maxima or minima to form corresponding envelope surfaces. Existing surface interpolation schemes are computationally very demanding and often induce artifacts in the extracted modes. This paper suggests a novel method of envelope surface interpolation based on Green's functions. Including surface tension greatly improves the stability of the new method which we call Green's function in tension-based BEMD (GiT-BEMD). Simulation results, using toy images with various textures, facial images and functional neuroimages, demonstrate the superior performance of the new method when compared to its canonical BEMD counterpart. GiT-BEMD strongly speeds up computations and achieves a higher quality of the extracted BIMFs. Furthermore, GiT-BEMD can be extended simply to an ensemble-based variant (GiT-BEEMD), if needed. In summary, the study suggests the new variant GiT-BEMD as a highly competitive, fast and stable alternative to existing BEMD techniques for image analysis.",
        "available": true,
        "clean_text": "serial JL 271625 291210 291691 291718 291773 291813 291814 291866 291870 31 Information Sciences INFORMATIONSCIENCES 2016-02-05 2016-02-05 2016-03-02 2016-03-02 2019-10-29T08:58:02 S0020-0255(16)30031-7 S0020025516300317 10.1016/j.ins.2016.01.089 S300 S300.2 FULL-TEXT 2019-10-31T06:20:26.63656Z 0 0 20160620 2016 2016-02-05T15:44:42.448574Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst orcid primabst ref 0020-0255 00200255 true 348 348 C Volume 348 20 305 321 305 321 20160620 20 June 2016 2016-06-20 2016 article fla Copyright © 2016 Elsevier Inc. All rights reserved. AGREENSFUNCTIONBASEDBIDIMENSIONALEMPIRICALMODEDECOMPOSITION ALBADDAI S 1 Introduction 1.1 Background 1.2 Motivation and outline 2 Bidimensional empirical mode decomposition 2.1 General BEMD 2.2 Canonical BEEMD 3 Green’s function-based BEEMD 3.1 Extraction of local extrema 3.2 Green’s function for estimating envelopes 4 Results and discussion 4.1 Artificial image 4.2 Face image 4.3 Biomedical image 5 Conclusion Acknowledgment References ABADAN 2014 3133 3143 S ALBADDAI 2014 218 236 S ALI 2015 1261 1277 H ALTAF 2007 1009 1012 M PROCIEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSING ROTATIONINVARIANTCOMPLEXEMPIRICALMODEDECOMPOSITION ARAFAT 2009 461 464 A ACOUSTICSSPEECHSIGNALPROCESSING2009ICASSP2009IEEEINTERNATIONALCONFERENCE AUTOMATICDETECTIONECGWAVEBOUNDARIESUSINGEMPIRICALMODEDECOMPOSITION BHUIYAN 2008 S IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP ANOVELAPPROACHFASTADAPTIVEBIDEIMENSIONALEMPIRICALMODEDECOMPOSITION BHUIYAN 2009 18 24 S 2009INTERNATIONALCONFERENCEMACHINELEARNINGAPPLICATIONS STUDYBIDIMENSIONALEMPIRICALMODEDECOMPOSITIONMETHODFORVARIOUSRADIALBASISFUNCTIONSURFACEINTERPOLATORS BOUGUILA 2012 5946 5959 N BRIGGS 1974 39 48 I CELEBI 2012 800 805 A CHANG 2011 1621 1624 L PROCICASSP PARALLELIMPLEMENTATIONMULTIDIMENSIONALENSEMBLEEMPIRICALMODEDECOMPOSITION CHEN 2006 171 195 Q DAMERVAL 2005 701 704 C DENG 2005 387 390 S DENG 2011 663 680 X FALTERMEIER 2011 509 R FLEUREAU 2011 2783 2792 J FLEUREAU 2011 1309 1316 J GALLIX 2012 13451 13461 A HASIMAH 2015 1261 1277 A HUANG 1998 903 995 N HUANG 2003 2317 2345 N JAGER 2010 G KOH 2014 95 98 M LEI 2011 7334 7341 Y LI 2008 2981 2986 K LINDERHED 2002 1 8 A WAVELETINDEPENDENTCOMPONENTANALYSISAPPLICATIONSIXPROCEEDINGSSPIE 2DEMPIRICALMODEDECOMPOSITIONSINSPIRITIMAGECOMPRESSION LIU 2005 33 36 Z LIU 2004 803 806 Z PROC17THIEEEINTERNATIONALCONFERENCEPATTERNRECOGNITIONICPR04 TEXTURECLASSIFICATIONTHROUGHDIRECTIONALEMPIRICALMODEDECOMPOSITION LIU 2004 279 282 Z PROCEEDINGSIEEEINTERNATIONALCONFERENCEIMAGEPROCESSINGICIP04 TEXTURESEGMENTATIONUSINGDIRECTIONALEMPIRICALMODEDECOMPOSITION LOONEY 2009 1626 1630 D LU 2013 149 157 Y MOHEBBI 2014 415 427 M NUNES 2003 J NUNES 2009 125 175 J NUNES 2005 177 188 J PACHORI 2015 4567 4581 R PARK 2011 366 373 C PRASHANTH 2014 3333 3342 R REHMAN 2009 3449 3452 N PROCICASSP QUALITATIVEANALYSISROTATIONALMODESWITHINTHREEDIMENSIONALEMPIRICALMODEDECOMPOSITION REHMAN 2010 1059 1068 N REHMAN 2010 1291 1302 N REHMAN 2010 1 7 N INTERNATIONALJOINTCONFERENCENEURALNETWORKSIJCNN2010 QUADRIVARIATEEMPIRICALMODEDECOMPOSITION REHMAN 2013 25 N REN 2015 P RILLING 2007 936 939 G ROJAS 2013 2756 2766 A RUTKOWSKI 2010 215 229 T SANDWELL 1987 139 142 D SHARMA 2015 1106 1117 R SHEN 2007 M IEEEINTERNATIONALCONFERENCEINTERNATIONALCONFERENCESIGNALPROCESSINGICSP6 MODIFIEDBIDIMENSIONALEMPIRICALMODEDECOMPOSITIONFORIMAGEDENOISING SHEN 2007 299 304 M PROCEEDINGSINTERNATIONALCONFERENCEINTELLIGENTCOMPUTINGICIC6 ANOVELBOUNDARYEXTENSIONAPPROACHFOREMPIRICALMODEDECOMPOSITION SMITH 1990 293 305 W TANAKA 2006 101 104 T WANG 2010 277 293 G WANG 2012 17 J WASHIZAWA 2006 1248 1255 Y PROCEEDINGS10THINTERNATIONALCONFERENCEKNOWLEDGEBASEDINTELLIGENTINFORMATIONENGINEERINGSYSTEMSKES06BGABRYSRJHOWLETTLCJAINEDS AFLEXIBLEMETHODFORENVELOPEESTIMATIONINEMPIRICALMODEDECOMPOSITION WEGMAN 1983 351 365 E WESSEL 2009 1247 1254 P WESSEL 1998 77 93 P WESSEL 1991 441 446 P WESSEL 1995 329 P WU 2011 6112 6117 J WU 2009 1 41 Z WU 2009 339 372 Z WU 2011 95 113 Z XIONG 2006 1516 1521 C XU 2006 3081 3096 Y YEH 2010 25 C ZEILER 2013 21 32 A ZHANG 2015 1389 1396 J ZHANG 2010 139 142 J PROCEEDINGSIEEESOUTHEASTCONSOUTHEASTCON EDGEDETECTIONUSINGFASTBIDIMENSIONALEMPIRICALMODEDECOMPOSITIONMATHEMATICALMORPHOLOGY ALBADDAIX2016X305 ALBADDAIX2016X305X321 ALBADDAIX2016X305XS ALBADDAIX2016X305X321XS 2018-03-02T00:00:00Z 2019-03-12T14:49:55.472Z S0020025516300317 DAAD Deutscher Akademischer Austauschdienst Financial support by the DAAD, Acciones Integradas Hispano - Alemanas is gratefully acknowledged. item S0020-0255(16)30031-7 S0020025516300317 10.1016/j.ins.2016.01.089 271625 2019-10-31T06:20:26.63656Z 2016-06-20 true 4108905 MAIN 17 42216 849 656 IMAGE-WEB-PDF 1 gr1 14095 163 139 gr10 26431 164 219 gr2 16300 114 219 gr3 5877 114 219 gr4 12773 57 219 gr5 29103 164 209 gr6 20575 131 219 gr7 25584 164 197 gr8 29113 164 219 gr9 18495 82 219 gr1 101270 759 646 gr10 89822 549 732 gr2 55560 335 645 gr3 32391 322 618 gr4 43549 168 646 gr5 123806 574 732 gr6 75011 438 732 gr7 132900 588 708 gr8 98240 480 641 gr9 65197 239 641 si1 285 13 60 si10 229 13 51 si11 354 15 77 si12 59 1 52 si13 608 16 144 si14 1190 17 303 si15 852 17 229 si16 836 58 164 si17 281 22 31 si18 425 16 131 si19 1467 26 375 si2 386 16 87 si20 2736 56 519 si21 745 24 140 si22 347 22 37 si23 3194 130 302 si24 738 55 135 si25 1175 58 228 si26 976 58 153 si27 1242 58 252 si28 630 55 101 si29 538 16 168 si3 252 13 63 si30 806 22 196 si31 1022 55 191 si32 421 16 137 si33 1213 25 305 si34 241 24 29 si35 333 22 81 si36 1176 21 317 si37 746 17 161 si38 586 19 128 si39 1205 46 220 si4 312 16 68 si40 256 16 55 si41 328 23 64 si42 1027 46 181 si43 1891 46 407 si44 2709 56 455 si45 1998 17 545 si46 2624 46 617 si47 255 11 59 si48 2287 123 321 si49 295 13 64 si5 287 13 60 si50 430 16 90 si51 413 16 128 si52 183 17 22 si53 1387 55 379 si54 512 16 157 si55 1190 19 387 si56 485 20 113 si57 294 13 64 si58 726 16 197 si59 324 16 82 si6 286 16 58 si60 367 20 81 si61 1226 19 331 si62 671 16 190 si63 595 16 187 si64 321 16 69 si65 423 16 81 si66 340 13 66 si67 352 16 83 si68 344 16 78 si69 373 15 93 si7 256 13 54 si70 417 24 110 si8 277 16 58 si9 306 16 64 INS 12040 S0020-0255(16)30031-7 10.1016/j.ins.2016.01.089 Elsevier Inc. Fig. 1 Top row: (right) component 1 (ATC-1) results from a superposition of (left) and (middle), Upper Middle row: (right) component 2 (ATC-2) has (left) and (middle) superimposed, Lower Middle row: (right) component 3 (ATC-3) results from a superposition of (left) and (middle), Bottom row: shows the produced original artificial texture image (ATI) by summing up the component images ATC-1, ATC-2 and ATC-3. 1D intensity profiles of ATC-1, ATC-2, and ATC-3 are shown also. Fig. 1 Fig. 2 Decomposition of the ATI using GiT-BEMD , Top: represents the extracted BIMFs (BIMF1, BIMF2, BIMF3) by GiT-BEMD and Bottom: the summation of BIMFs. Fig. 2 Fig. 3 Top: Intensity profiles of the original ATI, Left column: intensity profiles of BIMF1, BIMF2 and BIMF3 obtained by canonical BEEMD with ensemble size E = 50 and Right column: intensity profiles of corresponding BIMFs obtained by GiT-BEMD. Fig. 3 Fig. 4 Illustrates the components of ATI obtained by GiT-BEMD with different tension parameters T = 0.001 , T = 0.1 and T = 0.9 , respectively. Fig. 4 Fig. 5 Top: Decomposition of the ATI using canonical BEMD. From left to right BIMF1, BIMF2 and BIMF3. Middle: Decomposition of the ATI using canonical BEEMD with ensemble size E = 20 . Bottom: Decomposition of the ATI using canonical BEEMD with ensemble size E = 50 . Fig. 5 Fig. 6 Illustrate the effect of increasing the extracted modes, from ATI, by GiT-BEMD. From top to bottom M = 3 , M = 4 and M = 5 , respectively. Fig. 6 Fig. 7 Top left: the extracted BIMFs of Lena image obtained by pseudo-2D EMD, Top middle: by canonical BEMD, Top right: by canonical BEEMD with ensemble size E = 20 , Bottom left: by GiT-BEMD, Bottom middle: by GiT-BEEMD with E = 2 and Bottom right: by GiT-BEEMD with E = 20 . Fig. 7 Fig. 8 Top left: shows the original fMRI slice, Top right:shows the extracted BIMFs by GiT-BEMD, Bottom left: shows the extracted BIMFs by GiT-BEEMD with E = 2 , Bottom right: shows the extracted BIMFs by GiT-BEEMD with E = 20 . Fig. 8 Fig. 9 Left: shows the extracted BIMFs by canonical BEEMD with E = 20 and Right: shows the extracted BIMFs by GiT-BEEMD with E = 2 . Fig. 9 Fig. 10 Image modes resulting from a decomposition of the Lena image using GiT-BEMD with a decreasing surface tension. The tension parameter tk decreases from top left to bottom right in steps of Δ t k = 0.2 . Fig. 10 Table 1 Comparison among various GiT-BEMD/BEMDs for the three images discussed in this paper in terms of total time required. Where the number in the end of the methods refers to the number of ensemble which employed in each. Table 1 Image Method pseudo-2D BEMD (min) BEEMD20 (min) BEEMD50 (min) GiT-BEMD (min) GiT-BEEMD2 (min) GiT-BEEMD20 (min) EMD (min) Lena 0.23 2.60 52.00 − − − 0.50 2.00 20.00 ATI − − − 1.00 16.04 40.03 0.35 − − − − − − fMRI slice − − − 2.30 48.00 − − − 0.42 0.64 14.00 A green’s function-based Bi-dimensional empirical mode decomposition Saad Al-Baddai a b Karema Al-Subari a b Ana Maria Tomé c Jordi Solé-Casals d Elmar Wolfgang Lang * a a CIML Lab, Department of Biophysics, University of Regensbug, 93040 Regensburg, Germany CIML Lab, Department of Biophysics University of Regensbug Regensburg 93040 Germany b Department of Information Sciences, University of Regensburg, 93040 Regensburg, Germany Department of Information Sciences University of Regensburg Regensburg 93040 Germany c DETI - IEETA, Universidade de Aveiro, 3810-193 Aveiro, Portugal DETI - IEETA Universidade de Aveiro Aveiro 3810-193 Portugal d Data and Signal Processing Research Group, U Sciences Tech, University of Vic - Central University of Catalonia, C/Laura 13, 08500 Vic, Catalonia, Spain Data and Signal Processing Research Group U Sciences Tech University of Vic - Central University of Catalonia, C/Laura 13 Vic Catalonia 08500 Spain * Corresponding author. Tel.: +499419432599; fax: +499419432479. Bidimensional Empirical Mode Decomposition(BEMD) interprets an image as a superposition of Bidimensional Intrinsic Mode Functions (BIMFs). They are extracted by a process called sifting, which encompasses two-dimensional surface interpolations connecting a set of local maxima or minima to form corresponding envelope surfaces. Existing surface interpolation schemes are computationally very demanding and often induce artifacts in the extracted modes. This paper suggests a novel method of envelope surface interpolation based on Green’s functions. Including surface tension greatly improves the stability of the new method which we call Green’s function in tension-based BEMD (GiT-BEMD). Simulation results, using toy images with various textures, facial images and functional neuroimages, demonstrate the superior performance of the new method when compared to its canonical BEMD counterpart. GiT-BEMD strongly speeds up computations and achieves a higher quality of the extracted BIMFs. Furthermore, GiT-BEMD can be extended simply to an ensemble-based variant (GiT-BEEMD), if needed. In summary, the study suggests the new variant GiT-BEMD as a highly competitive, fast and stable alternative to existing BEMD techniques for image analysis. Keywords Empirical mode decomposition Green’s function Surface Interpolation 1 Introduction 1.1 Background Empirical mode decomposition (EMD), as pioneered by Huang et al. [21], is a data driven signal processing algorithm that quantitatively decomposes any nonlinear and non-stationary data into intrinsic modes, thereby obtaining local features, and their related time-frequency distribution. The first step of this method decomposes the data/signal into its characteristic intrinsic mode functions (IMFs) [12,58] while the second step finds the time frequency distribution of the data from each IMF by utilizing the concepts of Hilbert transform and instantaneous frequency. The complete process is also known as the Hilbert–Huang Transform (HHT) [4]. Lacking any rigorous mathematical basis, [68] advocated local physical rather than global mathematical constraints to preserve any physical meaning of the IMFs extracted. Soon after its invention, this decomposition technique has been extended to analyze multi-dimensional data sets [17,36,37,42,67] including complex-valued data sets [4,23,33,56] and implementations on GPUs for parallel processing [11]. Besides an extension to multi-dimensional data sets, EMD also has been extended to multi-variate data sets, most notably multi-channel recordings of biomedical signals [18,24,40,43–46,50]. Since its invention numerous applications of EMD have been reported in such diverse fields as functional neuroimaging [19,41,49], face recognition [20], facial emotion recognition [3], biomedical signals [5,25,35,39,47,52,71,72], neuromonitoring [16], analysis of complex networks [27], image enhancement [10], fault diagnosis of mechanical systems [26], ultrasound echo detection [34], speaker identification [65], speech enhancement [24], forecasting [1], moving target recognition [73] etc. just to mention a few more recent publications. However, most of these applications concern one-dimensional data sets and corresponding EMD algorithms. Obviously, two-dimensional image data sets were of special interest [48]. In a first approach, such two-dimensional data was treated as a collection of one-dimensional slices, which were decomposed with one-dimensional EMD [31,32]. This procedure is called pseudo-two-dimensional EMD [67]. The latter technique treats each row and/or each column of the 2D data set separately by a 1D EMD, which renders the sifting process faster than in a genuine 2D decomposition. But this parallel 1D implementation results in poor BIMF components compared to the canonical 2D procedure due to the fact that the former ignores the correlation among the rows and/or columns of a 2D image [29]. In addition, pseudo-2D-EMD needs a coherence structure associated with the spatial scales in a particular direction, which significantly limits its use. These recent developments in analysis methods for non-linear and non-stationary data sets have received considerable attention by image analysts. Thus several attempts have been started lately to extend EMD to multi-dimensional data sets like two-dimensional (2D) data arrays and images. These extensions are variously known as bidimensional EMD (BEMD), image EMD (IEMD), 2D EMD and so on [13,28,30,31,36–38,69,70]. Some of these works especially exploit mode decompositions to compute texture information contained in the images. In [74] a new two-dimensional EMD (2DEMD) method is proposed, which is claimed being faster and better-performing than current 2DEMD methods. In [69] rotation-invariant texture feature vectors are extracted at multiple scales or spatial frequencies based on a BEMD. In Nunes et al. [36–38] the BEMD-based texture extraction algorithm is demonstrated in experiments with both artificial and natural images. A major breakthrough has been achieved by Wu et al. [67], who recently proposed a Multi-dimensional Ensemble Empirical Mode Decomposition (mdEEMD) for multidimensional data arrays. This algorithm turned out to be very efficient in practical two-dimensional applications, especially if combined with Ensemble Empirical Mode Decomposition (EEMD) [66]. Lately also a full Bayesian approach, based on a reversible jump Markov Chain Monte Carlo procedure to sample from the unknown posteriors, has been proposed by Bouguila and Elguebaly [8] and applied for image texture retrieval and classification. Although this represents an optimal data analysis technique, its computational complexity is prohibitive, especially when big data have to be considered. The computationally most demanding operation of all these algorithms involves an envelope surface interpolation step. While cubic spline interpolation is preferred for 1D interpolation [57], various types of radial basis function, multilevel B-spline, Delaunay triangulation, Order-Statistics Filter, Finite Elements method and so on have been used for 2D scattered data interpolation [21,70]. Among them, Delaunay triangulation and a Finite Elements Method provide relatively faster decomposition compared to the other methods. In [13], the influence of various interpolation methods is studied, and a sifting process is proposed based on a Delaunay triangulation with subsequent cubic interpolation on triangles. Subsequently, the envelope surface interpolation step is replaced by either a direct envelope surface estimation method or radial basis function interpolators [6,7]. Finally, the modified 2D EMD algorithm proposed in [69] implements the FastRBF algorithm for the estimation of the envelope surfaces. 1.2 Motivation and outline However, in multi-dimensional EMD it is generally required finding local maxima and local minima, jointly known as local extrema, and subsequently interpolating these points in each iteration of the process. In general, surface interpolation schemes have to face the following issues: Computational load: Local extrema of an one-dimensional (1D) signal are obtained using either a sliding window or local derivative, and local extrema of a 2D data/image set are extracted using either a sliding window technique or various morphological operations [21,36–38]. Hence, detection and interpolation of local extrema during each iteration render the process complicated and time consuming. The situation is more difficult for the case of BEMD as it requires an interpolation of a set of 2D scattered two-dimensional data arrays during each iteration. For some images decomposition may take hours or days unless any additional stopping criterion is employed. However, additional stopping criteria may result in an inaccurate and incomplete decomposition [22,53]. Boundary artifacts: Another common and significant problem related to the interpolation of scattered 2D data in BEMD is that the maxima or minima map often does not contain any data points (interpolation centers) at the boundary region. This shortcoming becomes especially severe for bidimensional intrinsic modes, henceforth called BIMFs, extracted at the end of the decomposition process where spatial frequencies are low and the modes encompass only few extremal points. Currently available interpolation methods for scattered data are inefficient in handling this kind of situation. Additionally, the effect of incorrect interpolation at the boundary gradually propagates towards the central region from iteration to iteration, and from BIMF mode to BIMF mode, causing corrupted BIMFs. Over-and Undershooting: Overshooting or undershooting is another problem of interpolation-based envelope estimation, which causes incorrect BIMFs. This is especially true for higher order spline interpolation schemes. Although a few modifications have been suggested in the literature to reduce the number of iterations and/or to overcome boundary artifacts [13,22,29], the technique still suffers from the above-mentioned problems to some extent. In the BEMD process, the number of extrema decreases from BIMF to BIMF. For the later modes, there may be very few irregularly spaced local maxima or minima, which can cause highly erroneous and misleading upper or lower envelopes, and thus result in highly incorrect BIMFs. In order to improve the algorithm performance, some modifications have been suggested for one-dimensional EMD [54,59], which may not be useful for BEMD in the context of processing speed and algorithm complexity. For example, in fast and adaptive BEMD (FABEMD) [6], although it enhances the computational load noticeably with artificial texture, sometimes it may impose some difficulty in applying the FABEMD algorithm with real images; because it is difficult to determine the proper type of this method without a priori knowledge, which leads to improper intrinsic modes. On the other hand, the requirement for manipulation of window width for a BIMF may impose additional complexity, when the calculated value does not appear larger than the previous BIMF mode. Due to the property of order statistics, filter-based envelope estimation followed by a smoothing operation, oversifting in FABEMD may cause improper BIMFs as well. Moreover, any type of additional processing steps may turn the process more complex and computationally expensive. Nonetheless, BEMD is a promising image processing technique that has been applied successfully in various real world problems, for example, medical image analysis [2], image analysis [36], texture analysis [30], and so on. But the main problem for BEMD is either a high computational load or otherwise a reduced quality of the extracted intrinsic modes. Hence, any improvement of any BEMD algorithm is very important. Following we refer to the BEEMD approach developed by Wu et al. [67] as the canonical BEMD variant, and contrast it with our novel bi-dimensional EMD (GiT-BEMD) approach that replaces the direct spline interpolation step by an interpolation with splines in tension employing Green’s functions [62]. Although, cubic splines are in widespread use, because of their smooth shape, these functions can exhibit unwanted oscillations between data points. Adding tension to the spline overcomes this drawback. Here, we apply a technique for interpolation and gridding in bi-dimensional EMD applications using Green’s functions for splines in tension, and examine some of the properties of these functions. The technique is borrowed from geophysics where it is in use already since more than a decade [61]. Physical sciences have a frequent need for data interpolation and gridding. Such tasks are commonly accomplished by averaging [60] and finite difference methods [9] employing cubic splines on a regular grid. Introducing surface tension often helps to suppress undesired oscillations of such splines [55]. The Generic Mapping Tools offer a software package implementing an algorithm which uses continuous curvature splines in tension [63,64]. Minimum curvature gridding based on Green’s functions of the bi-harmonic operator has been proposed first by Sandwell [51]. It offered enhanced flexibility by employing both data values and gradients to constrain the interpolating surface. The method further allowed least squares fitting to noisy data sets and could evaluate the surface at any location instead of being confined to a regular grid. However, the appearance of extraneous inflection points, common to all minimum curvature methods, still represented a major obstacle to applications. Wessel and Bercovici [62] generalized the approach of Sandwell by including surface tension to the Green’s function expansion of the interpolating surface. In summary, for moderate amounts of data, the Green’s function technique is superior to conventional finite difference methods because both data values and directional gradients can be used to constrain the model surface. Also, noise can be suppressed easily by striving a least-squares fit rather than considering a strict interpolation, and the model can be estimated at arbitrary locations rather than only on a rectangular grid [62]. Moreover, including surface tension greatly improves the stability of the method relative to gridding without tension. Recently, Wessel presented a Green’s function based general purpose interpolator for both Cartesian and spherical surface data called Greenspline [61]. In this study we propose, for the first time, to join the Green’s function method with the BEMD technique thus eliminating the poor interpolation effects and reducing the computation time for each iteration. Most favorably, this interpolation technique only needs very few iterations for estimating each BIMF as it does not need to employ an ensemble. The proposed fast and stable GiT-BEMD method thus can be a good alternative providing an efficient BEMD processing. The organization of the paper is as follows: before introducing the novel concept of GiT-BEMD, the regular BEMD process is briefly summarized in Section 2 of this paper. The detailed description of the proposed GiT-BEMD algorithm is given in Section 3. Although the method of detecting local extrema suggested in GiT-BEMD is the same as in normal BEMD, it is explained in the first part of Section 3 to further understanding of the proposed envelope estimation technique, since it requires information about local extrema as its basic ingredients. The second part of Section 3 describes the new method of envelope estimation. Simulation results with various images comparing GiT-BEMD and canonical BEEMD are given in Section 4. Finally, a conclusion is drawn in Section 5. 2 Bidimensional empirical mode decomposition EMD or BEMD involves a sifting process that decomposes a signal into its IMFs or BIMFs and a residue based basically on the local frequency or oscillation information. The first IMF/BIMF contains the highest frequencies of local temporal or spatial oscillations, while the final IMF/BIMF contains the lowest frequencies of local oscillations and the residue resumes the trend of the signal/data. Like time frequency distributions with EMD, acquiring the spatial-frequency distribution of 2D data/image is possible with BEMD, which may be named as bidimensional HHT (BHHT). 2.1 General BEMD General BEMD is a sifting process that decomposes X(m, n) into multiple hierarchical components known as IMFs. A typical sifting process is summarized in the following iterations: 1. Initialization: set r ( m , n ) = X ( m , n ) . Identify all local maxima and local minima of r, i. e. all r(m, n). 2. Interpolate the local maxima to obtain an upper envelope surface emax (m, n), and local minima to obtain a lower envelope surface emin (m, n)). 3. The mean m ( m , n ) = [ e m a x ( m , n ) + e m i n ( m , n ) ] / 2 is computed and subtracted from r(m, n) to obtain r ′ ( m , n ) = r ( m , n ) − m ( m , n ) . 4. Update r(m, n) by r′(m, n). Repeat steps 1 to 3 until the stopping criterion is met. Besides this general BEMD, the canonical BEEMD as mentioned above will be summarized in the next section for later comparison (see [2] for details). 2.2 Canonical BEEMD EMD was developed from the assumption that from any signal locally simple oscillations can be extracted. The resulting component signals are called Intrinsic Mode Functions (IMF). Such IMFs are obtained from the signal by means of a sifting algorithm resulting locally in pure oscillations with zero mean. Amplitude and frequency of the IMFs may change over time. Furthermore, IMFs are ordered according to their frequency content. In contrary with wavelets, EMD is a data driven algorithm that decomposes the signal without prior knowledge. The decomposition of an image starts by applying EEMD to each column X *n ≡ x n of the M × N - dimensional data matrix X, where M denotes the number of samples and N gives the dimension of the data vectors. The 1D-EEMD decomposition of the nth column becomes (1) x n : = X * , n = ∑ j = 1 J C * , n ( j ) where the column vector C * , n ( J ) represents the residuum of the nth column vector of the data matrix. This finally results in J component matrices, each one containing the jth component of every column x n , n = 1 , … , N of the data matrix X. (2) C ( j ) = [ c 1 ( j ) c 2 ( j ) ⋯ c N ( j ) ] = [ C * , 1 ( j ) C * , 2 ( j ) ⋯ C * , N ( j ) ] Next one applies an EEMD to each row of Eq. (2) yielding (3) C m , * ( j ) = ( c m , 1 ( j ) c m , 2 ( j ) ⋯ c m , N ( j ) ) = ∑ k = 1 K ( h m , 1 ( j , k ) h m , 2 ( j , k ) ⋯ h m , N ( j , k ) ) = ∑ k = 1 K H m , * ( j , k ) where c m , n ( j ) = ∑ k = 1 K h m , n ( j , k ) represents the decomposition of the rows of matrix C (j). These components h m , n ( j , k ) can be arranged into a matrix H (j, k) according to (4) H ( j , k ) = [ h 1 , 1 ( j , k ) h 1 , 2 ( j , k ) ⋯ h 1 , N ( j , k ) h 2 , 1 ( j , k ) h 2 , 2 ( j , k ) ⋯ h 2 , N ( j , k ) ⋮ ⋮ ⋯ ⋮ h M , 1 ( j , k ) h M , 2 ( j , k ) ⋯ h M , N ( j , k ) ] The resulting component matrices have to be summed to obtain (5) C ( j ) = ∑ k = 1 K H ( j , k ) . Finally this yields the following decomposition of the original data matrix X (6) X = ∑ j = 1 J C ( j ) = ∑ j = 1 J ∑ k = 1 K H ( j , k ) where each element is given by (7) x m , n = ∑ j = 1 J ∑ k = 1 K h m , n ( j , k ) To yield meaningful results, components h m , n ( j , k ) with comparable scales, i.e. similar spatial frequencies of their textures, should finally be combined [67] according to the comparable minimal scale combination principle (CMSC). In practice, for two-dimensional data sets this implies that the components of each row, which represent a common horizontal scale, and the components of each column, which represent a common vertical scale, should be summed up [67]. Hence, the CMSC - principle leads to BIMFs given by (8) S ( k ′ ) = ∑ k = 1 K H ( k , k ′ ) + ∑ j = k ′ + 1 J H ( k ′ , j ) which thus yields a decomposition of the original data matrix X into BIMFs according to (9) X = ∑ k ′ = 1 K S ( k ′ ) where S (K) represents the non-oscillating residuum. The extracted BIMFs can be considered features of the data set which, according to the CMSC - principle, reveal local textures with characteristic spatial frequencies which help to further analysis. However, similar to most other BEMD techniques, also with canonical BEEMD the number of BIMFs and their characteristics are highly dependent on envelope estimation techniques in the sifting process, on the methods to detect extrema, and on stopping criteria during the iterations. The following section presents a new variant of BEMD, called fast and stable BEMD (GiT-BEMD), which is based on the Green’s function expansion of the interpolating surfaces containing all extremal values of the bi-dimensional data set. 3 Green’s function-based BEEMD With the intention of overcoming the difficulty in implementing BEMD via the application of surface interpolation, a novel approach is proposed here based on a representation of upper and lower surface envelopes via suitable Green’s functions for spline interpolation including surface tension. Introducing a tension parameter alleviates surface interpolation problems and greatly improves the stability of the method relative to gridding without tension. Based on the properties of the proposed approach, it is considered a fast and stable BEMD (GiT-BEMD). The latter thus differs from the canonical BEEMD algorithm basically in the process of robustly estimating the upper and lower surfaces, and in limiting the number of iterations per BIMF to a few iterations only. Hence, the GiT-BEMD is considered an algorithm of superior efficiency compared to other BEMD algorithms. The details of the extrema detection and surface formation of the GiT-BEMD process are discussed in the following section. 3.1 Extraction of local extrema Local extrema are points that have the largest or smallest pixel values relative to their K-connected neighbors, therefore in a 2D image the pixel with coordinates (x, y) has 8 connected neighbors with coordinates (x ± 1, y), (x, y ± 1), (x ± 1, y ± 1), (x ± 1, y∓1). The 2D region of local maxima is called a maxima map, and the 2D array of local minima is called a minima map, respectively. Like BEMD, a neighboring window method is employed to detect local extrema during intermediate steps of the sifting process for estimating a BIMF of any source image. In this method, a data point/pixel is considered as a local maximum (minimum), if its value is strictly higher (lower) than all of its neighbors. Let P = { P i | i = 1 , … . N } be a set of local minima (maxima) of an x × y- dimensional data matrix such that it exists a small (large) neighborhood around any such local optimal point Pi on which the pixel value is never larger (smaller) than f(xi, yi ) at Pi . Local extrema occur only at critical points. Let D ( x , y ) = f x x f y y − ( f x y ) 2 . If D > 0 at a critical point, then the critical point Pi is a local extremum. The signs of fxx and fyy determine whether the point is a maximum or a minimum. If D ≤ 0 at a critical point, then the point Pi is a saddle point. Though, in practice, a 3 × 3 window results in an optimal extrema map for a given 2D image for many applications, sometimes a larger window size is suitable especially with large scale images. 3.2 Green’s function for estimating envelopes In BEMD, spline interpolation is basically used to find the smoothest surface passing through a grid of irregularly spaced extrema, either maxima or minima. In this work it is proposed to employ Green’s functions, deduced from proper data constraints, to expand the interpolating surface under tension. Thus the envelope surfaces connecting local extrema in 2D space are determined to minimize the curvature of the surface in the presence of surface tension [62]. Interpolation with Green’s functions implies that the points of the interpolating envelope surface can be expressed as (10) s ( x u ) = ∑ n = 1 N w n Φ ( x u , x n ) where x u denotes any point where the surface is unknown, x n represents the nth data constraint, Φ(x u , x n ) is the Green’s function and wn is the respective weight in the envelope representation. Several works discuss the use of Green’s functions in interpolation problems (see for example [61]). Following we summarize the basics of the Green’s function method for spline interpolation using surface tension. It has been shown by Sandwell [51] that the Green’s function Φ(x) obeys the following relation at any data constraint x n , n = 1 , … , N (11) [ D Δ o p 2 − T Δ o p ] Φ ( x u , x n ) = δ ( x u − x n ) where, Δ o p 2 and Δ o p = ∇ 2 , denote the bi-harmonic, the Laplace and the Nabla operator, respectively, D is the flexural rigidity of the curve or surface, T is the tension used at the boundaries, and Φ(x u , x n ) represents the Green’s function containing the spatial position vectors x u , x n as argument. With vanishing surface-tension, i. e. T → 0, the minimum curvature solution Φ ( x u , x n ) = x u n 2 log ( x u n ) , x u n = | x u − x n | is achieved [51]; while in case of a vanishing surface rigidity, i. e. D → 0, the solution approaches Φ ( x u , x n ) = log ( x u n ) . The general solution is expected to retain these limiting characteristics. To obtain the former, rewrite Eq. (11) in terms of the curvature Ψ ( x ) = ∇ 2 Φ ( x ) of the Green’s function and transform it to the conjugate Fourier domain where it then reads (12) [ Δ o p + p 2 k 2 ] Ψ ( k ) = − 1 T p 2 k 2 . Here k = | k | represents the radial wavenumber, p 2 = T D , k denotes the wavenumber vector and Ψ(k) represents the Fourier transform of Ψ(x). In Fourier space, the solution is obtained as (13) Ψ ( k ) = − 1 T ( p 2 k 2 + p 2 ) From this the general solution of Eq. (11) in a 2-D spacial domain [62] can be achieved by using the inverse Hankel transform as (14) Ψ ( x ) = − 1 T ∫ 0 ∞ p 2 k 2 + p 2 J 0 ( k x ) k d k = − 1 T p 2 K 0 ( p x ) where K 0 denotes the modified Bessel function of the second kind and order zero given by (15) K 0 ( p x ) = ∫ 0 ∞ cos ( k p x ) d k k 2 + p 2 ∝ { exp ( − p x ) if p x → ∞ − log ( p x ) if p x → 0 Integrating Ψ(x) twice and rescaling, finally, yields the Green’s function Φ(x) and its local gradient ∇Φ(x) as (16) Φ ( x u , x n ) = l o g ( p | x u − x n | ) + K 0 ( p | x u − x n | ) = log ( p x u n ) + K 0 ( p x u n ) (17) ∇ Φ ( x u , x n ) = p · [ 1 p x u n − K 1 ( p x u n ) ] · ( x u − x n ) | x u − x n | = p · [ 1 p x u n − K 1 ( p x u n ) ] · e u n where p ∝ T represents the tension parameter, |....| denotes the Euclidean distance, K 0(.) represents the modified Bessel function and e un denotes the unit vector pointing along the direction x u − x n . Hence, by decreasing the tension parameter p∝T, the solution is expected to reach the minimum curvature solution represented by the bi-harmonic Green’s function [51]. In contrary, increasing the tension parameter T, thus also p, renders the arguments of Φ(x) large and leads to an interpolating surface dominated by tension. Thus varying the tension p achieves a continuous spectrum of Green’s functions reflecting the trade-off between the minimum curvature solution driven by the log (px) term and the impact of the surface tension via the modified Bessel function K 0(px). Finding an optimal tension parameter is still an open problem. If intrinsic data modes are to be used for classification purposes, an optimal tension parameter could be identified as the one achieving maximal classification accuracy. This, however, is left for a subsequent study and is not pursued further here. Including N data constraints yields for the defining equation and its solution [51] (18) Δ o p [ Δ o p − p 2 ] c ( x u ) = ∑ n = 1 N w n δ ( x u − x n ) c ( x u ) = ∑ n = 1 N w n Φ ( x u − x n ) The coefficients wn can be obtained by solving the system of linear equations G w = c where the Green’s matrix G collects all Green’s functions Φ ( x m − x n ) at the data constraints m , n = 1 , … , N . Corresponding slopes sm in directions n ^ m can be obtained by evaluating the relations s m = ∑ m = 1 N w m ∇ Φ ( x m − x n ) · n ^ m m = 1 , … , N . In summary, the interpolation procedure is based on two steps: the first step estimates the weights w = [ w 1 w 2 … w P ] and the second step estimates the interpolating envelope surface: • The surface values s ( x n ) = [ s ( x 1 , … , s ( x N ] T ≡ c = [ c 1 , c 2 , … , c N ] T are known in a total of N locations x n = [ x n , y n ] T , then using the interpolation Eq. (10) for each of the known points a linear system with N equations is obtained G w = c where the nth row of matrix G is the evaluation of the Green’s function Φ ( x n , x m ) , m = 1 , 2 … N . Therefore solving for the weights w = G − 1 c . • Using the weights w, the value s(x u ) ≡ cu of the envelope surface can be estimated at any point x u by solving Eq. (10), which can be re-written as (19) c u = w T Φ where the vector Φ = [ Φ ( x u , x 1 ) Φ ( x u , x 2 ) … Φ ( x u , x N ) ] T contains the Green’s function values of all distances between the N data constraints and the considered location. 4 Results and discussion Artificial images with textures as well as real-world images (face images, functional biomedical images) have been considered to test and validate the proposed approach. Images are selected to demonstrate efficiency and performance of the GiT-BEMD algorithm in extracting textures on various spatial scales from the different images. In addition, we provide a comparison of the performance of both algorithms, GiT-BEMD and BEEMD. 4.1 Artificial image An artificial texture image (ATI) of size 101 × 101 is considered, which is composed as a superposition of artificial texture component (ATCs) images of the same size. The ATCs represent harmonically varying spatial oscillations (sinusoids) with horizontal ( h 1 = 20 , h 2 = 4 , h 3 = 1 ) and vertical ( v 1 = 20 , v 2 = 4 , v 3 = 1 ) spatial frequencies and unit amplitudes. The first ATC contains the highest spatial frequency, the second ATC shows a medium spatial frequency, and the third ATC exhibits a very low spatial frequency. The ATI and ATCs are shown in Fig. 1 , while the intensity profiles of the ATI and ATCs are presented in Fig. 3. These artificial textures provide a good performance indicator of the algorithm even though some imperfections cannot be avoided. The latter arise from the fact that though the addition of the ATCs in Fig. 1 reproduces the original ATI of Fig. 3, application of either canonical BEMD or the newly proposed GiT-BEMD to the ATI might yield BIMFs which not necessarily reproduce the ATCs of Fig. 3 perfectly. This is due to the fact, that extracted BIMFs only represent approximations to the true intrinsic modes [68]. Let us first investigate the upper (UE) and lower (LE) envelope surfaces generated from the ATI by GiT-BEMD during the sifting process while extracting the first intrinsic mode (BIMF1). Due to repeated tests with the artificial image set, 5 iterations usually suffice to effectively build envelope surfaces with GiT-BEMD employing a proper tension parameter. The computational load of the proposed scheme of Green’s function-based envelope estimation is given in the Table 1 concerning the decomposition of the ATI set. There the computational load is compared with corresponding computational costs of other BEMD variants. These results clearly show that the envelope estimation takes much less time (nearly ten-fold) with GiT-BEMD than with canonical BEMD. In general, increasing the size E of the ensemble increases computational costs for the decomposition accordingly, both in case of a canonical BEMD but also for estimating the BIMFs in the GiT-BEEMD process. However, the number of extremal points decreases for BIMFs with lower spatial frequencies, leading to a decrease in computation time for envelope estimation. Anyway, the overall computational load of the canonical BEMD process still remains much higher compared to GiT-BEMD. Next we compare the decomposition of the ATI of Fig. 1 when effected with either the new GiT-BEMD or a canonical BEMD. Let us first consider the application of the GiT-BEMD algorithm. The resulting BIMFs are displayed in Fig. 2 . The latter reveal the close similarity of the BIMFs with the original ATCs. The corresponding intensity profiles are displayed in Fig. 3. Remember that the multi-dimensional EMD relies on the comparable-minimal-scale principle in determining the intrinsic modes. Hence harmonics with the same spatial frequency along orthogonal directions become combined to one intrinsic mode and will not be separated by the algorithm. In any case, the BIMFs may not be expected to match the original ATCs perfectly because of truncation/rounding errors introduced at various steps of the decomposition process. This holds true for both, the canonical BEMD and the GiT-BEMD. Still such errors remain very small as is corroborated with Fig. 2 where the reconstructed ATI represents an almost perfect copy of the original ATI. The intrinsic modes (BIMFs), as displayed in Fig. 2, were extracted from the original ATI by applying GiT-BEMD with a surface tension parameter T = 0.1 . Fig. 4 , instead, presents corresponding results for several surface tension parameters spanning the range of dominance of the log (xun ) term (corresponding to small tension parameters) to conditions where the modified Bessel function dominates the behavior of the resulting Green’s function (corresponding to large tension parameters). Increasing the tension parameter results in sharper contrast boarders which, however, in the end induces artificial intensity structures in BIMFs with low spatial frequencies, which stem from undue oscillations (over- or undershoots) and do not correspond to the underlying original modes. Thus the optimal surface tension needs to be balanced carefully to avoid such spurious structures. This will be a major goal for future investigations. The ATI is next decomposed using the canonical BEMD to purposely compare the results with those obtained from the new GiT-BEMD method. The BIMFs resulting with canonical BEMD are displayed in Fig. 5 . The quality of the extracted modes is clearly very bad due to the artifacts introduced during sifting. Remember that only this plain variant of BEMD showed comparable computational costs when compared with the new GiT-BEMD. Clearly, the latter is by far the better alternative. Similarly, the BIMFs resulting from an application of the canonical BEEMD with ensemble sizes of either E = 20 and E = 50 to the original ATI are illustrated in Fig. 5-Top and Fig. 5-Bottom, respectively. In all cases, three BIMFs were extracted. As one can see, the quality of the extracted BIMFs turns better with increasing size of the ensemble of the noise-assisted BEEMD. This is because of the self-compensating property of noise which helps to even cancel the noise included in the original images. However, the grain of salt concerns the exponentially increasing computation time when the ensemble grows larger. When applying the canonical BEMD, the quality of the resulting BIMFs can also be improved by increasing the number of sifting steps. On the one hand this yields better BIMFs in cases where the modes have high spatial frequencies but leads to over-sifting when modes have low spatial frequencies. Hence, it is generally advisable to limit the number of sifting steps, even if the standard deviation(SD) threshold stopping criterion is not met yet. Alternatively, one might wish to decrease the number of sifting iterations for the lower spatial frequency modes to prevent over-sifting [13,53]. As an additional performance assessment of the GiT-BEMD and BEEMD algorithms, the computational load for the decomposition of the ATI image is presented in Table 1. The algorithm GiT-BEMD, using a tension parameter T ≤ 0.1, is to be preferred clearly for decomposing the ATI image. Note that with the GiT-BEMD decomposition, using a small surface tension parameter, the number of sifting steps has been fixed to N s = 10 , while in the canonical BEEMD application, the condition for a vanishing mean everywhere had to be met as closely as possible irrespective of the number of sifting steps needed. This observation has been established already in literature [13,53]. Thus relaxing this condition in case of the canonical BEEMD algorithm, by reducing the number of sifting steps in order to prevent over-sifting, may compromise achieving optimal intrinsic modes. The GiT-BEMD algorithm, instead, only needs few sifting steps and does not yield better results if the number of steps is increased further. This of course helps to keep the computational load low. In fact, the condition of a vanishing local mean everywhere is seen to be met very closely in the simulations (the global mean for BIMF1 is − 0.00004 , for BIMF2 is − 0.0001 and for the BIMF3 is 0.0004). Thus all performance measures are in favor of the new GiT-BEMD algorithm to be used for image decomposition. 4.2 Face image In Fig. 7 , the famous Lena image is decomposed into intrinsic modes, thus providing characteristic spatial textures. This image is often used as a benchmark in image processing applications. The size of the image is 111 × 111 pixels. Different variants of EMD, including pseudo-2D EMD, canonical bi-dimensional EMD (BEMD), canonical bi-dimensional ensemble EMD (BEEMD) with an ensemble size E = 20 , the Green’s function-based GiT-FBEMD with small surface tension ( T = 0.1 ) , GiT-BEEMD with small surface tension ( T = 0.1 ) and an ensemble size E = 2 and GiT-BEEMD with T = 0.1 and E = 20 , have been applied for comparison and results are illustrated in Fig. 7. In all cases, the Lena image has been decomposed into four intrinsic modes (BIMFs) and a non-oscillating residue. Looking at the decomposition of the face image, it becomes obvious that the GiT-BEMD approach yields very well-defined BIMFs, which represent characteristic image textures ( features) at various spatial scales. These intrinsic modes have a much higher quality with much better defined textures and contrast edges than those obtained with pseudo-2D EMD, and still better or similar to the BIMFs resulting from the application of the canonical BEMD method. Disturbing stripes and other artifacts are often seen in the BIMFs when obtained via the canonical BEMD algorithm as is apparent in the Fig. 7. Such artifacts provide an extra challenge to a further processing of the extracted image modes and might render them even unsuited. On the contrary, as the application to artificial textures already demonstrated, GiT-BEMD is able to suppress most of these image distortions arising during the decomposition process (see Fig. 7). Preliminary studies on edge detection and noise removal using GiT-BEMD show promising results and a significantly better performance of GiT-BEMD compared to the analysis using canonical BEMD. Thus, the newly proposed algorithm GiT-BEMD provides a distinctly improved quality of the BIMFs extracted from the Lena image. The resulting intrinsic image modes show no or only few distortions, and reveal texture-related contrast edges and other characteristic features at various intrinsic spatial scales better than what could be achieved with applying other EMD variants. But, as Fig. 4 demonstrates, the effect of a large tension parameter, p → ∞, results in noisy textures because of under- and/or over-shooting problems, especially at low spatial frequencies. Thus a decomposition, employing Green’s functions with a moderately large tension parameter, is advisable to avoid unwanted oscillations of the interpolating surface envelope during sifting. This is especially true for modes with low spatial frequencies where the number of extrema turns small [14,15]. In fact, yet no solution is known to the problem of how to determine an optimal tension parameter, hence some trial and error is still unavoidable. 4.3 Biomedical image Finally, we test our new algorithm with biomedical images. Texture analysis is considered as a challenging task for the latter images. The ability to effectively and reliably extract textures from such images for classification and segmentation purposes is of key importance not only in medical image analysis but also in scene analysis and many other application areas. Here we apply GiT-BEMD to functional magnetic resonance images (fMRI). These images were already thoroughly analyzed in [2] within the framework of a canonical BEEMD analysis. With these images the performance of the GiT-BEMD algorithm can be tested in detail, as such images notoriously exhibit a low signal-to-noise ratio. Also their decomposition easily suffers from mode mixing and boundary artifacts. Because of this, most BEMD variants have problems to decompose them properly. Though canonical BEEMD could be shown to do well, it takes a prohibitively long time to compute the BIMFs, especially when larger ensembles are employed. Despite all efforts, stripe-like artifacts remained which were hard to get rid of with this technique. So, further post-processing the BIMFs was needed, for example by applying a proper filtering or any other technique able to deal with these artifacts. For this reason, applying GiT-BEMD directly will produce poor BIMFs also. However, it is well known that mode mixing as well as boundary artifacts can be avoided during sifting by employing a noise-assisted ensemble EMD (EEMD). Thus, GiT-BEEMD, applied with an ensemble size E = 2 only, produces proper BIMFs which exhibit a quality similar to the BIMFs resulting from a canonical BEEMD with an ensemble size E = 20 . Increasing the size of the ensemble to E = 20 in case of GiT-BEEMD yielded marginal improvements only of the BIMFs but boosted the computational load prohibitively. Here, we have to mention that a noise amplitude a n = 0.2 · σ was applied for all ensembles used in this study. Fig. 8 shows the BIMFs extracted by GiT-BEMD, GiT-BEEMD with E = 2 and GiT-BEEMD with E = 20 . Thus good quality BIMFs can be extracted at low computational costs when using GiT-BEEMD with an ensemble size as small as E = 2 . Also, compared with canonical BEEMD, the quality of the intrinsic modes extracted with GiT-BEEMD with E = 2 seems considerably better, and, as Table 1 shows, the computational costs are much lower. In summary, the newly proposed algorithm GiT-BEMD provides image modes with less distortions and better defined texture contours easing further processing of the component images. Essentially two properties render the GiT-BEMD algorithm superior to its predecessors: • Speed of computation: Most importantly, a dramatic improvement in computation time results from combining the Green’s function - based surface envelop estimation with the EMD decomposition. This is especially important in real world applications where a canonical BEEMD may take hours to be completed. Moreover, envelope estimation in GiT-BEMD, employing Green’s functions with surface tension, guaranties that the envelopes closely follow the image, i. e. undue oscillations are avoided. Also, GiT-BEMD is inherently free from boundary effects, and thus it does not require additional boundary processing. On the contrary, envelope estimation in the canonical BEMD method, employing surface interpolation, is highly dependent on the maxima or minima maps, and the envelopes are not guaranteed to follow the image. In some cases when there are only very few data points in the maxima or minima maps, BEMD tends to generate an inaccurate surface, which in turn leads to boundary and saddle point problems, and thus produces improper BIMFs. In Table 1, the time consumption for a BEMD-based decomposition is compared to the time consumption of a GiT-BEMD-based decomposition of three textures images. While GiT-BEMD often takes only seconds, and at most a few minutes, a canonical BEMD takes many hours, even if only very few iterations are performed per BIMF. This severely hampers the application of canonical BEMD in many practical applications. Note, the experimental computer is Intel(R) Core(TM)2 Quad CPU Q9650 @ 3.00GHz RAM 8GHz; and the platform is MATLAB R2014b (Version 8.4). • Stability: A proper tuning of the tension parameter provides increased stability to the GiT-BEMD process. A trial and error selection procedure may have to be performed to find a suitable value for the surface tension parameter while extracting the textures inherent to the image under investigation. But we noticed that setting the surface tension to a large value in case of modes with a high inherent spatial frequency, and decreasing the surface tension in proportion to the decrease in the inherent spatial frequency, i.e. according to t k + 1 = t k − 1 K , leads to a better decomposition of images. This reduction in surface tension avoids to have blob-like artifacts in low frequency modes which otherwise show up if a high tension parameter is used, see Fig. 10 . Also the GiT-BEMD algorithm can be extended simply to a noise-assisted variant GiT-BEEMD thus avoiding mode mixing problems in applications which suffer from this problem. Notably, GiT-BEEMD does not need more than two members in the ensemble to cope with mode mixing. In contrast, other BEMD algorithms such as canonical BEMD need at least 20 members in the ensemble to alleviate mode mixing problems. However, GiT-BEEMD suffers from on-board memory limitations when dealing with large images. A work-around this drawback can be achieved in future studies by extending GiT-BEEMD to a sliding window GiT-BEEMD similar to the one-dimensional WSEMD algorithm [16,72]. 5 Conclusion Bidimensional ensemble empirical mode decomposition (BEEMD) represents a fully unsupervised and data driven technique that permits analyzing non-linear and non-stationary data sets such as images. BEEMD encompasses a sifting process for estimating the intrinsic modes into which the data can be decomposed. In most of the existing implementations of BEEMD, the sifting process affords the frequent estimation of upper and lower envelope surfaces interpolating extremal data points. The computation of these envelope surfaces represents a bottleneck in existing BEEMD variants because of its high computational load – the computation time grows exponentially with the data dimension [49] – and the accompanying artifacts, like boundary artifacts and undue oscillations in high local gradient areas, which often corrupt the extracted modes. In [3] it has been proposed to circumvent such difficulties by way of employing a Radon transform of numerous image projections and decompose the resulting uni- dimensional signals by standard EMD. However, no precautions have been taken to avoid mode-mixing and aliasing. Furthermore, such data analysis neglects existing spatial correlations within the images. In a recent study we applied BEEMD to a set of functional magnetic resonance images (fMRI) recorded during a contour integration task [2]. The study encompassed 19 subject, 3 sessions per subject, on average 40 trials per session and roughly 50 slices per brain volume with 72 × 94 voxels per slice. Using BEEMD, the computational load was prohibitive for a single trial analysis. Hence the study focused on data obtained as averages over trials. This experience motivated the formulation of a fast and stable BEMD variant (GiT-BEMD) described in this paper. In the GiT-BEMD algorithm, the envelope estimation, as applied in a canonical BEMD, is replaced by a 2D surface interpolation based on Green’s functions with tension. This way, the computational load is dramatically reduced, and the quality of the extracted modes is substantially improved as well. In addition, including an optional tension parameter during Green’s function-based spline interpolation renders the process much more stable than if gridding without tension is used. Remember that the tension parameter controls the trade-off between the two contributions to the Green’s function in Eq. (17) where for large tension parameters the modified Bessel function predominates. As it is still an open problem how to estimate an optimal tension parameter for any given surface interpolation problem, we provided a screening of this parameter for a visual recognition of the induced differences in the resulting BIMFs. But, given our experience, we can strongly recommend a small value for the tension parameter in case of extracting a low-frequency mode. To the contrary, a large value for the tension parameter should be chosen in case of extracting a high-frequency mode. Furthermore, we also showed that GiT-BEMD does not need more than a few iterations, roughly Ns ≈ 5, during the sifting process to extract proper BIMFs. Also we demonstrated that an ensemble of size E = 2 suffices whereby the assisting noise is added and subtracted before averaging. Hence any ensemble technique, encompassing large ensembles with concomitant exponentially growing computation time, can be avoided largely. This also contributes to a substantial improvement of the algorithm in terms of computational load. Besides reducing the latter, GiT-BEMD achieves a higher quality of the estimated BIMFs as can be seen from a direct comparison of the results obtained with BEEMD and GiT-BEMD. Finally, extending GiT-BEMD to GiT-BEEMD is very simple, though hardly ever necessary. We have shown experimental results for both real and artificial images, thus demonstrating that GiT-BEMD can be applied to any image data set. However, with large images the Green’s function - based EMD encounters problems with on-board memory limitations much as it happens to any other BEMD algorithm as well. This handicap provides an additional motivation to study sliding window techniques like an extension of the wsEMD to bidimensional data sets. Future research should focus on elucidating several aspects of a Green’s function-based surface envelope estimation during sifting. An urgent problem to be solved concerns proper methods to estimate an optimal, problem dependent surface tension parameter T. There is yet no mathematically rigorous way known to achieve this goal. A possible heuristic could be to convert the mode decomposition problem into an optimization problem. This can be achieved easily if the extracted intrinsic modes undergo further post-processing to extract proper features which need to be clustered and/or classified. In this case any decision function will depend on the tension parameter which could be optimized via maximizing classification accuracy or optimizing any other proper statistic. Another urgent problem to solve concerns memory limitations when large images need to be decomposed. One work-around would be to reduce image dimensionality employing known exploratory matrix decomposition techniques like principal (PCA) or independent component analysis (ICA) or non-negative matrix factorization (NMF). Such techniques have already been used as feature extraction techniques when applied to the intrinsic modes to reduce their dimensionality for later classification. A more elegant work-around could be to extend the weighted sliding EMD, developed for one-dimensional time series data, to bidimensional data sets. Such an approach would need to optimize window size, step parameter and weighting function. As the use of Green’s functions under tension for envelope surface interpolation during sifting within an EMD algorithm has been proposed for the first time no systematic study exists yet which investigates the impact of the new method onto mode-mixing and mode-splitting effects. Obviously, including a noise-assisted ensemble technique should be advantageous, but it will fade away the savings in computation time achieved with GiT-BEMD. Given our experience that an ensemble of size E = 2 suffices, if the noise contribution is added and subtracted, artifacts like mode-mixing or -splitting, aliasing, boundary effects etc. seem to be manageable with little effort using GiT-BEMD. However, a systematic study of such effects is still missing. Finally, to objectively compare GiT-BEMD with results obtained by other BEMD variants, methods for quantitative comparisons need to be implemented. This search for similarity could be based on proper distance metrics but also on entropic or information-theoretic measures. Altogether, we believe that GiT-BEMD offers a highly competitive alternative to existing BEMD algorithms and represents a promising technique for blindly decomposing images and extracting textures thereof which may be used for image classification. Acknowledgment Financial support by the DAAD, Acciones Integradas Hispano - Alemanas is gratefully acknowledged. References [1] S. Abadan A. Shabri Hybrid empirical mode decomposition-arima for forecasting price of rice Applied Mathematical Sciences 8 63 2014 3133 3143 [2] S. Al-Baddai K. Al-Subari A. Tomé G. Volberg S. Hanslmayr R. Hammwöhner E. Lang Bidimensional ensemble empirical mode decomposition of functionalbiomedical images taken during a contour integration task Biomedical Signal Processing and Control 13 2014 218 236 [3] H. Ali M. Hariharan S. Yaacob A.H. Adom Facial emotion recognition using empirical mode decomposition Expert Systems with Applications 42 2015 1261 1277 [4] M.U. Altaf T. Gautama T. Tanaka D.P. Mandic Rotation invariant complex empirical mode decomposition Proc. IEEE International Conference on Acoustics, Speech, Signal Processing 2007 1009 1012 [5] A. Arafat T. Hasan Automatic detection of ecg wave boundaries using empirical mode decomposition Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on 2009 461 464 [6] S. Bhuiyan R. Adhami J. Khan A novel approach of fast and adaptive bideimensional empirical mode decomposition IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2008 [7] S. Bhuiyan J. Khan N.A.-O. R.R. Adhami Study of Bidimensional Empirical Mode Decomposition method for various radial basis function surface interpolators 2009 International Conference on Machine Learning and Applications 2009 18 24 [8] N. Bouguila T. Elguebaly A fully bayesian model based on reversible jump mcmc and finite beta mixtures for clustering Expert Systems with Applications 39 2012 5946 5959 [9] I.C. Briggs Machine contouring using minimum curvature Geophysics 39 1 1974 39 48 [10] A.T. Celebi S. Ertürk Visual enhancement of underwater images using Empirical Mode Decomposition Expert Systems with Applications 39 1 2012 800 805 [11] L.-W. Chang M.-T. Lo N. Anssari K.-H. Hsu N.E. Huang W.W. Hwu Parallel implementation of multi-dimensional ensemble empirical mode decomposition Proc. ICASSP 2011 IEEE 1621 1624 [12] Q. Chen N.E. Huang S. Riemenschneider Y. Xu A b-spline approach for empirical mode decomposition. Adv. Comput. Math. 24 2006 171 195 [13] C. Damerval S. Meignen V. Perrier A fast algorithm for bidimensional emd IEEE Signal Processing Letters 12 10 2005 701 704 [14] S.G. Deng W. Li Realization of smoothing curve with tension spline interpolation under Visual C++ Chin J Eng Geophys 2 5 2005 387 390 [15] X. Deng Z. Tang Moving surface spline interpolation based on Green’s function Math. Geosci. 43 2011 663 680 [16] R. Faltermeier A. Zeiler A.M. Tomé A. Brawanski E.W. Lang Weighted sliding Empirical Mode Decomposition Adv. Adapt. Data Analysis 3 4 2011 509 [17] J. Fleureau A. Kachenoura L. Albera J. Nunes L. Senhadji Multivariate empirical mode decomposition and application to multichannel filtering Signal Processing 91 12 2011 2783 2792 [18] J. Fleureau J.-C. Nunes A. Kachenoura L. Albera L. Senhadji Turning tangent empirical mode decomposition: A framework for mono- and multivariate signals IEEE Transactions on Signal Processing 59 3 2011 1309 1316 [19] A. Gallix J.M. Górriz J. Ramírez I.A. Illán E.W. Lang On the empirical mode decomposition applied to the analysis of brain SPECT images Expert Systems with Applications 39 18 2012 13451 13461 [20] A. Hasimah H. Muthusamy Y. Sazali A.H. Adom Facial emotion recognition using empirical mode decomposition Expert Systems with Applications 42 3 2015 1261 1277 [21] N.E. Huang Z. Shen S.R. Long M.L. Wu H.H. Shih Q. Zheng N.C. Yen C.C. Tung H.H. Liu The empirical mode decomposition and Hilbert spectrum for nonlinear and nonstationary time series analysis Proc. Roy. Soc. London A 8 454 1998 903 995 [22] N.E. Huang M.-L.C. Wu S. Long S.S. Shen W. Qu P. Gloersen K.L. Fan A confidence limit for the empirical mode decomposition and hilbert spectral analysis Proc. Roy. Soc. London A 459 2003 2317 2345 [23] G. Jager R. Koch A. Kunoth R. Pabel Fast empirical mode decompositions of multivariate data based on adaptive spline-wavelets and a generalization of the hilbert-huang-transformation (hht) to arbitrary space dimensions Advances in Adaptive Data Analysis 2010 [24] M. Koh Undecimated non-uniform multivariate empirical mode decomposition filter banks for arbitrary nodes and its application for speech enhancement Advanced Science and Technology Letters (Signal Processing) 58 2014 95 98 [25] E.W. Lang, R. Schachtner, D. Lutter, D. Herold, A. Kodewitz, F. Blöchl, F.J. Theis, I.R. Keck, J.M.G. Sáez, P.G. Vilda, A.M. Tomé, Exploratory Matrix Factorization Techniques for Large Scale Biomedical Data Sets, Bentham Science Publishers, doi:10.2174/97816080521891110101010026. [26] Y. Lei Z. He Y. Zi EEMD method and WNN for fault diagnosis of locomotive roller bearings Expert Systems with Applications 38 6 2011 7334 7341 [27] K.-P. Li Z.-Y. Gao X.-M. Zhao Multiple scale analysis of complex networks using the empirical mode decomposition method Physica A 387 2008 2981 2986 [28] A. Linderhed 2-D empirical mode decompositions in the spirit of image compression Wavelet and Independent Component Analysis Applications IX, Proceedings of SPIE Vol. 4738 2002 1 8 [29] Z. Liu S. Peng Boundary processing of bidimensional emd using texture synthesis IEEE Signal Processing Letters 12 2005 33 36 [30] Z. Liu H. Wang S. Peng Texture classification through directional empirical mode decomposition Proc. 17th IEEE International Conference on Pattern Recognition (ICPR ’04) 2004 803 806 [31] Z. Liu H. Wang S. Peng Texture segmentation using directional empirical mode decomposition Proceedings of IEEE International Conference on Image Processing (ICIP ’04) 2004 279 282 [32] S.R. Long, Applications of HHT in image analysis, World Scientific and River Edge and NJ and USA, pp. 289–305. [33] D. Looney D.P. Mandic Multi-scale image fusion using complex extensions of em IEEE Transactions in Signal Processing 57 4 2009 1626 1630 [34] Y. Lu E. Oruklu J. Saniie Chirplet signal and empirical mode decompositions of ultrasonic signals for echo detection and estimation Journal of Signal and Information Processing 4 2013 149 157 [35] M. Mohebbi H. Ghassemian Predicting termination of paroxysmal atrial fibrillation using empirical mode decomposition of the atrial activity and statistical features of the heart rate variability Med. Biol. Engineering and Computing 52 5 2014 415 427 [36] J. Nunes Y. Bouaoune E. Delechelle O. Niang P. Bunel Image analysis by bidimensional empirical mode decomposition Image Vis. Comput. 21 12 2003 [37] J. Nunes E. Deléchelle Empirical mode decomposition: Applications on signal and image processing Advances in Adaptive Data Analysis 1 2009 125 175 [38] J. Nunes S. Guyot E. Deléchelle Texture analysis based on local analysis of the bidimensional empirical mode decomposition Machine Vision and Applications 16 2005 177 188 [39] R.B. Pachori P. Avinash K. Shashank R. Sharma U.R. Acharya Application of empirical mode decomposition for analysis of normal and diabetic rr-interval signals Expert Systems with Applications 42 2015 4567 4581 [40] C. Park D. Looney P. Kidmose M. Ungstrup P. Mandic Time-frequency analysis of eeg asymmetry using bivariate empirical mode decomposition Neural Systems and Rehabilitation Engineering, IEEE Transactions on 19 4 2011 366 373 [41] R. Prashanth S.D. Roy P.K. Mandal S. Ghosh Automatic classification and prediction models for early parkinson’s disease diagnosis from spect imaging Expert Sytem with Applications 41 2014 3333 3342 [42] N. Rehman D.P. Mandic Qualitative analysis of rotational modes within three dimensional empirical mode decomposition Proc. ICASSP 2009 IEEE 3449 3452 [43] N. Rehman D.P. Mandic Empirical mode decomposition for trivariate signals IEEE Trans. Signal processing 58 3 2010 1059 1068 [44] N. Rehman D.P. Mandic Multivariate empirical mode decomposition Proceedings of the Royal Society A 466 2010 1291 1302 [45] N. Rehman D.P. Mandic Quadrivariate empirical mode decomposition International Joint Conference on Neural Networks (IJCNN 2010) 2010 1 7 [46] N. Rehman C. Park N.E. Huang D.P. Mandic Emd via memd: Multivariate noise-aided computation of standard emd Advances Adaptive Data Analysis 5 2 2013 25 1350007 [47] P. Ren S. Yao J. Li P.A. Valdes-Sosa K.M. Kendrick Improved prediction of preterm delivery using empirical mode decomposition analysis of uterine electromyography signals PLOSone 2015 [48] G. Rilling P. F1andrin P. Goncalves J.M. Lilly Bivariate empirical mode decomposition IEEE Signal Processing Letter 14 2007 936 939 [49] A. Rojas J.M. Górriz J. Ramírez I.A. Illán F.J. Martínez-Murcia A. Ortiz M.G. Rio M. Moreno-Caballero Application of Empirical Mode Decomposition (EMD) on DaTSCAN SPECT images to explore Parkinson disease Expert Systems with Applications 40 7 2013 2756 2766 [50] T.M. Rutkowski D.P. Mandic A. Cichocki A.W. Przybyszewski Emd approach to multichannel eeg data - the amplitude and phase components clustering analysis Journal of Circuits, Systems, and Computers 19 1 2010 215 229 [51] D.T. Sandwell Biharmonic spline interpolation of Geos-3 and Seasat altimeter data Geophysics Research Lett. 14 2 1987 139 142 [52] R. Sharma R.B. Pachori Classification of epileptic seizures in eeg signals based on phase space representation of intrinsic mode functions Expert Systems with Applications 42 3 2015 1106 1117 [53] M. Shen H. tang B. Li The modified bidimensional empirical mode decomposition for image denoising IEEE International Conference on International Conference on Signal Processing ICSP’6 vol. 4 2007 [54] M. Shen H. tang B. Li A novel boundary extension approach for empirical mode decomposition Proceedings of the International Conference on Intelligent Computing (ICIC’6) vol. 4 2007 299 304 [55] W.H.F. Smith P. Wessel Gridding with continuous curvature splines in tension Geophysics 55 3 1990 293 305 [56] T. Tanaka D.P. Mandic Complex empirical mode decomposition IEEE Signal Processing Letters 14 2 2006 101 104 [57] G. Wang X.-Y. Chen F.-L. Qiao Z. Wu N.E. Huang On intrinsic mode function Advances in Adaptive Data Analysis 2 3 2010 277 293 [58] J.-L. Wang Z.-J. Li What about the asymptotic behavior of the intrinsic mode functions as the sifting times tend to infinity? Advances Adaptive Data Analysis 4 1 & 2 2012 17 1250008 [59] Y. Washizawa T. Tanaka D.P. Mandic A. Cichocki A flexible method for envelope estimation in empirical mode decomposition Proceedings of the 10th International Conference on Knowledge-Based Intelligent Information and Engineering Systems, (KES’06), B. Gabrys, R. J. Howlett, and L. C. Jain, Eds. vol. 4 2006 1248 1255 [60] E.J. Wegman I.W. Wright Splines in statistics J. Am. Stat. Assoc. 78 382 1983 351 365 [61] P. Wessel A general-purpose green’s function-based interpolator Computers & Geosciences 35 2009 1247 1254 [62] P. Wessel D. Bercovici Interpolation with splines in tension: A green’s function approach Mathematical Geology 30 1 1998 77 93 [63] P. Wessel W.H.F. Smith Free software helps map and display data EOS Trans. AGU 72 41 1991 441 446 [64] P. Wessel W.H.F. Smith New version of the generic mapping tools released EOS Trans. AGU 76 33 1995 329 [65] J.-D. Wu Y.-J. Tsai Speaker identification system using empirical mode decomposition and an artificial neural network Expert Systems with Applications 38 5 2011 6112 6117 [66] Z. Wu N.E. Huang Ensemble Empirical Mode Decomposition: a noise-assisted data analysis method Adv. Adaptive Data Analysis 1 1 2009 1 41 [67] Z. Wu N.E. Huang X. Chen The Multidimensional Ensemble Empirical Mode Decomposition Method Adv. Adaptive Data Analysis 1 2009 339 372 [68] Z. Wu N.E. Huang X. Chen Some considerations on physical analysis of data Advances Adaptive Data Analysis 3 1 & 2 2011 95 113 [69] C.-Z. Xiong J. y. Xu J.-C. Zou D.-X. Qi Texture classification based on EMD and FFT Journal of Zhejiang University - Science A 7 2006 1516 1521 10.1631/jzus.2006.A1516 [70] Y. Xu B. Liu J. Liu S. Riemenschneider Two-dimensional empirical mode decomposition by finite elements Proceedings of the Royal Society A 462 2006 3081 3096 [71] C.-L. Yeh H.-C. Chang C.-H. Wu P.-L. Lee Extraction of single-trial cortical beta oscillatory activities in eeg signals using empirical mode decomposition BioMedical Engineering OnLine 9 2010 25 [72] A. Zeiler R. Faltermeier A. M.Tomé C.G. Puntonet A. Brawanski E.W. Lang Weighted sliding empirical mode decomposition for online analysis of biomedical time series Neural Process Lett 37 2013 21 32 [73] J. Zhang J. Ou R. Zhan Automatic target recognition of moving target based on empirical mode decomposition and genetic algorithm support vector machine J. Cent. South Univ. 22 2015 1389 1396 [74] J. Zhang Z. Qin Edge detection using fast Bidimensional Empirical Mode Decomposition and mathematical morphology Proceedings of the IEEE SoutheastCon (SoutheastCon) 2010 139 142 "
    },
    {
        "doc_title": "Functional biomedical images of Alzheimer’s disease. A green’s function-based empirical mode decomposition study",
        "doc_scopus_id": "84966455279",
        "doc_doi": "10.2174/1567205013666160322141726",
        "doc_eid": "2-s2.0-84966455279",
        "doc_date": "2016-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Neurology",
                "area_abbreviation": "NEUR",
                "area_code": "2808"
            },
            {
                "area_name": "Neurology (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2728"
            }
        ],
        "doc_keywords": [
            "Alzheimer Disease",
            "Brain",
            "Brain Mapping",
            "Cognitive Dysfunction",
            "Fluorodeoxyglucose F18",
            "Humans",
            "Nonlinear Dynamics",
            "Positron-Emission Tomography",
            "Radiopharmaceuticals",
            "Support Vector Machine"
        ],
        "doc_abstract": "© 2016 Bentham Science Publishers.Positron emission tomography (PET) provides a functional imaging modality to detect signs of dementias in human brains. Two-dimensional empirical mode decomposition (2D-EMD) provides means to analyze such images. It decomposes the latter into characteristic modes which represent textures on different spatial scales. These textures provide informative features for subsequent classification purposes. The study proposes a new EMD variant which relies on a Green’s function based estimation method including a tension parameter to fast and reliably estimate the envelope hypersurfaces interpolating extremal points of the twodimensional intensity distrubution of the images. The new method represents a fast and stable bi-dimensional EMD which speeds up computations roughly 100-fold. In combination with proper classifiers these exploratory feature extraction techniques can form a computer aided diagnosis (CAD) system to assist clinicians in identifying various diseases from functional images alone. PET images of subjects suffering from Alzheimer’s disease are taken to illustrate this ability.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Alzheimer’s disease brain areas: The machine learning support for blind localization",
        "doc_scopus_id": "84964091184",
        "doc_doi": "10.2174/1567205013666160314144822",
        "doc_eid": "2-s2.0-84964091184",
        "doc_date": "2016-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Neurology",
                "area_abbreviation": "NEUR",
                "area_code": "2808"
            },
            {
                "area_name": "Neurology (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2728"
            }
        ],
        "doc_keywords": [
            "Aged",
            "Aged, 80 and over",
            "Algorithms",
            "Alzheimer Disease",
            "Brain",
            "Brain Mapping",
            "Cognitive Dysfunction",
            "Databases, Factual",
            "Female",
            "Fluorodeoxyglucose F18",
            "Follow-Up Studies",
            "Humans",
            "Machine Learning",
            "Male",
            "Mental Status Schedule",
            "Middle Aged",
            "Positron-Emission Tomography"
        ],
        "doc_abstract": "© 2016 Bentham Science Publishers. All rights reserved.The analysis of positron emission tomography (PET) scan image is challenging due to a high level of noise and a low resolution and also because differences between healthy and demented are very subtle. High dimensional classification methods based on PET have been proposed to automatically discriminate between normal control group (NC) patients and patients with Alzheimer’s disease (AD), with mild cognitive impairment (MCI), and mild cognitive impairment converting to Alzheimer’s disease (MCIAD) (a group of patients that clearly degrades to AD). We developed a voxelbased method for volumetric image analysis. We performed 3 classification experiments AD vs CG, AD vs MCI, MCIAD vs MCI. We will also give a small demonstration of the presented method on a set of face images. This method is capable to extract information about the location of metabolic changes induced by Alzheimer’s disease that directly relies statistical features and brain regions of interest (ROIs). We produce “maps” to visualize the most informative regions of the brain and compare them with voxel-wise statistics. Using the mean intensity of about 2000 6 × 6 × 6mm patches, selected by the extracted map, as input for a classifier we obtain a classification rate of 95.5%.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hierarchical object representation for open-ended object category learning and recognition",
        "doc_scopus_id": "85019182538",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85019182538",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Hierarchical object representation",
            "Latent Dirichlet allocation",
            "Low-level features",
            "Object categories",
            "State-of-the-art system",
            "Statistical features",
            "Structural semantics"
        ],
        "doc_abstract": "© 2016 NIPS Foundation - All Rights Reserved.Most robots lack the ability to learn new objects from past experiences. To migrate a robot to a new environment one must often completely re-generate the knowledgebase that it is running with. Since in open-ended domains the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by robots. Therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent and interleaved fashion. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. topics) from low-level feature co-occurrences for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. The approach contains similarities with the organization of the visual cortex and builds a hierarchy of increasingly sophisticated representations. Results show the fulfilling performance of this approach on different types of objects. Moreover, this system demonstrates the capability of learning from few training examples and competes with state-of-the-art systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D object perception and perceptual learning in the RACE project",
        "doc_scopus_id": "84949908319",
        "doc_doi": "10.1016/j.robot.2015.09.019",
        "doc_eid": "2-s2.0-84949908319",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D object",
            "Interactive learning",
            "Memory systems",
            "Open-ended learning",
            "Point cloud"
        ],
        "doc_abstract": "© 2015 Elsevier B.V.This paper describes a 3D object perception and perceptual learning system developed for a complex artificial cognitive agent working in a restaurant scenario. This system, developed within the scope of the European project RACE, integrates detection, tracking, learning and recognition of tabletop objects. Interaction capabilities were also developed to enable a human user to take the role of instructor and teach new object categories. Thus, the system learns in an incremental and open-ended way from user-mediated experiences. Based on the analysis of memory requirements for storing both semantic and perceptual data, a dual memory approach, comprising a semantic memory and a perceptual memory, was adopted. The perceptual memory is the central data structure of the described perception and learning system. The goal of this paper is twofold: on one hand, we provide a thorough description of the developed system, starting with motivations, cognitive considerations and architecture design, then providing details on the developed modules, and finally presenting a detailed evaluation of the system; on the other hand, we emphasize the crucial importance of the Point Cloud Library (PCL) for developing such system.1",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2015-10-09 2015-10-09 2015-12-02 2015-12-02 2016-12-26T09:26:02 S0921-8890(15)00214-6 S0921889015002146 10.1016/j.robot.2015.09.019 S300 S300.2 FULL-TEXT 2016-12-26T04:38:02.271089-05:00 0 0 20160101 20160131 2016 2015-10-09T15:05:55.611708Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 75 75 PB Volume 75, Part B 38 614 626 614 626 201601 January 2016 2016-01-01 2016-01-31 2016 Special Section on 3D Perception with PCL; Edited by Matteo Munaro, Radu Bogdan Rusu, and Emanuele Menegatti article fla Copyright © 2015 Elsevier B.V. All rights reserved. 3DOBJECTPERCEPTIONPERCEPTUALLEARNINGINRACEPROJECT OLIVEIRA M 1 Introduction 2 Related work 3 A dual memory approach 4 The RACE object perception system 4.1 Architecture 4.2 The PCL foundation of this work 4.3 Object perception 4.4 User perception 4.5 Addressing computational issues 5 Perceptual learning 6 Profiling and evaluation 6.1 Profiling 6.2 Off-line evaluation of the perceptual learning approach 6.3 Open-ended learning evaluation 7 Conclusions Acknowledgments References HERTZBERG 2014 297 304 J KASAEI 2015 1 17 H MOKHTARIHASSANABAD 2015 V INTELLIGENTAUTONOMOUSSYSTEMS13PROCEEDINGS13THINTERNATIONALCONFERENCEIAS13 GATHERINGCONCEPTUALIZINGPLANBASEDROBOTACTIVITYEXPERIENCES COUSINS 2010 12 14 S COUSINS 2010 12 14 S NAU 2003 379 404 D BARSALOU 1999 577 609 L OLIVEIRA 2014 M PROCEEDINGSIEEERSJINTERNATIONALCONFERENCEINTELLIGENTROBOTSSYSTEMSIROS APERCEPTUALMEMORYSYSTEMFORGROUNDINGSEMANTICREPRESENTATIONSININTELLIGENTSERVICEROBOTS LIM 2011 492 509 G CORADESCHI 2003 85 96 S ZAMAN 2013 S IEEEINTERNATIONALCONFERENCEROBOTICSAUTOMATION INTEGRATEDMODELBASEDDIAGNOSISREPAIRARCHITECTUREFORROSBASEDROBOTSYSTEMS TULVING 1991 3 32 E MEMORYORGANIZATIONLOCUSCHANGE CONCEPTSHUMANMEMORY TULVING 2005 4 56 E MISSINGLINKINCOGNITION EPISODICMEMORYAUTONOESISUNIQUELYHUMAN WOOD 2011 81 103 R SEABRALOPES 2007 53 81 L SEABRALOPES 2008 277 297 L KIRSTEIN 2012 90 105 S KRUGER 2011 740 757 N ALDOMA 2012 A CLAPES 2013 799 808 A RUSU 2009 3212 3217 R ROBOTICSAUTOMATION2009ICRA09IEEEINTERNATIONALCONFERENCE FASTPOINTFEATUREHISTOGRAMSFPFHFOR3DREGISTRATION SEABRALOPES 2001 10 14 L SAHIB 2013 S LIM 2014 153 160 G ROBOTHUMANINTERACTIVECOMMUNICATION2014ROMAN23RDIEEEINTERNATIONALSYMPOSIUM INTERACTIVETEACHINGEXPERIENCEEXTRACTIONFORLEARNINGABOUTOBJECTSROBOTACTIVITIES EVANS 2008 255 278 J COHENOR 1995 453 461 D BARBER 1996 469 483 C FISCHLER 1981 381 395 M RUSU 2009 R SEMANTIC3DOBJECTMAPSFOREVERYDAYMANIPULATIONINHUMANLIVINGENVIRONMENTS YI 2000 76 78 W JOHNSON 1999 433 449 A CONNOR 2010 599 608 M MUNARO 2013 53 68 M FRONTIERSINTELLIGENTAUTONOMOUSSYSTEMS ASOFTWAREARCHITECTUREFORRGBDPEOPLETRACKINGBASEDROSFRAMEWORKFORAMOBILEROBOT BENTLEY 1975 509 517 J CHAUHAN 2011 341 354 A OLIVEIRAX2016X614 OLIVEIRAX2016X614X626 OLIVEIRAX2016X614XM OLIVEIRAX2016X614X626XM 2017-12-02T00:00:00Z UnderEmbargo item S0921-8890(15)00214-6 S0921889015002146 10.1016/j.robot.2015.09.019 271599 2016-12-26T04:38:02.271089-05:00 2016-01-01 2016-01-31 true 1868217 MAIN 13 53302 849 656 IMAGE-WEB-PDF 1 fx1 12594 28 219 gr1 23670 164 191 gr2 22730 164 219 gr3 20164 142 219 gr4 35390 128 219 gr5 22858 164 135 gr6 25048 164 212 gr7 25354 164 214 gr8 20489 130 219 gr9 19009 87 219 pic1 32766 164 140 pic2 33052 164 140 pic3 29650 164 140 pic4 26625 164 140 pic5 33620 164 140 pic6 30488 164 140 fx1 37984 59 461 gr1 80010 404 472 gr2 59338 275 367 gr3 101946 453 697 gr4 71163 198 339 gr5 217247 903 744 gr6 79263 291 376 gr7 75798 288 376 gr8 110529 378 636 gr9 94838 251 629 pic1 40427 132 113 pic2 44745 132 113 pic3 38146 132 113 pic4 40704 132 113 pic5 42891 132 113 pic6 38759 132 113 fx1 99682 263 2041 gr1 377574 1791 2091 gr2 203931 1218 1625 gr3 571474 2005 3085 gr4 362193 876 1500 gr5 1221470 3998 3295 gr6 269699 1291 1667 gr7 275599 1277 1667 gr8 756745 1677 2818 gr9 562149 1110 2784 pic1 97072 584 500 pic2 111383 584 500 pic3 86166 584 500 pic4 98879 584 500 pic5 122229 584 500 pic6 90792 584 500 si1 60 4 15 si13 914 48 174 si14 227 15 50 si15 1101 53 201 si17 354 15 57 si18 1092 40 178 si19 211 14 25 si2 195 11 42 si20 724 19 149 si21 145 8 14 si22 148 11 19 si23 1862 32 394 si24 171 11 18 si25 124 8 10 si3 113 10 8 si4 124 8 11 si5 1056 62 192 si6 136 13 11 si7 169 12 35 si8 136 12 10 si9 159 11 17 ROBOT 2547 S0921-8890(15)00214-6 10.1016/j.robot.2015.09.019 Elsevier B.V. Fig. 1 A high-level overviews of the RACE architecture. Fig. 2 Abstract cognitive architectures for hybrid reactive–deliberative robots: (a) with a single memory system; (b) with a dual memory system. Fig. 3 Architecture of the developed object perception and perceptual learning system. Fig. 4 Visualization of tracking, pointing and labeling. Fig. 5 Processing time in the object perception pipelines in the video sequence, comparing nodes (left column) with nodelets (right column). Six objects appear in the video, corresponding to pipelines 1 through 6. (a) tracker nodes; (b) feature extraction nodes; (c) object recognition nodes; (d) tracker nodelets; (e) feature extraction nodelets; (f) object recognition nodelets. Fig. 6 Perceptual memory usage during the experiment, in logarithmic scale. The blue (upper) curve represents the total size of all point clouds of object views extracted by the trackers. The green (middle) curve represents the total accumulated size of all point clouds of key views. The red (bottom) curve represents the actual perceptual memory content (shape-based representations of key views). Fig. 7 Object recognition performance (precision, recall and F-measure) during the experiment. Each point in these curves is computed based on the object recognition results in the previous 20 s. Fig. 8 (a) Simulated teacher experiment no. 3; (b) protocol success versus the number of learned categories, for the same experiment. Fig. 9 System performance during simulated user experiments: (a) global success vs. number of learned categories, a measure of how well the system learns; (b) number of learned categories vs. number of question/correction iterations, represents how fast the system learned object categories. Table 1 PCL functionalities used in the RACE object perception and perceptual learning system. The modules listed in the third column are those represented in Fig. 3. PCL class [refs.]/parameters Usage in RACE RACE modules ConditionalRemoval Removing points outside a 3D box a Object DetectorObject Tracker VoxelGrid [30] voxel size=0.015 m Downsampling a point cloud b Object Detector Object Tracker ConvexHull [31,32] Convex hulls of tables c Tabletop Segmenter RandomSampleConsensus [33] RANSAC iterations=200 Table plane detection d Tabletop Segmenter ExtractPolygonalPrismData minimum z=0.01 mmaximum z=0.6 m Tabletop object detection e Object Detector EuclideanClusterExtraction [34] minimum cluster size=10maximum cluster size = 10,000 clustering step=0.08 m Object segmentation f Object Detector PCA [35] Estimating the orientation of objects g Object Tracker ParticleFilterTracker [36] max number of particles=200 Tracking tabletop objects using RGBD h Object Tracking SpinImageEstimation [37] support length=0.1 mimage width=8, support angle=90° Features for representing object views i Feature Extractor KdTree [38] K = 1 Computation of distance between views j Object Recognizer Object Conceptualizer a b c d e f g h i j Table 2 Sequence of events in the experiment (see video). Time (s) Event Description 25 T1 in A Mug (T1) is placed on the table 40 T1 is a Mug T1 is labeled as Mug 60 T2 in A Vase (T2) is placed on the table 75 T2 is a Vase T2 is labeled as a Vase 90 T3 in Another Mug (T3) is placed on the table 135 T3 out T3 is removed from the table 140 T1 out T1 is removed from the table 145 T2 out T2 is removed from the table 165 T4 in A Plate (T4) is placed on the table 170 T4 is a Plate T4 is labeled as a Plate 175 T5 in A Bottle (T5) is placed on the table 190 T5 is a Bottle T5 is labeled as a Bottle 210 T6 in A Spoon (T6) is placed on the table Table 3 Average object recognition performance (F1 measure) for different parameters: voxel size (VS), image with (IW), support length (SL) and classification threshold (CT). Table 4 Summary of simulated teacher experiments a . Exp# #Iterations #Categories #Instances GS APS 1 706 22 14.55 0.65 0.76 2 217 13 8.69 0.68 0.86 3 533 24 10.46 0.68 0.74 4 699 21 15.57 0.63 0.74 5 747 29 11.31 0.69 0.79 6 711 31 9.83 0.72 0.75 7 1041 36 12.06 0.70 0.77 8 252 13 10.08 0.64 0.87 9 412 19 10.37 0.68 0.81 10 393 20 8.9 0.70 0.84 a Exp#, experiment number; #Categories, number of categories learned; #Iterations, number of iterations in the experiment; #Instances, average number of instances per category at the end of the experiment; GS, global success; APS, average protocol success. 3D object perception and perceptual learning in the RACE project Miguel Oliveira a Luís Seabra Lopes a b ⁎ Gi Hyun Lim a S. Hamidreza Kasaei a Ana Maria Tomé a b Aneesh Chauhan c a IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro Portugal b Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro, Portugal Departamento de Electrónica, Telecomunicações e Informática, Universidade de Aveiro Portugal c Center of Automation and Robotics, Universidad Politécnica de Madrid, Spain Center of Automation and Robotics, Universidad Politécnica de Madrid Spain ⁎ Corresponding author at: IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro, Portugal. IEETA — Instituto de Engenharia Electrónica e Telemática de Aveiro, Universidade de Aveiro Portugal This paper describes a 3D object perception and perceptual learning system developed for a complex artificial cognitive agent working in a restaurant scenario. This system, developed within the scope of the European project RACE, integrates detection, tracking, learning and recognition of tabletop objects. Interaction capabilities were also developed to enable a human user to take the role of instructor and teach new object categories. Thus, the system learns in an incremental and open-ended way from user-mediated experiences. Based on the analysis of memory requirements for storing both semantic and perceptual data, a dual memory approach, comprising a semantic memory and a perceptual memory, was adopted. The perceptual memory is the central data structure of the described perception and learning system. The goal of this paper is twofold: on one hand, we provide a thorough description of the developed system, starting with motivations, cognitive considerations and architecture design, then providing details on the developed modules, and finally presenting a detailed evaluation of the system; on the other hand, we emphasize the crucial importance of the Point Cloud Library (PCL) for developing such system. 1 1 This paper is a revised and extended version of Oliveira et al. (2014). Keywords 3D object perception Point-Cloud Library Dual memory systems Open-ended learning Interactive learning 1 Introduction One of the primary challenges of service robotics is the adaptation of robots to new tasks in changing environments, where they interact with non-expert users. The European project RACE (Robustness by Autonomous Competence Enhancement [1,2]), recently closed, assumed that versatility and competence enhancement can be obtained by learning from experiences. The project focused on acquiring and conceptualizing experiences about objects [3], scene layouts [4] and activities [5] as a means to enhance robot competence over time thus achieving robustness. Stimuli for learning can be collected, either autonomously by robots, or when they receive appropriate feedback from users. The functional components of the RACE architecture are represented by boxes in Fig. 1 . Each component may contain one or more modules, which are implemented as nodes (or nodelets) over the Robot Operating System (ROS) [6,7]. The Reasoning and Interpretation component includes a temporal reasoner, a spatial reasoner and a description logics reasoner. Perception contains several modules for symbolic proprioception and exteroception, which generate occurrences. The Experience Management and Conceptualization component pre-processes occurrences, extracts relevant experiences, uses them to create new concepts and stores these in the Memory component. The User Interface component receives instructions from the user and relays them to the Planning component. Planning is carried out using SHOP2, a Hierarchical Task Network planner [8]. The produced plans are executed by the Plan Execution Management component. One of the challenges in this type of projects is to ground [9] the semantic representations maintained by the robot, namely the model of the world state and the learned concepts, into the perception and action capabilities of the robot itself [10,11]. At least two types of grounding are involved here. For internal symbols that refer to real-world objects (e.g. “mug23”), the robot must maintain a perception-mediated mapping of symbols to objects. This is often called anchoring [12] and relies to some extent on (visual) tracking capabilities at the perception level and on semantic interpretation capabilities at the reasoning level. For category symbols (e.g. “Mug”), the robot must ground their meanings on concrete observations of instances of the categories. A related challenge is how to combine semantic (i.e. symbolic, relational, logic-based) with perceptual (numeric, pattern-based) representations, and how to store different types of representations. After analyzing the requirements of the different components of the RACE architecture, a key decision was made by the project: instead of a single memory system, two independent memory systems would be used, one for semantic information (the Semantic Memory) and the other for perceptual information (the Perceptual Memory) [2]. Through perceptual learning capabilities, the developed object perception system can be applied to open-ended environments. In this case, “open-ended” means that the robot does not know in advance which object categories it will have to learn, which observations will be available, and when they will be available to support this learning. This kind of perception system must comprise a significant number of software modules, which must be closely coupled in their structure and functionality [13]. Three main design options address the key computational issues involved in processing and storing perception data. First, a lightweight, NoSQL database, is used to implement the perceptual memory. Second, a thread-based approach with zero copy transport of messages is used in implementing the modules. Finally, a multiplexing scheme, for the processing of the different objects in the scene, enables parallelization. This way, the system is capable of real time object detection, tracking and recognition. The developed perception and perceptual learning capabilities target objects in table-top scenes, e.g. in a restaurant environment. These capabilities are fully integrated in the RACE architecture and are running on the PR2 robot used by the project. This work heavily relies on Point Cloud Library (PCL) functionalities, as will be detailed in Section 4. Because the developed perception system is a complex network of processing nodes, the whole paper is organized in such a way that the organization of the system and the module functionalities are well justified and presented in detail. However, since PCL is used in nearly every module of the system, the system could not have been developed easily without PCL. Therefore, this work shows the current importance of PCL in building sophisticated 3D perception systems in robotics and other domains. The remaining part of this paper is organized as follows. In the next section, related works are discussed. Memory and cognitive architecture issues are discussed in Section 3, leading to the choice of a dual memory approach and to the development of a perceptual memory system. The RACE object perception system and the perceptual learning approach are described in Sections 4 and 5. Profiling and evaluation of the developed system is the topic of Section 6. Finally, in Section 7, the conclusion is presented and future research is discussed. 2 Related work As robots are expected to increasingly interact and collaborate closely with humans, robotics researchers need to look at human cognition as a source of inspiration. Learning is closely related to memory in human cognition. In the cognitive science literature, the existence of multiple memory systems is widely accepted [14,15]. Biological findings about memory and learning have served as inspiration for the development of computational models and applications. Wood et al. [16] present a thorough review and discussion on memory systems in animals as well as artificial agents, having in mind further developments in artificial intelligence and cognitive science. In [17], an open-ended object category learning system, based on one-class learning and human–robot interaction, is described. The authors also proposed a teaching protocol for performance evaluation in open-ended learning. In [18], a multi-classifier system with similar goals is described in which a meta-learning component monitors classifier performance, reconfigures classifier combinations and chooses the classifier to be used for prediction. These works are based on 2D images collected in static scenes. Since there is no continuous stream of data being stored, memory requirements are easily satisfied. Kirstein et al. [19] proposed a lifelong approach for interactive learning of multiple categories from 2D perception, in this case based on vector quantization. This involves selecting the most crucial features from a series of high dimensional feature vectors that almost exclusively belong to each specific category. However, they still follow a standard train-and-test procedure, which is not plausible in open-ended scenarios. Moreover, the authors did not provide details on their memory system or computational architecture. The work is also not integrated in a hybrid perceptual/semantic processing system. Kruger et al. [20] use the so-called “object–action complexes” (OAC) to bind objects, actions and attributes associated with an agent in a causal way. These OACs are learnable/refinable semantic representations. To ground OACs, an agent requires an object perception and learning system, such as the one we propose below. Heintz et al. [21] propose a hierarchical framework designed to anchor symbols to continuous streams of sensor data. The approach dynamically constructs and maintains data association hypotheses at multiple levels. A traffic monitoring application is used to illustrate the system. This work proposes a general approach to the anchoring problem, but does not address object perception and learning. Willow Garage developed the Object Recognition Kitchen (ORK), 2 2 a 3D object recognition system built on top of the Ecto framework. 3 3 Ecto organizes computation as a directed acyclic graph, which implies important limitations in the architecture of the perception system. Moreover, in ORK, training/learning and detection/recognition are two separate stages. Such approach is not suitable for developing open-ended learning agents. In contrast, our system allows for concurrent or interleaved learning and recognition, and real-time performance is achieved through nodelets and multiplexing. Although the Point-Cloud Library (PCL) is increasingly popular, we do not know of other systems building upon PCL to integrate 3D object perception, memory, learning, recognition and interaction. Aldoma et al. [22] reviewed several state-of-the-art 3D shape descriptors from PCL to develop 3D object recognition and pose estimation capabilities. Throughout the paper, the properties, advantages and disadvantages of different local and global shape descriptors are considered. They also proposed two pipelines for object recognition based on PCL. In the first pipeline, an object view is described by a set of local shape features, which are computed around keypoints. Afterward, each feature is compared against all the features of all models in a database using Euclidean distance. The second pipeline is based on global descriptors, i.e. high-dimensional representations usually calculated for object candidates (subsets of the scene’s point cloud obtained through segmentation). Clapés et al. [23] proposed an automatic surveillance system for user identification and object recognition. In this work, the position of the RGB-D camera is fixed and the authors employed a background subtraction strategy to segment users and objects in the scene. In the case of object detection, the remaining connected components (those not previously selected as being part of the user) are considered as object candidates. During the recognition stage, Fast Point Feature Histogram (FPFH) [24] features are computed for each detected object view and matched against the training models. There is no learning process involved. 3 A dual memory approach Arguably, robots that interact closely with non-expert users should be [25]: animate, meaning that they react appropriately to different events, based on a tight coupling of perception and action; adaptive, to cope with changing users, tasks and environments, which requires reasoning and learning capabilities; and accessible, that is, they should be easy to command and instruct, and they should also be able to explain their beliefs, motivations and intentions. In an abstract architecture for intelligent robots, as shown in Fig. 2 (a), a Perception component processes all momentary information coming from sensors, including sensors that capture the actions and utterances of the user. A Reasoning component updates the world model and determines plans to achieve goals. An Action component reactively dispatches and monitors the execution of actions, taking into account the current plans and goals. Action processing ranges from low-level control to high-level execution management. Finally, a Learning component, which typically runs in the background, analyzes the trace of foreground activities recorded in a Memory component and extracts and conceptualizes possibly interesting experiences. The resulting conceptualizations are stored back in memory. Each component in such abstract architecture decomposes into a set of software modules, possibly distributed across multiple computers. The reasoning component manipulates primarily semantic representations of the current world state, goals and plans, that is, representations that are symbolic and relational in nature. In RACE, where case studies were carried out in a restaurant environment, semantic representations describe tables, chairs, table-top objects, guests, the robot, etc., the categories of these objects, the relations between them, and the actions and events that change these relations. The semantic information flowing between reasoning, execution management and memory is typically of small size, and its processing tends to be slow. One of the challenges in a project like RACE was to combine and store semantic and perceptual representations. Standard SQL databases do not cope well neither with semantic data nor with perception data, as both tend to be partially unstructured and/or of variable size. This suggests that modern NoSQL databases [26] should be used. Semantic data represents the world in terms of instances, categories and relations between them. A semantic representation of the state of the world can be simply a set of subject–predicate–object triples. A special kind of database, the triplestore, which shares some features with both SQL and graph databases, is especially optimized to store information in the form of a set of triples. Triplestores are clearly one of the database types to take into account when developing memory systems for robots. An RDF triplestore was in fact the choice for the initial memory component in the RACE architecture [1]. The contents of this memory system, which is used as blackboard for all processes, is semantic in nature. It keeps track of the evolution of both the internal state of the robot and the events observed in the environment. Access to the triplestore is granted via a ROS node that provides database query and write services for all other nodes (an interface node). Information exchange is performed using either publisher/subscriber or client/server mechanisms. ROS communications are a robust framework [13]. However, when the size of the messages is large (e.g., when passing 3D point clouds), the communication between processes is slow. In the case of perception related data, its large size implies large ROS messages to be passed between the database interface node and the other nodes. This is a major constraint, especially considering that, unlike semantic data, perceptual data flows continuously at the sensor output frequency. Using a database interface node creates a bottleneck for accessing the database, since it handles access requests in a first in, first out basis. Moreover, although triplestores are well suited for storing semantic information, they can hardly be considered suited for storing perception data. In fact, the perception modules will primarily process numeric information organized in structures like vectors and matrices, possibly grouped in sets. For instance the raw perception data about an object, after detection, can be a 3D point cloud, which is a set of points described by their 3D coordinates and possibly RGB information. Based on the point cloud, shape features can be extracted, and the object can be represented by a set of local shape features, where each of them can be a 2D shape histogram. To ensure timely reaction to events in the environment, perception modules run continuously at the frame rate of the used sensors. Although raw data tends to be massive (high-dimensional), the perception modules must run fast, and whatever memory support they use, must also be lightweight. In the context of RACE, to accommodate semantic and perceptual information in the same database, the only option would be to replace the triplestore with a more generic kind of database. However, we would loose the special features of triplestores, which are optimized for storing triples. In alternative, two different databases can be used, one for semantic information, and the other for perceptual information. The second alternative, which seems more promising, allows to use databases that are well suited for the kinds of data that each will store. In RACE, we converged to the second option. Fig. 2(b) shows an abstract architecture diagram in which we make explicit the dual memory approach. In what concerns reasoning, we make explicit both interpretation and planning capabilities. One of the most basic interpretation capabilities is anchoring, i.e. connecting object symbols used in the semantic memory to the perception of those objects that is recorded in the perceptual memory. Interpretation also includes computing spatial relations between objects to keep an updated relational model of the scene around the robot. In turn, this scene model can be taken into account for anchoring. The perceptual memory contains, not only object perception data, but also object category knowledge, in the form of perceptual categories that enable to recognize instances of those categories. These perceptual categories are learned in an open-ended fashion with user mediation [27]. The perceptual learning component primarily uses data from perceptual memory (e.g. shape features of objects) as well as from the semantic memory (e.g. teaching instructions from the user). In RACE, the implementation of the perceptual memory was carried out using a flexible and scalable NoSQL database which operates in memory (see the next section for details). It is worth emphasizing that, although our design choices were guided primarily by engineering criteria, we converged to a solution that is biologically and cognitively plausible. In fact, as previously pointed out, human memory is not a single monolithic system, but rather a combination of several memory subsystems specialized for storing different types of information and supporting different functionalities [14,15]. In particular, our perceptual memory resembles the so-called Perceptual Representation Memory System, used in human cognition for enhancing the identification of objects as structured physical–perceptual entities, a process referred to as perceptual priming [14]. Another key distinction in cognitive science is between processes that are fast, automatic and unconscious, and processes that are slow, deliberative and conscious [28]. Our dual semantic/perceptual memory approach is also in line with these findings. 4 The RACE object perception system The work presented in this paper was developed as an extension to the initial RACE architecture [1] (see also Fig. 1). In particular, the work focused on extending the perceptual capabilities of the system. The perception system developed around the perceptual memory supports the anchoring of object symbols into perceived object data as well as the grounding of category symbols into perceptual categories. Since the initial integration of the object perception system, the RACE system included basic capabilities for object symbol anchoring, allowing perceived objects to be represented not only in the perceptual memory, but also in the semantic memory. The object perception system targets table-top scenes in a restaurant scenario. This system became a salient portion of the full RACE system [2]. 4.1 Architecture The developed perception system is composed of six functional components: Object Detection, Multiplexed Object Perception, User Interface, Reasoning and Interpretation, Memory and Conceptualization. These are represented by the dashed rectangles in Fig. 3 . Functional components in Fig. 3 correspond to those highlighted in bold in the high-level RACE architecture (Fig. 1) and to the perception, interpretation, memory and perceptual learning components in Fig. 2(b). In turn, each functional component contains one or more software modules (solid line rectangles in Fig. 3). Arrows signal the exchange of information between software modules. Each software module is organized into a ROS package and will typically correspond to a node or a nodelet 4 4 at runtime. The implementation of the perceptual memory was carried out using LevelDB, a lightweight, flexible and scalable NoSQL database developed by Google. 5 5 LevelDB is a key–value storage database that provides an ordered mapping from string keys to string values. In addition, LevelDB operates in memory and is copied to the file system asynchronously. This significantly improves its access speed. 4.2 The PCL foundation of this work This work heavily relies on PCL [29] 6 6 functionalities. Table 1 lists the PCL classes used by the object perception and perceptual learning system along with values we typically use for the main configuration parameters. The reason why they are used as well as the modules in which they are used are also given. It can be seen from Table 1 that several PCL functionalities are used by our system. For point cloud size reduction, we use both conditional removal filter as well as voxel grid filters. Table planes are detected using RANSAC, and their boundaries extracted by estimating the 2D convex hull. Points belonging to tabletop objects are extracted using the polygonal prism extraction method. Tabletop objects are segmented using Euclidean cluster extraction. The pose of newly detected tabletop objects is estimated using PCA, which is useful to define well oriented bounding boxes. Objects are tracked over time using the particle filter tracker. Object views are represented by spin image feature descriptors, and object recognition uses an optimized view-to-view distance calculation based on K-d trees. Since PCL is used in nearly every module of the system, the system could not have been developed easily without PCL. This work shows the current importance of PCL in building sophisticated 3D perception systems in robotics and other domains. 4.3 Object perception An RGB-D sensor is used for the perception of both the user and the table-top scene. The starting point for the perception of the table-top scene is the Table-Top Segmenter (TTS) module, which uses ROS 7 7 and PCL functionalities to isolate (partial) point clouds of the objects placed on the table (see Table 1) [29,39]. The Object Detector (OD) module periodically requests the current list of objects from TTS. Then, OD will check if any of those objects is already being tracked. To do this, OD matches the point clouds of all objects on the table with the estimated bounding boxes of all objects currently being tracked. The percentages of points of the tabletop objects that lie inside the bounding boxes of the tracked objects are computed. A large percentage indicates that the tracked object and the segmented object are the same. Point clouds that cannot be matched with any of the tracked bounding boxes are assumed to represent new objects just added to the scene. OD will assign a new identifier (track_id) to each newly detected object. Also for each new object, OD will launch an object perception pipeline which contains three modules: Object Tracking, Feature Extraction and Object Recognition. Fig. 4 shows a situation where two objects are segmented and tracked, i.e., they have bounding boxes around them. Object Tracking (OT) is responsible for keeping track of the target object over time while it remains visible. Tracking is an essential base for anchoring. On initialization, OT receives the point cloud of the detected object and computes a bounding box for that point cloud, the center of which defines the pose of the object. A particle filter tracking approach from PCL (see Table 1) is then used to predict the next probable pose of the object. In each cycle, OT sends out the tracked pose of the object both to OD (as mentioned above) and to the Interpretation component. At a lower rate, OT sends the point cloud of the object (i.e. containing the points inside the predicted bounding box) to Feature Extraction. As expected, the system is sensitive to the speed with which objects move. If an object moves very fast, tracking is lost, then a new object is detected, and a new object perception pipeline is initiated. Nonetheless, our experiments have shown that the system is able to cope with users picking up the objects and moving them around in natural movements with typical speeds. The Feature Extraction (FE) module computes and stores object representations in the perceptual memory. Objects are represented by sets of local shape features computed in certain keypoints. For efficiency reasons, the number of keypoints should be much smaller than the total number of points. To select keypoints, a voxelized grid approach from PCL is used (see Table 1). We select, for each voxel, the point that is closest to the voxel center [3]. Thus, there will be one keypoint per voxel. The surrounding shape in each keypoint is described by a spin-image [37]. Spin-images are pose invariant, and therefore a suitable local shape descriptor for 3D perception in service robots. They are computed by projecting the 3D surface points of the object to the keypoint’s tangent plane We use an implementation of spin-image estimation available from PCL (see Table 1). In addition to storing object representations in the perceptual memory, FE also sends them to Object Recognition (OR). The perceptual categories learned so far and stored in the perceptual memory are used by OR to predict the category of the target object. OR is a low frequency module, which runs at 1 Hz. Accordingly, FE receives object point clouds from OT and sends the extracted representations for recognition at the same frequency. Thus, only OT itself uses object point clouds at the frame rate of the sensor (30 Hz). For better representing an object, it is important to store different views, which is possible when the object is moved (and thus its pose relative to the sensor changes). In contrast, storing all object representations computed by FE while the object is static would lead to unnecessary accumulation of highly redundant data. On a different line, it is important to minimize noise effects possibly affecting object views. Thus, to optimize memory usage while keeping potentially relevant and distinctive information, a heuristic is used to select key views, that is, object views that should be stored. Whenever the tracking of an object is initialized, or when it becomes static again after being moved, three consecutive object views are stored, provided that the hands of the user are not detected near the object. In case the hands are detected near the object, storing key views is postponed until the hands are withdrawn. OT is responsible for marking object views as key views. Then, when FE receives a point cloud marked as key view, it will store the respective representation in the perceptual memory. Object recognition results are also written to the perceptual memory, where the Interpretation component can fetch them to support symbol anchoring. The current implementation is capable of anchoring symbols that refer to objects only while these remain visible. Further work is ongoing to enable anchoring object symbols when the visual tracking is lost. 4.4 User perception The perceptual memory supports, not only the anchoring of object symbols into perceived object data, but also the grounding of category symbols into perceptual categories. Perceptual categories are acquired with user mediation, that is, the user points to objects and provides their category names. Verbal input is provided through interactive markers in RVIZ, a 3D visualization tool for ROS. Point gesture recognition is based on tracking the skeleton of the user. The Skeleton Tracker (ST) module is the one available in OpenNI. 8 8 It tracks the user skeleton pose over time based on RGB-D data. The skeleton pose information is passed to the Gesture Recognizer (GR) module, which computes a pointing direction. Currently, the pointing direction is assumed to be the direction of the right forearm (see an example in Fig. 4). The pointing direction is then passed to the Interpretation component. Upon receiving verbal input, the Interpretation component checks if the received pointing direction intersects the bounding box of any of the objects currently on the table according to the world state recorded in the semantic memory. If that is the case, then a teaching instruction is recorded in the semantic memory, stating that the target object was taught to belong to the given category. Teaching instructions trigger perceptual learning to create and/or update object categories. 4.5 Addressing computational issues In contrast with the reasoning processes supported by the semantic memory, the processes developed around the perceptual memory must run fast to cope with the continuous stream of massive sensor data. As pointed out, one of the reasons for using LevelDB to implement the perceptual memory is the fact that it operates in RAM. There is, however, the limitation that simultaneous access to LevelDB is only possible by threads within the same process. To comply with this constraint while keeping ROS as the framework for the newly developed modules, we use ROS nodelets. 9 9 Nodelets, which run as threads of a single process, were designed to provide a way of concurrently running different modules with zero copy transport between publisher and subscriber calls (as an example, see [40]). The motivation for ROS nodelets comes from systems with high throughput data flows as is common in perception systems. It is not surprising, therefore, that the developers of PCL and ROS nodelets are the same. In our system, in addition to handling high throughput data flows, nodelets come handy to implement modules that need to simultaneously access the perceptual memory (LevelDB). Another way of optimizing perception is to parallelize computations. In our system, instead of tracking all objects in a single tracking module, there is a tracker for each object. Similar strategy is used for feature extraction and object recognition. In other words, object perception is designed to be multiplexed. Every time a new object is detected, a corresponding instance of the object perception pipeline (see Fig. 3) is launched. Thus there are as many object perception pipelines as the number of currently tracked objects, and each pipeline targets a specific object. Since the modules in an object perception pipeline run as independent nodes/nodelets, they can be distributed to different CPU cores, thus improving the overall computational efficiency of perception. Note that the three modules in the object perception pipeline are traditionally amongst the heaviest in terms of computational requirements. The parallelization is aimed at the hotspot or bottleneck of the computation flow and takes full advantage of modern multi-core machines. In fact, experiments with a non-multiplexed version of this architecture show that it cannot run in real-time. We can easily configure the perception and perceptual learning modules to be launched with different runtime configurations, that is, using ROS nodelets only, ROS nodes only, or a combination of both. By default, the object perception pipelines, the perceptual learning module and the perceptual memory run as a set of nodelets of a single process. When debugging is necessary, we use a configuration where all modules run as nodes. In this configuration, the modules access the perceptual memory (LevelDB) using ROS services provided by a database interface node. 5 Perceptual learning Although other kinds of perceptual categories could be considered, perceptual learning currently focuses on object categories. We approach object category learning from a long-term perspective and with emphasis on open-endedness, i.e. not assuming a pre-defined set of categories [17,18]. For example, when learning how to serve a coffee [1], if the robot does not know how a mug looks like, it may ask the user to point to one. Such situation provides an opportunity to collect an experience for learning. Concerning category formation, a purely memory-based learning approach is adopted, in which a category is represented by a set of views of instances of the category. The recording of a teaching instruction in the semantic memory triggers the perceptual learning component. If a new category was taught, the key views of the target object stored by the feature extraction module (see above Section 4.3) are used to initialize the category. If the category is previously known, the key views are added to the existing category representation only if the agent cannot correctly recognize the category of the target object. The current approach differs from our recent previous work [3,10], in the distance measures used for classification, as will be pointed out. In order to estimate the dissimilarity between a target object view, t , and an object view, o , contained in a category model in the perceptual memory, the following distance function is used: (1) D ( t , o ) = ∑ l = 1 q min k d ( t l , o k ) q , where t l , l = 1 , …, q , are the spin-images of the target object, o k represents the spin-images of the model object, and q is the number of target object’s spin-images (see Section 4.3 and [37] on spin-images). Since a linear search in high-dimensional spaces has a high computational cost, which is not suitable in the case of an autonomous service robot, a fast approximate nearest neighbor search based on k-d trees [41] from PCL (see Table 1) is used instead of a traditional nearest neighbor search. The next step is to compute the object–category distance between the target object, t , and a certain category, C, as the average distance of the instances of C to t : (2) O C D ( t , C ) = ∑ u ∈ C D ( t , u ) n , where n = | C | is the total number of category instances. In object recognition, instances are more spread in some categories than in others. Normalizing distances will help to prevent misclassification. Distance normalization is based on the following intra-category distance: (3) I C D ( C ) = ∑ u ∈ C ∑ v ∈ C , v ≠ u D ( u , v ) n . ( n − 1 ) . The normalized distance of target object, t , to the category, C, N D ( t , C ) , is computed as follows: (4) N D ( t , C ) = 2 × O C D ( t , C ) I C D ( C ) + I C D ¯ where I C D ¯ is the average of the intra-category distances of all categories, i.e. I C D ¯ = ∑ i = 1 m I C D ( C i ) / m , and m is the number of categories. Finally, the target object is classified based on the minimum normalized distance of the known categories to the object. If, for all categories, the normalized distance is larger than a given Classification Threshold, C T , then the object is predicted to belong to an unknown category 10 10 By default, CT=2.0 is used. See Section 6.2 for an evaluation of alternatives. : (5) C a t e g o r y ( t ) = { unknown , if m i n C N D ( t , C ) > C T a r g m i n C N D ( t , C ) , otherwise . In open-ended environments where some objects may belong to not yet known categories, recognizing that an object belongs to an unknown category is important. It may prevent the agent from making further decisions based on wrong assumptions. On the other hand, detecting that an object belongs to an unknown category may be used by the agent to trigger some exploration or interaction process leading to the acquisition of a new category. In our recent previous work [3,10], the object–category distance was measured as the minimum distance between the target object and known instances of a given category. This measure was then normalized by an intra-category distance. If the normalized measure was larger than a given classification threshold, the target object could not belong to that category. The combination of minimum distance with normalization and threshold led to bad decisions in some limit situations. The new formulation here presented solves these problems. In addition, by taking into account the average intra-category distance, the current normalization is also more stable than the previously used normalization method. 6 Profiling and evaluation This section presents three sets of results. First, based on a session where users manipulate objects on a table and interact with the developed perception and perceptual learning system, we carry out a profiling analysis of the main modules of the system. Second, we present an off-line evaluation of the perceptual learning approach under different configurations of the system. Finally, we report on open-ended learning experiments carried out on a public domain dataset. 6.1 Profiling For profiling the modules, two human users interacted with the system in a short session of nearly 4 min (Table 2 ). All raw data from the RGB-D sensor as well as verbal input from the users during the session was recorded in a rosbag, which was then used to test different configurations of our system. Note that, when the system starts, the set of categories known to the system is empty. There is a video 11 11 The video can be found at that illustrates the behavior of the main modules of the system, from user/object tracking to learning and recognition. As discussed in Section 4.5, nodelets can significantly improve efficiency since they support zero copy transport and they enable simultaneous access to LevelDB. Fig. 5 compares the processing time of the object perception modules. The tracker modules (Fig. 5(a) nodes and (d) nodelets) tend to display a stable processing time shortly after their initialization. This is explained by the fact that the size of the input data is more or less stable over time. In this case, nodelets are more efficient when compared to nodes: for example for pipelines 1–3 in the 100–150 time interval, nodes display an average processing time of 45 ms, compared to 25 ms in the case of nodelets. Since the trackers do not access the database, the main factor contributing to the increase in efficiency is the zero copy transport. The messages that are received (sensor point cloud) and sent (partial object point cloud) by the trackers are of large size, which explains why zero copy transport enables such a significant improvement. The feature extraction modules (Fig. 5(b) nodes and (e) nodelets) show a different behavior. These modules periodically compute the spin-image representation from the partial object point cloud. At some points, the point cloud is signaled to belong to a key view, which will trigger the writing of that representation to the perceptual memory. The curves show these points in time with a rapid increase in processing time. Nodelets also display these peaks, but because access to the database is much faster, the peaks are smaller, as is the average processing time. The object recognition modules (Fig. 5 (c) nodes and (f) nodelets) receive a representation of the current object view from the feature extraction, and compare it against the representations of all known category views. Thus, they are continuously reading the database in the search for an update to the known categories. As a result, the larger the size of the database, the slower the reading of the complete set of categories. However, in the case of nodelets, this deterioration is minor when compared with nodes, since accessing the database is much more efficient. Fig. 6 shows the memory usage of the system. Notice that at the end of the experiment the memory size would be above 1 MB if all object point clouds extracted by the trackers would be stored (roughly 5 Kb/s). In a continuously running system, this rate of data accumulation would be hard to handle, and would not bring any real benefit. The total size of the point clouds of all the selected key views is much smaller (one order of magnitude in this experiment). The data actually accumulated in memory (shape representations based on spin-images) is even smaller. Fig. 7 shows the evolution of object recognition performance throughout the experiment. When the first Mug (T1) is placed on the table the system recognizes it as Unknown. After some time, the instructor labels T1 as a Mug and the system starts displaying a precision of 1.0. However, the recall score is under 0.2, because the system classified T1 as Unknown several times before the user labeled the object. After the labeling, the recall starts improving continuously. The instructor then places a Vase (T2) on the table. Because the category Vase has not been taught yet, the performance goes down. After labeling T2 as Vase, performance starts going up again. When a second Mug (T3) enters the scene, the system can correctly recognize it and the scores continue to increase. Then, a Plate (T4) enters the scene, causing recall to drop. Successively, the Plate is taught, a Bottle is placed on the table and then taught, and eventually performance starts going up again. This illustrates the process of acquiring categories in an open-ended fashion with user mediation. 6.2 Off-line evaluation of the perceptual learning approach More systematic experiments have been performed to evaluate the object category learning and recognition approach. An object dataset has been acquired, which contains 339 views of 10 categories of objects: bottle, bowl, flask, fork, knife, mug, plate, spoon, teapot and vase. In addition, there are 31 views of unknown and false objects. The point clouds of the objects were segmented using an offline version of the Object Detection (OD) module. Detected objects were manually labeled. The performance of the system was measured using a leave-one-out cross validation scheme. A total of 24 experiments were performed for different values of four parameters of the system, namely the voxel size (VS) which is related to number of key points extracted from each object view, the image width (IW) and support length (SL) of spin images, and the classification threshold (CT). Results are presented in Table 3 . The parameters that obtained the best average F1 score were selected as the default system parameters. They are the following: VS=0.03, IW=8, SL=0.1 and CT=2. The F-measure of the proposed system with the default parameters was 0.94. Results show that the overall performance of the recognition system is promising. Spin images are capable of collecting distinctive traits of the local surface patches of each object. 6.3 Open-ended learning evaluation Although there are well established methodologies to evaluate learning systems, e.g., k-fold cross validation, leave-one-out, etc., these approaches follow the classical train-and-test procedure, i.e., two separate stages, training followed by testing. Training is accomplished offline, and once it is complete the testing is performed. These methodologies are not well suited to evaluate open-ended learning systems, because they do not abide to the simultaneous nature of learning and recognition and because the number of categories must be predefined. A teaching protocol for evaluating open-ended learning systems was proposed in [17,42]. The protocol relies on three basic actions to interact with an object recognition system: teach, used for teaching a new object category; ask, used to ask the system what is the category of an object view; and correct, used for providing the system with an additional (labeled) view of an existing category. The idea is to continuously ask the system to recognize previously unseen views of known categories and provide corrections when needed. This way, the system is trained, and at the same time the recognition performance of the system is continuously estimated. A simulated teacher was developed to automate experiments following the teaching protocol [3]. To operate, the simulated teacher must be connected to a data-set of object views. In this work, we use the Washington RGB-D Object Dataset [43]. This dataset contains images of 300 common household objects from 51 categories. In the experiments presented, the system begins without category knowledge, i.e., it knows zero categories at start, and the training instances are gradually given to the system. Thus, object category models are incrementally built. Although the evaluation protocol is designed to test the system only using views of known categories, the system is designed to identify unknown categories when the views to be recognized are too far from all models in memory. Therefore, we use F-measure, which combines precision and recall, as the indicator of recognition performance. Protocol success is a local F-measure, computed in a sliding window of size 3 n (as defined in [42]), where n is the number of categories that have already been introduced. According to the teaching protocol, the system is ready to learn a new object category when the protocol success is above a threshold (0.67 in all experiments presented), and at least one instance of every known category has been tested. Simulated teacher experiments can be used to evaluate the performance of open-ended learning systems using several measures, namely: • The number of learned categories at the end of an experiment, an indicator of how much the system is capable of learning; • The number of question/correction iterations required to learn those categories and the average number of stored instances per category, indicators of time and memory resources required for learning; • Global success, an F-measure computed using all predictions in a complete experiment, and the (local) protocol success defined above, indicators of how well the system learns. Since the order of introducing new categories may affect the performance of the system, ten experiments were performed using random sequences of introduction of categories. Table 4 summarizes the 10 experiments. Fig. 8 (a) shows the evolution of the teaching protocol success in experiment 3. The introduced categories are signaled in the plot. Fig. 8(b) shows the protocol success as a function of the number of learned categories. Fig. 9 (a) shows the global success (i.e. since the beginning of the experiment) as a function of the number of learned categories. In this figure we can see that the global success decreases as more categories are learned. This is expected since the number of categories known by the system makes the classification task more difficult. Finally, Fig. 9(b) shows the number of learned categories as a function of the number of protocol iterations. This gives a measure of how fast the learning occurred in each of the experiments. 7 Conclusions This paper describes the 3D object perception and learning system developed for the RACE project. The system is designed to detect and track tabletop objects. It is also capable of classifying the tracked objects according to the object categories that are known by the system. Furthermore, this object category knowledge may be enhanced in real time through an RVIZ interface that enables humans to teach additional object categories. The paper describes a dual memory approach in which two databases with different characteristics are used to store semantic and perceptual data. The proposed nodelet software architecture is designed for efficiency purposes, since it enables actual simultaneous access to the perceptual database. Results show that the nodelet based approach is significantly faster when compared to the standard node approach. In addition, we also propose a multiplexed object perception pipeline, in which a set of nodes is launched during execution to handle each newly detected object. This mechanism allocates separate computational resources for each tracked object, which creates a dynamic computation graph in run time, and facilitates the parallelization of processing. As a consequence, the system is capable of simultaneously tracking several objects moving in the scene. The RACE object perception and learning system contains a large number of nodes that process 3D data for different purposes, e.g., segmentation, feature extraction, tracking, etc. Thus, it also serves as a good example of how PCL functionalities can be used to create an efficient and complex 3D perception system. This paper presented a perceptual memory system designed to enable grounding of object symbols and object category symbols in an open ended fashion. This system is integrated in a dual memory architecture which also includes a semantic memory component. The dual memory approach contributes to the optimization of both the semantic and perceptual components, and is in line with findings in cognitive science regarding the human memory system. The perceptual memory implementation was carried out using a lightweight, NoSQL database which, when combined with a nodelet based infrastructure, allows the simultaneous access of several modules to the storage. The system also supports runtime multiplexing of the object perception pipelines, which leads to the parallelization of the bottlenecks in the data processing. Results show that the perceptual memory combined with a nodelet infrastructure significantly outperforms a node based approach. The presented architecture can seamlessly integrate user-mediated experience acquisition, conceptualization and recognition. The system is open-ended since it can continuously acquire new object categories. Because the system is open-ended and receives a continuous stream of data, ongoing work is using data stream clustering methods to update a dictionary of local features [44]. Acknowledgments This work was funded by the EC 7th FP theme FP7-ICT-2011-7, Grant agreement No. 287752 (project RACE — Robustness by Autonomous Competence Enhancement). We would like to thank the other RACE project partners for their efforts in the integration and the demonstrations, and especially to the Technical Aspects of Multimodal Systems (TAMS) group, University of Hamburg, for making the PR2 robot available to the project. References [1] S. Rockel, B. Neumann, J. Zhang, K.S.R. Dubba, A.G. Cohn, S˘. Konec˘ný, M. Mansouri, F. Pecora, A. Saffiotti, M. Günther, S. Stock, J. Hertzberg, A.M. Tomé, A.J. Pinho, L. Seabra Lopes, S. von Riegen, L. Hotz, An ontology-based multi-level robot architecture for learning from experiences, in: Designing Intelligent Robots: Reintegrating AI II, AAAI Spring Symposium on, Stanford, USA, 2013. [2] J. Hertzberg J. Zhang L. Zhang S. Rockel B. Neumann J. Lehmann K.S.R. Dubba A.G. Cohn A. Saffiotti F. Pecora M. Mansouri Š Konec˘ný M. Günther S. Stock L. Seabra Lopes M. Oliveira G.H. Lim H. Kasaei V. Mokhtari L. Hotz W. Bohlken The RACE project KI—Künstliche Intelligenz 28 4 2014 297 304 [3] H. Kasaei M. Oliveira G.H. Lim L. Seabra Lopes A.M. Tomé Interactive open-ended learning for 3D object recognition: An approach and experiments J. Intell. Robot. Syst. 2015 1 17 [4] K. Dubba, M. de Oliveira, G. Lim, H. Kasaei, L. Seabra Lopes, A. Tomé, A. Cohn, Grounding language in perception for scene conceptualization in autonomous robots, in: AAAI 2014 Spring Symposium on Qualitative Representations for Robots, 2014. [5] V. Mokhtari~Hassanabad G.H. Lim L. Seabra~Lopes A.J. Pinho Gathering and conceptualizing plan-based robot activity experiences E. Menegatti N. Michael K. Berns H. Yamaguchi Intelligent Autonomous Systems 13: Proceedings of the 13th International Conference IAS-13 Advances in Intelligent Systems and Computing vol. 302 2015 Springer [6] S. Cousins B. Gerkey K. Conley W. Garage Welcome to ROS topics IEEE Robot. Autom. Mag. 17 1 2010 12 14 [7] S. Cousins B. Gerkey K. Conley W. Garage Sharing software with ROS IEEE Robot. Autom. Mag. 17 2 2010 12 14 [8] D.S. Nau T.-C. Au O. Ilghami U. Kuter J.W. Murdock D. Wu F. Yaman Shop2: An htn planning system J. Artificial Intelligence Res. 20 2003 379 404 [9] L. Barsalou Perceptual symbol systems Behav. Brain Sci. 22 4 1999 577 609 [10] M. Oliveira G.H. Lim L. Seabra Lopes H. Kasaei A. Tome A. Chauhan A perceptual memory system for grounding semantic representations in intelligent service robots Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2014 IEEE Chicago, Illinois [11] G.H. Lim I.H. Suh H. Suh Ontology-based unified robot knowledge for service robots in indoor environments Systems, IEEE Trans. Syst. Man Cybern. 41 3 2011 492 509 [12] S. Coradeschi A. Saffiotti An introduction to the anchoring problem Robot. Auton. Syst. 43 2–3 2003 85 96 special issue on perceptual anchoring [13] S. Zaman G. Steinbauer J. Maurer P. Lepej S. Uran An integrated model-based diagnosis and repair architecture for ROS-based robot systems IEEE International Conference on Robotics and Automation 2013 Karlsruhe Germany [14] E. Tulving Concepts of human memory L. Squire G. Lynch N. Weinberger J. McGaugh Memory: Organization and Locus of Change 1991 Oxford Univ. Press 3 32 [15] E. Tulving Episodic memory and autonoesis: Uniquely human? J.M.H.S. Terrace The Missing Link in Cognition 2005 Oxford Univ. Press NewYork, NY 4 56 [16] R. Wood P. Baxter T. Belpaeme A review of long-term memory in natural and synthetic systems Adapt. Behav. 20 2011 81 103 [17] L. Seabra~Lopes A. Chauhan How many words can my robot learn?: An approach and experiments with one-class learning Interact. Stud. 8 1 2007 53 81 [18] L. Seabra~Lopes A. Chauhan Open-ended category learning for language acquisition Connect. Sci. 20 4 2008 277 297 [19] S. Kirstein H. Wersing H.-M. Gross E. Körner A life-long learning vector quantization approach for interactive learning of multiple categories Neural Netw. 28 2012 90 105 [20] N. Krüger C. Geib J. Piater R. Petrick M. Steedman F. Wörgötter A. Ude T. Asfour D. Kraft D. Omrčen A. Agostini R. Dillmann Object-action complexes: Grounded abstractions of sensory-motor processes Robot. Auton. Syst. 59 10 2011 740 757 [21] F. Heintz, J. Kvarnstrom, P. Doherty, A stream-based hierarchical anchoring framework, in: Intelligent Robots and Systems, 2009, IROS 2009, IEEE/RSJ International Conference on, 2009, pp. 5254–5260. [22] A. Aldoma Z.-C. Marton F. Tombari W. Wohlkinger C. Potthast B. Zeisl R.B. Rusu S. Gedikli M. Vincze Point cloud library IEEE Robot. Autom. Mag. 1070 9932 2012 [23] A. Clapés M. Reyes S. Escalera Multi-modal user identification and object recognition surveillance system Pattern Recognit. Lett. 34 7 2013 799 808 [24] R.B. Rusu N. Blodow M. Beetz Fast point feature histograms (FPFH) for 3D registration Robotics and Automation, 2009, ICRA’09, IEEE International Conference on 2009 IEEE 3212 3217 [25] L. Seabra~Lopes J. Connell Semisentient robots: routes to integrated intelligence IEEE Intell. syst. 16 5 2001 10 14 [26] S. Sahib A review of non relational databases, their types, advantages and disadvantages Int. J. Eng. Technol. 2 2 2013 [27] G.H. Lim M. Oliveira V. Mokhtari S. Hamidreza~Kasaei A. Chauhan L. Seabra Lopes A.M. Tomé Interactive teaching and experience extraction for learning about objects and robot activities Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on 2014 IEEE 153 160 [28] J.S. Evans Dual-processing accounts of reasoning, judgment, and social cognition Ann. Rev. Psychol. 59 1 2008 255 278 [29] R. Rusu, S. Cousins, 3D is here: Point cloud library (PCL), in: Robotics and Automation, ICRA, 2011 IEEE International Conference on, 2011, pp. 1–4. [30] D. Cohen-Or A. Kaufman Fundamentals of surface voxelization Graph. Models Image Process. 57 6 1995 453 461 [31] C.B. Barber D.P. Dobkin H. Huhdanpaa The Quickhull algorithm for convex hulls ACM Trans. Math. Softw. 22 4 1996 469 483 [32] N.M. Amato, F.P. Preparata, An NC parallel 3D convex hull algorithm, in: Proceedings of the ninth annual symposium on Computational geometry, SCG’93, 1993, pp. 289–297. [33] M.A. Fischler R.C. Bolles Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography Commun. ACM 24 6 1981 381 395 [34] R.B. Rusu Semantic 3D object maps for everyday manipulation in human living environments (Ph.D. thesis) 2009 Computer Science department, Technische Universitaet Muenchen Germany October [35] W. Yi S. Marshall Principal component analysis in application to object orientation Geo-spatial Inf. Sci. 3 3 2000 76 78 [36] Y. Salih, A. Malik, 3D tracking using particle filters, in: Instrumentation and Measurement Technology Conference, I2MTC, 2011 IEEE, 2011, pp. 1–4. [37] A. Johnson M. Hebert Using spin images for efficient object recognition in cluttered 3D scenes IEEE Trans. Pattern Anal. Mach. Intell. 21 5 1999 433 449 [38] M. Connor P. Kumar Fast construction of k-nearest neighbor graphs for point clouds IEEE Trans. Vis. Comput. Graphics 16 4 2010 599 608 [39] R. Rusu, N. Blodow, Z. Marton, M. Beetz, Close-range scene segmentation and reconstruction of 3D point cloud maps for mobile manipulation in domestic environments, in: Intelligent Robots and Systems, 2009, IROS 2009, IEEE/RSJ International Conference on, 2009, pp. 1–6. [40] M. Munaro F. Basso S. Michieletto E. Pagello E. Menegatti A software architecture for RGB-D people tracking based on ROS framework for a mobile robot Frontiers of Intelligent Autonomous Systems Studies in Computational Intelligence vol. 466 2013 53 68 [41] J.L. Bentley Multidimensional binary search trees used for associative searching Commun. ACM 18 9 1975 509 517 [42] A. Chauhan L. Seabra~Lopes Using spoken words to guide open-ended category formation Cogn. Process. 12 2011 341 354 [43] K. Lai, L. Bo, X. Ren, D. Fox, A large-scale hierarchical multi-view RGB-D object dataset, in: Robotics and Automation, ICRA, 2011 IEEE International Conference on, 2011, pp. 1817–1824. [44] M. Oliveira, L. Seabra Lopes, G.H. Lim, H. Kasaei, A.D. Sappa, A. Tome, A. Chauhan, Concurrent learning of visual codebooks and object categories in open-ended domains, in: Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, IEEE (in press). Miguel Oliveira received the B.Sc., and M.Sc., degrees in Mechanical Engineering from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering with specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Luís Seabra Lopes is an Associate Professor of Informatics in the Department of Electronics, Telecommunications and Informatics of the University of Aveiro, Portugal. He received a Ph.D. in Robotics and Integrated Manufacturing from the New University of Lisbon, Portugal, in 1998. He has longstanding interests in robot learning, cognitive robotic architectures, and human–robot interaction. Gi Hyun Lim received the B.S. degree in Metallurgical Engineering and the M.S. and Ph.D degrees in Electronics and Computer Engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Post-doctoral Researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal. His research interests lie in the area of intelligence and learning for robots, including perception and semantics. From 1997 to 1998, he was an Engineer with the Dongbu Electronics, Ltd., Chungcheongbuk-do, Korea, where he was involved in research on semi-conductor factory automation. From 1999 to 2005, he was a Senior Research Engineer at a venture business, where he was involved in research on real-time operating systems and embedded systems. S. Hamidreza Kasaei obtained B.Sc. (2010) and M.Sc. (2012) in Computer Engineering field of Artificial Intelligence from the University of Isfahan (Iran). Currently he is a Ph.D. student at the University of Minho, Aveiro and Porto (Portugal), where he works on 3D object category learning and recognition in open-ended domains as a research student at the Institute of Electronics and Telematics Engineering of Aveiro. He worked on Middle size soccer robot and Humanoid robot and obtained different ranks in Robocup competition. His main research interest are in computer vision, robotics and multi agent systems. Ana Maria Tomé is an Associate Professor of Electrical Engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as machine learning applications. Aneesh Chauhan is a Post-doctoral Researcher in the Computer Vision Group at the Centre of Automatics and Robotics at Universidad Politecnica de Madrid. He holds a Bachelor of Engineering degree in Computer Science and Engineering from Baba Ambedkar Marathwada University, Maharashtra, India, a Master of Science degree in Autonomous Systems from the University of Exeter, UK and Ph.D. in Informatics Engineering from Universidade de Aveiro, Aveiro, Portugal. His research interests include intelligent robotics, language grounding, human–robot interaction as well as the application of computer vision and machine learning approaches for autonomous perception tasks. "
    },
    {
        "doc_title": "Empirical mode decomposition for noninvasive atrial fibrillation dominant frequency estimation",
        "doc_scopus_id": "84963968457",
        "doc_doi": "10.1109/EUSIPCO.2015.7362851",
        "doc_eid": "2-s2.0-84963968457",
        "doc_date": "2015-12-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Accurate estimation",
            "Atrial fibrillation",
            "Cardiac arrhythmia",
            "Dominant frequency",
            "Empirical Mode Decomposition",
            "Ensemble empirical mode decompositions (EEMD)",
            "Noninvasive methods",
            "Noninvasive technique"
        ],
        "doc_abstract": "© 2015 EURASIP.The dominant frequency (DF) of the atrial activity signal is arguably one of the most relevant features characterizing atrial fibrillation (AF), the most common cardiac arrhythmia. Its accurate estimation from noninvasive acquisition modalities such as the electrocardiogram (ECG) can avoid risks of potential complications to patients in a cost-effective manner. However, the approximation of the underlying intracardiac atrial activity by noninvasive techniques such as average beat subtraction or blind source separation has not always been satisfactory. In the present work, a new approach based on the ensemble empirical mode decomposition (EEMD) is proposed for AF DF estimation. Our results suggest that EEMD provides more accurate estimates of intracardiac AF DF than alternative noninvasive methods. In addition, the empirical nature of EEMD overcomes important drawbacks of other techniques, simplifying its implementation in automatic tools for diagnosis aid.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Concurrent learning of visual codebooks and object categories in open-ended domains",
        "doc_scopus_id": "84958162360",
        "doc_doi": "10.1109/IROS.2015.7353715",
        "doc_eid": "2-s2.0-84958162360",
        "doc_date": "2015-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Bag-of-words models",
            "Gaussian Mixture Model",
            "Multiple levels",
            "Object categories",
            "On-line fashion",
            "Updated using",
            "Visual object recognition",
            "Visual objects"
        ],
        "doc_abstract": "© 2015 IEEE.In open-ended domains, robots must continuously learn new object categories. When the training sets are created offline, it is not possible to ensure their representativeness with respect to the object categories and features the system will find when operating online. In the Bag of Words model, visual codebooks are usually constructed from training sets created offline. This might lead to non-discriminative visual words and, as a consequence, to poor recognition performance. This paper proposes a visual object recognition system which concurrently learns in an incremental and online fashion both the visual object category representations as well as the codebook words used to encode them. The codebook is defined using Gaussian Mixture Models which are updated using new object views. The approach contains similarities with the human visual object recognition system: evidence suggests that the development of recognition capabilities occurs on multiple levels and is sustained over large periods of time. Results show that the proposed system with concurrent learning of object categories and codebooks is capable of learning more categories, requiring less examples, and with similar accuracies, when compared to the classical Bag of Words approach using codebooks constructed offline.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Empirical mode decomposition of multiple ECG leads for catheter ablation long-term outcome prediction in persistent atrial fibrillation",
        "doc_scopus_id": "84953256634",
        "doc_doi": "10.1109/EMBC.2015.7318311",
        "doc_eid": "2-s2.0-84953256634",
        "doc_date": "2015-11-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Aged",
            "Area Under Curve",
            "Atrial Fibrillation",
            "Catheter Ablation",
            "Electrocardiography",
            "Female",
            "Humans",
            "Logistic Models",
            "Male",
            "Middle Aged",
            "Prognosis",
            "ROC Curve"
        ],
        "doc_abstract": "© 2015 IEEE.Predictive models arouse increasing interest in clinical practice, not only to improve successful intervention rates but also to extract information of diverse physiological disorders. This is the case of persistent atrial fibrillation (AF), the most common cardiac arrhythmia in adults. Currently, catheter ablation (CA) is one of the preferred therapies to face this disease. However, selecting the best responders to CA by standard noninvasive techniques such as the electrocardiogram (ECG) remains a challenge. This work presents different predictive models for determining long-term CA outcome based on the dominant frequency (DF) of atrial activity measured in the ECG. The ensemble empirical mode decomposition (EEMD) is employed to obtain the intrinsic mode functions (IMFs) composing the ECG signal in each lead. The IMF DFs computed in multiple leads are then combined into a logistic regression (LR) model. The IMF DF features are discriminant enough to reach 79% accuracy for long-term CA outcome prediction, outperforming other methods based on DF computation. Our study shows EEMD as a valuable alternative to extract clinically relevant spectral information from AF ECGs and confirms the advantage of LR to build multivariate predictive models as compared with univariate analysis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From binary NMF to variational bayes NMF: A probabilistic approach",
        "doc_scopus_id": "84956667895",
        "doc_doi": "10.1007/978-3-662-48331-2_1",
        "doc_eid": "2-s2.0-84956667895",
        "doc_date": "2015-09-25",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Failure patterns",
            "Intrinsic dimensions",
            "Logistic functions",
            "Manufacturing process",
            "Nonnegative matrix factorization",
            "Optimality criteria",
            "Probabilistic approaches",
            "Variational bayes"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 2016. All rights reserved.Asurvey of our recentwork on probabilisticNMFis provided. All variants discussed here are illustrated by their application to the analysis of failure patterns emerging frommanufacturing and processing siliconwafers. It startswith binNMF, a variant developed to applyNMFto binary data sets. The latter are modeled as a probabilistic superposition of a finite number of intrinsic continuous-valued failure patterns characteristic for the manufacturing process. We further discuss related theoretical work on a semi-non-negative matrix factorization based on the logistic function, which we called logistic NMF. While addressing uniqueness issues, we propose a Bayesian Optimality Criterion for NMF and a determinant criterion to geometrically constrain the solutions of NMF problems, leading to detNMF. This approach also provides an intuitive explanation for the often used multilayer approach. Finally, we present a Variational Bayes NMF (VBNMF) algorithm which represents a generalization of the famous Lee-Seung method.We also demonstrate its ability to estimate the intrinsic dimension (model order) of the NMF method.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "EMDLAB: A toolbox for analysis of single-trial EEG dynamics using empirical mode decomposition",
        "doc_scopus_id": "84937118241",
        "doc_doi": "10.1016/j.jneumeth.2015.06.020",
        "doc_eid": "2-s2.0-84937118241",
        "doc_date": "2015-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Neuroscience (all)",
                "area_abbreviation": "NEUR",
                "area_code": "2800"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Brain",
            "Electroencephalography",
            "Electromyography",
            "Humans",
            "Nonlinear Dynamics",
            "Signal Processing, Computer-Assisted",
            "Software"
        ],
        "doc_abstract": "© 2015 Elsevier B.V.Background: Empirical mode decomposition (EMD) is an empirical data decomposition technique. Recently there is growing interest in applying EMD in the biomedical field. New method: EMDLAB is an extensible plug-in for the EEGLAB toolbox, which is an open software environment for electrophysiological data analysis. Results: EMDLAB can be used to perform, easily and effectively, four common types of EMD: plain EMD, ensemble EMD (EEMD), weighted sliding EMD (wSEMD) and multivariate EMD (MEMD) on EEG data. In addition, EMDLAB is a user-friendly toolbox and closely implemented in the EEGLAB toolbox. Comparison with existing methods: EMDLAB gains an advantage over other open-source toolboxes by exploiting the advantageous visualization capabilities of EEGLAB for extracted intrinsic mode functions (IMFs) and Event-Related Modes (ERMs) of the signal. Conclusions: EMDLAB is a reliable, efficient, and automated solution for extracting and visualizing the extracted IMFs and ERMs by EMD algorithms in EEG study.",
        "available": true,
        "clean_text": "serial JL 271055 291210 291735 31 Journal of Neuroscience Methods JOURNALNEUROSCIENCEMETHODS 2015-07-08 2015-07-08 2015-07-20 2015-07-20 2017-01-02T14:04:05 S0165-0270(15)00244-7 S0165027015002447 10.1016/j.jneumeth.2015.06.020 S300 S300.2 FULL-TEXT 2017-01-02T10:00:26.443824-05:00 0 0 20150930 2015 2015-07-08T01:00:26.53972Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure e-component body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast grantsponsor grantsponsorid highlightsabst primabst pubtype ref 0165-0270 01650270 true 253 253 C Volume 253 20 193 205 193 205 20150930 30 September 2015 2015-09-30 2015 article fla Copyright © 2015 Elsevier B.V. All rights reserved. EMDLABATOOLBOXFORANALYSISSINGLETRIALEEGDYNAMICSUSINGEMPIRICALMODEDECOMPOSITION ALSUBARI K 1 Introduction 2 EEGLAB and EMDLAB toolboxes 2.1 EEGLAB toolbox 2.2 EMDLAB toolbox 3 Methods 3.1 Empirical mode decomposition 3.2 Ensemble empirical mode decomposition 3.3 Weighted sliding empirical mode decomposition 3.4 Multivariate empirical mode decomposition 4 Results 4.1 Run EMD 4.2 Analyze and visualize modes 5 Conclusion Acknowledgement Appendix A Supplementary data References ACAR 2010 258 270 Z ALBADDAI 2014 218 236 S ALSUBARI 2015 e0119489 K ATTOHOKINE 2008 N DEXIANG 2008 2131 2134 Z 2NDINTERNATIONALCONFERENCEBIOINFORMATICSBIOMEDICALENGINEERING2008ICBBE2008 EEGSIGNALPREPROCESSINGBASEDEMPIRICALMODEDECOMPOSITION DELORME 2004 9 21 A DELORME 2011 12 A DURKA 2005 49 59 P FALTERMEIER 2011 509 526 R FALTERMEIER 2010 1 9 R 2010INTERNATIONALJOINTCONFERENCENEURALNETWORKSIJCNN SLIDINGEMPIRICALMODEDECOMPOSITION FLANDRIN 2004 112 114 P HANSLMAYR 2013 1 6 S HU 2012 1075 1086 X HUANG 1998 903 995 N HUANG 2008 RG2006 N KIM 2009 40 46 D KOTHE 2012 C BCILAB KOTHE 2013 056014 C LANG 2011 26 47 E EXPLORATORYMATRIXFACTORIZATIONTECHNIQUESFORLARGESCALEBIOMEDICALDATASETS LANG 2012 21 E LOONEY 2015 D MAKEIG 2004 204 210 S MALLAT 1993 3397 3415 S MANDIC 2013 74 86 D MARPLE 1999 2600 2603 S MULLEN 2010 T SOCIETYFORNEUROSCIENCECONFERENCE ELECTROPHYSIOLOGICALINFORMATIONFLOWTOOLBOXFOREEGLAB MUTLU 2011 1 13 A REHMAN 2009 1291 1302 N REHMAN 2011 2421 2426 N REHMAN 2013 1350007 N RILLING 2003 8 11 G IEEEEURASIPWORKSHOPNONLINEARSIGNALIMAGEPROCESSING EMPIRICALMODEDECOMPOSITIONALGORITHMS RUTKOWSKI 2010 215 229 T RUTKOWSKI 2006 1232 1239 T SHATTUCK 2007 D WANG 2005 213 244 Q COMPARISONHILBERTHUANGWAVELETFOURIERTRANSFORMSFORSELECTEDAPPLICATIONS WU 2004 1597 1611 Z WU 2009 1 41 Z ZEILER 2013 21 32 A ALSUBARIX2015X193 ALSUBARIX2015X193X205 ALSUBARIX2015X193XK ALSUBARIX2015X193X205XK 2017-01-20T00:00:00Z UnderEmbargo item S0165-0270(15)00244-7 S0165027015002447 10.1016/j.jneumeth.2015.06.020 271055 2017-01-02T10:00:26.443824-05:00 2015-09-30 true 5471426 MAIN 13 50103 849 656 IMAGE-WEB-PDF 1 gr1 3656 164 105 gr10 25591 164 201 gr11 28260 164 184 gr12 19323 164 131 gr13 11580 104 219 gr14 17522 148 219 gr15 17874 90 219 gr16 15286 90 219 gr17 25096 102 219 gr18 29319 164 184 gr19 26982 164 200 gr2 8898 161 219 gr20 21106 164 182 gr21 15166 146 219 gr3 14248 141 219 gr4 11427 164 219 gr5 10292 161 219 gr6 14097 164 217 gr7 20403 164 215 gr8 10595 63 219 gr9 19361 84 219 gr1 31295 590 377 gr10 85438 307 376 gr11 95438 343 386 gr12 90489 483 386 gr13 41934 231 489 gr14 60175 331 489 gr15 120064 308 753 gr16 83507 310 753 gr17 141299 350 753 gr18 84966 335 376 gr19 89113 308 376 gr2 73704 416 565 gr20 74224 338 376 gr21 40713 250 376 gr3 34584 243 376 gr4 79495 418 558 gr5 73269 416 565 gr6 113609 427 565 gr7 53259 286 376 gr8 34142 161 565 gr9 126410 289 753 gr1 270127 2610 1669 gr10 606999 1361 1667 gr11 575823 1518 1708 gr12 600805 2136 1708 gr13 242734 1025 2167 gr14 375849 1466 2167 gr15 910305 1365 3333 gr16 565215 1374 3333 gr17 1031435 1549 3333 gr18 565297 1484 1667 gr19 724734 1367 1667 gr2 532311 1841 2501 gr20 445105 1498 1667 gr21 227303 1108 1667 gr3 236476 1291 2000 gr4 580325 1851 2472 gr5 535028 1841 2501 gr6 860322 1891 2500 gr7 260126 1268 1667 gr8 197679 714 2500 gr9 1026052 1279 3333 mmc1 mmc1.flv flv 245474385 VIDEO-FLASH mmc2 mmc2.zip zip 3992051 APPLICATION mmc1 mmc1.mp4 mp4 92555516 VIDEO mmc1 75108 313 565 IMAGE-MMC-DOWNSAMPLED mmc1 22961 121 219 IMAGE-MMC-THUMBNAIL si1 1036 50 210 si10 235 19 42 si2 1228 39 295 si3 1939 41 440 si4 442 22 83 si5 1671 59 289 si6 830 16 197 si7 1120 39 256 si8 208 11 38 si9 283 15 50 NSM 7265 S0165-0270(15)00244-7 10.1016/j.jneumeth.2015.06.020 Elsevier B.V. Fig. 1 Flowchart diagram of the EMD algorithm. Fig. 2 EMD decomposition of EEG signal. Fig. 3 Hilbert and Fourier spectrum of ERMs of an EEG signal. Top: Hilbert spectrum of all ERMs, colorbar indicates the frequency of ERMs. Down: Fourier spectrum of all ERMs, different colors refer to different ERMs. Fig. 4 EEMD decomposition of EEG signal. Fig. 5 EEG signal followed by its intrinsic mode functions (IMFs) using wSEMD decomposition. Here the window size M =50 corresponds to Δt =40ms. Fig. 6 Decomposition of EEG signal by employing MEMD with noise assisted. Fig. 7 The main EEGLAB graphical user interface, with the EMDLAB menu activated. The datasets menu presents a list of currently active EEG sets, and the EMDLAB menu shows a list of currently active sets. Fig. 8 The EMDLAB main user interface. This window is used for EMD decomposition. Through this window, user can choose the data, algorithm and appropriate parameters for the decomposition. Fig. 9 Data and its extracted mode (IMF3) scrolling. Here, five data epochs (separated by dashed lines) are plotted at 5 electrode sites (channel names on the left). The arrow buttons scroll horizontally through the data. Fig. 10 The mode spectra and associated topographical maps. The figure shows the power spectrum of the 62 channels and the scalp power maps at specific frequencies. Each colored trace represents the spectrum of the activity of IMF4 of one data channel. The scalp maps indicate the distribution of power at specified frequencies 5.9, 9.8 and 22.5Hz. Fig. 11 Topographical 2-D scalp maps of ERM4 at different latencies. They represent distribution of the activity (given by the scaled color bar) over the head. Dots overlaid on the scalp maps indicate channels. Fig. 12 IMF4 properties of channel 10 (O2). Top left: The head plot (top left) containing a red dot indicates the position of the selected channel. Top right: ERP image of IMF4, each horizontal line in this colored image representing IMF4 of a single trial in an event-related dataset. The trace below the ERM image shows the average of the IMF4 of a single-trial activity, i.e. the ERM average of the imaged data epochs. Bottom panel: the activity power spectrum of IMF4 for channel 10. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 Hilbert–Huang and Fourier spectrum user interface. Fig. 14 Hilbert and Fourier spectrum for ERM4–ERM7 of an EEG signal. Top: Hilbert spectrum of selected ERMs, colorbar indicates the instantaneous amplitude of ERMs. Down: Fourier spectrum of selected ERMs, different colors refer to different ERMs. Fig. 15 From left to right, top to bottom: Hilbert spectrum image of IMF1–IMF6 of O2 single channel. It is a color-coded image of IMF1–IMF6 of all single trials. Colorbars indicate the instantaneous frequency of each IMF image. Fig. 16 From left to right, top to bottom: Fourier spectrum image of IMF1–IMF6 of O2 single channel. It is a color-coded image of IMF1–IMF6 of all single trials. Colorbars indicate the amplitude of each IMF image. Fig. 17 Hilbert and Fourier spectrum image of IMF5 of O2 channel. Left: Hilbert spectrum image of IMF5 at selected frequency [1 12]. Right: Fourier Spectrum image of IMF5 at the same frequencies. Fig. 18 Plot of ERM5 image of O2 channel. It is a rectangular colored image in which every horizontal line represents activity of IMF5 of a single experimental trial. The signal below the image shows the ERM average of the imaged IMF5 of data epochs. Fig. 19 ERM4 waveforms of 62 channels and their scalp maps at specified latencies. Top: Scalp maps show the topographic distribution of average potential of IMF4 at 50, 100, 150, 200ms. Down: Each trace in the waveforms plots the averaged ERM4 at one channel. Fig. 20 62 different ERM plots. Each plot represent ERM5 of two datasets: contour and non-contour conditions and the difference between them. Fig. 21 ERM5 of contour and non-contour conditions and the difference between them for O2 channel. Computational Neuroscience EMDLAB: A toolbox for analysis of single-trial EEG dynamics using empirical mode decomposition K. Al-Subari a b S. Al-Baddai a b ⁎ A.M. Tomé c M. Goldhacker a d R. Faltermeier e E.W. Lang a a CIML Group, Institute of Biophysics, University of Regensburg, 93040 Regensburg, Germany CIML Group, Institute of Biophysics, University of Regensburg Regensburg 93040 Germany b Institute of Information Science, University of Regensburg, Germany Institute of Information Science, University of Regensburg Germany c IEETA, DETI, Universidade de Aveiro, 3810-193 Aveiro, Portugal IEETA, DETI, Universidade de Aveiro Aveiro 3810-193 Portugal d Institute of Experimental Psychology, University of Regensburg, Germany Institute of Experimental Psychology, University of Regensburg Germany e Clinic of Neurosurgery, University Hospital Regensburg, Germany Clinic of Neurosurgery, University Hospital Regensburg Germany ⁎ Corresponding author at: CIML Group, Institute of Biophysics, University of Regensburg, 93040 Regensburg, Germany. Tel.: +49 941 943 2599. Background Empirical mode decomposition (EMD) is an empirical data decomposition technique. Recently there is growing interest in applying EMD in the biomedical field. New method EMDLAB is an extensible plug-in for the EEGLAB toolbox, which is an open software environment for electrophysiological data analysis. Results EMDLAB can be used to perform, easily and effectively, four common types of EMD: plain EMD, ensemble EMD (EEMD), weighted sliding EMD (wSEMD) and multivariate EMD (MEMD) on EEG data. In addition, EMDLAB is a user-friendly toolbox and closely implemented in the EEGLAB toolbox. Comparison with existing methods EMDLAB gains an advantage over other open-source toolboxes by exploiting the advantageous visualization capabilities of EEGLAB for extracted intrinsic mode functions (IMFs) and Event-Related Modes (ERMs) of the signal. Conclusions EMDLAB is a reliable, efficient, and automated solution for extracting and visualizing the extracted IMFs and ERMs by EMD algorithms in EEG study. Keywords EMDLAB EEGLAB toolbox EEG Empirical mode decomposition Intrinsic mode functions 1 Introduction Brain states analysis using non-invasive monitoring techniques such as electroencephalogram (EEG) have been receiving much attention because of increasing interest and popularity of research related to brain computer/machine interfacing (BCI/BMI) methods, due to the very interesting possibility of computer-aided communication with the outside world (Rutkowski et al., 2006). Therefore, this technology is expected to be at the core of future intelligent computing. Recently, several new signal processing methods have been used in EEG signal processing (Makeig et al., 2004). Most of these new methods need new tools to adapt routines of EEG data processing. Additionally, this sort of interactive tools makes the possibility of analysis the multimodal EEG data collected using more complex experimental designs much easier, for example a Neuroelectromagnetic Forward Head Modeling Toolbox (NFT) (Acar and Makeig, 2010), a Source Information Flow Toolbox (SIFT) (Mullen et al., 2010; Delorme et al., 2011), Measure Projection Toolbox (MPT) (Shattuck et al., 2007) and Brain-Computer Interface LAP toolbox (BCILAB) (Kothe and Makeig, 2013; Kothe, 2012). These toolboxes are integrated with the well-established EEGLAB software environment (Delorme and Makeig, 2004), an interactive menu-based and scripting software for processing electrophysiological (EEG) data under the MATLAB interpreted programming script environment. Lately, Empirical Mode Decomposition (EMD) has been applied to both nonlinear and non-stationary mono-variate time series in many areas (Huang et al., 1998). It locally decomposes any non-linear and non-stationary time series into simple oscillations on various frequency scales. Note that a local oscillation is understood here as a signal which locally exhibits a maximum and a minimum separated by a zero-crossing. As such, riding waves are not considered simple oscillations. The resulting oscillatory modes are called intrinsic mode functions (IMFs), and if combined with Hilbert transform, is also called Hilbert–Huang transform. Furthermore, EMD has proven its efficiency in analyzing EEG data (De-xiang et al., 2008; Rutkowski et al., 2010). Because EMD operates on sequences of local extremes, and the decomposition is carried out by direct extraction of the local energy associated with the intrinsic time scales of the signal itself, the method is thus similar to traditional Fourier or Wavelet decompositions. Alternatively, a matching pursuit (MP) algorithm (Mallat and Zhang, 1993; Durka et al., 2005) finds a sub-optimal solution to the problem of an adaptive approximation of a signal in a redundant set, also called dictionary, of deliberately chosen functions. EMD differs from the wavelet-based multi-scale analysis, however, which characterizes the scale of a signal event using pre-specified basis functions. It also differs from a matching pursuit approach by not having to rely on pre-specified dictionaries. Rather the IMFs are generated from the data in an adaptive way, hence should form an optimal dictionary to expand the signals under study. EMD suffers from certain shortcomings which are addressed in an ensemble approach, called Ensemble Empirical Mode decomposition (EEMD) (Wu and Huang, 2009). Also, practical applications call for an on-line variant which is provided as weighted sliding EMD (wSEMD) (Faltermeier et al., 2011; Zeiler et al., 2013). Additionally, several mono-variate, i.e. single channel, EEG signals can be processed in parallel representing a multichannel approach where these signals are decomposed in parallel. The extracted modes from each channel can be analyzed further to facilitate a joint understanding of multi-channel EEG recordings. Alternatively, multi-channel EEG recordings can be analyzed in a multi-variate fashion (Rehman and Mandic, 2009). Hence, we propose a new toolbox, called EMDLAB, which serves the growing interest of the signal processing community in applying EMD as a decomposition technique. Traditionally, EEGs are studied at the level of event related potentials (ERPs) which represent averages over a sufficiently large number of single trial recordings. In case of multi-trial data, EMD is applied to data either trial by trial or as single-trial response signal at each channel separately. Then all corresponding IMFs become averaged over all trials to yield event-related modes ERMs. The main goal of EMDLAB toolbox is to extract characteristics of either the EEG signal by IMFs or ERMs. Since IMFs reflect characteristics of the original EEG signal, ERMs reflect characteristics of ERPs of the original signal. In EMDLAB, data structure and visualization of the extracted modes is adopted from the EEGLAB MATLAB toolbox (Delorme and Makeig, 2004), therefore it is suggested to use EMDLAB as a plug-in for EEGLAB. Similarly, the toolbox is primarily designed for EEG data, although both EEGLAB and EMDLAB can process MEG data as well. EMDLAB toolbox offers a comprehensive range of EMD methods, including many popular designs EMD, EEMD, wSEMD, and MEMD. The latter three are extensions of the original EMD algorithm. Each of the EMD algorithms is discussed in the Methods section. Contrary to other toolboxes dedicated to the analysis of ERPs, EMDLAB is used to analyze the characteristic of ERPs based on the extracted modes. This paper is organized as follows: Section EEGLAB and EMDLAB represents an overview of EEGLAB toolbox and the developed plug-in EMDLAB. Next, Section Methods is devoted to a short description of the EMD method and its extensions. In section Results we provide an example of EEG data trials drawn from an experiment of a contour integration task (Al-Subari et al., 2015; Hanslmayr et al., 2013) in order to illustrate the utility of EMDLAB. In this section, the toolbox components, together with some screenshots from the GUI are described. 2 EEGLAB and EMDLAB toolboxes 2.1 EEGLAB toolbox EEGLAB is an interactive toolbox, running under MATLAB, for processing continuous and event-related EEG, MEG and other electrophysiological data (Delorme and Makeig, 2004). It provides a graphical user interface (GUI) which enables users to flexibly and interactively process their EEG and other brain data using independent component analysis (ICA) and/or time/frequency analysis (TFA). This GUI allows users processing their data and tuning their parameters. EEGLAB provides plenty of methods for importing, visualizing, preprocessing data and modeling event-related brain dynamics. Using the GUI of EEGLAB, users can apply advanced signal processing techniques to their data (Delorme and Makeig, 2004) such as: (a) Multiformat data importing. (b) Data visualization through interactive plotting functions. (c) Data processing such as artifact removal, event and channel localization, component extraction, time/frequency analysis and forward/inverse source modeling. (d) Open source plug-in facility. One of the strengths of EEGLAB is offering a programming environment which allows users to distribute newly developed tools and algorithms through EEGLAB plug-in functions. The latter can be downloaded and installed by other users and are available to them within EEGLAB menus. The toolbox also has an important merit by providing an easy-to-follow path to write flexible scripts. Each time a user performs any operation in the GUI, the called function is automatically appended to the EEGLAB command history. 2.2 EMDLAB toolbox The proposed EMDLAB toolbox can be closely implemented into the EEGLAB toolbox via the plug-in facility of the GUI. EMDLAB relies largely on the functions of EEGLAB for importing data and visualizing results. EMDLAB offers additional signal processing modalities – more specifically EMD and its derivates – to be integrated into EEGLAB for the analysis of biomedical signals opening up the possibility to extensively process extracted modes – comparable to the processing of independent components resulting from an ICA, which is traditionally done with EEGLAB. EMDLAB also provides a GUI, which helps the user to handle his data and perform signal processing tasks fitted to EMD analysis. It renders all implemented options immediately available without the need to resort to the documentation. The GUI of EMDLAB is designed in such a way that it also can be used as a teaching tool for beginners. EMDLAB uses the same approach as EEGLAB in a sense that each time the user selects an operation from the menu the function called by MATLAB – including all input parameters – is saved to the command history of the EEGLAB session. 3 Methods EMD is a useful method for analyzing natural signals, specially those which are often non-linear and non-stationary. Decomposing real world signals using EMD, and obtaining their IMFs, is important because natural processes often have many causes, and each of these causes may occur only at certain time intervals. 3.1 Empirical mode decomposition Empirical mode decomposition (EMD) is a purely data-driven analysis tool for complex non-linear and non-stationary time series which has been proposed by Huang et al. (1998). It locally decomposes any non-linear and non-stationary time series into oscillations on various frequency scales. These resulting oscillatory modes are called intrinsic mode functions (IMFs) which can be identified through the following two conditions: • the number of maxima and minima can at most differ by one; • the local mean m(t) should be close to zero everywhere. IMFs are naturally ordered according to their decreasing frequency content, and end with a non-oscillatory trend. The process of creating intrinsic modes by an iterative procedure is called sifting. During the sifting mechanism, an intermediate signal r(t), initially created with a copy of the original time series r(t)= x(t), is processed as follows: 1. Finding local extremes of r(t). 2. Compute two envelopes using local maxima and local minima, respectively. Different interpolation functions can be used (Hu et al., 2012). In this toolbox, a cubic spline interpolation scheme is employed. 3. The average m(t) of the two envelopes is calculated and subtracted from r(t) yielding h(t)= r(t)− m(t). 4. If h(t) is not an IMF then repeat sifting after updating r(t)= h(t). Otherwise, set: c j (t)= h(t) and estimate j +1 - th IMF after updating r(t)= x(t)−∑ j<(j+1) c j (t). Note that in practice no more than 10 sifting steps are needed usually (Huang et al., 1998). Hence, the toolbox stops the sifting process after 10 iterations thus avoiding a tedious verification of the defining conditions of an IMF. This procedure stops when a non-oscillatory residual signal is encountered and all IMFs are extracted. Alternatively, the number of modes to be extracted can purposely be fixed by the user. Note that the EEMD needs the number of modes to be fixed, hence the toolbox only considers this alternative. The complete procedure of decomposing a time series via EMD is explained schematically in Fig. 1 . EMD also ensures the perfect reconstruction property, i.e. summing up all extracted IMFs together with the residual trend reconstructs the original signal without information loss or distortion. This also implies that the amplitudes of the extracted components reflect the contribution of the component to the original signal. There is no scale indeterminacy as is familiar with an ICA decomposition, for example. Over other signal decomposition methods, EMD has the advantage of not being constrained by conditions that often only apply approximately because of its empirical nature (Lang et al., 2011). Especially in the case of biological signal processing, when there is only a rough idea about the underlying modes or component images, and often their number is unknown (Zeiler et al., 2013; Faltermeier et al., 2011). At the end, the original signal x(t) can be expanded into its intrinsic modes c j (t) and a non-oscillating residual signal r(t) according to (1) x ( t ) = ∑ j = 1 J c j ( t ) + r ( t ) = ∑ j = 1 J + 1 c j ( t ) where the c j (t) represent the IMFs and r(t) the remaining non-oscillating trend. For the last equality, c J+1(t)= r(t) has been taken. Assuming that x(t) is Lebesgue-integrable, and employing a Hilbert Transformation, one transforms each mode c j (t) into a complex-valued analytical signal according to (2) b j ( t ) = c j ( t ) * ( π · t ) − 1 = 1 π P ∫ − ∞ ∞ c j ( τ ) t − τ d τ (3) z j ( t ) = c j ( t ) + i b j ( t ) = a j ( t ) exp i ϕ j ( t ) = a j ( t ) exp i ∫ − ∞ t ω j ( t ′ ) dt ′ Here the operator * indicates a convolution of c j (t) with 1/π · t, thus emphasizing the local properties of the modes c j (t). Furthermore, P denotes the Cauchy Principal Value, a j (t) represents a time-dependent amplitude, ϕ j (t)=∫ω j (t)dt a time-dependent phase and ω j ( t ) = d ϕ j ( t ) dt denotes the related instantaneous frequency. Because of the linearity of the convolution operator, the original time series can be expressed as (4) x ( t ) = Re ∑ j a j ( t ) exp i ∫ − ∞ t ω j ( t ′ ) dt ′ which, to a large part, represents a generalized Fourier Transform. For the latter amplitude and frequency would have to be constant. For discrete-time signals c j (nT s ), there is a simple approach to compute the corresponding analytical signal (5) z j ( nT s ) = a ( nT s ) exp ( i ϕ ( nT s ) ) which is based on the discrete Fourier-Transform (Marple, 1999). Here a(nT s ) denotes the instantaneous amplitude and the corresponding instantaneous phase is denoted as ϕ(nT S ). From the latter, the discrete instantaneous frequency can be obtained via (6) ω ( nT s ) = ϕ ( nT s ) − ϕ ( ( n − 1 ) T s ) T s rad s Here T s denotes the sampling period and n ∈ ℕ . The frequency–time (ω, t) distribution of the amplitude a j (t) of each extracted IMF represents a Hilbert–Huang Spectrum H ( ω , t ) (Huang and Wu, 2008; Attoh-Okine et al., 2008). Finally, Fig. 2 illustrates the application of plain EMD to an EEG signal recorded during a contour integration task (for details see Al-Subari et al., 2015), and Fig. 3 illustrates its corresponding Hilbert Huang Spectrum of the fifth event related mode as obtained with the help of the EMDLAB toolbox. EMD has characteristic features which render it an interesting alternative to other signal decomposition techniques: (1) Non-linearity and non-stationarity: Rather than most traditional methods, EMD can easily deal with non-linear and non-stationary data (Huang et al., 1998; Wang and Quek, 2005). (2) Locality: IMFs represent local oscillations within the signal (Rilling et al., 2003). (3) Adaptivity: The decomposition is fully data-driven (Lang et al., 2012). (4) Completeness and orthogonality: EMD preserves the full reconstruction capability in the sense that summing up all components (IMFs) recovers the original signal without any loss. This also means that the amplitudes of the components reflect their contribution to the original signal quantitatively. Furthermore, consecutive IMFs can be considered to be locally orthogonal to each other (Huang et al., 1998). (5) Multi-resolution: IMFs are a multi-resolution tool and spectral analysis which provide local information with time-varying amplitude and phase according to the scales (Kim and Oh, 2009). Spectral analysis can be performed via the Hilbert–Huang Transformation (HHT) of the IMFs. (6) Simplicity and robustness: a key feature of the EMD method is its simplicity of application and robustness across a large range of input time series. In real applications, plain EMD showed several shortcomings which initiated the development of several variants like Ensemble Empirical Mode Decomposition (EEMD), an on-line variant called weighted sliding EMD (wSEMD) or a multi-variate variant called Multivariate Empirical Mode Decomposition (MEMD). 3.2 Ensemble empirical mode decomposition One of the major drawbacks is the occurrence of mode mixing which could happen during the sifting process. It describes a situation where a single mode appears in different IMFs or local oscillations with widely disparate frequency scales show up in a single IMF (see Fig. 2, IMF 1 for example). It is a consequence of signal intermittency. In addition, often boundary artifacts result from the sifting process due to improper envelope estimation at the boundaries. To overcome these problem, a noise assisted data analysis method, called Ensemble Empirical Mode Decomposition (EEMD), was proposed by Wu and Huang (2009). The basic idea is to exploit the self-compensating property of white noise by applying EMD to an ensemble of noisy versions of the original signal. Final IMFs are obtained as averages over the corresponding modes extracted from each member of the ensemble. The approach is based on studies of the statistical properties of fractional Gaussian noise, a versatile model for broadband noise including white noise, reported by Flandrin et al. (2004) and Wu and Huang (2004). They showed that EMD operates as an adaptive dyadic filter bank when applied to fractional Gaussian noise. Thereby the mean IMFs keep the natural dyadic filter windows. Thus, mode mixing is significantly alleviated and the dyadic property is preserved. An example of applying EEMD to EEG signals is given in Fig. 4 , where EEMD was applied to the signal of Fig. 2. 3.3 Weighted sliding empirical mode decomposition One of the shortcomings of plain EMD is its need of the complete time series before the analysis can start. Many practical applications record biomedical data over long time spans thus producing an amount of data which cannot be handled in one stroke because of limited computational resources. Weighted Sliding Empirical Mode Decomposition (wSEMD) was introduced to allow for an on-line analysis of such data (Faltermeier et al., 2010, 2011). It considers sub-segments (windows) of the original time series of length M. These data windows overlap each other in accord with the step size K of the sliding window mechanism. EMD (or EEMD) is applied to each window separately. Therefore, as each sample makes part of E = M K consecutive subsegments, the final value of an IMF sample results from combining corresponding samples from E subsegments. In overlapping subsegments, samples are weighted by an appropriate weighting function to suppress artifacts from the segmentation process (Zeiler et al., 2013). Fig. 5 illustrates the application of wSEMD to the EEG signal mentioned above. Note that in practical applications wSEMD is used only to much longer time series. However, for the sake of simplicity we apply it to the same signal that is used to illustrate all other EMD variants as well. 3.4 Multivariate empirical mode decomposition MEMD (Rehman and Mandic, 2009; Mandic et al., 2013) is the multivariate extension to plain EMD introduced above. When interpreting plain EMD as an univariate approach of decomposing an one dimensional time course, MEMD can be seen as a multivariate approach by taking the response of a system from several channels as a multi-dimensional signal with each channel – like EEG-channels for instance – representing one dimension in an n-dimensional space. The time course can be seen as a trajectory propagating in this multi-dimensional space and MEMD tries to decompose this signal into multi-dimensional IMFs, which then can rather be seen as rotational than oscillatory modes. The main problem for the multivariate approach is that there is no proper definition of extrema in n dimensions. Rehman and Mandic (2009) tackle this issue by generalizing the aspect of creating envelopes around the time course to n dimensions. At first, a set of Hammersly-sequenced n-dimensional direction vectors is introduced sampling the n-dimensional space as uniformly as possible. Afterwards the signal is projected onto each direction vector to get one-dimensional representations of the signal. Then the extrema of those projections are extracted and re-projected into the n-dimensional space resulting in sets of n-dimensional maxima and minima for each one-dimensional projection, which then can be used to construct n-dimensional envelopes. Eventually, the core principle of EMD – averaging envelopes – can be applied to a multivariate signal by averaging these sets of n-dimensional envelopes. Afterwards, the same procedure as in plain EMD is applied. The stopping criterion for a multi-dimensional IMF is similar to that of the plain EMD algorithm, besides the comparison of number of zero crossings and number of extrema does not hold anymore, as there is no proper definition of zero crossings in n-dimensional space. An additional benefit of doing MEMD to plain EMD is that – besides improvement in avoiding mode mixing – frequency scales of same index IMFs align (Rehman and Mandic, 2009). As for the plain EMD approach, an ensemble noise assisted method is proposed for MEMD. It is important here do distinguish between noise assisted and ensemble approaches. For plain EMD these two approaches are synonymous. In case of a MEMD it has to be mentioned, whether only a noise assisted MEMD (NA-MEMD) (Rehman and Mandic, 2011) or an ensemble noise assisted MEMD (ENA-MEMD) (Rehman et al., 2013) is applied. Compared to MEMD, the NA-MEMD introduces l channels consisting of white Gaussian noise in addition to the signal channels spanning the n-dimensional space of the multivariate signal, since it is suggested not to add noise onto the already existing signal itself in the multivariate case. After EMD is done the l noise channels are discarded. For the noise amplitude 2–10% of the signal amplitude is suggested (Rehman et al., 2013). The ensemble principle is introduced by creating several realizations of IMFs by using different initializations of noise for several NA-MEMD runs resembling the same principle as for EEMD in the univariate case. The multivariate aspect of MEMD can be exploited in EEG studies by looking for multivariate phase synchrony (Mutlu and Aviyente, 2011) or several other intrinsic multi-scale measures (Looney et al., 2015). An example of the application of MEMD to a EEG signal is shown in Fig. 6 . 4 Results The basic functionality of the EMDLAB toolbox is to provide a plug-in to the EEGLAB toolbox for analyzing EEG data with EMD or one of its variants implemented in the EMDLAB toolbox. The latter includes a group of matlab functions for applying EMD (Huang et al., 1998), EEMD (Wu and Huang, 2009), wSEMD (Faltermeier et al., 2011) and MEMD (Rehman and Mandic, 2009). EMDLAB relies heavily on EEGLAB's functions for importing recorded EEG data. Additionally, EMDLAB uses functions of EEGLAB to analyze and visualize the modes resulting from an EMD analysis. Corresponding code has been modified to include, for example, plotting EEG waveforms of each mode or obtaining Event Related Modes (ERMs) and related topographies, so-called scalp maps. Much like Event-Related Potentials (ERPs), ERMs are obtained as an average of corresponding IMFs over a set of trials. Fig. 7 shows a screen shot of the graphical user interface (GUI) of the EEGLAB toolbox which now contains an EMDLAB menu in case EMDLAB has been installed already. As illustrated in the figure, the EMDLAB menu includes two main parts: • The first part is used to decompose signals using EMD or one of its variants. • The second part contains various methods to analyze and visualize the extracted modes. 4.1 Run EMD Fig. 8 presents the main window of the EMDLAB toolbox. It is used to perform the decomposition using different types of EMD. The system will automatically recognize if the data set is epoched or continuous. In the former case, an information saying “trial-by-trial” will show up in the main window. Otherwise, the information given will be “continuous”. Before being decomposed into IMFs, data can be normalized to zero mean and unitary standard deviation by checking the Normalize box. Normalization is advisable as it helps to have the same scale for all single trial signals to be compared. Also through this window, users are able to select any of 4 available EMD algorithms, namely EMD (Huang et al., 1998), EEMD (Wu and Huang, 2009), wSEMD (Faltermeier et al., 2011) and MEMD (Rehman and Mandic, 2009). Any of these algorithms can be applied directly to each single-trial EEG data set. EMDLAB asks users to assign the number of modes J which the signal will be decomposed into. This is needed for the EEMD algorithm to assure comparability of the extracted modes. However, it also allows the user to deliberately integrate slow modes into the “residuum”, i.e. the component J +1. Additionally, this GUI allows the user to easily enter and tune parameters of each EMD variant such as the variance of the added noise and the ensemble number in case of EEMD, the window size and the step size in case of wSEMD and the variance of the added noise and number of noise channels as well as the ensemble number in case of MEEMD. Furthermore, the studied channels can be selected simply by clicking the channels button, otherwise all channels will be considered by default. 4.2 Analyze and visualize modes After the signal decomposition has been performed, the resulting modes can be analyzed further using the additional choices of the main EMDLAB menu: - Scrolling modes: Allows the user to visualize the extracted modes (IMFs). The user can simply move between the different modes by using the drop-down menu of the combo box aside of the “Select Mode” label (see Fig. 9 ). - Power spectra and maps: Here the power spectra of the extracted ERMs can be plotted as well as scalp maps of the power within chosen frequency bands. However, first the ERM of interest should be selected as well as additional required inputs (see Fig. 10 ). - ERM maps: Here the dynamics of topographic maps of the ERMs can be visualized in either 2D or 3D (see Fig. 11 ). – Mode properties: Single trial IMFs, related ERMs and their activity power spectrum can be plotted directly by selecting the mode of interest first (see Fig. 12 ). - Hilbert–Huang/Fourier transform: Plotting the Hilbert–Huang spectrum as well as the Fourier spectrum of a selected mode (see Fig. 14 ). - IMFs and ERM: It offers, for any chosen mode of interest, a color-coded image of all single-trial IMFs and the related ERM (see Fig. 18). - ERM and maps: This option allows plotting all channels of an ERM of interest plus their topographic representation (scalp maps) at selected latencies (see Fig. 19). - Compare ERMs: This allows the user to compare ERMs of different datasets, i.e. recorded under different conditions, using simple statistics (see Figs. 20 and 21). In order to demonstrate the utility of the EMDLAB toolbox, we employ a small set of EEG data trials collected during a contour integration task (Al-Subari et al., 2015). During EEG recording, a subject was seated in a sound-attenuated chamber in front of a monitor. After a random interval from 1000 to 3000ms, a visual stimulus was presented for 194ms, followed by a blank screen. The next trial started after the response, or after a time-out of 3000ms if the subject did not respond. This EEG data (Al-Subari et al., 2015; Hanslmayr et al., 2013) was recorded jointly with fMRI data as described in Al-Baddai et al. (2014) and Hanslmayr et al. (2013). This sample data set, that is one subject EEG data, consists of 114 EEG epochs of 1s duration which are time-locked to a visual stimulus (arrays or Gabor elements that did contain a path of collinear oriented elements) presented on a 17-inch flat screen LCD monitor with a resolution of 1024 by 768 pixels and a refresh rate of 60Hz. The EEG data was recorded inside the scanner using an MR-compatible 64 channel EEG system (Brain Products). Out of these, 62 channels were used to record scalp EEG, and were mounted in an elastic cap (EasyCap, Herrsching-Breitbrunn) positioned according to the international 10–20 system. FCz was used as reference electrode. Two addition channels, one placed below the left eye for recording vertical eye movements, and the other one placed below the left scapula in order to facilitate off-line removal of cardioballistic artifacts. Data from 62 scalp electrodes were sampled at 5000Hz (later reduced to 500Hz). In this sample data, only one stimulus condition with correct answer, contour true (CT), was selected. Scrolling modes. Fig. 9 represents 5 epochs of 5 EEG channels and their IMF3 after applying EMD. This scrolling data figure allows users to review data by visual inspection and navigate their different modes. Power spectra and maps. Here the user can exhibit the power spectra, and the related topographic maps, of all or selected channels at specific frequencies. Fig. 10 exemplifies the power spectra of all channels for three different frequencies specified. Note that this offers the possibility to illustrate the power spectra of pooled electrodes in certain brain areas like the occipital or frontal regions of the brain, for example. ERM maps. Fig. 11 exhibits a series of scalp maps representing activity distributions of ERM4 at a selected series of trial latencies, i.e. every 20ms from 50ms to 270ms. These topographies represent the mapping of electrical activity across the surface of the brain at different latencies. They show the brain regions which act in response to a stimulus. Mode properties. Fig. 12 illustrates the properties of a specific mode, IMF4 for a specific channel O2. This plot includes a scalp location of O2 channel, its activity power spectrum, and an ERP-image plot of its activity in single-epochs. The leftmost topogram exhibits the scalp location of channel O2. On the right, single trial EEG signals composing ERM4 of the EEG signal recorded at channel O2 is illustrated. Channel signals are color-coded and the resulting ERM4 signal is added at the bottom of the subfigure. Such a plot can be used to analyze single trial IMFs of single channel data within an epochs across latencies. These data can be plotted in any sorting order of interest. The bottom row in Fig. 12 shows the activity power spectrum of IMF4 of O2 channel at a frequency range between 2 and 50Hz. Hilbert-Huang/Fourier transform. Fig. 13 presents the window which offers tools for spectral analysis of the extracted modes. As EMD is most useful for non-stationary time series, time–frequency analysis is most appropriate. As explained above, Hilbert–Huang spectra are obtained from a combination of an EMD of a non-stationary time series with concomitant Hilbert transform of the real valued signal. Note that the resulting IMFs represent wide-sense stationary signals, hence can be Fourier-transformed as well. EMDLAB has implemented both spectral modalities. Figs. 3 and 14 illustrate the use of these modalities employing the EEG signal used throughout this discussion. While Hilbert–Huang spectra allow to track temporal changes of each local mode, Fourier spectra represent their spectral content. Both modes of analysis are shown within one window to ease visual comparison. Fig. 14 illustrates Hilbert and Fourier spectrum of several ERMs for an EEG single channel, top: Hilbert spectrum explains the instantaneous frequency over the time and the different colors refers to the instantaneous amplitude. Down: the Fourier spectrum for the same EEG single channel. Fourier spectrum of each ERM was represented in different color. It is observed that first ERMs in Hilbert and Fourier spectrum have high frequencies than the last ERMs which may reveal that these ERMs mainly describes noise. It is not easily apparent from the plot of the ERMs in time whether the function is likely to contain meaningful information. Taking the Hilbert spectrum of the ERMs individually is helpful in identifying which of the modes contributes most strongly to the overall signal energy. Also temporal changes in frequency of the ERMs can be followed easily, while their Fourier spectrum only displays the spread in energy density and shows strong overlap of the different modes (see Fig. 14). To further clarify the use of the Hilbert–Huang Spectra, Fig. 15 illustrates the variation of the time-dependent frequency with time for all trials of one epoch. Hereby the user can choose the channel as well as the number of IMFs to be shown. Frequency is color-coded, while the related plot exhibited in Fig. 16 shows the corresponding variation of the amplitude of the IMFs during each trial. Here the IMF amplitude is color-coded as shown. Last but not least Fig. 17 illustrates the variation in time of the frequency of the specified IMF for all trials within a certain frequency band selected, and the related variation of the IMF amplitude as a function of the frequency for all trials of one epoch. IMFs and ERM. This option offers a plot of the activity of a prespecified extracted IMF for all trials within an epoch. Alternatively the user can select certain trials instead of choosing all trials. Also shown is the ERM resulting from an average of the specified IMF over all trials. Such trials and ERMs can be illustrated for each of the electrodes involved. The response activity is encoded by a color bar exhibited in Fig. 18. Note that, similar to EEGLAB, the zero indicates the stimulus onset. ERM and maps. Fig. 19 overlays time courses of all ERM4 modes extracted from 62 EEG channel recordings, Related scalp maps can be illustrated at selected latencies as well. Each of these traces plots the ERM4 at one channel. The scalp maps shown represent ‘snapshots’ of the topographic distribution at various latencies 50ms, 100ms, 150ms, 200ms during the average ERM4 time course. Compare ERMs. This option offers an overview of specified ERMs for the different conditions as well as their differences at each channel as shown in Fig. 20. In addition, the user can choose showing related simple statistics like standard deviations, etc. By simply clicking on one of the channels, the user gets to a more detailed view of the ERMs shown at a specific channel selected as is illustrated in Fig. 21 for channel O2. This provides an especially clear illustration of how the estimated ERMs differ for the two conditions studied. 5 Conclusion In this contribution we present a new toolbox, called EMDLAB, which allows users to conveniently analyze non-stationary time series data sets, especially EEG data, with a heuristic signal decomposition techniques called Empirical Mode Decomposition. The latter decomposes any signal locally without imposing additional conditions like orthogonality, independence or sparseness. The new toolbox EMDLAB is provided as a plug-in to the well-known EEGLAB toolbox and offers an alternative decomposition technique in four different variants. Beneath plain EMD, a noise-assisted variant, named Ensemble EMD is offered which is suitable as long as the data size is small enough to be stored in main memory. For very long time series, the weighted sliding EMD is more appropriate. Finally, a simultaneous multi-channel analysis can be pursued employing a multi-variate EMD variant. The plug-in allows to use any data visualization tool provided by the EEGLAB toolbox as well as statistical data analysis techniques provided there. Future versions will provide additional statistical analysis tools in EMDLAB as well. Acknowledgement Financial support by the DAAD, Acciones Integradas Hispano-Alemanas is gratefully acknowledged. Appendix A Supplementary data Supplementary data associated with this article can be found, in the online version, at Appendix A Supplementary data References [Acar and Makeig, 2010] Z.A. Acar S. Makeig Neuroelectromagnetic forward head modeling toolbox J Neurosci Methods 190 2 2010 258 270 [Al-Baddai et al., 2014] S. Al-Baddai K. Al-Subari A. Tomé G. Volberg S. Hanslmayr R. Hammwöhner Bidimensional ensemble empirical mode decomposition of functional biomedical images taken during a contour integration task Biomed Signal Process Control 13 2014 218 236 [Al-Subari et al., 2015] K. Al-Subari S. Al-Baddai A. Tomé G. Volberg R. Hammwöhner E. Lang Ensemble empirical mode decomposition analysis of EEG data collected during a contour integration task PLOS ONE 10 4 2015 e0119489 [Attoh-Okine et al., 2008] N. Attoh-Okine K. Barner D. Bentil R. Zhang The empirical mode decomposition and the Hilbert–Huang transform EURASIP J Adv Signal Process 2008 [De-xiang et al., 2008] Z. De-xiang W. Xiao-pei G. Xiao-jing The EEG signal preprocessing based on empirical mode decomposition The 2nd international conference on bioinformatics and biomedical engineering, 2008. ICBBE 2008 2008 2131 2134 [Delorme and Makeig, 2004] A. Delorme S. Makeig EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis J Neurosci Methods 1 134 2004 9 21 [Delorme et al., 2011] A. Delorme T. Mullen C. Kothe Z. Acar N. Bigdely-Shamlo A. Vankov EEGLAB, SIFT, NFT, BCILAB, and ERICA: new tools for advanced EEG processing Comput Intell Neurosci 2011 2011 12 [Durka et al., 2005] P. Durka A. Matysiak E.M. Montes P.V. Sosa K. Blinowska Multichannel matching pursuit and EEG inverse solutions J Neurosci Methods 148 2005 49 59 [Faltermeier et al., 2011] R. Faltermeier A. Zeiler A. Tomé A. Brawanski E. Lang Weighted sliding empirical mode decomposition Adv Adapt Data Anal 3 2011 509 526 [Faltermeier et al., 2010] R. Faltermeier I.R. Keck A. Zeiler A. Tomé A. Brawanski E. Lang Sliding empirical mode decomposition The 2010 international joint conference on neural networks (IJCNN) 2010 IEEE 1 9 [Flandrin et al., 2004] P. Flandrin G. Rilling P. Goncalves Empirical mode decomposition as a filter bank IEEE Signal Process Lett 2 2004 112 114 [Hanslmayr et al., 2013] S. Hanslmayr G. Volberg M. Wimber S. Dalal M. Greenlee Prestimulus oscillatory phase at 7Hz gates cortical information flow and visual perception Curr Biol 23 2013 1 6 [Hu et al., 2012] X. Hu S. Peng W.-L. Hwang EMD revisited: a new understanding of the envelope and resolving the mode-mixing problem in AM-FM signals IEEE Trans Signal Process 60 3 2012 1075 1086 [Huang et al., 1998] N.E. Huang Z. Shen S.R. Long M.L. Wu H.H. Shih Q. Zheng The empirical mode decomposition and Hilbert spectrum for nonlinear and non-stationary time series analysis Proc R Soc Lond A 454 1998 903 995 [Huang and Wu, 2008] N.E. Huang Z. Wu A review on Hilbert–Huang transform: method and its applications to geophysical studies Rev Geophys 46 2008 RG2006 [Kim and Oh, 2009] D. Kim H. Oh EMD: a package for empirical mode decomposition and Hilbert spectrum R J 1 1 2009 40 46 [Kothe, 2012] C. Kothe BCILAB 2012 [Kothe and Makeig, 2013] C.A. Kothe S. Makeig BCILAB: a platform for brain-computer interface development J Neural Eng 10 5 2013 056014 [Lang et al., 2011] E.W. Lang R. Schachtner D. Lutter D. Herold A. Kodewitz F. Blöchl Exploratory matrix factorization techniques for large scale biomedical data sets 2011 Bentham Science Publishers 26 47 [Lang et al., 2012] E.W. Lang A.M. Tomé I.R. Keck J.M.G. Sáez C.G. Puntonet Brain connectivity analysis – a short survey Comput Intell Neurosci 2012 412512 2012 21 [Looney et al., 2015] D. Looney A. Hemakom D.P. Mandic Intrinsic multi-scale analysis: a multi-variate empirical mode decomposition framework Proc R Soc A 471 20140709 2015 [Makeig et al., 2004] S. Makeig S. Debener J. Onton A. Delorme Mining event-related brain dynamics. View at Publisher View at Google Scholar View at Scopus Trends Cognit Sci 5 8 2004 204 210 [Mallat and Zhang, 1993] S. Mallat Z. Zhang Matching pursuit with time–frequency dictionaries IEEE Trans Signal Process 41 1993 3397 3415 [Mandic et al., 2013] D.P. Mandic N. Rehman Z. Wu N.E. Huang Empirical mode decomposition-based time–frequency analysis of multivariate signals: the power of adaptive data analysis IEEE Signal Process Mag 30 6 2013 74 86 [Marple, 1999] S.L. Marple Computing the discrete-time “analytic” signal via FFT IEEE Trans Signal Process 47 9 1999 2600 2603 [Mullen et al., 2010] T. Mullen A. Delorme C. Kothe S. Makeig An electrophysiological information flow toolbox for EEGLAB Society for neuroscience conference San Diego, CA, USA 2010 [Mutlu and Aviyente, 2011] A.Y. Mutlu S. Aviyente Multivariate empirical mode decomposition for quantifying multivariate phase synchronization EURASIP J Adv Signal Process 2011 2011 1 13 [Rehman and Mandic, 2009] N. Rehman D.P. Mandic Multivariate empirical mode decomposition Proc R Soc A Math Phys Eng Sci 466 2117 2009 1291 1302 [Rehman and Mandic, 2011] N. Rehman D.P. Mandic Filter bank property of multivariate empirical mode decomposition IEEE Trans Signal Process 59 5 2011 2421 2426 [Rehman et al., 2013] N. Rehman C. Park N.E. Huang D.P. Mandic EMD via MEMD: multivariate noise-aided computation of standard EMD Adv Adapt Data Anal 05 02 2013 1350007 [Rilling et al., 2003] G. Rilling P. F1andrin P. Goncalves On empirical mode decomposition and its algorithms IEEE-EURASIP workshop on nonlinear signal and image processing 3 2003 8 11 [Rutkowski et al., 2010] T.M. Rutkowski D.P. Mandic A. Cichocki A.W. Przybyszewski EMD approach to multichannel EEG data- the amplitude and phase components clustering analysis J Circuits Syst Comput 19 1 2010 215 229 [Rutkowski et al., 2006] T.M. Rutkowski F. Vialatte A. Cichocki D. Mandic A.K. Barros Auditory feedback for brain computer interface management an EEG data sonification approach Lect Notes Comput Sci 4253 2006 1232 1239 [Shattuck et al., 2007] D. Shattuck M. Mirza V. Adisetiyo C. Hojatkashani G. Salamon K. Narr Construction of a 3D probabilistic atlas of human cortical structures Neuroimage 2007 [Wang and Quek, 2005] Q. Wang S.-T. Quek Comparison of Hilbert–Huang, wavelet, and Fourier transforms for selected applications 2005 CRC Press 213 244 [Wu and Huang, 2004] Z. Wu N.E. Huang A study of the characteristics of white noise using the empirical mode decomposition method Proc R Soc Lond Ser A Math Phys Eng Sci 460 2046 2004 1597 1611 [Wu and Huang, 2009] Z. Wu N.E. Huang Ensemble empirical mode decomposition: a noise-assisted data analysis method Adv Adapt Data Anal 1 1 2009 1 41 [Zeiler et al., 2013] A. Zeiler R. Faltermeier A. Tomé C. Puntonet A. Brawanski E. Lang Weighted sliding empirical mode decomposition for online analysis of biomedical time series Neural Process Lett 37 2013 21 32 "
    },
    {
        "doc_title": "An adaptive object perception system based on environment exploration and Bayesian learning",
        "doc_scopus_id": "84933040723",
        "doc_doi": "10.1109/ICARSC.2015.37",
        "doc_eid": "2-s2.0-84933040723",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Bayesian learning",
            "Cognitive robotics",
            "Environment exploration",
            "Object perception",
            "Perception capability",
            "Probabilistic models",
            "Qualitative analysis",
            "Quantitative evaluation"
        ],
        "doc_abstract": "© 2015 IEEE.Cognitive robotics looks at human cognition as a source of inspiration for automatic perception capabilities that will allow robots to learn and reason out how to behave in response to complex goals. For instance, humans learn to recognize object categories ceaselessly over time. This ability to refine knowledge from the set of accumulated experiences facilitates the adaptation to new environments. Inspired by such abilities, this paper proposes an efficient approach towards 3D object category learning and recognition in an interactive and open-ended manner. To achieve this goal, this paper focuses on two state-of-the-art questions: (i) How to use unsupervised object exploration to construct a dictionary of visual words for representing objects in a highly compact and distinctive way. (II) How to learn incrementally probabilistic models of object categories to achieve adaptability. To examine the performance of the proposed approach, a quantitative evaluation and a qualitative analysis are used. The experimental results showed the fulfilling performance of this approach on different types of objects. The proposed system is able to interact with human users and learn new object categories over time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ensemble empirical mode decomposition analysis of EEG data collected during a contour integration task",
        "doc_scopus_id": "84929353619",
        "doc_doi": "10.1371/journal.pone.0119489",
        "doc_eid": "2-s2.0-84929353619",
        "doc_date": "2015-04-24",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Algorithms",
            "Brain",
            "Brain Mapping",
            "Electroencephalography",
            "Evoked Potentials",
            "Female",
            "Humans",
            "Magnetic Resonance Imaging",
            "Male",
            "Models, Biological",
            "Photic Stimulation",
            "Young Adult"
        ],
        "doc_abstract": "© 2015 Al-Subari et al.We discuss a data-driven analysis of EEG data recorded during a combined EEG/fMRI study of visual processing during a contour integration task. The analysis is based on an ensemble empirical mode decomposition (EEMD) and discusses characteristic features of event related modes (ERMs) resulting from the decomposition. We identify clear differences in certain ERMs in response to contour vs noncontour Gabor stimuli mainly for response amplitudes peaking around 100 [ms] (called P100) and 200 [ms] (called N200) after stimulus onset, respectively. We observe early P100 and N200 responses at electrodes located in the occipital area of the brain, while late P100 and N200 responses appear at electrodes located in frontal brain areas. Signals at electrodes in central brain areas show bimodal early/late response signatures in certain ERMs. Head topographies clearly localize statistically significant response differences to both stimulus conditions. Our findings provide an independent proof of recent models which suggest that contour integration depends on distributed network activity within the brain.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive Open-Ended Learning for 3D Object Recognition: An Approach and Experiments",
        "doc_scopus_id": "84945464134",
        "doc_doi": "10.1007/s10846-015-0189-z",
        "doc_eid": "2-s2.0-84945464134",
        "doc_date": "2015-01-31",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Feature descriptors",
            "High level knowledge",
            "Leave-one-out cross validations",
            "Nearest neighbor classification",
            "Open-ended learning",
            "Precision and recall",
            "Spin images"
        ],
        "doc_abstract": "© 2015, Springer Science+Business Media Dordrecht.3D object detection and recognition is increasingly used for manipulation and navigation tasks in service robots. It involves segmenting the objects present in a scene, estimating a feature descriptor for the object view and, finally, recognizing the object view by comparing it to the known object categories. This paper presents an efficient approach capable of learning and recognizing object categories in an interactive and open-ended manner. In this paper, “open-ended” implies that the set of object categories to be learned is not known in advance. The training instances are extracted from on-line experiences of a robot, and thus become gradually available over time, rather than at the beginning of the learning process. This paper focuses on two state-of-the-art questions: (1) How to automatically detect, conceptualize and recognize objects in 3D scenes in an open-ended manner? (2) How to acquire and use high-level knowledge obtained from the interaction with human users, namely when they provide category labels, in order to improve the system performance? This approach starts with a pre-processing step to remove irrelevant data and prepare a suitable point cloud for the subsequent processing. Clustering is then applied to detect object candidates, and object views are described based on a 3D shape descriptor called spin-image. Finally, a nearest-neighbor classification rule is used to predict the categories of the detected objects. A leave-one-out cross validation algorithm is used to compute precision and recall, in a classical off-line evaluation setting, for different system parameters. Also, an on-line evaluation protocol is used to assess the performance of the system in an open-ended setting. Results show that the proposed system is able to interact with human users, learning new object categories continuously over time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A backward-adaptive perceptual audio coder",
        "doc_scopus_id": "85113855627",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85113855627",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Adaptiveness",
            "Audio coders",
            "Audio compression",
            "Entropy coding",
            "Nonuniform filter banks",
            "Original signal",
            "Perceptual coding",
            "Psychoacoustic model"
        ],
        "doc_abstract": "© 2015 European Signal Processing Conference, EUSIPCO. All rights reserved.This paper presents a new audio compression algorithm that includes a nonuniform filter bank, gain-adaptive logarithmic quantizers, arithmetic entropy coding and an explicit psychoacoustic model to adapt the quantization according to perceptual considerations. Unlike existing perceptual coders, the new system is backward-adaptive, i.e., adaptation depends exclusively on already quantized samples, not on the original signal. We discuss the advantages of backward adaptiveness and show that it can be successfully applied to perceptual coding.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Face recognition by fast and stable bi-dimensional empirical mode decomposition",
        "doc_scopus_id": "84938888746",
        "doc_doi": "10.5220/0005337003850391",
        "doc_eid": "2-s2.0-84938888746",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Bi-dimensional empirical mode decompositions",
            "Classification rates",
            "Decomposition technique",
            "Linear discriminant analysis",
            "Minimum distance",
            "Reference image"
        ],
        "doc_abstract": "In this study the use of a new fast and stable decomposition technique, bi-dimensional empirical mode decomposition, is used for face recognition tasks. Images are decomposed individually, and then the distance with reference images is computed. Three different types of distances are tested. Then class association is based on minimum distance and by using a classifier. Preliminary results (90.0% of classification rate) are satisfactory and will justify a deep investigation on how to apply this bi-dimensional decomposition technique for face recognition.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving performance of bio-radars for remote heartbeat and breathing detection by using cyclostationary features",
        "doc_scopus_id": "84938886964",
        "doc_doi": "10.5220/0005287603440349",
        "doc_eid": "2-s2.0-84938886964",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Cognitive radars",
            "Cyclic autocorrelation",
            "Cyclic spectrum",
            "Cyclostationarity",
            "Cyclostationary",
            "Energy detection",
            "Improving performance",
            "Software-defined radios"
        ],
        "doc_abstract": "In this paper we present a continuous wave radar created using a software defined radio platform that uses doppler effect to measure the heart-rate and breathing. The measurements are evaluated using a classic energy detection method and a cyclic spectrum estimation technique, then the two methods are compared. The results show that by taking advantaging of the cyclic autocorrelation of the bio-signals we can get better detection than the usual energy detection.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A logistic non-negative matrix factorization approach to binary data sets",
        "doc_scopus_id": "84921296370",
        "doc_doi": "10.1007/s11045-013-0240-9",
        "doc_eid": "2-s2.0-84921296370",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Analysis of binaries",
            "Binary matrix",
            "Compact representation",
            "Non negatives",
            "Non-negative factorization",
            "Nonnegative matrix factorization",
            "Reconstruction quality",
            "Related matrices"
        ],
        "doc_abstract": "© 2013, Springer Science+Business Media New York.An analysis of binary data sets employing Bernoulli statistics and a partially non-negative factorization of the related matrix of log-odds is presented. The model places several constraints onto the factorization process rendering the estimated basis system strictly non-negative or even binary. Thereby the proposed model places itself in between a logistic PCA and a binary NMF approach. We show with proper toy data sets that different variants of the proposed model yield reasonable results and indeed are able to estimate with good precision the underlying basis system which forms a new and often more compact representation of the observations. An application of the method to the USPS data set reveals the performance of the various variants of the model and shows good reconstruction quality even with a low rank binary basis set.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Blind detection of cyclostacionary signals taking advantage of cyclic spectrum leakage",
        "doc_scopus_id": "84921272043",
        "doc_doi": "10.1109/EuMIC.2014.6997912",
        "doc_eid": "2-s2.0-84921272043",
        "doc_date": "2014-12-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Blind detection",
            "Cyclic spectrum",
            "Cyclo-stationary signals",
            "Cyclostationary",
            "Digital datas",
            "Digital modulations",
            "Spectrum sensing",
            "Traditional models"
        ],
        "doc_abstract": "© 2014 European Microwave Association-EUMA.In this article we describe a novel process for blind detection in spectrum sensing taking advantage of the cyclostationary features of any digital modulation that uses a finite number of distinct signals to represent digital data. Traditional models only look at any peak in a cyclic spectrum that satisfies a given threshold, in this method we take advantage of the cyclic spectrum leakage to identify a signal. This method shows better performance at detecting low SNR signals than the traditional cyclostationary signal detection.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Blind detection of cyclostacionary signals taking advantage of cyclic spectrum leakage",
        "doc_scopus_id": "84929253961",
        "doc_doi": "10.1109/EuMC.2014.6986728",
        "doc_eid": "2-s2.0-84929253961",
        "doc_date": "2014-12-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Blind detection",
            "Cyclic spectrum",
            "Cyclo-stationary signals",
            "Cyclostationary",
            "Digital datas",
            "Digital modulations",
            "Spectrum sensing",
            "Traditional models"
        ],
        "doc_abstract": "© 2014 European Microwave Association.In this article we describe a novel process for blind detection in spectrum sensing taking advantage of the cyclostationary features of any digital modulation that uses a finite number of distinct signals to represent digital data. Traditional models only look at any peak in a cyclic spectrum that satisfies a given threshold, in this method we take advantage of the cyclic spectrum leakage to identify a signal. This method shows better performance at detecting low SNR signals than the traditional cyclostationary signal detection.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Wideband spectrum sensing for Cognitive Radio",
        "doc_scopus_id": "84911866464",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84911866464",
        "doc_date": "2014-11-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Digital counterparts",
            "FPGA-based implementation",
            "Hybrid filter bank",
            "Parallelized version",
            "Sampling rates",
            "Spectrum sensing",
            "Wide band frequencies",
            "Wideband spectrum sensing"
        ],
        "doc_abstract": "© 2014 EURASIP.In this work we propose a wideband spectrum sensing system based on hybrid filter banks. The polyphase implementation of the digital counterpart of the filter bank can be modified to include a parallelized version of discrete Fourier transform algorithm (FFT) avoiding this way any sampling rate expanders. In this work we show how to incorporate the FFT block in the structure in order to estimate the wideband frequency contents of the signal. The proposed structure is particularly suitable for FPGA based implementations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A perceptual memory system for grounding semantic representations in intelligent service robots",
        "doc_scopus_id": "84911489131",
        "doc_doi": "10.1109/IROS.2014.6942861",
        "doc_eid": "2-s2.0-84911489131",
        "doc_date": "2014-10-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computational issues",
            "High-level reasoning",
            "Intelligent Service robots",
            "Low level control",
            "Memory requirements",
            "Multiplexing schemes",
            "Perceptual memory",
            "Semantic representation"
        ],
        "doc_abstract": "© 2014 IEEE.This paper addresses the problem of grounding semantic representations in intelligent service robots. In particular, this work contributes to addressing two important aspects, namely the anchoring of object symbols into the perception of the objects and the grounding of object category symbols into the perception of known instances of the categories. The paper discusses memory requirements for storing both semantic and perceptual data and, based on the analysis of these requirements, proposes an approach based on two memory components, namely a semantic memory and a perceptual memory. The perception, memory, learning and interaction capabilities, and the perceptual memory, are the main focus of the paper. Three main design options address the key computational issues involved in processing and storing perception data: a lightweight, NoSQL database, is used to implement the perceptual memory; a thread-based approach with zero copy transport of messages is used in implementing the modules; and a multiplexing scheme, for the processing of the different objects in the scene, enables parallelization. The system is designed to acquire new object categories in an incremental and open-ended way based on user-mediated experiences. The system is fully integrated in a broader robot system comprising low-level control and reactivity to high-level reasoning and learning.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive teaching and experience extraction for learning about objects and robot activities",
        "doc_scopus_id": "84937567763",
        "doc_doi": "10.1109/ROMAN.2014.6926246",
        "doc_eid": "2-s2.0-84937567763",
        "doc_date": "2014-10-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Continuous interactions",
            "Human users",
            "Intelligent Service robots",
            "Interactive learning",
            "Learning methods",
            "Perceptual domain",
            "Robot architecture",
            "Teaching activities"
        ],
        "doc_abstract": "© 2014 IEEE.Intelligent service robots should be able to improve their knowledge from accumulated experiences through continuous interaction with the environment, and in particular with humans. A human user may guide the process of experience acquisition, teaching new concepts, or correcting insufficient or erroneous concepts through interaction. This paper reports on work towards interactive learning of objects and robot activities in an incremental and open-ended way. In particular, this paper addresses human-robot interaction and experience gathering. The robot's ontology is extended with concepts for representing human-robot interactions as well as the experiences of the robot. The human-robot interaction ontology includes not only instructor teaching activities but also robot activities to support appropriate feedback from the robot. Two simplified interfaces are implemented for the different types of instructions including the teach instruction, which triggers the robot to extract experiences. These experiences, both in the robot activity domain and in the perceptual domain, are extracted and stored in memory, and they are used as input for learning methods. The functionalities described above are completely integrated in a robot architecture, and are demonstrated in a PR2 robot.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Physarum Learner: A bio-inspired way of learning structure from data",
        "doc_scopus_id": "84898456127",
        "doc_doi": "10.1016/j.eswa.2014.03.002",
        "doc_eid": "2-s2.0-84898456127",
        "doc_date": "2014-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Bayesian network structure",
            "Benchmark data",
            "Learning Bayesian networks",
            "Learning structure",
            "Physarum",
            "Source and sink",
            "Structure learning algorithm",
            "Structure-learning"
        ],
        "doc_abstract": "A novel Score-based Physarum Learner algorithm for learning Bayesian Network structure from data is introduced and shown to outperform common score based structure learning algorithms for some benchmark data sets. The Score-based Physarum Learner first initializes a fully connected Physarum-Maze with random conductances. In each Physarum Solver iteration, the source and sink nodes are changed randomly, and the conductances are updated. Connections exceeding a predefined conductance threshold are considered as Bayesian Network edges, and the score of the connected nodes are examined in both directions. A positive or negative feedback is given to the edge conductance based on the calculated scores. Due to randomness in selecting connections for evaluation, an ensemble of Score-based Physarum Learner is used to build the final Bayesian Network structure. © 2014 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271506 291210 291817 291820 291862 291866 291870 291883 31 Expert Systems with Applications EXPERTSYSTEMSAPPLICATIONS 2014-03-11 2014-03-11 2014-04-24T08:33:20 S0957-4174(14)00127-4 S0957417414001274 10.1016/j.eswa.2014.03.002 S300 S300.1 FULL-TEXT 2015-05-15T03:46:20.691395-04:00 0 0 20140901 2014 2014-03-11T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref alllist content subj ssids 0957-4174 09574174 true 41 41 11 11 Volume 41, Issue 11 31 5353 5370 5353 5370 20140901 1 September 2014 2014-09-01 2014 article fla Copyright © 2014 Elsevier Ltd. All rights reserved. PHYSARUMLEARNERABIOINSPIREDWAYLEARNINGSTRUCTUREDATA SCHON T 1 Introduction 1.1 Finding shortest paths 1.2 Network optimization problems 1.3 Physarum Machines and computers 1.4 Motivation and outline 2 Materials and methods 2.1 Bayesian Network 2.2 Physarum Solver 2.3 Physarum Learner – the SO-PhyL algorithm 2.3.1 Initializing the Physarum-Maze 2.3.2 Scoring of edge connections 3 Results of parameter estimation 3.1 Number of iterations 3.2 Ensemble size 3.3 Exponent μ of squashing function 3.4 Momentum parameter λ 3.5 Weight parameter w 3.6 Network input–output I 0 3.7 Conductance parameters 3.8 Score-feedback impact factor k 4 Discussion of simulations 4.1 Simulations with artificial benchmark networks 4.2 Simulations with real world benchmark networks 4.3 Detailed analysis of the learning dynamics 5 Conclusion Acknowledgments Appendix A References ADAMATZKY 2007 455 467 A ADAMATZKY 2008 105127 A ADAMATZKY 2010 A PHYSARUMMACHINESCOMPUTERSSLIMEMOLD ADAMATZKY 2005 A REACTIONDIFFUSIONCOMPUTERS BECHETTI 2013 472 483 L AUTOMATALANGUAGESPROGRAMMING PHYSARUMCANCOMPUTESHORTESTPATHSCONVERGENCEPROOFSCOMPLEXITYBOUNDS BECKER 2011 285 291 M PROCEEDINGSCONFERENCEEVOLUTIONARYCOMPUTATIONCEC DESIGNFAULTTOLERANTNETWORKSAGENTBASEDSIMULATIONPHYSARUMPOLYCEPHALUM BEINLICH 1989 247 256 I PROCEEDINGS2NDEUROPEANCONFERENCEARTIFICIALINTELLIGENCEINMEDICINE ALARMMONITORINGSYSTEMACASESTUDYTWOPROBABILISTICINFERENCETECHNIQUESFORBELIEFNETWORKS BONIFACI 2013 4 7 V BONIFACI 2012 121 133 V BOUCHAALA 2010 54705475 L BRUMMITT 2010 1 24 C COOPER 1992 309 347 G FRICKER 2009 M ADAPTIVENETWORKSTHEORYMODELSAPPLICATIONS GLOVER 1990 74 94 F GLOVER 1986 F GUNJIA 2011 187 200 Y HECKERMAN 1995 197 243 D KIRKPATRICK 1983 671 680 S KOIVISTO 2004 549 573 M KOLLER 2009 D PROBABILISTICGRAPHICALMODELSPRINCIPLESTECHNIQUES KORB 2010 K BAYESIANARTIFICIALINTELLIGENCE LAM 1994 269 293 W LAURITZEN 1988 157 224 S LIN 1973 498 516 S LI 2011 183 223 K MASI 2013 L EVOLVEABRIDGEBETWEENPROBABILITYSETORIENTEDNUMERICSEVOLUTIONARYCOMPUTATIONIII MIYAJI 2007 245 465 T MIYAJI 2008 353 369 T MIYAJI 2008 210 219 T NAKAGAKI 2001 767 770 T NAKAGAKI 2007 068104 T NAKAGAKI 2009 57 81 T NAKAGAKI 2004 1 5 T NAKAGAKI 2000 470 T NAKAGAKI 2001 47 52 T NIIZATOA 2010 108 112 T PEARL 1988 J PROBABILISTICREASONINGININTELLIGENTSYSTEMSNETWORKSPLAUSIBLEINFERENCE SAIGUSA 2008 018101 T SCHUMANN 2009 483 498 A SHIRAKAWA 2009 31093117 T SOHIER 2012 315 329 D OPODIS2012 PHYSARUMINSPIREDSELFBIASEDWALKERSFORDISTRIBUTEDCLUSTERING TERO 2006 115 119 A TERO 2007 553 564 A TERO 2010 439 442 A TERO 2008 89 94 A TSUDA 2004 45 55 S TSUDA 2009 S ARTIFICIALLIFE TSUDA 2007 215 223 S ZHANG 2013 7607 7616 X ZHANG 2012 369 376 Y SCHONX2014X5353 SCHONX2014X5353X5370 SCHONX2014X5353XT SCHONX2014X5353X5370XT item S0957-4174(14)00127-4 S0957417414001274 10.1016/j.eswa.2014.03.002 271506 2014-05-02T09:23:25.109794-04:00 2014-09-01 true 1412872 MAIN 18 60150 849 656 IMAGE-WEB-PDF 1 si308 290 18 23 si307 168 6 17 si306 445 17 74 si154 10812 101 371 si150 5808 21 413 si149 5721 21 414 si140 3186 40 227 si135 3336 21 238 si128 2256 36 154 si110 3078 43 164 si108 1968 24 105 si107 3390 38 216 si105 3297 85 113 si89 4338 43 330 si99 585 15 15 si98 561 13 15 si97 627 17 17 si96 588 14 17 si95 660 17 19 si94 765 18 23 si93 585 17 16 si92 699 18 20 si91 477 10 12 si90 1638 27 100 si9 588 15 19 si88 627 17 17 si87 588 14 17 si86 765 18 23 si85 720 19 21 si84 1179 17 52 si83 1722 17 93 si82 564 14 17 si81 1716 18 94 si80 2868 18 184 si8 1122 16 71 si79 861 17 50 si78 555 13 16 si77 1923 21 114 si76 834 17 31 si75 882 17 32 si74 1206 17 73 si73 843 15 35 si72 774 17 33 si71 684 16 24 si70 798 15 33 si7 534 14 13 si69 753 14 31 si68 444 13 10 si67 531 14 13 si66 672 14 23 si65 843 15 35 si64 774 17 33 si63 684 16 24 si62 798 15 33 si61 753 14 31 si60 444 13 10 si6 906 14 59 si59 531 14 13 si58 672 14 23 si57 906 14 55 si56 909 14 55 si55 921 14 55 si54 915 14 55 si53 855 14 55 si52 774 17 33 si51 684 16 24 si50 798 15 33 si5 948 14 59 si49 753 14 31 si48 588 15 19 si47 588 15 19 si46 588 15 19 si45 912 15 54 si44 909 15 54 si43 927 15 54 si42 879 15 54 si41 738 14 45 si40 756 15 45 si4 900 14 57 si39 693 14 43 si38 534 14 13 si37 981 18 58 si36 966 18 58 si35 888 17 58 si340 942 19 51 si34 888 18 58 si339 1005 19 51 si338 1227 18 75 si337 1281 18 75 si336 1875 19 138 si335 528 16 11 si334 621 17 17 si333 825 15 32 si332 1071 15 65 si331 1866 15 127 si330 765 18 23 si33 918 18 58 si329 765 18 23 si328 627 17 17 si327 621 17 17 si326 420 10 13 si325 1521 17 75 si324 1521 17 75 si323 5319 20 401 si322 627 17 17 si321 5277 20 401 si320 621 17 17 si319 501 13 12 si318 765 18 23 si317 765 18 23 si316 501 13 12 si315 765 18 23 si32 531 14 13 si314 765 18 23 si313 1041 18 59 si312 765 18 23 si311 399 6 17 si310 1248 17 74 si31 471 12 12 si309 555 13 13 si304 825 15 32 si303 2418 23 196 si302 1071 15 65 si301 1866 15 127 si300 555 13 13 si30 585 12 17 si3 588 15 19 si299 1551 19 109 si298 1038 18 59 si297 1041 19 48 si296 588 17 17 si295 564 14 17 si294 1041 19 48 si293 630 14 35 si292 903 14 53 si291 621 17 17 si290 765 18 23 si29 501 12 13 si289 621 17 17 si288 906 17 41 si287 765 18 23 si286 2364 18 159 si285 966 19 61 si284 948 17 53 si283 564 14 17 si282 564 14 17 si281 399 6 17 si280 399 6 17 si28 438 12 12 si279 534 14 13 si278 534 14 13 si277 417 6 17 si276 417 6 17 si275 417 6 17 si274 417 6 17 si273 588 15 19 si272 843 15 35 si271 1344 18 75 si270 1227 18 75 si27 537 13 14 si269 831 14 51 si268 648 14 38 si267 693 12 30 si266 444 13 10 si265 534 14 13 si264 444 13 10 si263 1041 17 54 si262 1350 14 86 si261 915 15 59 si260 534 14 13 si26 648 14 38 si259 444 13 10 si258 873 14 33 si257 708 14 41 si256 1059 14 67 si255 2580 17 190 si254 795 14 49 si253 3645 17 296 si252 588 15 19 si251 867 13 59 si250 588 15 19 si25 903 14 58 si249 891 14 56 si248 588 15 19 si247 660 17 19 si246 705 13 49 si245 7074 17 791 si244 558 14 18 si243 534 14 13 si242 2319 17 221 si241 774 17 33 si240 1158 17 63 si24 813 14 49 si239 753 14 31 si238 918 14 34 si237 534 14 13 si236 1506 16 95 si235 534 14 13 si234 534 14 13 si233 1101 16 72 si232 738 14 45 si231 912 15 54 si230 660 17 19 si229 534 14 13 si228 2637 17 196 si227 588 15 19 si226 534 14 13 si225 534 14 13 si224 534 14 13 si223 720 19 21 si222 534 14 13 si221 534 14 13 si220 948 14 59 si23 750 14 49 si22 621 13 39 si219 900 14 57 si218 861 14 51 si217 1119 18 63 si216 900 14 57 si215 2793 17 214 si214 444 13 10 si213 855 14 54 si212 534 14 13 si211 444 13 10 si210 861 14 51 si21 624 13 39 si209 1119 18 63 si208 861 14 51 si207 807 14 51 si206 1122 14 96 si205 588 15 19 si204 444 13 10 si203 444 13 10 si202 804 14 52 si201 948 14 62 si200 2358 18 224 si20 648 14 38 si2 444 13 10 si199 2943 17 236 si198 183 13 10 si197 433 18 67 si196 361 18 38 si195 183 13 10 si194 183 13 10 si193 341 18 58 si192 212 14 13 si191 341 18 58 si190 212 14 13 si19 244 13 39 si189 1006 18 225 si188 212 14 13 si187 212 14 13 si186 341 18 58 si185 212 14 13 si184 212 14 13 si183 312 14 51 si182 312 14 51 si181 312 14 51 si180 327 14 51 si18 223 12 37 si179 312 14 51 si178 251 14 38 si177 312 14 51 si176 265 13 39 si175 251 14 38 si174 548 17 107 si173 285 14 49 si172 223 12 37 si171 434 16 81 si170 285 14 49 si17 316 15 35 si169 392 17 76 si168 564 17 118 si167 771 17 165 si166 1061 17 239 si165 266 12 30 si164 210 14 12 si163 220 13 13 si162 266 12 30 si161 220 13 13 si160 609 19 114 si159 255 17 19 si158 255 17 19 si157 255 17 19 si156 287 16 39 si155 290 16 39 si153 542 17 75 si152 542 17 75 si151 436 15 57 si16 293 17 33 si15 263 16 24 si148 452 17 93 si147 452 17 93 si146 290 18 23 si145 333 17 56 si144 331 17 56 si143 231 17 17 si142 223 14 17 si141 600 18 93 si14 301 15 33 si139 266 12 30 si138 293 17 33 si137 263 16 24 si136 231 15 19 si134 183 13 10 si133 255 17 19 si132 194 10 12 si131 361 18 38 si130 467 21 70 si13 286 14 31 si129 221 14 18 si127 275 15 45 si126 265 15 23 si125 438 18 63 si124 290 18 23 si123 309 19 28 si122 244 17 17 si121 231 14 17 si120 309 19 28 si12 213 14 13 si119 266 16 25 si118 338 16 61 si117 230 17 16 si116 773 19 151 si115 398 19 45 si114 202 13 12 si113 210 14 12 si112 220 13 13 si111 190 13 10 si11 183 13 10 si109 273 17 42 si106 581 18 96 si104 213 14 13 si103 342 16 46 si102 464 21 72 si101 255 17 19 si100 481 19 85 si10 212 14 13 si1 231 15 19 si305 219 13 13 gr1 21606 223 487 gr9 16297 175 509 gr8 64335 771 534 gr7 23480 176 485 gr6 82459 384 622 gr5 32989 245 485 gr4 25129 237 497 gr3 44424 244 620 gr2 13226 165 487 gr10 47071 198 535 gr1 3633 100 219 gr9 2825 75 219 gr8 3700 163 113 gr7 3905 80 219 gr6 8148 135 219 gr5 5257 111 219 gr4 4257 104 219 gr3 4928 86 219 gr2 2521 74 219 gr10 6416 81 219 ESWA 9211 S0957-4174(14)00127-4 10.1016/j.eswa.2014.03.002 Elsevier Ltd Fig. 1 Score development of network A_n5_p3_a7_c3 over 100 iterations. Fig. 2 Score development of network A_n50_p3_a97_c3 over 12,250 iterations. Fig. 3 Development of number of connections higher than D τ for network A_n50_p3_a97_c3 with different values of λ . Fig. 4 Score development over iterations for network A_n50_p3_a97_c3 using different values of w. Fig. 5 Development of number of connections higher than D τ for network A_n50_p3_a97_c3 with different values of w. Fig. 6 Conductance changes for all connections of network A_n10_p3_a17_c3 using different values of w. Vertical axis shows the conductances of the connections and the horizontal axis shows the number of iterations. Top: w = 0.1 , Middle: w = 0.5 , Bottom: w = 1.0 . Fig. 7 Average percentage of active edges with respect to the correct number of edges as function of network input I 0 across all 90 benchmark networks and grouped by the number of nodes. Fig. 8 Structure of the Insurance network learned by SOPhyL-1. Fig. 9 Logarithm of execution time in milliseconds as a function of the number of instances in the Asia dataset where learning algorithms are applied to. Fig. 10 Evolution of conductivity values of Asia connections over r total = 84 iterations using SO-PhyL-1. The threshold D τ is shown by a dashed line. Table 1 Overview of configuration parameters for the SO-PhyL algorithm. Column Component shows where the parameter is used within SO-PhyL. The last column Initial value defines the default value of the parameter that is used for experiments if not stated otherwise. Parameter Component Initial value r SO-PhyL 2.00 Ensemble size SO-PhyL 10.00 μ MFS-Physarum Solver 1.20 λ MFS-Physarum Solver 0.20 w MFS-Physarum Solver 0.50 I 0 MFS-Physarum Solver 3.00 D min MFS-Physarum Solver 0.78 D max MFS-Physarum Solver 0.79 D τ 0 SO-PhyL 0.80 D τ end SO-PhyL 0.80 D limit SO-PhyL 2.50 k SO-PhyL 3.00 Table 2 Different values of r are compared regarding the Bayesian score. The counts how often each configuration has learned the highest scoring network compared to the other configurations are given. Row All shows the total counts for all 75 benchmark networks while other rows provide fragmented counts for networks with the same number of nodes, parents and cardinality. The boldface numbers indicate best results. r = 1 r = 2 r = 3 r = 4 r = 5 r = 10 r = 50 r = 100 All 22 27 28 32 31 35 49 58 n5 15 14 14 14 14 14 14 14 n10 6 8 8 9 10 10 11 12 n15 1 3 4 6 5 5 10 11 n20 0 2 2 3 2 2 6 12 n25 0 0 0 0 0 4 8 9 p1 6 9 9 10 9 12 13 14 p2 5 6 7 8 7 9 10 12 p3 4 4 4 6 7 6 10 14 p4 3 3 3 3 4 4 8 8 p5 4 5 5 5 4 4 8 10 c2 7 7 9 9 9 10 13 19 c3 6 11 10 12 11 12 18 20 c4 9 9 9 11 11 13 18 19 Table 3 Results of ten individual runs of network A_n50_p3_a97_c3 with r = 3 and an ensemble size of 10. Here the statistical measures of the algorithm are: A denotes the total number of learnt edges (arcs), T gives the number of correct edges, R is the number of reversed edges, M denotes the number of missing edges not learnt, and E denotes the number of extra edges falsely estimated. Run Bayes A T R M E 1 −44696.75 72 56 12 29 4 2 −44727.80 72 55 13 29 4 3 −44811.98 71 55 12 30 4 4 −44787.64 72 55 13 29 4 5 −44724.93 71 55 13 29 3 6 −44836.09 70 54 12 31 4 7 −44815.21 69 55 11 31 3 8 −44799.84 71 55 12 30 4 9 −44902.99 69 53 13 31 3 10 −44746.24 71 55 12 30 4 Table 4 Counts where SO-PhyL using different values of μ learned the highest scoring networks structure among 90 benchmark datasets. The boldface numbers indicate best results. μ = 1.0 μ = 1.2 μ = 1.5 μ = 2.0 μ = 3.0 All 60 45 42 32 27 n5 13 14 14 14 12 n10 14 11 10 7 6 n15 9 5 6 2 5 n20 11 5 5 4 3 n25 4 7 4 5 1 n50 9 3 3 0 0 p1 11 12 13 9 10 p2 14 10 9 10 9 p3 12 8 7 4 3 p4 12 7 7 4 2 p5 11 8 6 5 3 c2 19 12 11 12 9 c3 23 18 13 9 8 c4 18 15 18 11 10 Table 5 Counts how often SO-PhyL using different values of I 0 has learned the highest scoring network. Rows show results of networks fragmented by their number of nodes, parents and cardinality. The boldface numbers indicate best results. I 0 = 1 I 0 = 2 I 0 = 5 I 0 = 10 I 0 = 20 I 0 = 35 I 0 = 50 All 19 26 33 40 49 53 80 n5 12 13 15 14 14 14 13 n10 4 8 12 12 12 12 14 n15 2 3 3 10 13 11 12 n20 1 2 3 4 7 10 12 n25 0 0 0 0 3 6 14 n50 0 0 0 0 0 0 15 p1 6 9 9 11 13 13 18 p2 5 7 9 11 13 14 17 p3 3 5 6 8 8 10 15 p4 2 2 4 6 6 6 15 p5 3 3 5 4 9 10 15 c2 5 6 8 9 14 16 26 c3 7 10 12 16 18 17 27 c4 7 10 13 15 17 20 27 Table 6 Different configurations for conductivity parameters. In configuration 1–15, initially no connections exceed threshold D τ . In configurations 16–21, some connections are initially higher than D τ and for configurations 22–27, all connections are exceeding D τ at initial state. Config D min D max D τ 0 D τ end 1 0.5 1.0 1.1 1.0 2 0.5 1.0 1.1 1.5 3 0.5 1.0 1.1 2.0 4 0.8 1.0 1.1 1.0 5 0.8 1.0 1.1 1.5 6 0.8 1.0 1.1 2.0 7 0.99 1.0 1.1 1.0 8 0.99 1.0 1.1 1.5 9 0.99 1.0 1.1 2.0 10 0.5 0.51 0.8 0.8 11 0.5 0.51 0.8 1.0 12 0.5 0.51 1.0 1.5 13 0.78 0.79 0.8 0.8 14 0.78 0.79 0.8 1.0 15 0.78 0.79 1.0 1.5 16 0.5 1.0 0.75 1.0 17 0.5 1.0 0.8 1.5 18 0.5 1.0 0.9 1.0 19 0.8 1.0 0.9 1.0 20 0.8 1.0 0.9 1.5 21 0.8 1.0 0.95 1.5 22 0.8 1.0 0.8 0.8 23 0.8 1.0 0.8 1.0 24 0.8 1.0 0.8 1.5 25 0.99 1.0 0.9 1.0 26 0.99 1.0 0.9 1.5 27 0.99 1.0 0.9 2.0 Table 7 Counts of how often different values of k learned the highest scoring network structure. Lines show results fragmented by the number of nodes, parents and cardinality. The boldface numbers indicate best results. k = 1.0 k = 2.0 k = 3.0 k = 4.0 k = 5.0 All 44 54 54 62 71 n5 13 14 14 15 14 n10 12 12 12 13 11 n15 10 10 11 12 12 n20 7 10 8 10 9 n25 2 8 9 9 10 n50 0 0 0 3 15 p1 13 15 14 16 17 p2 11 12 15 14 16 p3 7 8 7 11 12 p4 8 11 8 11 13 p5 5 8 10 10 13 c2 15 16 15 16 21 c3 16 20 18 21 24 c4 13 18 21 25 26 Table 8 Different parameter configurations for SO-PhyL for artificial networks. r E . S . μ λ w D min D max D τ 0 D τ end D limit k SO-PhyL-1 3 10 1 0.2 0.5 0.78 0.79 0.8 0.8 4.5 5 SO-PhyL-2 3 10 1 0.01 0.1 0.78 0.79 0.8 0.8 4.5 5 Table 9 Comparison of five different structure learning algorithms for networks with 5 or 10 nodes. The highest score for each network is written in bold face. Dataset Learner Bayes A T R M E t A_n5_p1_a4_c2 SO-PhyL-1 −3357.08 3 1 2 1 0 0.492 SO-PhyL-2 −3357.08 3 1 2 1 0 0.515 LAGD −3357.08 3 1 2 1 0 0.136 Tabu −3357.08 3 1 2 1 0 0.063 SA −3357.08 3 2 1 1 0 0.171 A_n5_p3_a7_c3 SO-PhyL-1 −4613.24 7 3 4 0 0 0.133 SO-PhyL-2 −4608.62 7 4 3 0 0 0.132 LAGD −4575.17 7 7 0 0 0 0.031 Tabu −4613.24 7 3 4 0 0 0.072 SA −4614.23 7 2 5 0 0 0.215 A_n5_p5_a7_c4 SO-PhyL-1 −5454.76 7 6 1 0 0 0.123 SO-PhyL-2 −5454.76 7 6 1 0 0 0.121 LAGD −5426.88 7 7 0 0 0 0.028 Tabu −5454.76 7 6 1 0 0 0.071 SA −5477.31 8 4 3 0 1 0.467 A_n10_p1_a9_c2 SO-PhyL-1 −4162.01 7 3 2 4 2 0.732 SO-PhyL-2 −4162.01 7 3 2 4 2 0.693 LAGD −4159.06 9 4 2 3 3 0.035 Tabu −4161.09 9 3 2 4 4 0.094 SA −4161.61 11 5 2 2 4 0.212 A_n10_p3_a17_c3 SO-PhyL-1 −7432.82 17 13 3 1 1 1.196 SO-PhyL-2 −7321.45 17 16 1 0 0 1.017 LAGD −7318.79 17 17 0 0 0 0.052 Tabu −7364.46 20 13 4 0 3 0.124 SA −7439.09 22 14 3 0 5 0.482 A_n10_p5_a22_c4 SO-PhyL-1 −12154.17 13 8 4 10 1 0.891 SO-PhyL-2 −12154.17 13 8 4 10 1 0.769 LAGD −12087.45 14 11 3 8 0 0.066 Tabu −12154.45 13 7 5 10 1 0.119 SA −12247.75 16 6 8 8 2 0.463 Table 10 Comparison of five different structure learning algorithms for networks with 15 or 20 nodes. The highest score for each network is written in bold face. Dataset Learner Bayes A T R M E t A_n15_p1_a14_c2 SO-PhyL-1 −8034.11 15 7 5 2 3 3.741 SO-PhyL-2 −8034.11 15 7 5 2 3 4.719 LAGD −8022.11 15 9 4 1 2 0.058 Tabu −8026.04 16 8 5 1 3 0.175 SA −8039.21 17 8 5 1 4 0.216 A_n15_p3_a27_c3 SO-PhyL-1 −12529.00 26 21 3 3 2 4.298 SO-PhyL-2 −12507.52 26 21 3 3 2 4.563 LAGD −12391.77 27 25 2 0 0 0.097 Tabu −12414.65 29 24 3 0 2 0.180 SA −12592.11 32 18 7 2 7 0.503 A_n15_p5_a37_c4 SO-PhyL-1 −18933.27 12 11 1 25 0 2.641 SO-PhyL-2 −18933.27 12 11 1 25 0 2.650 LAGD −18927.73 12 12 0 25 0 0.101 Tabu −19081.44 13 9 3 25 1 0.252 SA −19071.30 11 9 2 26 0 0.279 A_n20_p1_a19_c2 SO-PhyL-1 −9827.77 23 11 6 2 6 15.241 SO-PhyL-2 −9827.77 23 11 6 2 6 23.506 LAGD −9814.13 24 18 0 1 6 0.116 Tabu −9819.85 24 11 6 2 7 0.321 SA −9867.50 34 5 11 3 18 0.258 A_n20_p3_a37_c3 SO-PhyL-1 −18285.67 35 32 3 2 0 16.480 SO-PhyL-2 −18266.23 37 33 2 2 2 17.828 LAGD −18012.18 37 36 1 0 0 0.156 Tabu −18212.56 37 33 3 1 1 0.302 SA −18924.24 38 13 13 11 12 0.443 A_n20_p5_a52_c4 SO-PhyL-1 −25700.10 21 8 9 35 4 10.229 SO-PhyL-2 −25700.10 21 8 9 35 4 12.674 LAGD −25553.45 19 17 2 33 0 0.193 Tabu −25705.61 22 11 7 34 4 0.343 SA −25705.49 21 10 7 35 4 0.413 Table 11 Comparison of five different structure learning algorithms for networks with 25 or 50 nodes. The highest score for each network is written in bold face. Dataset Learner Bayes A T R M E t A_n25_p1_a24_c2 SO-PhyL-1 −12600.69 25 17 4 3 4 34.478 SO-PhyL-2 −12600.63 26 17 4 3 5 68.441 LAGD −12595.36 25 19 1 4 5 0.179 Tabu −12600.83 27 15 6 3 6 0.418 SA −12669.60 38 9 11 4 18 0.257 A_n25_p3_a47_c3 SO-PhyL-1 −22592.73 49 30 11 6 8 60.765 SO-PhyL-2 −22502.16 50 29 11 7 10 72.099 LAGD −21797.08 47 44 3 0 0 0.441 Tabu −22440.99 52 31 11 5 10 0.651 SA −22354.81 57 28 16 3 13 8.351 A_n25_p5_a67_c4 SO-PhyL-1 −32382.57 21 19 1 47 1 20.406 SO-PhyL-2 −32382.57 21 19 1 47 1 29.328 LAGD −32328.02 23 15 5 47 3 0.394 Tabu −32101.45 27 25 1 41 1 0.482 SA −32494.87 24 16 4 47 4 0.439 A_n50_p1_a49_c2 SO-PhyL-1 −25188.25 37 23 9 17 5 222.666 SO-PhyL-2 −25132.44 70 28 10 11 32 3031.382 LAGD −25059.52 72 31 10 8 31 2.439 Tabu −25058.40 70 29 10 10 31 3.520 SA −25167.02 107 23 15 11 69 489.163 A_n50_p3_a97_c3 SO-PhyL-1 −44028.47 88 69 13 15 6 751.566 SO-PhyL-2 −43942.11 91 75 10 12 6 1785.637 LAGD −43520.83 95 83 6 8 6 5.357 Tabu −43773.03 98 76 15 6 7 3.597 SA −44917.91 105 54 26 17 25 0.590 A_n50_p5_a142_c4 SO-PhyL-1 −64241.13 29 21 6 115 2 171.988 SO-PhyL-2 −63934.40 40 26 8 108 6 667.907 LAGD −63318.81 44 41 3 98 0 1.737 Tabu −63605.19 46 32 8 102 6 2.950 SA −63939.13 41 28 8 106 5 0.539 Table 12 Different parameter configurations for SO-PhyL for real networks. r E . S . μ λ w D min D max D τ 0 D τ end D limit k SO-PhyL-1 3 10 1 0.2 0.5 0.78 0.79 0.8 0.8 4.5 5 SO-PhyL-2 3 10 1 0.01 0.1 0.78 0.79 0.8 0.8 4.5 5 SO-PhyL-3 5 15 1 0.2 0.5 0.78 0.79 0.8 0.8 4.5 5 Table 13 Comparison of three different SO-PhyL configurations to LAGD and Tabu Search using seven real benchmark networks. First column shows the name of the networks. Column Nodes shows how many nodes the original network has and column Edges indicated the number of edges in the original network. Remaining columns show the metrics by which learning methods are evaluated. The maximal score achieved for each network is marked bold face. Data set Nodes Edges Learner Bayes A T R M E t Cancer 5 4 SO-PhyL-1 −2235.90 4 2 2 0 0 0.698 SO-PhyL-2 −2235.90 4 2 2 0 0 0.491 SO-PhyL-3 −2235.90 4 2 2 0 0 0.129 LAGD −2235.90 4 2 2 0 0 0.092 Tabu −2235.90 4 2 2 0 0 0.073 Earthquake 5 4 SO-PhyL-1 −521.43 4 4 0 0 0 0.194 SO-PhyL-2 −521.43 4 4 0 0 0 0.117 SO-PhyL-3 −521.43 4 4 0 0 0 0.113 LAGD −521.43 4 4 0 0 0 0.077 Tabu −521.43 4 4 0 0 0 0.077 Asia 8 8 SO-PhyL-1 −2319.06 9 4 2 2 3 0.474 SO-PhyL-2 −2319.06 9 4 2 2 3 0.410 SO-PhyL-3 −2319.06 9 4 2 2 3 0.756 LAGD −2318.45 8 4 3 1 1 0.030 Tabu −2318.45 8 4 3 1 1 0.102 Insurance 27 52 SO-PhyL-1 −15429.57 50 29 12 11 9 106.994 SO-PhyL-2 −15637.49 53 29 12 11 12 144.044 SO-PhyL-3 −15436.49 51 29 12 11 10 261.776 LAGD −15772.38 55 25 13 14 17 0.655 Tabu −15740.36 58 26 14 12 18 1.053 Alarm 37 46 SO-PhyL-1 −11350.81 50 33 8 5 9 191.712 SO-PhyL-2 −11349.07 54 32 8 6 14 478.263 SO-PhyL-3 −11349.40 52 32 8 6 12 486.408 LAGD −11247.43 54 38 4 4 12 0.867 Tabu −11299.76 55 34 8 4 13 1.340 Barley 48 84 SO-PhyL-1 −63576.16 65 15 27 42 23 569.444 SO-PhyL-2 −63200.43 73 15 30 39 28 2048.472 SO-PhyL-3 −63502.07 58 14 26 44 18 1440.081 LAGD −61790.39 81 23 28 33 30 15.067 Tabu −63021.69 80 15 33 36 32 10.189 Hailfinder 56 66 SO-PhyL-1 −51455.73 57 29 15 22 13 664.784 SO-PhyL-2 −51376.04 72 33 15 18 24 2933.756 SO-PhyL-3 −51437.35 59 28 16 22 15 1700.770 LAGD −51322.70 75 35 14 17 26 5.006 Tabu −51374.87 76 32 17 17 27 5.295 Physarum Learner: A bio-inspired way of learning structure from data T. Schön a b M. Stetter b A.M. Tomé c C.G. Puntonet d E.W. Lang a ⁎ a CIML Lab, Dept. Biophysics, University of Regensbug, Germany CIML Lab Dept. Biophysics University of Regensbug Germany b Dept. Bioinformatics, University of Applied Science Weihenstephan-Triesdorf, Germany Dept. Bioinformatics University of Applied Science Weihenstephan-Triesdorf Germany c DETI – IEETA, Universidade de Aveiro, Portugal DETI – IEETA Universidade de Aveiro Portugal d Dept. Architecture and Technology of Computers, Universidad Granada, Spain Dept. Architecture and Technology of Computers Universidad Granada Spain ⁎ Corresponding author. Tel.: +49 941 943 2599; fax: +49 941 943 2479. A novel Score-based Physarum Learner algorithm for learning Bayesian Network structure from data is introduced and shown to outperform common score based structure learning algorithms for some benchmark data sets. The Score-based Physarum Learner first initializes a fully connected Physarum-Maze with random conductances. In each Physarum Solver iteration, the source and sink nodes are changed randomly, and the conductances are updated. Connections exceeding a predefined conductance threshold are considered as Bayesian Network edges, and the score of the connected nodes are examined in both directions. A positive or negative feedback is given to the edge conductance based on the calculated scores. Due to randomness in selecting connections for evaluation, an ensemble of Score-based Physarum Learner is used to build the final Bayesian Network structure. Keywords Physarum Learner Structure learning Bayesian Network 1 Introduction Bioinformatics intends to help solving biological problems with the help of computer science. In favorable cases, however, it is just the other way round and nature offers solutions for algorithmic problems as is the case with the slime mold Physarum polycephalum. The latter recently has emerged as a fascinating example of biological computation through morphogenesis. Through its growth process, this single cell organism was shown to be able to solve various minimum cost flow problems (Nakagaki, Yamada, & Toth, 2000). Recently, a number of bio-inspired optimization methods based on the Physarum Solver model proposed by Tero, Kobayashi, and Nakagaki (2006) have been proposed and applied to various optimization problems. Following we provide a short survey of the current literature about the subject and motivate our own research in this field. 1.1 Finding shortest paths Nakagaki et al. (2000) convincingly demonstrated how the slime mold P. polycephalum can indeed find the shortest path between two food sources in a given maze. In an environment with distributed food sources, the plasmodium forms a network of protoplasmic tubes connecting these food sources. This network thus represents a planar graph with food sources as nodes and tubes as edges. Nakagaki, Yamada, and Tóth (2001, 2007) demonstrated that the topology of the network optimizes harvesting on scattered food sources and renders the flow and transport of intracellular components more efficient. Consequently, research efforts in the following years focused on examining the strategy used by Physarum to understand network formation (Niizatoa, Shirakawab, & Gunjia, 2010) and the adaptive dynamics of its transport network (Nakagaki et al., 2001; Nakagaki, 2001; Nakagaki, Yamada, & Hara, 2004). Indeed, learning how biological systems solve problems (Nakagaki, Tero, Kobayashi, Ohnishi, & Miyaji, 2009) could help designing new methods of computation. Information processing in simple cellular organisms thus is interesting to learn about solving combinatorial optimization problems, such as solving a maze or a shortest paths network problem. Tero et al. (2006, 2007) were the first to propose a mathematical model for the underlying transport network based on hydrodynamics. They showed that their model, (called Physarum Solver) (PhySol), can solve the maze introduced by Nakagaki et al. (2000) in the same manner as the biological slime mold does. Further, Tero, Yumiki, Kobayashi, Saigusa, and Nakagaki (2008) adapted the mathematical model to problems with more than two food sources by randomly changing the source and sink node within the network at each iteration of the solving algorithm. Last but not least, Nakagaki et al. (2007) described a mathematical model for an adaptive – tube network which was in good agreement with a risk minimizing behavior of a real slime mold whose embedding maze was exposed locally to light (see also Fricker, Boddy, Nakagaki, & Bebber (2009)). Roughly at the same time, Miyaji and Ohnishi (2007) proved that Tero’s model with linear dynamics converges to an optimal solution in any instance of a planar graph. Miyaji and Ohnishi (2008) later proved with mathematical rigor the shortest path-finding ability of the Physarum Solver model on a graph embedded in a two-dimensional manifold with Riemannian metric, i. e. a Riemannian surface. They showed that the equilibrium point corresponding to the shortest path is globally asymptotically stable for the Physarum Solver on a Riemannian surface. They also extended their proof to a surface composed of a finite number of Riemannian surfaces pasted with one another continuously. A detailed mathematical analysis and convergence study by Miyaji and Ohnishi (2008) and Brummitt et al. (2010) further showed that in ring-shaped and Wheatstone bridge-shaped networks the global asymptotically stable equilibrium point of the model corresponds to the shortest path connecting two special points on the network in cases where the shortest paths are determined uniquely. Miyaji, Ohnishi, Tero, and Nakagaki (2008) also explained why in laboratory experiments with networks encompassing a double-edge, P. polycephalum sometimes yields erroneous results. Similarly, Ito, Johansson, Nakagaki, and Tero (2011) discussed shortest path finding of the Physarum Solver on a di-graph G = ( V , E ) , where E denotes edges and V vertices or nodes of the graph. They proved that the volumetric flow rate Q ( t ) , following Fick’s law for stationary flows, and the related conductance D ( t ) , considered a time-dependent state variable, converge with an exponential rate to a positive flow minimizing the total length or cost L ( Q ) = ∑ ij L ij Q ij . The limit flow has full support on an optimal set H which often forms a spanning subgraph. Bonifaci, Mehlhorn, and Varma (2012) proved that, considering the Physarum Solver model, the mass of the mold will converge to the shortest path between two specified nodes of the network that the mold lies on, independently of the initial mass distribution or the structure of the network, thus relieving the restriction to planar graphs. In addition, Bonifaci (2013) recently presented a short proof of a standard model for the P. polycephalum slime mold which correctly computes the shortest path in an undirected weighted graph. Furthermore, Bechetti, Bonifaci, Dirnberger, Karrenbauer, and Mehlhorn (2013) proved that a discretization (Euler integration) of the coupled system of differential equations, as proposed by Tero et al. (2007), computes a ( 1 + ∊ ) – approximation of the shortest path in O ( mL ( log ( n ) + log ( L ) ) ∊ - 3 iterations. Arithmetics were performed on numbers of O ( log ( nL ∊ - 1 ) bits, where n and m represent the number of nodes and edges of the graph, respectively, and L measures the largest length of an edge. In addition, the authors also obtained two results for a directed Physarum model, proposed by Ito et al. (2011), concerning convergence in the general, nonuniform case and convergence and complexity bounds for the discretization of the uniform case (see also an enlightening discussion by Richard Lipton at In an attempt to understand locomotion and morphogenesis of the slime mold, Gunjia, Shirakawab, Niizatoa, Yamachiyoa, and Tania (2011) recently presented an adaptive and robust vacant – particle transportation model. They could emulate the network formed by P. polycephalum and reproduce morphogenic patterns quantitatively. This corroborates that both, the slime mold as well as the model, balance the exploration–exploitation trade-off inherent to many bio-inspired optimization algorithms. Similarly, Johannson and Zou (2012) analyzed a mathematical model of the amoeboid growth dynamics. They showed how to encode general linear programming (LP) problems as instances of PhySol and proved that under the growth dynamics considered, PhySol is guaranteed to converge to the optimal solution of the LP. 1.2 Network optimization problems Tero et al. (2010) recently discussed rules for biologically inspired adaptive network design. Robust network performance involves a complex trade-off between cost, transport efficiency and fault tolerance. In addition, they develop without centralized control and may represent a readily scalable solution for growing networks in general. Zhang, Zhang, Hu, Deng, and Mahadevan (2013) discussed network optimization problems, more specifically constrained shortest path (CSP) problems. The authors combine an adaptive amoeba algorithm with a Lagrangian relaxation algorithm to solve the CSP problem. Within a two-step process they first solve the shortest path problem (SPP) in a directed network, and in a second step the adaptive amoeba model is combined with a Lagrangian relaxation method to solve the CSP problem. Recently, Zhang and Liu et al. (2013) extended their adaptive amoeba algorithm to address shortest path tree (SPT) problems in dynamic graphs. In the latter edge weight updates can result in either edge weight increases or decreases or a mix of both. Existing work analyzes the nodes influenced by edge weight updates and recompute these affected vertices. In large networks, this approach becomes very complex and computationally costly, however. The adaptive amoeba algorithm can recognize edge weight updates and the affected vertices and reconstruct them spontaneously. To evaluate the proposed adaptive amoeba algorithm, the authors compared it with the Label Setting algorithm and Bellman-Ford algorithm. Besides solving shortest path finding problems, recent work also considers characteristics of the underlying networks. Zhang, Zhang, Wei, and Deng (2012) studied centrality measures in weighted networks based on an amoeboid algorithm. For weighted networks, the authors proposed a new centrality measure based on an amoeboid algorithm, which they called Physarum centrality. In addition to shortest paths, the measure also includes contributions from competing paths. With PhySol, each edge is endowed with a flux and edges on shorter paths are of greater flux. By defining flux as criticality of edges, Physarum centrality of a node is simply calculated by summing up the criticality of edges attached to it. Biological systems, like the slime mold P. polycephalum, are composed of unreliable components which self-organize into balanced systems with respect to efficiency and robustness. Becker (2011) therefore studied slime mold inspired algorithms for constructing fault tolerant connection networks. Given that transport network solutions learnt with PhySol suffer from a strong dependence on a single parameter, the author presents an agent-based approach for constructing fault tolerant connection networks using PhySol. The agent-based simulation reproduced the variance in the behavior of the natural slime mold much better. However, analysis of the cost-benefit ratio of bio-inspired network construction lead the author to conclude that considering classical efficient computational algorithms for the problem of constructing minimal fault tolerant networks might be worthwhile. Following similar ideas, Li, Torres, Thomas, Rossi, and Shen (2011) considered slime mold inspired routing protocols for wireless sensor networks. Taking both efficiency and robustness into account, the authors exploit two different mechanisms to design localized routing protocols for wireless sensor networks (WSNs). Inspired by the slime mold’s mechanism of path growth, the authors construct path growth routing protocols by treating data sources and sinks as singular potentials thereby establishing routes from the sinks to all the data sources. Similarly, in the plasmodium, path evolution is driven by a non-linear feedback mechanism of flux intensity onto tube diameter. The authors thus adapted tube dynamics for the purpose of a path evolution routing protocol and identify one key adaptation parameter to adjust the trade-off between efficiency and robustness of network routes. Inspired by the slime mold’s flow dynamics which is driven by non-linear feedback mechanisms, Sohier, Georgiadis, Clavière, Papatriantafilou, and Bui (2012) studied self-biased walkers for distributed clustering. They proposed a mechanism based on the flow of distributed walkers, circulating randomly between a source and a sink to compute a shortest path. Similar in spirit to other bio-inspired optimization algorithms, each time a walker takes an edge, it reinforces the probability that subsequent walkers take it, too. Based on this feedback mechanism, the authors showed how several sources compute a shortest path directed acyclic graph (DAG) to a given sink node. Given some cluster-heads, acting like sinks, this process is shown to converge to distance-based clusters with shortest-path DAGs. The algorithm is designed with a special focus on dynamic networks: the flow locally adapts to the appearance and disappearance of links and nodes, including cluster heads. Georgiadis (2013), in his Ph.D. thesis, discussed aspects of modeling and constructing unstructured overlay networks with special emphasis on smart grids (SG). As well as a multitude of sensor, mobile and wireless networks, SGs herald a new era of unprecedented connectivity and networking. In his thesis, the author provided algorithms and techniques that enable users and overlay designers to model, construct and address practical considerations of such overlay networks. With this thesis, network designers are offered a framework for the analysis and systematic study of probabilistic techniques such as random walks in close conjunction with unstructured overlay networks. Inspired by the natural computing ability of P. Polycephalum and the well known relation between random walks and electric circuits, the author proposed a randomized distributed algorithm that uses walkers to construct a distance-based clustered overlay. Considering smart grids, the author presented a novel modeling method on resource dispatch in power grids that takes into account node preferences and domain-related constraints. The proposed algorithm uses only local information to solve the problem with guaranteed performance, Masi and Vasile (2013), finally, analyzed the potential of a multi-directional modified Physarum algorithm for optimal multi-objective discrete decision making. The proposed algorithm incrementally grows decision graphs in multiple directions for discrete multi-objective optimization. Applications to multi-objective traveling salesman (TS) and vehicle routing (VR) problems revealed that a multi-directional approach is an advantageous compared to an unidirectional approach. Evaluating decisions from multiple directions enhances the performance of the solver in the construction and selection of optimal decision sequences. 1.3 Physarum Machines and computers Adamatzky (2010) published a book, called Physarum Machines where the computing power of Physarum-like machines was summarized. There it is demonstrated how to create experimental Physarum Machines for computational geometry and optimization, distributed manipulation and transportation, and general-purpose computation. Such experimental laboratory prototypes of Physarum Machines are designed to solve such diverse problems as computing shortest paths (Nakagaki et al., 2007) or Voronoi diagrams (Shirakawa, Adamatzky, Gunji, & Miyake, 2009), constructing spanning trees (Adamatzky, 2007) and proximity graphs (Adamatzky, 2008), implementing logical gates (Tsuda, Aono, & Gunji, 2004), primitive memory (Saigusa, Tero, Nakagaki, & Kuramoto, 2008) and spatial logic and process algebra (Schumann & Adamatzky, 2009) as well as controlling a robot (Tsuda, Zauner, & Gunji, 2007, 2009). These experiments showed that the slime mold functions as a parallel amorphous computer where data are represented by spatial configurations of food sources. Computer code is implemented via configurations of repellents and attractants. Results are represented by network configurations and the localization of the plasmodium. In summary, the plasmodium of P. polycephalum resembles a parallel computing substrate which is often more efficient than a massively parallel reaction–diffusion chemical processor (Adamatzky, de Lacy Costello, & Asai, 2005). 1.4 Motivation and outline Maze-solving represents an NP-hard optimization problem much like the problem of learning structure from data. Being aware of the elegant and efficient maze-solving strategy of the slime mold P. polycephalum, in 2012, we thus presented a novel approach where the strategy used by Physarum has been adapted to solving the NP-hard problem of learning structure of a Bayesian Network from data (Schoen, Stetter, & Lang, 2012, 2013). We showed that it is generally possible to use the Physarum Solver introduced by Tero et al. (2006) for structure learning with a novel algorithm called Physarum Learner. This algorithm, however, still suffered from some limitations as the directions of the learnt connections were required to be known and the data sets had to be binary. In this research contribution, we present an improved Physarum Learner algorithm for learning structure-from-data by integrating a Bayesian score into the Physarum Learner to overcome the afore mentioned limitations. Hence, the goal was to provide an algorithm that learns a directed acyclic graph (DAG), more specifically a Bayesian Network, from a given set of observations. The structure of the paper is as follows: Section 1 provides a motivation for the work as well as a short survey of Physarum-based computing and its applications. Section 2 provides some background material concerning Bayesian Networks and the Physarum Solver model. It then describes in much detail the new Physarum Leraner model including scoring for learning the directions of edges in the network. Section 3 provides a thorough discussion of estimating proper parameters of the model and provides the learning dynamics of model development concerning all parameters along the simulations. Section 4 considers learning-structure-from-data applications of the new algorithm to a wide range of artificial and real world benchmark networks, being either sparsely, normally or densely connected. Finally, Section 5 provides a conclusion to the work. Several algorithms are given in an Appendix in Section 6. 2 Materials and methods The main goal of the investigation is to develop a bioinspired optimization method for solving the NP-hard problem of learning structure from data. Following, we provide some background on directed acyclic graphs, more specifically Bayesian Networks, and the Physarum Solver which provides a bioinspired way to solve the NP-hard problem of shortest path finding. Subsequently we present our new Physarum Learner algorithm and discuss its properties. 2.1 Bayesian Network A Bayesian Network (Pearl, 1988) is a probabilistic graphical model represented by a directed acyclic graph (DAG) and, for categorical data, a conditional probability table (CPT) for each node where a node represents a random variable. The CPT for a node X i defines the conditional probability distribution P ( X i | Par ( X i ) ) , given its parents Par ( X i ) . As the scope of this paper is on structure learning, estimating the CPT’s is not discussed any further. An interested reader is referred to Koller and Friedman (2009) and Kjaerulff and Madsen (2005). Learning the graph structure from data is NP complete, and most exact methods are limited to about 25 nodes (Parviainen & Koivisto, 2009; Koivisto & Sood, 2004). Several heuristic methods have been introduced that can handle larger networks, but they often get stuck in local optima (Koller & Friedman, 2009). Most of these heuristic search approaches optimize the correspondence of the learned structure with the data as measured by a score metric (Bouchaala, Masmoudi, Gargouri, & Rebai, 2010). Probably the most famous search strategies are K2 (Cooper & Herskovits, 1992), Simulated Annealing (Kirkpatrick, Gelatt, & Vecchi, 1983) and variations of the greedy hill climbing algorithm (Lin & Kernighan, 1973). In Abramovici, Neubach, Fathi, and Holland (2008) introduced a novel hill climbing algorithm, called LAGD hill climbing algorithm, which considers a sequence of best moves in each step instead of considering a single move only. The two most commonly used scoring functions are the Bayesian Score (BDeu) (Heckerman, Geiger, & Chickering, 1995) and the minimum description length (MDL) (Lam & Bacchus, 1994). 2.2 Physarum Solver As mentioned above, the slime mould P. polycephalum is able to efficiently solve the NP – hard shortest path problem. Tero et al. (2007) introduced a graphical model for the maze created by Nakagaki et al. (2000). As this algorithm, henceforth called Physarum Solver (PhySol), forms the basis of the new Physarum Learner algorithm to be discussed in the following, a short summary of the PhySol – algorithm will be presented here. For a wide-sense stationary system, the mass flux Q ij through the tube segment M ij between two nodes N i and N j follows Fick’s first law (1) Q ij = - D ij ∇ p ij = D ij · ( p j - p i ) L ij = G ij · ( p j - p i ) where G ij = D ij L ij = σ · Φ ij L ij is the macroscopic conductance and σ the related microscopic conductivity, Φ ij the cross-section and L ij the length of the edge M ij and D ij the related diffusion coefficient. Finally, the pressures at node N i and N j , representing free energy densities, are denoted by p i and p j . Considering graphical models, the legth of the edges can be set to a constant value L ij = L 0 ∀ i , j , hence diffusion coefficient and conductance can be treated on an equal footing. Following, we sloppily speak of D ij as the conductance of the edge therefore. By considering mass conservation ∑ j Q ij = 0 following Kirchhoff’s first law, the Poisson equation for the pressures result. At the input nodes N 1 , N 2 , the in- and outflow is denoted I 0 , hence (2) ∑ j Q 1 j - I 0 = 0 ∑ j Q 2 j + I 0 = 0 The flux through each edge is controlled by a time-dependent conductance G ij ( t ) ∼ D ij ( t ) following a Master equation (3) ∂ D ij ∂ t = f ( | Q ij | ) - aD ij , a > 0 which models a balance between a positive feedback term, causing conductance to increase with increasing flux, and a relaxation term, with a a positive decay rate of the first order kinetics, which occasionally causes edges to vanish from the graph. For the Physarum Solver, it is assumed: (4) f ( | Q ij | ) = | Q ij μ | where μ = 1 was set by default in Tero et al. (2006). A more realistic functional form for multiple input–output nodes is (5) f ( | Q ij | ) = | Q ij | ( γ - 1 + | Q ij | ) where γ accounts for some non-linear deviations from Fick’s first law, and the sigmoidal shape accounts for saturation effects. 2.3 Physarum Learner – the SO-PhyL algorithm The Physarum Learner (Schoen et al., 2012, Schoen, Stetter, & Lang, 2013) (SO-PhyL) is a new development which, much like its predecessor C-PhyL (Schoen et al., 2012), builds upon the Physarum Solver but extends it with a scoring scheme to learn a directed acyclic graph, i. e. a Bayesian Network structure, from data. Other than the correlation-based approach taken with C-PhyL, the new SO-PhyL algorithm operates by optimizing a proper scoring function while searching the space of all possible network structures. Thus, SO-PhyL belongs to the group of score-based search algorithms similar to hill climbing approaches like, for example, the LAGD algorithm (Holland, Fathi, Abramovici, & Neubach, 2008) or a Tabu Search (Glover & McMillan, 1986; Glover, 1990). Learning structure from data by employing a bioinspired optimization method based on the Physarum Solver has not been discussed in literature and represents an original and new contribution to the field. Following we present the new Physarum Learner in detail and discuss its properties. 2.3.1 Initializing the Physarum-Maze A Bayesian Network B represented by graph G containing a node for each attribute of the data set and an empty set of edges E is initialized (see the Algorithm 1 given in the appendix). The Physarum Learner thus first transforms the data set into a graphical model which is called a Physarum-Maze. A fully connected Physarum-Maze is generated by adding a node for each attribute in the data set and by inserting edges between all pairs of nodes. The initial conductances D ij ( t 0 ) of the edges are chosen randomly in the range of D min ⩽ D ij ( t 0 ) ⩽ D max . The length L ij of the connections is set to a constant value of L 0 = 1.0 . In this way actually the stationary pressure gradient ∇ p , which forms the driving force for the fluxes, is solely determined by the pressure differences Δ p ij between the nodes N i and N j . As SO-PhyL transforms the Physarum-Maze to a Bayesian Network, i. e. an acyclic graph with directed edges, in each iteration, the flux direction through each such connection – note that following we loosely speak of directed edges – needs to be updated at any time, i. e. the sign of Δ p ij needs to be determined. Initially, edge directions are chosen at random. At this stage it is not yet necessary to check if directions are valid in the sense that the resulting Bayesian Network forms an acyclic graph, or that the number of parents per node is exceeded. Initial directions are only needed to initialize the Physarum-Maze in a valid state to be used by SO-PhyL. Edge directions are evaluated correctly after the first iteration is completed. 2.3.2 Scoring of edge connections The basic idea of SO-PhyL now is to transform the Physarum-Maze to a Bayesian Network during each iteration of the Score-based Physarum Learner. The transformation can be achieved by removing edges M ij with a conductance value D ij ⩽ D th less than a predefined threshold D th . To do so, the contribution of each connection to the score of the Bayesian Network is evaluated in each iteration. Then the conductance of edges which improve the score of the network becomes increased and the conductance of edges which diminish the score is decreased. Obviously, the canonical Physarum Solver is no longer applicable if there are more than two input nodes. Rather, each node is considered an input node once, thereby avoiding that nodes become cut out from the Bayesian Network. Hence, SO-PhyL uses the Physarum Solver updated for multiple input nodes (Tero et al., 2008), henceforth referred to as MFS-Physarum Solver. Following Tero et al. (2008), the Physarum Solver with multiple input–output nodes is run by switching between pairs of nodes in each Physarum Solver iteration. First, the input node, called source node, is chosen randomly from the set of nodes. Then, the output node, called sink node, is chosen randomly too, but the probability of each node to be selected as sink node is growing with distance to the source node. As the nodes in the Physarum-Maze generated for the Score-based Physarum Learner are all connected to each other directly, the distances between the nodes have equal length L 0 = 1 . Selecting a pair of source and sink nodes at random leads to an imbalance of preferred paths as the chance for a connection to survive grows with the number of times it is part of the shortest path. Therefore, a list of all possible pairs of nodes is generated from which a pair of nodes is chosen randomly in each MFS-Physarum Solver iteration. These two nodes are set as source and sink nodes and, simultaneously, are removed from the list. Consequently, the number of total MFS-Physarum Solver iterations has to be defined as a multiple of the number of valid node pairs. Hence, a parameter r can be defined as configuration parameter which does not represent the total number of MFS-Physarum Solver iterations, but determines how often the list of all possible node pairs is processed. The total number of MFS-Physarum Solver iterations then can be calculated by (6) r total = r Z n · ( Z n - 1 ) 2 where Z n is the number of nodes in the Physarum-Maze. Hence the number of iterations roughly scales as r total ∝ Z N 2 . This procedure guarantees that each node becomes considered an input node an equal number of times. In this way, the Physarum Learner algorithm balances the selection process in that each node is set as source or sink node as often as any other node. As soon as source and sink nodes are selected, the pressures, fluxes and conductances are updated according to the Physarum Solver model equations. Note that in the Physarum Learner, the conductance D ij ( t ) is considered a generic dynamic parameter of the model disregarding its connection to the microscopic transport coefficient, i. e. the inherent conductivity σ of the system, and related geometric parameters of the conducting medium as in a real physical model. Different from the canonical Physarum Solver, the equation determining the dynamics of D ij has been extended by a weighting constant w to include the previous value of the conductance in a spirit similar to the momentum term known from neural network theory. It allows to manipulate the speed of adapting the conductance as shown in Eq. (7) with λ being a constant. (7) D ij new = wf ( | Q ij | ) + ( 1 - λ w ) D ij old Afterwards, each connection is checked if its updated conductance value exceeds a predefined threshold D τ . The latter is adjusted linearly from its initial value D τ 0 to D τ end across r total iterations according to (8) D τ ( i ) = D τ 0 + i r total ( D τ end - D τ 0 ) where i indicates the current iteration. If the threshold is surpassed D ij ( i ) ⩽ D τ ( i ) , the connection is considered a valid connection of the Bayesian Network and its conductance is included into subsequent iterations. Note that, at this point, the connection is still undirected. Next, a positive or negative feedback is given to the conductance by evaluating the scores of the two connected nodes, say nodes X i and X j . The flux could be directed either from X i → X j or from X j → X i . The flux direction, reflecting the direction of the edge, is determined by evaluating the score of both nodes in either case with respect to the question: Does adding the other node as parent node increase or decrease the score of the node considered? Therefore, for each direction, the score of the child node is calculated with or without having the other node as parent. This difference of the scores is determined for either direction. Consider, for example, edge M ij which can be an edge with direction E ij = X i → X j or E ji = X j → X i , respectively. For both directions, the difference in scores is calculated as (9) score Diff ( E ij ) = score X j ( X i ∈ Pa ( X j ) ) - score X j ( X i ∉ Pa ( X j ) ) (10) score Diff ( E ji ) = score X i ( X j ∈ Pa ( X i ) ) - score X i ( X j ∉ Pa ( X i ) ) where the score of the child node without the parent node is subtracted from the score of the child including the parent node in its parent set. A positive value of score Diff means that adding the connection increases the score while a negative value means that the score is decreased. The more profitable direction can easily be determined by comparing score Diff ( E ij ) and score Diff ( E ji ) whereby the direction with higher score improvement is chosen finally. Next, the conductances of the edges which either have been added to the Bayesian Network or which remained in the white-list are updated based on the relation between the scores with and without the extra parent with respect to the selected direction: (11) D ij = D ij + k ( 1 - β ) β = score X j ( X i ∈ Pa ( X j ) ) score X j ( X i ∉ Pa ( X j ) ) if score Diff ( E ij ) > score Diff ( E ji ) score X i ( X j ∈ Pa ( X i ) ) score X i ( X j ∉ Pa ( X i ) ) else If β < 1 , the score with the connection added is higher than the score without extra connection as all scores are negative. If β > 1 , adding the connection would result in a decrease in total score. The conductances D ij are thus updated by adding a positive or negative value which is proportional to the increase or decrease in score when adding the connection. Hence, if a connection was selected by MFS-Physarum Solver that decreases the score, D ij is lowered so that the probability that this connection is chosen by the next iteration of the MFS-Physarum Solver is decreased. On the other hand, if adding the connection increases the score, the conductivity is increased too and the connection is more likely to be part of a shortest path in the next MFS-Physarum Solver iteration. The amount of strengthening or diminishing the conductance D ij can be controlled by the constant k. However, an upper limit D limit ⩾ D ij ⩾ 0 has to be defined preventing an unlimited growth of the conductances. Also note that before a directed edge is added to the Bayesian Network, it is verified that the connection does not generate a cycle, and that the child node is valid to accept another parent (see function addArkMakesSense () in Algorithm 3). Before continuing with the next MFS-Physarum Solver iteration, the global score of Bayesian Network B is determined, and SO-PhyL stores in memory the highest scoring network across all r total iterations. Stable structure learning performance, however, is only achieved if an ensemble of SO-PhyLs is used from which the highest scoring network is selected as final Bayesian Network. Finally, conditional probability distributions of B regarding G are estimated using any parameter estimation method (the Weka tool box has been used here for this purpose). 3 Results of parameter estimation Four real data sets are considered to evaluate the performance of the new SO-PhyL algorithm: the Asia data set (8 nodes, 8 edges, average degree of 2) (Lauritzen & Spiegelhalter, 1988), the Cancer data set (Korb & Nicholson, 2010) and the Earthquake data set (5 nodes, 4 edges, average degree of 1.6, each) (Korb & Nicholson, 2010) as well as the well-known Alarm data set (37 nodes, 46 edges, average degree of 2.49) (Beinlich, Suermondt, Chavez, & Cooper, 1989). The SO-PhyL algorithm is parameterized by a set of variables which influence the learning dynamics. Table 1 collects these parameters and their default values if not stated otherwise. The first column indicates the parameter and the second column shows in which algorithm the parameter is used. Default values, derived from preliminary simulations and being used in the following experiments, unless stated otherwise, are given in the last column. For all structure learning algorithms used within this paper, the number of maximal parents per node is limited to five parents, as no benchmark network with higher in-degree is used. Learning performance of SO-PhyL under different parameter configurations is evaluated using data sets of 1000 instances. 3.1 Number of iterations The SO-PhyL algorithm optimizes a score needed to update the conductances of the edges and to estimate the direction of flow in these edges. During each of the r total MFS-Physarum Solver iterations, scores need to be computed in each step which renders the whole procedure computationally expensive, especially for large networks. However, a minimal number of iterations is needed for estimating the scores reliably. This minimal number of iterations needs to be optimized to render the SO-PhyL algorithm tractable and efficient. Therefore using SO-PhyL, simulations were run with eight values of r ∈ { 1 , 2 , 3 , 4 , 5 , 10 , 50 and 100 } to learn the network structure of artificial networks with different characteristics. The networks vary in their number of nodes ( Z N = 5 , 10 , 15 , 20 , 25 ) . Furthermore, nodes in these networks can have a maximal number of parents of either N P = 1 , 2 , 3 , 4 , 5 and a cardinality of C = 2 , 3 , 4 resulting in a total of 75 networks. Each network has been learnt with each of the values of r, and the number of times a network with highest Bayesian score resulted for any r is collected in Table 2 . The first row shows the number of times SO-PhyL, using any specific r, was able to learn a network structure with highest Bayesian score. Subsequent rows show results for specific networks sharing the same number of nodes, parents or the same cardinality. With increasing r, and thus increasing number of iterations, a highly scoring network structure has been found more often. More iterations provide better chances to optimize the score, obviously. It can also be seen, that for networks with a small number of nodes, less iterations are needed to achieve an optimal score during learning. The number of possible edges between nodes grows quadratically with the number of nodes and thus more options have to be evaluated with increasing number of nodes, thereby increasing the number of necessary iterations considerably. On the other hand, variations in the number of parents or the cardinality seem to have a weak impact only on the learning performance. Although increasing r improves the quality of the structure of the learnt network, it also increases the computational load dramatically. This is illustrated for two different networks, A_n5_p3_a7_c3 and A_n50_p3_a97_c3, respectively, where the score development is plotted as a function of the number of iterations. Fig. 1 shows the development of the score for network A_n5_p3_a7_c3 during one SO-PhyL simulation using r = 10 , resulting in r total = 100 iterations. It can be seen that a network structure with maximal score has been learnt already after eight iterations. This result is in line with the results presented in Table 2 where r = 1 was enough for networks with 5 nodes to find an optimal, i. e. highly scoring, network structure. The subsequent iterations do not achieve any further score improvement. Rather a dynamic instability can be observed driven either by noise artifacts within the data set or by fluctuations of the MFS-Physarum Solver. Thus, learning small networks with a high value of r easily results in overfitting. Score dynamics for the largest network with 50 nodes A_n50_p3_a97_c3 is shown in Fig. 2 where again one SO-PhyL with r = 10 resulting in r total = 12 , 250 iterations has been performed. Once more a major score improvement is achieved in the beginning ( r = 3 , corresponding to 3675 iterations), whereas subsequent iterations only lead to a slight further improvement of the score at the cost of an exploding execution time. It can thus be concluded, that learning network structures using SO-PhyL with small values of r < 5 offers a good compromise in balancing score optimization and computational load. However, preliminary results indicated a strong dependence of the optimal score achieved on the initialization of the score dynamics. It is thus necessary to use an ensemble of SO-PhyLs with a small number r of iterations and select the best scoring network obtained. 3.2 Ensemble size The networks just mentioned have been used also to test if an ensemble size of E = 10 is appropriate. It is further examined how stable learning results are by performing the same experiment for each data set ten times. Concerning the number of iterations, r = 3 is chosen. Obviously, increasing ensemble size increases the probability of finding networks with high scores. On the other hand, the computational load increases in proportion to the ensemble size. Simulations employing the data set A_n5_p3_a7_c3 revealed that all ten runs delivered exactly the same network structure. Hence, an ensemble size E = 10 is sufficient to yield a robust learning performance for networks with only five nodes. Also for a network with n = 20 nodes, an ensemble of E = 10 learners is still sufficient. Only slight variations between the ten runs have been observed in the simulations in this case. For the largest network A_n50_p3_a97_c3, learning results for an ensemble size E = 10 and 10 individually performed runs are given in Table 3 . Results show that characteristic measures vary slightly between individual runs. Here, learned structures differ in at most 3 edges. These variations, however, already exhibit a noticeable influence on the resulting Bayesian score. Increasing both, r and the ensemble size E could stabilize the performance at the expense of a major increase in execution time. Hence, small variations in network structure were accepted in subsequent simulations in order to keep execution time short, and thus the number of learners used is set to E = 10 for all following simulations. 3.3 Exponent μ of squashing function Exponent μ determines the slope of the squashing function describing conductance dynamics by Eq. (4). The canonical Physarum Solver uses a value of μ = 1.0 which assured finding the shortest path between two sources within a maze in all experiments reported. For the extension to the multiple input–output situation, no systematic investigation concerning the impact of μ upon the conductance dynamics exists. Tero et al. simply report different values of μ for their experiments using the MFS-Physarum Solver. In this study, five different values of μ ∈ { 1.0 , 1.2 , 1.5 , 2.0 and 3.0 } were tested in order to estimate a proper value where SO-PhyL performs best concerning Bayesian scores. Hence, a set of 90 benchmark networks, including the 75 networks already introduced plus 15 additional networks with 50 nodes in the same configuration as the other networks, was analyzed. Each of the networks has been learnt with each of the values of μ , and the number of times where SO-PhyL learned the highest scoring network was counted. Results are presented in Table 4 and are grouped again by the number of nodes, parents and cardinality as described in previous simulations. Table 4 indicates a clear preference for μ = 1.0 . Indeed, results were getting worse with increasing values of μ and the worsening became quite pronounced in networks with a large number of nodes. Hence, a value of μ = 1.0 was used for further simulations to be discussed next. 3.4 Momentum parameter λ Parameter λ of the conductance dynamics defines the weight given to the value of D ij ( t ) of the last iteration when calculating an updated value of D ij ( t + 1 ) . The influence of λ onto learning performance was tested again using the 90 benchmark networks, and λ ∈ { 0.01 , 0.1 , 0.2 , 0.5.0.75 , 1.0 } was chosen while the other parameters were set to w = 0.5 , r = 3 , μ = 1.0 , E = 10 in accord with the experiences reported so far. For 84 of the 90 networks, with λ = 0.01 learning resulted in a network structure with maximal score. In the remaining six networks, λ = 0.1 was needed to achieve maximal scores. The simulations further indicated that different values of λ led to strong fluctuations in the related Bayesian scores. Indeed, for small values of λ a continuous growth in conductance is observed which quickly exceeded the threshold D τ . In such a state, most of the connections are participating as valid connections to the Bayesian Network. Learning then performs similarly to greedy hill climbing with a concomitant strong increase in computational load: reducing λ = 1 → 0.01 increases execution time 5-times in a network with n = 10 nodes and 10-fold in a network with n = 50 nodes. The growth in number of critical connections, i. e. where the conductance exceeds the threshold D ij ⩾ D τ , is illustrated in Fig. 3 for a network with n = 50 nodes. Especially for small values of λ , the number of overcritical edges participating in the Bayesian Network increases strongly, and the increase is the faster the smaller the number of nodes in the network is. Indeed, the relative number of overcritical edges compared to all edges in the network approaches almost one in networks with a small number of nodes, while it remains rather moderate in networks with a large number of nodes due to a relatively small maximal input current I 0 provided to the network. If the latter increases, also in networks with a large number of nodes the relative number of critical nodes would approach one. Given these subtleties, setting λ = 0.2 is considered a good compromise which, on the one hand inhibits continuous growth of the number of active edges and, on the other hand, considers edges only if they have been pushed above critical by score feedback or by selection within the MFS-Physarum Solver. 3.5 Weight parameter w The weight parameter w (see Eq. (7)) controls the influence on conductance dynamics of the MFS-Physarum Solver compared to score feedback. Further, when using a small momentum parameter λ , the weight parameter w also controls the relation between flux and conductance of the MFS-Physarum Solver. Hence, w was expected to have a high impact on learning performance which was tested using values w ∈ { 0.1 , 0.25 , 0.5 , 0.75 , 1.0 } on the same 90 benchmark data sets. With 83 networks the weight parameter w = 0.1 lead to networks with maximal scores. This suggests that the stronger the influence of the score feedback is, the better learning performs. A more detailed insight into the impact of the weight parameter w upon learning dynamics is illustrated in Figs. 4 and 5 . There the development of the Bayesian score and the number of edges with overcritical conductances D ij ⩾ D τ is presented in case of a large network with n = 50 nodes. The network included 97 edges, and again with w = 0.1 , i. e. a stronger impact of score feedback compared to the impact of the MFS-Physarum Solver, almost all edges became overcritical during learning. Thus, conductances changed during learning primarily based on calculated scores. Fig. 6 illustrates the switching-like behavior of the squashing function describing conductance dynamics. It is interesting to see that edges seemed to cluster in a way that some edges switched to high conductance values quickly, while the rest remained with low conductances for most of the learning period. Hence, additional experiments were performed with w = 0.5 to keep balance between influence on conductivity by score feedback and by MFS-Physarum Solver. 3.6 Network input–output I 0 The input I 0 to the system directly influences the fluxes Q ij within the Physarum-Maze, hence influences also the conductances between the nodes and thus the learning dynamics. For the Physarum Solver it was reported (Tero et al., 2008) that settings for I 0 are crucial to the final result when using the Physarum Solver with multiple input–output nodes. In all the experiments reported by Tero, different values of I 0 were used, and obviously no value could be found that worked well for all maze configurations. In SO-PhyL, it is supposed that with increasing I 0 , the number of connections with conductances higher than the threshold D τ increases, too. Consequently this also leads to an increase in the number of learnt connections, especially for networks of larger size. In this study, seven different values I 0 ∈ { 1 , 2 , 5 , 10 , 20 , 35 , 50 } were used for learning the network structures behind the 90 benchmark data sets. The number of times, SO-PhyL is performing best with respect to the Bayesian score is counted and presented in Table 5 . Results clearly favor large inputs I 0 , as expected. With large inputs, more conductances D ij are pushed beyond critical causing them to contribute to the scoring process. However, this also increases the computational load. For example, using an input I 0 = 50 roughly doubles the computational load compared to I 0 = 5 . Small networks with few nodes learn a proper network structure with smaller inputs, as can be seen from Table 5. As a rule of thumb, using as input I 0 ≃ 2 · Z n approximately two times the number of nodes should provide a good trade-off between learning performance and computational load. Next, the number of edges in the network learnt from the data with respect to the number of edges in the original network, where the data set has been sampled from and which was taken from the 90 benchmark networks and seven configurations already mentioned above, was examined for different values of I 0 . The percentage of learned edges was computed by dividing the number of edges learnt by the number of edges in the original network. Then, networks with the same number of nodes are grouped together and the percentage values are averaged among the group. For each value of I 0 , these average percentage values are plotted as a function of the number of nodes in Fig. 7 . With small networks, an almost perfect network structure could be learnt whatever was the amplitude of the input. For larger networks, higher inputs were more beneficial for learning, with an optimum achieved for the group of networks with 20 nodes and an input amplitude in the range 20 ⩽ I 0 ⩽ 50 . Remarkably, the benefit was lost in the largest networks studied where learning became again almost independent of the value of the input I 0 to the network. There only 60 % of the edges existing in the original network could be learnt from the data set. 3.7 Conductance parameters The SO-PhyL algorithm is parameterized by several quantities related to the conductances in the system, i. e. D min , D max , D τ 0 and D τ end , which need to be adjusted properly. According to results obtained so far, all simulations estimating these parameters were carried out with the following set of the remaining model parameters r = 3 , E = 10 , λ = 0.2 , w = 0.5 . The input I 0 was chosen differently for networks with different numbers Z n of nodes in accord with the rule of thumb proposed above: ( Z n = 5 , I 0 = 5 ) , ( Z n = 10 , I 0 = 20 ) , ( Z n = 15 , I 0 = 20 ) , ( Z n = 20 , I 0 = 35 ) , ( Z n = 25 , I 0 = 50 ) , ( Z n = 50 , I 0 = 50 ) . Altogether, 27 different configurations of conductance parameters were used as collected in Table 6 . Note that in configurations 1 - 15 , initially no conductance D ij exceeds the threshold D τ . In configurations 16 - 21 , only few initial conductances exceed D τ , and for configurations 22 - 27 , all initial conductances exceed D τ . In addition, the limiting conductance parameter was chosen from the set D limit ∈ { 1.5 , 2.5 , 3.5 , 4.5 , 6.0 , 10.0 , 15.0 } . The set of 75 benchmark networks encompassing between 5 - 25 nodes, was learnt employing these 27 conductance parameter configurations, and learning was evaluated with regard to their final Bayesian score. Overall, only marginal differences between the scores, obtained with the 27 configurations, could be observed, demonstrating that SO-PhyL robustly converges to rather similar network structures independent of initial conductances. 3.8 Score-feedback impact factor k Finally, the influence of the score feedback impact factor k of Eq. (11) was studied using k ∈ { 1.0 , 2.0 , 3.0 , 4.0 , 5.0 } and the remaining parameters were set as before including D limit = 5 . Learning was performed using the 90 benchmark networks mentioned already. Results show a clear preference for a value k = 5 , where the number of times the highest scoring network has been learnt with respect to a Bayesian score was 71 out of 90 meaning that roughly 70 % of the network structures could be learnt correctly from the data sets presented to the SO-PhyL (see Table 7 ). 4 Discussion of simulations After having estimated a set of SO-PhyL parameters yielding optimal learning performance, benchmark data sets were now used to compare the network structures learnt from the data by SO-PhyL to results obtained by state-of-the-art learning algorithms, more specifically to a greedy hill climbing algorithm (LAGD), a Tabu Search (TS) and Simulated Annealing (SA). Results were compared to the known structures of the benchmark networks. SO-PhyL was used with two different configurations which only differ in the parameters λ and w as given in Table 8 . Furthermore, inputs I 0 were chosen according to the rule of thumb as explained above. The LAGD algorithm was used with 5 good operations and a look ahead step size of 2. Tabu-Search was performed with a tabu list size of 5 and 500 runs were executed. With the SA algorithm, the temperature was initially set to T 0 = 10 and was reduced in each iteration by Δ T = 0.999 . Furthermore, with SA 10 , 000 runs were executed. As all benchmark networks considered do not contain nodes with more than five parents, the maximum in-degree was set to 5 for all learning algorithms. 4.1 Simulations with artificial benchmark networks At first, 18 artificial benchmark networks were selected to compare learning performance of the five learning algorithms mentioned above. Networks have been created with different numbers of nodes, parents and cardinalities to encompass sparsely, normally and densely connected networks (Schoen et al., 2012). For all networks, data sets containing 1000 instances were sampled and used for structure learning. Results were analyzed using metrics already introduced with C-PhyL and are presented in Tables 9–11 , respectively. It can be seen that the greedy hill climbing LAGD algorithm performs best on all but two benchmark networks. Nevertheless, SO-PhyL shows comparable performance with only marginal differences in Bayesian score compared to LAGD. In addition, the second configuration SO-PhyL-2 exhibits a better learning performance than SO-PhyL-1 at the costs of a higher computational load as more connections have to be evaluated for score feedback with SO-PhyL-2. Also in six of the 18 networks, SO-PhyL-2 outperformed Tabu Search and for two more networks, SO-PhyL achieved identical results. Compared to Simulated Annealing, SO-PhyL showed a better learning performance with 14 benchmark networks and learning resulted with an identical network structure for one additional benchmark network. Concerning the different types of networks, SO-PhyL shows a superior learning performance for small networks, but learning becomes less efficient for larger networks. Additional preliminary simulations for parameter estimation showed that choosing a proper parameter configuration of SO-PhyL becomes more difficult in networks with a large number of nodes, simply because more iterations and a larger ensemble size is needed to achieve learning highly scoring network structures. But computational load renders a systematic study with large ensembles prohibitive. Hence, for the SO-PhyL to become a practical alternative to existing approaches, simulations have to be performed on a GPU and code optimization deems to be necessary. 4.2 Simulations with real world benchmark networks Next, SO-PhyL was applied to seven real world benchmark networks. Learning performance and quality of learnt network structures was compared to LAGD and TS. Data sets with 1000 instances were used to learn the corresponding structures of these benchmark networks. Three different parameter configurations of SO-PhyL were compared as presented in Table 12 . Note that SO-PhyL-1 and SO-PhyL-3 differ only in the parameter r, while SO-PhyL-2 differs from both especially in much smaller values of λ and w. Inputs I 0 again follow the rule of thumb as detailed above. Results are collected in Table 13 , where the maximal score achieved for each network type is marked in bold face. For small networks, all test methods deliver identical network structures (see network Cancer and Earthquake). For the slightly larger network Asia, SO-PhyL performs slightly worse but the difference in score is marginal only. But all three configurations of SO-PhyL learned one additional edge and resulted with one reversed edge less than LAGD and TS. For the Insurance network, SO-PhyL-1 clearly outperforms LAGD and TS with respect to the Bayesian score, the number of correctly learnt as well as additional edges, and the difference in score is quite respectable. This medium-sized benchmark network thus demonstrates that SO-PhyL is indeed able to outperform state of the art learning methods. The learnt structure for the Insurance network by SO-PhyL-1 is illustrated in Fig. 8 . For the remaining larger networks, the greedy hill climbing algorithm LAGD performed best and achieved the network structure with highest score. However, the score achieved by SO-PhyL is, for different configurations each time, still very close for the Alarm and the Hailfinder networks. The difference is largest for the Barley network, which is the most densely connected network encompassing the largest number of edges of all benchmark networks considered. A closer look to the simulations of all networks reveals that network structures learnt with SO-PhyL-1 and SO-PhyL-3 generally contain less edges compared to the other two learning algorithms and the original networks. As a benefit they naturally also learn less additional edges, not existing in the original networks, than LAGD and TS. On the contrary, SO-PhyL-2 learns network structures with more edges due to very small values of parameters λ and w and thus performs better for the three largest networks than the other two SO-PhyL configurations. With respect to computational load, the SO-PhyL algorithm again takes the longest execution time (see Fig. 9 for the Asia network), as was the case with the artificial benchmark networks discussed above. 4.3 Detailed analysis of the learning dynamics The main principle of SO-PhyL is to consider only a subset of all possible connections as active Bayesian Network connections in each MFS-Physarum Solver iteration. Hence, identifying this subset and tuning the related conductance parameters is the most crucial task when using SO-PhyL for learning structure from data. Consequently, conductance dynamics were analyzed in greater detail using the Asia network as an example. Fig. 10 illustrates the evolution of individual conductances during r total iterations using r = 3 for one learner, out of an ensemble of E = 10 learners, and applying parameters as defined for SO-PhyL-1. The dynamical evolution of the conductances roughly clusters in three groups. There are conductances which reach maximal, i. e. D ij = D limit , or near maximal D ij ≈ D limit conductances, some of them very fast indeed. Others increase steadily but reach only medium level conductance values, while the third group, which forms the group with the most numerous connections, stays with low conductances throughout all iterations performed. The latter group obviously had to miss positive score feedback during the whole evolution time. Few conductances are also observed to increase quickly but later decline to low values and stay there up to the end of the learning process. This behavior is caused by score feedback which corrects early maladjustment of certain conductances (see discussion below). The group with the highest conductances contributes the most to the final score of the network, hence represent the most important connections and their number is surprisingly small. The most problematic group is the one with medium-level conductances. The corresponding connections obviously do not receive sufficient positive score feedback to be pushed to the group with near maximal conductivities. It remains to be studied how the algorithm can be modified to achieve a clustering of connections and their related conductances into only two groups with either maximal or minimal conductance values. This would stabilize learning dynamics and probably render ensemble learning obsolete. An interesting behavior can be observed at iteration 11, where two connections with conductances at D limit start to reduce their conductance values towards the critical conductance D τ . These two connections XRay ↔ Tuberculosis are illustrated in a darker blue and XRay ↔ Lung Cancer is printed in skin color. On the other hand, the conductance of connection XRay ↔ Tuberculosis or Lung Cancer, drawn in light blue, rapidly increases conductance at iteration 11. This is an example of how SO-PhyL replaced two initially score increasing connections by another connection as a consequence of the fact that conditional probabilities of XRay on Lung Cancer and Tuberculosis are modeled by connection XRay ↔ Tuberculosis or Lung Cancer leading to a situation where the former two connections don’t result in a score increase any more, rather decrease the score. Hence, the conductances of these two connections were turned inactive and other connections became active in the Bayesian Network. 5 Conclusion In this study, a novel approach to solve the NP-hard problem of learning structure from data has been presented. The new method is called Physarum Learner, while the related algorithm is denoted SO-PhyL. It provides a new bioinspired optimization algorithm which builds upon the MFS-Physarum Solver algorithm proposed by Tero et al. (2008). The latter was constructed to explain the shortest path finding ability of P. polycephalum. SO-PhyL is a new development which builds upon the learning dynamics of the Physarum Solver. But instead of dealing with shortest path finding or network optimization problems, it considers learning a directed acyclic graph, more precisely a Bayesian Network structure, from a given set of data. Other than the correlation-based approach taken with C-PhyL (Schoen et al., 2012), the new SO-PhyL algorithm operates by optimizing a proper scoring function while searching the space of all possible network structures. Thus, SO-PhyL offers a bio-inspired variant of score-based search algorithms similar to hill climbing approaches like LAGD or Tabu Search. SO-Phyl thus searches the space of possible network structures in order to optimize a score measuring how well the distribution observed in the data set approximates the distribution of the underlying networks from which the samples were drawn. The algorithm starts with a random network structure and uses the MFS-Physarum Solver to select a subset of connections with concomitant adaptive conductances to be evaluated if their inclusion in the network as active connections either increases or decreases the global score of the current network. A positive or negative feedback is given to the conductance value of each connection based on the relative score change. Depending on score feedback and mediated by the learning dynamics of SO-PhyL, conductances change along iterations and thus also the set of active connections of the Bayesian Network. The algorithm executes a predefined number of iterations, and, ideally, connections are converging to either a large conductance or disappear leaving a subset of active connections which form the final Bayesian Network structure. Simulations with artificial and real benchmark networks, however, revealed that not all conductances are converging to their respective limiting values. Furthermore, SO-PhyL needs careful parameter selection, at least for some parameters, to retrieve from the data a network with maximal score. Especially the input I 0 to the network controls the number of connections learnt by SO-PhyL, and large networks require large inputs I 0 in order to perform comparably to other state-of-the-art structure learning methods. The learnt networks still exhibited a moderate dependence on the initialization of the conductances. Hence an ensemble of SO-PhyL learners needs to be invoked to render the learning process robust. Further studies are currently performed investigating the impact of SO-PhyL on conductance dynamics as well as the interplay between the SO-PhyL and the score feedback mechanism. The algorithm SO-PhyL has been applied to a set of artificial as well as real world benchmark networks and showed comparable or even superior learning performance to state-of-the-art learning methods like greedy hill climbing (LAGD), Tabu Search and Simulated Annealing. This is especially true for smaller networks with few nodes and edges. Preliminary results for larger and densely connected networks with many nodes are still suboptimal. This results from a prohibitive computational load which asked for some compromise concerning parameter tuning and the still insufficient robustness of the learning dynamics. In future, improving learning performance affords implementation of the algorithm on a GPU to allow for the use of larger ensembles and a larger number of iterations. Corresponding investigations are under way in our group. Nevertheless, SO-PhyL showed results comparable in quality to competing structure learning algorithms, and especially configuration SO-PhyL-1 tends to learn less edges than other methods which is one reason for its excellent performance for the Insurance network. This represents a promising feature to be explored in future studies. Also, future investigations will explore information preservation for learning the directions of the edges encompassing the DAG. Acknowledgments This work was supported by BioPersMed (COMET K-project 825329), which is funded by the Federal Ministry of Transport, Innovation and Technology (BMVIT) and the Federal Ministry of Economics and Labour/the Federal Ministry of Economy, Family and Youth (BMWA/BMWFJ) and the Styrian Business Promotion Agency (SFG). Appendix A Algorithm 1: The procedure of initializing a Physarum-Maze from data to be used in SO-PhyL Algorithm 1. SO-PhyL: Building Physarum-Maze from dataset require nodes[], connections[] require dataset nodes ← Empty connections ← Empty for all variable X i in dataset do Add X i to nodes[] for all variable X j ≠ X i in dataset do L ij = 1.0 D ij = Rand ( D min , D max ) M ij = new connection ( L ij , D ij ) E ij = set initial direction of M ij randomly Add E ij to connections[] end for end for Algorithm 2: Summary the complete SO-PhyL procedure. Algorithm 2. SO-PhyL: Applying MFS-Physarum Solver on Physarum-Maze require nodes[], connections[] require r B best ← null for Size of Ensemble do for i < r do nodePairs[] = list of all possible pairs of nodes while nodePairs[] not empty do Select random pair ( X i , X j ) out of nodePairs[] Set X i and X j as food sources Remove pair ( X i , X j ) from nodePairs[] Perform one Physarum Solver iteration Reset food sources if D ij < D τ then D ij = D τ + 0.01 end if B = evaluateConductivityByScore () if score B > score B best then B best = B end if D τ = D τ 0 + i r total ( D τ end - D τ 0 ) end while end for end for return B best Algorithm 3: Scoring procedure of conductances. Algorithm 3. SO-PhyL: Implementation of function evaluateConductivityByScore () require connections[] require Bayesian Network B defined by graph G = ( X , E ) whitelist[] ← Empty for allconnections M ij do if D ij > D τ then Add M ij to whitelist else Remove M ij from E end if end for while whitelist not empty do bestScoreImprovement =0.0; bestConnection; for all connections M ij in whitelist do Remove M ij from E if addArkMakesSense( E ij ) then score Diff ( E ij ) = score X j ( X i ∈ Pa ( X j ) ) - score X j ( X i ∉ Pa ( X j ) ) end if if addArkMakesSense( E ji ) then score Diff ( E ji ) = score X i ( X j ∈ Pa ( X i ) ) - score X i ( X j ∉ Pa ( X i ) ) end if scoreChange = Max( score Diff ( E ij ) , score Diff ( E ji ) ) if scoreChange > bestScoreImprovement then bestScoreImprovement =scoreChange bestConnection = E ij or E ji based on better score end if end for if not bestConnection then for connections M ij in whitelist do giveFeedback( M ij ) end for if score B > score B best then B best = B end if return B best end if Add bestConnection to E Remove bestConnection from whitelist giveFeedback(bestConnection) end while if score B > score B best then B best = B end if return B best Algorithm 4: Procedure of providing score feedback to conductances. Algorithm 4. SO-PhyL: Implementation of function giveFeedback ( E ij ) β = calculateBeta () D ij = D ij + k ( 1 - β ) if D ij > D limit then D ij = D limit end if if D ij < 0 then D ij = 0 end if References Abramovici et al., 2008 Abramovici, M., Neubach, M., Fathi, M., & Holland, A. (2008). Competing fusion for Bayesian applications. In 12th Information processing and management of uncertainty in knowledge-based systems (pp. 378–385). Adamatzky, 2007 A. Adamatzky Physarum machine: Implementation of a Kolmogorov–Uspensky machine on a biological substrate Parallel Processing Letters 17 2007 455 467 Adamatzky, 2008 A. Adamatzky Developing proximity graphs by Physarum polycephalum: Does the plasmodium follow Toussaint hierarchy? Parallel Processing Letters 19 2008 105127 Adamatzky, 2010 A. Adamatzky Physarum machines: Computers from slime mold 2010 World Scientific Publishing Adamatzky et al., 2005 A. Adamatzky B. de Lacy Costello T. Asai Reaction–diffusion computers 2005 Elsevier Bechetti et al., 2013 L. Bechetti V. Bonifaci M. Dirnberger A. Karrenbauer K. Mehlhorn Physarum can compute shortest paths: Convergence proofs and complexity bounds Automata, languages and programming LNCS 7966 2013 Springer 472 483 Becker, 2011 M. Becker Design of fault tolerant networks with agent-based simulation of physarum polycephalum Proceedings of the conference on evolutionary computation (CEC) 2011 IEEE 285 291 10.1109/CEC.2011.5949630 Beinlich et al., 1989 I.A. Beinlich H.J. Suermondt R.M. Chavez G.F. Cooper The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks Proceedings of the 2nd European conference on artificial intelligence in medicine 1989 Springer-Verlag 247 256 Bonifaci, 2013 V. Bonifaci Physarum can compute shortest paths. A short proof Information Processing Letters 113 2013 4 7 Bonifaci et al., 2012 V. Bonifaci K. Mehlhorn G. Varma Physarum can compute shortest paths Journal of Theoretical Biology 309 2012 121 133 Bouchaala et al., 2010 L. Bouchaala A. Masmoudi F. Gargouri A. Rebai Improving algorithms for structure learning in Bayesian Networks using a new implicit score Expert Systems with Applications 37 2010 54705475 Brummitt et al., 2010 C. Brummitt I. Laureyns T. Lin D. Martin D. Parry D. Timmers A mathematical study of Physarum polycephalum The Tero Model 2010 1 24 Cooper and Herskovits, 1992 G. Cooper E. Herskovits A Bayesian method for the induction of probabilistic networks from data Machine Learning 9 1992 309 347 Fricker et al., 2009 M. Fricker L. Boddy T. Nakagaki D.P. Bebber Adaptive networks: Theory, models and applications 2009 Springer Berlin Georgiadis, 2013 Georgiadis, G. (2013). Modeling and constructing unstructured overlay networks: Algorithms, techniques and the smart grid case. Glover, 1990 F. Glover Tabu search: A tutorial Interfaces 20 1990 74 94 Glover and McMillan, 1986 F. Glover C. McMillan The general employee scheduling problem: An integration of MS and AI Computers and Operations Research 1986 Gunjia et al., 2011 Y.-P. Gunjia T. Shirakawab T. Niizatoa M. Yamachiyoa I. Tania An adaptive and robust biological network based on the vacant-particle transportation model Journal of Theoretical Biology 272 2011 187 200 Heckerman et al., 1995 D. Heckerman D. Geiger D. Chickering Learning Bayesian networks: The combination of knowledge and statistical data Machine Learning 20 1995 197 243 Holland et al., 2008 Holland, A., Fathi, M., Abramovici, M., & Neubach, M. (2008). Competing fusion for bayesian applications. In Proceedings of the 12th international conference on information processing and management of uncertainty in knowledge-based systems (IPMU 2008), Malaga, Spain (pp. 378–385). Ito et al., 2011 Ito, K., Johansson, A., Nakagaki, T. & Tero, A. (2011). Convergence properties for the Physarum solver. arXiv:1101.5249v1. Johannson and Zou, 2012 Johannson, A., & Zou, J. (2012). A slime mold solver for linear programming problems. In S.B. Cooper, A. Dawar, & B. Löwe (Eds.), CiE 2012, LNCS 7318 (pp. 344–354). Springer. Kirkpatrick et al., 1983 S. Kirkpatrick C.D. Gelatt M.P. Vecchi Optimization by simulated annealing Science 220 1983 671 680 Kjaerulff and Madsen, 2005 Kjaerulff, U. B., & Madsen, A. L. (2005). Probabilistic networks: An introduction to bayesian networks and influence diagrams. Koivisto and Sood, 2004 M. Koivisto K. Sood Exact Bayesian structure discovery in Bayesian networks The Journal of Machine Learning Research 5 2004 549 573 Koller and Friedman, 2009 D. Koller N. Friedman Probabilistic graphical models: Principles and techniques 2009 The MIT Press Korb and Nicholson, 2010 K. Korb A. Nicholson Bayesian artificial intelligence 2nd ed. 2010 Chapman and Hall Lam and Bacchus, 1994 W. Lam F. Bacchus Learning Bayesian belief networks: An approach based on the MDL principle Computational Intelligence 10 1994 269 293 Lauritzen and Spiegelhalter, 1988 S.L. Lauritzen D.J. Spiegelhalter Local computation with probabilities on graphical structures and their application to expert systems Journal of the Royal Statistical Society: Series B (Statistical Methodology) 50 1988 157 224 Lin and Kernighan, 1973 S. Lin B.W. Kernighan An effective heuristic for the traveling salesman problem Operational Research 21 1973 498 516 Li et al., 2011 K. Li C.E. Torres K. Thomas L.F. Rossi C.-C. Shen Slime mold inspired routing protocols for wireless sensor networks Swarm Intelligence 5 2011 183 223 Masi and Vasile, 2013 L. Masi M. Vasile EVOLVE: A bridge between probability, set oriented numerics and evolutionary computation III 2013 Springer Berlin, Heidelberg Miyaji and Ohnishi, 2007 T. Miyaji I. Ohnishi Mathematical analysis to an adaptive network of the Plasmodium system Hokkaido Mathematics Journal 36 2007 245 465 Miyaji and Ohnishi, 2008 T. Miyaji I. Ohnishi Physarum can solve the shortest path problem on riemannian surface mathematically rigorously International Journal of Pure and Applied Mathematics 47 2008 353 369 Miyaji et al., 2008 T. Miyaji I. Ohnishi A. Tero T. Nakagaki Failure to the shortest path decision of an adaptive transport network with double edges in Plasmodium system International Journal of Dynamical Systems and Differential Equations 1 2008 210 219 Nakagaki, 2001 T. Nakagaki Smart behavior of true slime mold in a labyrinth Research in Microbiology 152 2001 767 770 Nakagaki et al., 2007 T. Nakagaki M. Iima T. Ueda Y. Nishiura T. Saigusa A. Tero Minimum-risk path finding by an adaptive amoebal network Physical Review Letters 99 2007 068104 Nakagaki et al., 2009 T. Nakagaki A. Tero R. Kobayashi I. Ohnishi T. Miyaji Computational ability of cells based on cell dynamics and adaptability New Generation Computing 27 2009 57 81 Nakagaki et al., 2004 T. Nakagaki H. Yamada M. Hara Smart network solutions in an amoeboid organism Biophysical Chemistry 107 2004 1 5 Nakagaki et al., 2000 T. Nakagaki H. Yamada A. Toth Intelligence: Maze-solving by an amoeboid organism Nature 407 2000 470 Nakagaki et al., 2001 T. Nakagaki H. Yamada A. Tóth Path finding by tube morphogenesis in an amoeboid organism Biophysical Chemistry 92 2001 47 52 Niizatoa et al., 2010 T. Niizatoa T. Shirakawab Y.P. Gunjia A model of network formation by Physarum plasmodium: Interplay between cell mobility and morphogenesis Biosystems 100 2010 108 112 Parviainen and Koivisto, 2009 Parviainen, P., & Koivisto, M. (2009). Exact structure discovery in Bayesian networks with less space. In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence, UAI ’09 (pp. 436–443). Pearl, 1988 J. Pearl Probabilistic reasoning in intelligent systems: Networks of plausible inference 2nd ed. 1988 Morgan Kaufmann San Francisco, California Saigusa et al., 2008 T. Saigusa A. Tero T. Nakagaki Y. Kuramoto Amoebae anticipate periodic events Physics Review Letters 100 2008 018101 Schoen et al., 2012 Schoen, T., Stetter, M., & Lang, E. (2012). Structure learning for bayesian networks using the Physarum solver. In Proceedings of the 11th international conference on machine learning and applications, ICMLA 2012, IEEEXPlore (pp. 488–493). Schoen et al., 2013 Schoen, T., Stetter, M., & Lang, E. (2013). A new Physarum learner for network structure learning from biomedical data. In Proceedings of the 6th international conference on bio-inspired systems and signal processing. Schumann and Adamatzky, 2009 A. Schumann A. Adamatzky Physarum spatial logic New Mathematics and Natural Computation (NMNC) 7 2009 483 498 Shirakawa et al., 2009 T. Shirakawa A. Adamatzky Y.-P. Gunji Y. Miyake On simultaneous construction of Voronoi diagram and Delaunay triangulation by Physarum polycephalum International Journal of Bifurcation and Chaos 19 2009 31093117 Sohier et al., 2012 D. Sohier G. Georgiadis S. Clavière M. Papatriantafilou A. Bui Physarum-inspired self-biased walkers for distributed clustering R. Baldoni P. Flocchini R. Binoy OPODIS 2012 LNCS Vol. 7702 2012 Springer-Verlag Berlin, Heidelberg 315 329 Tero et al., 2006 A. Tero R. Kobayashi T. Nakagaki Physarum solver: A biologically inspired method of road-network navigation Physica A 363 2006 115 119 Tero et al., 2007 A. Tero R. Kobayashi T. Nakagaki A mathematical model for adaptive transport network in path finding by true slime mold Journal of Theoretical Biology 244 2007 553 564 Tero et al., 2010 A. Tero S. Takagi T. Saigusa K. Ito D.P. Bebber M.D. Fricker Rules for biologically inspired adaptive network design Science 327 2010 439 442 Tero et al., 2008 A. Tero K. Yumiki R. Kobayashi T. Saigusa T. Nakagaki Flow-network adaptation in Physarum amoebae Theory in Biosciences 127 2008 89 94 Tsuda et al., 2004 S. Tsuda M. Aono Y.-P. Gunji Robust and emergent Physarum logical computing Biosystems 73 2004 45 55 Tsuda et al., 2009 S. Tsuda S. Artmann K.-P. Zauner Artificial life 2009 Springer-Verlag London pp. 213–232 Tsuda et al., 2007 S. Tsuda K.-P. Zauner Y.-P. Gunji Robot control with biological cell Biosystems 87 2007 215 223 Zhang and Liu et al., 2013 Zhang, X., Liu, Q., Hu, Y., Chan, F. T. S., Mahadevan, S., Zhang, Z., & Deng, Y. (2013). An adaptive amoeba algorithm for shortest path tree computation in dynamic graphs. arXiv:1311.0460. Zhang et al., 2013 X. Zhang Y. Zhang Y. Hu Y. Deng S. Mahadevan An adaptive amoeba algorithm for constrained shortest paths Expert Systems with Applications 40 2013 7607 7616 Zhang et al., 2012 Y. Zhang Z. Zhang D. Wei Y. Deng Centrality measure in weighted networks based on an amoeboid algorithm Journal of Information & Computational Science 9 2012 369 376 "
    },
    {
        "doc_title": "A new Bayesian approach to nonnegative matrix factorization: Uniqueness and model order selection",
        "doc_scopus_id": "84899975861",
        "doc_doi": "10.1016/j.neucom.2014.02.021",
        "doc_eid": "2-s2.0-84899975861",
        "doc_date": "2014-08-22",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Cognitive Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2805"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Bayes NMF",
            "Bayesian approaches",
            "Generalized lee-seung update rules",
            "Model-order selection",
            "Nonnegative matrix factorization",
            "Number of components",
            "Optimality criteria",
            "Variational bayes"
        ],
        "doc_abstract": "NMF is a blind source separation technique decomposing multivariate non-negative data sets into meaningful non-negative basis components and non-negative weights. There are still open problems to be solved: uniqueness and model order selection as well as developing efficient NMF algorithms for large scale problems. Addressing uniqueness issues, we propose a Bayesian optimality criterion (BOC) for NMF solutions which can be derived in the absence of prior knowledge. Furthermore, we present a new Variational Bayes NMF algorithm VBNMF which is a straight forward generalization of the canonical Lee-Seung method for the Euclidean NMF problem and demonstrate its ability to automatically detect the actual number of components in non-negative data. © 2014 Elsevier B.V.",
        "available": true,
        "clean_text": "serial JL 271597 291210 291735 291866 31 Neurocomputing NEUROCOMPUTING 2014-04-08 2014-04-08 2014-05-07T08:44:13 S0925-2312(14)00411-1 S0925231214004111 10.1016/j.neucom.2014.02.021 S300 S300.1 FULL-TEXT 2020-03-11T22:07:23.416921Z 0 0 20140822 2014 2014-04-08T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings suppl volfirst volissue figure body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast misctext orcid primabst ref vitae alllist content subj ssids 0925-2312 09252312 true 138 138 C Volume 138 17 142 156 142 156 20140822 22 August 2014 2014-08-22 2014 Regular Papers article fla Copyright © 2014 Elsevier B.V. All rights reserved. ANEWBAYESIANAPPROACHNONNEGATIVEMATRIXFACTORIZATIONUNIQUENESSMODELORDERSELECTION SCHACHTNER R 1 Introduction 2 Bayesian approaches to NMF 3 Bayesian optimality condition for NMF with Gaussian likelihood and fixed model order K 3.1 The most probable matrix H 3.2 Laplace׳s approximation for NMF 3.3 The Bayesian optimality condition 4 Variational Bayes NMF 4.1 Variational Bayes for NMF with Gaussian likelihood 4.2 The rectified Gaussian distribution 4.3 The VBNMF update rules 4.4 Relation to the Lee–Seung method 5 Simulations on toy data sets 5.1 Valid priors for the weights 5.2 Invalid priors for the weights 5.3 Discussion of the simulation results 6 Binary test data example 6.1 A short primer on binNMF 6.2 VBNMF on binary data 7 Conclusion Acknowledgments Appendix A A.1 Laplace׳s method A.2 The Jensen trick A.3 Rectified Gaussian expectations References AKAIKE 1974 716 723 H BEAL 2004 1 44 M 2001 COMPUTATIONALINFORMATIONRETRIEVAL BISHOP 1996 C NEURALNETWORKSFORPATTERNRECOGNITION CEMGIL 2009 A CHAGOYEN 2006 41 M CICHOCKI 2007 431 446 A CICHOCKI 2009 A NONNEGATIVEMATRIXTENSORFACTORIZATIONS DEVARAJAN 2008 e1000029 K FOSTER 1994 1947 1975 D GEORGE 2000 731 747 E GRIFFITHS 2011 1185 1224 T HARVA 2007 509 527 M HOYER 2004 P JENSEN 1906 175 193 J KABAN 2008 2291 2308 A LAURBERG 2008 H LEE 1999 788 791 D LIN 2007 1589 1596 C MACKAY 1992 415 447 D MACKAY 2003 D INFORMATIONTHEORYINFERENCELEARNINGALGORITHMS PAATERO 1997 23 35 P PAATERO 1994 111 126 P PLUMBLEY 2003 534 543 M PLUMBLEY 2004 66 76 M RICHARDSON 1972 55 59 H RISSANEN 1983 416 431 J SAJDA 2004 1453 1465 P SAUL 2002 897 904 L SCHACHTNER 2008 1688 1697 R SCHACHTNER 2010 1439 1448 R SCHACHTNER 2011 528 534 R SCHMIDT 2008 M SCHWARZ 1978 461 464 G STADLTHANNER 2008 2356 2376 K VAVASIS 2009 1364 1377 S ZHONG 2009 663 670 M SCHACHTNERX2014X142 SCHACHTNERX2014X142X156 SCHACHTNERX2014X142XR SCHACHTNERX2014X142X156XR 2020-03-06T22:07:21.702Z S0925231214004111 R.S. has been supported by a scholarship provided by Infineon AG , Regensburg, which is gratefully acknowledged. item S0925-2312(14)00411-1 S0925231214004111 10.1016/j.neucom.2014.02.021 271597 2014-05-08T13:12:25.501617-04:00 2014-08-22 true 3119933 MAIN 15 59039 849 656 IMAGE-WEB-PDF 1 si0282 5722 226 388 si0281 4767 200 365 si0280 1578 43 351 si0279 1490 43 324 si0278 894 32 175 si0277 877 32 173 si0276 1409 19 391 si0275 197 13 20 si0274 1514 33 304 si0273 1787 45 290 si0272 1736 45 290 si0271 626 18 108 si0270 153 13 15 si0269 1153 40 163 si0268 1232 32 240 si0267 146 12 10 si0266 833 36 157 si0265 1128 28 253 si0264 166 14 14 si0263 461 29 62 si0262 247 16 26 si0261 146 12 10 si0260 132 10 12 si0259 163 10 16 si0258 300 10 51 si0257 383 14 65 si0256 511 15 86 si0255 468 15 80 si0254 216 13 26 si0253 183 13 21 si0252 512 13 122 si0251 550 14 133 si0250 215 14 28 si0249 249 14 32 si0248 204 13 27 si0247 231 13 31 si0246 713 17 156 si0245 708 16 162 si0244 139 13 11 si0243 163 10 16 si0242 132 10 12 si0241 163 10 16 si0240 132 10 12 si0239 163 10 16 si0238 132 10 12 si0237 132 10 12 si0236 1261 34 255 si0235 876 18 223 si0234 618 16 139 si0233 2725 83 383 si0232 765 15 172 si0231 266 15 48 si0230 286 15 48 si0229 455 13 107 si0228 376 11 88 si0227 168 14 15 si0226 397 14 93 si0225 163 10 16 si0224 207 11 34 si0223 132 10 12 si0222 168 14 15 si0221 469 15 80 si0220 168 14 15 si0219 207 11 34 si0218 168 14 15 si0217 736 15 178 si0216 163 10 16 si0215 180 14 18 si0214 390 15 78 si0213 163 10 16 si0212 163 10 16 si0211 163 10 16 si0210 163 10 16 si0209 278 12 66 si0208 273 14 52 si0207 284 15 52 si0206 163 10 16 si0205 216 13 26 si0204 1016 55 150 si0203 140 10 12 si0202 959 40 138 si0201 1089 37 148 si0200 1020 48 123 si0199 789 44 96 si0198 198 14 23 si0197 271 13 49 si0196 188 13 23 si0195 269 15 36 si0194 741 38 90 si0193 203 18 19 si0192 218 16 23 si0191 359 16 55 si0190 380 15 62 si0189 142 8 16 si0188 121 8 9 si0187 168 14 15 si0186 190 11 26 si0185 205 13 27 si0184 188 13 23 si0183 198 14 23 si0182 106 6 8 si0181 190 11 26 si0180 205 13 27 si0179 188 13 23 si0178 198 14 23 si0177 106 6 8 si0176 198 11 28 si0175 214 13 29 si0174 298 13 55 si0173 106 6 8 si0172 801 34 133 si0171 818 34 136 si0170 298 13 55 si0169 321 13 62 si0168 1167 43 233 si0167 1533 42 207 si0166 1156 43 230 si0165 1526 42 208 si0164 769 20 159 si0163 805 18 174 si0162 739 18 164 si0161 183 13 21 si0160 216 13 26 si0159 809 18 179 si0158 163 10 16 si0157 551 19 134 si0156 952 34 178 si0155 905 32 170 si0154 877 32 173 si0153 1896 62 322 si0152 516 15 86 si0151 460 15 80 si0150 2444 84 269 si0149 1870 52 205 si0148 168 14 15 si0147 444 13 80 si0146 303 13 43 si0145 269 15 36 si0144 1606 68 177 si0143 1121 20 223 si0142 298 15 39 si0141 1494 33 274 si0140 298 15 39 si0139 298 15 39 si0138 303 13 43 si0137 168 14 15 si0136 284 14 42 si0135 168 14 15 si0134 317 15 59 si0133 4981 132 408 si0132 626 18 108 si0131 626 18 108 si0130 4547 117 387 si0129 354 14 55 si0128 180 14 18 si0127 976 24 194 si0126 1029 24 198 si0125 381 13 61 si0124 132 10 12 si0123 163 10 16 si0122 628 13 125 si0121 175 13 28 si0120 284 14 42 si0119 180 14 18 si0118 696 29 127 si0117 343 13 50 si0116 3043 68 334 si0115 1198 16 263 si0114 132 10 12 si0113 431 15 99 si0112 358 14 56 si0111 140 10 12 si0110 321 17 53 si0109 272 14 42 si0108 132 10 12 si0107 163 10 16 si0106 396 13 64 si0105 2670 83 349 si0104 132 10 12 si0103 163 10 16 si0102 163 10 16 si0101 1284 32 272 si0100 423 15 86 si0099 137 10 11 si0098 191 13 28 si0097 253 14 53 si0096 268 16 41 si0095 146 11 15 si0094 191 13 28 si0093 146 11 15 si0092 137 10 11 si0091 191 13 28 si0090 137 10 11 si0089 1748 35 338 si0088 163 10 16 si0087 460 15 95 si0086 291 10 55 si0085 1163 16 266 si0084 1423 15 309 si0083 186 12 21 si0082 186 12 21 si0081 987 29 193 si0080 296 13 41 si0079 132 10 12 si0078 781 31 126 si0077 132 10 12 si0076 163 10 16 si0075 140 10 12 si0074 132 10 12 si0073 140 10 12 si0072 132 10 12 si0071 140 10 12 si0070 132 10 12 si0069 372 14 96 si0068 1091 16 232 si0067 441 13 75 si0066 222 12 31 si0065 1101 13 249 si0064 282 15 38 si0063 243 15 43 si0062 1714 40 300 si0061 129 10 10 si0060 158 13 16 si0059 172 13 19 si0058 1593 33 325 si0057 481 13 92 si0056 897 14 232 si0055 132 10 12 si0054 291 13 48 si0053 163 10 16 si0052 276 13 45 si0051 140 10 12 si0050 298 13 48 si0049 129 10 10 si0048 222 11 35 si0047 250 11 39 si0046 1045 33 190 si0045 132 10 12 si0044 163 10 16 si0043 140 10 12 si0042 249 14 32 si0041 231 13 31 si0040 215 14 28 si0039 480 16 111 si0038 249 14 32 si0037 215 14 28 si0036 142 11 11 si0035 713 17 156 si0034 359 13 77 si0033 163 10 16 si0032 184 13 22 si0031 182 13 22 si0030 180 13 22 si0029 174 13 22 si0028 132 10 12 si0027 168 14 15 si0026 216 13 26 si0025 183 13 21 si0024 207 11 34 si0023 163 10 16 si0022 132 10 12 si0021 239 12 47 si0020 229 12 47 si0019 178 12 37 si0018 168 14 15 si0017 239 12 47 si0016 229 12 47 si0015 178 12 37 si0014 168 14 15 si0013 765 15 192 si0012 390 15 78 si0011 163 10 16 si0010 282 10 54 si0009 285 15 49 si0008 262 13 49 si0007 189 12 23 si0006 190 12 23 si0005 181 12 23 si0004 132 10 12 si0003 140 10 12 si0002 283 11 49 si0001 282 10 51 gr9 57761 307 809 gr8 76717 375 809 gr7 29471 251 662 gr6 39268 352 659 gr5 136957 994 646 gr4 47070 499 687 gr3 50321 498 670 gr2 41047 260 650 gr10 137620 939 793 gr1 16181 244 270 fx5 11863 151 113 fx4 11596 151 113 fx3 13069 151 113 fx2 12293 151 113 fx1 10905 151 113 gr9 4652 83 219 gr8 5186 102 219 gr7 3229 83 219 gr6 4277 117 219 gr5 5304 163 106 gr4 4082 159 219 gr3 4203 163 219 gr2 3881 88 219 gr10 3798 163 138 gr1 3337 164 181 fx5 10372 164 123 fx4 10363 164 123 fx3 10603 164 123 fx2 11079 164 123 fx1 10137 164 123 NEUCOM 14045 S0925-2312(14)00411-1 10.1016/j.neucom.2014.02.021 Elsevier B.V. Fig. 1 Geometrical interpretation of the NMF-problem X = WH . The M − dim data X lie in the positive span of K basis vectors in the rows of H . For illustrative purposes, M=3 and K=2. The coordinate axes are given by three unit vectors e X , 1 , e X , 2 , e X , 3 ; The data lies inside a 2 dimensional region between the basis vectors H 1 ⁎ , H 2 ⁎ , but also between the alternative basis vectors H 1 ⁎ ′ , H 2 ⁎ ′ . Fig. 2 Histograms of the original columns of the 1000 × 3 matrix W . Left: rectified Gaussian P ( W ik ) = N + . Right: rectified Gaussian with additional peak at 0 P ( W ik ) = τ N + + ( 1 − τ ) δ ( W ik − 0 ) . Fig. 3 The log evidence bound B q w.r.t. the number of components K for various reconstruction errors σ r if the model assumptions hold true. top, left: no noise; top, right: σ r = 1 ; bottom, left: σ r = 10 ; bottom, right: σ r = 20 . The correct number of sources K=3 is recognized in all cases. Fig. 4 The log evidence bound B q w.r.t. the number of components K for various reconstruction errors σ r if the prior distribution assumption is violated. top, left: no noise; top, right: σ r = 1 ; bottom, left: σ r = 10 ; bottom, right: σ r = 20 . Only in the noise-free case the true number of sources K=3 is recognized. Fig. 5 Simulation results for various model orders obtained with the toy data set described above. The true order of the model, i.e. the number of hidden components, is K=3. The left column shows the hidden features H extracted with a VBNMF model of order K=2 to K=5. The right column exhibits the corresponding distributions of the weights W . For decompositions utilizing K > 3 , a typical outcome reveals one or more components H k ⁎ with corresponding weights W ⁎ k are driven to constant values by the VBNMF algorithm thus maximizing the lower bound B q via the penalty terms, containing the log-priors. Fig. 6 Left: original H first row: H 1 ⁎ H 2 ⁎ , second row: H 3 ⁎ , H 4 ⁎ . Right: original weights W from top to bottom: W ⁎ 1 , … , W ⁎ 4 . The last row contains weights for a constant pattern modeling noise. Fig. 7 Left: histogram of the approximation error in the binary toy data set E ij = ln ( 1 − 0.6 X ij ) + [ WH ] ij . Obviously the noise is not distributed according to a zero mean Gaussian as stated in the likelihood assumption of the VBNMF algorithm. Right: the log evidence bound B (y-axis) w.r.t. the number of components K (x-axis) in the binary toy data example has a maximum at K=5. Fig. 8 Expected basic patterns 〈 H 〉 Q and weights 〈 W 〉 Q using VBNMF and K=5. The four original patterns (compare Fig. 6 and their weights) are well approximated. The weights of an additional noise pattern differ from the original shape. One possible reason is the additional noise induced by the binarization of the data. Fig. 9 Results of a VBNMF application to a toy data example with K=10. Left: basic patterns H ALS = ( H 1 ⁎ ⋯ H 10 ⁎ ) . Right: corresponding expected basic patterns 〈 H 〉 Q . Fig. 10 Results of a VBNMF application to a toy data example with K=10. Left: weight distributions W ALS as estimated by the ALS algorithm. Right: related expected weights 〈 W 〉 Q as finally obtained by the VBNMF algorithm. All weights belonging to irrelevant components show a flat distribution. A new Bayesian approach to nonnegative matrix factorization: Uniqueness and model order selection R. Schachtner a d G. Po¨ppel a A.M. A.M. Tomé b C.G. C.G. Puntonet c E.W. E.W. Lang d ⁎ a Infineon Technologies AG, 93049 Regensburg, Germany Infineon Technologies AG Regensburg 93049 Germany b IEETA, DETI, Universidade de Aveiro, P-3810-193 Aveiro, Portugal IEETA DETI Universidade de Aveiro Aveiro P-3810-193 Portugal c DATC/ESTII, Universidad de Granada, E-18071 Granada, Spain DATC/ESTII Universidad de Granada Granada E-18071 Spain d CIML Group, Biophysics Department, University of Regensburg, D-93040 Regensburg, Germany CIML Group Biophysics Department University of Regensburg Regensburg D-93040 Germany ⁎ Corresponding author. Communicated by A. Prieto NMF is a blind source separation technique decomposing multivariate non-negative data sets into meaningful non-negative basis components and non-negative weights. There are still open problems to be solved: uniqueness and model order selection as well as developing efficient NMF algorithms for large scale problems. Addressing uniqueness issues, we propose a Bayesian optimality criterion (BOC) for NMF solutions which can be derived in the absence of prior knowledge. Furthermore, we present a new Variational Bayes NMF algorithm VBNMF which is a straight forward generalization of the canonical Lee–Seung method for the Euclidean NMF problem and demonstrate its ability to automatically detect the actual number of components in non-negative data. Keywords Bayes NMF Variational Bayes Bayesian optimality criterion Generalized Lee–Seung update rules 1 Introduction Decomposing any given data set into nonnegative factors has received wide-spread interest within the machine learning community in recent years [16]. The idea first appeared as positive matrix factorization in [50,49] and became popular as non-negative matrix factorization (NMF) through the seminal paper by Lee and Seung [40]. Similar objectives were pursued also by nonnegative independent component analysis [51,52] or rectified factor analysis [29]. Originally, NMF was introduced as an unsupervised, parts-based learning paradigm involving the approximative decomposition of a nonnegative matrix X into a product of two nonnegative matrices, W and H , via a multiplicative update algorithm. All these data analysis tools account for the fact that many physical measurements yield results with exclusively non-negative quantities which can be approximated by a non-subtractive superposition of exclusively non-negative underlying features which explain the systematic structure of the data set. Also for physical reasons the superimposed components cannot partially compensate each other. Hence decomposing any measurements into a parts-based representation seems more natural than other constraints often invoked in exploratory matrix factorization procedures which would allow for partial compensation of the components upon their superposition. NMF has seen numerous applications in recent years [16,8] where it has been primarily applied in an unsupervised setting in image and natural language processing [42,58,13] and sparse coding [33,70]. Further applications include text mining [7] and music transcription [68]. More recently, it has been successfully utilized also in a variety of applications in computational biology [17], especially in molecular pattern discovery [56,57,59]. Basically, NMF can be formulated as a minimization problem. A suitable cost function such as the quadratic error function D E (1) D E ( X , WH ) = 1 2 ∑ i ∑ j ( X ij − [ WH ] ij ) 2 comes to be minimized subject to the constraint that either factor matrix has only non-negative entries, i.e. W ≥ 0 , H ≥ 0 . The cost function D E quantifies the reconstruction error E between an ( N × M ) -dimensional non-negative data matrix X and the product of an ( N × K ) -dimensional weight matrix W and a ( K × M ) -dimensional matrix H of hidden features or sources according to the data model: (2) X = WH + E subject to W ≥ 0 , H ≥ 0 . Vavasis [74] recently proofed that this problem is NP-hard. Without the nonnegativity constraint, singular value decomposition (SVD) provides an optimal solution to the factorization problem. Arora et al. recently discussed conditions when the NMF problem can be solved in polynomial time [4]. As posed above, NMF can be considered a constrained, unsupervised feature extraction technique. However, usually K is chosen such that K ( N + M ) ⪯¡ NM which brings clustering aspects into play as well but also raises the question of proper model order selection (MOS). The cost function (1) is just the most popular choice for NMF as it is based on the Frobenius norm of the reconstruction error of the data matrix factorization as a proper distance measure. Other popular cost functions are based on information theoretic concepts like the generalized Kullback–Leibler divergence, the Itakura–Saito divergence or similar divergences [15,18] (see [16] for a recent review). Positivity constraints are, however, rarely sufficient to extract the underlying features uniquely. Hence, further constraints based on sparseness considerations [32,70,27] or minimum volume requirements [63,3] have been considered in addition. Such regularizing constraints have been added to enforce certain characteristics of the solutions, or to impose prior knowledge about the application considered. Technically, additional constraints can be realized via additional penalty terms in the NMF cost function, for example (3) D constr ( X , WH ) = 1 2 ∑ i ∑ j ( X ij − [ WH ] ij ) 2 + λ W f ( W ) + λ H g ( H ) where the scalars λ W , λ H determine the balance between reconstruction accuracy and the desired properties of the factor matrices which are expressed by suitable functions f and g. Also various optimization techniques and sampling procedures such as Expectation–Maximization (EM), Markov Chain Monte Carlo (MCMC) or variational Bayes (VB) [19], based on the Indian Buffet Process factor analysis model [37,28] or nonparametric Bayesian approaches [30] have been presented since. To further improve the performance of NMF algorithms and to reduce the risk of getting stuck in local minima, a multi-layer technique has been advocated recently [15,14]. This technique has been proposed earlier already in connection with hybridizing sparse NMF with a genetic algorithm to optimize the related discontinuous cost function [71]. Proofs of convergence of NMF algorithms are scarce (see the discussions in [69,8]) though for the popular class of multiplicative update algorithms convergence to a stationary point could be proven [43]. Also uniqueness of NMF-solutions is still an open issue despite some recent attempts to deal with the subject [20,33,73]. Necessary and sufficient conditions have recently been formulated for a given NMF solution to be unique [39]. In [63] a geometric approach has been taken considering the determinant of the span of basis vectors and optimizing the decomposition for a minimal determinant. Unique solutions, as the term will be used in the following, may still be subject to scaling and permutation indeterminacies which are ubiquitous in many blind source separation contexts. There are two popular routes to enforce uniqueness of the solutions. While in [32], arguments from sparse coding are invoked, the development of positive matrix factorization as surveyed in [31] rather proposes application-driven solutions requiring background knowledge. Both approaches are limited to special applications where specific information about the data is available or specific assumptions concerning the composition of data are necessary. The issue of model order selection (MOS) within NMF decompositions relates to the application of information theoretic criteria like Akaike׳s Information Criterion (AIC) [1,2], the Minimum Description Length (MDL) criterion [54] which is equivalent to the Bayes Information Criterion (BIC) [67], or the Risk Inflation Criterion (RIC) [22–24]. Recently, however, automatic relevance detection (ARD) schemes have been discussed in relation with Bayesian approaches to NMF [10,21,72] and showed promising results. The rest of this paper is organized as follows: in Section 2 we present a short overview of Bayesian approaches addressing issues of uniqueness and model order selection. Next we present in Section 3 a Bayesian Optimality Condition (BOC) for NMF with an Euclidean distance measure, and show that it leads to a minimum volume constraint for the optimization of an L 2-norm reconstruction error proposed earlier [63]. In Section 4 we finally present a new variational Bayesian NMF (VBNMF) approach to tackle the problem of model order selection and show in Section 5 with toy data sets that this approach implements an automatic relevance detection scheme. Finally, in 6, the potential of the VBNMF algorithm on binary toy data is explored. A preliminary version of this paper has been presented at a conference [60]. 2 Bayesian approaches to NMF While NMF was introduced in terms of optimizing a suitable cost function subject to non-negativity constraints, it is well-known that many popular NMF cost functions can be related to statistical models of the reconstruction error of the decomposition via Maximum Likelihood (ML) estimations. For example, the squared Euclidean distance measure is based on Gaussian error statistics, while KL- or IS-divergences as cost functions relate to alternative error statistics given by Poisson or Gamma distributed noise kernels. Hence, constrained optimization of proper cost functions can be interpreted within a statistical perspective as maximum likelihood estimation problems (see e.g. [55,15,64,21]). This opens the field to a conceptually more principled approach based on Bayesian probabilistic interpretations of NMF. It is not accidental that the Richardson–Lucy algorithm which was first presented in 1972 in a paper entitled Bayesian-Based Iterative Method of Image Restoration [53] is one of the forefathers of modern NMF algorithms [16]. Maximum Likelihood estimation is based on adequate reconstruction error statistics. For example, assuming the entries E ij of the reconstruction error matrix E in Eq. (2) to be independently and identically distributed according to a Gaussian distribution with zero mean and variance σ r 2, the joint distribution of all data items X ij factorizes according to the following equation: (4) P ( X | W , H ) = ∏ i ∏ j 1 2 π σ r exp ( − 1 2 ( X ij − [ WH ] ij σ r ) 2 ) Note that Eq. (2) implies the assumption of additive i.i.d. Gaussian noise. Hence, it is only an approximation, since the left hand side, i.e. the difference X ij − E ij could turn negative, in principle. Since the right hand side of this equation, i.e. the term [ WH ] ij is non-negative by definition, and the observed data is also non-negative, the noise cannot be independent from the data and small observations X ij are related to low noise levels E ij in practice. Since in the following we will consider situations only where the noise parameter σ r is sufficiently small, this subtlety can be neglected here. In the following, we will refer to Eq. (4) as Gaussian likelihood for NMF. For Gaussian noise kernels, maximizing the log-likelihood of the data corresponds to minimizing the quadratic error function D E (see Eq. (1)). However, in addition to specifying proper cost functions according to data statistics, suitable prior distributions can be used to integrate existing knowledge about the data and enforce desired characteristics of the solutions. For example, non-negative sparse coding [32] actually is a maximum a posteriori (MAP) estimation, assuming independent exponential prior distributions of the weights W ij and flat priors on the features H kj . Several papers suggest Bayesian techniques to explicitly incorporate prior knowledge on the factor matrices in NMF, including independent Gamma priors [48], Gaussian process priors [64], or Gamma chain priors [75] for audio signal modeling. Also various volume priors have been discussed in the context of volume constrained spectral unmixing methods to favor solutions which span a minimum volume of the simplex formed by the data (see [3,62] for short discussions). The examples mentioned above seek to model the characteristics of desired solutions by specific shapes or properties of the prior distributions. Contrasting such approaches, we will show how Bayesian techniques can also cope with cases where additional information on the properties of the factor matrices is missing and derive a Bayesian optimality criterion for NMF solutions in paragraph 3. We will see that this approach is closely related to minimum volume constraints imposed on the optimization of Frobenius-like cost functions to tackle the uniqueness problem of NMF (see [63]). In general, Bayesian probability theory offers a principled way to handle uncertainty and incorporate prior knowledge to source separation problems [38]. Bayesian techniques are not only suitable for determining optimal parameter settings in source separation problems, they also provide a framework for model comparison [47]. In NMF settings, different models can be identified with different numbers of underlying sources K. Assuming a model K, the posterior distribution of parameters times the evidence which the data provides for model K equals the likelihood multiplied by the prior according to Bayes׳ rule [47,9]: (5) P ( W , H | X , K ) P ( X | K ) = P ( X | W , H ) P ( W , H | K ) By means of Eq. (5) the relevant quantities can be transformed into each other. In a first level of Bayesian inference, the most probable set of parameters W , H is estimated by maximizing P ( W , H | X , K ) within a model with a fixed number K of relevant features. A second level of inference then compares alternative models with each other by evaluating the evidence for model K (6) P ( X | K ) = ∬ P ( X | W , H ) P ( W , H | K ) d W d H for different values of K. In Section 4 we will interpret different models by different factorization ranks K and see how Bayesian inference can help to determine the optimal number of sources in a given data set under specific prior assumptions. Finally, full Bayesian approaches to NMF including the task of model order selection have been studied also recently. Approaches to solve the model order selection problem of NMF are closely related to automatic relevance detection (ARD) schemes which have been first mentioned in [45] and have been put forward by Bishop [10] in a Bayesian PCA approach. While Cemgil [12] discusses the Poisson likelihood case, Schmidt et al. [66] demonstrates Gibbs sampling schemes in connection with Chib׳s method for the Gaussian likelihood case. The latter group [3] also discusses a Bayesian NMF approach with a volume prior and an inference scheme based on Gibbs sampling. Furthermore, Zhong and Girolami [76] and Schmidt and Mørup [65] use two different reversible jump MCMC approaches for model order selection in the Gaussian likelihood case. In [21], the use of Markov Chain Monte Carlo (MCMC) and variational Bayes methods is sketched to perform Bayesian inference on NMF problems using the Frobenius norm, the KL-divergence and the Itakura–Saito divergence as cost functions. Bayesian approaches to model order selection for NMF have further been discussed in [72] in a MAP estimation framework to find the number K of meaningful components into which to decompose the given data matrix. 3 Bayesian optimality condition for NMF with Gaussian likelihood and fixed model order K In the following, we will formulate a Bayesian Optimality Condition (BOC) for estimating a set of features, represented by the rows H k ⁎ , k = 1 , … , K of matrix H , which optimally explain the set of observations X under the prerequisite that the statistics underlying the reconstruction error follow a Gaussian distribution and the order of the model, i.e. the number of sources K is known. 3.1 The most probable matrix H Given a non-negative data set X and the number K of underlying features but no additional prior information, we are interested in the most probable non-negative matrix H representing the non-negative observations X via non-negative weights W . This estimation requires the computation of the posterior distribution of H , which can be deduced from the associated data likelihood and related priors according to Bayes׳ rule, (see Eq. (5)), as (7) P ( H | X ) = P ( X | H ) P ( H ) P ( X ) . If no specific knowledge on the values of H , except their non-negativity, is available, a flat prior can be employed. The latter can be realized with a rectified Gaussian with large spread parameter (see Eq. (24)). This renders the resulting posterior approximately proportional to P ( X | H ) , which can be computed via (8) P ( X | H ) = ∫ P ( X | W , H ) P ( W ) d W . The last expression represents the likelihood integrated over all possible weights W ik and involves NK dimensions. In the following, we will utilize Laplace׳s method to approximate the high-dimensional integral in Eq. (8) which cannot be computed analytically in the general case. 3.2 Laplace׳s approximation for NMF Let W ⁎ be the argument which maximizes the integrand in Eq. (8) and, for the moment, assume it is known. Then utilizing Laplace׳s method (see Appendix A.1), the integral in Eq. (8) can be approximated by a Gaussian integral around W ⁎ , yielding (9) P ( X | H ) ≈ P ( X | W ⁎ , H ) P ( W ⁎ ) ( 2 π ) NK / 2 ( det ( A ( W ⁎ ) ) ) − 1 / 2 where (10) A ( W ⁎ ) ≕ − ∇ W ∇ W { ln ( P ( X | W , H ) P ( W ) ) } | W = W ⁎ is a NK × NK Hessian matrix containing the second order derivatives w.r.t. all entries W ik . 3.3 The Bayesian optimality condition Assuming a Gaussian likelihood function, i.e. Gaussian statistics underlying the reconstruction error E ij = X ij − [ WH ] ij for NMF, as given in Eq. (4) and a sufficiently flat prior for the non-negative weights W , the second order derivatives (see Eq. (10)) can be approximated by (11) ∂ 2 ∂ W ab ∂ W cd { ln ( P ( X | W , H ) ) + ln ( P ( W ) ) } ≈ − 1 σ r 2 ∑ j H bj H dj δ ac . Hence, the Hessian matrix A is built up of N 2 copies of the matrix HH T and exhibits a simple block structure. In fact, the whole matrix A can be brought to upper triangular form A ◃ by exactly the same row manipulations which are necessary for the constituent block matrix HH T . Moreover, the resulting diagonal entries of A ◃ are N respective copies of the diagonal entries of the corresponding triangular matrix ( HH T ) ◃ . If λ 1 , … , λ K are the eigenvalues of HH T , then A possesses eigenvalues ( λ 1 ) N , … , ( λ K ) N and a determinant (12) det ( A ) = ∏ k = 1 K ( λ k ) N = ( ∏ k = 1 K λ k ) N = ( det ( HH T ) ) N . Note that, since the second order derivatives in Eq. (11) do not depend on W any more, all higher order terms in the Taylor expansion of the Laplace approximation (see Appendix A.1) vanish in case of a Gaussian likelihood function and flat priors on W . In that special case, an optimal H can be obtained by (13) H ⁎ = arg max H { ln P ( X | H ) } ≈ arg max H { − 1 2 σ r 2 ∑ i ∑ j ( X ij − [ W ⁎ H ] ij ) 2 − N 2 ln det ( HH T ) } , W ≥ 0 , H ≥ 0 . Eq. (13) represents a Bayesian optimality condition for NMF with Gaussian likelihood, conveniently called BOC, assuming flat priors P ( W ) , P ( H ) . Note that flat priors represent rather mild constraints compared to the widely discussed sparsity constraints. The derivation of the BOC just detailed renders it rather similar to the famous Bayesian Information Criterion (BIC) [67]. The latter can be derived by applying a Laplace approximation to all parameters, while the BOC treats the parameters W and H differently. The second term in Eq. (13) can be interpreted as a constraint penalizing large volumes. Remarkably, the BOC deduced above favors a solution almost identical to the one obtained from the heuristical detNMF criterion suggested in [63]. Compared to the detNMF criterion, the penalty term of the BOC differs in that it contains the logarithm of the determinant. Utilizing the fact that the logarithm is a concave function, it can be shown that the algorithm which minimizes the detNMF cost function (see [63]) also serves to implement the BOC condition yielding optimal W ⁎ , H ⁎ . As is illustrated in Fig. 1 , the detNMF criterion results in a natural choice of an optimal solution, i. e. a set of nonnegative vectors { H k ⁎ } k = 1 K , enclosing the data X in the tightest possible way (see [63] for details). Note that the discussion so far assumes the actual number K of components to be known. The remaining discussion deals with the model order selection problem of how to automatically obtain an appropriate K from the data. 4 Variational Bayes NMF In Section 3, in a first level of inference, an approximation to the log-likelihood ln P ( X | H ) has been deduced based on Gaussian statistics and a given model order K, i.e. a known number K of underlying sources H = ( H 1 ⁎ … H K ⁎ ) T . This approximation resulted in a Bayesian optimality criterion (BOC) for estimating the K×M-dimensional matrix H . In this section, now a second level of inference (see discussion to Eq. (5)) is considered where a variational approximation to the log-evidence (14) ln P ( X | K ) = ln ∬ P ( X | W , H ) P ( W , H | K ) d W d H is derived by integrating over all H kj as well as over all W ik . The following discussion thus derives a Variational Bayes NMF Algorithm (VBNMF) which allows an iterative and approximative computation of Eq. (14). However, arriving at proper update rules is only possible if a number of simplifying assumptions is introduced. We will show that thereby the model order selection problem can be relieved and an optimal number of sources K can be determined automatically. Variational methods have been extensively used in the field of Bayesian learning [35,11,5,26,29,36]. In general, they intend to transform a complicated integration problem such as the computation of the evidence in Eq. (14) into an optimization problem. Note that Jensen׳s inequality [34] can be employed to convert the, generally intractable, logarithm of an expectation (see Eq. (14)) into expectations of logarithms with respect to properly chosen auxiliary distributions: (15) ln P ( X ) ≥ ∬ Q ( W , H ) ln P ( X | W , H ) P ( W , H ) Q ( W , H ) d W d H = 〈 ln P ( X | W , H ) 〉 Q ( W , H ) + 〈 ln P ( W , H ) Q ( W , H ) 〉 Q ( W , H ) ≔ B Q . The auxiliary variational distribution Q ( W , H ) , obeying the normalization constraint ∫ Q ( W , H ) d W d H = 1 , induces a lower bound B Q on ln P ( X ) . The term 〈 · 〉 Q denotes the expectation w.r.t. the distribution Q. It can be shown [25,6] that equality holds, if Q ( W , H ) = P ( W , H | X ) where the latter represents the posterior distribution of the parameters W and H . However, computing the true posterior is not feasible, in general, due to the high-dimensional integral. One way around this difficulty is provided by sampling procedures as mentioned in [12,66] who apply Gibbs sampling to draw samples from the true posterior P ( W , H | X ) . Here, instead, we follow a variational approach, which is similar to the one proposed in [12], except that we invoke a Gaussian likelihood assumption rather than Poisson statistics. In effect, this approach forms a variant of the Variational Bayesian EM Algorithm proposed in [5] and further discussed in [26,6]. 4.1 Variational Bayes for NMF with Gaussian likelihood Under the assumption of a Gaussian likelihood, i.e. Gaussian statistics prevailing to the reconstruction error of the NMF approximation (Eq. (4)), and furthermore assuming completely factorized distributions (16) Q ( W , H ) = ∏ i ∏ k Q ( W ik ) ∏ k ∏ j Q ( H kj ) (17) P ( W , H ) = ∏ i ∏ k P ( W ik ) ∏ k ∏ j P ( H kj ) , the lower bound B Q to the log-evidence ln P ( X | K ) takes the form (18) B Q = − NM 2 ln ( 2 π ) − NM ln σ r − 1 2 σ r 2 × ∑ i ∑ j { X ij 2 − 2 X ij ∑ k 〈 W ik 〉 Q ( W ik ) 〈 H kj 〉 Q ( H kj ) + 〈 ( [ WH ] ij ) 2 〉 Q ( W ) , Q ( H ) } + ∑ i ∑ k 〈 log P ( W ik ) Q ( W ik ) 〉 Q ( W ik ) + ∑ k ∑ j 〈 log P ( H kj ) Q ( H kj ) 〉 Q ( H kj ) . Note that the quadratic term 〈 ( [ WH ] ij ) 2 〉 Q ( W ) , Q ( H ) can be expressed in terms of first and second order expectations. However, applying Jensen׳s inequality once more allows us to directly derive a Bayesian extension of the famous Lee–Seung multiplicative update equations. Utilizing the convexity of a quadratic function leads to a decoupling of the joint ensemble average 〈 ( [ WH ] ij ) 2 〉 Q ( W ) , Q ( H ) into separate averages yielding a new lower bound (19) B Q ≥ B q ≔ − NM 2 [ ln ( 2 π σ r 2 ) ] − 1 2 σ r 2 ∑ i ∑ j { X ij 2 − 2 X ij ∑ k 〈 W ik 〉 Q ( W ik ) 〈 H kj 〉 Q ( H kj ) + ∑ k 1 q ikj 〈 ( W ik ) 2 〉 Q ( W ik ) 〈 ( H kj ) 2 〉 Q ( H kj ) } + ∑ i ∑ k 〈 log P ( W ik ) Q ( W ik ) 〉 Q ( W ik ) + ∑ k ∑ j 〈 log P ( H kj ) Q ( H kj ) 〉 Q ( H kj ) where the q ikj obey the closure relation ∑ k q ikj = 1 for every observation ij. Now Eq. (19) only contains expressions which are computable. B q is always a lower bound to the log-evidence. The best approximation to ln P ( X ) is achieved by maximizing B q w.r.t. all auxiliary variational distributions Q ( W ik ) and Q ( H kj ) as well as the decoupling parameters q ikj . Optimal variational densities Q ( H kj ) can be obtained by solving the fixed point equation (see e.g. [9] for a brief introduction to variational calculus) (20) 0 = ! ∂ ∂ Q ( H kj ) ( B q − λ kj ( ∫ Q ( H kj ) dQ ( H kj ) − 1 ) ) where λ kj is a Lagrange parameter ensuring that Q ( H kj ) is a density. Straightforward computation leads to (21) Q ( H kj ) ∝ P ( H kj ) exp ( α kj H ( H kj ) 2 + β kj H H kj ) where α kj H = − 1 2 σ r 2 ∑ i 1 q ikj 〈 ( W ik ) 2 〉 Q ( W ik ) β kj H = 1 σ r 2 ∑ i X ij 〈 W ik 〉 Q ( W ik ) . Note that this relation holds whatever the priors P ( H kj ) might be. Optimal densities Q ( W ik ) can be computed in complete analogy to the calculation given above. The price to pay for introducing the decoupling approximation (see Eq. (19)) comprises additional parameters q ikj . A strict Bayesian approach would require assigning prior distributions to the parameters σ r as well and integrating them out. Following the discussion in [44,46], we assume that the distribution P ( σ r | X , W , H ) is sharply peaked at the maximum and maximizes B q w.r.t. σ r instead of performing an additional integration. The decoupling parameters q ikj render the bound as tight as possible by setting (22) q ikj = 〈 ( W ik ) 2 〉 Q ( W ik ) 〈 ( H kj ) 2 〉 Q ( H kj ) ∑ l 〈 ( W il ) 2 〉 Q ( W il ) 〈 ( H lj ) 2 〉 Q ( H lj ) . Finally, the optimal noise parameter σ r is obtained as (23) σ r 2 = 1 NM ∑ i ∑ j { X ij 2 − 2 X ij ∑ k 〈 W ik 〉 Q ( W ik ) 〈 H kj 〉 Q ( H kj ) + ∑ k 1 q ikj 〈 ( W ik ) 2 〉 Q ( W ik ) 〈 ( H kj ) 2 〉 Q ( H kj ) } . To proceed, the prior distributions P ( H kj ) , P ( W ik ) need to be specified. Following Eq. (21) this automatically determines the auxiliary distributions Q ( H kj ) , Q ( W ik ) . 4.2 The rectified Gaussian distribution A prior which is nonzero only for non-negative real-valued arguments can be provided by the rectified Gaussian distribution [29]. It derives from a Gaussian distribution by truncating all negative entries by a unit step function, and re-normalizing the integral to unity. The resulting distribution is given by (24) N + ( θ | μ , σ ) ≕ { ( Z N + ) − 1 exp ( − 1 2 ( θ − μ σ ) 2 ) , θ ≥ 0 0 , θ < 0 The normalizing constant is given by (25) Z N + = 1 2 σ 2 π erfc ( − μ σ 2 ) where (26) erfc ( z ) ≕ 2 π ∫ z ∞ exp ( − t 2 ) dt represents the complementary error function. The expectations required for our purpose are then given by (27) 〈 θ 〉 N + = μ + σ 2 Z N + exp ( − μ 2 2 σ 2 ) (28) 〈 θ 2 〉 N + = σ 2 + μ 〈 θ 〉 N + . 4.3 The VBNMF update rules Following we choose a rectified Gaussian prior distribution for the weights W according to (29) P ( W ik ) = N + ( W ik | μ W 0 k , σ W 0 k ) . One such distribution is assumed for each column W ⁎ k and expresses our “belief” that the a priori weights of source k are drawn from the same distribution. Similarly, we choose the prior distribution on each row H k ⁎ to be (30) P ( H kj ) = N + ( H kj | μ H k 0 , σ H k 0 ) . Due to the conjugation principle, the corresponding auxiliary variational distributions are obtained as (31) Q ( W ik ) = N + ( W ik | μ W ik , σ W ik ) and (32) Q ( H kj ) = N + ( H kj | μ H kj , σ H kj ) where (33) μ W ik = μ W 0 k σ W k 0 − 2 + ( σ r ) − 2 ∑ j X ij 〈 H kj 〉 σ W 0 k − 2 + ( σ r ) − 2 ∑ j q ikj − 1 〈 H kj 2 〉 (34) σ W ik = ( σ W 0 k − 2 − ( σ r ) − 2 ∑ j q ikj − 1 〈 H kj 2 〉 ) − 1 / 2 and (35) μ H kj = μ H k 0 σ H k 0 − 2 + ( σ r ) − 2 ∑ i X ij 〈 W ik 〉 σ H k 0 − 2 + ( σ r ) − 2 ∑ i q ikj − 1 〈 W ik 2 〉 (36) σ H kj = ( σ H k 0 − 2 − ( σ r ) − 2 ∑ i q ikj − 1 〈 W ik 2 〉 ) − 1 / 2 . If we have no knowledge on the actual values of the parameters of the priors, these hyperparameters μ W 0 k , σ W 0 k and μ H k 0 , σ H k 0 can be updated by iteratively solving the corresponding fixed point equations: (37) ∂ B q ∂ μ W 0 k = ! 0 , ∂ B q ∂ σ W 0 k = ! 0 (38) ∂ B q ∂ μ H k 0 = ! 0 , ∂ B q ∂ σ H k 0 = ! 0 . The overall Variational Bayes NMF (VBNMF) algorithm, as proposed above, can be summarized by • set hyperparameters μ H k 0 , σ H k 0 , μ W 0 k , σ W 0 k • initialize μ H kj , σ H kj , μ W ik , σ W ik and σ r • Repeat 1. compute all q ikj (Eq. 22) 2. update all μ H kj , σ H kj (35, 36) 3. update all μ W ik and σ W ik (33,34) 4. update σ r (23) ⁎ 5. update hyperparameters (37,38) ⁎ 6. compute B q (Eq. 19) ⁎⁎ until convergence Steps 4 and 5 ⁎ are optional if no prior knowledge exists, step 6 ⁎ ⁎ should be performed only occasionally testing the state of convergence of the algorithm. Since the latter represents a generalized Expectation–Maximization (EM) algorithm [5,26,6], convergence is assured. 4.4 Relation to the Lee–Seung method It is interesting to note that the VBNMF algorithm derived above is a straight forward Bayesian generalization of the multiplicative NMF algorithm for the Euclidean distance proposed by Lee and Seung [41]. To see this, we derive a MAP version of the VBNMF algorithm as follows: replacing the expectations of the form 〈 W ik 〉 Q ( W ik ) , 〈 H kj 〉 Q ( H kj ) by the MAP values W ik ⁎ H kj ⁎ , the update equation for the decoupling parameters q ikj Eq. (22) reduces to (39) q ikj = W ik ⁎ H kj ⁎ ∑ l W il ⁎ H lj ⁎ . A prior P ( H kj ) of the form of Eq. (30) becomes a flat prior if it has a large spread parameter σ H kj , and in the limit σ H kj ↦ ∞ the first terms in the numerator and denominator of Eq. (35) go to zero. After further cancelation of σ r , the update rule for the mean parameter μ H kj reduces to (40) μ H kj = ∑ i X ij W ik ⁎ ∑ i ( W ik ⁎ ) 2 q ikj . Plugging in the q ikj from Eq. (39) finally yields (41) μ H kj = ∑ i X ij W ik ⁎ ∑ i ( W ik ⁎ ) 2 ∑ l W il ⁎ H lj ⁎ W ik ⁎ H kj ⁎ (42) μ H kj = H kj ⁎ ∑ i X ij W ik ⁎ ∑ i W ik ⁎ ∑ l W il ⁎ H lj ⁎ (43) μ H kj = H kj ⁎ [ W ⁎ T X ] kj [ W ⁎ T W ⁎ H ⁎ ] kj . The last expression is exactly one of the update equations proposed by Lee and Seung [41]. Due to symmetry, the same can be shown for the weight parameters W ik . Note that due to the uncoupling approximation in Eq. (16) the determinant constraint (see Section 3) disappears. 5 Simulations on toy data sets Artificial toy data sets X were generated according to Eq. (2), with the elements of the error term drawn from a Gaussian distribution with zero mean and variance σ r 2, using the fixed 3×5 matrix of basis vectors: (44) H = ( 3 1 0 1 3 1 2 3 4 5 0 3 3 3 0 ) . The columns W ⁎ k of the corresponding weight matrix W were generated from rectified Gaussian distributions with parameters set to μ 0 k = 3 k , σ 0 k = 3 k . The simulations employed the VBNMF algorithm given above while varying the model order over the range K = 2 , … , 8 . The hyper-parameters were updated algorithmically as well, since in general their distribution is not available as prior knowledge. Hence, the assumptions underlying the simulations are a Gaussian noise distribution and a rectified Gaussian prior distribution of the weights W . Several instances of the noise parameter σ r were tested and we eliminated possible negative data entries induced by the noise by setting them to zero. Note that this truncation constitutes a slight violation of the likelihood assumption which increases with increasing noise level. In the following, we discuss two cases where 1. matrix W obeys the prior assumptions and is generated from a rectified distribution (Fig. 2 , left), 2. matrix W is drawn from a mixture of a rectified Gaussian and a peak at zero (Fig. 2, right). 5.1 Valid priors for the weights If the assumption concerning the distribution of W holds true, i.e. (45) P ( W ik ) = N + the correct number of sources is, for various noise levels, detected by the VBNMF algorithm (see Fig. 3 ) through a maximum in the B Q plot. 5.2 Invalid priors for the weights Next, the assumption on the prior of W has been violated deliberately by adding a delta peak at the origin of the rectified Gaussian distribution (Fig. 2, right) according to (46) P ( W ik ) = τ N + + ( 1 − τ ) δ ( W ik ) . Such priors could represent sparse priors in practical situations. In the noise-free case, the true number of sources K=3 is detected (see Fig. 4 ). If the noise level is low (σ r =1 here), the lower bound B q of the true log-evidence still identifies the correct order but also suggests a slightly increased order of the model, i.e. it overestimates the number of sources slightly, while in the case of high noise levels the method fails completely and bad estimates of the order of the model result. As discussed e.g. in [36], there is another approach to determine an optimal K. If the algorithm is initialized with a large number of components K > 3 , “the priors will drive unnecessary components to distinction”. Fig. 5 illustrates such a situation. According to Eq. (19), the lower bound B q on the log-evidence contains a data dependent part (terms two to four), and a data independent part (terms five and six). The more components are available, the higher the chance for a good reconstruction (terms two to four) of the observations. However, for each additional component, there is also an additional cost expressed by the corresponding priors ( P ( W ik , P ( H kj ) . The VBNMF algorithm seeks to maximize the overall bound B q . If the data reconstruction does not improve significantly in consequence of an additional component, the algorithm will drive the penalty terms, i.e. those related to the priors, to large values. Since every element of a row of H is associated to the same prior distribution, Eq. (30), a locally optimal solution will set unnecessary components and weights to some constant values. Fig. 5 illustrates that for K > 3 the VBNMF algorithm not necessarily finds the global optimum but ends in a locally optimal solution. Ideally, the three original components should be part of a globally optimal decomposition, plus all odd components set to constant values. Hence, the algorithm got stuck in a spurious optimum while paying too much attention to the priors. 5.3 Discussion of the simulation results The simulations of this toy data set consider the two cases where the assumed model either conforms to the prior assumptions or where the distribution underlying the weight matrix W is not identical to the true functional form of the prior. In the first case, the VBNMF algorithm could indeed estimate the order of the model, i.e. it could recover the correct number of sources W ⁎ k , k = 1 , … , 3 by exhibiting a clear maximum at K=3 (see Fig. 3). In the second case, only at low noise levels did the algorithm detected the correct model order K but failed at high noise levels. We furthermore observed that (irrelevant) estimated sources were set to a constant value, an effect which was also reported in similar investigations, e.g. [45,36,72]. Thus, the only maximizing the lower bound B q does not tell the whole story. This is because the solution can contain dummy sources, and the effective number of parameters which is used to explain the data is actually smaller than the possible number N × K + K × M . This is because terms concerning the reconstruction of the data (the terms 2–4 in Eq. (19)) and terms penalizing a large number of parameters (the terms 5 and 6 in Eq. (19)) have to be balanced. This would result in an optimal number of parameters necessary to properly explain the observations. If the model assumptions are violated, in general, the correct balance cannot be achieved automatically. 6 Binary test data example In [61] an extension of NMF for binary data sets, called binNMF, is introduced and discussed in relation with binary test data sets collected during the fabrication of semiconductor microchips. The following example is meant to test the potential of the VBNMF algorithm on such a binary toy data set. 6.1 A short primer on binNMF The i-th observation constitutes a row vector X i ⁎ = ( X i 1 , … , X iM ) whose components represent the binary pass ( X ij = 0 ) or fail ( X ij = 1 ) status of M measurements (47) P ( X ij = 0 | S k ) = exp ( − W ik H kj ) where S k represents the k-th underlying source. Given K independent sources S k , the overall conditional pass and fail probabilities are P ( X ij = 0 | S 1 , … , S K ) = ∏ k = 1 K exp ( − W ik H kj ) = exp ( − ∑ k = 1 K W ik H kj ) PX ij = 1 | S 1 , … , S K = 1 − exp ( − ∑ k = 1 K W ik H kj ) . Although the Bernoulli likelihood with parameter Θ ij = 1 − exp ( − [ WH ] ij ) would be the natural choice for binary data, in [61] a rough approximation is discussed, which sets (48) X ˜ ≈ WH , where X ˜ ij = − ln ( 1 − α X ij ) for some suitable parameter α and optimizes the quadratic cost function (49) E ( α , W , H ) = ∑ i = 1 N ∑ j = 1 M ( ln ( 1 − α X ij ) + [ WH ] ij ) 2 instead of the more complicated Bernoulli log likelihood. 6.2 VBNMF on binary data The basic patterns given by the 4×900 dimensional matrix H are shown in Fig. 6 . Each 30×30 subplot contains one row of H . The original 1000×4 dimensional matrix of weights W is designed by rows with characteristic shapes for an easy visual identification. Of course, these weights and pattern shapes are unknown to the VBNMF algorithm. An extra component is added to both H and W and is intended to reflect a randomly distributed error component. This can be realized by either adding a row of ones to H and by adding a row of weights to W as shown in the last row of Fig. 6, or by explicitly modeling the noise by a column vector c j with gradually increasing components. The Bernoulli parameter Θ ij then corresponds to the expression Θ ij = 1 − exp ( − [ WH ] ij + c j ) . The algorithm binNMF was shown to correctly extract both the sources and the related weights as long as the number of sources is known a priori. The following considers a variational Bayes analog to the Bernoulli likelihood case. To do so, the Bernoulli log-likelihood has to be replaced by the approximation given in Eq. (49) with α=0.6 which yields a good approximation to the original data. Obviously, the reconstruction error E ij = ln ( 1 − 0.6 X ij ) + [ WH ] ij is not distributed according to a Gaussian with mean zero as was assumed for the deduction of the VBNMF algorithm (see Fig. 7 ). Hence, neither the assumption concerning the form of the likelihood nor the prior distributions are met in this example. Still we will apply the VBNMF algorithm to it to test the robustness of the algorithm against “invalid” conditions violating the assumptions underlying its construction. During the simulations, an ordinary Alternating Least Squares (ALS) NMF algorithm (see e.g. [16]) was run for K=2,…, 19 and α=0.6 to generate parameters W ALS and H ALS with which to initialize 〈 W 〉 Q and 〈 H 〉 Q , respectively. The initial values for the parameters of the VBNMF algorithm μ H kj , σ H kj , μ W ik , σ W ik , σ r and the hyper-parameters μ H k 0 , σ H k 0 , μ W 0 k , σ W 0 k where approximated from the ALS solutions and their statistics. An alternative but much more time consuming approach would be to multiple random initializations of the VBNMF algorithm to avoid getting stuck in local maxima. Although most of the assumptions were violated, results obtained yield a clear maximum at K=5 (see Fig. 7, right) and good estimates of the underlying sources and their corresponding weight patterns (see Fig. 8 ), corroborating the robustness of the algorithm. Even more interesting is the behavior of the VBNMF algorithm if the number of underlying sources is chosen too large. Figs. 9 and 10 present results obtained when K=10 was chosen for the ALS algorithm which was used to estimate good starting values for the VBNMF algorithm. The ALS algorithm decomposed the observations into K=10 components, some of which resemble the original patterns H k ⁎ and W ⁎ k already reasonably well. The latter are then used to initialize the VBNMF algorithm which finally retains only 5 patterns and drives the remaining components to constants. As can be seen in Fig. 9, the four patterns 1, 5, 6, 10 represent close approximations of the original patterns, while component 9 represents the reconstruction error estimate of the algorithm. Since the latter has the freedom to adjust the prior hyper-parameters during optimization, the prior distributions P ( W ik ) , P ( H kj ) of unnecessary components are driven towards narrow Gauss peaks. In order to avoid a large penalty, the corresponding variational distributions Q ( W ik ) , Q ( H kj ) are set equal to the priors by the algorithm, resulting in constant expectations 〈 W 〉 Q , 〈 H 〉 Q . This effect resembles Automatic Relevance Determination [45,72] and is clearly demonstrated in Figs. 5, 9 and 10. In the first set of simulations, the ability of the VBNMF algorithm to properly deal with binary data sets whose related parameter distributions do not conform with the underlying assumptions was proven. Furthermore, its ability to automatically determine the optimal number of components was demonstrated in an example with a grossly overestimated number of components. A preprocessing of the data with an ALS algorithm to estimate good starting values of the parameters for the VBNMF algorithm proved advantageous to avoid repeated re-initialization to avoid local maxima. 7 Conclusion In this investigation we addressed open problems concerning NMF decompositions X ≈ WH , such as uniqueness and model order selection, by employing Bayesian methods of various sophistication. Considering Gaussian error statistics of an L 2-norm reconstruction error and a flat prior on the feature distribution, we derived a Bayesian optimality criterion (BOC) for an optimal solution, given the correct factorization rank K. This BOC is achieved by integrating over all possible weight matrices W to estimate the posterior distribution of the feature matrix H . To perform the integration, we utilized a Laplace approximation of the related integral. In case of a Gaussian likelihood this yields a simple criterion which is identical to the heuristical detNMF criterion proposed earlier [63]. In the second part of the paper, we introduced a novel variational Bayes algorithm which solves the problem of model order selection and implements an automatic relevance detection mechanism which is able to discover the actual number of components underlying artificially constructed toy data sets. The presented VBNMF algorithm was also shown to be a Bayesian generalization of the famous Lee–Seung NMF algorithm. Finally we explored the potential of the proposed algorithm to analyze binary data sets. There the underlying assumptions relating to the likelihood function and the prior distributions are only roughly valid. Still very acceptable results are obtained. Acknowledgments R.S. has been supported by a scholarship provided by Infineon AG, Regensburg, which is gratefully acknowledged. Appendix A A.1 Laplace׳s method Laplace׳s method, being a saddle point method, is a common technique to approximate high-dimensional integrals (see e.g. [47]). Let θ be an L-dimensional vector and P ˜ ( θ ) an un-normalized probability density. Expanding the logarithm of the integrand in (50) ∫ P ˜ ( θ ) d θ by a Taylor series around its maximum θ ⁎ yields (51) ln P ˜ ( θ ) ≈ ln P ˜ ( θ ⁎ ) − 1 2 ( θ − θ ⁎ ) T A ( θ − θ ⁎ ) + ⋯ where the first derivatives are zero due to the maximum condition and A ij = − ∂ 2 ∂ θ i ∂ θ j ln P ˜ ( θ ) | θ = θ ⁎ is the negative L×L Hessian matrix collecting the second order partial derivatives w.r.t. the entries of vector θ . Exponentiating both sides of Eq. (51) yields (52) P ˜ ( θ ) ≈ P ˜ ( θ ⁎ ) exp ( − 1 2 ( θ − θ ⁎ ) T A ( θ − θ ⁎ ) ) and the original integral can be approximated by the well-known L-dimensional Gaussian: (53) ∫ P ˜ ( θ ) d θ ≈ P ˜ ( θ ⁎ ) ( 2 π ) L det ( A ) . A.2 The Jensen trick For each i , j in Eq. (18), Jensen׳s inequality [34] yields (54) 〈 ( [ WH ] ij ) 2 〉 Q ( W ) , Q ( H ) (55) 〈 ( [ WH ] ij ) 2 〉 Q ( W ) , Q ( H ) = 〈 ( ∑ k q ikj W ik H kj q ikj ) 2 〉 Q ( W ) , Q ( H ) (56) 〈 ( [ WH ] ij ) 2 〉 Q ( W ) , Q ( H ) ≤ ∑ k q ikj 〈 ( W ik H kj q ikj ) 2 〉 Q ( W ) , Q ( H ) (57) 〈 ( [ WH ] ij ) 2 〉 Q ( W ) , Q ( H ) = ∑ k 1 q ikj 〈 ( W ik ) 2 〉 Q ( W ik ) 〈 ( H kj ) 2 〉 Q ( H kj ) . Note that the quadratic terms can be expressed in terms of first and second order expectations. Thus, the second application of Jensen׳s inequality is not a necessary requirement to construct a VB algorithm for the Gaussian likelihood case. For example, the approach presented in [21] relies on the property that Gaussian distributions are closed under summation and reports a variational Bayes NMF algorithm without the second Jensen step used here. Our method thus differs from the one presented in [21] mainly in utilizing the second Jensen bound, which automatically yields a Bayesian version of Lee and Seung׳s multiplicative NMF algorithm. It is also possible to design VBNMF algorithms which do not assume all variational distributions Q ( ) to be mutually independent. The respective choice of a method depends on implementation issues and, depending on the data structure, will be important for the correct model order estimation. For real world data sets, the choice of the likelihood function is also expected to have additional influence on the performance of model order selection. It would be interesting to set up a study designing representative settings for a fair comparison of Bayesian NMF methods with sampling like in [64,65,76] or variational methods [12,21] or this one, which utilize Gaussian, Poisson and other likelihood functions and various prior assumptions. A.3 Rectified Gaussian expectations The Rectified Gaussian distribution is generated from a usual Gaussian by truncating all negative values and renormalizing the integral over the positive reals to one: (58) N + ( θ | μ , σ ) = ( Z N + ) − 1 e − 1 / 2 ( ( θ − μ ) / σ ) 2 , 0 ≤ θ < ∞ , 0 otherwise with normalization constant Z N + = 1 2 σ 2 π erfc ( − μ σ 2 ) and erfc ( z ) = 2 π ∫ z ∞ exp ( − t 2 ) dt is the complementary error function. The sufficient statistics for the rectified Gaussian are (59) 〈 θ 〉 N + = μ + e − μ 2 / 2 σ 2 2 σ π erfc ( − μ σ 2 ) = μ + σ 2 Z RG e − μ 2 / 2 σ 2 (60) 〈 θ 2 〉 N + = σ 2 + μ 2 + e − μ 2 / 2 σ 2 2 μ σ π erfc ( − μ σ 2 ) = σ 2 + μ 〈 θ 〉 N + . Proof: 〈 θ 〉 N + = 1 Z N + ∫ 0 ∞ e − 1 / 2 ( ( θ − μ ) / σ ) 2 θ d θ = 1 Z N + ∫ − μ / σ 2 ∞ e − u 2 ( μ + 2 σ u ) 2 σ du = 1 Z N + ( 2 σ μ ∫ − μ / σ 2 ∞ e − u 2 du + σ 2 ∫ − μ σ 2 ∞ e − u 2 2 u du ) = μ + σ 2 Z N + [ − e − z ] μ 2 / 2 σ 2 ∞ = μ + σ 2 Z N + e − μ 2 / 2 σ 2 = μ + e − μ 2 / 2 σ 2 2 σ π erfc ( − μ σ 2 ) 〈 θ 2 〉 N + = 1 Z N + ∫ 0 ∞ e − 1 / 2 ( ( θ − μ ) / σ ) 2 θ 2 d θ = 1 Z N + ( − σ 2 ∫ 0 ∞ e − 1 / 2 ( ( θ − μ ) / σ ) 2 ( − θ − μ σ 2 ) θ d θ + ∫ 0 ∞ e − 1 / 2 ( ( θ − μ ) / σ ) 2 μ θ d θ ) = 1 Z N + ( − σ 2 [ θ e − 1 / 2 ( ( θ − μ ) / σ ) 2 ] 0 ∞ + σ 2 ∫ 0 ∞ e − 1 / 2 ( ( θ − μ ) / σ ) 2 d θ + ∫ 0 ∞ e − 1 / 2 ( ( θ − μ ) / σ ) 2 μ θ d θ ) = σ 2 + μ 〈 θ 〉 N + = σ 2 + μ 2 + e − μ 2 / 2 σ 2 2 μ σ π erfc ( − μ σ 2 ) . References [1] H. Akaike A new look at the statistical model identification IEEE Trans. Autom. Control 19 6 1974 716 723 [2] H. Akaike, Likelihood and the Bayes procedure, in: J.M. Bernardo, et al. (Ed.), Bayesian Statistics, University Press, Valencia, pp. 143–166. [3] M. Arngren, M.N. Schmidt, J. Larsen, Unmixing of hyperspectral images using Bayesian nonnegative matrix factorization with volume prior, in: Journal of Signal Processing Systems, January 2010. [4] S. Arora, R. Ge, R. Kannan, A. Moitra, Computing a nonnegative matrix factorization – provably, in: Proceedings of the 44th Symposium on Theory of Computing (STOC ׳12), 2012, pp. 145–162, arXiv:1111.0952v1[cs.DS]3.Nov.2011. [5] H. Attias, A variational Bayesian framework for graphical models, in: Advances in Neural Information Processing Systems, vol. 12, MIT Press, Cambridge, USA, 2000, pp. 209–215. [6] M.J. Beal Z. Ghahramani Variational Bayesian learning of directed graphical models with hidden variables Bayesian Anal. 1 2004 1 44 [7] M.W. Berry Computational information retrieval 2001 Society for Industrial and Applied Mathematics Philadelphia, PA, USA [8] M.W. Berry, M. Browne, A.N. Langville, V.P. Pauca, R.J. Plemmons, Algorithms and applications for approximate nonnegative matrix factorization, in: Computational Statistics and Data Analysis, 2006, pp. 155–173. [9] C.M. Bishop Neural Networks for Pattern Recognition 1996 Oxford University Press UK [10] C.M. Bishop, Bayesian pca, in: Advances in Neural Information Processing Systems (NIPS), 1999, pp. 382–388. [11] C.M. Bishop, Variational principal components, in: Proceedings Ninth International Conference on Artificial Neural Networks (ICANN), 1999, pp. 509–514. [12] A.T. Cemgil Bayesian inference in non-negative matrix factorisation models Comput. Intell. Neurosci. 2009 , 17. [13] M. Chagoyen P. Carmona-Saez H. Shatkay J.M. Carazo A. Pascual-Montano Discovering semantic features in the literature a foundation for building functional associations BMC Bioinf. 7 2006 41 [14] A. Cichocki R. Zdunek Multilayer nonnegative matrix factorization using projected gradient approaches Int. J. Neural Syst. 17 6 2007 431 446 [15] A. Cichocki, R. Zdunek, S.-I. Amari, Csiszar׳s divergences for non-negative matrix factorization: family of new algorithms, in: Lecture Notes in Computer Science, vol. 3889, Springer, Berlin, 2006, pp. 32–39. [16] A. Cichocki R. Zdunek A.-H. Phan S.-I. Amari Nonnegative Matrix and Tensor Factorizations Applications to Exploratory Multi-way Data Analysis 2009 John Wiley USA (November) [17] K. Devarajan Nonnegative matrix factorization an analytical and interpretive tool in computational biology PLoS Comput. Biol. 4 7 2008 e1000029 [18] I.S. Dhillon, S. Sra, Generalized nonnegative matrix approximations with Bregman divergences, in: Neural Information Processing Systems, MIT Press, Cambridge, USA, 2005, pp. 283–290. [19] N. Ding, Y. Qi, R. Xiang, I. Molloy, N. Li, Nonparametric Bayesian matrix factorization by power-EP, in: Journal of Machine Learning Research – Proceedings Track, 2010, pp. 169–176. [20] D. Donoho, V. Stodden, When does non-negative matrix factorization give a correct decomposition into parts? in: Neural Information Processing System 2003, MIT Press, Cambridge, USA, 2004. [21] C. Févotte, A.T. Cemgil, Nonnegative matrix factorisations as probabilistic inference in composite models, in: Proceedings of the 17th European Signal Processing Conference (EUSIPCO׳09), Glasgow, 2009. [22] D. Foster, E. George, The risk inflation factor in multiple linear regression, Technical Report, University of Texas, USA, 1993. [23] D. Foster E. George The risk inflation criterion for multiple regression Ann. Stat. 22 1994 1947 1975 [24] E.I. George D.P. Foster Calibration and empirical Bayes variable selection Biometrika 87 4 2000 731 747 [25] Z. Ghahramani, Unsupervised learning, in: Advanced Lectures on Machine Learning, Springer-Verlag, Berlin, 2004, pp. 72–112. [26] Z. Ghahramani, M.J. Beal, Propagation algorithms for variational Bayesian learning, in: Advances in Neural Information Processing Systems, vol. 13, MIT Press, Cambridge, USA, 2001, pp. 507–513. [27] N. Gillis, Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing, 11th April 2012, arXiv:1204.2436v1[stat.ML]. [28] Th.L. Griffiths Z. Ghahramani The Indian buffet process an introduction and review J. Mach. Learn. Res. 12 2011 1185 1224 [29] M. Harva A. Kabán Variational learning for rectified factor analysis Signal Process. 87 3 2007 509 527 [30] M.D. Hoffman, D.M. Blei, P.R. Cook, Baysian nonparametric matrix factorization for recorded music, in: Proceedings of the International Conference on Machine Learning (ICML), Haifa, 2010. [31] P.K. Hopke, A guide to positive matrix factorization, 2000. Available online at: [32] P. Hoyer, Non-negative sparse coding, in: Proceedings of the IEEE Workshop on Neural Networks for Signal Processing, 2002, pp. 557–565. [33] P. Hoyer Non-negative matrix factorization with sparseness constraints J. Mach. Learn. Res. 5 2004 [34] J.L.W.V. Jensen Sur les fonctions convexes et les inégalités entre les valeurs moyennes Acta Math. 30 1906 175 193 [35] M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, L.K. Saul, An introduction to variational methods for graphical methods, in: Machine Learning, MIT Press, Cambridge, USA, 1998, pp. 183–233. [36] A. Kabán E. Bingham Factorisation and denoising of 0-1 data: a variational approach Neurocomputing 71 10–12 2008 2291 2308 [37] D.A. Knowles, Z. Ghahramani, Infinite sparse factor analysis and infinite independent component analysis, in: 7th International Conference on Independent Component Analysis, 2007. [38] K.H. Knuth, Informed source separation: a Bayesian tutorial, in: Proceedings of the 13th European Signal Processing Conference (EUSIPCO 2005), 2005. [39] H. Laurberg M.G. Christensen M.D. Plumbley L.K. Hansen S.H. Jensen Theorems on positive data on the uniqueness of nmf Comput. Intell. Neurosci. 2008 [40] D.D. Lee H.S. Seung Learning the parts of objects by non-negative matrix factorization Nature 401 October (6755) 1999 788 791 [41] D.D. Lee, H.S. Seung, Algorithms for non-negative matrix factorization, in: Neural Information Processing System, vol. 13, 2001, pp. 556–562. [42] S.Z. Li, X.W. Hou, H.J. Zhang, Q.S. Cheng, Learning spatially localized, parts-based representation, in: Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 1, April 2003, pp. I-207–I-212. [43] C.-J. Lin On the convergence of multiplicative update algorithms for nonnegative matrix factorization IEEE Trans. Neural Netw. 18 6 2007 1589 1596 [44] D.J.C. MacKay Bayesian interpolation Neural Comput. 4 3 1992 415 447 [45] D.J.C. MacKay, Ensemble learning and evidence maximization, Technical Report, Cavendish Laboratory, University of Cambridge, 1995. [46] D.J.C. MacKay, Hyperparameters: optimize, or integrate out?, in: Maximum Entropy and Bayesian Methods, Santa Barbara 1993, Kluwer, Dordrecht, NL, 1996, pp. 43–60. [47] D.J.C. MacKay Information Theory, Inference, and Learning Algorithms 2003 Cambridge University Press Cambridge, UK Available from [48] S. Moussaoui, D. Brie, O. Caspary, A. Mohammad-Djafari, A Bayesian method for positive source separation, in: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2004, pp. 485–488. [49] P. Paatero Least squares formulation of robust non-negative factor analysis Chemom. Intell. Lab. Syst. 37 1997 23 35 [50] P. Paatero U. Tapper Positive matrix factorization a non-negative factor model with optimal utilization of error estimates of data values Environmetrics 5 2 1994 111 126 [51] M. Plumbley Algorithms for nonnegative independent component analysis IEEE Trans. Neural Netw. 14 2003 534 543 [52] M. Plumbley E. Oja A nonnegative pca algorithm for independent component analysis IEEE Trans. Neural Netw. 15 2004 66 76 [53] H.W. Richardson Bayesian-based iterative method of image restoration J. Opt. Soc. Am. 62 January (1) 1972 55 59 [54] J. Rissanen A universal prior for integers and estimation by minimum description length Ann. Stat. 11 2 1983 416 431 [55] P. Sajda, S. Du, T. Brown, L. Parra, R. Stoyanova, Recovery of constituent spectra in 3d chemical shift imaging using non-negative matrix fatorization, in: 4th International Symposium on Independent Component Analysis and Blind Signal Separation, 2003, pp. 71–76. [56] P. Sajda S. Du T.R. Brown R.S. Stoyanova D.C. Shungu X. Mao L.C. Parra Nonnegative matrix factorization for rapid recovery of constituent spectra in magnetic resonance chemical shift imaging of the brain MedImg 23 December (12) 2004 1453 1465 [57] P. Sajda, S. Du, L. Parra, Recovery of constituent spectra using non-negative matrix fatorization, in: Proceedings of the Wavelets: Applications to Signal and Image Processing (SPIE), vol. 5207, 2003, pp. 321–331. [58] L.K. Saul D.D. Lee Multiplicative updates for classification by mixture models Neural Inf. Process. Syst. 14 2002 897 904 [59] R. Schachtner D. Lutter P. Knollmüller A.M. Tomé F.J. Theis G. Schmitz M. Stetter P. Gómez-Vilda E.W. Lang Knowledge-based gene expression classification via matrix factorization Bioinformatics 24 2008 1688 1697 [60] R. Schachtner, G. Pöppel, E.W. Lang, Bayesian extensions to non-negative matrix factorization, in: Proceedings of the 2nd International Workshop on Cognitive Information Processing on Elba Island (CIP2010), June 2010. [61] R. Schachtner G. Pöppel E.W. Lang A nonnegative blind source separation model for binary test data Trans. Circ. Syst. Part I 57 7 2010 1439 1448 [62] R. Schachtner G. Pöppel E.W. Lang Towards unique solutions of non-negative matrix factorization problems by a determinant criterion Digit. Signal Process. 2011 528 534 [63] R. Schachtner, G. Pöppel, A.M. Tomé, E.W. Lang, Minimum determinant constraint for non-negative matrix factorization, in: International Conference on Independent Component Analysis and Signal Separation, Lecture Notes in Computer Science (LNCS), vol. 5441, Springer, Berlin, 2009, pp. 106–113. [64] M.N. Schmidt H. Laurberg Non-negative matrix factorization with Gaussian process priors Comput. Intell. Neurosci. 2008 [65] M.N. Schmidt, M. Mørup, Infinite non-negative matrix factorization, in: European Signal Processing Conference (EUSIPCO), August 2010. [66] M.N. Schmidt, O. Winther, L.K. Hansen, Bayesian non-negative matrix factorization, in: International Conference on Independent Component Analysis and Signal Separation, Lecture Notes in Computer Science (LNCS), vol. 5441, Springer, Berlin, 2009, pp. 540–547. [67] G.E. Schwarz Estimating the dimension of a model Ann. Stat. 6 2 1978 461 464 [68] P. Smaragdis, J.C. Brown, Non-negative matrix factorization for polyphonic music transcription, in: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2003, pp. 177–180. [69] S. Sra, I.S. Dhillon, Nonnegative matrix approximation: algorithms and applications, Technical Report, UTCS Report TR-06-27, 2006. [70] K. Stadlthanner F.J. Theis A.M. Tomé C.G. Puntonet J.M. Górriz E.W. Lang Hybridizing sparse component analysis with genetic algorithms for microarray analysis Neurocomputing 71 10–12 2008 2356 2376 [71] K. Stadlthanner, F.J. Theis, C.G. Puntonet, J.-M. Górriz, A.M. Tomé, E.W. Lang, Hybridizing sparse component analysis with genetic algorithms for blind source separation, in: ISBMDA, Lecture Notes in Computer Science (LNBI), vol. 3745, Springer, Heidelberg, 2005, pp. 137–148. [72] V.Y.F. Tan, C. Fevotte, Automatic relevance determination in nonnegative matrix factorization, in: Proceedings of the Workshop on Signal Processing with Adaptative Sparse Structured Representations (SPARS׳09), 2009. [73] F.J. Theis, K. Stadlthanner, T. Tanaka, First results on uniqueness of sparse non-negative matrix factorization, in: European Signal Processing Conference, 2005. [74] S. Vavasis On the complexity of nonnegative matrix factorization SIAM J. Optim. 2009 1364 1377 [75] T.O. Virtanen, A.T. Cemgil, S.J. Godsill, Bayesian extensions to nonnegative matrix factorisation for audio signal modelling, in: Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing, 2008. [76] M. Zhong M. Girolami Reversible jump mcmc for non-negative matrix factorization J. Mach. Learn. Res. 5 2009 663 670 R. Schachtner received his Diploma in Physics 2007 at the department of biophysics at the University of Regensburg and his Ph.D. in Physics, 2010, at the University of Regensburg. He was working on machine learning applications to microarray analysis and the analysis of wafer test patterns in collaboration with Infineon Technologies AG in Regensburg. In 2011 he joined Infineon Technologies AG in Regensburg working on the development and application of data analysis methods. G. Po¨ppel received the Diploma in physics and the Ph.D. degree in theoretical physics from the University of Regensburg. In 1996, after working as a consultant in statistics and optimization, he joined Infineon Technologies AG. He is currently responsible for the application of data analysis and the development of methods for data analysis at Infineon Technologies. A.M. Tomé is a Ph.D. in Electrical Engineering from University of Aveiro in 1990. Currently she is an Associate Professor of Electrical Engineering in the Department of Electronics and Telecomunications/IEETA of the University of Aveiro where she teaches courses on Digital Signal Processing for Electronics and Computer Engineering Diplomas. Her research interests include Digital and Statistical Signal Processing, Independent Component Analysis and Blind Source Separation as well as Classification and Pattern Recognition Applications of Neural Networks. C.G. Puntonet received the B.Sc. degree in 1982, the M.Sc. degree in 1986 and his Ph.D. degree in 1994, all from the University of Granada, Spain. These degrees are in electronics physics. He has been Visiting Researcher at the “Laboratorie de Traitement d׳Images et Reconnaissance de Formes” (INPG, Grenoble, France), at the Institute of Biophysics, Neuro and Bioinformatics Group (Regensburg, Germany) and at the Institute of Physical and Chemical Research (RIKEN, Nagoya, Japan). Currently, he is a Full Professor at the department of “Architecture and Computer Technology” at the University of Granada, Spain. His research interests lie in the fields of signal processing, independent component analysis and blind separation of sources, artificial neural networks, optimization methods and biomedical applications. E.W. Lang He received his physics diploma in 1977 and his Ph.D. in physics in 1980 and habilitated in Biophysics in 1988 at the University of Regensburg. He is an Adjunct Professor of Biophysics at the University of Regensburg, where he is the head of the Computational Intelligence and Machine Learning Group. Currently he serves as an associate editor of Neurocomputing, Neural Information Processing – Letters and Reviews and ISRN Signal Processing. His current research interests include biomedical signal and image processing, independent component analysis and blind source separation, time series analysis and empirical mode decomposition and neural networks for classification and pattern recognition. "
    },
    {
        "doc_title": "A Bayesian approach to the Lee-Seung update rules for NMF",
        "doc_scopus_id": "84901397987",
        "doc_doi": "10.1016/j.patrec.2014.04.013",
        "doc_eid": "2-s2.0-84901397987",
        "doc_date": "2014-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Bayesian approaches",
            "Canonical form",
            "Lee-Seung update rules",
            "Multiplicative updates",
            "Non negatives",
            "Optimality criteria",
            "Variational bayes"
        ],
        "doc_abstract": "NMF is a Blind Source Separation technique decomposing multivariate non-negative data sets into meaningful non-negative basis components and non-negative weights. In its canonical form an NMF algorithm was proposed by Lee and Seung (1999) [31] employing multiplicative update rules. In this study we show how the latter follow from a new variational Bayes NMF algorithm VBNMF employing a Gaussian noise kernel. © 2014 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271524 291210 291718 291872 291874 31 Pattern Recognition Letters PATTERNRECOGNITIONLETTERS 2014-04-30 2014-04-30 2014-06-02T16:51:58 S0167-8655(14)00131-7 S0167865514001317 10.1016/j.patrec.2014.04.013 S300 S300.1 FULL-TEXT 2015-05-14T05:08:13.307592-04:00 0 0 20140801 2014 2014-04-30T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref alllist content subj ssids 0167-8655 01678655 true 45 45 C Volume 45 34 251 256 251 256 20140801 1 August 2014 2014-08-01 2014 article sco Copyright © 2014 Elsevier B.V. All rights reserved. ABAYESIANAPPROACHLEESEUNGUPDATERULESFORNMF SCHACHTNER R 1 Introduction 2 Bayesian approaches to NMF 3 Variational Bayes NMF 3.1 Variational Bayes for Gaussian NMF 3.2 The VBNMF update rules 3.3 Relation to the Lee–Seung method 3.4 VBNMF for real world data 4 Conclusion Acknowledgment References ARNGREN 2010 M ATTIAS 2000 209 215 H ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS AVARIATIONALBAYESIANFRAMEWORKFORGRAPHICALMODELS BEAL 2004 1 44 M 2001 COMPUTATIONALINFORMATIONRETRIEVAL BERRY 2006 155 173 M ALGORITHMSAPPLICATIONSFORAPPROXIMATENONNEGATIVEMATRIXFACTORIZATION BISHOP 1996 C NEURALNETWORKSFORPATTERNRECOGNITION BISHOP 1999 382 388 C ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS BAYESIANPCA BISHOP 1999 509 514 C PROCEEDINGSNINTHINTERNATIONALCONFERENCEARTIFICIALNEURALNETWORKS VARIATIONALPRINCIPALCOMPONENTS CEMGIL 2009 17 A CHAGOYEN 2006 41 M CICHOCKI 2006 32 39 A LNCS CSISZARSDIVERGENCESFORNONNEGATIVEMATRIXFACTORIZATIONFAMILYNEWALGORITHMS CICHOCKI 2007 793 802 A LNCS REGULARIZEDALTERNATINGLEASTSQUARESALGORITHMSFORNONNEGATIVEMATRIXTENSORFACTORIZATION CICHOCKI 2009 A NONNEGATIVEMATRIXTENSORFACTORIZATIONSAPPLICATIONSEXPLORATORYMULTIWAYDATAANALYSIS DEVARAJAN 2008 e1000029 K DHILLON 2005 283 290 I NEURALINFORMATIONPROCESSINGSYSTEMS GENERALIZEDNONNEGATIVEMATRIXAPPROXIMATIONSBREGMANDIVERGENCES FEVOTTE 2009 C PROCEEDINGS17THEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO09 NONNEGATIVEMATRIXFACTORISATIONSPROBABILISTICINFERENCEINCOMPOSITEMODELS GHAHRAMANI 2004 72 112 Z ADVANCEDLECTURESMACHINELEARNING UNSUPERVISEDLEARNING GHAHRAMANI 2001 507 513 Z ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS PROPAGATIONALGORITHMSFORVARIATIONALBAYESIANLEARNING GRIFFITHS 2011 1185 1224 T HARVA 2007 509 527 M HOFFMAN 2010 M PROCEEDINGSINTERNATIONALCONFERENCEMACHINELEARNINGICML BAYESIANNONPARAMETRICMATRIXFACTORIZATIONFORRECORDEDMUSIC HOYER 2004 P JORDAN 1998 183 233 M MACHINELEARNING INTRODUCTIONVARIATIONALMETHODSFORGRAPHICALMETHODS KABAN 2008 2291 2308 A LEE 1999 788 791 D LEE 2001 556 562 D LIN 2007 1589 1596 C MACKAY 1992 415 447 D MACKAY 2003 D INFORMATIONTHEORYINFERENCELEARNINGALGORITHMS PAATERO 1997 23 35 P PAATERO 1994 111 126 P PLUMBLEY 2003 534 543 M PLUMBLEY 2004 66 76 M RICHARDSON 1972 55 59 H SAUL 2002 897 904 L SCHACHTNER 2008 1688 1697 R SCHACHTNER 2011 528 534 R SCHACHTNER 2009 106 113 R ICA09PROCEEDINGS8THINTERNATIONALCONFERENCEINDEPENDENTCOMPONENTANALYSISSIGNALSEPARATION MINIMUMDETERMINANTCONSTRAINTFORNONNEGATIVEMATRIXFACTORIZATION SCHMIDT 2008 10 M SCHMIDT 2009 540 547 M INTERNATIONALCONFERENCEINDEPENDENTCOMPONENTANALYSISSIGNALSEPARATION BAYESIANNONNEGATIVEMATRIXFACTORIZATION STADLTHANNER 2008 2356 2376 K VAVASIS 2009 1364 1377 S ZHONG 2009 663 670 M SCHACHTNERX2014X251 SCHACHTNERX2014X251X256 SCHACHTNERX2014X251XR SCHACHTNERX2014X251X256XR item S0167-8655(14)00131-7 S0167865514001317 10.1016/j.patrec.2014.04.013 271524 2014-06-02T17:22:41.205887-04:00 2014-08-01 true 807915 MAIN 6 57138 849 656 IMAGE-WEB-PDF 1 si142 2105 21 235 si129 2317 55 137 si128 2591 45 158 si127 2985 60 162 si125 2263 56 120 si119 2013 48 112 si105 2305 44 164 si104 2391 44 171 si101 3479 58 284 si100 4565 62 259 si99 3389 59 279 si98 4619 62 268 si97 2165 26 199 si96 2305 24 216 si95 1507 23 159 si94 2683 44 226 si93 2535 42 228 si92 2581 42 220 si91 4613 54 376 si89 2125 24 201 si86 2323 24 224 si84 7773 112 372 si82 6027 89 276 si78 2435 44 183 si77 3063 44 246 si76 3383 32 302 si73 4193 43 346 si60 15815 210 504 si57 14143 155 570 si55 3169 38 269 si54 3247 38 272 si44 9021 94 440 si41 3491 39 309 si40 3565 39 306 si37 3173 18 321 si30 5109 54 403 si25 2555 18 322 si15 3307 47 279 si90 1051 21 81 si9 451 14 25 si88 451 14 25 si87 561 15 30 si85 243 12 19 si83 215 11 17 si81 277 16 22 si80 446 17 52 si8 373 19 39 si79 394 19 44 si75 429 19 47 si74 238 18 18 si72 215 11 17 si71 257 17 19 si70 624 17 98 si7 332 15 36 si69 215 11 17 si68 277 16 22 si67 429 19 47 si66 446 17 52 si65 257 17 19 si64 408 17 51 si63 257 17 19 si62 453 19 74 si61 277 16 22 si6 289 16 32 si59 944 33 139 si58 944 33 139 si56 276 17 22 si53 535 17 75 si52 167 12 14 si51 243 12 19 si50 847 17 158 si5 328 20 35 si49 269 19 27 si48 846 19 151 si47 488 17 62 si46 408 17 51 si45 276 17 22 si43 288 14 27 si42 274 18 22 si4 289 16 32 si39 606 17 93 si38 330 16 44 si36 274 18 22 si35 285 17 24 si34 215 11 17 si33 242 17 17 si32 244 17 19 si31 641 21 122 si3 311 13 53 si29 244 17 19 si28 249 18 18 si27 192 12 12 si26 242 17 17 si24 167 12 14 si23 422 17 58 si22 243 12 19 si21 399 17 55 si20 206 12 14 si2 277 13 44 si19 427 17 59 si18 192 12 12 si17 241 14 19 si16 569 17 113 si141 302 15 30 si140 442 20 52 si14 241 14 19 si139 253 14 26 si138 311 13 53 si137 564 20 86 si136 488 19 79 si135 167 12 14 si134 243 12 19 si133 206 12 14 si132 277 13 44 si131 257 17 19 si130 288 14 27 si13 167 12 14 si126 277 16 22 si124 301 18 28 si123 215 11 17 si122 408 16 68 si121 285 16 27 si120 394 19 44 si12 243 12 19 si118 277 16 22 si117 445 20 48 si116 896 22 153 si115 214 14 16 si114 257 17 19 si113 215 11 17 si112 282 14 29 si111 304 16 30 si110 437 18 68 si11 206 12 14 si109 277 16 22 si108 215 11 17 si107 737 18 154 si106 742 16 163 si103 442 16 72 si102 470 16 79 si10 302 15 30 si1 220 13 13 gr1 45828 721 949 gr2 262856 1842 1894 gr1 19141 271 357 gr2 95791 693 713 gr1 3690 164 216 gr2 8111 163 168 PATREC 6002 S0167-8655(14)00131-7 10.1016/j.patrec.2014.04.013 Elsevier B.V. Fig. 1 The lower bound B of the log-evidence as function of the number of elementary components K of the real world data set exhibits a maximum at K = 5 . Fig. 2 Automatic relevance determination by VBNMF shown in an real data example where K = 15 top,left: basic patterns H ALS gained by approximate maximum likelihood top,right: expected basic patterns 〈 H 〉 Q gained by VBNMF, initialized by H ALS . bottom,left: histograms of weights W ALS bottom,right: histograms of expected weights 〈 W 〉 Q Only relevant components are retained while the others are switched off by setting the respective basic pattern H k ∗ to a constant value and the distribution of the related weights W ∗ k to a narrow Gaussian peak. ☆ This paper has been recommended for acceptance by L. Heutte. A Bayesian approach to the Lee–Seung update rules for NMF R. Schachtner a b 1 G. Poeppel b A.M. Tomé c E.W. Lang a ⁎ a CIML, Biophysics, University of Regensburg, D-93040 Regensburg, Germany CIML, Biophysics University of Regensburg D-93040 Regensburg Germany b Infineon Technologies AG, 93049 Regensburg, Germany Infineon Technologies AG 93049 Regensburg Germany c IEETA, DETI, Universidade de Aveiro, P-3510 Aveiro, Portugal IEETA DETI Universidade de Aveiro P-3510 Aveiro Portugal ⁎ Corresponding author. Tel.: +49 9419432599; fax: +49 9419432479. 1 Tel.: +49 9419432599; fax: +49 9419432479. NMF is a Blind Source Separation technique decomposing multivariate non-negative data sets into meaningful non-negative basis components and non-negative weights. In its canonical form an NMF algorithm was proposed by Lee and Seung (1999) [31] employing multiplicative update rules. In this study we show how the latter follow from a new variational Bayes NMF algorithm VBNMF employing a Gaussian noise kernel. Keywords Variational Bayes NMF Bayesian optimality criteria Lee–Seung update rules 1 Introduction Decomposing any given data set into nonnegative factors has received wide-spread interest within the machine learning community in recent years [14]. The idea first appeared as positive matrix factorization in [40,39] and became popular as non-negative matrix factorization (NMF) through the seminal paper by Lee and Seung [31]. They introduced NMF as an unsupervised, parts-based learning paradigm involving the approximative decomposition of a nonnegative matrix X into a product of two nonnegative matrices, W and H , via a multiplicative update algorithm. Similar objectives were pursued also by nonnegative independent component analysis [41,42] or rectified factor analysis [23]. All these data analysis tools account for the fact that many physical measurements yield results with exclusively non-negative quantities which can be approximated by a non-subtractive superposition of exclusively non-negative underlying features which explain the systematic structure of the data set. Also for physical reasons the superimposed components cannot partially compensate each other. Hence decomposing any measurements into a parts-based representation seems more natural than other constraints often invoked in exploratory matrix factorization procedures which would allow for partial compensation of the components upon their superposition. NMF has seen numerous applications in recent years [14,6,55] where it has been primarily applied in an unsupervised setting in image and natural language processing [33,45,11] and sparse coding [26,56]. Further applications include text mining [5] and music transcription [54]. More recently, it has been successfully utilized also in a variety of applications in computational biology [15], especially in molecular pattern discovery [46]. Basically, NMF can be formulated as a minimization problem. A suitable cost function such as the quadratic error function D E (1) D E ( X , WH ) = 1 2 ∑ i ∑ j X ij - [ W H ] ij 2 comes to be minimized subject to the constraint that either factor matrix has only non-negative entries, i.e. W ⩾ 0 , H ⩾ 0 . The cost function D E quantifies the reconstruction error E between an ( N × M ) -dimensional non-negative data matrix X and the product of an ( N × K ) -dimensional weight matrix W and a ( K × M ) -dimensional matrix H of hidden features or sources according to the following data model: (2) X = WH + E subject to W ⩾ 0 , H ⩾ 0 Vavasis [58] recently proofed that this problem is NP-hard. Arora et al. [2] recently discussed conditions when the NMF problem can be solved in polynomial time. Without the nonnegativity constraint, singular value decomposition (SVD) provides an optimal solution to the factorization problem. The cost function (1) is just the most popular choice for NMF as it is based on the Frobenius norm of the reconstruction error of the data matrix factorization as a proper distance measure. Other popular cost functions are based on information theoretic concepts like the generalized Kullback–Leibler divergence, the Itakura–Saito divergence or similar divergences [12,16] (see [14] for a recent review). Positivity constraints are, however, rarely sufficient to extract the underlying features uniquely. Hence, further constraints based on sparseness considerations [25,56,21] or minimum volume requirements [50,1] have been considered in addition. Such regularizing constraints have been added to enforce certain characteristics of the solutions, or to impose prior knowledge about the application considered. Also various optimization techniques and sampling procedures such as Expectation–Maximization (EM), nonparametric Bayesian approaches [24] and Markov Chain Monte Carlo (MCMC) or variational Bayes (VB) [17], based on the Indian Buffet Process factor analysis model [29,22] have been presented since. In the following we sketch a full variational Bayes approach to NMF [47] and show how the famous Lee–Seung multiplicative update rules appear as limiting cases of such an approach. The rest of this paper is organized as follows: In Section 2 we present a short overview of Bayesian approaches to NMF. Next, in Section 3 we present a new variational Bayesian NMF (VBNMF) approach and discuss its relation to the canonical Lee–Seung multiplicative update rules. 2 Bayesian approaches to NMF While NMF was introduced in terms of optimizing a suitable cost function subject to non-negativity constraints, it is well-known that many popular NMF cost functions can be related to statistical models of the reconstruction error of the decomposition via Maximum Likelihood (ML) estimations. For example, the squared Euclidean distance measure is based on Gaussian error statistics, while Kullback–Leibler (KL) – or Itakura–Saito (IS) – divergences [12,16,14] as cost functions relate to alternative error statistics given by Poisson or Gamma distributed noise kernels. Hence, constrained optimization of proper cost functions can be interpreted within a statistical perspective as maximum likelihood estimation problems (see e.g. [44,12,51,18]). This opens the field to a conceptually more principled approach based on Bayesian probabilistic interpretations of NMF. It is not accidental that the Richardson–Lucy algorithm, first presented in 1972 [43], is one of the forefathers of modern NMF algorithms [14]. Maximum likelihood estimation is based on adequate reconstruction error statistics. For example, assuming the entries E ij of the reconstruction error matrix E in Eq. (2) to be independently and identically distributed (i. i. d.) according to a Gaussian distribution with zero mean and variance σ r 2 , the joint distribution of all data items X ij factorizes according to (3) P ( X | W , H ) = ∏ i ∏ j 1 2 π σ r exp - 1 2 X ij - [ WH ] ij σ r 2 Note that Eq. (2) also implies the assumption of additive i.i.d. Gaussian noise. Hence, Eq. (2) is only an approximation, since the left hand side of X ij - E ij = [ WH ] ij could be negative in principle. Since the right hand side of this equation is non-negative by definition, and the observed data is also non-negative, in general, the noise is not independent from the data and small observations X ij are related to low noise levels E ij in practice. Since in the following we will consider situations only where the noise parameter σ r is sufficiently small, this subtlety will not be pursued further here. In addition to specifying proper cost functions according to data statistics, suitable prior distributions can be used to integrate existing knowledge about the data and enforce desired characteristics of the solutions. For example, non-negative sparse coding [25] actually is a maximum a posteriori (MAP) estimation, assuming independent exponential prior distributions of the weights W ij and flat priors on the features H kj . Several papers suggest Bayesian techniques to explicitly incorporate prior knowledge on the factor matrices in NMF, including independent Gamma priors [38], Gaussian process priors [51], or Gamma chain priors [59] for audio signal modeling. Also various volume priors have been discussed in the context of volume constrained spectral unmixing methods to favor solutions which span a minimum volume of the simplex formed by the data (see [1,49] for short discussions). The examples mentioned above seek to model the characteristics of desired solutions by specific shapes or properties of the prior distributions. Contrasting such approaches, we will show how Bayesian techniques can also cope with cases where additional information on the properties of the factor matrices is missing, and derive a Bayesian optimality criterion for NMF solutions. In general, Bayesian probability theory offers a principled way to handle uncertainty and incorporate prior knowledge to source separation problems [30]. Bayesian techniques are not only suitable for determining optimal parameter settings in source separation problems, they also provide a framework for model comparison [37]. In NMF settings, different models can be identified with different numbers of underlying sources K. Assuming a model K, the posterior distribution of parameters times the evidence which the data provides for model K equals the likelihood multiplied by the prior according to Bayes’ rule [37,7] (4) P ( W , H | X , K ) P ( X | K ) = P ( X | W , H ) P ( W , H | K ) By means of Eq. (4) the relevant quantities can be transformed into each other. In a first level of Bayesian inference, the most probable set of parameters W , H is estimated by maximizing P ( W , H | X , K ) within a model with a fixed number K of relevant features. A second level of inference then compares alternative models with each other by evaluating the evidence for model K (5) P ( X | K ) = ∬ P ( X | W , H ) P ( W , H | K ) d W d H for different values of K. In Section 3 we will interpret different models by different factorization ranks K and see how Bayesian inference can help to determine the optimal number of sources in a given data set under specific prior assumptions. Finally, full Bayesian approaches to NMF including the task of model order selection have been studied also recently. Approaches to solve the model order selection problem of NMF are closely related to automatic relevance detection (ARD) schemes which have been first mentioned in [36] and have been put forward by [8] in a Bayesian PCA approach. While [10] discusses the Poisson likelihood case, [53] demonstrates Gibbs sampling schemes in connection with Chib’s method for the Gaussian likelihood case. The latter group [1] also discusses a Bayesian NMF approach with a volume prior and an inference scheme based on Gibbs sampling. Furthermore, [60,52] use two different reversible jump MCMC approaches for model order selection in the Gaussian likelihood case. In [18], the use of Markov Chain Monte Carlo (MCMC) and variational Bayes methods is sketched to perform Bayesian inference on NMF problems using the Frobenius norm, the KL-divergence and the Itakura–Saito divergence as cost functions. Bayesian approaches to model order selection for NMF have further been discussed in [57] in a MAP estimation framework to find the number K of meaningful components into which to decompose the given data matrix. 3 Variational Bayes NMF Following, we discuss a variational approximation of the log-evidence (6) ln P ( X ) = ln ∬ P ( X | W , H ) P ( W , H ) d W d H integrating over all H kj as well as over all W ik . Variational methods have been extensively used in the field of Bayesian learning [27,9,3,20,23,28]. Before we proceed, we note that by Jensen’s inequality (7) ln P ( X ) ⩾ ∬ Q ( W , H ) ln P ( X | W , H ) P ( W , H ) Q ( W , H ) d W d H = ln P ( X | W , H ) Q ( W , H ) + ln P ( W , H ) Q ( W , H ) Q ( W , H ) ≔ B Q a lower bound B Q on ln P ( X ) is induced where Q ( W , H ) is an auxiliary variational distribution obeying the normalization constraint ∫ Q ( W , H ) d W d H = 1 , and where 〈 · 〉 Q denotes the expectation w.r.t. the distribution Q. It can be shown [19,4] that equality holds, if Q ( W , H ) = P ( W , H | X ) where the latter represents the posterior distribution of the parameters W and H . However, computing the latter distribution is not feasible, in general, due to the high-dimensional integral involved which is intractable. One way around this difficulty is provided by sampling procedures as mentioned in [10,53] who apply Gibbs sampling to draw samples from the true posterior P ( W , H | X ) . Here, instead, we follow a variational approach, which is similar to the one proposed in [10], except that we invoke a Gaussian rather than a Poisson noise kernel. In fact, this approach forms a variant of the Variational Bayesian EM Algorithm proposed in [3] and further discussed in [20,4]. 3.1 Variational Bayes for Gaussian NMF Under the assumption of a Gaussian noise kernel, and further assuming completely factorized auxiliary variational distributions (8) Q ( W , H ) = ∏ i ∏ k Q ( W ik ) ∏ k ∏ j Q ( H kj ) (9) P ( W , H ) = ∏ i ∏ k P ( W ik ) ∏ k ∏ j P ( H kj ) , the lower bound B Q to the log-evidence takes the form (10) B Q = - NM 2 ln ( 2 π ) - NM ln σ r - 1 2 σ r 2 ∑ i ∑ j X ij 2 - 2 X ij ∑ k W ik Q ( W ik ) H kj Q ( H kj ) + ( [ WH ] ij ) 2 Q ( W ) , Q ( H ) + ∑ i ∑ k log P ( W ik ) Q ( W ik ) Q ( W ik ) + ∑ k ∑ j log P ( H kj ) Q ( H kj ) Q ( H kj ) Note that the quadratic term ( [ WH ] ij ) 2 Q ( W ) , Q ( H ) can be expressed in terms of first and second order expectations. However, applying Jensen’s inequality once more allows us to directly derive a Bayesian extension of the famous Lee–Seung multiplicative update equations. Utilizing the convexity of a quadratic function leads to a decoupling of the joint ensemble average ( [ WH ] ij ) 2 Q ( W ) , Q ( H ) into separate averages yielding a new lower bound (11) B Q ⩾ B q ≔ - NM 2 ln ( 2 π ) - NM ln σ r - 1 2 σ r 2 ∑ i ∑ j X ij 2 - 2 X ij ∑ k W ik Q ( W ik ) H kj Q ( H kj ) + ∑ k 1 q ikj ( W ik ) 2 Q ( W ik ) ( H kj ) 2 Q ( H kj ) + ∑ i ∑ k log P ( W ik ) Q ( W ik ) Q ( W ik ) + ∑ k ∑ j log P ( H kj ) Q ( H kj ) Q ( H kj ) The price to pay for the decoupling approximation comprises additional parameters q ikj which follow the closure relation ∑ k q ikj = 1 for every observation ij. B q is always a lower bound to the log-evidence and we gain the best approximation to ln P ( X ) if we maximize B q w.r.t. all auxiliary variational distributions Q ( W ik ) and Q ( H kj ) as well as the decoupling parameters q ikj . A strict Bayesian approach would require assigning prior distributions to the parameters σ r as well and integrating them out. Following the discussion in [35,36], we assume that the distribution P ( σ r | X , W , H ) is sharply peaked at the maximum and maximize B q w.r.t. σ r instead of performing an additional integration. Optimal variational densities can be obtained by solving the fixed point equation (12) 0 = ! ∂ ∂ Q ( H kj ) B q - λ kj ∫ Q ( H kj ) dQ ( H kj ) - 1 where λ kj is a Lagrange parameter ensuring that Q ( H kj ) is a density. Straightforward computation yields (13) Q ( H kj ) ∝ P ( H kj ) exp α kj H ( H kj ) 2 + β kj H H kj where (14) α kj H = - 1 2 σ r 2 ∑ i 1 q ikj ( W ik ) 2 Q ( W ik ) (15) β kj H = 1 σ r 2 ∑ i X ij W ik Q ( W ik ) Note that this relation holds irrespective of the type of priors P ( H kj ) involved. Corresponding optimal densities Q ( W ik ) can be computed in complete analogy to the calculation given above. The decoupling parameters q ikj render the bound as tight as possible by setting (16) q ikj = ( W ik ) 2 Q ( W ik ) ( H kj ) 2 Q ( H kj ) ∑ l ( W il ) 2 Q ( W il ) ( H lj ) 2 Q ( H lj ) Finally, the optimal noise parameter σ r is obtained as (17) σ r 2 = 1 NM ∑ i ∑ j X ij 2 - 2 X ij ∑ k W ik Q ( W ik ) H kj Q ( H kj ) + ∑ k 1 q ikj ( W ik ) 2 Q ( W ik ) ( H kj ) 2 Q ( H kj ) 3.2 The VBNMF update rules Following we choose a rectified Gaussian prior distribution for the weights W according to (18) P ( W ik ) = N + ( W ik | μ W 0 k , σ W 0 k ) . One such distribution is assumed for each column W ∗ k and expresses our “belief” that the a priori weights of source k are drawn from the same distribution. Similarly, we choose the prior distribution on each row H k ∗ to be (19) P ( H kj ) = N + ( H kj | μ H k 0 , σ H k 0 ) A truncated Gaussian distribution N + ( θ | μ , σ ) [23] derives from a Gaussian distribution by truncating all negative entries and re-normalizing the integral to unity. It is given by (20) N + ( θ | μ , σ ) ≕ 1 Z N + exp - 1 2 θ - μ σ 2 , 0 ⩽ θ < ∞ 0 otherwise The normalizing constant is given by (21) Z N + = 1 2 σ 2 π erfc - μ σ 2 where (22) erfc ( z ) = : 2 π ∫ z ∞ exp ( - t 2 ) dt represents the complementary error function. The expectations required for our purpose are then given by (23) 〈 θ 〉 N + = μ + σ 2 Z N + exp - μ 2 2 σ 2 (24) 〈 θ 2 〉 N + = σ 2 + μ 〈 θ 〉 N + Due to the conjugation principle, the corresponding auxiliary variational distributions are obtained as (25) Q ( W ik ) = N + ( W ik | μ W ik , σ W ik ) and (26) Q ( H kj ) = N + ( H kj | μ H kj , σ H kj ) where (27) μ W ik = μ W 0 k σ W k 0 - 2 + ( σ r ) - 2 ∑ j X ij H kj σ W 0 k - 2 + ( σ r ) - 2 ∑ j q ikj - 1 H kj 2 (28) σ W ik = σ W 0 k - 2 - ( σ r ) - 2 ∑ j q ikj - 1 〈 H kj 2 〉 - 1 2 and (29) μ H kj = μ H k 0 σ H k 0 - 2 + ( σ r ) - 2 ∑ i X ij W ik σ H k 0 - 2 + ( σ r ) - 2 ∑ i q ikj - 1 W ik 2 (30) σ H kj = σ H k 0 - 2 - ( σ r ) - 2 ∑ i q ikj - 1 W ik 2 - 1 2 If we have no knowledge on the actual values of the parameters of the priors, these hyperparameters μ W 0 k , σ W 0 k and μ H k 0 , σ H k 0 can be updated by iteratively solving the corresponding fixed point equations: (31) ∂ B q ∂ μ W 0 k = ! 0 ∂ B q ∂ σ W 0 k = ! 0 (32) ∂ B q ∂ μ H k 0 = ! 0 ∂ B q ∂ σ H k 0 = ! 0 The overall VBNMF algorithm, as proposed above, can be summarized as follows: Variational Bayes NMF algorithm • set hyperparameters μ H k 0 , σ H k 0 , μ W 0 k , σ W 0 k • initialize μ H kj , σ H kj , μ W ik , σ W ik and σ r • Repeat 1. compute all q ikj (Eq. (4)) 2. update all μ H kj , σ H kj (29) and (30) 3. update all μ W ik and σ W ik (27) and (28) 4. update σ r (17) 5. update hyperparameters (31) and (32) 6. compute B q (Eq. (4)) until convergence Steps 4–5 are optional if no prior knowledge exists, step 6 can be performed only occasionally testing the state of convergence of the algorithm. Since the latter represents a generalized Expectation–Maximization (EM) algorithm [3,20,4], convergence is assured. 3.3 Relation to the Lee–Seung method It is interesting to note that the VBNMF algorithm sketched above is a straight forward Bayesian generalization of the multiplicative NMF algorithm proposed by Lee and Seung [32,34] in case of an Euclidean distance measure ( L 2 -norm) for the reconstruction error corresponding to a Gaussian noise kernel. To see this, we derive a MAP version of the VBNMF algorithm as follows: Replacing the expectations of the form W ik Q ( W ik ) , 〈 H kj 〉 Q ( H kj ) by their MAP estimates W ik ∗ H kj ∗ , the update equation for the decoupling parameters q ikj Eq. (16) reduces to (33) q ikj = W ik ∗ H kj ∗ ∑ l W il ∗ H lj ∗ A prior P ( H kj ) of the form (19) becomes a flat prior if it has a large spread parameter σ H kj , and in the limit σ H kj ↦ ∞ the first terms in the numerator and denominator of Eq. (29) tend towards zero. After further cancelation of σ r , the update rule for the mean parameter μ H kj reduces to (34) μ H kj = ∑ i X ij W ik ∗ ∑ i ( W ik ∗ ) 2 q ikj Plugging in the q ikj from Eq. (33) finally yields (35) μ H kj = ∑ i X ij W ik ∗ ∑ i ( W ik ∗ ) 2 ∑ l W il ∗ H lj ∗ W ik ∗ H kj ∗ (36) = H kj ∗ ∑ i X ij W ik ∗ ∑ i W ik ∗ ∑ l W il ∗ H lj ∗ (37) = H kj ∗ [ W ∗ T X ] kj [ W ∗ T W ∗ H ∗ ] kj The last expression is exactly one of the update equations proposed by Lee and Seung [32]. Due to symmetry, the same can be shown for the weight parameters W ik . 3.4 VBNMF for real world data The performance of the VBNMF algorithm is tested on a real world data set collected during processing of 1000 semi-conductor wafers [48]. A plot of the lower bound B q to the log-evidence exhibits a maximum at K = 5 indicating five elementary patterns which underly the observations (see Fig. 1 ). The VBNMF algorithm explains the observations X with nonnegative factors W and H with concomitant rectified Gaussian priors. Fig. 2 compares maximum likelihood solutions W ALS , H ALS , obtained with an alternating least squares (ALS) algorithm [13], with VBNMF results 〈 W 〉 Q , 〈 H 〉 Q for K = 15 . While the ALS algorithm uses all 15 components H k ∗ to explain the data, the VBNMF only retains five components and renders all remaining components irrelevant. The histograms of the weights 〈 W ∗ k 〉 Q show that those corresponding to the irrelevant components resemble delta distributions, while the ones for the elementary components correspond to rectified Gaussians. Note that this behavior resembles automatic relevance determination [57]. All ALS maximum likelihood counterparts (Fig. 2, bottom left) show a clear peak at the origin of the distribution of the weights W ∗ k . This suggests a more flexible shape for the prior distributions, such as a superposition of a rectified Gaussian and a delta peak at the origin (38) P ( W ik ) = τ N + + ( 1 - τ ) δ ( W ik ) . The latter represents a limiting case of a Gaussian mixture model. 4 Conclusion In this investigation we discuss a novel variational Bayes NMF algorithm (VBNMF) which represents a Bayesian generalization of the famous Lee–Seung multiplicative NMF algorithm. The latter results from replacing the expectations of the elements of the factor matrices over the auxiliary variational distributions by their MAP estimates. Application to a real world data set, collected during semi-conductor wafer processing, demonstrates the ability of the VBNMF algorithm to solve the problem of model order selection. Other than an ALS-based maximum likelihood estimation, it can implement a mechanism similar to automatic relevance determination which is able to discover the dimension of the underlying feature space in which to represent the observations considered [48]. Acknowledgment R. S. has been supported by a scholarship provided by Infineon AG, Regensburg, which is gratefully acknowledged. References [1] M. Arngren M.N. Schmidt J. Larsen Unmixing of hyperspectral images using Bayesian nonnegative matrix factorization with volume prior J. Signal Process. Syst. January 2010 [2] S. Arora, R. Ge, R. Kannan, A. Moitra, Computing a nonnegative matrix factorization – provably, in: Proceeding STOC’12, Proceedings of the 44th Symposium on Theory of Computing, 2012, pp. 145–162, <arXiv:1111.0952v1[cs.DS]> 3. Nov. 2011. [3] H. Attias A variational Bayesian framework for graphical models Advances in Neural Information Processing Systems vol. 12 2000 MIT Press 209 215 [4] M.J. Beal Z. Ghahramani Variational Bayesian learning of directed graphical models with hidden variables Bayesian Anal. 2004 1 44 [5] M.W. Berry Computational Information Retrieval 2001 Society for Industrial and Applied Mathematics Philadelphia, PA, USA [6] M.W. Berry M. Browne A.N. Langville V.P. Pauca R.J. Plemmons Algorithms and applications for approximate nonnegative matrix factorization Computational Statistics and Data Analysis 2006 Elsevier 155 173 [7] C.M. Bishop Neural Networks for Pattern Recognition 1996 Oxford University Press [8] C.M. Bishop Bayesian pca Advances in Neural Information Processing Systems 1999 NIPS 382 388 [9] C.M. Bishop Variational principal components Proceedings Ninth International Conference on Artificial Neural Networks 1999 ICANN 509 514 [10] A.T. Cemgil Bayesian inference in non-negative matrix factorisation models Comput. Intel. Neurosci. 2009 2009 17 [11] M. Chagoyen P. Carmona-Saez H. Shatkay J.M. Carazo A. Pascual-Montano Discovering semantic features in the literature: a foundation for building functional associations BMC Bioinform. 7 2006 41 [12] A. Cichocki R. Zdunek S.-I. Amari Csiszar’s divergences for non-negative matrix factorization: family of new algorithms LNCS vol. 3889 2006 Springer 32 39 [13] A. Cichocki R. Zdunek Regularized alternating least squares algorithms for non-negative matrix–tensor factorization LNCS vol. 4493 2007 Springer 793 802 [14] A. Cichocki R. Zdunek A.-H. Phan S.-I. Amari Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis November 2009 John Wiley [15] K. Devarajan Nonnegative matrix factorization: an analytical and interpretive tool in computational biology PLoS Comput. Biol. 4 7 July 2008 e1000029 [16] I.S. Dhillon S. Sra Generalized nonnegative matrix approximations with Bregman divergences Neural Information Processing Systems 2005 MIT Press 283 290 [17] N. Ding, Y. Qi, R. Xiang, I. Molloy, N. Li, Nonparametric Bayesian matrix factorization by power-EP, in: Journal of Machine Learning Research – Proceedings Track, 2010, pp. 169–176. [18] C. Févotte A.T. Cemgil Nonnegative matrix factorisations as probabilistic inference in composite models Proceedings of the 17th European Signal Processing Conference (EUSIPCO’09) 2009 Glasgow [19] Z. Ghahramani Unsupervised learning Advanced Lectures on Machine Learning 2004 Springer-Verlag 72 112 [20] Z. Ghahramani M.J. Beal Propagation algorithms for variational Bayesian learning Advances in Neural Information Processing Systems 13 2001 MIT Press 507 513 [21] N. Gillis, Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing, <arXiv:1204.2436v1[stat.ML]> 11 April 2012. [22] Th.L. Griffiths Z. Ghahramani The Indian buffet process: an introduction and review J. Mach. Learn. Res. 12 2011 1185 1224 [23] M. Harva A. Kabán Variational learning for rectified factor analysis Signal Process. 87 3 2007 509 527 [24] M.D. Hoffman D.M. Blei P.R. Cook Bayesian nonparametric matrix factorization for recorded music Proceedings of the International Conference on Machine Learning (ICML) 2010 Haifa [25] P. Hoyer, Non-negative sparse coding, in: Proceedings of the IEEE Workshop on Neural Networks for Signal Processing, 2002, pp. 557–565. [26] P. Hoyer Non-negative matrix factorization with sparseness constraints J. Mach. Learn. Res. 5 2004 [27] M.I. Jordan Z. Ghahramani T.S. Jaakkola L.K. Saul An introduction to variational methods for graphical methods Machine Learning 1998 MIT Press 183 233 [28] A. Kabán E. Bingham Factorisation and denoising of 0–1 data: a variational approach Neurocomputing 71 10–12 2008 2291 2308 [29] D.A. Knowles, Z. Ghahramani, Infinite sparse factor analysis and infinite independent component analysis, in: 7th International Conference on Independent Component, Analysis, 2007. [30] K.H. Knuth, Informed source separation: a Bayesian tutorial, in: Proceedings of the 13th European Signal Processing Conference (EUSIPCO 2005), 2005. [31] D.D. Lee H.S. Seung Learning the parts of objects by non-negative matrix factorization Nature 401 6755 1999 788 791 [32] D.D. Lee H.S. Seung Algorithms for non-negative matrix factorization NIPS 13 2001 556 562 [33] S.Z. Li, X.W. Hou, H.J. Zhang, Q.S. Cheng, Learning spatially localized, parts-based representation, in: Proceedings of the 2001 IEEE Computer Society Conference, CVPR 2001, vol. 1, pp. I–207–I–212, April 2001. [34] C.-J. Lin On the convergence of multiplicative update algorithms for nonnegative matrix factorization IEEE Trans. Neural Netw. 18 6 2007 1589 1596 [35] D.J.C. MacKay Bayesian interpolation Neural Comput. 4 3 1992 415 447 [36] D.J.C. MacKay, Ensemble learning and evidence maximization, Technical report, Cavendish Laboratory, University of Cambridge, 1995. [37] D.J.C. MacKay Information Theory, Inference, and Learning Algorithms 2003 Cambridge University Press Available from: [38] S. Moussaoui, D. Brie, O. Caspary, A. Mohammad-Djafari, A Bayesian method for positive source separation, in: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, 2004, pp. 485–488. [39] P. Paatero Least squares formulation of robust non-negative factor analysis Chemom. Intel. Lab. Syst. 37 1997 23 35 [40] P. Paatero U. Tapper Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values Environmetrics 5 2 1994 111 126 [41] M. Plumbley Algorithms for nonnegative independent component analysis IEEE Trans. Neural Netw. 14 2003 534 543 [42] M. Plumbley E. Oja A nonnegative pca algorithm for independent component analysis IEEE Trans. Neural Netw. 15 2004 66 76 [43] H.W. Richardson Bayesian-based iterative method of image restoration J. Opt. Soc. Am. 62 1 January 1972 55 59 [44] P. Sajda, S. Du, T. Brown, L. Parra, R. Stoyanova, Recovery of constituent spectra in 3d chemical shift imaging using non-negative matrix factorization, in: 4th International Symposium on Independent Component Analysis and Blind, Signal Separation, 2003, pp. 71–76. [45] L.K. Saul D.D. Lee Multiplicative updates for classification by mixture models NIPS 14 2002 897 904 [46] R. Schachtner D. Lutter P. Knollmüller A.M. Tomé F.J. Theis G. Schmitz M. Stetter P. Gómez-Vilda E.W. Lang Knowledge-based gene expression classification via matrix factorization Bioinformatics 24 2008 1688 1697 [47] R. Schachtner, G. Pöppel, E.W. Lang. Bayesian extensions to non-negative matrix factorization, in: CIP2010: Proceedings of the 2nd International Workshop on Cognitive Information Processing on Elba Island, June 2010. [48] R. Schachtner, Extensions of non-negative matrix factorization and their application to the analysis of wafer test data. [49] R. Schachtner G. Pöppel E.W. Lang Towards unique solutions of non-negative matrix factorization problems by a determinant criterion Digital Signal Process. 2011 528 534 [50] R. Schachtner G. Pöppel A.M. Tomé E.W. Lang Minimum determinant constraint for non-negative matrix factorization ICA’09: Proceedings of the 8th International Conference on Independent Component Analysis and Signal Separation 2009 Springer 106 113 [51] M.N. Schmidt H. Laurberg Non-negative matrix factorization with Gaussian process priors Comput. Intel. Neurosci. 2008 2008 10 [52] M.N. Schmidt, M. Mrup, Infinite non-negative matrix factorization, in: European Signal Processing Conference (EUSIPCO), August 2010. [53] M.N. Schmidt O. Winther L.K. Hansen Bayesian non-negative matrix factorization International Conference on Independent Component Analysis and Signal Separation Lecture Notes in Computer Science (LNCS) vol. 5441 2009 Springer 540 547 [54] P. Smaragdis, J.C. Brown, Non-negative matrix factorization for polyphonic music transcription, in: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2003, pp. 177–180. [55] S. Sra, I.S. Dhillon, Nonnegative matrix approximation: algorithms and applications, Technical report, UTCS, Report TR-06-27, 2006. [56] K. Stadlthanner F.J. Theis A.M. Tomé C.G. Puntonet J.M. Górriz E.W. Lang Hybridizing sparse component analysis with genetic algorithms for microarray analysis Neurocomputing 71 10–12 2008 2356 2376 [57] V.Y.F. Tan, C. Fevotte, Automatic relevance determination in nonnegative matrix factorization, in: Proceedings of the Workshop on Signal Processing with Adaptative Sparse Structured, Representations (SPARS’09), 2009. [58] S. Vavasis On the complexity of nonnegative matrix factorization SIAM J. Optim. 2009 1364 1377 [59] T.O. Virtanen, A.T. Cemgil, S.J. Godsill, Bayesian extensions to nonnegative matrix factorisation for audio signal modelling, in: Proceedings of IEEE ICASSP 08, 2008. [60] M. Zhong M. Girolami Reversible jump mcmc for non-negative matrix factorization J. Mach. Learn. Res. 5 2009 663 670 "
    },
    {
        "doc_title": "EEG study on affective valence elicited by novel and familiar pictures using ERD/ERS and SVM-RFE",
        "doc_scopus_id": "84896701469",
        "doc_doi": "10.1007/s11517-013-1126-6",
        "doc_eid": "2-s2.0-84896701469",
        "doc_date": "2014-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Affective valence",
            "Emotional response",
            "ERD/ERS",
            "Event related desynchronization",
            "Eventrelated potential (ERPs)",
            "Habituation",
            "SVM-RFE",
            "Time and frequency domains"
        ],
        "doc_abstract": "EEG signals have been widely explored in emotional processing analyses, both in time and frequency domains. However, in such studies, habituation phenomenon is barely considered in the discrimination of different emotional responses. In this work, spectral features of the event-related potentials (ERPs) are studied by means of event-related desynchronization/synchronization computation. In order to determine the most relevant ERP features for distinguishing how positive and negative affective valences are processed within the brain, support vector machine-recursive feature elimination is employed. The proposed approach was applied for investigating in which way the familiarity of stimuli affects the affective valence processing as well as which frequency bands and scalp regions are more involved in this process. In a group composed of young adult women, results prove that parietooccipital region and theta band are especially involved in the processing of novelty in emotional stimuli. Furthermore, the proposed method has shown to perform successfully using a moderated number of trials. © 2013 International Federation for Medical and Biological Engineering.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An interactive open-ended learning approach for 3D object recognition",
        "doc_scopus_id": "84905034271",
        "doc_doi": "10.1109/ICARSC.2014.6849761",
        "doc_eid": "2-s2.0-84905034271",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "3d object recognition",
            "Descriptors",
            "High level knowledge",
            "Leave-one-out cross validations",
            "Nearest-neighbor classifications",
            "open-ended learning",
            "Precision and recall",
            "Three-dimensional object"
        ],
        "doc_abstract": "Three-dimensional object detection and recognition is increasingly in manipulation and navigation applications in autonomous service robots. It involves clustering points of the point cloud from an unstructured scene into objects candidates and estimating features to recognize the objects under different circumstances such as occlusions and clutter. This paper presents an efficient approach capable of learning and recognizing object categories in an interactive and open-ended manner. In this paper, 'open-ended' implies that the set of object categories to be learned is not known in advance. The training instances are extracted from actual experiences of a robot, and thus become gradually available, rather than being available at the beginning of the learning process. This paper focuses on two state-of-the-art questions: (1) How to automatically detect, conceptualize and recognize objects in 3D unstructured scenes in an open-ended manner? (2) How to acquire and utilize high-level knowledge obtained from the user (e.g. category label) to improve the system performance? This approach starts with a pre-processing phase to remove unnecessary information and prepare a suitable point cloud. Clustering is then applied to detect object candidates. Subsequently, all object candidates are described based on a 3D shape descriptor called spin-image. Finally, a nearest-neighbor classification rule is used to assign category labels to the detected objects. To examine the performance of the proposed approach, a leave-one-out cross validation algorithm is utilized to compute precision and recall. The experimental results show the fulfilling performance of this approach on different types of objects. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Grounding language in perception for scene conceptualization in autonomous robots",
        "doc_scopus_id": "84904861654",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84904861654",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Cognitive architectures",
            "Grounding language",
            "Human supervision",
            "Learning frameworks",
            "Linguistic descriptions",
            "Natural languages",
            "Spatial structure",
            "Textual description"
        ],
        "doc_abstract": "In order to behave autonomously, it is desirable for robots to have the ability to use human supervision and learn from different input sources (perception, gestures, verbal and textual descriptions etc). In many machine learning tasks, the supervision is directed specifically towards machines and hence is straight forward clearly annotated examples. But this is not always very practical and recently it was found that the most preferred interface to robots is natural language. Also the supervision might only be available in a rather indirect form, which may be vague and incomplete. This is frequently the case when humans teach other humans since they may assume a particular context and existing world knowledge. We explore this idea here in the setting of conceptualizing objects and scene layouts. Initially the robot undergoes training from a human in recognizing some objects in the world and armed with this acquired knowledge it sets out in the world to explore and learn more higher level concepts like static scene layouts and environment activities. Here it has to exploit its learned knowledge and ground language into perception to use inputs from different sources that might have overlapping as well as novel information. When exploring, we assume that the robot is given visual input, without explicit type labels for objects, and also that it has access to more or less generic linguistic descriptions of scene layout. Thus our task here is to learn the spatial structure of a scene layout and simultaneously visual object models it was not trained on. In this paper, we present a cognitive architecture and learning framework for robot learning through natural human supervision and using multiple input sources by grounding language in perception. Copyright © 2014, Association for the Advancement of Artificial Intelligence. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Bidimensional ensemble empirical mode decomposition of functional biomedical images taken during a contour integration task",
        "doc_scopus_id": "84902172144",
        "doc_doi": "10.1016/j.bspc.2014.04.011",
        "doc_eid": "2-s2.0-84902172144",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Classification performance",
            "Cognitive neurosciences",
            "Discriminative power",
            "Ensemble empirical mode decomposition",
            "Functional magnetic resonance images (fMRI)",
            "General linear modeling",
            "Intrinsic Mode functions",
            "Statistical parametric mapping"
        ],
        "doc_abstract": "In cognitive neuroscience, extracting characteristic textures and features from functional imaging modalities which could be useful in identifying particular cognitive states across different conditions is still an important field of study. This paper explores the potential of two-dimensional ensemble empirical mode decomposition (2DEEMD) to extract such textures, so-called bidimensional intrinsic mode functions (BIMFs), of functional biomedical images, especially functional magnetic resonance images (fMRI) taken while performing a contour integration task. To identify most informative textures, i.e. BIMFs, a support vector machine (SVM) as well as a random forest (RF) classifier is trained for two different stimulus/response conditions. Classification performance is used to estimate the discriminative power of extracted BIMFs. The latter are then analyzed according to their spatial distribution of brain activations related with contour integration. Results distinctly show the participation of frontal brain areas in contour integration. Employing features generated from textures represented by BIMFs exhibit superior classification performance when compared with a canonical general linear model (GLM) analysis employing statistical parametric mapping (SPM). © 2014 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 273545 291210 291874 291880 291883 31 Biomedical Signal Processing and Control BIOMEDICALSIGNALPROCESSINGCONTROL 2014-06-06 2014-06-06 2014-07-19T01:28:56 S1746-8094(14)00082-2 S1746809414000822 10.1016/j.bspc.2014.04.011 S300 S300.1 FULL-TEXT 2015-05-15T07:07:46.13173-04:00 0 0 20140901 20140930 2014 2014-06-06T00:00:00Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath affil appendices articletitle auth authfirstini authfull authlast highlightsabst orcid primabst ref specialabst alllist content subj ssids 1746-8094 17468094 true 13 13 C Volume 13 24 218 236 218 236 201409 September 2014 2014-09-01 2014-09-30 2014 article fla Copyright © 2014 Elsevier Ltd. All rights reserved. BIDIMENSIONALENSEMBLEEMPIRICALMODEDECOMPOSITIONFUNCTIONALBIOMEDICALIMAGESTAKENDURINGACONTOURINTEGRATIONTASK ALBADDAI S 1 Introduction 2 Theory – empirical mode decomposition 2.1 Background 2.2 Two-dimensional ensemble empirical mode decomposition 3 Materials 3.1 Experimental setup 3.2 Data set 4 Methods 4.1 2D-EEMD parameter estimation for fMRI images 4.1.1 Number of sifting steps 4.1.2 Ensemble size 4.1.3 Noise amplitude 4.1.4 Number of image modes 4.2 Classification 4.2.1 Feature generation 4.2.2 Support vector machine classifier 4.2.3 Random forest classifier 5 Results 5.1 Resulting volume modes 5.2 Classification results 5.2.1 Raw data 5.2.2 Volume image modes 6 Discussion 6.1 Relation to other works 7 Conclusion Appendix A References ALTAF 2007 M PROCIEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSING ROTATIONINVARIANTCOMPLEXEMPIRICALMODEDECOMPOSITION ALTMANN 2003 342 349 C ALTMANN 2004 794 804 C ATTOHOKINE 2008 N BHUIYAN 2009 1313 1316 S IEEEINTERNATIONALCONFERENCEACOUSTICSSPEECHSIGNALPROCESSINGICASSP BHUIYAN 2009 18 24 S IN2009INTERNATIONALCONFERENCEMACHINELEARNINGAPPLICATIONS STUDYBIDIMENSIONALEMPIRICALMODEDECOMPOSITIONMETHODFORVARIOUSRADIALBASISFUNCTIONSURFACEINTERPOLATORS LEO 2001 5 32 B CALHOUN 2003 281 288 V PROCEEDINGSINTERNATIONALWORKSHOPINDEPENDENTCOMPONENTANALYSISBLINDSIGNALSEPARATION ICAFUNCTIONALMRIDATAOVERVIEW CHANG 2001 C ALIBRARYFORSUPPORTVECTORMACHINES CICHOCKI 2007 A ICALABTOOLBOX CICHOCKI 2009 A NONNEGATIVEMATRIXTENSORFACTORIZATIONSAPPLICATIONSEXPLORATORYMULTIWAYDATAANALYSISBLINDSOURCESEPARATION COMMON 2010 P HANDBOOKBLINDSOURCESEPARATIONINDEPENDENTCOMPONENTANALYSISAPPLICATIONS DAMERVAL 2005 701 704 C DENG 2013 42 54 F DEVOR 2008 14347 14357 A FAN 2007 93 105 Y FIELD 1993 173 193 D FLANDRIN 2004 112 114 P FLEUREAU 2011 2783 2792 J HAACK 1999 E MAGNETICRESONANCEIMAGINGPHYSICALPRINCIPLESSEQUENCEDESIGN HANSON 2008 486 503 S HAYNES 2006 523 534 J HUANG 1998 903 995 N JAIANTILAL 2009 A CLASSIFICATIONREGRESSIONBYRANDOMFORESTMATLAB KLOPPEL 2009 681 689 S KOURTZI 2005 440 452 Z KOURTZI 2003 333 346 Z KRIEGESKORTE 2006 3863 3868 N LANG 2010 E EXPLORATORYMATRIXFACTORIZATIONTECHNIQUESFORLARGESCALEBIOMEDICALDATASETS LAO 2004 46 57 Z LI 2008 442 451 W LINDERHED 2002 1 8 A WAVELETINDEPENDENTCOMPONENTANALYSISAPPLICATIONSIX 2DEMPIRICALMODEDECOMPOSITIONSINSPIRITIMAGECOMPRESSION LIU 2005 33 36 Z LIU 2004 803 806 Z PROC17THIEEEINTERNATIONALCONFERENCEPATTERNRECOGNITIONICPR04 TEXTURECLASSIFICATIONTHROUGHDIRECTIONALEMPIRICALMODEDECOMPOSITION LIU 2004 279 282 Z PROCEEDINGSIEEEINTERNATIONALCONFERENCEIMAGEPROCESSINGICIP04 TEXTURESEGMENTATIONUSINGDIRECTIONALEMPIRICALMODEDECOMPOSITION LONG 2005 289 305 S APPLICATIONSHHTINIMAGEANALYSIS LOONEY 2009 1626 1630 D MATHES 2006 98 112 B MOURAOMIRANDA 2005 980 995 J NEUBAUER 2014 A NORMAN 2006 424 430 K NUNES 2003 1019 1026 J NUNES 2009 125 175 J NUNES 2005 177 188 J PITTS 2012 287 303 M REHMAN 2010 1059 1068 N REHMAN 2010 1291 1302 N REHMAN 2010 1 7 N INTERNATIONALJOINTCONFERENCENEURALNETWORKSIJCNN2010 QUADRIVARIATEEMPIRICALMODEDECOMPOSITION RILLING 2007 936 939 G ROELFSEMA 2006 203 227 P ROJAS 2013 2756 2766 A RORDEN 2012 C MRICRO SCHNIDER 2000 5880 5884 A SCHOLKOPF 2002 B LEARNINGKERNELS SHPANER 2013 146 156 M GIJSBERT 2009 228 234 S TANAKA 2006 101 104 T TANSKANEN 2008 T VEMURI 2008 1186 1197 P WEBSTER 1998 E MEDICALINSTRUMENTATIONAPPLICATIONDESIGN WU 2009 1 41 Z WU 2009 339 372 Z XIONG 2006 1516 1521 C XU 2006 3081 3096 Y ZEMZAMI 2013 299 309 O ZHANG 2010 139 142 J PROCEEDINGSIEEESOUTHEASTCONSOUTHEASTCON EDGEDETECTIONUSINGFASTBIDIMENSIONALEMPIRICALMODEDECOMPOSITIONMATHEMATICALMORPHOLOGY ZHENG 2010 407 427 T ALBADDAIX2014X218 ALBADDAIX2014X218X236 ALBADDAIX2014X218XS ALBADDAIX2014X218X236XS item S1746-8094(14)00082-2 S1746809414000822 10.1016/j.bspc.2014.04.011 273545 2014-07-18T22:39:15.08576-04:00 2014-09-01 2014-09-30 true 7488424 MAIN 19 58725 849 656 IMAGE-WEB-PDF 1 si9 3092 147 346 si8 217 19 24 si7 1862 114 234 si6 1784 99 254 si5 389 18 87 si45 906 14 212 si44 159 13 17 si43 389 17 84 si42 145 13 14 si41 130 11 13 si40 159 13 17 si4 399 18 108 si39 145 13 14 si38 130 11 13 si37 1115 30 233 si36 670 46 150 si35 1315 49 262 si34 874 20 202 si33 524 47 82 si32 905 15 306 si31 1847 49 346 si30 473 14 122 si3 1055 37 259 si29 303 20 66 si28 659 17 183 si27 1431 21 417 si26 327 14 74 si25 1365 98 163 si24 180 15 22 si23 376 15 135 si22 588 15 166 si21 1250 40 306 si20 557 14 166 si2 1758 39 393 si19 516 48 78 si18 952 50 190 si17 247 19 29 si16 764 50 123 si15 936 50 173 si14 613 48 101 si13 2484 114 223 si12 247 19 29 si11 580 21 111 si10 2292 109 413 si1 735 37 148 gr9 22504 239 393 gr8 145256 812 659 gr7 92525 439 659 gr6 86616 371 753 gr5 153718 416 753 gr4 128965 419 753 gr3 98464 287 753 gr2 38864 452 602 gr16 130674 677 565 gr15 42068 340 393 gr14 40630 282 715 gr13 70082 309 753 gr12 51421 356 602 gr11 24852 256 565 gr10 147778 484 659 gr1 136564 682 602 fx1 true 27780 141 500 gr9 3652 133 219 gr8 10447 164 133 gr7 14646 146 219 gr6 8779 108 219 gr5 13457 121 219 gr4 13015 122 219 gr3 9246 83 219 gr2 4606 164 218 gr16 11043 164 137 gr15 5775 164 189 gr14 3872 86 219 gr13 5099 90 219 gr12 3890 129 219 gr11 2991 99 219 gr10 16686 161 219 gr1 8036 164 145 fx1 true 5462 62 219 BSPC 531 S1746-8094(14)00082-2 10.1016/j.bspc.2014.04.011 Elsevier Ltd Fig. 1 An illustration of the 2DEEMD decomposition of an fMRI image. IMFs along each row or column represent textures of comparable scale and are to be summed up to yield a BIMF. To improve visibility, histogram equalization has been applied on each image separately. Fig. 2 Stimuli and stimulus design: (a) stimulus protocol and Gabor patches either forming a contour line (CT) or not (NCT), (b) prototypical hemodynamic response function (HRF). Fig. 3 Illustration of the BIMFs resulting from an 2DEEMD decomposition of a single brain slice for both stimulus conditions, i.e. CT and NCT. Note that BIMFs for both conditions have been normalized to the same scale to render them comparable, while the difference images have been normalized separately for enhancing visibility of small differences. Fig. 4 2DEEMD with spatial smoothing and an added noise level of a n =0.2σ: Top left: Ensemble size N E =20, Top right: Ensemble size N E =20, Gaussian filtering, Bottom left: N E =100, Bottom right: N E =200. Fig. 5 2DEEMD modes with noise added or ensemble size increased: Top left: N E =20, a n =0.2σ, Top right: N E =20, a n =1.5σ, Bottom left: N E =20, a n =2.5σ, Bottom right: N E =100, a n =2.5σ. Fig. 6 2DEEMD with variable number K of modes extracted. From Top left to Bottom right: K =4, 5, 6, 7. Fig. 7 Illustration of the six difference VIMFs resulting from an 2DEEMD decomposition of a whole brain volume. The difference refers to the VIMFs for the two conditions CT and NCT, respectively. Each difference VIMF is normalized separately to enhance visibility. Fig. 8 Illustration of the difference maps of VIMF2, VIMF3 and VIMF4 compared to the SPM difference map. All difference maps are averages over all 19 subjects. The difference refers to the VIMFs for the two conditions CT and NCT, respectively. Fig. 9 Normalized eigenvalue spectrum and related cumulative variance for volume mode VIMF3 after decomposition with PCA. Fig. 10 Top: Illustration of the eigenvolume of the raw data obtained by PCA (left) and ICA (right) feature extraction. Bottom: Illustration of the eigenvolume of VIMF3 obtained by PCA (left) and ICA (right) feature extraction. Fig. 11 Boxplot comparing the accuracy achieved by the SVM classifier using projections of the “raw” data as well as of the volume modes (VIMFs) resulting from a 2DEEMD analysis with subsequent Gaussian smoothing. Fig. 12 Variation of statistical measures, obtained with SVM and Gaussian filtering, with the number of principal components extracted from volume modes VIMF3 and VIMF1, respectively. Fig. 13 Experiment 2 receiver operating characteristics (ROC) from all six volume modes (VIMFs). Left: ROC curves of VIMFs resulting from an SVM classification. Right: ROC curves of VIMFs resulting from an RF classification. Fig. 14 The plot shows the Mean Decrease Gini Index for each feature (PCs) generated by PCA-RF applied to VIMF3. Fig. 15 Out-of-Bag (OoB) error rate from all six volume modes (VIMFs) resulting from an RF classifier using Gaussian filtering (exp. 2). The OoB curve for the raw data is shown for comparison, too. Fig. 16 Illustration of the activity distribution corresponding to VIMF3 overlaid to an anatomical image. The activity distribution corresponds to the response to a contour stimulus, denoted as “contour true” (CT). Table 1 MNI coordinates of the activity distributions highlighted in Figs. 7 and 8. Modes x y z Anatomical structure VIMF1 −63 −16 10 Left superior temporal gyrus −63 −22 16 Left postcentral gyrus −69 −31 4 Left middle temporal gyrus 60 −43 −5 Right middle temporal gyrus VIMF2 0 44 −17 Left rectal gyrus VIMF3 −48 8 −35 Left inferior temporal gyrus −48 5 −29 Left middle temporal gyrus 57 −46 −14 Right inferior temporal gyrus −45 2 −14 Left superior temporal gyrus −42 11 −17 Left temporal pole 42 8 −38 Right medial temporal pole 54 −22 −20 Right inferior temporal gyrus VIMF4 −18 −85 4 Left superior occipital gyrus −30 −70 −17 Left fusiform gyrus 0 −13 70 Left paracentral lobule −3 32 −17 Left rectal gyrus VIMF5 0 44 40 Left superior medial gyrus −60 −52 −2 Left medial temporal gyrus 0 32 61 Left superior medial gyrus 63 −46 −2 Right middle temporal gyrus SPM1 45 −25 64 Right postcentral gyrus 36 −19 70 Right precentral gyrus −42 −22 64 Left precentral gyrus SPM2 0 50 1 Left anterior cingulate cortex 0 32 58 Left superior medial gyrus −36 −13 67 Left precentral gyrus 51 20 −11 Right inferior frontal gyrus 30 8 −20 Right temporal pole −6 2 10 Left caudate nucleus Table 2 Statistical measures evaluating classification results obtained by PCA and and SVM classifier either without (exp. 1) or with (exp. 2) applying a linear Gaussian filter to the VIMFs. SVM Experiment 1 Experiment 2 Acc ± std Spec ± std Sens ± std Acc ± std Spec ± std Sens ± std VIMF1 0.72±0.05 0.69±0.06 0.75±0.56 0.66±0.05 0.62±0.07 0.70±0.07 VIMF2 0.79±0.06 0.79±0.04 0.78±0.10 0.75±0.05 0.75±0.04 0.75±0.09 VIMF3 0.80±0.05 0.82±0.07 0.79±0.05 0.81±0.04 0.84±0.04 0.78±0.05 VIMF4 0.79±0.06 0.83±0.08 0.75±0.08 0.77±0.03 0.75±0.04 0.77±0.05 VIMF5 0.74±0.03 0.75±0.04 0.72±0.07 0.72±0.11 0.66±0.10 0.77±0.14 Res 0.77±0.03 0.74±0.07 0.79±0.04 0.72±0.03 0.71±0.07 0.73±0.04 Table 3 Statistical measures evaluating classification results obtained with PCA and an RF classifier either without (exp. 1) or with (exp. 2) applying a linear Gaussian filter to the VIMFs. RF Experiment 1 Experiment 2 Acc ± std Spec ± std Sens ± std Acc ± std Spec ± std Sens ± std VIMF1 0.48±0.02 0.47±0.03 0.50±0.03 0.53±0.04 0.56±0.04 0.51±0.04 VIMF2 0.58±0.02 0.63±0.03 0.53±0.03 0.51±0.02 0.48±0.03 0.53±0.03 VIMF3 0.75±0.02 0.79±0.04 0.71±0.04 0.78±0.02 0.81±0.04 0.75±0.04 VIMF4 0.71±0.02 0.71±0.03 0.71±0.03 0.73±0.02 0.70±0.03 0.77±0.03 VIMF5 0.61±0.02 0.57±0.04 0.65±0.04 0.47±0.01 0.46±0.02 0.48±0.02 Res 0.65±0.04 0.68±0.05 0.61±0.05 0.52±0.03 0.57±0.04 0.47±0.04 Table 4 Statistical measures evaluating classification results obtained with ICA and an SVM classifier either without (exp. 1) or with (exp. 2) applying a linear Gaussian filter to the VIMFs. SVM Experiment 1 Experiment 2 Acc ± std Spec ± std Sens ± std Acc ± std Spec ± std Sens ± std VIMF1 0.67±0.01 0.64±0.03 0.70±0.04 0.64±0.03 0.65±0.02 0.62±0.05 VIMF2 0.69±0.04 0.70±0.07 0.68±0.03 0.70±0.03 0.70±0.05 0.69±0.04 VIMF3 0.76±0.03 0.78±0.06 0.74±0.03 0.73±0.01 0.68±0.00 0.77±0.02 VIMF4 0.70±0.03 0.67±0.04 0.73±0.08 0.72±0.04 0.68±0.04 0.76±0.04 VIMF5 0.70±0.06 0.72±0.06 0.68±0.10 0.63±0.02 0.63±0.04 0.64±0.04 Res 0.60±0.02 0.66±0.04 0.53±0.02 0.62±0.05 0.67±0.03 0.56±0.10 Table 5 Statistical measures evaluating classification results obtained with ICA and an RF classifier either without (exp. 1) or with (exp. 2) applying a linear Gaussian filter to the VIMFs. RF Experiment 1 Experiment 2 Acc ± std Spec ± std Sens ± std Acc ± std Spec ± std Sens ± std VIMF1 0.39±0.01 0.41±0.03 0.37±0.02 0.44±0.00 0.52±0.00 0.36±0.00 VIMF2 0.56±0.02 0.59±0.00 0.54±0.05 0.42±0.03 0.44±0.02 0.50±0.04 VIMF3 0.70±0.02 0.70±0.03 0.66±0.03 0.76±0.03 0.77±0.04 0.76±0.03 VIMF4 0.61±0.02 0.52±0.02 0.65±0.02 0.74±0.02 0.74±0.04 0.74±0.04 VIMF5 0.55±0.04 0.56±0.07 0.55±0.03 0.40±0.00 0.38±0.00 0.42±0.00 Res 0.66±0.01 0.68±0.00 0.65±0.02 0.53±0.00 0.47±0.00 0.58±0.00 Bidimensional ensemble empirical mode decomposition of functional biomedical images taken during a contour integration task S. Al-Baddai a b K. Al-Subari a b A.M. Tomé c G. Volberg d S. Hanslmayr e R. Hammwöhner b E.W. Lang a ⁎ a CIML Group, Biophysics, University of Regensburg, 93040 Regensburg, Germany CIML Group, Biophysics, University of Regensburg Regensburg 93040 Germany b Information Sciences, University of Regensburg, Germany Information Sciences, University of Regensburg Germany c IEETA, DETI, Universidade de Aveiro, 3810 Aveiro, Portugal IEETA, DETI, Universidade de Aveiro Aveiro 3810 Portugal d Experimental Psychology, University of Regensburg, Germany Experimental Psychology, University of Regensburg Germany e Cognition and Oscillations Lab, Department of Psychology, University of Konstanz, Germany Cognition and Oscillations Lab, Department of Psychology, University of Konstanz Germany ⁎ Corresponding author. Tel.: +49 941 943 2599; fax: +49 941 943 2479. Graphical abstract In cognitive neuroscience, extracting characteristic textures and features from functional imaging modalities which could be useful in identifying particular cognitive states across different conditions is still an important field of study. This paper explores the potential of two-dimensional ensemble empirical mode decomposition (2DEEMD) to extract such textures, so-called bidimensional intrinsic mode functions (BIMFs), of functional biomedical images, especially functional magnetic resonance images (fMRI) taken while performing a contour integration task. To identify most informative textures, i.e. BIMFs, a support vector machine (SVM) as well as a random forest (RF) classifier is trained for two different stimulus/response conditions. Classification performance is used to estimate the discriminative power of extracted BIMFs. The latter are then analyzed according to their spatial distribution of brain activations related with contour integration. Results distinctly show the participation of frontal brain areas in contour integration. Employing features generated from textures represented by BIMFs exhibit superior classification performance when compared with a canonical general linear model (GLM) analysis employing statistical parametric mapping (SPM). 1 Introduction The increasing importance of functional imaging techniques in cognitive brain studies creates the need for adapting modern data mining and machine learning methods to the specific requirements of biomedical images, most notably functional magnetic resonance imaging techniques (fMRI). The latter are able to reveal neural processing and cognitive states during cognitive task performance. Also fMRI can be applied repeatedly as it does not apply harmful ionizing radiation to subjects [62,20]. The resulting large volume of such image series renders their analysis and interpretation tedious. This creates the need for robust and automatized techniques to extract the information buried in such images, to analyze them objectively and to classify them properly. Brain mapping using magnetic resonance imaging (MRI) is traditionally performed using voxel-wise statistical hypothesis testing. It is typically based on β – images resulting from a linear regression analysis, calculated by powerful tools like the widely used statistical parametric mapping (SPM). Such univariate approach ignores subtle spatial interactions. Alternatively, exploratory matrix factorization (EMF) techniques like independent component analysis (ICA) [8,10] and nonnegative matrix and tensor factorization (NMF/NTF) [11] provide a more global analysis taking such interactions into account. In general, voxel time courses, or corresponding spatial variations, of activity distributions in fMRI images represent non-linear and non-stationary signals. Recently, an empirical nonlinear analysis tool for such complex, non-stationary spatio-temporal signal variations has been pioneered by Huang et al. [23]. Afterwards an extension to multi-dimensional spatio-temporal signal variations was put forward by Nunes et al. [45], Mandic et al. [49] and, recently, especially by Wu et al. [64]. Such techniques are commonly referred to as empirical mode decomposition (EMD), and if combined with Hilbert spectral analysis, they are called Hilbert–Huang transform (HHT). They adaptively and locally decompose any non-stationary signal in a sum of intrinsic mode functions (IMF) which represent zero-mean, amplitude and (spatial-) frequency modulated components. EMD, and its two-dimensional counterpart 2DEMD, represent a fully data-driven, unsupervised signal decomposition which does not need any a priori defined basis system like Fourier- or Wavelet-based techniques. Furthermore, EMD satisfies the perfect reconstruction property. Thus it lacks the scaling and permutation indeterminacy familiar from blind source separation (BSS) techniques [12]. Owing to these characteristics, 2DEMD, and even more so its noise-assisted variant called ensemble empirical mode decomposition (2DEEMD), is highly promising in analyzing such spatio-temporal signal variations of a multi-scale nature. Note that, for fMRI purposes, EEMD could be even extended to a full 3D-EEMD of a whole brain volume, or even 4D-EEMD if time should be included as well. However, the computational load would increase considerably then which renders such analyses still impractical with large data sets. If two-dimensional IMFs, which represent the same spatial scale, are combined to more global bidimensional IMFs, henceforth called BIMFs, the latter reveal characteristic underlying textures of the multivariate spatio-temporal activity distributions, and provide proper features for classification purposes. Such automated feature extraction proofs especially useful in cognitive neuroscience research for later stimulus response classification. Only few studies have been performed yet to apply multi-dimensional EMD to biomedical images. Zheng et al. [69] reported the use of EMD for activation detection in fMRI images of the brain. Rojas et al. [53] report on applying EMD to Single Photon Emission Computed Tomography (SPECT) images to explore Parkinson's disease. Also very recently, Zemzami et al. [67] used a bi-dimensional EMD approach to decompose 3D biomedical magnetic resonance images (MRI) into bi-dimensional IMFs which then have been reconstructed into 3D volume image modes. Deng et al. [14] recently proposed an new method for fMRI analysis based on empirical mean curve decomposition. Kriegeskorte et al. [29] used searchlight approaches which use a multivariate predictive model in each local neighborhood in brain space. The classification performance is then reported at the center of the searchlight to build an information map. Also classification methods that analyze the brain as a whole, based on an SPM analysis, have been tested (e.g. [21,41]). Lately, there has been a growing interest in state-of-the-art feature extraction techniques for investigating whether stimulus information is present in fMRI response patterns, and attempting to decode the stimuli from the response patterns with a multivariate classifier. However, little is known about the relative performance of different classifiers on fMRI data. These techniques have been successfully applied to the individual classification of a variety of neurological conditions [31,16,26,61], and allow capturing complex multivariate relationships in the data. Multivariate machine learning methods allow for multi-voxel pattern analysis and can reveal patterns amongst voxels in fMRI data [22,43], and so may provide much more detailed information about brain activity, i.e. not only local increases but distributed patterns of activity are identified. This paper explores the potential of 2DEEMD to decompose fMRI images recorded while performing a cognitive task, more specifically a contour integration task while viewing oriented Gabor patches presented as visual stimuli. Further post-processing serves to reduce the dimension of the data sets and to transform the component images according to additional statistical constraints like orthogonality or independence. Feature quality is assessed by statistical measures of properly trained SVM and RF classifiers. First various parameters of the method like the number of sifting steps, the amplitude of added white noise, the number of extracted BIMFs, etc., of the algorithm need to be varied systematically to develop strategies for determining respective optimal values, followed by employing 2DEEMD to extract characteristic textures on different spatial scales. In a second step, features generated from the related volume intrinsic mode functions (VIMFs) are fed into the classifiers. The classifiers serve to corroborate the discriminative power of the extracted features, and by way of proper statistical measures, component images most discriminative for decision making can be identified. Subsequently, activity distributions related to these BIMFs or VIMFs, respectively, can be analyzed with respect to activated brain areas involved and available knowledge about visual processing and contour integration accumulated in the open literature. As during the scan the subjects are asked to indicate the presence or absence of a contour in the stimulus pattern, there are two classes to differentiate: contour true (CT) and non-contour true (NCT). From these results the performance of the classifiers as well as the sensitivity and specificity of their responses, can be deduced. These classification results will reveal those component images and their concomitant neuronal activity distributions, which best differentiate between the classes hence contain the most information as to where neuronal activations are localized in the brain which operate on contour integration tasks within visual processing. 2 Theory – empirical mode decomposition 2.1 Background Roughly a decade ago, an empirical nonlinear analysis tool for complex, non-stationary time series has been pioneered by Huang et al. [23]. It is commonly referred to as empirical mode decomposition (EMD) and if combined with Hilbert spectral analysis it is called Hilbert–Huang transform (HHT). It can be applied to any non-stationary and also nonlinear data set and represents a heuristic data decomposition technique which adaptively and locally decomposes any non-stationary time series in a sum of intrinsic mode functions (IMF) which represent zero-mean amplitude and frequency modulated components. The EMD represents a fully data-driven, unsupervised signal decomposition and does not need any a priori defined basis system. EMD also assures perfect reconstruction, i.e. superimposing all extracted IMFs together with the residual trend reconstructs the original signal without information loss or distortion. However, if only partial reconstruction is intended, it is not based on any optimality criterion rather on a binary include or not include decision. The empirical nature of EMD offers the advantage over other signal decomposition techniques like exploratory matrix factorization (EMF) [30] of not being constrained by conditions which often only apply approximately. Especially with cognitive signal processing, one often has only a rough idea about the underlying modes or component images, and frequently their number is unknown. Eventually, the original signal x(t) can be expressed as x ( t ) = ∑ j c ( j ) ( t ) + r ( t ) (1) c ( j ) ( t ) = Re { a j ( t ) exp ( i ϕ j ( t ) ) } = Re { a j ( t ) exp ( i ∫ − ∞ t ω j ( t ′ ) dt ′ ) } where the c (j)(t) represent the IMFs and r(t) the remaining non-oscillating trend. Furthermore, a j (t) denotes a time-dependent amplitude, ϕ j (t)=∫ω j (t)dt represents a time-dependent phase and ω j [rad/s]=(dϕ j (t))/(dt) denotes the related instantaneous frequency. Plotting both amplitude a j (t) and phase ϕ j (t) as a function of time for each extracted IMF represents a Hilbert–Huang spectrogram [4]. During sifting, mode mixing as well as boundary artifacts can be avoided by a variant called ensemble empirical mode decomposition (EEMD) which has been introduced by Wu and Huang [63]. It represents a noise-assisted data analysis method. First white noise of finite amplitude is added to the data, and then the EMD algorithm is applied. This procedure is repeated many times, and the IMFs are calculated as the mean of an ensemble, consisting of the signal and added white noise. With a growing ensemble number, the IMF converges to the true IMF [63]. Adding white noise to the data can be considered a physical experiment which is repeated many times. The added noise is treated as random noise, which appears in the measurement. In this case, the nth noisy observation will be (2) x n ( t ) = x ( t ) + ϵ n ( t ) = ∑ j c n ( j ) ( t ) + r n ( t ) , where x(t) is the true signal, ϵ n (t) is the random noise and c n ( j ) = c ( j ) + ϵ n ( t ) represents the IMF obtained for the nth noise observation. For the sake of simplicity, following we denote the residuum as r n ( t ) ≡ c n ( J ) ( t ) , hence formally include it into the summation over the IMFs. Soon after its invention, EMD has been extended to higher dimensions [44,64,49,48,50,19] including complex-valued data sets [59,1,38]. Obviously, two-dimensional image data sets were of special interest [51]. In a first approach, such two-dimensional data was treated as a collection of one-dimensional slices, which were decomposed with one-dimensional EMD. This procedure is called pseudo-two-dimensional EMD [64]. However, pseudo-2D-EMD needs a coherence structure associated with the spatial scales in a particular direction, which significantly limits its use. These recent developments in analysis methods for non-linear and non-stationary data sets have received considerable attention by image analysts. Thus several attempts have been started lately to extend EMD to multi-array data sets like two-dimensional (2D) data arrays and images. These extensions are variously known as bidimensional EMD (BEMD), image EMD (IEMD), 2D EMD and so on [44–46,33,13,66,35,36,65]. The most demanding operation of these algorithms involves an envelope surface interpolation step. In [13] the influence of various interpolation methods is studied, and a sifting process is proposed based on a Delaunay triangulation with subsequent cubic interpolation on triangles. In [5,6] the envelope surface interpolation step is replaced by either a direct envelope surface estimation method or radial basis function interpolators. Finally, the modified 2D EMD algorithm proposed in [65] implements the FastRBF algorithm in an estimation of the envelope surfaces. Some of these works exploit decompositions to compute texture information contained in the images. In [68] a new two-dimensional EMD (2DEMD) method is proposed, which is claimed being faster and better-performing than current 2DEMD methods. In [65] rotation-invariant texture feature vectors are extracted at multiple scales or spatial frequencies based on a BEMD. In Nunes et al. [44–46] the BEMD-based texture extraction algorithm is demonstrated in experiments with both synthetic and natural images. Textures are an especially relevant feature of biomedical images if their subsequent classification is intended as is the goal of this study also. Besides a genuine 2D implementation of the BEMD process, 1D EMD has also been applied to images to extract 2D IMFs or bidimensional IMFs (BIMFs) [36,37]. The latter technique treats each row and/or each column of the 2D data set separately by a 1D EMD, which renders the sifting process faster than in a genuine 2D decomposition. But this parallel 1D implementation results in poor BIMF components compared to the standard 2D procedure due to the fact that the former ignores the correlation among the rows and/or columns of a 2D image [34]. A major breakthrough has been achieved by Wu et al. [64], who recently proposed a multi-dimensional ensemble empirical mode decomposition (MEEMD) for multidimensional data arrays. The well-known ensemble empirical mode decomposition (EEMD) is applied to slices of data in each and every dimension involved. The combination of partial IMFs to the final multi-dimensional IMF is based on a comparable minimal scale combination principle (CMSC-principle). Note that the spatial frequencies of the characteristic textures of the extracted BIMFs are related in correspondence with a dyadic filter bank property of the EMD analysis [18]. MEEMD turned out to be very efficient in practical applications, especially if applied to the two-dimensional case, and will be used in this work to analyze fMRI images taken while subjects were performing a cognitive task. Hence, a short summary of the method, introducing the notation used further on, will be given in the following for the convenience of the reader. 2.2 Two-dimensional ensemble empirical mode decomposition Analyzing 2D data arrays, for example an fMRI brain slice, one starts by applying EEMD to each column X *n ≡ x n of the M × N – dimensional data matrix X (3) X = [ x m , n ] = x 1 , 1 x 1 , 2 ⋯ x 1 , N x 2 , 1 x 2 , 2 ⋯ x 2 , N ⋮ ⋮ ⋯ ⋮ x M , 1 x M , 2 ⋯ x M , N The 1D-EEMD decomposition of the nth column becomes (4) x n : = X * , n = ∑ j = 1 J C * , n ( j ) = ∑ j = 1 J c 1 , n ( j ) c 2 , n ( j ) ⋮ c M , n ( j ) where the column vector C * , n ( J ) represents the residuum of the nth column vector of the data matrix. This finally results in J component matrices, each one containing the jth component of every column x n , n =1, …, N of the data matrix X. (5) C ( j ) = [ c 1 ( j ) c 2 ( j ) ⋯ c N ( j ) ] = [ C * , 1 ( j ) C * , 2 ( j ) ⋯ C * , N ( j ) ] = c 1 , 1 ( j ) c 1 , 2 ( j ) ⋯ c 1 , N ( j ) c 2 , 1 ( j ) c 2 , 2 ( j ) ⋯ c 2 , N ( j ) ⋮ ⋮ ⋯ ⋮ c M , 1 ( j ) c M , 2 ( j ) ⋯ c M , N ( j ) Next one applies an EEMD to each row of Eq. (5) yielding (6) C m , * ( j ) = ( c m , 1 ( j ) c m , 2 ( j ) ⋯ c m , N ( j ) ) = ∑ k = 1 K ( h m , 1 ( j , k ) h m , 2 ( j , k ) ⋯ h m , N ( j , k ) ) = ∑ k = 1 K H m , * ( j , k ) where c m , n ( j ) = ∑ k = 1 K h m , n ( j , k ) represents the decomposition of the rows of matrix C (j). These components h m , n ( j , k ) can be arranged into a matrix H (j,k) according to (7) H ( j , k ) = h 1 , 1 ( j , k ) h 1 , 2 ( j , k ) ⋯ h 1 , N ( j , k ) h 2 , 1 ( j , k ) h 2 , 2 ( j , k ) ⋯ h 2 , N ( j , k ) ⋮ ⋮ ⋯ ⋮ h M , 1 ( j , k ) h M , 2 ( j , k ) ⋯ h M , N ( j , k ) The resulting component matrices have to be summed to obtain (8) C ( j ) = ∑ k = 1 K H ( j , k ) . Finally this yields the following decomposition of the original data matrix X (9) X = ∑ j = 1 J C ( j ) = ∑ j = 1 J ∑ k = 1 K H ( j , k ) where each element is given by (10) x m , n = ∑ j = 1 J ∑ k = 1 K h m , n ( j , k ) To yield meaningful results, components h m , n ( j , k ) with comparable scales, i.e. similar spatial frequencies of their textures, should finally be combined [64]. Note, that the CMSC-principle affords to have K = J. In practice, for two-dimensional data sets this implies that the components of each row, which represent a common horizontal scale, and the components of each column, which represent a common vertical scale, should be summed up [64]. Hence, the CMSC – principle leads to BIMFs given by (11) S ( k ′ ) = ∑ k = 1 K H ( k , k ′ ) + ∑ j = k ′ + 1 J H ( k ′ , j ) which thus yields a decomposition of the original data matrix X into BIMFs according to (12) X = ∑ k ′ = 1 K S ( k ′ ) where S (K) represents the non-oscillating residuum. The extracted BIMFs can be considered features of the data set which, according to the CMSC – principle, reveal local textures with characteristic spatial frequencies which help to discriminate the functional images under study. Fig. 1 illustrates the 2DEEMD and the CMSC – principle in case of a decomposition of an fMRI image. It clearly describes the steps of the 2DEEMD algorithm using an fMRI image, i.e. a single brain slice, as an example. The image presented in the upper left corner represents the original fMRI image. The first column on the left side, with the exception of the original image in the upper left corner and the reconstructed image represented on the bottom left side, represents the component images resulting from decompositions along the x-direction. The columns 2→6, except the last row, represent the component images resulting from a decomposition along the y-direction of each component image which resulted from a decomposition along the x-dimension. The last row, except the reconstructed image on the bottom left side, represents the final BIMFs of the original fMRI image obtained by applying the CMSC – principal. 3 Materials 3.1 Experimental setup In this study, functional images were recorded with a 3-Tesla head scanner (Siemens Allegra, Erlangen, Germany) at the courtesy of Prof. M. Greenlee, Experimental Psychology, University of Regensburg, Germany. For the functional series whole brain images were continuously acquired with 46 interleaved axial slices using a standard T 2*-weighted echo-planar imaging sequence employing the following parameters: repetition time TR =2000ms; echo time TE =30ms; flip angle θ =90°; 64×64 matrices; in-plane resolution: 3mm×3mm; slice thickness: 3mm. After the functional scans, high-resolution sagittal T 1-weighted images were acquired for obtaining a 3D structural scan, using a magnetization prepared rapid gradient echo sequence (MP − RAGE) and employing the following parameters: TR =2250ms; TE =2.6ms; 1mm isotropic voxel size. This sequence is optimized to differentiate between white and gray matter. Subjects were positioned supine in the scanner with their head secured in the head coil to minimize head movement. Visual stimuli were presented, using the software package Presentation 12.0 (Neurobehavioral Systems Inc., Albany, Canada), on a standard PC equipped with a 3D graphics card, and back-projected via an LCD video projector (JVC, DLA-G20, Yokohama, Japan) onto a translucent circular screen. The stimuli were seen on a mirror reflecting the image from the projection screen. The projector had a resolution of 800×600 pixels and a refresh rate of 72Hz. The viewing distance to the projection screen was 64cm. Participants were subjected to a perceptual detection task, see Fig. 2 . In each trial, a visual stimulus was presented for 194ms, followed by a blank screen. In half of the stimuli, some Gabor patches formed contours as targets to be detected, while the rest of the patches were oriented randomly as was the case in all the remaining stimuli. Each stimulus array contained 90–100 Gabor patterns and subtended 16.6 by 16.5° of visual angle. A cohort of 19 subjects has been studied during 3 sessions, each encompassing numerous (≲150) trials with Gabor stimuli, organized in 5 blocks. In each trial, the subjects classified the stimulus as contour or non-contour by pressing an associated response button with either the left of the right hand. This resulted in 4 conditions: (1) there was a contour and the subject recognized it correctly (decoded as CT), (2) there was a contour and the subject did not recognize it (decoded as CF), (3) there was no contour but the subject falsely recognized a contour (decoded as NCF) and (4) there was no contour and the subject recognize its absence correctly (decoded as NCT). In this study we concentrate on the two conditions CT and NCT, only as for the other two conditions far less trials were available. Functional data were preprocessed with the software package SPM8 (Wellcome Department of Imaging Neuroscience, London, UK), running under MATLAB 7.0 (Mathworks, Natick, MA). This preprocessing included slice-time correction, motion-correction, spatial normalization and spatial Gaussian smoothing. Except from this no further pre-processing has been considered and these “raw” images have been analyzed. 3.2 Data set Note that with fMRI recordings, the brain is scanned in a number of slices, here (N s =46), which together comprise a 3D activity distribution, henceforth called a volume of activations. Each such brain slice represents a 2D data array which is decomposed by 2DEEMD into K =6 BIMFs per slice. Fig. 3 illustrates corresponding results for a single brain slice. Note that the BIMFs for both conditions have been normalized jointly to render their relative activation levels comparable, but that the resulting difference BIMFs have been normalized separately to enhance visibility of sometimes small differences. Given there are N c =790 scans per session, each one comprising N s =46 brain slices, we consider each complete scan, further on called volume scan, as being equivalent to a data volume V c (x, y, z) where x, y, z denote the spatial coordinates of the voxels in the brain volume. Here index c counts these scans, thus forming an index set { C | c ∈ ℕ , c = 1 , … , 790 } . Now choose the subset of indices corresponding to those volume scans acquired during the time when the hemodynamic response (HR) appeared after the stimulus onset. The HR usually happens roughly τ ≈5s after the stimulus onset. So, for any given stimulus/response condition, the term trials corresponds to those volume scans registered while the hemodynamic response was active. For each of the conditions contour true (CT) and non-contour true (NCT) there were N t ≈90–120 trials across all three sessions, corresponding to an equal number of volume scans. To summarize, the whole data set being analyzed is thus characterized by the following parameters: • the number of slices per volume: N s =46 • the number of voxels per slice: N vs =53×63=3339 • the number of stimulus conditions analyzed: N st =2, corresponding to the conditions CT – contour true, NCT – non-contour true • the number of trials per condition: 90≤ N t ≤120 • the number of subjects analyzed: N sj =19 • the number of voxels per volume: R = N s × N vs • the size of the analyzed data set: S = N sj × N st =38 To reduce the computational load, per condition, these volume scans were averaged, yielding one volume of average activations per condition and session 〈V sc (x, y, z)〉 according to V ¯ ( x , y , z ) ≡ 〈 V sc ( x , y , z ) 〉 = 1 N t ∑ sc ∈ C V sc ∈ C ( x , y , z ) where sc ∈ C denotes those indices of the volumes, belonging to stimulus condition sc ∈{CT, NCT}. Note that 〈 V sc ∈ C ( x , y , z = n s ) 〉 ≡ X n s represents an average brain slice to be decomposed by 2DEEMD (see Eq. (12)) into K BIMFs. If repeated for all brain slices X n s , n s = 1 , … , N s , this decomposition results in K volume intrinsic mode functions VIMFk, k =1, …, K. For further processing, each VIMF is concatenated into a column vector. 4 Methods 4.1 2D-EEMD parameter estimation for fMRI images Despite the introduction of VIMFs, the 2DEEMD decomposition has been performed at the level of average brain slices. As was already mentioned in the introduction, proper parameters, such as the number of sifting steps, the ensemble size, the noise amplitude etc., have to be assigned before one can apply 2DEEMD to fMRI images. Thus in this study, the space of parameters has been explored in a systematic fashion to decide on appropriate values for each parameter as is discussed in the following paragraphs. 4.1.1 Number of sifting steps For rendering decompositions comparable, one cannot use a stopping criterion which is deduced from the data as then each BIMF of a brain slice X n s will result from a different number of sifting steps. Then the BIMFs will vary in each slice and their comparison will be unjust. It is common experience that only a small number of sifting steps is needed usually to extract a proper BIMF. Huang et al. [23] suggest that 15 sifting steps should be sufficient to create reliable BIMFs. To check this assertion, trial decompositions have been performed where sifting was stopped whenever a residuum resulted. From these experiments it was verified that indeed 15 sifting steps sufficed to achieve a stable and reproducible decomposition, except for the lowest spatial frequency modes. In addition, note that with a larger number of sifting steps no improvement of the resulting modes could be observed but the computational load is strongly increased. So as a good compromise between image quality and computational costs, we fixed the number of sifting steps to this suggested number in our study. 4.1.2 Ensemble size A large number of members forming an ensemble, henceforth called ensemble size N E , results in high quality decompositions as, due to self-averaging, the noise in the data becomes almost perfectly canceled out. But this strategy also results in a heavy computational load which often becomes prohibitive. Fig 4 shows the effect upon the BIMFs of varying the number of the members of an ensemble. Especially BIMF4 and BIMF5 when decomposed with a small number of members of the ensemble, show decomposition artifacts like the vertical lines visible in the images. Accordingly, in a large scale simulation, the number N E of members comprising an ensemble has been varied systematically. As the ensemble size increases, these artifacts almost vanish. They rather disappear when the number of members of the ensemble exceeds N E ≥100. It can be seen that there is not much difference between an ensemble size of N E =100 and N E =200 members, respectively. However, the computational load increases linearly with he number of members forming an ensemble. Hence, an 2DEEMD with N E =100 members takes five times more computation time than an 2DEEMD with N E =20 members. Thus keeping computational costs low and still keeping image modes free from artifacts, we suggest applying a linear filter in combination with an 2DEEMD (2DEEMD-LF). 2DEEMD-LF works simply by applying a linear Gaussian filter which replaces each voxel in a BIMF with a Gaussian weighted average intensity in a 5×5 neighborhood of voxels around each voxel. As a result, this spatial smoothing almost eliminates all artifacts from the BIMFs. In Fig. 4 one can see clearly the effect of this smoothing filter. Consequently, in this work an ensemble size N E =20 with spatial smoothing to the activity distributions is used. 4.1.3 Noise amplitude A clear effect on the quality of BIMFs is seen after changing the amplitude of the added noise. Fig 5 clearly demonstrates that with increasing noise amplitude mainly streak artifacts become very prominent. This is because the size of the ensemble is kept constant rather than increased with increasing noise added. This is enforced by the prohibitive computational load otherwise. As a consequence, in this study only a small noise amplitude of a n =0.2· σ, where σ denotes the standard deviation of the intensity distribution of the original fMRI images, has been chosen. 4.1.4 Number of image modes Finally, in order to assure comparable results, the number of extracted BIMFs needs to be kept constant throughout this study. Fig. 6 shows that an optimal number of extracted BIMFs turned out to be K =6. With seven or eight extracted BIMFs no further textures at larger spatial scales appeared. On the other hand, with less than K <6 BIMFs, one can see that the last component does not represent a trend yet. 4.2 Classification Visual inspection of the figures illustrating the differences between both conditions of the resulting BIMFs and VIMFs of a 2DEEMD analysis reveals clues as to whether and where differences in activations appear according to the two stimulus conditions applied. Such observations may be quantified by evaluating the performance of properly trained classifiers which provide adequate means to corroborate whether or not the textures making up the extracted image modes (BIMFs or VIMFs) contain discriminating differential activity distributions allowing for a robust classification of these features. Classification algorithms operate by dividing a high-dimensional space into regions corresponding to different classes of data. For this application, the two classes considered are (contour, non-contour) C = CT, NCT. For each class, an activity distribution of a whole brain volume 〈V(x, y, z))〉 is computed which corresponds to an average over all trials for any of the two stimulus/response conditions. These data volumes are either concatenated into column vectors, or a 2DEEMD is applied to them to extract related volume modes VIMF1–VIMF6. Each of the latter also becomes concatenated into a column vector m for further processing. All such data vectors comprise many components rendering them impractical as input to a classifier. Hence, the high-dimensional textures need to be transformed into low-dimensional features proper for classification. 4.2.1 Feature generation Given the high dimensionality of the data sets collected from only few subjects some dimensionality reduction is needed to cope with the small S, large R problem where S is the number of instances and R is the number of parameters. Features will be generated by decomposing either the raw data or the VIMFs, resulting from an 2DEEMD analysis, with either principal or independent component analysis. Dimension reduction can be achieved by applying a Principal Component Analysis (PCA). To perform a PCA, VIMFs from all N sj =19 subjects and for the two conditions, CT and NCT, studied are collected into a data matrix (13) M = m 11 … m 1 S m 21 … m 2 S ⋮ ⋱ ⋮ m R 1 … m RS Then VIMFs can be projected onto eigenvolumes resulting from a PCA and the resulting projections are used as appropriate features for classification. The goal of a principal component decomposition of the VIMFs, obtained through a 2DEEMD, is to compute the eigenvolumes U of the co-variance or correlation matrix R that span the space of all voxels. This allows for a new representation of the data in an orthogonal axis system which maximizes the variance of the data along each principal direction. Projecting all VIMFs onto these eigenvector directions reveals the contribution of each eigenvector to the volume mode. PCA eigenvolumes can be determined by decomposing the R × S dimensional matrix M of VIMFs with the help of a singular value decomposition (SVD) (14) M = U D V T where, D is a diagonal matrix with min(R, S) non-zero singular values and U and V are the eigenvector matrices of the non-normalized correlation matrix R = MM T and the related kernel matrix K = M T M, respectively, whose corresponding column vectors u, v form orthogonal eigenvectors in spaces of dimension R × R and S × S, respectively. Therefore the columns of U span the voxel space while the columns of V span the (participants × conditions) space. It is important to note that from only S ≪ R observations the components of the eigenvector matrix U cannot be determined reliably. Hence one has to resort to study the related kernel matrix as only an eigenvalue decomposition of matrix K is feasible. This finally yields, substituting results from above, for the singular value decomposition of the matrix of observations M the following relation (15) K = M T M = ( U D V T ) T U D V T = ( V D U T ) U D V T = V D 2 V T = V Λ V T Therefore the eigenvalues λ s are the square of the singular values d s and the eigenvector matrix V is the right eigenvector matrix of SVD. Hence, Λ = D 2 and the eigenvector matrix U of the correlation matrix R can be obtained from the matrix of observations M and its SVD via U = MVΛ −1/2. Now consider projecting all data vectors onto the new basis vectors, i.e. consider the projections Z := U T M. Hence Z represents the matrix of projections of the data vectors M onto the eigenvectors U of the correlation matrix R which also span the space of all voxels of the component images. From the discussion above it follows that (16) U T M : = Z = D V T = Λ 1 / 2 V T Note that both, D and V follow from an eigendecomposition of the kernel matrix of the observations. Every row Z n,* of the projection matrix Z contains a projection of a data vector onto one of the new basis vectors U *,n . The latter are ordered via their corresponding eigenvalues d s = λ s . Independent components are achieved by applying a, yet to be determined, S × S rotation matrix W to the projected data Z. The INFOMAX algorithm [12] was used to estimate the rotation matrix, and the new representation of the projected data is obtained via (17) S = W Z = W U T M Afterwards the rows of S are re-ordered according to their correlations to the rows of Z. Therefore after re-ordering, the first row of S should have the largest correlation with the first row of Z, the second row of S should have the largest correlation with the second row of Z and so on. This allows to select only the most informative projections to be used as features for a classifier, for example. Anyway, only S ≪ R projections can be obtained which renders the problem tractable. In summary, the eigenvalue decomposition of the kernel matrix K, i.e. the matrix of inner products of the data vectors, provides all ingredients to compute the projections onto at most S directions. Each row of Z contains a projection onto one such basis vector. Again note that the latter are ordered via their corresponding eigenvalues λ s . Considering the data set studied, a further dimension reduction is possible by selecting a subset of those S projections, e.g. choosing the L < S leading rows of the projection matrix Z. Concerning the independent projections S, a similar dimension reduction can be achieved but the selected L independent projections are linear combinations of all orthogonal projections Z. These independent projections do not obey any natural ordering principle, but the re-ordering step allows to align the projections according to the variance of the data. Then the classifier will only have L ≤ S inputs and S examples which represents a stable situation in the sense that sufficient information is provided to the classifier to achieve a reliable classification of the data. 4.2.2 Support vector machine classifier For classification, a soft margin SVM classifier, more specifically the C – SVM [9] algorithm as contained in the MATLAB Statistics Toolbox [40], has been used. The parameter C allows controlling the number of margin errors and support vectors. The optimization problem can be written in its dual form as (cf. [56]): (18) maximize α ∈ ℝ m W ( α ) = ∑ i = 1 S − 1 α i − 1 2 ∑ i , j = 1 S − 1 α i α j y i y j k ( z ( i ) , z ( j ) ) (19) subject to 0 ≤ α i ≤ C for all i = 1 , … , S − 1 and (20) ∑ i = 1 S − 1 α i y i = 0 . where k(z (i), z (j)) represents the kernel dot product between the non-linearly mapped training data. The mapping is represented by a sigmoidal kernel, (21) k ( z ( i ) , z ( j ) ) = tanh ( γ z ( i ) T z ( j ) + r ) with parameters γ >0, r <0. These parameters represent user-defined parameters to be assigned before running the optimization algorithm. The outcome of the optimization are the Lagrangian values, 0≤ α i ≤ C. The training examples, known as support vectors, are related with the non-zero Lagrangian coefficients. Cross-validation is effected by a leave-one-out technique where every data vector is once used for testing. The resulting decision function, considering a test vector z test to be classified takes the form (22) f ( z test ) = sgn ∑ i = 1 S − 1 α i y i k ( z test , z ( i ) ) + b where b denotes the distance of the hyperplane from the origin. Note that only the training examples z (i) with α i ≠0 (support vectors) need to be available for testing. 4.2.3 Random forest classifier The random forest algorithm, developed by Breiman [7], is a set of binary decision trees, each performing a classification. The final decision is taken by majority voting. Each tree is grown using a bootstrap sample from the original data set. Each node of the tree randomly selects a small subset of features for splitting the data into two subsets. An optimal split separates the set of samples at each node into two supposedly more homogeneous or pure sub-groups with respect to the class of its elements. The impurity level of each set of samples with respect to class membership can be measured by the Gini index. Denoting this class labels by ω c , c =1… C, the Gini index of node i is defined as G ( i ) = 1 − ∑ c = 1 C ( P ( ω c ) ) 2 where P(ω c ) is the probability of class ω c in the set of instances that belong to node i. Note that G(i)=0 whenever node i is pure, e.g. if its data set contains only instances of one class. To perform a split, one feature z l is tested on the set of samples with n elements according to z l > z th , which is then divided into two sub-groups (left and right) with n l and n r elements. The change in impurity is computed as Δ G ( i ) = G ( i ) − n l n G ( i l ) + n r n G ( i r ) That feature among all z l and the threshold z th which yield the largest decrease of the Gini index is chosen to perform the split at node i. Each tree is grown independently, and no pruning is applied on the grown trees. The main steps of this algorithm [25,24] are the following: 1. Given a data set T with N examples, each with L features, select the number T of trees, the dimension of the subset mtry < L of features, and the parameter that controls the size of the tree (it can be the maximum depth of the tree, the minimum size of the subset in a node to perform a split). 2. Construct the t =1… T trees. (a) Create a training set T t with N examples by sampling with replacement the original data set. The out-of-bag data set O t is formed with the remaining examples of T not belonging to T t . (b) Perform the split of node i by testing one of the mtry = ⌊ L ⌋ randomly selected features. (c) Repeat step 2b up to the tree t is complete. (d) Use the out-of-bag (OoB) data O t as test set of the t tree. Keep track of the votes for each class instances each time they are in out-bag data sets. The number of false classifications averaged over all cases yields the OoB error estimate. In this way an out-of-bag (OoB) error results measuring the classification performance of the RF classifier. 3. Repeat step 2 to grow next tree if t ≠ T. In this work T =500 decision trees were employed. After training, the importance of each feature z l in the ensemble of trees can be computed by adding up the values of ΔG(i) of all nodes i where the feature z l is used to perform a split. Then it is possible to identify the relative importance of the features. 5 Results 5.1 Resulting volume modes Note that the analysis of the fMRI experiments focusses onto the two stimulus/response conditions, i.e. contour true (CT) and non-contour true (NCT). Hence, results will be discussed later on in terms of differences of normalized VIMFs for both conditions. Altogether VIMFs from 19 subjects and for two stimulus conditions have been obtained as an average over N t trials during the three sessions, resulting in a total of 38 VIMFk for each k. The activity distributions within the VIMFs have been normalized for both conditions taken together. Results are presented as difference images (23) Δ VIMFk = VIMFk CT − VIMFk NCT which have been normalized separately to clearly show the, sometimes small, differences. Only differences above 0.7Δ max are shown in the images which exhibit an axial, a sagittal and a coronal view. In general, a high biological variability is observed as seen in the figure for VIMF3 which is provided as supplementary material. However, across all 19 subjects activity is consistently localized in the area of the temporal gyrus, though to a varying extent. For visualization purposes, the MRIcro analyze viewer [54] has been used. Fig. 7 illustrates differences ΔVIMFk, k =1, …, K as averages over all subjects to highlight robustly obtained activation loci. The differences corresponding to k =2 to k =4 clearly show highly focused and spatially localized activities. ΔVIMF2 exhibits activity almost exclusively in the left rectal gyrus. ΔVIMF3, instead, shows activity mainly in the temporal gyrus, but activity is more pronounced in the left temporal gyrus compared to the right temporal gyrus, especially in the left inferior and middle temporal gyrus. ΔVIMF4, finally, exhibits a pronounced activity in the (left) paracentral lobule. Note that the activity distribution in each difference image has been normalized separately to the range 0–1 and only the highest 30% of the activities are shown in the images, overlaid onto an anatomical image. Corresponding MNI coordinates of these localized activity blobs are collected in Table 1 . Considering a comparative analysis of the activity distributions resulting from a 2DEEMD analysis with the ones obtained using the canonical SPM tool, Fig. 8 clearly demonstrates the superior detail and spatial localization which ΔVIMF2, ΔVIMF3 and ΔVIMF4 exhibit compared to the SPM results which most closely resembles ΔVIMF4 on a first level analysis, denoted SPM1. Only at a second level of analysis, denoted SPM2, also other activity blobs become visible, though less localized and focused. Note that only activations corresponding to the same level of statistical significance are exhibited to render images from both approaches comparable. 5.2 Classification results For classification either orthogonal and independent projections are the inputs of the classifiers. Considering orthogonal projections, the following two data sets have been considered: • Projections onto principal directions of average volumes 〈V sc (x, y, z)〉 deduced from “raw” data and collected in an L × S matrix Z av . • Projections onto principal directions of volume modes VIMFk, k =1, …, K deduced from a 2DEEMD decomposition of average volumes 〈V(x, y, z)〉 and collected in an L × S matrix Z vm . These projections are called orthogonal projections and are used as features for classification. The scree plot, see Fig. 9 , illustrates the normalized eigenvalue spectrum and related cumulative sum of variances for the most discriminative volume mode VIMF3. It can be seen that the eigenvalue spectrum levels off after only L =10 eigenvalues which in total already explain roughly 80% of the data variance. The eigenvolumes, i.e. the columns of matrix U, represent uncorrelated partial activity distribution in brain. Fig. 10 illustrates the eigenvolume related with the second largest eigenvalue for both the raw data and VIMF3, respectively. Both underline the relevance of the occipital area for contour integration. However, the activity distribution provided by VIMF3 is much more focused than in case of the raw data. Given all orthogonal projections Z discussed above, a rotation matrix W is estimated. The resulting independent projections, after re-ordering, are collected • in an L × S matrix S av in case of the projected “raw” data, or • in an L × S matrix S vm in case of the projected volume modes. The orthogonal or independent projections thus obtained have been input to either an SVM or an RF classifier. Both RF and SVM training represent stochastic algorithms. SVM training can be performed efficiently by applying a sequential minimization optimization (SMO) technique which breaks the large quadratic programming (QP) problem into a sequence of smaller QP sub-problems which can be solved analytically [56]. For both classifiers, training and testing was repeated 10 times while permuting the whole data set randomly, and a leave-one-out cross-validation (LOOCV) strategy was invoked to obtain reliable results. Several metrics were used to measure the performance of the classifiers, namely accuracy (acc), sensitivity (sens), specificity (spec) and the receiver operating characteristic (ROC) curve. 5.2.1 Raw data First “raw” features, i.e. the projections Z av of average volumes resulting from “raw” data onto their principal directions, have been fed to an SVM classifier yielding an average accuracy acc =0.75±0.03, an average specificity spec =0.72±0.03 and an average sensitivity sens =0.77±0.05 (see first column of Fig. 11 ). These figures have been obtained by varying the number of principal components L and choosing the optimal number of PCs (L =10) according to the highest accuracy achieved. Next, the same “raw” features have been fed to an RF classifier resulting in an average accuracy acc =0.66±0.04, an average specificity spec =0.66±0.07 and an average sensitivity sens =0.65±0.07. These results has been obtained with the total number of features (L =38) according to the highest accuracy obtained. The orthogonal projections of the raw data onto the PCs leave some statistical dependencies, hence ICA was applied to remove the latter. The resulting independent projections S av were again fed into the SVM classifier resulting in the following statistical measures: an average accuracy acc =0.64±0.02, an average specificity spec =0.64±0.02 and an average sensitivity sens =0.64±0.05. Then, the same ICs features have been fed to a RF classifier getting an average accuracy acc =0.66±0.00, an average specificality spec =0.68±0.00 and an average sensitivity sens =0.63±0.00 5.2.2 Volume image modes Obviously projections Z av of “raw” data yield poor classification accuracies only. A 2DEEMD decomposition of the “raw” data set resulted in six volume modes VIMFk, k =1, …, K =6 which might have been further processed by a Gaussian smoothing filter. Afterwards, the corresponding orthogonal (Z vm ) or independent (S vm ) projections of the VIMFs were used as appropriate features and have been fed into either an SVM or an RF classifier trained to differentiate the stimulus/response conditions CT and NCT. An exhaustive experimental evaluation of the classifiers has been conducted, having as input either Z vm or S vm . Here the number of features L was also a variable for the SVM classifier, but kept constant to L =38 in case of the RF classifier. The results show that the SVM presents the best performance with L =11±2 input features. Furthermore, both classifiers exhibit their highest performance with features extracted from VIMF3. Even the remaining modes, with the exception of VIMF1, also present better results than the raw data in many cases. Moreover, the performance of the classifiers is better with orthogonal features than with independent features. And the linear filtering, applied to the VIMFs, also has a positive impact on the performances. Best results are summarized in Tables 2–5 provided in an. Fig. 11 illustrates the accuracy achieved with all VIMFs resulting from an SVM classifier and Gaussian filtering (exp. 2). Fig. 12 provides a more complete picture. It illustrates the dependence of accuracy (acc), specificity (spec) and sensitivity (sens) on the number of PCs extracted, hence the dimension of the feature subspace. These statistical measures were achieved using features, i.e. projections, of the first Z VIMF1 and third Z VIMF3 volume modes, respectively. While VIMF1 mostly represents noise, VIMF3 exhibits textures which provide the most discriminative features for contour vs non-contour stimuli. As an additional statistical measure, Fig. 13 presents the related receiver operating characteristic (ROC) curves for all VIMFs and average volumes resulting from a data decomposition involving Gaussian filtering (exp. 2). The related area-under-curve (AUC) indicator yields AUC =0.86 and AUC =0.80 in case of VIMF3 classified with SVM and RF, respectively. Corresponding measures in case of VIMF4 amount to AUC =0.82 and AUC =0.78, respectively. 6 Discussion The data analysis presented in this study considers a decomposition, employing 2DEEMD, into intrinsic modes which represent textures on characteristic spatial scales of local variations in the neuronal activity distribution during a contour integration task. According to the EMD principle, the first intrinsic mode (VIMF1) represents textures with the highest spatial frequencies contained in the data, and subsequent modes represent textures with correspondingly lower spatial frequencies. Finally, the residual represents any spatially non-oscillating background activity. Proper features have been deduced from projecting these intrinsic modes onto their respective principal (PC) or independent (IC) components. These features finally have been fed into two classifiers, a SVM and a RF of decision trees. The aim of the classification was to corroborate the superior performance, with respect to the classifying “raw” data, of the deduced features in discriminating between the two stimulus conditions. Thereby classification also corroborates the decisive information content of the highest scoring modes with respect to the spatial distribution of neuronal activity when processing visual stimuli during contour integration. These modes highlight those brain areas which actively integrate oriented contrast edges into contours. For grouping stimulus-related activity distributions, two classifiers were considered: a SVM, employing a sigmoidal kernel, and a RF of decision trees. With a kernel SVM, best classification performance was achieved constructing a feature vector with the first L =11±2 projections (either orthogonal or independent) of the VIMFs. The eigenvalue spectrum indicates that the first L =11 PCs already explain roughly 90% of the variance of the data. They thus span a subspace of the data space which provides the relevant information for decision making of the classifier. Including further projections onto PCs with L >11 decreases the accuracy achieved. This indicates that these projections provide distracting information to the SVM classifier. A RF of decision trees employs a random variable selection scheme. Good classification results are only obtained if all projections, i.e. all information being available, contribute to the decision making. In addition, the RF uses a bagging technique to improve classification predictions with unstable classification models. The RF algorithm also offers an easy way to measure the importance of any variable for decision making. Generally, two importance measures are discussed in literature: mean decrease accuracy and mean decrease Gini. The latter is reported to show greater robustness against small variations in the data, hence was chosen here. So, an RF classifier measures the importance of each feature on the decision process by using the Gini importance value. The latter is the sum (or the average) of the Gini index reduction over all nodes in which the specific feature is used to perform the split. Fig. 14 reveals that the Gini importance parameter exhibits a strong peak around features nr. 19–23 contrary to the SVM classifier which works better employing the first L ≈11 components. In order to make a fair comparison between the two classifiers SVM and RF, leave-one-out cross-validation (LOOCV) is applied to both. The RF does cross-validation intrinsically by using a bagging technique. The out-of-bag (OoB) error normalized by the number of trees is reported in Fig. 15 . It illustrates the general behavior of the running OoB-error when we train a random forest of 500 trees with mtry =6 variables at each node. For VIMF3 and VIMF4, the OOB-error drops quickly to its minimal value OoBerror ≈0.27 after ∼200 trees for both VIMF3 and VIMF4. For the “raw” data, the OoB-error shows a much more moderate decline within ∼200 trees and then levels off at OoBerror ≈0.40. These results clearly show that the highest accuracy is achieved with projections of VIMF3 consistently with both classifiers, SVM and RF, respectively, which both substantially outperform a direct classification of average volumes. Gaussian filtering applied to the VIMFs before the feature generation stage improve results considerably. Thus Gaussian filtering obviously can deal with artifacts resulting from an EMD decomposition. This also helps to keep the number of ensembles, hence the computational load, conveniently small. With or without Gaussian linear filtering, the best discriminative power is observed with VIMF3. The latter can be visualized by overlaying the related activity distribution onto an anatomical image of a brain slice. Fig. 16 illustrates VIMF3 of one subject. For clarity of presentation, only 12 slices are shown. These slices are ordered from bottom to top as indicated by the position of the white lines on the axial brain slice. The highlighted brain regions are in good accordance with results of previous neuroimaging studies that typically revealed an increase of neural activity for contour compared to non-contour conditions within middle and lateral occipital areas [2,57]. Also, left frontal activity was indicative for the stimulus type in this subject. Most importantly, the results clearly demonstrate that contour integration involves higher brain areas also and is not confined to the visual cortex. 6.1 Relation to other works In general, classification analysis tests hypotheses in terms of separating pairs (or more) of conditions. Note that the hypothesis is that a different pattern of activity occurs in the voxels making up a region, not that the activation level is different. This enables us to stay away from interpreting BOLD patterns in terms of activated voxels, a term which means that these neurons are active more than others, which may or may not be correct [15]. The type of hypothesis and associated test is especially useful if the conditions under investigation recruit different neural networks. We here used a visual detection task where spatially distributed Gabor patterns had to be grouped into continuous contours according to their relative orientation and position [17]. Because the contours extend the receptive field size of neurons in lower (occipital) visual processing regions, an integration across space, administered through parietal and frontal brain activity, is necessary for contour integration and detection [52]. The fact that partly different brain regions are involved into contour and non-contour processing makes the task suitable for a whole-brain classification analysis. Previous neuroimaging results on contour integration suggest that both early retinotopic areas as well as higher visual brain sites contribute to contour processing. In a set of MRI adaptation studies Kourtzi and colleagues found that contours compared to non-contour patterns evoked increased BOLD responses all along earlier visual areas V1 to V4, as well as in lateral occipital and posterior fusiform areas within the inferior temporal lobe [28,2,3,27]. Other authors combined magneto- or electroencephalographic recordings (MEG/EEG) with source reconstruction methods to investigate the temporal dynamics and the neural sources of contour processing. They uniformly found that differences between contour and non-contour stimuli do not occur before 160ms after stimulus onset, within the N1 to P2 time range of the event-related potentials or fields (ERP/ERF). The neural sources of the P1 / N1 differences were located within middle occipital [60,47] and occipito-temporal areas [57], as well as in primary visual cortex [47,57]. These results generally comply with the view that different visual areas contribute to contour perception. Additionally, due to the relatively late onset of ERP/ERF differences in primary visual areas, they suggest that the increased BOLD and ERP responses in early visual cortex during contour processing are mainly driven by feedback from higher visual sites. Importantly, however, it is also true that neural responses to contour stimuli are highly variable across tasks. For example, the N1 ERP difference occurs later for misaligned compared to readily detectable contours [39], and differences between contour and non-contour stimuli can even be absent in untrained observers [32]. In order to explain the variability of brain responses, we need to consider that contours are no well-defined targets for detection. Numerous instances of contours can occur during the experiment so that frequent updates of the target representation and the associated task are necessary for a successful contour detection. The updating and maintenance of task-related information, e.g. of task-related memories [55] or the task representation itself [58], are commonly considered frontal brain functions. In the present study we show for the first time that frontal brain activity alone, captured in VIMF2, performs reasonably as a classifier for contour and non-contour trials. The results thus underline the importance of frontal brain activity in contour integration and may mark a starting point for further investigations on that topic. 7 Conclusion The investigation presented discusses the application of two-dimensional ensemble empirical mode decomposition (2DEEMD) to an fMRI study of a contour integration task. Because of the enormous computational load involved we currently only discussed data sets averaged over many trials and sessions. A systematic optimization of the parameters inherent to the method lead to a decomposition of whole brain scans into so-called volume modes, the equivalent of intrinsic mode functions in plain EMD, which exhibited characteristic textures on various spatial scales. Related activity distributions showed strong spatial localization and different modes exhibited activation in clearly separated areas of the brain. Though in agreement with results of a canonical analysis with a GLM approach (using SPM 8), the 2DEEMD results show better localization, and activations appear more sparse and highly focused. Hence, the superior precision in spatial localization of activity blobs highlights the potential of 2DEEMD when analyzing functional imaging data sets. The evaluation of the classification performance based on VIMFs, most notably VIMF3 and VIMF4, also revealed a superior classification accuracy compared to “raw” data. Moreover, comparing two classifiers based on different principles, namely a RF and a SVM, the latter significantly outperforms a RF in terms of accuracy and ROC/AUC characteristics. With respect to the perceptual task, there is a consensus that contour integration relies on distributed activity within higher as well as lower visual brain areas (e.g. [2,28]). However, a systematic investigation on whole-brain patterns of activity that might discriminate between contour and non-contour conditions has yet not been conducted. Our data show, for the first time, that distributed activity in bilateral inferior temporal (VIMF3) and superior occipital (VIMF4) lobe is maximally predictive for the stimulus condition. Moreover, we demonstrate that pre-frontal brain activity (VIMF2) discriminates between contour and non-contour patterns. The result adds up to those of previous fMRI studies [2,28]) and underlines the importance of higher brain areas for the perceptual integration of local stimulus details into a global form [27]. Generally, results clearly demonstrate the potential usefulness of a 2DEEMD analysis of functional imaging data [42]. A subsequent classification based on features generated from the intrinsic modes (VIMFs), extracted by 2DEEMD, relies on two feature characteristics: highly localized activity distributions in fMRI component images (VIMFs) compared to an GLM/SPM analysis, and a higher discriminating power compared to a classification of “raw” data sets. Appendix A Both classifiers have been evaluated by repeating training and testing 10 times while employing a Leave-One-Out-Cross-Validation scheme and a random shuffling of the data sets presented to the classifiers. The number of features L in the SVM was varied while in the RF it was kept constant to L =38. References [1] M.U. Altaf T. Gautama T. Tanaka D.P. Mandic Rotation invariant complex empirical mode decomposition Proc. IEEE International Conference on Acoustics, Speech, Signal Processing 2007 [2] Ch.F. Altmann H.H. Bülthoff Z. Kourtzi Perceptual organization of local elements into global shapes in the human visual cortex Curr. Biol. 13 4 2003 342 349 [3] Ch.F. Altmann A. Deubelius Z. Kourtzi Shape saliency modulates contextual processing in the human lateral occipital complex J. Cogn. Neurosci. 16 5 2004 794 804 [4] N. Attoh-Okine K. Barner D. Bentil R. Zhang The empirical mode decomposition and the Hilbert–Huang transform EURASIP J. Adv. Signal Process. 2008 [5] S.M.A. Bhuiyan R.R. Adhami J.F. Khan IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2009 1313 1316 [6] S.M.A. Bhuiyan J.F. Khan N.O. Attoh-Okine R.R. Adhami Study of bidimensional empirical mode decomposition method for various radial basis function surface interpolators In 2009 International Conference on Machine Learning and Applications 2009 IEEE 18 24 [7] Breiman Leo Random forests Mach. Learn. 45 1 2001 5 32 [8] V.D. Calhoun T. Adali L.K. Hansen J. Larsen J.J. Pekar ICA of functional MRI data: an overview Proceedings of the International Workshop on Independent Component Analysis and Blind Signal Separation 2003 281 288 [9] Ch.-Ch. Chang Ch.-J. Lin A Library for Support Vector Machines 2001 available at [10] A. Cichocki S. Amari K. Siwek T. Tanaka Anh Huy Phan ICALAB Toolbox 2007 available at [11] A. Cichocki R. Zdunek A.H. Pham S. Amari Nonnegative Matrix and Tensor Factorizations Applications to Exploratory Multi-way Data Analysis and Blind Source Separation 2009 Wiley and Sons [12] P. Common Ch. Jutten Handbook of Blind Source Separation: Independent Component Analysis and its Applications 2010 Academic Press [13] Ch. Damerval S. Meignen V. Perrier A fast algorithm for bidimensional EMD IEEE Signal Process. Lett. 12 10 2005 701 704 [14] F. Deng D. Zhu L. Jinglei L. Guo T. Liu FMRI signal analysis using empirical mean curve decomposition IEEE TBME 60 1 2013 42 54 [15] A. Devor E.M.C. Hillman P. Tian C. Waeber I.C. Teng L. Ruvinskaya M.H. Shalinsky H. Zhu R.H. Haslinger S.N. Narayanan I. Ulbert A.K. Dunn E.H. Lo B.R. Rosen A.M. Dale D. Kleinfeld D.A. Boas Stimulus-induced changes in blood flow and 2-deoxyglucose uptake dissociate in ipsilateral somatosensory cortex J. Neurosci. 28 2008 14347 14357 [16] Y. Fan D. Shen R.C. Gur R.E. Gur C. Davatzikos COMPARE: classification of morphological patterns using adaptive regional elements IEEE TMI 26 1 2007 93 105 [17] D.J. Field A. Hayes R.F. Hess Contour integration by the human visual system: evidence for a local “association field” Vision Res. 33 2 1993 173 193 [18] P. Flandrin G. Rilling P. Goncalves Empirical mode decomposition as a filter bank IEEE Signal Process. Lett. 2 2004 112 114 [19] J. Fleureau A. Kachenoura L. Albera J.-C. Nunes L. Senhadji Multivariate empirical mode decomposition and application to multichannel filtering Signal Process. 91 12 2011 2783 2792 [20] E. Haack Magnetic Resonance Imaging, Physical Principles and Sequence Design 1999 Wieley-Liss [21] S.J. Hanson Y.O. Halchenko Brain reading using full brain support vector machines for object recognition: there is no “face” identification area Neural Comput. 20 2008 486 503 [22] J.-D. Haynes G. Rees Decoding mental states from brain activity in humans Nat. Rev. Neurosci. 7 2006 523 534 [23] N.E. Huang Z. Shen S.R. Long M.L. Wu H.H. Shih Q. Zheng N.C. Yen C.C. Tung H.H. Liu The empirical mode decomposition and Hilbert spectrum for nonlinear and nonstationary time series analysis Proc. Roy. Soc. London A 454 1998 903 995 [24] A. Jaiantilal Classification and Regression by Randomforest-Matlab. 2009 available at [25] A. Jaiantilal, 2010. [26] S. Klöppel C.M. Stonnington C. Chu1 B. Draganski R.I. Scahill J.D. Rohrer N.C. Fox C.R. Jack J. Ashburner R.S.J. Frackowiak Automatic classification of MR scans in Alzheimer's disease Brain 131 3 2009 681 689 [27] Z. Kourtzi E. Huberle Spatiotemporal characteristics of form analysis in the human visual cortex revealed by rapid event-related fMRI adaptation Neuroimage 28 2 2005 440 452 [28] Z. Kourtzi A.S. Tolias Ch.F. Altmann M. Augath N.K. Logothetis Integration of local features into global shapes: monkey and human FMRI studies Neuron 37 2 2003 333 346 [29] N. Kriegeskorte R. Goebel P. Bandettini Information-based functional brain mapping Proc. Natl. Acad. Sci. 103 2006 3863 3868 [30] E.W. Lang R. Schachtner D. Lutter D. Herold A. Kodewitz F. Blöchl F.J. Theis I.R. Keck J.M. Górriz Sáez P. Gómez Vilda A.M. Tomé Exploratory Matrix Factorization Techniques for Large Scale Biomedical Data Sets 2010 Bentham Science Publishers [31] Z. Lao Morphological classification of brains via high-dimensional shape transformations and machine learning methods NeuroImage 21 2004 46 57 [32] Wu Li Valentin Piëch Charles D. Gilbert Learning to link visual contours Neuron 57 3 2008 442 451 [33] A. Linderhed 2-D empirical mode decompositions in the spirit of image compression Wavelet and Independent Component Analysis Applications IX Proceedings of SPIE vol. 4738 2002 1 8 [34] Z. Liu S. Peng Boundary processing of bidimensional EMD using texture synthesis IEEE Signal Process. Lett. 12 2005 33 36 [35] Z. Liu H. Wang S. Peng Texture classification through directional empirical mode decomposition Proc. 17th IEEE International Conference on Pattern Recognition (ICPR’04) 2004 803 806 [36] Z. Liu H. Wang S. Peng Texture segmentation using directional empirical mode decomposition Proceedings of IEEE International Conference on Image Processing (ICIP’04) 2004 279 282 [37] S.R. Long Applications of HHT in Image Analysis 2005 World Scientific River Edge, NJ, USA 289 305 [38] D. Looney D.P. Mandic Multi-scale image fusion using complex extensions of EM IEEE Trans. Signal Process. 57 4 2009 1626 1630 [39] Birgit Mathes Dennis Trenner Fahle Manfred The electrophysiological correlate of contour integration is modulated by task demands Brain Res. 1114 1 2006 98 112 [40] MATLAB and Statistics Toolbox. Release 2013, 2013. [41] J. Mourao-Miranda A.L.W. Bokde C. Born H. Hampel M. Stetter Classifying brain states and determining the discriminating activation patterns: support vector machine on functional MRI data NeuroImage 28 2005 980 995 [42] A. Neubauer A. Tomé A. Kodewitz J. Górriz G. Puntonet E. Lang Bidimensional ensemble empirical mode decomposition of functional biomedical images Adv. Adaptive Data Anal. 06 2014 1450004, 36 pages [43] K.A. Norman S.M. Polyn G.J. Detre J.V. Haxby Beyond mind-reading: multi-voxel pattern analysis of fMRI data Trends Cogn. Sci. 10 2006 424 430 [44] J.C. Nunes Y. Bouaoune E. Delechelle O. Niang Ph Bunel Image analysis by bidimensional empirical mode decomposition Image Vis. Comput. 21 12 2003 1019 1026 [45] J.C. Nunes E. Deléchelle Empirical mode decomposition: applications on signal and image processing Adv. Adaptive Data Anal. 1 2009 125 175 [46] J.C. Nunes S. Guyot E. Deléchelle Texture analysis based on local analysis of the bidimensional empirical mode decomposition Mach. Vision Appl. 16 2005 177 188 [47] M.A. Pitts A. Martínez S.A. Hillyard Visual processing of contour patterns under conditions of inattentional blindness J. Cogn. Neurosci. 24 2 2012 287 303 [48] N. Rehman D.P. Mandic Empirical mode decomposition for trivariate signals IEEE Trans. Signal Process. 58 3 2010 1059 1068 [49] N. Rehman D.P. Mandic Multivariate empirical mode decomposition Proc. Roy. Soc. A 466 2010 1291 1302 [50] N. Rehman D.P. Mandic Quadrivariate empirical mode decomposition International Joint Conference on Neural Networks (IJCNN 2010) 2010 1 7 [51] G. Rilling P. F1andrin P. Goncalves J.M. Lilly Bivariate empirical mode decomposition IEEE Signal Process. Lett. 14 2007 936 939 [52] P.R. Roelfsema Cortical algorithms for perceptual grouping Ann. Rev. Neurosci. 29 2006 203 227 [53] A. Rojas J.M. Górriz J. Ramírez I.A. Ill’an F.J. Martínez Murcia anmd A. Ortiz M. Gómez Río M. Moreno Caballero Application of empirical mode decomposition (EMD) on DaTSCAN SPECXT images to explore Parkinson disease Expert Syst. Appl. 40 2013 2756 2766 [54] Ch. Rorden Mricro 2012 available at [55] Armin Schnider Valerie Treyer Alfred Buck Selection of currently relevant memories by the human posterior medial orbitofrontal cortex J. Neurosci. 20 15 2000 5880 5884 [56] Bernhard Schölkopf Alexander J. Smola Learning with Kernels 2002 The MIT Press [57] M. Shpaner S. Molholm E. Forde J.J. Foxe Disambiguating the roles of area V1 and the lateral occipital complex (LOC) in contour integration NeuroImage 69 2013 146 156 [58] Stoet Gijsbert Lawrence H. Snyder Neural correlates of executive control functions in the monkey Trends Cogn. Sci. 13 5 2009 228 234 [59] T. Tanaka D.P. Mandic Complex empirical mode decomposition IEEE Signal Process. Lett. 14 2 2006 101 104 [60] T. Tanskanen J. Saarinen L. Parkkonen R. Hari From local to global: cortical dynamics of contour integration J Vis. 8 7 2008 15. 1-1512, [61] P. Vemuri J.L. Gunter M.L. Senjem J.L. Whitwell K. Kantarci D.S. Knopman B.F. Boeve R.C. Petersen Jr. C.R. Jack Alzheimer's disease diagnosis in individual subjects using structural MR images: validation studies NeuroImage 39 3 2008 1186 1197 [62] E. Webster Medical Instrumentation: Application and Design 1998 John Wiley & Sons, Inc. [63] Zh. Wu N.E. Huang Ensemble empirical mode decomposition: a noise-assisted data analysis method Adv. Adaptive Data Anal. 1 1 2009 1 41 [64] Zh. Wu N.E. Huang X. Chen The multidimensional ensemble empirical mode decomposition method Adv. Adaptive Data Anal. 1 2009 339 372 [65] Ch.-Zh. Xiong J.y. Xu J.-Ch. Zou D.-X. Qi Texture classification based on EMD and FFT J. Zhejiang Univ. – Sci. A 7 2006 1516 1521 10.1631/jzus.2006.A1516 [66] Y. Xu B. Liu J. Liu S. Riemenschneider Two-dimensional empirical mode decomposition by finite elements Proc. Roy. Soc. A 462 2006 3081 3096 [67] Omar Ait Zemzami Hamid Akasse Mohammed Ouanan Brahim Akasse Benkider Aziza Decomposition of 3D medical image based on fast and adaptive bidimenisional empirical mode decomposition Int. J. Comput. Networks Commun. Sec. 7 2013 299 309 [68] J.Z. Zhang Z. Qin Edge detection using fast Bidimensional Empirical Mode Decomposition and mathematical morphology Proceedings of the IEEE SoutheastCon (SoutheastCon) 2010 139 142 [69] T. Zheng M. Cai T. Jiang A novel approach to activation detection in fMRI based on empirical mode decomposition J. Integrative Neurosci. 9 4 2010 407 427 "
    },
    {
        "doc_title": "An ontology-based multi-level robot architecture for learning from experiences",
        "doc_scopus_id": "84883297909",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84883297909",
        "doc_date": "2013-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Eu projects",
            "High level semantics",
            "Learning from experiences",
            "Ontology-based",
            "Robot architecture",
            "Robot performance",
            "Service robots"
        ],
        "doc_abstract": "One way to improve the robustness and flexibility of robot performance is to let the robot learn from its experiences. In this paper, we describe the architecture and knowledge-representation framework for a service robot being developed in the EU project RACE, and present examples illustrating how learning from experiences will be achieved. As a unique innovative feature, the framework combines memory records of low-level robot activities with ontology-based high-level semantic descriptions. © 2013, Association for the Advancement of artificial intelligence.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Individual EEG differences in affective valence processing in women with low and high neuroticism",
        "doc_scopus_id": "84881095782",
        "doc_doi": "10.1016/j.clinph.2013.03.026",
        "doc_eid": "2-s2.0-84881095782",
        "doc_date": "2013-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Sensory Systems",
                "area_abbreviation": "NEUR",
                "area_code": "2809"
            },
            {
                "area_name": "Neurology",
                "area_abbreviation": "NEUR",
                "area_code": "2808"
            },
            {
                "area_name": "Neurology (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2728"
            },
            {
                "area_name": "Physiology (medical)",
                "area_abbreviation": "MEDI",
                "area_code": "2737"
            }
        ],
        "doc_keywords": [
            "Affective valence",
            "EEG",
            "Individual differences",
            "Neuroticism",
            "Single-trial",
            "SVM"
        ],
        "doc_abstract": "Objective: In this study, individual differences in brain electrophysiology during positive and negative affective valence processing in women with different neuroticism scores are quantified. Methods: Twenty-six women scoring high and low on neuroticism participated on this experiment. A support vector machine (SVM)-based classifier was applied on the EEG single trials elicited by high arousal pictures with negative and positive valence scores. Based on the accuracy values obtained from subject identification tasks, the most distinguishing EEG channels among participants were detected, pointing which scalp regions show more distinct patterns. Results: Significant differences were obtained, in the EEG heterogeneity between positive and negative valence stimuli, yielding higher accuracy in subject identification using negative pictures. Regarding the topographical analysis, significantly higher accuracy values were reached in occipital areas and in the right hemisphere (p < 0.001). Conclusions: Mainly, individual differences in EEG can be located in parietooccipital regions. These differences are likely to be due to the different reactivity and coping strategies to unpleasant stimuli in individuals with high neuroticism. In addition, the right hemisphere shows a greater individual specificity. Significance: An SVM-based classifier asserts the individual specificity and its topographical differences in electrophysiological activity for women with high neuroticism compared to low neuroticism. © 2013 International Federation of Clinical Neurophysiology.",
        "available": true,
        "clean_text": "serial JL 272006 291210 291703 291736 291834 291905 31 Clinical Neurophysiology CLINICALNEUROPHYSIOLOGY 2013-05-06 2013-05-06 2014-09-09T09:45:40 S1388-2457(13)00277-0 S1388245713002770 10.1016/j.clinph.2013.03.026 S300 S300.2 FULL-TEXT 2015-05-15T06:09:51.661076-04:00 0 0 20130901 20130930 2013 2013-05-06T00:00:00Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref 1388-2457 13882457 true 124 124 9 9 Volume 124, Issue 9 17 1798 1806 1798 1806 201309 September 2013 2013-09-01 2013-09-30 2013 Psychophysiology and Psychopathology article fla Copyright © 2013 International Federation of Clinical Neurophysiology. Published by Elsevier Ireland Ltd. All rights reserved. INDIVIDUALEEGDIFFERENCESINAFFECTIVEVALENCEPROCESSINGINWOMENLOWHIGHNEUROTICISM HIDALGOMUNOZ A 1 Introduction 2 Methods 2.1 Participants 2.2 Procedure 2.2.1 Stimuli 2.2.2 EEG recording 2.3 Data analysis 2.3.1 SVM classification 2.3.2 Cross validation 2.3.3 Implementation 2.3.4 Individual differences 2.3.5 Statistical comparisons 3 Results 3.1 Neuroticism and valence analysis 3.2 Topographical analysis 3.2.1 Front-Occipital location 3.2.2 Laterality 3.2.3 Most relevant EEG channels 4 Discussion 5 Conclusions Acknowledgements References AFTANAS 2001 115 118 L ALDHAFEERI 2012 94 98 F AMIN 2004 15 23 Z BAI 2007 2637 2655 O BENHUR 2008 e1000173 A BERAHA 2012 e46931 E BRITTON 2007 263 267 J BROWN 2013 93 99 K BURGES 1998 121 167 C BUSS 1984 1135 1147 D CARRETIE 2001 1109 1128 L COAN 2006 198 207 J DELPLANQUE 2004 1 4 S DENISSEN 2008 1285 1302 J DIS 1979 87 94 H DUUNHENRIKSEN 2012 84 92 J EYSENCK 1967 H BIOLOGICALBASISPERSONALITY FJELL 2005 493 502 A FRANTZIDIS 2010 589 597 C GIBSON 1991 29 63 K BRAINMATURATIONCOGNITIVEDEVELOPMENT MYELINATIONBEHAVIORALDEVELOPMENTACOMPARATIVEPERSPECTIVEQUESTIONSNEOTENYALTRICIALITYINTELLIGENCE GRATTON 1993 468 484 G GUNTEKIN 2010 173 178 B GUYON 2002 389 422 I HAGEMANN 2004 157 182 D HAMANN 2004 233 238 S HELLER 1993 476 489 W JAUSOVEC 2007 215 224 N KEMP 2004 632 646 A LITHARI 2010 27 40 C LOPEZ 2013 44 51 M LOPEZ 2009 233 238 M MARTINI 2012 922 932 N MERIAU 2009 73 80 K NORRIS 2007 823 826 C OCHSNER 2004 483 499 K OLOFSSON 2008 247 265 J ORMEL 2013 59 72 J PARASURAMAN 2012 70 82 R SCHMIDTKE 2004 717 732 J STEMMLER 2010 541 551 G STRIEN 2009 223 235 J TAN 2006 P INTRODUCTIONDATAMINING TOGA 2006 952 966 A WANG 2010 1195 1268 X WRIGHT 2006 1809 1819 C HIDALGOMUNOZX2013X1798 HIDALGOMUNOZX2013X1798X1806 HIDALGOMUNOZX2013X1798XA HIDALGOMUNOZX2013X1798X1806XA item S1388-2457(13)00277-0 S1388245713002770 10.1016/j.clinph.2013.03.026 272006 2014-09-15T08:09:10.294248-04:00 2013-09-01 2013-09-30 true 782957 MAIN 9 48871 849 656 IMAGE-WEB-PDF 1 si9 202 11 14 si8 518 19 109 si7 246 12 22 si6 238 12 21 si50 1361 21 309 si5 216 13 14 si49 1358 21 310 si48 1372 21 317 si47 2037 19 523 si46 1341 18 318 si45 383 17 57 si44 1413 21 317 si43 2080 18 552 si42 1307 22 302 si41 2109 20 549 si40 1367 21 310 si4 144 11 13 si39 1354 18 318 si38 453 17 81 si37 2883 22 775 si36 1911 18 505 si35 400 16 62 si34 1927 18 503 si33 327 13 47 si32 406 13 62 si31 246 12 22 si30 238 12 21 si3 144 11 13 si29 239 13 39 si28 388 15 76 si27 229 13 19 si26 555 21 99 si25 199 9 13 si24 243 12 22 si23 246 12 22 si22 171 8 10 si21 238 12 21 si20 141 7 8 si2 424 16 68 si19 371 17 67 si18 243 12 22 si17 2158 44 408 si16 183 8 11 si15 442 16 108 si14 199 9 13 si13 661 20 148 si12 318 17 35 si11 1212 21 302 si10 203 12 13 si1 true 424 16 68 gr9 424961 1591 2115 gr8 374411 1623 2123 gr7 255597 1128 2229 gr6 241088 1125 2231 gr5 384559 1132 2232 gr4 427840 1169 2233 gr3 264513 1291 3130 gr2 445677 1309 3098 gr1 244244 1433 1581 gr9 54020 359 477 gr8 47163 366 479 gr7 33068 255 503 gr6 33207 254 504 gr5 49963 256 504 gr4 51502 264 504 gr3 36462 292 707 gr2 54679 296 700 gr1 35271 324 357 gr9 7870 164 218 gr8 7268 164 214 gr7 4561 111 219 gr6 4711 110 219 gr5 6389 111 219 gr4 6568 115 219 gr3 3528 90 219 gr2 4820 93 219 gr1 7629 164 181 CLINPH 2006665 S1388-2457(13)00277-0 10.1016/j.clinph.2013.03.026 International Federation of Clinical Neurophysiology Fig. 1 Electrodes cap scheme. Channels inside red square were used in front-occipital location analysis. Channels inside blue square were used in laterality analysis. Fig. 2 Examples of single trials recorded in Oz channel from two different participants from the two experimental conditions. Low neuroticism scores and negative valence (upper left), high neuroticism scores and negative valence (upper right), low neuroticism scores and positive valence (down left) and high neuroticism scores and positive valence (down right). Fig. 3 SVM schemes using different C values. Fig. 4 Mean accuracy values for every EEG channel in high neuroticism (HN group) per condition. Fig. 5 Mean accuracy values for every EEG channel in low neuroticism (LN group) per condition. Fig. 6 Global accuracy values, averaging all EEG channel results, for every participant from high neuroticism (HN group) per condition. Fig. 7 Global accuracy values, averaging all EEG channel results, for every participant from low neuroticism (LN group) per condition. Fig. 8 Grand average from event related potentials (ERPs) from Fp1 channel. Positive valence (upper), negative valence (down). Fig. 9 Grand average from event related potentials (ERPs) from Pz channel. Positive valence (upper), negative valence (down). Individual EEG differences in affective valence processing in women with low and high neuroticism A.R. Hidalgo-Muñoz a ⁎ A.T. Pereira b c M.M. López d A. Galvao-Carmona a A.M. Tomé e M. Vázquez-Marrufo a I.M. Santos b c a Department of Experimental Psychology, University of Seville, 41018 Seville, Spain Department of Experimental Psychology University of Seville 41018 Seville Spain b Department of Education, University of Aveiro, 3810-193 Aveiro, Portugal Department of Education University of Aveiro 3810-193 Aveiro Portugal c IBILI-Institute of Biomedical Research in Light and Image, Faculty of Medicine, University of Coimbra, Coimbra, Portugal IBILI-Institute of Biomedical Research in Light and Image Faculty of Medicine University of Coimbra Coimbra Portugal d IEETA, University of Aveiro, 3810-193 Aveiro, Portugal IEETA University of Aveiro 3810-193 Aveiro Portugal e DETI/IEETA, University of Aveiro, 3810-193 Aveiro, Portugal DETI/IEETA University of Aveiro 3810-193 Aveiro Portugal ⁎ Corresponding author. Address: Psychophysiology Lab, Department of Experimental Psychology, University of Seville, C/ Camilo José Cela s/n, C.P. 41018 Sevilla, Spain. Tel.: +34 954556941. Objective In this study, individual differences in brain electrophysiology during positive and negative affective valence processing in women with different neuroticism scores are quantified. Methods Twenty-six women scoring high and low on neuroticism participated on this experiment. A support vector machine (SVM)-based classifier was applied on the EEG single trials elicited by high arousal pictures with negative and positive valence scores. Based on the accuracy values obtained from subject identification tasks, the most distinguishing EEG channels among participants were detected, pointing which scalp regions show more distinct patterns. Results Significant differences were obtained, in the EEG heterogeneity between positive and negative valence stimuli, yielding higher accuracy in subject identification using negative pictures. Regarding the topographical analysis, significantly higher accuracy values were reached in occipital areas and in the right hemisphere ( p < 0.001 ). Conclusions Mainly, individual differences in EEG can be located in parietooccipital regions. These differences are likely to be due to the different reactivity and coping strategies to unpleasant stimuli in individuals with high neuroticism. In addition, the right hemisphere shows a greater individual specificity. Significance An SVM-based classifier asserts the individual specificity and its topographical differences in electrophysiological activity for women with high neuroticism compared to low neuroticism. Keywords Affective valence EEG Individual differences Neuroticism Single-trial SVM 1 Introduction Nowadays, individual differences analysis in human beings is a field which receives more and more attention from interdisciplinary approaches, from the more classical psychological approaches like the studies of Eysenck (1967), Dis et al. (1979) or Buss (1984) to the modern studies about genetic and neuroscience (Parasuraman and Jiang, 2012; Toga et al., 2006). Nonetheless, few works research on the individual differences within a determined personality factor. The importance of quantifying the individual differences influence is well known in any clinical and psychophysiological context (Stemmler and Wacker, 2010). For instance, an accurate diagnosis of personality or emotional disorders or measuring correctly how deep a trauma impact is become important issues to guarantee later therapies effectiveness. However, the huge variability in the spectrum of the emotions in human beings, due to their genetic contribution and their experiences throughout their lives, makes difficult to get general patterns from the emotional processes. It is necessary to find out which characteristics are the most relevant depending on the application goal. Many research works on personality aim to find the relations between personality types and their specific behaviours (Amin et al., 2004), while other studies intend to discover the underlying anatomical structures or neurophysiological processes from diverse profiles (Britton et al., 2007; Ormel et al., 2013). In this study, the individual differences reflected in the EEG during affective valence processing are analyzed in two populations of women with different tendency to neuroticism. Affective valence is one of the main axis from the dimensional model of emotions (Heller, 1993; Lang et al., 2008; Olofsson et al., 2008) and refers to the pleasure or displeasure elicited by a stimulus. On the other hand, neuroticism is defined as a personality trait characterised by a predisposition to experience negative emotions like anxiety, frustration, stress or anger and is evaluated on a continuum that varies between emotional stability and instability (Denissen and Penke, 2008). This personality trait is associated with a negative emotional bias which generally leads to rapid detection and reaction to negative stimulus (Wright et al., 2006; Norris et al., 2007). Electroencephalography is a non invasive technique which can be explored to check individual specificity for better understanding the neurophysiological processes linked to neuroticism and emotions in general. Research works dealing with neuroimage and electrophysiology-based techniques have demonstrated that event related potentials (ERPs) are modulated by the personality type, specifically by neuroticism (Fjell et al., 2005; Georgiev et al., 2008; Jausovec and Jausovec, 2007). However, some of the results reported in the literature can be confusing due to the mentioned inter-individual heterogeneity since these peculiarities go accompanied by diverse affective reactions (Coan et al., 2006; Hagemann, 2004). Therefore, it is convenient to study neurophysiological individual differences merging these two concepts (emotion and neuroticism) with different emotional stimuli (Coan et al., 2006; Hamann and Canli, 2004). Approaches which take into account the inherent individual influence in EEG signals taking them as whole entities have been hardly developed. Some components from the averaged ERPs have been widely investigated both in time domain (Aftanas et al., 2001; Carretie et al., 2001; Delplanque et al., 2004) and frequency domain (Güntekin and Basar, 2010; Herrmann, 2005; Wang, 2010) without considering any pattern from the original single trials. ERP computing is very useful in the field of the psychophysiology and for selecting the most relevant EEG channels (Duun-Henriksen et al., 2012), but the increasing research works on online applications, such as brain computer interface (BCI) for medical, therapeutic or recreational purposes, make a reliable single trial processing more and more needed (Bai et al., 2007). Single trials give valuable information that could be lost after averaging for computing the ERP, although their processing is harder and it is more difficult to reach so good results (Schuster et al., 2012). Thus, it is important to expound new manageable ideas to extract relevant information from single trials. In this work, a classification technique is employed to study individual differences, instead of other traditional statistical methods. This approach allows tackling the digital EEG signals processing from a new point of view, taking each single trial as a complete set of features and representing them in a multidimensional space, where two classes can be split. This fact provides an advantageous and practical methodology and provides valuable information apart from other mathematical parameters like Pearson’s correlation coefficient, standard deviation or similar measures of variance. The core of the method lies in making up two classes based on the experimental conditions (high neuroticism vs low neuroticism and negative valence vs positive valence) and quantifying, by means of the classifier accuracy (acc), how easy identifying a concrete subject is. The more accurate the classifier is, the more particular the EEG signal will be, so that the presence of an individual difference would be obvious. Numerous algorithms and methods for validation, feature extraction and feature selection are described in the literature dealing with classifiers or machine learning (Guyon et al., 2002; Tan et al., 2006). In this article, a linear support vector machine (SVM)-based classifier is implemented (Burges, 1998). There are various reasons for this choice. In the first place, SVM has demonstrated to give excellent results in different neuroscience applications (Lopez et al., 2009; Lopez et al., 2013). Secondly, although the complete design could be quite complex due to the computing of some optimization factors, the main bases are easily understandable. Finally, the main idea of this technique, consisting of demarcating a hyperplane that separates both classes, is essential to obtain suitable conclusions about the existing individual differences in a specific experimental group (see Section 2.3.1). There is evidence that women show different patterns of brain activation from men in response to emotional stimuli (Kemp et al., 2004). Women typically score higher in neuroticism scale than men and women with high neuroticism are more reactive to negative visual stimulation compared to those who score low in the same scale (Hamann and Canli, 2004; Lithari et al., 2010; Strien et al., 2009). Therefore only female participants were chosen in this experiment to guarantee higher homogeneity of the data and avoid gender differences. 2 Methods 2.1 Participants Firstly, the initial sample was composed of 164 healthy female volunteers, who participated in the study responding to the neuroticism scale of the revised version of the NEO Personality Inventory (NEO PI-R) (Portuguese version) (Costa and McCrae, 2000). A general explanation and instructions were given to all the volunteers in a classroom. Twenty-six participants were finally selected for completing the laboratory study (age 18–62years; mean=24.19; sd=10.46), after sorting all the initial sample of 164 participants on the basis of their neuroticism score. Two groups, statistically different regarding their score ( p < 0.001 ) were constituted: • High neuroticism (HN group): The 13 women (belonging to the initial sample of 164 participants) with highest scores in neuroticism from the initial distribution (mean=62.15; sd=15.48) according to the NEO PI-R neuroticism scale. • Low neuroticism (LN group): The 13 women (belonging to the initial sample of 164 participants) with lowest scores in neuroticism from the initial distribution (mean=125.08; sd=8.95) according to the NEO PI-R neuroticism scale. All participants had normal or corrected to normal vision and none of them had a history of severe medical treatment, neither psychological nor neurological disorders. A signed informed consent was obtained from each participant before carrying out the experiment. This study was approved in accordance with the Declaration of Helsinki. 2.2 Procedure Each one of the selected participants was comfortably seated at 70cm from a computer screen (17′), alone in an enclosed room. The volunteer was instructed to visualize some pictures, which appeared on the center of the screen, and to stay quiet. No responses were required. 2.2.1 Stimuli The pictures were chosen from the International Affective Picture System (IAPS) (Lang et al., 2008), widely used in psychology research works (Aldhafeeri et al., 2012; Martini et al., 2012; Frantzidis et al., 2010). A total of 24 images with high arousal ratings (>6) were selected, 12 of them with positive affective valence (7.29 ± 0.65) and the other 12 with negative affective valence (1.47 ± 0.24). In order to match as closely as possible the levels of arousal between positive and negative valence stimuli, only high arousal pictures were presented, avoiding neutral stimuli. Three blocks with the same 24 images were presented consecutively. The picture order in each block was pseudo random to avoid expectancy phenomena. In each trial a fixation single cross was presented on the center of the screen during 750ms, then, one image was presented during 500ms and finally a black screen during 2250ms (total duration=3500ms). 2.2.2 EEG recording EEG activity on the scalp was recorded from 21 Ag/AgCl sintered electrodes (Fp1, Fpz, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, P8, O1, Oz, O2) mounted on an electrode cap from EasyCap according to the international 10/20 system (see Fig. 1 ), internally referenced to an electrode on the tip of the nose. The impedances of all electrodes were kept below 5k Ω . EEG signals were recorded, sampled at 1kHz and pre-processed using software Scan 4.3 (Compumedics Neuroscan, Germany). Firstly, a notch filter centered in 50Hz was applied to eliminate AC contribution. EEG signals were then filtered using a Butterworth passband filter from 0.1Hz to 30Hz. Artifacts rejection was performed based on amplitude values, following a visual inspection of all signals and rejecting epochs that presented clear artifacts. Regarding the ocular correction a semi-automatic method is applied by following the procedure described in software package Scan 4.3 (Compumedics Neuroscan, Germany). The method employs a regression analysis in combination with artifact averaging to produce a reliable model of the VEOG signal to be subtracted from the EEG channels (Gratton et al., 1993). Finally, the signals are segmented into time locked epochs using the stimulus onset (picture presentation) as reference and baseline corrected. The length of the time windows was 950ms: from 150ms before picture onset to 800ms after it (baseline=150ms). Fig. 2 shows examples of EEG single trials. 2.3 Data analysis 2.3.1 SVM classification A SVM-based classifier separates a given set of binary labeled training data with a hyperplane, known as the maximal margin hyperplane, which is maximally distant from the two classes ( ω 1 and ω 2 ). The objective is to build a function f : R M ⟶ { ± 1 } using training data, that is, M-dimensional patterns x i and class labels y i : (1) ( x 1 , y 1 ) , ( x 2 , y 2 ) , … , ( x S , y S ) ∈ R M × { ± 1 } , so that f will correctly classify new examples ( x , y ) . S is the number of samples in the database. Linear discriminant functions define the decision hyperplanes, which separates both classes, in a multidimensional feature space: (2) g ( x ) = w T x + b = 0 , where w is known as the weight vector and b as the threshold. The optimization task to design the classifier consists of finding the unknown parameters ( w i , i = 1 , … , M , and b), which separate the two classes optimally. Given a new element x , the predicted label is then: g ( x ) = w T x + b ⇒ g ( x ) > 0 x ∈ ω 1 ⇔ label ≡ 1 g ( x ) < 0 x ∈ ω 2 ⇔ label ≡ - 1 Fig. 3 illustrates a 2 D toy-example of a binary classification problem, where the points x = [ x 1 x 2 ] marked as ∘ belong to class ω 1 and the ones marked as × belong to class ω 2 . The problem is not linearly separable because it is not possible to find a hyperplane (line, in 2 D case) that separates all training instances of the two classes. However, if a small number of misclassifications is tolerated, the problem remains linearly separable. The figure shows the result of two training sessions with the same data but different margins. The decision hyperplane, in all examples, is represented by the thicker line which is in the middle of two other hyperplanes whose distance characterize the margin of the classifier. The position of the decision hyperplane is determined by vector w and b: the vector is orthogonal to the decision plane and b determines its distance to the origin. The vector w = ∑ i N s y i λ i x i is a weighted sum of the support vectors which are the N s elements of training set chosen during the training phase inside the margin or misclassified. In the Fig. 3 these support vectors are marked with circles around the training data points. And 0 < λ i < C are the corresponding Lagrangian parameters which are also optimized during training. Finally, the value of the threshold b is estimated by solving the equations related to the hyperplanes that define the margin. In (Ben-Hur et al., 2008) an extensive algebraic explanation of SVM, applied to biological sciences, is reported. The value of C needs to be assigned to run the training optimization algorithm. It is a parameter that indirectly controls the width of the margin of the classifier (see Fig. 3). However, during the optimization process C represents the weight of the penalty term of the optimization function that is related with the misclassification error in the training set. Therefore, the optimization determines the trade-off between the width of the margin and the number of accepted misclassifications. There is no optimal procedure to assign this parameter but it has to be expected that: • If C is large, the misclassification errors are relevant during optimization. A narrow margin has to be expected. • If C is small, the misclassification errors are not relevant during the optimization. A large margin has to be expected. 2.3.2 Cross validation The SVM classifier was evaluated using the leave-one-out (LOO) cross validation strategy that consists of using all the samples in the dataset for training the system except one, which is used as test. This procedure is repeated S times, being S the number of samples in the dataset, after which a global value of accuracy is computed. 2.3.3 Implementation All the routines needed to apply the completed method were implemented in the software MATLAB (version R2011a, Mathworks, USA). In particular, tools from SVM-package were required. The internal parameter C was kept by default on C = 1 . No special optimization process was required, since the aim of this study is to compare the accuracy values of the subject identification tasks, rather than designing any complex classifier. The LOO strategy was also implemented in MATLAB in order to get automatically the global accuracy. 2.3.4 Individual differences The classification technique was applied on binary subject identification tasks. The classification tasks consisted of taking all possible pairs of two different subjects belonging to the same group (HN group or LN group) in similar condition (negative or positive valence) and carrying out binary classifications for discriminating the identity of the subject from one test trial. Therefore, the two classes ω 1 and ω 2 were constituted by single trials of each subject, respectively, both of the same valence condition. The maximal number of possible couples combinations by condition is equal to 78 (i.e. N (N −1)/2 pairs, where N is the number of participants in each group (N =13)). Therefore 78 binary classifiers (pair-wise distinctions) are trained. In the complete set of test there are always 12 evolving decisions related to a particular participant, resulting from the comparison with the other 12 participants of the same group. The average of accuracies of the 12 classifiers related to a person gives an indication of how the person is distinct from the others. As it was mentioned before, the capability to discriminate was measured by means of a linear SVM classifier (see Section 2.3.1) and assessed by LOO strategy (Section 2.3.2). Single trials are directly used as input for the classifier, taking the time instants from 0ms to 800ms post-stimulus (i.e. a total of M = 800 samples by trial). In each single classification task, a total of 30 artifact free single trials were selected per condition for each participant, so that S = 60 in each of these classification tasks. The higher the accuracy rates, the more distinguishable the subjects are between them. Therefore, higher accuracy values imply greater contribution to individual differences. In contrast, accuracy values close to 50% imply that the two classes are hardly distinguishable and may not be properly separated using a hyperplane due to an overlap in the M-dimensional space. This overlap can be interpreted as a high stability in the evoked potentials among participants or as a result of an extreme variability intra-subject. Then, high accuracy results represent peculiar patterns in EEG signals, whereas low accuracy values are related to a more general pattern. 2.3.5 Statistical comparisons Analysis of variance (ANOVA) and t-tests were used for computing statistical significance for the pertinent intra-group and inter-group comparisons. Mauchy’s sphericity test was performed on the analysis and the Greenhouse–Geisser correction was applied on the degrees of freedom when sphericity could not be assumed. Bonferroni correction has been conveniently applied in post hoc analyses when multiple comparisons have been carried out. ANOVAs were taking the 78 accuracy values obtained as output of the classifiers from the binary classifications in both groups (factor 1: neuroticism; two levels: HN and LN) in the two experimental conditions (factor 2: affective valence; two levels: negative (−) and positive (+)). Additional factors were defined for topographical analysis (front-occipital location and left–right location) with five levels each one. 3 Results 3.1 Neuroticism and valence analysis Figs. 4 and 5 show the averaged accuracies for each experimental condition. Significant differences were not found between groups ( F ( 1 , 154 ) = 1.484 ; p = 0.225 ; η 2 = 0.01 ; acc HN = 74.15 % ; acc LN = 75.07 % ). However, significant differences ( p < 0.05 ) were obtained by comparing both affective valence conditions within participants ( F ( 1 , 154 ) = 16.161 ; p < 0.001 ; η 2 = 0.095 ; acc - = 75.45 % ; acc + = 73.77 % ). Also, a significant valence*neuroticism interaction was found ( F ( 1 , 154 ) = 9.363 ; p = 0.003 ; η p 2 = 0.057 ; acc HN - = 75.63 % ; acc HN + = 72.66 % ; acc LN - = 75.27 % ; acc LN + = 74.87 % ). Figs. 6 and 7 show the global accuracies per condition for each participant, averaging the accuracy values from the 12 binary classifications task related to one person (Section 2.3.4) and from all the EEG channels. Note that in HN group only two participants obtained a slightly higher accuracy value with positive than with negative valence. 3.2 Topographical analysis In order to complete a more detailed topographical study on the individual differences, two different ANOVAs were performed on two different subsets from the complete database. The selected EEG channels for each analysis are delimited in Fig. 1. 3.2.1 Front-Occipital location In order to study the relevance of the EEG channels depending on the front-occipital axis position, besides the previous defined factors (see Section 2.3.5) a third factor was added to the ANOVA (factor 3 front - occipital : front-occipital location; five levels: prefrontal, frontal, central, parietal and occipital). As Fig. 1 shows, each level is composed of three EEG channels centered in the middle of the scalp region. A clear and statistically significant influence of this factor was confirmed ( F ( 2.729 , 152.3 ) = 197.5 ; p < 0.001 ; η 2 = 0.562 ). Parietal and occipital areas showed higher accuracy values than more frontal regions (see Figs. 4 and 5). Some interactions were significant, as location*neuroticism ( F ( 2.729 , 152.3 ) = 6.72 ; p < 0.001 ; η p 2 = 0.042 ), being more evident the influence of the EEG channels position in the LN group ( acc HN prefront = 70.5 % ; acc HN occip = 83.75 % ; acc LN prefront = 67.79 % ; acc LN occip = 86.87 % ), and location*valence interaction ( F ( 3.305 , 151.7 ) = 3.16 ; p = 0.021 ; η p 2 = 0.02 ) ( acc prefront + = 68.52 % ; acc prefront - = 69.77 % ; acc occip + = 84.6 % ; acc occip - = 86.02 % ). On the other hand, interaction location*neuroticism*valence was not proved ( F ( 3.305 , 151.7 ) = 0.278 ; p = 0.859 ; η p 2 = 0.002 ). 3.2.2 Laterality In addiction, the laterality influence was studied. In this case, the third factor was defined according to the left–right location (factor 3 left - right : left–right location) and a total of five levels were defined from left to right sides, where the middle level corresponds to the central position. Each level groups three EEG channels (see Fig. 1). A significant effect from this factor was evidenced ( F ( 2.752 , 152.2 ) = 10.73 ; p < 0.001 ; η 2 = 0.065 ). Generally, right hemisphere region showed higher accuracy values than left hemisphere, specially in HN group ( acc HN left = 70.69 % ; acc HN right = 74.78 % ; acc LN left = 73.61 % ; acc LN right = 74.46 % ) which was statistically proved with the laterality*neuroticism interaction ( F ( 2.752 , 152.2 ) = 5.062 ; p = 0.003 ; η p 2 = 0.032 ). The laterality*valence interaction ( F ( 3.136 , 151.9 ) = 0.945 ; p = 0.422 ; η p 2 = 0.06 ) and laterality*neuroticism*valence interaction ( F ( 3.136 , 151.9 ) = 1.448 ; p = 0.227 ; η p 2 = 0.09 ) were not significant. 3.2.3 Most relevant EEG channels Finally, it is interesting to point out the EEG channels which reach the maximal and minimal accuracy peaks, since they will be important to determine global neurophysiological patterns or more particular electrophysiological features dependent on the specific application goal. Thus, the EEG channel that yields the highest accuracy values is Oz in 46% of the participants (see examples from single trials recorded from this electrode in Fig. 2), whereas the EEG channels that give the lowest accuracy values are located in the left front-temporal region (i.e. Fp1, F7 and T7). Fig. 8 shows the grand average of the ERP from Fp1. The signals recorded with Fp1 channel are the most similar among participants in affective valence processing, according to the obtained accuracy values in both groups and both valences. 4 Discussion The first question that has to be answered is if the women with high neuroticism scores differ among them, more, less or similarly compared to the group composed of women with low neuroticism scores. According to the obtained results (see Section 3.1) the individual differences contribution is equivalent in both populations. Nonetheless, it is necessary to analyze some interaction effects that can show more specific differences in the pattern from any delimited scalp region. It is important to stress that these results do not mean that participants that score high in neuroticism show identical patterns to stable participants while they are visualizing pictures, but within both groups the general level of their own patterns is the same, showing high within-group homogeneity. As far as we are aware, there are not similar previous research works which study the individual differences with this aim. In regard to the affective valence, the results suggest that a more generic pattern exists when positive visual stimuli are processed. The emotional processing of a picture with negative valence elicits more varied brain responses among different people. This might be due to the interaction of particular traumas or bad experiences and personality characteristics, which can have a remarkable influence on the reactions to negative stimuli for a better environmental adaptation. One possible explanation to these individual differences can be related to different individual forms of reappraisal that subjects might use when confronted with aversive stimuli (Ochsner et al., 2004). Another possibility can be related to individual differences in state negative affect associated with an increase of the left insular activity, which has been suggested to be implicated in interoceptive processes that contribute to subjective emotional experience (Mériau et al., 2009). Overall, the evidence seems to indicate that the neural activity elicited by viewing negative pictures depends both on biological factors and on the individual’s personal characteristics and histories, which is compatible with our findings of higher inter-individual variability in response to negative valence stimuli. The neuroticism*valence interaction suggests different strategies for negative stimuli processing in women with high neuroticism. This result strongly confirms the conclusion explained in the previous paragraph specially for participants that score high in neuroticism. The greater reactivity of people with high neuroticism (Wright et al., 2006; Norris et al., 2007) makes them more prone to vary their cognitive skills, behaviours and underlying neurophysiology, becoming more pronounced their individual differences in negative contexts. Compatible with this idea, a recent study has shown that neural activity at the early phases of affective processing can be modulated by personal traits, such as neuroticism or mindfulness, being related to more difficulties in emotion regulation or more healthy emotional functioning, respectively (Brown et al., 2013). One of the most interesting conclusions extracted from this study is the topographical distribution of the individual differences (Section 3.2 and Figs. 4 and 5). In general, for every participant from both groups (LN group and HN group) and for any valence, the most remarkable inter-individual differences exist in occipital and parietal regions. These regions are mainly linked to perceptive processes in visual cortex, although some findings claim the implication of posterior areas in affective processing, specifically with high neuroticism (Heller, 1993; Schmidtke and Heller, 2004). On the other hand, the unequal individual heterogeneity, across the scalp, likely could be influenced by the myelination development. The interaction between the location in the rostrocaudal brain axis and the neuroticism hints that the individual differences are wider spread all over the scalp in LN group than in HN group. As it is known, myelination progression covers the rostrocaudal axis from posterior to anterior brain areas and lasts several years (Gibson, 1991). Therefore, whereas frontal regions depend notably on the personal experiences, parietal and occipital regions might preserve a more evident and distinguisable genetic brain signature. Thus, the higher susceptibility to personal experiences attributed to frontal regions could lead to higher intra-individual variability, since it is more likely that individuals will respond differently to different stimuli, based on their individual experiences with that stimulus, situation, etc. This higher intra-individual variability would lead to a lower classification accuracy (see Section 2.3.4), since the various trials of a particular individual would be less homogeneous, and thus it would be more difficult to distinguish between individuals. As a consequence, a lower classification accuracy would be expected in frontal areas, compared to posterior areas, which is what was found in the present study. Regarding the individual specificity in laterality considering which group a participant belongs to (HN or LN), it can be asserted that women with high neuroticism show a greater difference between hemispheres, showing a slight more generic pattern on the left scalp area, specially for frontal channels (see Fig. 4). This conclusion is extracted from the significant laterality*neuroticism interaction (Section 3.2.2). In many experiments dealing with emotional processes, an anterior EEG asymmetry has been widely analyzed, specially in alpha band (Coan et al., 2006; Hagemann, 2004). Although, this inter-hemisphere asymmetry has not been the target of this study, the statistical results mentioned previously from Section 3.2.2, partially suggest that the two brain hemispheres contribute differently to the affective valence processing specially in HN group. In the case of low neuroticism, the inter-hemisphere difference is not so evident, although a general left-lateralized negative valence processing has been suggested in some research works (Beraha et al., 2012). No hemisphere could be considered as clearly dominant in the affective valence processing based on the results from this study. This fact can be related to the lower reactivity elicited by negative stimuli in stable people compared to people that score high in neuroticism (Amin et al., 2004; Britton et al., 2007). However, a significantly higher accuracy is obtained from the middle EEG channel (Cz) with respect to its sides. Among all the employed EEG channels, it is worth pointing out two of them: Oz and T7. Oz is the channel which yields the highest accuracy values for identifying subjects. This result suggests that occipital area is one of the most suitable scalp region for determining particular contributions to the evoked potential’s shape in affective valence processing. In contrast, T7 is the least relevant EEG channel for such purpose, specially for HN group, i.e. T7 is the channel that would show a more generic pattern, probably influenced by the noise generated by the blood stream from the temples. Furthermore, according to the results, frontal EEG channels generally are the least relevant to investigate the individual differences contribution in emotional processing. Therefore, frontal areas could provide valuable information for inter-groups comparisons in research on emotion (see Fig. 8), although they are not the most used for computing the psychological components from ERPs. Comparing the similar shapes of the grand averages signals from Fig. 9 to the extremely different single trials taken as examples in Fig. 2, the usefulness of the classification technique for tackling single trial signals, where it is impossible to measure ERP components directly, is evident. In Fig. 9 the grand average of Pz channel is represented, where P3 component is often measured. While single trials do not allow a clear measurement of ERPs, grand averages do not allow drawing conclusions about individual differences, which is the strongest contribution of the method used in the present work. Determining the most relevant EEG channels where a greater individual contribution exists is essential for extracting reliable features from a patient. Moreover, in BCI devices and other apparatus for diagnosis or therapies it is very important to take into account the scalp regions where signals are more stable, either for obtaining accurate results or optimizing electronic resources. The larger the database for training the classifier is, the more reliable the conclusions will be. In future works it would be interesting to provide vast databases of different populations. In addition, this methodology can provide valuable information to applications which deal with other types of neurophysiological signals apart from electroencephalography, such as magnetoencephalography among others, in order to determine the individual differences existing in diverse pathologies or to quantify gender differences. 5 Conclusions In the present study, individual EEG differences on scalp regions have been identified, mostly in right hemisphere and parietooccipital areas in women who score high in neuroticism. Moreover, a higher homogeneity between individuals is clear with positive stimuli in stable people, whereas it is not so evident in the high neuroticism group, particularly with negative stimuli. The use of different emotion-regulatory strategies, such as reappraisal, and differential insular activity reflecting differences in state negative affect could be linked to the higher heterogeneity of individuals scoring high in neuroticism. This study points out that there may be important individual differences even in samples commonly studied as homogeneous groups, such as individuals scoring high in neuroticism. This fact should be carefully taken into account both in experimental or clinical studies, since variation between individuals is not always considered. Acknowledgements This work is partially funded by FEDER through the Operational Program Competitiveness Factors – COMPETE and by National Funds through FCT – Foundation for Science and Technology in the context of the project FCOMP-01-0124-FEDER-022682 (FCT reference PEst-C/EEI/UI0127/2011). References Aftanas et al., 2001 L. Aftanas A. Varlamow S. Pavlov V. Makhnev N. Reva Affective picture processing: event-related synchronization within individually defined human theta band is modulated by valence dimension Neurosci Lett 303 2001 115 118 Aldhafeeri et al., 2012 F.M. Aldhafeeri I. Mackenzie T. Kay J. Alghamdi V. Sluming Regional brain responses to pleasant and unpleasant IAPS pictures: different networks Neurosci Lett 512 2012 94 98 Amin et al., 2004 Z. Amin R. Constable T. Canli Attentional bias for valence stimuli as a function of personality in the dot-probe task J Res Pers 38 2004 15 23 Bai et al., 2007 O. Bai P. Lin S. Vorbach J. Li S. Furlani M. Hallet Exploration of computational methods for classification of movement intention during human voluntary movement from single trial EEG Clin Neurophysiol 118 2007 2637 2655 Ben-Hur et al., 2008 A. Ben-Hur C.S. Ong S. Sonnenburg B. Schölkopf G. Rätsch Support Vector Machines and Kernels for Computational Biology PLoS Comput Biol 4 2008 e1000173 10.1371/journal.pcbi.1000173 Beraha et al., 2012 E. Beraha J. Eggers C.H. Attar S. Gutwinski F. Schlagenhauf M. Stoy Hemispheric asymmetry for affective stimulus processing in healthy subjects – a fMRI study PLoS ONE 7 2012 e46931 10.1371/journal.pone.0046931 Britton et al., 2007 J. Britton S. Ho S. Taylor I. Liberzon Neuroticism associated with neural activation patterns to positive stimuli Psychiatry Res Neuroimaging 156 2007 263 267 Brown et al., 2013 K.W. Brown R.J. Goodman M. Inzlicht Dispositional mindfulness and the attenuation of neural responses to emotional stimuli Soc Cogn Affect Neurosci 8 2013 93 99 Burges, 1998 C. Burges A tutorial on support vector machines for pattern recognition Data Min Knowl Discov 2 1998 121 167 Buss, 1984 D.M. Buss Evolutionary biology and personality psychology Am Psychol 39 1984 1135 1147 Carretie et al., 2001 L. Carretié M. Martín-Loeches J. Hinojosa F. Mercado Emotion and attention interaction studied through event-related potentials J Cogn Neurosci 13 2001 1109 1128 Coan et al., 2006 J.A. Coan J.J. Allen P.E. McKnight A capability model of individual differences in frontal EEG asymmetry Biol Psychol 72 2006 198 207 Costa and McCrae, 2000 Costa P, McCrae R. NEO PI-R: Manual Profissional: NEO PI-R, Inventário de Personalidade NEO Revisto. Lisboa: CEGOC-TEA, Lda; 2000. Delplanque et al., 2004 S. Delplanque M.E. Lavoic P. Hot L. Silvert H. Sequeira Modulation of cognitive processing by emotional valence studied through event-related potentials in humans Neurosci Lett 356 2004 1 4 Denissen and Penke, 2008 J. Denissen L. Penke Motivational individual reaction norms underlying the five-factor model of personality: first steps towards a theory-based conceptual framework J Res Pers 42 2008 1285 1302 Dis et al., 1979 H.V. Dis M. Corner R. Dapper G. Hanewald H. Kok Individual differences in the human electroencephalogram during quiet wakefulness Electroencephalogr Clin Neurophysiol 47 1979 87 94 Duun-Henriksen et al., 2012 J. Duun-Henriksen T.W. Kjaer R.E. Madsen L.S. Remvig C.E. Thomsen H.B.D. Sorensen Channel selection for automatic seizure detection Clin Neurophysiol 123 2012 84 92 Eysenck, 1967 H.J. Eysenck The biological basis of personality 1967 Charles C. Thomas Fjell et al., 2005 A. Fjell K. Walhovd S. Meling M. Johansen Basic information processing of neurotics and stables: an experimental ERP approach to personality and distractibility Scand J Psychol 46 2005 493 502 Frantzidis et al., 2010 C.A. Frantzidis C. Bratsas C.L. Papadelis E. Konstantinidis C. Pappas P.D. Bamidis Toward emotion aware computing: an integrated approach using multichannel neurophysiological recordings and affective visual stimuli IEEE Trans Inf Technol Biomed 14 2010 589 597 Georgiev et al., 2008 Georgiev S, Minchev Z, Philipova D, Christova C. Time-frequency spectral differences in event-related potentials between neurotic and stable persons in human EEG. In: 21st international symposium bioprocess systems. Sofia, Bulgary; 2008. Gibson, 1991 K.R. Gibson Myelination and behavioral development: a comparative perspective on questions of neoteny, altriciality and intelligence K.R. Gibson A.C. Petersen Brain maturation and cognitive development 1991 Aldine de Gruyter New York 29 63 Gratton et al., 1993 G. Gratton M.G. Coles E. Donchin A new method for off-line removal of ocular artifact Electroencephalogr Clin Neurophysiol 55 1993 468 484 Güntekin and Basar, 2010 B. Güntekin E. Basar Event-related beta oscillations are affected by emotional eliciting stimuli Neurosci Lett 483 2010 173 178 Guyon et al., 2002 I. Guyon J. Weston S. Barnhill V. Vapnik Gene selection for cancer classification using support vector machines Mach Learn 46 2002 389 422 Hagemann, 2004 D. Hagemann Individual differences in anterior EEG asymmetry: methodological problems and solutions Biol Psychol 67 2004 157 182 Hamann and Canli, 2004 S. Hamann T. Canli Individual differences in emotion processing Curr Opin Neurobiol 14 2004 233 238 Heller, 1993 W. Heller Neuropsychological mechanisms of individual differences in emotion, personality, and arousal Neuropsychology 7 1993 476 489 Herrmann, 2005 Herrmann CS, Grigutsch M, Busch N. EEG oscillations and wavelet analysis. Event related potentials: a methods handbook. Handy. p. 229–59. Jausovec and Jausovec, 2007 N. Jausovec K. Jausovec Personality, gender and brain oscillations Int J Psychophysiol 66 2007 215 224 Kemp et al., 2004 A.H. Kemp R.B. Silberstein S.M. Armstrong P.J. Nathan Gender differences in the cortical electrophysiological processing of visual emotional stimuli Neuroimage 21 2004 632 646 Lang et al., 2008 Lang P, Bradley M, Cuthbert B. International affective picture system (IAPS): instruction manual and affective ratings. Technical Report A-8; 2008. Lithari et al., 2010 C. Lithari C. Frantzidis C. Papadelis A.B. Vivas M. Klados C. Kourtidou-Papadeli Are females more responsive to emotional stimuli? a neurophysiological study across arousal and valence dimensions Brain Topogr 23 2010 27 40 Lopez et al., 2013 M. López J. Górriz J. Ramírez M. Gómez-Río J. Verdejo J. Vas Component-based technique for determining the effects of acupuncture for fighting migraine using SPECT images Expert Syst Appl 40 2013 44 51 Lopez et al., 2009 M. López J. Ramírez J. Górriz I. Álvarez D. Salas-Gonzalez R. Chaves SVM-based CAD system for early detection of the alzheimer’s disease using kernel PCA and LDA Neurosci Lett 464 2009 233 238 Martini et al., 2012 N. Martini D. Menicucci L. Sebastiani R. Bedini A. Pingitore N. Vanello The dynamics of EEG gamma responses to unpleasant visual stimuli: From local activity to functional connectivity Neuroimage 60 2012 922 932 Mériau et al., 2009 K. Mériau I. Wartenburger P. Kazzer K. Prehn A. Villringer E. van der Meer Insular activity during passive viewing of aversive stimuli reflects individual differences in state negative affect Brain Cogn 69 2009 73 80 Norris et al., 2007 C. Norris J. Larsen J. Cacioppo Neuroticism is associated with larger and more prolonged electrodermal responses to emotionally evocative pictures Psychophysiology 44 2007 823 826 Ochsner et al., 2004 K.N. Ochsner R.D. Ray J.C. Cooper E.R. Robertson S. Chopra J.D. Gabrieli For better or for worse: neural systems supporting the cognitive down- and up-regulation of negative emotion Neuroimage 23 2004 483 499 Olofsson et al., 2008 J.K. Olofsson S. Nordin H. Sequeira J. Polich Affective picture processing: an integrative review of ERP findings Biol Psychol 77 2008 247 265 Ormel et al., 2013 J. Ormel J.A. Bastiaansen H. Riese E.H. Bos M. Servaas M. Ellenbogen The biological and psychological basis of neuroticism: current status and future directions Neurosci Biobehav Rev 37 2013 59 72 Parasuraman and Jiang, 2012 R. Parasuraman Y. Jiang Individual differences in cognition, affect, and performance: behavioral, neuroimaging, and molecular genetic approaches Neuroimage 59 2012 70 82 Schmidtke and Heller, 2004 J.I. Schmidtke W. Heller Personality, affect and EEG: predicting patterns of regional brain activity related to extraversion and neuroticism Pers Indivi Dif 36 2004 717 732 Schuster et al., 2012 Schuster T, Gruss S, Rukavina S, Walter S, Traue HC. EEG-based valence recognition: what do we know about the influence of individual specificity? In: 4th International conference on advanced cognitive technologies and applications. Nice, France; 2012. Stemmler and Wacker, 2010 G. Stemmler J. Wacker Personality, emotion, and individual differences in physiological responses Biol Psychol 84 2010 541 551 Strien et al., 2009 J.V. Strien S. Langeslag N. Strekalova L. Gootjes I. Franken Valence interacts with the early ERP old/new effect and arousal with the sustained ERP old/new effect for affective pictures Brain Res 1251 2009 223 235 Tan et al., 2006 P.N. Tan M. Steinbach V. Kumar Introduction to data mining 2006 Addison-Wesley Toga et al., 2006 A.W. Toga P.M. Thompson S. Mori K. Amunts K. Zilles Towards multimodal atlases of the human brain Nat Rev 7 2006 952 966 Wang, 2010 X.J. Wang Neurophysiological and computational principles of cortical rhythms in cognition Physiol Rev 90 2010 1195 1268 Wright et al., 2006 C. Wright D. Williams E. Feczko L. Barrett B. Dickerson C. Schwartz Neuroanatomical correlates of extraversion and neuroticism Cereb Cortex 16 2006 1809 1819 "
    },
    {
        "doc_title": "A new physarum learner for network structure learning from biomedical data",
        "doc_scopus_id": "84877966645",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84877966645",
        "doc_date": "2013-05-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Biomedical data",
            "LAGD Hill Climber",
            "Network structure learning",
            "Novel structures",
            "Pearson correlation coefficients",
            "Physarum",
            "Structure-learning",
            "Time efficiencies"
        ],
        "doc_abstract": "A novel structure learning algorithm for Bayesian Networks based on a Physarum Learner is presented. The length of the connections within an initially fully connected Physarum-Maze is taken as the inverse Pearson correlation coefficient between the connected nodes. The Physarum Learner then estimates the shortest indirect paths between each pair of nodes. In each iteration, a score of the surviving edges is incremented. Finally, the highest scored connections are combined to form a Bayesian Network. The novel Physarum Learner method is evaluated with different configurations and compared to the LAGD Hill Climber showing comparable performance with respect to quality of training results and increased time efficiency for large data sets.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Feature extraction and classification of biosignals: Emotion valence detection from EEG signals",
        "doc_scopus_id": "84877946225",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84877946225",
        "doc_date": "2013-05-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Ensemble classifiers",
            "ERD/ERS",
            "Feature extraction and classification",
            "Importance measure",
            "Random forests",
            "Recognition systems",
            "Relative energies",
            "Short time intervals"
        ],
        "doc_abstract": "In this work a valence recognition system based on electroencephalograms is presented. The performance of the system is evaluated for two settings: single subjects (intra-subject) and between subjects (inter-subject). The feature extraction is based on measures of relative energies computed in short time intervals and certain frequency bands. The feature extraction is performed either on signals averaged over an ensemble of trials or on single-trial response signals. The subsequent classification stage is based on an ensemble classifier, i. e. a random forest of tree classifiers. The classification is performed considering the ensemble average responses of all subjects (inter-subject) or considering the single-trial responses of single subjects (intra-subject). Applying a proper importance measure of the classifier, feature elimination has been used to identify the most relevant features of the decision making.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Application of SVM-RFE on EEG signals for detecting the most relevant scalp regions linked to affective valence processing",
        "doc_scopus_id": "84872833087",
        "doc_doi": "10.1016/j.eswa.2012.10.013",
        "doc_eid": "2-s2.0-84872833087",
        "doc_date": "2013-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Affective valence",
            "Brain cortex",
            "Brain oscillations",
            "Clinical application",
            "EEG signals",
            "Event related potentials",
            "Feature extraction and selection",
            "Feature selection methods",
            "Feature space",
            "Input space",
            "Morlet Wavelet",
            "Morlet wavelet filter",
            "Picture system",
            "Spectral dynamics",
            "Support vector",
            "SVM-RFE",
            "Time interval",
            "Visual stimulus"
        ],
        "doc_abstract": "In this work, event related potentials (ERPs) induced by visual stimuli categorized with different value of affective valence are studied. EEG signals are recorded during visualization of selected pictures belonging to International Affective Picture System (IAPS). A Morlet wavelet filter is used to transform the EEG input space to a topography-time-frequency feature space. Support vector machine-recursive feature elimination (SVM-RFE) is applied for detecting scalp spectral dynamics of interest (SSDOIs) in this feature space, allowing to identify the most relevant time intervals, frequency bands and EEG channels. This feature selection method has proven to outperform the classical t-test in the discrimination of brain cortex regions involved in affective valence processing. Furthermore, the presented combination of feature extraction and selection techniques can be applied as an alternative in other different clinical applications. © 2012 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271506 291210 291817 291820 291862 291866 291870 291883 31 Expert Systems with Applications EXPERTSYSTEMSAPPLICATIONS 2012-10-22 2012-10-22 2014-08-31T02:41:22 S0957-4174(12)01127-X S095741741201127X 10.1016/j.eswa.2012.10.013 S300 S300.2 FULL-TEXT 2015-05-15T03:46:20.610912-04:00 0 0 20130501 20130531 2013 2012-10-22T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref specialabst 0957-4174 09574174 false 40 40 6 6 Volume 40, Issue 6 20 2102 2108 2102 2108 201305 May 2013 2013-05-01 2013-05-31 2013 Regular articles article fla Copyright © 2012 Elsevier Ltd. All rights reserved. APPLICATIONSVMRFEEEGSIGNALSFORDETECTINGMOSTRELEVANTSCALPREGIONSLINKEDAFFECTIVEVALENCEPROCESSING HIDALGOMUNOZ A 1 Introduction 2 Materials and methods 2.1 Dataset description and recording 2.2 Time–frequency analysis: Morlet wavelet 2.3 Feature selection: SVM-RFE 3 Results and discussion 3.1 Spectrograms: original data 3.2 SSDOIs detection 3.3 Selected features 4 Conclusions Acknowledgements References ADNANE 2012 1401 1413 M AFTANAS 2001 115 118 L AKIN 2002 20 M ALDHAFEERI 2012 94 98 F BURGES 1998 121 167 C CARRETIE 2001 1109 1128 L DELPLANQUE 2004 1 4 S FRANTZIDIS 2010 589 597 C GUNTEKIN 2010 173 178 B GUYON 2002 389 422 I JENSEN 2002 1395 1399 O LITHARI 2010 27 40 C LOPEZ 2009 233 238 M MARTINI 2012 922 932 N MIWAKEICHI 2004 1035 1045 F OLOFSSON 2008 247 265 J SAUSENG 2007 1435 1444 P SHIEH 2008 531 541 M SINKKONEN 1995 99 104 J SUBASI 2007 1084 1093 A TAN 2006 P INTRODUCTIONDATAMINING TEIXEIRA 2006 125 138 A VAZQUEZMARRUFO 2008 27 38 M VAZQUEZMARRUFO 2001 315 320 M WANG 2010 1195 1268 X YEUNG 2004 822 832 N ZHANG 2009 4664 4671 C HIDALGOMUNOZX2013X2102 HIDALGOMUNOZX2013X2102X2108 HIDALGOMUNOZX2013X2102XA HIDALGOMUNOZX2013X2102X2108XA item S0957-4174(12)01127-X S095741741201127X 10.1016/j.eswa.2012.10.013 271506 2014-08-31T01:36:13.141244-04:00 2013-05-01 2013-05-31 true 1001550 MAIN 7 60480 849 656 IMAGE-WEB-PDF 1 si9 224 18 15 si8 1204 18 338 si7 429 17 72 si6 1002 19 297 si5 278 15 29 si4 1012 19 321 si3 2353 52 393 si2 555 23 109 si14 440 20 60 si13 2198 44 407 si12 661 20 148 si11 1205 20 300 si10 512 19 110 si1 1878 46 324 gr7 15251 211 378 gr6 11135 284 352 gr5 27790 201 520 gr4 81895 685 640 gr3 57430 374 623 gr2 106137 688 646 gr1 28982 159 378 gr7 5882 122 219 gr6 4675 164 203 gr5 4413 85 219 gr4 7919 164 153 gr3 6015 131 219 gr2 9781 164 154 gr1 8925 92 219 ESWA 8167 S0957-4174(12)01127-X 10.1016/j.eswa.2012.10.013 Elsevier Ltd Fig. 1 Volume of features. Determining SSDOIs inside this volume will allow to identify frequency ranges, time intervals and scalp regions (channels). Fig. 2 Spectrograms of the two conditions at two latencies for different frequencies. Color bar range from top row to bottom row: (0 −7.5×10−13), (0−1.4×10−13), (0−2×10−14), (0−5×10−15). Fig. 3 Instantaneous power of the ERP signal (channel Cz) filtered with Morlet wavelet (7 cycles) with different f o values. Fig. 4 Topography of the features belonging to the 20% most relevant ones by the two methods at two specific latencies. Fig. 5 Performance of the SVM classifier using the two methods for features selection. Left: t-test; right: SVM-RFE. Fig. 6 Contribution of each time interval of width=100ms (from stimulus onset (0ms) to 800ms) to the 10% most relevant features selected by SVM-RFE. Fig. 7 Contribution of each EEG channel to the 10% most relevant features selected by SVM-RFE. Application of SVM-RFE on EEG signals for detecting the most relevant scalp regions linked to affective valence processing A.R. Hidalgo-Muñoz a ⁎ M.M. López b I.M. Santos c A.T. Pereira c M. Vázquez-Marrufo a A. Galvao-Carmona a A.M. Tomé d a Dept. Experimental Psychology, University of Seville, 41018 Seville, Spain b IEETA, University of Aveiro, 3810-193 Aveiro, Portugal c Dept. Ciências de Educação, University of Aveiro, 3810-193 Aveiro, Portugal d DETI/IEETA, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Address: Psychophysiology Lab, Dept. Experimental Psychology, University of Seville, C/Camilo José Cela s/n C.P., 41018 Sevilla, Spain. Tel.: +34 954556941. Highlights ► A wrapper method is suggested to get discriminant features in neuroscience. ► SVM-RFE is proposed for feature selection related to affective valence. ► Topography-time–frequency analysis is used to get relevant scalp regions. ► Morlet wavelet filter is used for feature extraction from event related potentials. In this work, event related potentials (ERPs) induced by visual stimuli categorized with different value of affective valence are studied. EEG signals are recorded during visualization of selected pictures belonging to International Affective Picture System (IAPS). A Morlet wavelet filter is used to transform the EEG input space to a topography-time–frequency feature space. Support vector machine-recursive feature elimination (SVM-RFE) is applied for detecting scalp spectral dynamics of interest (SSDOIs) in this feature space, allowing to identify the most relevant time intervals, frequency bands and EEG channels. This feature selection method has proven to outperform the classical t-test in the discrimination of brain cortex regions involved in affective valence processing. Furthermore, the presented combination of feature extraction and selection techniques can be applied as an alternative in other different clinical applications. Keywords Affective valence Brain oscillations EEG Feature extraction Morlet wavelet SVM-RFE 1 Introduction Emotion is always a very fascinating field for discussing and researching. From the dawn of the humanity, human beings have been very interested in understanding our feelings, fears, sorrow or happiness, in finding out the roots of our emotions. Several research works try to explain the emotional system and its psychophysiology and many models aim to classify and order the colorful spectrum of emotions. One of the most popular approaches for emotion classification is the dimensional model of emotions, which sets that the emotion induced by a stimulus can be well defined in two scales: arousal and affective valence (Lang, Bradley, & Cuthbert, 2008). Arousal is related to the alerting state of the body and affective valence concerns to the pleasantness or displeasure we feel when the stimulus is presented. A reliable discrimination of the brain regions related to affective valence is a crucial item in clinical and technological applications which depend on user’s emotional state, such as mental disorders evaluation or human–computer interaction (HCI) (Frantzidis et al., 2010). Event related potentials (ERPs) have been widely studied in emotional context (Aldhafeeri, Mackenzie, Kay, Alghamdi, & Sluming, 2012; Delplanque, Lavoic, Hot, Silvert, & Sequeira, 2004). In time domain, for instance, it has been demonstrated that amplitude and latency of some relevant components (P2, N2, P3) are modulated by affective valence and arousal (Carretié, Martín-Loeches, Hinojosa, & Mercado, 2001; Olofsson, Nordin, Sequeira, & Polich, 2008). Besides of studying time domain, some works suggest that ERP components are induced by a phase resetting (Sauseng et al., 2007), therefore brain oscillations analysis is a promising possibility to explore in detail (Aftanas, Varlamow, Pavlov, Makhnev, & Reva, 2001; Miwakeichi, Martínez-Montes, Valdés-Sosa, Nishiyama, & Mizuhara, 2004; Yoon & Chung, 2011). Each functional band in frequency domain is an important source of features (Wang, 2010), which might let distinguish properly between positive and negative affective valences. In the last few years, some researchers have taken discrete features from filtered signals to classify emotional valence (Brown, Grundlehner, & Penders, 2011; Frantzidis et al., 2010). Following this line, it is advisable to complete a time–frequency analysis in depth. Spectral modulations in ERPs have been broadly studied in different psychological processes and let find connections between them and emotions. Furthermore, several neurological diseases involve alterations in the dynamics of specific frequency bands (e.g. multiple sclerosis (Vázquez-Marrufo et al., 2008)). Several methods have been proposed for computing spectral power of EEG signals (Yeung, Bogacz, Holroyd, & Cohen, 2004). In this work, wavelet transformation (specifically Morlet wavelet) is used for such purpose (Akin, 2002). Wavelet filter has been extensively used in different applications dealing with EEG signals classification (Subasi, 2007; Zhang, Zheng, & Yu, 2009). Its zooming property and the similarity between the shape of the wavelet filter and EEG signal pulses make it one of the most advantageous tool in EEG signal decomposition (Herrmann, Grigutsch, & Busch, 2005). In addition, topographic information of these oscillations is required to specify the scalp spectral dynamics of interest (SSDOIs) involved in emotional processing. For this reason, it is convenient to use a number of EEG channels large enough to get an adequate spatial resolution. Traditionally, in psychological paradigms, statistical methods have been used for comparing different populations or pairs of experimental conditions. In this study, support vector machine-recursive feature elimination (SVM-RFE) is proposed for selecting features. It consists of a wrapped method of feature selection (Guyon, Weston, Barnhill, & Vapnik, 2002), applied in very different contexts such as medicine (Adnane, Jiang, & Yan, 2012) or marketing (Shieh & Yang, 2008) and based on the powerful SVM classification (Burges, 1998), which has been successfully used for numerous applications (López et al., 2009, 2013). In this work, it is proposed for selecting features from the frequency domain of the EEG signals. SVM-RFE allows to work with a large number of features without requiring any statistical correction. Moreover, this algorithm takes into account the influence of individual differences by discarding irrelevant or extreme data. In this study, a novel technique is presented consisting of merging SVM-RFE with an exhaustive analysis of the brain oscillation dynamics from EEG signals throughout the scalp. Three aspects are considered (temporal, spectral and topographical dimensions) in order to accurately detect the SSDOIs related to affective valence evoked by visual stimuli. This work is focused on inspecting a particular emotional process, nevertheless it sets out a complete methodology for examining any other aspect in this kind of psychological processes. In addition, the presented methodology outperforms other traditional statistical tools, involving a powerful tool that could be applied in other applications, e.g. studies on emotional dysfunctions, evaluation of rehabilitation programs in phobias, neurofeedback therapies or HCI, among others. 2 Materials and methods 2.1 Dataset description and recording Twenty-six healthy female subjects were included in this study (age 18–62years; mean=24.19; sd=10.46). All participants had normal or corrected to normal vision and none of them had a history of severe medical treatment, neither psychological nor neurological disorders. A signed informed consent was obtained from each subject before carrying out the experiment. This study was approved in accordance with the Declaration of Helsinki. Participants were seated at 70cm from a computer screen (17’). Subjects were instructed to visualize some pictures which appeared on the screen. All the images were chosen from the International Affective Picture System (IAPS) (Lang et al., 2008). A total of 24 images with high arousal ratings (>6) were selected, 12 of them with positive valence (7.29±0.65) and 12 with negative valence (1.47±0.24). In order to induce a state of general alertness in the subjects, only high arousal stimuli have been presented, avoiding neutral stimuli. In this way, affective valence processing can be studied without breaks in the vigilance level. Furthermore, in medical applications, patients could usually suffer anxiety or nervousness, therefore the conclusions extracted from healthy participants are more reliable if a high level of arousal is kept along the experiment. Each image was presented 3 times, the different conditions were counterbalanced along the experiment and trial order was pseudo-random. In each trial (duration=3500ms), a fixation single cross was presented in the center of the screen during the first 750ms, then, one of the images was presented during 500ms and finally a black screen during 2250ms. EEG activity on the scalp was recorded from 21 Ag/AgCl electrodes (Fp1, Fpz, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, P8, O1, Oz, O2) mounted on an electrode cap from EasyCap according to the international 10/20 system, internally referenced to an electrode on the nose. The impedances of all electrodes were kept below 5kΩ. EEG signals were recorded, sampled at 1000samples/s, and pre-processed using software Scan 4.3. from Neuroscan (Compumedics, USA). Ocular and other artifacts rejection were performed based on amplitude values (Teixeira, Tomé, Lang, Gruber, & da Silva, 2006), in all subjects the number of trials in each condition (positive or negative) was larger than 30. ERP were segmented into time locked epochs using the picture onset as reference. The length of the time window was 950ms: from 150ms before picture onset to 800ms after it (baseline=150ms). For each subject, two ensemble average signals were computed (positive valence and negative valence conditions) to obtain the evoked potentials. The single-trial signals were re-sampled at 250samples/s after using a fourth order zero-phase forward and reverse digital Butterworth filter with passband between 0.5Hz and 60Hz. A notch filter centered in 50Hz was applied to avoid AC contribution. 2.2 Time–frequency analysis: Morlet wavelet EEG signals in general are often described by measurements taken into a segment of N samples. The assumption that frequency contents do not change during the segmented trial (stationary signals) is not realistic, therefore the application of time–frequency analysis is an alternative to overcome this problem. In order to obtain a good time–frequency representation, Morlet wavelet transform analysis is applied (Sinkkonen, Tiitinen, & Näätänen, 1995). The Morlet wavelet is a complex wavelet, comprising real and imaginary sinusoidal oscillations multiplied by a Gaussian window so that the magnitude is largest at the center and tapered towards its edges. The wavelet expression is: (1) W ( t , f o ) = 1 σ t π exp - t 2 2 σ t 2 exp ( 2 j π f o t ) , where the frequency and time resolutions are controlled by the following parameters • The frequency f o , in conjunction with the number of oscillations C of the wavelet, determines the frequency resolution as σ f = f o /C. To avoid a normalization term in Eq. (1) the number of oscillations is usually C >5 (in this study C =7 cycles). Maintaining C constant, the spectral resolution decreases by increasing f o . • The time resolution is determined by the Gaussian envelope (window length) and it is controlled by σ t , which must be chosen such that C is kept constant whatever the f o is. Therefore as σ t = 1 2 π σ f = C 2 π f o , the resolution in time increases by increasing f o . The discrete version is then written as (2) W [ n , f 0 ] = 1 σ t π exp - ( n / F s ) 2 2 σ t 2 exp ( 2 j π f o ( n / F s ) ) , where F s is the sampling rate and n = - T ( f o ) , - T ( f o ) + 1 , … , 0 , … , T ( f o ) - 1 , T ( f o ) . And the value T ( f o ) should be chosen such that encompasses the time support of the wavelet, for instance the range −[2σ t ,2σ t ]. Furthermore, Eq. (2) can be considered the impulse response of an anti-causal filter (zero-phase), with symmetrical imaginary and real parts around n =0. Then, the filtered version of x[n],n =0,…, N −1, around the frequency f 0 are obtained by the convolution y[n, f o ]= x[n]∗ W[n, f o ]. The resulting signal y [ n , f o ] , n = - T ( f o ) , … , 0 , … , T ( f o ) + N - 1 has then 2 T ( f o ) + N samples. The stationary part is used to compute the instantaneous frequency as P ( f o , n ) = y ∗ [ n , f o ] y [ n , f o ] , n = 0 , 1 , … , N - 1 , where (y ∗[n,f o ]) is the complex conjugate. Fig. 3 shows some examples of the instantaneous power where f o takes values in the range of the characteristic bands of the EEG. The time–frequency analysis can be performed for a grid of Q equidistant frequencies, such as f o = f low + q△ f , q =0,…, Q −1, where △ is the step size and f low is the lowest frequency. In this case, frequency contents information is a Q × N matrix where each qth row contains the frequency contents around f low +(q −1)△ f at n F s s. In EEG multichannel analysis the channel is also a third dimension to be considered, then P(q, l, c) stands for q-frequency dimension, l-time dimension and c-space dimension. In this study we use 21 channels (c =1,…,21), we have a range of frequency from 1Hz to 60Hz (q =1,…,60) and the number of time samples is 200 (l =1,…,200) (segmentation from 0ms to 800ms after stimulus onset). A schematic representation of the volume of features computed is shown in Fig. 1 . The total number of features computed for each condition is then M =200×60×21=252000. 2.3 Feature selection: SVM-RFE Traditionally, statistical methods have been used to extract features and to rank them depending on their significance for comparing different populations or different conditions in an experiment. t-test and analysis of variance are among the most popular (Lithari et al., 2010). Modern techniques such as wrapped methods consist of including classification techniques for determining the main features to discriminate between two conditions. SVMs separate a given set of binary labeled training data with a hyperplane that is maximally distant from the two classes (ω 1 and ω 2) (known as the maximal margin hyper-plane). The objective is to build a function f : R M → { ± 1 } using training data, that is, M-dimensional patterns x i and class labels y i : (3) ( x 1 , y 1 ) , ( x 2 , y 2 ) , … , ( x S , y S ) ∈ R M × { ± 1 } , so that f will correctly classify new examples (x,y). S is the number of samples in the database. Linear discriminant functions define decision hyper-surfaces or hyperplane in a multidimensional feature space, that is: (4) g ( x ) = w T x + b = 0 , where w is known as the weight vector and b as the threshold. The weight vector w is orthogonal to the decision hyperplane and b determines the distance of the plan to the origin. The optimization task consists of finding the unknown parameters (w i , i =1,…, m, and b). Given a new element x, the predicted label is then: g ( x ) = w T x + b ⇒ g ( x ) > 0 x ∈ ω 1 ⇔ label ≡ 1 , g ( x ) < 0 x ∈ ω 2 ⇔ label ≡ - 1 . Note that if there are elements in the vector w close to zero they have no influence to g(x) and consequently they do not contribute to the decision. In Guyon et al. (2002) it was suggested a feature selection technique (SVM-RFE) that eliminates recursively the features corresponding to the lowest values ∣w i ∣. Then, the algorithm uses all the features the first time and rejects consecutively the τ features considered less relevant by ordering the absolute values of the entries of w. This process is repeated as long as the classifier performance improves. After the elimination of the features the classifier is evaluated using leave-one-out strategy (Tan, Steinbach, & Kumar, 2006). Note that after k iterations the vector w has M ← M − kτ elements as well as the feature vector x. In this work the initial feature vector x is formed by the concatenation of 3-D energy information P(q, l, c) (Section 2.2, Fig. 1). After applying the SVM-RFE algorithm, the feature vector x has a lower dimension, but the algorithm keeps track of the (q, l, c) of each selected feature. Therefore, a 3-D structure as the original volume can be reconstructed by fulfilling P ∼ ( q , l , c ) with a value ≠0 if the corresponding value of energy is considered relevant, while all the other entries are set to zero. In this way, the regions of interests in scalp, in time and in frequency are easily identified and can be illustrated in a topographical map. In order to compare the results with a statistical approach, a paired (feature-by-feature) t-test is performed on the same feature vector x between the two conditions (positive and negative valence). The features selected by both methods are later used as input for training a linear SVM classifier. The performance of the classifier, which has been assessed using the leave-one-out (LOO) cross-validation strategy (Tan et al., 2006), is an indication of the precision on selecting the best discriminant features for detecting the two different affective valences. 3 Results and discussion 3.1 Spectrograms: original data For visualization purposes, the P(q, l, c) has been averaged by condition across all subjects. Some particular time instants and frequencies have been picked as examples. In Fig. 2 , the energy has been represented on a topographic map for the central frequency belonging to the characteristic bands of EEG (theta: 4–7Hz; alpha: 8–12Hz; beta: 13–30Hz; gamma: 31–60Hz). Note that each frequency has been represented on its own color bar scale since each band contributes with different level of energy to the global signal power. Two time latencies have been chosen belonging to the central instant of the most relevant time intervals (see Section 3.3) for the illustration. Comparing both latencies, an event related desynchronization (ERD) (power decrease) in theta, alpha and beta bands can be observed (Figs. 2 and 3 ). The brain sources of alpha oscillations are mainly located in occipital and parietal areas. Therefore, in the case of alpha band, the ERD is more evident in these areas and it can be related to alertness and expectancy (Vázquez-Marrufo, Vaquero, Cardoso, & Gómez, 2001), whereas the highest power in theta band, which has been related to working memory (Jensen & Tesche, 2002), spreads in central areas. These desynchronizations are principally implied in generic attentional rather than emotional processes, so neither ERD in theta nor alpha band are supposed to be very different between conditions in our paradigm. Looking at these spectrograms, it would be expected to find significant differences between positive and negative conditions in beta band: the averaged power in the second case is greater widespread the scalp at latency of 350ms. Some research works assert that beta oscillations are affected by emotional processes (Güntekin & Basar, 2010). Regarding the gamma band, a slight difference between conditions appears at 450ms due to the power being larger for the positive valence condition. Some studies suggest gamma band as a good candidate to classify emotions (Martini et al., 2012), although this wide band of frequency is also implied in very different psychological processes. 3.2 SSDOIs detection The two methods (t-test and SVM-RFE), mentioned in Section 2.3, have been used for detecting the SSDOIs. In the first case, features are ranked based on t-test (the greater value of t, the more relevant the feature) and, in the other case, features are ranked based on SVM-RFE algorithm (the larger absolute value of w i , the more relevant the feature). The relevance level is determined by the position in the ranking of the features, therefore a threshold in this ranking let define the SSDOIs (made up by joining the selected features). In Fig. 4 , the relevance of the features is represented by the color map level. Only the features belonging to the 20% most relevant set are colored, setting to white color the remaining 80%. Note that the 20% in the first case (t-test method) does not correspond to a threshold of the p-value of statistical significance obtained by the t-test. The features are ranked based on the t values, taking the 20% with greatest values, independently its associated p-value. Therefore a correction of multiple comparisons (e.g. Bonferroni correction) is not required. Roughly, the obtained SSDOIs for both methods match up, although some differences exist. There is not any significant difference (between conditions) in theta band in the two latencies we have chosen (Fig. 4) as it was expected. In alpha band, the differences appear in central and frontal areas, being wider for t-test method than SVM-RFE at the second latency, in contrast to beta band, where the most relevant regions are extended in parietal and occipital areas and only for t-test method at these specific latencies. The SSDOIs in gamma band are more diffused and vary their extension along the frequencies, although they are more evident in occipital and parietal areas as for beta band. 3.3 Selected features In order to find out the suitable number of relevant features to delimit the principal SSDOIs (spatial, temporal and spectral regions), a linear SVM classifier has been applied to both subsets of selected features (SVM-RFE and t-test) (see Section 2.3). Note that it is important to get a large enough number of features to get consistent regions for avoiding spurious noisy features that could reach a good accuracy with a specific sample by chance. A general trade-off exists between the necessary number of features to get consistent regions and the computation time. The highest accuracy peak was 96.15% for features selected by the t-test criterion, keeping approximately 8% of features, whereas with SVM-RFE we got a total accuracy of 100% for a wide range (from 1% to 22%) of number of selected features (see Fig. 5 ). Note that, for t-test selection the number of features increases in each iteration, whereas for SVM-RFE method the number of features decreases from the total of elements to run out the features. Experimentally, after running the supervised classification task (Section 2.3), the best choice for both methods simultaneously (based on accuracy results) is keeping the 10% most relevant features. Due to the advantages explained in Section 1, SVM-RFE has been the chosen method for selecting the final features and determining the main time intervals, frequency bands and EEG channels in affective valence processing. By aggregating the selected features in time intervals of 100ms of width (from stimulus onset), it has been found that the time interval where the greatest number of features appears as relevant (36% of the selected features) is comprised between 300ms and 500ms post-stimulus (see Fig. 6 ), simultaneously with the popular component P3 in time domain. It has been suggested that P3 is modulated by affective valence (Carretié et al., 2001; Olofsson et al., 2008). Previous time intervals, and its related components in time-domain, are rather linked to visual or perceptual processes and the differences found there have been probably influenced by morphological or chromatic characteristics of the IAPS pictures. Regarding to the most relevant frequency bands, as it was expected, delta range (1–3Hz) is the band with least contribution to discriminate affective valence because the periods of delta waves are too large to be measured in the epoch time. Besides, some border effects might prevent from quantifying the power properly. Regarding alpha band, the main areas to extract information relative to valence are central and frontal lobes, which do not necessarily match the most energetic area, which is the occipital region. It is important to point out the high contribution of beta and gamma ranges in terms of their relative power, specially in parietal regions, in spite of the noise artifacts usually present in these high frequencies. In previous studies, gamma band has already been pointed out as an important band for classifying emotional responses with visual stimuli (Martini et al., 2012). Finally, scalp regions contributions to the 10% most relevant features could be computed by clustering EEG channels. Fig. 7 shows the contribution of each EEG channel. The differences between conditions are widespread from the center of the scalp and a larger number of features are selected from the left hemisphere. This fact could be due to a deep subcortical projection from the limbic system, essential for processing the affective valence of the stimulus. 4 Conclusions In this work, a supervised method is proposed for detecting the main cortex regions and the brain oscillations dynamics implied in affective valence processing after visualizing high arousal pictures. By means of the Morlet wavelet, a complete volume (with three dimensions representing time, frequency and space) is obtained and is subsequently used to delimit exhaustively the scalp spectral dynamics of interest (SSDOIs) linked to affective valence. For this purpose, SVM-RFE was applied on this set of features, giving rise to SSDOIs that yield 100% accuracy in distinguishing the two conditions, outperforming other traditional statistical contrasts. The results of the present study conclude that, for classifying the affective valence induced by picture visualization, from 300ms to 500ms after stimulus onset is the most relevant time interval. The most relevant spectral band is alpha band in central areas, although beta and gamma bands are relevant in parietal and occipital areas in terms of their relative power. In later research works it could be interesting to focus on single trial analysis and to use similar volumes of features for discriminating among other psychological processes. This method, applied on large databases from different populations, could give accurate and useful information to aid medical diagnosis for diverse brain pathologies. Acknowledgements This work is partially funded by FEDER through the Operational Program Competitiveness Factors - COMPETE and by National Funds through FCT - Foundation for Science and Technology in the context of the project FCOMP-01-0124-FEDER-022682 (FCT reference PEst-C/EEI/UI0127/2011). References Adnane et al., 2012 M. Adnane Z. Jiang Z. Yan Sleep-wake stages classification and sleep efficiency estimation using single-lead electrocardiogram Expert Systems with Applications 39 2012 1401 1413 Aftanas et al., 2001 L. Aftanas A. Varlamow S. Pavlov V. Makhnev N. Reva Affective picture processing: event-related synchronization within individually defined human theta band is modulated by valence dimension Neuroscience Letters 303 2001 115 118 Akin, 2002 M. Akin Comparison of wavelet transform and FFT methods in the analysis of EEG signals Journal of Medical Systems 2002 20 Aldhafeeri et al., 2012 F.M. Aldhafeeri I. Mackenzie T. Kay J. Alghamdi V. Sluming Regional brain responses to pleasant and unpleasant IAPS pictures: Different networks Neuroscience Letters 512 2012 94 98 Brown et al., 2011 Brown, L., Grundlehner, B., & Penders, J. (2011). Towards wireless emotional valence detection from EEG. In: 33rd Annual international conference of the IEEE EMBS, (2011). Boston, Massachusetts, USA. Burges, 1998 C. Burges A tutorial on support vector machines for pattern recognition Data Mining and Knowledge Discovery 2 1998 121 167 Carretié et al., 2001 L. Carretié M. Martín-Loeches J. Hinojosa F. Mercado Emotion and attention interaction studied through event-related potentials Journal of Cognitive Neuroscience 13 2001 1109 1128 Delplanque et al., 2004 S. Delplanque M.E. Lavoic P. Hot L. Silvert H. Sequeira Modulation of cognitive processing by emotional valence studied through event-related potentials in humans Neuroscience Letters 356 2004 1 4 Frantzidis et al., 2010 C.A. Frantzidis C. Bratsas C.L. Papadelis E. Konstantinidis C. Pappas P.D. Bamidis Toward emotion aware computing: An integrated approach using multichannel neurophysiological recordings and affective visual stimuli IEEE Transactions on Information Technology in Biomedicine 14 2010 589 597 Güntekin and Basar, 2010 B. Güntekin E. Basar Event-related beta oscillations are affected by emotional eliciting stimuli Neuroscience Letters 483 2010 173 178 Guyon et al., 2002 I. Guyon J. Weston S. Barnhill V. Vapnik Gene selection for cancer classification using support vector machines Machine Learning 46 2002 389 422 Herrmann et al., 2005 Herrmann, C. S., Grigutsch, M., & Busch, N. (2005). EEG oscillations and wavelet analysis. In: Handy (Ed.), Event-related potentials: A methods handbook (pp. 229–259). Jensen and Tesche, 2002 O. Jensen C.D. Tesche Frontal theta activity in humans increases with memory load in a working memory task European Journal of Neuroscience 15 2002 1395 1399 Lang et al., 2008 Lang, P., Bradley, M., & Cuthbert, B. (2008). International affective picture system (IAPS): Instruction manual and affective ratings. Technical Report A-8. Lithari et al., 2010 C. Lithari C. Frantzidis C. Papadelis A.B. Vivas M. Klados C. Kourtidou-Papadeli Are females more responsive to emotional stimuli? A neurophysiological study across arousal and valence dimensions Brain Topography 23 2010 27 40 López et al., 2013 López, M., Górriz, J., Ramírez, J., Gómez-Río, M., Verdejo, J., & Vas, J. (2013). Component-based technique for determining the effects of acupuncture for fighting migraine using SPECT images. Expert Systems with Applications, 40, 44–51. López et al., 2009 M. López J. Ramírez J. Górriz I. Álvarez D. Salas-Gonzalez R. Chaves SVM-based CAD system for early detection of the alzheimer’s disease using kernel PCA and LDA Neuroscience Letters 464 2009 233 238 Martini et al., 2012 N. Martini D. Menicucci L. Sebastiani R. Bedini A. Pingitore N. Vanello The dynamics of EEG gamma responses to unpleasant visual stimuli: From local activity to functional connectivity Neuroimage 60 2012 922 932 Miwakeichi et al., 2004 F. Miwakeichi E. Martínez-Montes P. Valdés-Sosa N. Nishiyama H. Mizuhara Decomposing EEG data into space-time-frequency components using parallel factor analysis Neuroimage 22 2004 1035 1045 Olofsson et al., 2008 J.K. Olofsson S. Nordin H. Sequeira J. Polich Affective picture processing: An integrative review of ERP findings Biological Psychology 77 2008 247 265 Sauseng et al., 2007 P. Sauseng W. Klimesch W. Gruber S. Hanslmayr R. Freunberger M. Doppelmayr Are event-related potential components generated by phase resetting of brain oscillations? A critical discussion Neuroscience 146 2007 1435 1444 Shieh and Yang, 2008 M.-D. Shieh C.-C. Yang Multiclass SVM-RFE for product form feature selection Expert Systems with Applications 35 2008 531 541 Sinkkonen et al., 1995 J. Sinkkonen H. Tiitinen R. Näätänen Gabor filters: An informative way for analysing event-related brain activity Journal of Neuroscience Methods 56 1995 99 104 Subasi, 2007 A. Subasi EEG signal classification using wavelet feature extraction and a mixture of expert model Expert Systems with Applications 32 2007 1084 1093 Tan et al., 2006 P.-N. Tan M. Steinbach V. Kumar Introduction to data mining 2006 Addison-Wesley Teixeira et al., 2006 A. Teixeira A. Tomé E.-W. Lang P. Gruber A. da Silva Automatic removal of high-amplitude artifacts from single-channel electroencephalograms Computer Methods and Programs in Biomedicine 83 2006 125 138 Vázquez-Marrufo et al., 2008 M. Vázquez-Marrufo J. González-Rosa E. Vaquero P. Duque C. Escera M. Borges Abnormal ERPs and high frequency bands power in multiple sclerosis International Journal of Neurosciencce 118 2008 27 38 Vázquez-Marrufo et al., 2001 M. Vázquez-Marrufo E. Vaquero M. Cardoso C. Gómez Temporal evolution of α and β bands during visual spatial attention Cognitive Brain Research 12 2001 315 320 Wang, 2010 X.J. Wang Neurophysiological and computational principles of cortical rhythms in cognition Physiological Reviews 90 2010 1195 1268 Yeung et al., 2004 N. Yeung R. Bogacz C.B. Holroyd J.D. Cohen Detection of synchronized oscillations in the electroencephalogram: An evaluation of methods Psychophysiology 41 2004 822 832 Yoon and Chung, 2011 Yoon, H. -J., & Chung, S. -Y. (2011). EEG spectral analysis in valence and arousal dimensions of emotion. In: International conference on control, automation and systems, (2011). Gyeonggi-do, Korea. Zhang et al., 2009 C. Zhang C.-X. Zheng X.-L. Yu Automatic recognition of cognitive fatigue from physiological indices by using wavelet packet transform and kernel learning algorithms Expert Systems with Applications 36 2009 4664 4671 "
    },
    {
        "doc_title": "Weighted sliding empirical mode decomposition for online analysis of biomedical time series",
        "doc_scopus_id": "84873408833",
        "doc_doi": "10.1007/s11063-012-9270-9",
        "doc_eid": "2-s2.0-84873408833",
        "doc_date": "2013-02-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Neuroscience (all)",
                "area_abbreviation": "NEUR",
                "area_code": "2800"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Arterial blood pressure",
            "Biomedical signal",
            "Empirical Mode Decomposition",
            "Fourier",
            "Hilbert Huang transforms",
            "Intra-cranial pressure",
            "Intrinsic Mode functions",
            "Mutual informations",
            "Neuromonitoring",
            "Non-stationary time series",
            "Nonstationary",
            "On-line analysis",
            "Orthogonal basis function",
            "Temporal correlations",
            "Weighting process"
        ],
        "doc_abstract": "Biomedical signals are in general non-linear and non-stationary. empirical mode decomposition in conjunction with a Hilbert-Huang Transform provides a fully adaptive and data-driven technique to extract intrinsic mode functions. The latter represent a complete set of locally orthogonal basis functions to represent non-linear and non-stationary time series. Large scale biomedical time series necessitate an online analysis, which is presented in this contribution. It shortly reviews the technique of EMD and related algorithms, discusses the recently proposed weighted sliding EMD algorithm (wSEMD) and, additionally, proposes a more sophisticated implementation of the weighting process. As an application to biomedical signals we will show that wSEMD in combination with mutual information could be used to detect temporal correlations of arterial blood pressure and intracranial pressure monitored at a neurosurgical intensive care unit. We will demonstrate that the wSEMD technique renders itself much more flexible than the Fourier based method used in Faltermeier et al. (Acta Neurochir Suppl, 114, 35-38, 2012). © 2012 Springer Science+Business Media New York.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual detection of vehicles using a bag-of-features approach",
        "doc_scopus_id": "84890891280",
        "doc_doi": "10.1109/Robotica.2013.6623539",
        "doc_eid": "2-s2.0-84890891280",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Attribute selection",
            "Bag-of-Features",
            "Speeded up robust features",
            "Vehicle classification",
            "Vehicle detection",
            "Visual detection"
        ],
        "doc_abstract": "This paper presents and evaluates the performance of a method for vehicle detection using a bag-of-features methodology. The algorithm combines Speeded Up Robust Features with a Support Vector Machine. An optimization to the bag-of-features dictionary based on a genetic algorithm for attribute selection is also described. The results obtained show that this method can successfully address the problem of vehicle classification. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Spectral turbulence measuring as feature extraction method from EEG on affective computing",
        "doc_scopus_id": "84885651684",
        "doc_doi": "10.1016/j.bspc.2013.09.006",
        "doc_eid": "2-s2.0-84885651684",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Affective Computing",
            "EEG classification",
            "Emotion",
            "Spectral turbulences",
            "SVM-RFE"
        ],
        "doc_abstract": "In biomedical and psychological applications dealing with EEG, a suitable selection of the most relevant electrodes is useful for lightening the data acquisition and facilitating the signal processing. Therefore, an efficient method for extracting and selecting features from EEG channels is desirable. Classification methods are more and more applied for obtaining important conclusions from diverse psychological processes, and specifically for emotional processing. In this work, an original and straightforward method, inspired by the spectral turbulence (ST) measure from electrocardiogram and the support vector machine-recursive feature elimination (SVM-RFE) algorithm, is proposed for classifying EEG signals. The goal of this study is to introduce the ST concept in applications of artificial intelligence related to cognitive processes and to determine the best EEG channels for distinguishing between two different experimental conditions. By means of this method, the left temporal region of the brain has revealed to be greatly involved in the affective valence processing elicited by visual stimuli. © 2013 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 273545 291210 291874 291880 291883 31 Biomedical Signal Processing and Control BIOMEDICALSIGNALPROCESSINGCONTROL 2013-10-11 2013-10-11 2014-10-01T12:02:42 S1746-8094(13)00131-6 S1746809413001316 10.1016/j.bspc.2013.09.006 S300 S300.2 FULL-TEXT 2015-05-15T07:07:46.13173-04:00 0 0 20131101 20131130 2013 2013-10-11T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor highlightsabst primabst ref 1746-8094 17468094 true 8 8 6 6 Volume 8, Issue 6 51 945 950 945 950 201311 November 2013 2013-11-01 2013-11-30 2013 Regular Articles article fla Copyright © 2013 Elsevier Ltd. All rights reserved. SPECTRALTURBULENCEMEASURINGFEATUREEXTRACTIONMETHODEEGAFFECTIVECOMPUTING HIDALGOMUNOZ A 1 Introduction 2 Materials and methods 2.1 Feature extraction 2.2 Classification and feature selection 2.3 Experiment 3 Results 3.1 Topographical maps 3.2 Inter-subject classification 3.3 Intra-subject classification 3.4 Most relevant EEG sensors 4 Discussion 5 Conclusions Acknowledgments References SAUSENG 2007 1435 1444 P KELEN 1991 965 975 G MALIK 1994 227 232 M BARBOSA 2006 307 316 P POZA 2008 5712 5715 J 30THANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBS ANALYSISSPONTANEOUSMEGACTIVITYINALZHEIMERSDISEASEUSINGTIMEFREQUENCYPARAMETERS BROWN 2011 L 33RDANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBS2011 TOWARDSWIRELESSEMOTIONALVALENCEDETECTIONEEG LEON 2007 337 345 E OLOFSSON 2008 247 265 J GUNTEKIN 2010 173 178 B DELPLANQUE 2004 1 4 S MACAS 2009 1 4 M 9THINTERNATIONALCONFERENCEINFORMATIONTECHNOLOGYAPPLICATIONSINBIOMEDICINENONOVEMBERIEEE CLASSIFICATIONEMOTIONALSTATESBASEDEEGSIGNALPROCESSING FRANTZIDIS 2010 589 597 C HUANG 2012 1 7 D INTERNATIONALJOINTCONFERENCENEURALNETWORKSNOJUNEIEEE ASYMMETRICSPATIALPATTERNFOREEGBASEDEMOTIONDETECTION HERRMANN 2005 229 259 C EVENTRELATEDPOTENTIALSAMETHODSHANDBOOK EEGOSCILLATIONSWAVELETANALYSIS KLADOS 2009 549419 M GUYON 2002 389 422 I BURGES 1998 121 167 C LOPEZ 2013 44 51 M BENHUR 2008 e1000173 A HIDALGOMUNOZ 2013 2102 2108 A XU 2013 135 143 T GRATTON 1993 468 484 G MERIAU 2009 73 80 K AFTANAS 2001 115 118 L TOME 2013 54 60 A BIOSIGNALS2013INTERNATIONALCONFERENCEBIOINSPIREDSYSTEMSSIGNALNOFEBRUARY FEATUREEXTRACTIONCLASSIFICATIONBIOSIGNALSEMOTIONVALENCEDETECTIONEEGSIGNALS PFURTSCHELLER 1999 1842 1857 G HIDALGOMUNOZX2013X945 HIDALGOMUNOZX2013X945X950 HIDALGOMUNOZX2013X945XA HIDALGOMUNOZX2013X945X950XA item S1746-8094(13)00131-6 S1746809413001316 10.1016/j.bspc.2013.09.006 273545 2014-10-01T21:59:12.120779-04:00 2013-11-01 2013-11-30 true 963444 MAIN 6 61941 849 656 IMAGE-WEB-PDF 1 si9 184 12 23 si8 169 11 20 si7 488 21 100 si6 1669 49 378 si5 169 11 20 si4 394 14 123 si3 169 11 20 si2 527 16 148 si1 380 16 109 gr5 19868 218 376 gr4 22149 213 382 gr3 27795 198 393 gr2 27514 200 393 gr1 65010 827 242 gr5 4783 127 219 gr4 5081 122 219 gr3 5817 110 219 gr2 7307 111 219 gr1 4139 164 48 BSPC 451 S1746-8094(13)00131-6 10.1016/j.bspc.2013.09.006 Elsevier Ltd Fig. 1 Topographical scalp maps of the spectral turbulence (ST) (left: positive valence, right: negative valence). Upper to down: 1st to 9th ST values computed from 250ms after the stimulus onset. The ST is represented by a color map, where red color represents the highest spectral variations and blue color represents the highest spectral stability, i.e. blue represents the highest ST values (ρ ≈1) and red the lowest ST values (ρ ≈0). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of the article.) Fig. 2 Inter-subject classification: Accuracy peaks (%) for each channel using ERPs. SVM: linear SVM classifier; SVM-RFE: applying feature selection; SVM-RFE (test in training): test sample is included in the feature selection before applying LOO. Fig. 3 Intra-subject classification: Mean of accuracy peaks (%) across all subjects per channel using single-trials. The error bars represent the standard deviation. Fig. 4 Grand average of ERPs from T7 channel. 0ms represents the stimulus onset. Fig. 5 Grand average of ST from T7 channel (markers are located on the first time instants of the defined time intervals). Table 1 Statistical significance (p) obtained by means of t-test contrasts of the ST values (between positive and negative conditions) for every EEG channel and every time interval considered. *Numbers in bold indicate statistically significant values (α =0.05). Note that the variables refer to the correlation between two consecutive time intervals, not to the time intervals itself. Thus, the columns title indicates when the latter time intervals involved in the correlations begins (ms). Channel 0 50 100 150 200 250 300 350 400 450 500 550 600 650 700 Fp1 0.22 0.27 0.61 0.32 0.90 0.09 0.76 0.80 0.08 0.19 0.70 0.84 0.63 0.41 Fpz 0.45 0.69 0.77 0.69 0.83 0.93 0.49 0.70 0.67 0.39 0.85 0.80 0.05 0.48 Fp2 0.36 0.71 0.12 0.29 0.51 0.77 0.95 0.77 0.83 0.14 0.13 0.16 0.20 0.76 F7 0.21 0.79 0.50 0.45 0.74 0.61 0.15 0.39 0.55 0.85 0.88 0.16 0.99 0.45 F3 0.59 0.67 0.14 0.93 0.38 0.59 0.55 0.02* 0.09 0.98 0.97 0.57 0.18 0.47 Fz 0.97 0.06 0.33 0.67 0.74 0.34 0.30 0.07 0.13 0.40 0.28 0.10 0.11 0.13 F4 0.33 0.06 0.51 0.83 0.82 0.60 0.99 0.54 0.35 0.13 0.75 0.02* 0.22 0.37 F8 0.90 0.89 0.77 0.47 0.55 0.86 0.19 0.22 0.08 0.07 0.49 0.29 0.77 0.61 T7 0.83 0.78 0.94 0.27 0.32 0.26 0.04* 0.04* 0.00* 0.00* 0.34 0.81 0.06 0.71 C3 0.41 0.18 0.09 0.29 0.91 0.29 0.86 0.09 0.16 0.17 0.16 0.06 0.04* 0.20 Cz 0.31 0.01* 0.04* 0.40 0.36 0.37 0.76 0.42 0.96 0.56 0.04* 0.02* 0.01* 0.46 C4 0.57 0.33 0.21 0.84 0.60 0.38 0.21 0.58 0.07 0.10 0.01* 0.02* 0.28 0.77 T8 0.01* 0.42 0.34 0.89 0.65 1.00 0.10 0.06 0.20 0.01* 0.34 0.02* 0.66 0.63 P7 0.97 0.84 0.53 0.06 0.82 0.86 0.92 0.56 0.36 0.73 0.85 0.98 0.25 0.21 P3 0.04* 0.47 0.24 0.59 0.34 0.48 0.13 0.04* 0.18 0.01* 0.01* 0.07 0.11 0.55 Pz 0.43 0.04* 0.02* 0.87 0.88 0.23 0.86 0.43 0.40 0.23 0.04* 0.05* 0.16 0.69 P4 0.64 0.16 0.12 0.81 0.88 0.79 0.55 0.28 0.05* 0.10 0.01* 0.02* 0.23 0.25 P8 0.56 0.72 0.80 0.12 0.28 0.80 0.24 0.43 0.59 0.90 0.89 0.20 0.39 0.17 O1 0.11 0.97 0.91 0.98 0.24 0.85 0.71 0.32 0.45 0.21 0.04* 0.23 0.50 0.51 Oz 0.58 0.46 0.56 1.00 0.32 0.68 0.40 0.05* 0.20 0.90 0.20 0.28 0.80 0.38 O2 0.46 0.82 0.40 0.07 0.28 0.85 0.72 0.40 0.34 0.52 0.05* 0.11 0.55 0.54 Table 2 Maximal accuracy values (max acc) yielded up, across all the participants, performing intra-subject classification using both directly SVM and SVM-RFE methods; relevance columns quantify the number of participants whose most relevant electrode (the one where the max acc is reached) is the electrode indicated by the row (in the Electrodes column); features column shows the number of features selected by SVM-RFE, averaging the results from all the subjects. Electrodes SVM SVM-RFE Max acc (%) Relevance Max acc (%) Relevance Features Fp1 63.3 1 81.7 2 4.15 Fpz 65 1 71.7 1 4.08 Fp2 65 2 73.3 1 5.31 F7 70 1 75 3 3.69 F3 66.7 1 75 1 4.23 Fz 75 1 83.3 1 3.54 F4 68.3 2 75 1 3.50 F8 65 2 78.3 2 3.69 T7 73.3 4 81.7 5 4.56 C3 73.3 1 81.7 2 5.08 Cz 66.7 0 75 0 5.08 C4 71.7 3 76.7 1 4.85 T8 68.3 2 76.7 0 4.00 P7 63.3 1 75 1 3.65 P3 60 0 73.3 0 4.12 Pz 63.3 1 73.3 1 4.50 P4 70 2 81.7 2 4.42 P8 70 2 81.7 1 4.38 O1 66.7 0 73.3 0 4.81 Oz 65 0 75 0 3.12 O2 70 0 80 1 4.31 Spectral turbulence measuring as feature extraction method from EEG on affective computing A.R. Hidalgo-Muñoz a ⁎ M.M. López b A.T. Pereira c d I.M. Santos c d A.M. Tomé e a Department of Experimental Psychology, University of Seville, 41018 Seville, Spain Department of Experimental Psychology, University of Seville Seville 41018 Spain b IEETA, University of Aveiro, 3810-193 Aveiro, Portugal IEETA, University of Aveiro Aveiro 3810-193 Portugal c Department of Education, University of Aveiro, 3810-193 Aveiro, Portugal Department of Education, University of Aveiro Aveiro 3810-193 Portugal d IBILI-Institute of Biomedical Research in Light and Image, Faculty of Medicine, University of Coimbra, Portugal IBILI-Institute of Biomedical Research in Light and Image, Faculty of Medicine, University of Coimbra Portugal e DETI/IEETA, University of Aveiro, 3810-193 Aveiro, Portugal DETI/IEETA, University of Aveiro Aveiro 3810-193 Portugal ⁎ Corresponding author at: Psychophysiology Lab, Department of Experimental Psychology, University of Seville, C/ Camilo José Cela s/n, C.P. 41018 Sevilla, Spain. Tel.: +34 954556941; fax: +351-234-370545. In biomedical and psychological applications dealing with EEG, a suitable selection of the most relevant electrodes is useful for lightening the data acquisition and facilitating the signal processing. Therefore, an efficient method for extracting and selecting features from EEG channels is desirable. Classification methods are more and more applied for obtaining important conclusions from diverse psychological processes, and specifically for emotional processing. In this work, an original and straightforward method, inspired by the spectral turbulence (ST) measure from electrocardiogram and the support vector machine-recursive feature elimination (SVM-RFE) algorithm, is proposed for classifying EEG signals. The goal of this study is to introduce the ST concept in applications of artificial intelligence related to cognitive processes and to determine the best EEG channels for distinguishing between two different experimental conditions. By means of this method, the left temporal region of the brain has revealed to be greatly involved in the affective valence processing elicited by visual stimuli. Keywords Affective computing EEG classification Emotion Spectral turbulence SVM-RFE 1 Introduction In order to classify properly EEG signals corresponding to different experimental conditions, it is necessary to extract efficient features that are able to describe the signals unequivocally. As it is well known, event-related potentials (ERPs) have been widely studied to define diverse psychological processes, and specifically for studying the emotional processing. In addition, some works suggest that ERP components are induced by a phase resetting [1], which makes specially relevant the analysis of the spectral dynamics of the EEG signals. In this work, a new feature based on the spectral turbulence (ST) parameter used in electrocardiogram analysis [2], never used for classification purposes on EEG signals neither inter-subject nor intra-subject approaches, is proposed for distinguishing the affective valence elicited by an image (negative vs positive valence). ST provides a measure from the time-frequency map by comparing neighboring spectral estimates, which has been specifically used as a marker of determined cardiac disorders. From a signal, ST could be considered like an index of the stability of its spectral content over time. In [2] ST is analyzed and computed over averaged electrocardiogram (ECG) signals for predicting patients with inducible sustained monomorphic ventricular tachycardia. Following the same line, Malik et al. [3] analyzed ST, also over ECG signals, in survivors of acute myocardial infarction. More recently, Barbosa et al. [4] employed ST analysis as a predictor of recurrence of idiopathic atrial fibrillation. Nevertheless, few examples of ST applied on EEG signals can be found in the literature, as far as the authors are aware. For instance, in [5] it is proposed to compute ST in order to determine the evolution of the cognitive impairment in the Alzheimer disease, but using magnetoencephalography (MEG). Thus, ST analysis is a wide field to explore in EEG. The concrete approach described in this work consists of a valence recognition system using visual stimuli (images), which is an essential component in some brain computer interface (BCI) applications [6] and emotion-based systems [7]. According to the dimensional model of emotions, the brain activity elicited by a stimulus is modulated by two attributes of it: arousal and affective valence. The arousal indicates the alertness level (defined in a continuum from relax to excitement) which is produced by the stimuli. On the other hand, the affective valence refers to the pleasure (or displeasure) elicited by the stimuli. A large number of research works have focused on studying the underlying neurophysiology and the behavioral correlates linked to each dimension and have suggested EEG as a valuable source of emotional information [8–10]. Generally, the automatic classification tasks of EEG signals aiming at identifying the sign of the affective valence, using a reasonable small number of features, are difficult and yield moderated results in terms of accuracy (hardly exceeds 70% on average) [11]. However, the obtained accuracy values depend on diverse factors: number of emotional categories, presentation and type of the stimuli, number of available trials, etc. Different methods for feature extraction from EEG signals have been used, e.g. the computation of amplitude peaks and latencies from the ERP in time domain [12], inter-hemisphere asymmetry [13] or wavelet filtering [14] and event-related desynchronization/synchronization (ERD/ERS) [15] in spectral domain, among others. In this work, the ST is computed, allowing to reduce greatly the number of initial features and to tackle directly the EEG single-trial signals. The goal of this paper is to introduce and check the usefulness of ST measure as suitable feature for classifying emotional states. Moreover, it is desirable to select the most relevant electrode positions for optimizing the signal acquisition and avoiding too large sets of features. For this purpose, a wrapper method, known as support vector machine-recursive feature elimination (SVM-RFE) based on a SVM classifier [16], is implemented to optimize accuracy and computation resources. 2 Materials and methods 2.1 Feature extraction The method used for extracting features from the EEG signals is based on the first part of the ST computation reported by Kelen [2] and consists of computing the Pearson's correlation coefficient (ρ) between adjacent columns of the spectrogram (time×frequency) from EEG epochs. Note that lower ρ values imply higher spectral variations. The columns of the spectrogram are computed taking consecutive 100ms long time-intervals from the stimulus onset (image presentation=0ms) with an overlap of 50% and computing the fast Fourier transform (FFT) (FFT were computed by means of Goertzel algorithm) on these intervals. Experimentally (see Section 3.1), the most discriminant time intervals for determining the affective valence are the time intervals just after 250ms from the onset of the stimulus. Therefore, the extracted features (ST values) used as input for the classifier are taken from 300ms post stimulus (i.e. from the correlation between the time interval which begins at 250ms and the next overlapped interval), when the P3 component in time domain simultaneously occurs and the first perceptive processing of the stimulus has been concluded [8]. Thus, 9 ST values are extracted from each spectrogram, since the final time intervals are taking from 250ms to 800ms (100ms long, 50ms overlap, as it is explained before, i.e. ten time intervals), belonging to the 9 possible comparisons between adjacent intervals. The frequencies considered range was from 8Hz to 30Hz (alpha and beta bands) using a step of 1Hz (23 frequency samples per column). 2.2 Classification and feature selection Support vector machines (SVMs) separate a given set of binary labeled training data with a hyperplane that is maximally distant from the two classes, known as the maximal margin hyper-plane [17,18]. The objective is to build a function f : ℝ M ⟶ { ± 1 } using training data, that is, M-dimensional patterns x i and class labels y i , so that f will correctly classify new examples (x, y). Linear discriminant functions define such decision hyper-surfaces or hyperplane in a multidimensional feature space, that is: (1) g ( x ) = w T x + w 0 = 0 , where w is known as the weight vector and w 0 as the threshold. The weight vector w is orthogonal to the decision hyperplane. The optimization task consists of finding the unknown parameters w m , m = 1 , … , M and w 0 , which separate the two classes optimally [19]. For the Linear SVM, the vector w can be explicitly computed and this constitutes an advantage as it decreases the complexity during the test phase. Given a new element x, the predicted label is then: g ( x ) = w T x + w 0 ⇒ g ( x ) > 0 x ∈ ω 1 ⇔ label ≡ 1 g ( x ) < 0 x ∈ ω 2 ⇔ label ≡ − 1 The vector w = ∑ i N s y i λ i x i is a weighted sum of the support vectors which are the N s elements of training set chosen during the training phase inside the margin. And 0< λ i < C are the corresponding Lagrangian parameters which are also optimized during training. The value of C needs to be assigned to run the training optimization algorithm. It is a parameter that indirectly controls the width of the margin of the classifier. However, during the optimization process, C represents the weight of the penalty term of the optimization function that is related with the misclassification error in the training set. Therefore, the optimization determines the trade-off between the width of the margin and the number of accepted misclassifications. There is no optimal procedure to assign this parameter, so the required parameter was kept by default as C =1. Finally, the value of the threshold w 0 is estimated by solving the equations related to the hyperplanes that define the margin. Guyon et al. [16] suggested the SVM-RFE as a feature selection method, which eliminates recursively the features corresponding to the lowest value | w i | , since the elements in the vector w close to zero have minor influence to g(x) and consequently they hardly contribute to the decision. Thus, a linear kernel is used. The algorithm uses all the features the first time and rejects consecutively the feature considered less relevant by ordering the absolute values of the entries of w. The process is repeated as long as the classifier performance (using linear SVM), which is evaluated using leave-one-out (LOO) strategy, improves. This algorithm has been successfully used in other neuroscientist applications [20,21]. In order to design the LOO strategy, two options exist. One alternative consists of ignoring completely a feature vector in the classification system design and finally to use this vector only like a test vector. However, the test vector could be taken into account in the training when the most relevant features are determined by SVM-RFE. In other words, as a second option, the test vector participates in building the model of the feature elimination stage. Anyway, in both cases, the test vector is come out from the SVM classifier design. In the former method, the test vector does not participate in the selection of the features, but in the latter the test vector takes part in the SVM-RFE. In this work, the second method (test in training) will be only applied on the inter-subject classification in order to check the influence of the LOO strategy in the SVM-RFE. For inter-subject classification tasks, feature vectors x are made up by computing the ST from averaged EEG signals (ERPs) belonging to each condition (30–36 trials), that is, for each subject, two 9-dimensional feature vectors are extracted, one per condition (positive and negative valence). In intra-subject classification tasks, ST values are computed from the single-trials and one classification task is carried out for each subject and for each channel. All the numerical routines, needed for the implementation of the method explained above, were implemented using software MATLAB. In particular, tools from BioInformatics toolbox were required. In order to complete the conclusions with some statistical contrasts, paired t-tests were performed using STATISTICA software. 2.3 Experiment A total of 26 female volunteers participated in the study. EEG activity was recorded from 21 Ag/AgCl electrodes (EEG channels), positioned on an electrode cap from EasyCap according to the 10/20 system (Fp1, Fpz, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, T8, O1, Oz, O2), internally referenced to an electrode on the tip of the nose, and 2 electrooculograms (vertical and horizontal) were sampled at 1kHz and stored. The impedances of all electrodes were kept below 5kΩ. The signals were preprocessed (artifact rejected based on amplitude values, eye-movement correction based on [22] and segmented) using the SCAN 4.3 software (Compumedics Neuroscan, Germany). The EEG was recorded while the volunteers were visualizing pictures selected from the International Affective Picture System (IAPS), which is widely used in psychological experiments. It is composed of pictures classified in terms of arousal and affective valence. Twenty four high-arousal (score s >6) images, 12 of them corresponding to positive valence (7.29±0.65) and 12 to negative valence (1.47±0.24) were selected. Each image was presented on the center of the screen three times in a pseudo-random order and each trial lasted 3500ms: during the first 750ms, a fixation cross was presented, then one of the images was shown during 500ms, and finally a black screen followed for a period of 2250ms. The single-trial signal length was 950ms, including 150ms previous to the stimulus onset. 3 Results 3.1 Topographical maps In Fig. 1, the most interesting topographical averaged maps of the ST are shown for visualization purposes. As it can be observed, very similar ST is reflected in both experimental conditions. A larger stability is observed in parietal and occipital regions in early time intervals (after 250ms), whereas a higher spectral variations are obtained in frontal and temporal areas, which decrease progressively. Table 1 shows the statistical differences found between the two valence conditions (negative vs positive) applying t-tests on ST values. These results cover the complete single-trial epoch except on the baseline line (i.e. single-trial: from 0ms to 800ms) and confirm a higher relevance of the time intervals after 300ms post stimulus. Specifically, the time interval from 500ms to 650ms shows a great significance, mainly for central and parietal regions. In regard to the most relevant channel, T7 deserves a special attention from 350ms to 550ms. 3.2 Inter-subject classification Fig. 2 shows the classification results when positive and negative ERPs are compared using all subjects in the database. A different classification task is performed for each EEG channel. SVM-RFE selection improves the accuracy in 7.51% on average, this improvement is statistically significant (p <0.001). 3.3 Intra-subject classification Fig. 3 shows the global results for the intra-subject classification tasks. SVM-RFE selection improves significantly (p <0.001) the accuracy an average of 13.25%. This classification task yielded up to 83.33% accuracy in some cases (see Table 2). In general, the results did not improved using a majority voting-based classifier or taking the features from all channels as a whole set, neither for intra-subject nor for inter-subject classification tasks. 3.4 Most relevant EEG sensors According to the previous results, using the proposed feature selection method for both inter-subject (80.77% accuracy) and intra-subject (67.12% as global mean and 81.67% as maximal accuracy) classification tasks, the most relevant EEG sensor is T7 (temporal channel) located on the left scalp region. From this area, next to the left temple, the highest accuracy rates were reached for 43% of the subjects (see Table 2 for a more detailed analysis of the maximal accuracy values obtained and the most relevant sensors among participants). Figs. 4 and 5 show ERP and ST values respectively for T7 channel for both experimental conditions. 4 Discussion Spectral turbulence has proven to be an alternative to other spectral analyses for EEG signals in order to compare two experimental conditions and obtain relevant conclusions, since statistically significant differences are obtained and minor computational resources are required. According to the representations provided in Fig. 1 , a clear difference in the spectral turbulence between positive and negative valence is evident in the left temporal region. This scalp region is often associated to the insular activity that takes part in the emotional processing of visual stimuli [23]. Furthermore, slight differences can be observed on the central regions of the scalp, mainly in the last time intervals. Note that although some research works point theta band (from 4Hz to 7Hz) as a relevant frequency band in emotional processing [24], it is not so suitable for ST computing because its waves variation is too slow to give accurate information about the turbulence in narrow intervals. Regarding the accuracy results from the classification tasks, as it is reported in other research works, intra-subject classification tasks using EEG single trials give usually lower accuracy values than inter-subject classification tasks [25]. The moderated global results are influenced by the high individual variability. This fact makes more difficult to extract a generic pattern from the ST values. In regard to the most relevant EEG channels selected by the algorithm (T7 channel), inspecting the Figs. 4 and 5, it is risky to draw conclusions from a visual scanning of the averages, since they show very similar shapes. For this reason it was advisable to analysis the induced potentials (spectral domain) that are not phase-locked with the stimulus presentation [26]. By means of using single trials for computing ST, the mentioned induced contribution is taken into account. The higher spectral stability (higher ρ values) on the temporal region with negative stimuli could mean a longer synchronization of the neural networks linked to the insular activity [23]. Furthermore, it is worth to note that T7, besides of reflecting the activity of the insula, might also provide any hidden information about the blood stream from the temples veins. Thus, a more detailed analysis of ST could even provide appreciable information for calibrating determined artifacts from EEG. It would be bold to claim that T7 channel is the only one relevant channel when ST is used as feature. The conclusions drawn from this study may be considered as preliminary results. Therefore, in future works it would be convenient to analyze in depth the possibilities of employing ST for machine learning applications related to EEG in general and particularly for affective computing. Nowadays, in research works on emotion, the ST measure has not been used yet, as the authors concern. However, as it is demonstrated that different frequency bands are modulated depending on emotions, it is suitable to test new approaches on the spectral domain that could provide new valuable information about how brain processes its surroundings. 5 Conclusions Spectral turbulence (ST) measure is presented as a good alternative in feature extraction and data input reduction for classifying EEG signals and selecting the most suitable EEG channels for discriminating among psychological processes, specifically affective valence detection. SVM-RFE algorithm for feature selection applied on a SVM-based classifier is implemented, increasing up to 13.25% the accuracy rates. The most relevant EEG sensor found is T7, which measures the electrical activity from the left temporal lobe, and reaches up to 80.77% in inter-subject classification using ERPs. Results derived from inter-subject are more reliable than those from intra-subject classification, due to the individual differences. In future works, it can be recommendable to compute ST from EEG signals for determining psychological disorders and BCI applications. Acknowledgments This work is partially funded by FEDER through the Operational Program Competitiveness Factors – COMPETE and by National Funds through FCT – Foundation for Science and Technology in the context of the project FCOMP-01-0124-FEDER 022682 (FCT reference PEst-C/EEI/UI0127/2011). References [1] P. Sauseng W. Klimesch W. Gruber S. Hanslmayr R. Freunberger M. Doppelmayr Are event-related potential components generated by phase resetting of brain oscillations? A critical discussion Neuroscience 146 2007 1435 1444 [2] G.J. Kelen R. Henkin A.-M. Starr E.B. Caref D. Bloomfield N. El-Sherif Spectral turbulence analysis of the signal-averaged electrocardiogram and its predictive accuracy for inducible sustained monomorphic ventricular tachycardia American Journal of Cardiology 67 1991 965 975 [3] M. Malik P. Kulakowski K. Hnatkova A. Staunton A.J. Camm Spectral turbulence analysis versus time-domain analysis of the signal-averaged ECG in survivors of acute myocardial infarction Journal of Electrocardiology 27 Suppl. 1994 227 232 [4] P.R.B. Barbosa A. de Souza Bomfim E.C. Barbosa P. Ginefra S.H.C. Boghossian C. Destro J. Nadal Spectral turbulence analysis of the signal-averaged electrocardiogram of the atrial activation as predictor of recurrence of idiopathic and persistent atrial fibrillation International Journal of Cardiology 107 2006 307 316 [5] J. Poza R. Hornero J. Escudero A. Fernández C. Gomez Analysis of spontaneous MEG activity in Alzheimer's disease using time-frequency parameters 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS) 2008 5712 5715 [6] L. Brown B. Grundlehner J. Penders Towards wireless emotional valence detection from EEG 33rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS) (2011) Boston, MA, USA 2011 [7] E. León G. Clarke V. Callaghan F. Sepúlveda A user-independent real-time emotion recognition system for software agents in domestic environments Engineering Applications of Artificial Intelligence 20 2007 337 345 [8] J.K. Olofsson S. Nordin H. Sequeira J. Polich Affective picture processing: an integrative review of ERP findings Biological Psychology 77 2008 247 265 [9] B. Güntekin E. Basar Event-related beta oscillations are affected by emotional eliciting stimuli Neuroscience Letters 483 2010 173 178 [10] S. Delplanque M.E. Lavoic P. Hot L. Silvert H. Sequeira Modulation of cognitive processing by emotional valence studied through event-related potentials in humans Neuroscience Letters 356 2004 1 4 [11] M. Macas M. Vavrecka V. Gerla L. Lhotska Classification of the emotional states based on the EEG signal processing 9th International Conference on Information Technology and Applications in Biomedicine, no. November, IEEE Larnaka, Cyprus 2009 1 4 [12] C.A. Frantzidis C. Bratsas C.L. Papadelis E. Konstantinidis C. Pappas P.D. Bamidis Toward emotion aware computing: an integrated approach using multichannel neurophysiological recordings and affective visual stimuli IEEE Transactions on Information Technology in Biomedicine 14 2010 589 597 [13] D. Huang C. Guan K.K. Ang H. Zhang Y. Pan Asymmetric spatial pattern for EEG-based emotion detection International Joint Conference on Neural Networks, no. June, IEEE Brisbane, Australia 2012 1 7 [14] C.S. Herrmann M. Grigutsch N. Busch EEG oscillations and wavelet analysis Event-related Potentials: A Methods Handbook 2005 Handy 229 259 [15] M.A. Klados C. Frantzidis A.B. Vivas C. Papadelis C. Lithari C. Pappas P.D. Bamidis A framework combining delta event-related oscillations (EROs) and synchronisation effects (ERD/ERS) to study emotional processing Computational Intelligence and Neuroscience 2009 549419 [16] I. Guyon J. Weston S. Barnhill V. Vapnik Gene selection for cancer classification using support vector machines Machine Learning 46 2002 389 422 [17] C. Burges A tutorial on support vector machines for pattern recognition Data Mining and Knowledge Discovery 2 1998 121 167 [18] M. López J. Górriz J. Ramírez M. Gómez-Río J. Verdejo J. Vas Component-based technique for determining the effects of acupuncture for fighting migraine using SPECT images Expert Systems with Applications 40 2013 44 51 [19] A. Ben-Hur C.S. Ong S. Sonnenburg B. Schölkopf G. Rätsch Support vector machines and kernels for computational biology PLoS Computational Biology 4 2008 e1000173 10.1371/journal.pcbi.1000173 [20] A.R. Hidalgo-Muñoz M.M. López I.M. Santos A.T. Pereira M. Vázquez-Marrufo A. Galvao-Carmona A.M. Tomé Application of SVM-RFE on EEG signals for detecting the most relevant scalp regions linked to affective valence processing Expert Systems with Applications 40 2013 2102 2108 [21] T. Xu M. Stephane K.K. Parhi Multidimensional analysis of the abnormal neural oscillations associated with lexical processing in schizophrenia Clinical EEG and Neuroscience 44 2013 135 143 [22] G. Gratton M. Coles E. Donchin A new method for off-line removal of ocular artifact Electroencephalography and Clinical Neurophysiology 55 1993 468 484 [23] K. Mériau I. Wartenburger P. Kazzer K. Prehn A. Villringer E. van der Meer H.R. Heekeren Insular activity during passive viewing of aversive stimuli reflects individual differences in state negative affect Brain and Cognition 69 2009 73 80 [24] L. Aftanas A. Varlamow S. Pavlov V. Makhnev N. Reva Affective picture processing: event-related synchronization within individually defined human theta band is modulated by valence dimension Neuroscience Letters 303 2001 115 118 [25] A.M. Tomé A.R. Hidalgo-Muñoz M.M. López A.R. Teixeira I.M. Santos A.T. Pereira M. Vázquez-Marrufo E.W. Lang Feature extraction and classification of biosignals: emotion valence detection from EEG signals BIOSIGNALS 2013. International Conference on Bio-inspired Systems and Signal, no. February Barcelona, Spain 2013 54 60 [26] G. Pfurtscheller F.H.L. da Silva Event-related EEG/MEG synchronization and desynchronization: basic principles Clinical Neurophysiology 110 1999 1842 1857 "
    },
    {
        "doc_title": "Sliding empirical mode decomposition-brain status data analysis and modeling",
        "doc_scopus_id": "84867630248",
        "doc_doi": "10.1007/978-3-642-28696-4_12",
        "doc_eid": "2-s2.0-84867630248",
        "doc_date": "2013-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Biomedical signals are in general non-linear and non-stationary. Empirical Mode Decomposition in conjunction with Hilbert-Huang Transform provides a fully adaptive and data-driven technique to extract Intrinsic Mode Functions (IMFs). The latter represent a complete set of locally orthogonal basis functions to represent non-linear and non-stationary time series. Large scale biomedical time series necessitate an online analysis which is presented in this contribution. It shortly reviews the technique of EMD and related algorithms, discusses the newly proposed SEMD algorithm and presents some applications to biomedical time series recorded during neuromonitoring. © 2013 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Brain connectivity analysis: A short survey",
        "doc_scopus_id": "84869061502",
        "doc_doi": "10.1155/2012/412512",
        "doc_eid": "2-s2.0-84869061502",
        "doc_date": "2012-11-19",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Neuroscience (all)",
                "area_abbreviation": "NEUR",
                "area_code": "2800"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Abstract concept",
            "Brain connectivity",
            "Functional imaging",
            "GraphicaL model",
            "Resting state",
            "Static and dynamic"
        ],
        "doc_abstract": "This short survey the reviews recent literature on brain connectivity studies. It encompasses all forms of static and dynamic connectivity whether anatomical, functional, or effective. The last decade has seen an ever increasing number of studies devoted to deduce functional or effective connectivity, mostly from functional neuroimaging experiments. Resting state conditions have become a dominant experimental paradigm, and a number of resting state networks, among them the prominent default mode network, have been identified. Graphical models represent a convenient vehicle to formalize experimental findings and to closely and quantitatively characterize the various networks identified. Underlying these abstract concepts are anatomical networks, the so-called connectome, which can be investigated by functional imaging techniques as well. Future studies have to bridge the gap between anatomical neuronal connections and related functional or effective connectivities. © 2012 E. W. Lang et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sliding empirical mode decomposition for on-line analysis of biomedical time series",
        "doc_scopus_id": "79957969968",
        "doc_doi": "10.1007/978-3-642-21501-8_37",
        "doc_eid": "2-s2.0-79957969968",
        "doc_date": "2011-06-08",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical signal",
            "Data-driven",
            "Empirical Mode Decomposition",
            "Hilbert Huang transforms",
            "Intrinsic mode functions",
            "Neuromonitoring",
            "Non-linear",
            "Non-stationary time series",
            "Nonstationary",
            "On-line analysis",
            "Orthogonal basis function",
            "Biomedical signal",
            "Data driven technique",
            "Empirical Mode Decomposition",
            "Hilbert Huang transforms",
            "Intrinsic Mode functions",
            "Non-stationary time series",
            "Orthogonal basis function",
            "Related algorithms"
        ],
        "doc_abstract": "Biomedical signals are in general non-linear and non-stationary. Empirical Mode Decomposition in conjunction with Hilbert-Huang Transform provides a fully adaptive and data-driven technique to extract Intrinsic Mode Functions (IMFs). The latter represent a complete set of orthogonal basis functions to represent non-linear and non-stationary time series. Large scale biomedical time series necessitate an on-line analysis which is presented in this contribution. It shortly reviews the technique of EMD and related algorithms, discusses the newly proposed slidingEMD algorithm and presents some applications to biomedical time series from neuromonitoring. © 2011 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Singular spectrum analysis for an automated solvent artifact removal and baseline correction of 1D NMR spectra",
        "doc_scopus_id": "79955901962",
        "doc_doi": "10.1016/j.jmr.2011.03.001",
        "doc_eid": "2-s2.0-79955901962",
        "doc_date": "2011-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biophysics",
                "area_abbreviation": "BIOC",
                "area_code": "1304"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Nuclear and High Energy Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3106"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            }
        ],
        "doc_keywords": [
            "AUREMOL-SSA/ALS",
            "Baseline correction",
            "Finite response filter",
            "Oversampled",
            "Singular spectrum analysis",
            "Solvent suppression"
        ],
        "doc_abstract": "NMR spectroscopy in biology and medicine is generally performed in aqueous solutions, thus in 1H NMR spectroscopy, the dominant signal often stems from the partly suppressed solvent and can be many orders of magnitude larger than the resonances of interest. Strong solvent signals lead to a disappearance of weak resonances of interest close to the solvent artifact and to base plane variations all over the spectrum. The AUREMOL-SSA/ALS approach for automated solvent artifact removal and baseline correction has been originally developed for multi-dimensional NMR spectroscopy. Here, we describe the necessary adaptations for an automated application to one-dimensional NMR spectra. Its core algorithm is still based on singular spectrum analysis (SSA) applied on time domain signals (FIDs) and it is still combined with an automated baseline correction (ALS) in the frequency domain. However, both steps (SSA and ALS) have been modified in order to achieve optimal results when dealing with one-dimensional spectra. The performance of the method has been tested on one-dimensional synthetic and experimental spectra including the back-calculated spectrum of HPr protein and an experimental spectrum of a human urine sample. The latter has been recorded with the typically used NOESY-type 1D pulse sequence including water pre-saturation. Furthermore, the fully automated AUREMOL-SSA/ALS procedure includes the managing of oversampled, digitally filtered and zero-filled data and the correction of the frequency domain phase shift caused by the group delay time shift from the digital finite response filtering. © 2011 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272577 291210 291687 291703 291719 291790 291857 31 Journal of Magnetic Resonance JOURNALMAGNETICRESONANCE 2011-03-06 2011-03-06 2011-05-10T22:30:19 S1090-7807(11)00084-X S109078071100084X 10.1016/j.jmr.2011.03.001 S300 S300.1 FULL-TEXT 2015-05-15T05:11:43.768924-04:00 0 0 20110601 20110630 2011 2011-03-06T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav absattachment articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast otherkwds primabst ref specialabst alllist content subj ssids 1090-7807 10907807 210 210 2 2 Volume 210, Issue 2 5 177 183 177 183 201106 June 2011 2011-06-01 2011-06-30 2011 Regular Articles article fla Copyright © 2011 Elsevier Inc. All rights reserved. SINGULARSPECTRUMANALYSISFORAUTOMATEDSOLVENTARTIFACTREMOVALBASELINECORRECTION1DNMRSPECTRA DESANCTIS S 1 Introduction 2 Materials and methods 2.1 Back-calculated dataset 2.2 Experimental data 2.3 Software 3 Theory and implementation 3.1 Singular spectrum analysis 3.2 Preparation of data for SSA 3.3 Implementation of SSA 3.4 Baseline correction 4 Results and discussion 4.1 Solvent suppression by SSA/ALS 4.2 Automated linear spline (ALS) baseline correction 5 Conclusion Acknowledgments References BROWN 1989 193 201 J NICHOLSON 1984 365 375 J PETROFF 1986 1270 1276 O SAHRBACHER 2002 827 840 U GRONWALD 2004 33 96 W MALLONI 2010 101 111 W SAFFRICH 1993 304 308 R RIED 2004 121 131 A MAURER 2004 5906 5918 T GHIL 2002 3.1 3.41 M MOSKAU 2002 164 176 D DIAMANTARAS 1996 K PRINCIPALCOMPONENTNEURALNETWORKSTHEORYAPPLICATION JOLIFFE 2002 I PRINCIPALCOMPONENTANALYSIS GOLYANDINA 2001 N ANALYSISTIMESERIESSTRUCTURESSARELATEDTECHNIQUES TEIXEIRA 2009 2433 2443 A GUENTERT 1992 403 407 P SMALLCOMBE 1995 295 303 S PIOTTO 1992 661 665 M SKLENAR 1993 241 245 V HWANG 1995 275 279 T SIMPSON 2005 340 346 A DESANCTISX2011X177 DESANCTISX2011X177X183 DESANCTISX2011X177XS DESANCTISX2011X177X183XS item S1090-7807(11)00084-X S109078071100084X 10.1016/j.jmr.2011.03.001 272577 2011-05-11T17:12:23.49879-04:00 2011-06-01 2011-06-30 true 701907 MAIN 7 82594 849 656 IMAGE-WEB-PDF 1 si6 1200 47 229 si5 429 15 79 si4 563 40 79 si3 976 19 265 si2 978 46 138 si1 653 19 116 si9 302 19 29 si8 218 17 14 si7 218 17 14 si11 302 19 29 si10 302 19 29 gr2 22531 264 336 gr2 3807 164 209 gr2 83910 760 969 gr3 16961 263 337 gr3 2530 164 210 gr3 56040 759 974 gr4a 40580 505 685 gr4a 3249 161 219 gr4a 161554 1787 2426 gr4b 40014 517 685 gr4b 3213 164 217 gr4b 143184 1612 2135 gr5 50574 860 365 gr5 2186 163 69 gr5 226707 2788 1183 fx1 true 16707 244 198 fx1 true 4164 164 133 fx1 50712 651 528 gr1 48759 773 377 gr1 2855 164 80 gr1 205338 2826 1378 YJMRE 4647 S1090-7807(11)00084-X 10.1016/j.jmr.2011.03.001 Elsevier Inc. Fig. 1 Flow chart of AUREMOL-SSA/ALS. Fig. 2 AUREMOL-SSA/ALS applied to a 1D spectrum of a protein. The one-dimensional spectrum of HPr (H15A) from Staphylococcus aureus has been calculated with the AUREMOL routine RELAX-JT2. An experimental one-dimensional solvent FID measured in 90 % H2O 10 % D2O at 600MHz with solvent pre-saturation was added to the protein time domain data simulated with the same parameters as used for the experimental data. The water artifact is approximately 500 times stronger than a typical protein resonance. Comparison of the complete simulated protein spectrum (middle trace) with the spectrum containing the water artifact (bottom signal) and the spectrum after application of AUREMOL-SSA/ALS (top trace). Fig. 3 Resonances close to the water line recovered by AUREMOL-SSA/ALS. Zoom of the spectrum shown in Fig. 2 with the protein spectrum including the water resonance (bottom signal), the original spectrum without any water signal added (middle trace) and after application of SSA/ALS (top spectrum). Fig. 4 Quantitative analysis of the performance of SSA. As in Figs. 2 and 3 a one-dimensional spectrum of HPr from S. aureus was simulated, the obtained FID was combined with an FID of an experimental water artifact signal with various relative intensities. As a measure for the performance of AUREMOL-SSA/ALS the l2-norm was calculated for the original simulated HPr spectrum and an HPr spectrum where a water signal was added and that was processed with AUREMOL-SSA/ALS. The l2 norm of a spectrum where the solvent signal was 500-times stronger than the most intense protein signal was arbitrarily set to 1. (a) Dependence of the l2 norm on the relative intensity of the solvent signal for a spectrum of HPr with 2K complex data points. (b) Dependence of the performance of SSA on the number of components used. The l2 norm was calculated for a 2K and 16K FID with a ratio of solvent to protein signal intensity of 2. Fig. 5 AUREMOL-SSA/ALS applied on experimental data with strong baseline distortions. The one-dimensional urine spectrum is recorded at 600MHz. Solvent removal and baseline correction by means of SSA and ALS. (a) Complete experimental spectrum of the urine, (b) spectrum after application of SSA, and (c) the spectrum after application of ALS. Singular spectrum analysis for an automated solvent artifact removal and baseline correction of 1D NMR spectra Silvia De Sanctis a Wilhelm M. Malloni a Werner Kremer a Ana M. Tomé b Elmar W. Lang a Klaus-Peter. Neidig c Hans Robert Kalbitzer a ⁎ a Institute of Biophysics and Physical Biochemistry, University of Regensburg, D-93040 Regensburg, Germany b Department of Electrical Engineering, Telecommunications and Informatics, University of Aveiro, P-3100 Aveiro, Portugal c BioSpin GmbH, Software Department, Silberstreifen 4, D-76287 Rheinstetten, Germany ⁎ Corresponding author. Fax: +49 941 943 2479. Graphical abstract Schematic view of ID AUREMOL-SSA/ALS. Highlights ► Automated management of digitally filtered one-dimensional NMR spectra. ► Automated solvent removal in the time domain by singular spectrum analysis (SSA). ► Automated base line correction in the frequency domain by automated linear spline (ALS) including an automated line width assessment. NMR spectroscopy in biology and medicine is generally performed in aqueous solutions, thus in 1H NMR spectroscopy, the dominant signal often stems from the partly suppressed solvent and can be many orders of magnitude larger than the resonances of interest. Strong solvent signals lead to a disappearance of weak resonances of interest close to the solvent artifact and to base plane variations all over the spectrum. The AUREMOL-SSA/ALS approach for automated solvent artifact removal and baseline correction has been originally developed for multi-dimensional NMR spectroscopy. Here, we describe the necessary adaptations for an automated application to one-dimensional NMR spectra. Its core algorithm is still based on singular spectrum analysis (SSA) applied on time domain signals (FIDs) and it is still combined with an automated baseline correction (ALS) in the frequency domain. However, both steps (SSA and ALS) have been modified in order to achieve optimal results when dealing with one-dimensional spectra. The performance of the method has been tested on one-dimensional synthetic and experimental spectra including the back-calculated spectrum of HPr protein and an experimental spectrum of a human urine sample. The latter has been recorded with the typically used NOESY-type 1D pulse sequence including water pre-saturation. Furthermore, the fully automated AUREMOL-SSA/ALS procedure includes the managing of oversampled, digitally filtered and zero-filled data and the correction of the frequency domain phase shift caused by the group delay time shift from the digital finite response filtering. Abbreviations SSA singular spectrum analysis ALS automated linear spline FIR finite impulse response filter Keywords AUREMOL-SSA/ALS Singular spectrum analysis Solvent suppression Baseline correction Oversampled digitally filtered data Group delay Finite response filter 1 Introduction Since its discovery, nuclear magnetic resonance spectroscopy has become an essential tool for numerous applications in biomedicine, biophysics and biochemistry. Multi-dimensional NMR spectroscopy of isotope-enriched proteins and nucleic acids clearly represents a powerful technique to solve complicated structural problems. However, one-dimensional spectroscopy of unlabelled samples has still important applications. Examples are functional studies of bio-macromolecules that are not available in isotope-enriched form, including interaction studies with small ligands in drug design or folding/unfolding studies of proteins. Another example may be the high-throughput NMR analysis of biofluids such as urine [1], blood plasma [2], bile, cerebrospinal fluids [3] and cervical mucus [4]. The quantitative one-dimensional 1H NMR spectroscopic analysis of body fluids as well as macromolecules such as proteins is typically hampered by the presence of a dominant resonance arising from the solvent signal. Suppressing this solvent artifact, either by experimental methods (water suppression and/or tailored excitation) or by computational post-processing methods, is thus a key issue in proton NMR (for a review see [5]). Ideally, a suppression procedure should not (1) alter the intensities of solute resonances, (2) introduce baseline artifacts, or (3) obscure significant signals hidden underneath the intense solvent signal. With modern NMR spectrometers but not with the majority of older NMR spectrometers still in use, the two first points can be achieved experimentally rather well when the concentration of the solute is not too small, the third point represents a problem especially when using tailored excitation methods where a significant region around the water resonance frequency is blanked out. Many post-processing suppression methods have been proposed in literature to solve the latter problem (listed e.g. by [5,6]). However, the existing filtering techniques developed so far adequately cannot recover the shapes and intensities of the solute resonances under the solvent peak and/or cannot be used automatically without human intervention. Malloni et al. [6] used singular spectrum analysis (SSA) combined with automated linear spline (ALS) baseline correction [6] for an automated removal of the solvent artifact and of baseline distortions in multi-dimensional NMR spectra. AUREMOL-SSA/ALS automatically identifies the solvent artifact signal and removes it while keeping those hidden resonances of interest that appear under the water signal. Moreover, an automated baseline correction is performed by a linear spline interpolation [7]. The crucial point here is an automated recognition of signals and baseline points independently of the type of the spectrum and the kind of sample. The published implementation of AUREMOL-SSA/ ALS was only developed for higher-dimensional NMR data [6] but did not give satisfactory results for one-dimensional NMR spectra. In particular, it is necessary to take into account that one-dimensional spectra of biomolecules are usually very crowded and may contain resonances with widely differing line widths. In addition, the digital resolution of one-dimensional NMR spectra is typically much higher than in multi-dimensional spectra and thus leads to the necessity of extracting much more components from the time domain signal in order to achieve an accurate separation of the solvent from the rest of the spectrum. In this paper we will introduce the necessary adaptations on AUREMOL-SSA/ALS for the use in one-dimensional spectroscopy and show the applicability of the method and its limits in two typical situations: NMR spectroscopy of proteins and biofluids. 2 Materials and methods 2.1 Back-calculated dataset A synthetic one-dimensional spectrum has been calculated with the AUREMOL routine RELAX-JT2 [8] starting from the three-dimensional structure of a mutant of histidine-containing phosphocarrier protein (HPr(H15A)) [9] from Staphylococcus aureus and using the corresponding experimental chemical shifts. RELAX-JT2 simulates multiplet structures as well as line widths. A one-dimensional 600MHz NOESY spectrum with a mixing time of 0.15s, a relaxation delay of 1.5s, a spectral width of 12.65ppm, and 2048 complex time domain data points was simulated. The resulting time domain data was filtered by exponential multiplication with a line broadening of 3Hz. The water artifact was produced by measuring a one-dimensional spectrum of 90% H2O/10% D2O with solvent pre-saturation at 600MHz, having the same acquisition parameters as those used for the HPr spectrum simulation. Before the Fourier transformation the water artifact signal was added to the synthetic time domain signal of the protein scaled in such a way that the maximum of the water signal was about 500 times stronger than a typical protein resonance. 2.2 Experimental data The urine spectrum was recorded with a Bruker Avance-600 spectrometer operating at a 1H frequency of 600.13MHz. It was acquired using oversampling and digital filtering (Bruker DQD mode). A NOESY-type 1D pulse sequence was used for the sample, including a selective pre-saturation of the solvent resonance and a spoiling z-gradient pulse applied during the mixing time. For obtaining standardized conditions 133mM sodium phosphate buffer, pH7.4, 5% D2O, and 0.1mM DSS (4,4-dimethyl-4-silapentane-1-sulfonic acid) was added. The spectrum was recorded at 298K using a mixing time of 10ms and a relaxation delay of 5s. The spectral width was 20.0ppm and 128K complex time domain points were sampled. 2.3 Software The NMR data were acquired with the program TOPSPIN 2.0 (Bruker, Karlsruhe). All routines developed have been integrated in the AUREMOL software package and can be downloaded from www.auremol.de. 3 Theory and implementation 3.1 Singular spectrum analysis Only aspects essential for an application of SSA [10] to one-dimensional spectra will be discussed here, since the general method has been described earlier for multi-dimensional spectra by Malloni et al. [6]. The general procedure is summarized in a flow chart (Fig. 1 ). The method starts in the time domain and if the data are recorded by oversampling and finite response (FIR) filtering they have to be preprocessed adequately. After subtracting the mean of the signal, SSA is applied. The obtained time domain data are multiplied with an appropriate filter function and Fourier transformed. A phase correction has to be applied in accordance to the group delay. The last step is the baseline correction performed by automated linear spline (ALS): the baseline points are identified automatically. The interpolated baseline regions are subtracted from the frequency domain data. 3.2 Preparation of data for SSA Meaningful application of SSA for separating and removing the strongest signal (the solvent signal) requires data with periodic structure as it is represented by a complex FID signal. Modern spectrometers use oversampling of the data followed by digital frequency filtering by finite impulse response filters and a reduction of the stored spectral range [11]. FIR filtering leads to a delayed response where the first time points are corrupted. For practical reasons, these data do not contain useful spectral information and can be considered as a delayed data acquisition characterized by the group delay Δ g = A gΔt with A g a factor and Δt the dwell time. One of the manufacturers (Bruker Biospin) stores these data points as part of the FID. The resulting time domain signal thus is increasing from zero before the damped “normal” FID s(nΔt) begins. For application of SSA (and other time domain applications), the FID of length N has to be left shifted by N g data points with (1) N g = floor ( 2 A g ) since the inclusion of this data leads to spectral artifacts. Therefore, before applying SSA, the data points belonging to the group delay are removed and stored for a subsequent reconstruction. They are excluded while the SSA identifies and nullifies the solvent component but they are re-introduced before the Fourier transformation. The time shift due to the removal of these points causes a phase shift in the frequency data. A correspondent phase correction in the frequency domain is automatically applied before correcting the baseline. 3.3 Implementation of SSA The reduced, centered FID of length N r = N − N g is embedded into a feature space of dimension M that corresponds to the number of desired components extracted, whereby a fixed shift of Δn t =1 is used to generate the trajectory matrix X of dimension M × Q with Q = N r − M +1. Thus X contains as many time lagged copies of the complex FID s(nΔt) as the number of components, shifted by one data point for every row of the data matrix. Due to the higher resolution of an usual one-dimensional spectrum, an embedding of M =20 (used in default as described in [6]) is no more suitable thus it is increased to 40, whereas the rest of the algorithm is almost unchanged (see Section 4). Centering of the FID is done by calculating the mean of s as (2) 〈 s 〉 = 1 N r ∑ n = 0 N r - 1 s ( n Δ t ) and subtracting it from s. Finally, the zero mean elements of X are computed as (3) x ij = s ( n Δ t ) with n = M - i + j - 1 The correlation matrix C is defined as (4) C = 1 Q XX t with Xt the Hermitian transpose of X. The Hermitian matrix C can be decomposed by (5) C = UDU t with the unitary matrix U of eigenvectors ui and the diagonal matrix D. Diagonalization of C is done by standard techniques according to [12,13]. X can be written as (6) X = UU t X = ∑ i = 1 M u i u i t X = ∑ i = 1 M X i with the (M ×1) and (1× M) matrices u i and u i t containing the ith column of U and the ith row of U t , respectively. For solvent suppression the u i and u i t corresponding to the largest eigenvalue of C (highest covariance) is set to zero resulting in a matrix X red of reduced rank. Generally, the elements along each descending diagonal of X red will not be identical as they are in the original trajectory matrix X. This can be cured, however, by replacing the entries in each diagonal by their average, obtaining again a Toeplitz matrix X red ′ [14,15]. As last step, an FID is reconstructed reverting the embedding from the matrix X red ′ and the previously stored group delay points are appended to the reconstructed time domain signal. Since the group delay data points start invariably with a stretch of zeros, for avoiding truncation artifacts, the mean of the last quarter of the total data points of the FID obtained from X red ′ has to be subtracted before appending the stored group delay data points. Since the group delay itself is usually not an integer multiple of the dwell time Δt, a first order phase correction φ can be calculated directly by φ =2π(2A g − N g) and has later to be applied after the final Fourier transformation of the data. 3.4 Baseline correction After removing the strong solvent signal if present, the baseline usually needs to be further corrected in the frequency domain. For higher-dimensional spectra the automated linear spline (ALS) [6,7] has shown to be efficient. The automated recognition of baseline points and signal points is crucial for the proper function of ALS. A method similar to that one proposed by Guentert and Wuethrich [16] but with a variable window size W is used in ALS. The size W of the window examined around a data point k must be clearly larger than the expected line width of a protein resonance peak for the recognition of baseline points. This parameter has to be automatically determined from an analysis of the line widths occurring in the spectrum and its correct choice is especially important in the complicated spectra of biological samples with large variations in line widths. In the multi-dimensional case [6] a histogram of all line widths occurring in the spectrum is built and W is fixed to twice the most frequently occurring line width. Tests on one-dimensional spectra of several biomaterials with a large variation of occurring line width values showed that this definition of W does not work satisfactorily since resonance lines with large line widths are not recognized correctly. It turned out that a window size W set to the double of the maximal occurring value yielded optimal results. 4 Results and discussion 4.1 Solvent suppression by SSA/ALS SSA and ALS were applied to a synthetic one-dimensional spectrum of HPr protein and to an experimental one-dimensional spectrum of human urine. The synthetic spectrum allows the quantitative investigation of the performance of SSA since the original protein spectrum before adding the solvent signal can be compared with the result after application of SSA/ALS. Especially, the back-calculation of the one-dimensional time domain signal of HPr protein allows the identification of the peaks of interest that are located in the water artifact region before adding the solvent signal. Since the back-calculated data are generated without any digital filtering, the algorithm automatically recognized that no group delay data points have to be taken into account. After the addition of the partly saturated water signal to the time domain signal of the protein, the SSA removal procedure was applied. As demonstrated later, for 1D-spectra consisting of 16K ore more complex time domain data points, 40 components would be extracted by SSA. Since the simulated 1D spectrum of HPr has not a high digital resolution (2K), 20 components showed to be sufficient and a window size W of 92Hz was automatically calculated for ALS. The central trace of Fig. 2 shows the simulated HPr spectrum after exponential filtering and Fourier transformation of the time domain data. The bottom spectrum was obtained by addition of a very strong solvent time domain to the simulated FID using an identical domain filtering followed by a Fourier transformation. The strong water signal obscures the protein resonances lying in the center of the spectrum and introduces strong baseline distortions with anti-phase contributions and also additional truncation artifacts especially visible in the high field and the low field regions of the spectrum. After application of the 1D version of AUREMOL (Fig. 2 top trace), the water signal is almost reduced to zero, the baseline is almost perfect, and most of the truncation artifacts have been removed. The intensities of the protein signals are not influenced by the procedure, a property very important for quantitative evaluations of NMR spectra. A zoom of the central area of the spectrum is shown in Fig. 3 . The HPr resonances in the range between 4.4ppm and 5.5ppm are severely compromised by the residual water signal and can hardly be evaluated. The situation changes when SSA/ALS is applied (Fig. 3, top trace): apart from a small region between 4.7ppm and 5.0ppm all HPr resonances are recovered with accurate intensities. Thus, the algorithm shows a good performance by significantly reducing the distortions all over the spectrum and in the critical solvent region, but apparently the original water signal was too intense to allow a complete recovery of the peaks in the most compromised central region (see top trace of Fig. 3). The qualitative assessment of the performance of AUREMOL-SSA/ALS by inspection of the spectra (Figs. 2 and 3) has been supported by a quantitative analysis. As starting point, the l2-norm (Euclidian norm) between the original simulated HPr spectrum and the same spectrum with the addition of a solvent signal 5000 times stronger than a typical amide protein resonance or 500 times stronger than the strongest protein signals (the superposed signals in methyl region) has been calculated after application of AUREMOL-SSA/ALS. The l2-norm of that spectrum has arbitrarily been set to 1 and the values obtained with other solvent/signal amplitude ratios were scaled correspondingly (Fig. 4 ). As demonstrated in Fig. 4a the use of SSA even on a very distorted spectrum (where the solvent signal is 150 times stronger than the most intense protein resonance) can definitively improve the spectral analysis. As to be expected, when the relative intensity of the solvent resonance is reduced, the performance of the algorithm as measured by the l2-norm is improved reaching an optimum when the water artifact amplitude is about twice that one of the strongest protein resonance (see the region zoomed out from Fig. 4a). Further reduction of the solvent signal mixed to the protein spectra leads to a slow increase of the l2-norm. This behavior directly follows from the method itself: SSA removes the component of the FID having the largest variance. When the intensity of the solvent signal becomes of the order of the intensity of the protein resonances, the water signal has not the largest variance and thus is not recognized properly by SSA. When its intensity is smaller than roughly the half of the most intense protein resonance, the algorithm removes just this component of the protein signal. Thus, a meaningful application of SSA is clearly related to the relative amplitude of the water artifact thus NMR spectra obtained by excitation sculpting are not suitable for SSA processing. AUREMOL gives out a warning that SSA should not be applied to data where the water signal is not the dominant signal. In Fig. 4a an FID with low digital resolution (2K complex data) was used and only 20 components were extracted for the SSA processing. However, as Fig. 4b shows for spectra with higher digital resolution more than 20 components have to be used for the analysis. Starting with FIDs with an optimal solvent to protein intensity ratio of 2, the dependence of the l2-norm on number of components used in the SSA analysis was followed for a 2K and a 16K FID. In accordance with Malloni et al. [6], optimal values are obtained for the low resolution FID with 20 components. Such a low resolution FID is usually recorded in multi-dimensional spectra. For the high resolution data the optimal removal of the water signal is reached using 40 components (see the region zoomed out in Fig. 4b). In AUREMOL the number of components used is automatically adapted to the size of the data handled. The experimental urine spectrum shown in Fig. 5 is characterized by a large number of well resolved, strongly overlapping resonance lines. The water signal is still very strong (note that the negative parts of the signal were cut in Fig. 5a) but its line width is so small that only small distortions in the central part are visible. The actual spectrum was mainly selected because it also showed a strong baseline distortion that could possibly compromise SSA or ALS. The urine spectrum was also recorded with oversampling and digitally filtering and it is thus a test for the applicability of SSA/ALS to this type of experimental data. As described above, the algorithm automatically calculates the time domain data points belonging to the group delay in accordance with the acquisition parameters (72 time domain data points). They were removed before applying the SSA procedure and appended to the signal only before the final Fourier transformation. As shown above a decomposition by SSA in 20 components is not optimal for 1D spectra with high digital resolution. In all cases tested where SSA was applied to one-dimensional spectra (with a data size of 16K and up to 128K), empirically a separation of 40 components (as in the case reported here) was sufficient and mandatory to avoid the disappearance or the reduction of signals of interest that are close to the solvent resonance. 4.2 Automated linear spline (ALS) baseline correction The automated baseline correction by ALS is an integral component of the solvent suppression technique by SSA since this latter attenuates baseline distortions caused by the solvent signal itself but cannot remove them completely. ALS can, of course, be used independently of SSA. A typical case where ALS has to be applied after solvent suppression by SSA would be on the urine spectrum (Fig. 5). Here, SSA as described in this application removes the strongest signal (that is the water signal) from the spectrum but has only a small effect on the baseline (see Fig. 5b). A sinusoidal baseline distortion caused by data clipping was additionally introduced in the spectrum. As Fig. 5c shows AUREMOL-ALS perfectly removes the baseline distortion. With the calculation method for the determination of the window size W used in multi-dimensional NMR a too small size of 6Hz would result, however the window size calculated from the largest effective line width is 120Hz as it is required for a proper baseline correction. 5 Conclusion We show in this paper that singular spectral analysis (SSA) combined with automated linear spline (ALS) provides a tool to automatically remove the solvent signal from NMR spectra of aqueous solutions and to correct the baseline distortions in one-dimensional NMR spectra. The method has been tested successfully for higher-dimensional NMR spectra earlier [6]. We demonstrate that with the modifications given here it can also be applied to one-dimensional spectra that usually show a much larger degree of signal overlap and large variations of line widths compared to the typical two- or three-dimensional spectra of proteins. Moreover, it has been re-arranged in a way to deal with time domain signals having a higher digital resolution where an increased number of components need to be extracted. In principle, it could be useful to nullify more than one component before reconstructing the signal, an idea that will be worked out in future. We have demonstrated that when a dominant solvent signal is no more present, ALS alone leads to an excellent baseline correction. A perfect baseline is especially mandatory for the quantification of the signals since even small baseline variations lead to large errors in the calculation of concentrations of the compounds in the sample. The solvent peak could hide many solute resonances of interest close to it or provide a bias to the determination of the peak integral. In protein NMR spectroscopy, application of SSA may recover the structurally important Hα-resonances that are otherwise only visible in spectra recorded in D2O. It is clear that using modern NMR spectrometers an excellent water suppression and almost perfect baselines can be achieved with selective excitation techniques such as WET [17], WATERGATE [18,19], and excitation sculpting [20] or by applying more complicated selective pre-saturation sequences such as PURGE [21] instead of the simple NOESY-type pre-saturation sequence used here. Selective excitation methods have always the disadvantage, that a rather large spectral range around the water signal is attenuated or not visible at all that may contain valuable information. In addition, the quality of the water suppression also depends on the concentration of solute under consideration. At millimolar concentrations these methods can attenuate the water signal to such a degree that it is not stronger than a typical resonance of the solute. In contrast at micromolar concentrations often relevant in biology these methods fail and the application of SSA/ALS would lead to a significant improvement of the spectral quality. Since the proposed algorithm is completely automated and also includes the handling of digitally filtered (oversampled) data, Fourier transformation, and phase correction in accordance with the group delay without any user intervention, it can be used just routinely on these data. Acknowledgments This work was supported by the Bavarian Science Foundation (ForNeuroCell), the European Union (SPINE-2), and the Fonds of the Chemical Industry (FCI). References [1] J.C.C. Brown G.A. Mills P.J. Sadler V. Walker 1H NMR studies of urine from premature and sick babies Magn. Reson. Med. 11 1989 193 201 [2] J.K. Nicholson M.P. O’Flynn P.J. Sadler A.F. MacLeod S.M. Juul P.H. Soenksen Proton-nuclear-magnetic-resonance studies of serum, plasma and urine from fasting normal and diabetic subject Biochem. J. 217 1984 365 375 [3] O.A.C. Petroff R.K. Yu T. Ogino High-resolution proton magnetic resonance analysis of human cerebrospinal fluid J. Neurochem. 47 1986 1270 1276 [4] U. Sahrbacher A. Pehlke-Rimpf G. Rohr W. Eggert-Kruse H.R. Kalbitzer High resolution proton magnetic resonance spectroscopy of human cervical mucus J. Pharm. Biomed. Anal. 28 2002 827 840 [5] W. Gronwald H.R. Kalbitzer Automated structure determination of proteins by NMR spectroscopy Prog. NMR Spectr. 44 2004 33 96 [6] W.M. Malloni S. De Sanctis A.M. Tomé E.W. Lang C.E. Munte K.P. Neidig H.R. Kalbitzer Automated solvent artifact removal and base plane correction from multidimensional NMR protein spectra by AUREMOL-SSA J. Biomol. NMR 47 2 2010 101 111 [7] R. Saffrich W. Beneicke K.P. Neidig H.R. Kalbitzer Baseline correction in n-dimensional NMR spectra by sectionally linear interpolation J. Magn. Reson. 101 B 1993 304 308 [8] A. Ried W. Gronwald J.M. Trenner K. Brunner K.P. Neidig H.R. Kalbitzer Improved simulation of NOESY spectra by RELAX-JT2 including effects of J-coupling, transverse relaxation and chemical shift anisotropy J. Biomol. NMR 30 2 2004 121 131 [9] T. Maurer S. Meier N. Kachel C.E. Munte S. Hasenbein B. Koch W. Hengstenberg H.R. Kalbitzer High-resolution structure of the histidine-containing phosphocarrier protein (HPr) from Staphylococcus aureus and characterization of its interaction with the bifunctional HPr kinase/phosphorylase J. Bacteriol. 186 2004 5906 5918 [10] M. Ghil M.R. Allen M.D. Dettinger K. Ide Advanced spectral methods for climatic time series Rev. Geophys. 40 2002 3.1 3.41 [11] D. Moskau Application of real time digital filters in NMR spectroscopy Concepts Magn. Reson. 15 2002 164 176 [12] K.I. Diamantaras S.Y. Kung Principal Component Neural Networks: Theory and Application 1996 John Wiley NY [13] I.T. Joliffe Principal Component Analysis 2002 Springer Berlin [14] N. Golyandina V. Nekrutkin A. Zhigljavsky Analysis of Time Series Structure: SSA and Related Techniques 2001 Chapman and HALL/CRC [15] A.R. Teixeira A.M. Tomé M. Boehm C.G. Puntonet E.W. Lang How to apply non-linear subspace techniques to univariate biomedical time series IEEE Trans. Instrum. Meas. 58 8 2009 2433 2443 [16] P. Guentert K. Wuethrich FLATT-A new procedure for high-quality baseline correction of multidimensional NMR spectra J. Magn. Reson. 96 1992 403 407 [17] S.H. Smallcombe S.L. Patt P.A. Keifer WET solvent suppression and its applications to LC-NMR and high-resolution NMR spectroscopy J. Magn. Reson. Ser. A 117 1995 295 303 [18] M. Piotto V. Saudek V. Sklenar Gradient-tailored excitation for single-quantum NMR-spectroscopy of aqueous-solutions J. Biomol. NMR 2 1992 661 665 [19] V. Sklenar M. Piotto R. Leppik V. Saudek Gradient-tailored water suppression for H-1-N-15 HSQC experiments optimized to retain full sensitivity J. Magn. Reson. A 102 1993 241 245 [20] T.L. Hwang A.J. Shaka Water suppression that works—excitation sculpting using arbitrary wave-forms and pulsed-field gradients J. Magn. Reson. A 112 1995 275 279 [21] A.J. Simpson S.A. Brown Purge NMR: effective and easy solvent suppression J. Magn. Reson. 175 2005 340 346 "
    },
    {
        "doc_title": "Brain status data analysis by sliding EMD",
        "doc_scopus_id": "79956332239",
        "doc_doi": "10.1007/978-3-642-21326-7_9",
        "doc_eid": "2-s2.0-79956332239",
        "doc_date": "2011-05-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical signal",
            "Data analysis",
            "Data-driven",
            "Empirical mode decomposition",
            "Hilbert",
            "Hilbert Huang transforms",
            "Intrinsic mode functions",
            "Neuromonitoring",
            "Non-linear",
            "Non-stationary time series",
            "Nonstationary",
            "Orthogonal basis function",
            "Physiological process",
            "Real time",
            "Series analysis",
            "Spectral transform",
            "Time-periods"
        ],
        "doc_abstract": "Biomedical signals are in general non-linear and non-stationary which renders them difficult to analyze with classical time series analysis techniques. Empirical Mode Decomposition (EMD) in conjunction with a Hilbert spectral transform, together called Hilbert-Huang Transform, is ideally suited to extract informative components which are characteristic of underlying biological or physiological processes. The method is fully adaptive and generates a complete set of orthogonal basis functions, called Intrinsic Mode Functions (IMFs), in a purely data-driven manner. Amplitude and frequency of IMFs may vary over time which renders them different from conventional basis systems and ideally suited to study non-linear and non-stationary time series. However, biomedical time series are often recorded over long time periods. This generates the need for efficient EMD algorithms which can analyze the data in real time. No such algorithms yet exist which are robust, efficient and easy to implement. The contribution shortly reviews the technique of EMD and related algorithms and develops an on-line variant, called slidingEMD, which is shown to perform well on large scale biomedical time series recorded during neuromonitoring. © 2011 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Unsupervised feature extraction via kernel subspace techniques",
        "doc_scopus_id": "78650724578",
        "doc_doi": "10.1016/j.neucom.2010.11.011",
        "doc_eid": "2-s2.0-78650724578",
        "doc_date": "2011-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Cognitive Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2805"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Basis vector",
            "Benchmark data",
            "Data representations",
            "Dual form",
            "Feature extraction methods",
            "Feature extraction techniques",
            "Incomplete Cholesky decomposition",
            "Kernel matrices",
            "Kernel PCA",
            "Low-rank decomposition",
            "Subspace models",
            "Subspace techniques",
            "Training data"
        ],
        "doc_abstract": "This paper provides a new insight into unsupervised feature extraction techniques based on kernel subspace models. The data projected onto kernel subspace models are new data representations which might be better suited for classification. The kernel subspace models are always described exploiting the dual form for the basis vectors which requires that the training data must be available even during the test phase. By exploiting an incomplete Cholesky decomposition of the kernel matrix, a computationally less demanding implementation is proposed. Online benchmark data sets allow the evaluation of these feature extraction methods comparing the performance of two classifiers which both have as input either the raw data or the new representations. © 2010 Elsevier B.V.",
        "available": true,
        "clean_text": "serial JL 271597 291210 291735 291866 31 Neurocomputing NEUROCOMPUTING 2010-12-15 2010-12-15 2011-01-03T22:12:38 S0925-2312(10)00472-8 S0925231210004728 10.1016/j.neucom.2010.11.011 S300 S300.1 FULL-TEXT 2015-05-15T03:05:58.335083-04:00 0 0 20110201 20110228 2011 2010-12-15T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes misctext primabst ref vitae alllist content subj ssids 0925-2312 09252312 74 74 5 5 Volume 74, Issue 5 20 820 830 820 830 201102 February 2011 2011-02-01 2011-02-28 2011 Regular Papers article fla Copyright © 2010 Elsevier B.V. All rights reserved. UNSUPERVISEDFEATUREEXTRACTIONVIAKERNELSUBSPACETECHNIQUES TEIXEIRA A 1 Introduction 2 Feature extraction and classification 2.1 The subspace model for feature generation 2.2 Low-rank approximations to the subspace model 2.2.1 The Nyström method based on a random subset selection 2.2.2 Nyström method based on an incomplete Cholesky decomposition 2.2.3 Basis vectors and Cholesky decomposition 2.3 Centering the data 2.3.1 KPCA with a complete training set 2.3.2 KPCA with a reduced training set 3 Numerical simulations 3.1 Subspace model parameters 3.2 Benchmark data sets 3.2.1 Raw data sets 3.2.2 Transformed data sets 3.3 USPS data set 3.3.1 The raw data set 3.3.2 PCA feature extraction 3.3.3 Kernel feature extraction 4 Concluding remarks Acknowledgments Appendix: Implementation of the Cholesky decomposition References YANG 2002 34 58 M MOGHADDAM 2002 780 788 B MULLER 2001 181 202 K SCHOLKOPF 1998 1299 1319 B SCHOLKOPF 1999 1000 1016 B LI 2008 3244 3250 J FOWLKES 2004 214 225 C YANG 2005 1784 1787 J PRESS 1994 W NUMERICALRECIPESINCARTSCIENTIFICCOMPUTATION CHENG 2009 303 308 P SMOLA 2000 911 918 A INTERNATIONALCONFERENCEMACHINELEARNING SPARSEGREEDYMATRIXAPPROXIMATIONFORMACHINELEARNING GOLUB 1996 G MATRIXCOMPUTATIONS CHOI 1990 304 311 I FINE 2001 243 264 S BAKER 1977 C NUMERICALTREATMENTINTEGRALEQUATIONS WILLIAMS 2001 682 688 C ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMSNIPS13 USINGNYSTROMMETHODSPEEDUPKERNELMACHINES SCHOLKOPF 2002 B LEARNINGKERNELSSUPPORTVECTORMACHINESREGULARIZATIONOPTIMIZATIONBEYOND WANG 2007 254 259 H BACH 2002 1 48 F RATSCH 2001 287 320 G TEIXEIRA 2007 A IEEEINTERNATIONALWORKSHOPMACHINELEARNINGFORSIGNALPROCESSINGMLSP2007 EXPLOITINGLOWRANKAPPROXIMATIONSKERNELMATRICESINDENOISINGAPPLICATIONS XU 2007 1056 1061 Y KIM 2004 227 339 S ACHLIOPTAS 2002 335 342 D ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS SAMPLINGTECHNIQUESFORKERNELMETHODS BAUDAT 2003 21 38 G FRANC 2003 426 433 V 10THINTERNATIONALCONFERENCECOMPUTERANALYSISIMAGESPATTERNS GREEDYALGORITHMFORATRAININGSETREDUCTIONINKERNELMETHODS CAWLEY 2003 2585 2592 G TEIXEIRA 2009 115 124 A ARTIFICIALNEURALNETWORKSICANN2009 FEATUREEXTRACTIONUSINGLINEARNONLINEARSUBSPACETECHNIQUES ZHANG 2009 121 146 K DUDA 2001 R PATTERNCLASSIFICATION MIKA 1999 532 536 S ADVANCESINNEURALINFORMATIONPROCESSING11 KERNELPCADENOISINGINFEATURESPACES CAWLEY 2002 1 6 G EUROPEANSYMPOSIUMARTIFICIALNEURALNETWORKS EFFICIENTFORMATIONABASISINAKERNELINDUCEDFEATURESPACE TEIXEIRAX2011X820 TEIXEIRAX2011X820X830 TEIXEIRAX2011X820XA TEIXEIRAX2011X820X830XA item S0925-2312(10)00472-8 S0925231210004728 10.1016/j.neucom.2010.11.011 271597 2011-01-04T16:02:02.961147-05:00 2011-02-01 2011-02-28 true 435337 MAIN 11 83954 849 656 IMAGE-WEB-PDF 1 si0240 114 13 8 si0239 114 13 8 si0238 114 13 8 si0237 114 13 8 si0236 110 6 9 si0235 110 6 9 si0234 110 6 9 si0233 110 6 9 si0232 146 9 28 si0231 198 9 37 si0230 415 13 84 si0229 273 14 36 si0228 132 9 12 si0227 219 13 27 si0226 111 6 9 si0225 146 9 28 si0224 132 9 12 si0223 166 11 20 si0222 521 16 123 si0221 106 6 7 si0220 111 6 9 si0219 111 6 9 si0218 170 10 31 si0217 159 10 31 si0216 111 6 9 si0215 159 10 31 si0214 231 10 47 si0213 159 10 31 si0212 210 10 40 si0211 111 6 9 si0210 111 6 9 si0209 700 40 139 si0208 138 14 12 si0207 129 12 11 si0206 159 10 31 si0205 111 6 9 si0204 161 11 17 si0203 210 10 40 si0202 996 35 241 si0201 161 11 17 si0200 177 10 35 si0199 177 10 35 si0198 315 13 66 si0197 205 10 37 si0196 135 10 12 si0195 131 10 12 si0194 139 9 14 si0193 130 9 13 si0192 580 13 191 si0191 614 35 126 si0190 111 6 9 si0189 111 6 9 si0188 111 6 9 si0187 111 6 9 si0186 111 6 9 si0185 407 16 89 si0184 283 12 60 si0183 552 15 134 si0182 207 10 37 si0181 111 6 9 si0180 252 11 57 si0179 544 14 136 si0178 135 9 13 si0177 1049 17 328 si0176 596 21 114 si0175 392 13 91 si0174 306 13 70 si0173 449 14 94 si0172 361 14 73 si0171 218 13 26 si0170 586 14 128 si0169 218 13 26 si0168 350 14 103 si0167 166 11 20 si0166 265 15 56 si0165 184 10 35 si0164 184 10 35 si0163 373 12 98 si0162 219 12 51 si0161 426 14 121 si0160 796 16 239 si0159 184 10 35 si0158 437 16 97 si0157 288 15 53 si0156 269 15 57 si0155 774 40 144 si0154 141 12 12 si0153 395 18 98 si0152 480 16 122 si0151 481 16 121 si0150 166 11 20 si0149 248 13 60 si0148 370 14 94 si0147 286 15 57 si0146 285 16 65 si0145 363 15 83 si0144 166 11 20 si0143 449 15 104 si0142 160 11 18 si0141 160 11 18 si0140 166 11 20 si0139 166 11 20 si0138 620 40 97 si0137 588 14 161 si0136 380 16 79 si0135 205 10 37 si0134 218 13 26 si0133 441 14 97 si0132 295 13 63 si0131 300 13 68 si0130 342 14 85 si0129 359 16 81 si0128 718 13 227 si0127 191 10 36 si0126 111 6 9 si0125 111 6 9 si0124 1192 40 246 si0123 253 12 50 si0122 372 14 108 si0121 328 16 65 si0120 128 10 13 si0119 522 12 240 si0118 254 11 54 si0117 335 13 80 si0116 360 14 77 si0115 233 13 32 si0114 434 14 129 si0113 616 13 153 si0112 135 13 11 si0111 305 12 62 si0110 315 11 74 si0109 106 6 7 si0108 287 15 50 si0107 141 12 12 si0106 517 14 110 si0105 213 13 26 si0104 111 6 9 si0103 111 6 9 si0102 166 11 20 si0101 135 10 12 si0100 146 9 16 si0099 135 10 12 si0098 173 9 22 si0097 135 10 12 si0096 158 13 16 si0095 135 10 12 si0094 170 12 18 si0093 135 10 12 si0092 131 9 18 si0091 135 10 12 si0090 131 9 17 si0089 135 10 12 si0088 123 9 14 si0087 135 10 12 si0086 148 12 17 si0085 135 10 12 si0084 149 9 17 si0083 135 10 12 si0082 135 9 18 si0081 135 10 12 si0080 138 9 15 si0079 135 10 12 si0078 136 9 16 si0077 135 10 12 si0076 157 9 19 si0075 131 10 12 si0074 135 10 12 si0073 131 10 12 si0072 135 10 12 si0071 146 9 16 si0070 131 10 12 si0069 135 10 12 si0068 173 9 22 si0067 131 10 12 si0066 135 10 12 si0065 158 13 16 si0064 131 10 12 si0063 135 10 12 si0062 170 12 18 si0061 131 10 12 si0060 135 10 12 si0059 131 9 18 si0058 135 10 12 si0057 135 10 12 si0056 131 9 17 si0055 135 10 12 si0054 135 10 12 si0053 123 9 14 si0052 135 10 12 si0051 135 10 12 si0050 148 12 17 si0049 135 10 12 si0048 135 10 12 si0047 149 9 17 si0046 135 10 12 si0045 135 10 12 si0044 135 9 18 si0043 135 10 12 si0042 131 10 12 si0041 138 9 15 si0040 135 10 12 si0039 135 10 12 si0038 136 9 16 si0037 135 10 12 si0036 135 10 12 si0035 157 9 19 si0034 131 10 12 si0033 135 10 12 si0032 131 10 12 si0031 323 13 72 si0030 131 10 12 si0029 449 13 91 si0028 131 10 12 si0027 352 14 63 si0026 131 10 12 si0025 413 13 86 si0024 131 10 12 si0023 315 13 64 si0022 131 10 12 si0021 369 13 83 si0020 135 10 12 si0019 310 13 67 si0018 131 10 12 si0017 368 13 75 si0016 135 10 12 si0015 361 13 78 si0014 135 10 12 si0013 297 13 64 si0012 135 10 12 si0011 347 13 72 si0010 135 10 12 si0009 354 13 75 si0008 135 10 12 si0007 396 13 91 si0006 159 10 31 si0005 170 10 31 si0004 190 10 38 si0003 106 6 8 si0002 108 9 12 si0001 129 10 11 gr1 12160 126 508 gr1 1232 54 219 gr2 42090 335 809 gr2 2834 91 219 gr3 21190 299 386 gr3 2915 164 211 gr4 29490 312 809 gr4 1784 84 219 gr5 43824 310 809 gr5 2239 84 219 gr6 42658 311 809 gr6 2163 84 219 fx1 11310 151 113 fx1 10312 164 123 fx2 13642 151 113 fx2 10369 164 123 fx3 12305 151 113 fx3 10092 164 123 NEUCOM 12009 S0925-2312(10)00472-8 10.1016/j.neucom.2010.11.011 Elsevier B.V. Fig. 1 Computing features (z k ) using subspace techniques. Fig. 2 Relative size of the subsets and cumulative sum of normalized eigenvalues S(p). Left: ratio of the average size of subset (R) vs the size of the full training set (N) (%). Right: S(p) of the data sets Im (°), Rg ( Δ ), Sp ( □ ), Wv ( • ) and Ba (×). Fig. 3 Relative error between the largest eigenvalues of the kernel matrices deduced from KPCA and greedy KPCA, respectively, N=729. Fig. 4 Normalized cumulative sum of eigenvalues for the reduced training set, left: N=729, and the complete training set, right: N=7291. The different curves correspond to top: σ = 10 , middle: σ = 8 and down: σ = 5 . Fig. 5 Performance of NN using projections in input space (PCA) and in feature space. Training set with: 729 (left) or 7291 (right) images. Fig. 6 Performance of the RL using projections in input space (PCA) and in feature space. Training set with: 729 (left) or 7291 (right) images. Table 1 Data set description: D is dimension of the input vector x n and N is the size of training set. Classification of raw data with a nearest neighbor (NN) classifier and a linear discriminant function (RL). Best [21] D N NN RL t-test B . Cancer ( BC ) 25.9±4.6 9 200 32.5±4.8 26.9±2.7 ⊕ Diabetis ( Di ) 23.5±1.7 8 468 30.1±2.0 23.4±1.7 ⊕ German ( Gr ) 23.6±2.1 20 700 29.4±2.4 24.3±2.9 ⊕ Heart ( Hr ) 16.0±3.3 13 170 23.2±3.7 15.8±3.1 ⊕ F . Solar ( FS ) 32.4±1.8 9 666 39.0±4.9 33.5±1.5 ⊕ Thyroid ( Ty ) 4.4±2.2 5 140 4.3±2.2 14.7±3.1 ⊖ Titanic ( Ti ) 22.4±1.0 3 150 33.0±11.0 22.6±1.0 ⊕ Twonorm ( Tn ) 2.7±0.2 20 400 6.6±0.7 2.6±0.17 ⊖ Image ( Im ) 2.7±0.7 18 1010 3.3±5.4 16.5±0.9 ⊖ Ringnorm ( Rg ) 1.6±0.1 20 400 35.1±1.3 24.7±0.7 ⊖ Splice ( Sp ) 9.5±0.7 60 1000 28.8±1.5 16.2±0.6 ⊖ Waveform ( Wv ) 9.8±0.8 21 400 15.8±0.6 14.8±0.2 ⊖ Banana ( Ba ) 10.7±0.4 2 400 13.6±7.0 46.9±7.0 ⊖ Table 2 KPCA: Error rate (%) using KPCA for feature extraction and a nearest neighbor (NN) as well as a linear discriminant (RL) classifier. t-test: results of a student t-test: Best results reported by [21] versus KPCA (column I1) and KPCA versus KPCA D (column I2), where ⊕ accepts H0 and ⊖ rejects H0. D is the dimension of the input data vector and L is the dimension in feature space. N D KPCA t-test L NN L RL I1 I2 BC 200 9 7 32.5±4.8 21 25.2±4.5 ⊕ ⊕ Di 468 8 17 25.3±1.8 10 23.2±1.6 ⊕ ⊕ Gr 700 20 12 30.0±2.5 12 23.3±2.1 ⊖ ⊕ Hr 170 13 8 22.7±3.4 12 15.8±3.0 ⊕ ⊕ FS 666 9 55 32.2±0.5 25 32.1±0.6 ⊕ ⊕ Ty 140 5 6 4.0±2.2 15 5.8±2.4 ⊕ ⊕ Ti 150 3 9 32.3±1.1 10 22.3±1.0 ⊕ ⊕ Tn 400 20 1 3.4±0.4 1 2.3±0.1 ⊕ ⊕ Im 1010 18 23 2.8±0.6 75 7.9±1.3 ⊕ ⊖ Rg 400 20 40 3.5±0.4 25 1.6±0.1 ⊕ ⊖ Sp 1000 60 600 7.5±2.6 720 4.3±2.1 ⊕ ⊖ Wv 400 21 29 9.7±0.7 2 12.0±0.8 ⊕ ⊖ Ba 400 2 5 13.6±0.4 34 10.7±0.4 ⊕ ⊖ Table 3 Error rate (%) using greedy KPCA. Results of t-test: greedy versus KPCA, where ⊕ accepts H0 and ⊖ rejects H0. The entries of column R are the range of values for the subset size in the training sets. N Greedy KPCA t-test R L NN L RL I1 BC 200 [89,100] 7 32.5±4.8 22 25.2±4.5 ⊕ Di 468 [141,155] 61 30.2±1.9 10 23.1±1.6 ⊕ Gr 700 [412,428] 13 29.1±2.4 12 23.4±2.3 ⊕ Hr 170 [116,124] 48 22.79±2.9 11 15.8±3.1 ⊕ FS 666 [65,83] 65 36.8±0.7 48 33.8±0.6 ⊕ Ty 140 [26,34] 6 3.9±2.2 25 5.3±2.3 ⊕ Ti 150 [6,9] 6 31.2±1.4 6 21.8±1.0 ⊕ Tn 400 [320,325] 1 3.5±0.6 1 2.3±0.1 ⊕ Im 1010 [105,123] 21 2.9±0.7 80 8.1±1.2 ⊕ Rg 400 [264,275] 45 3.8±0.4 31 1.7±0.1 ⊕ Sp 1000 [871,889] 620 7.7±2.6 764 4.4±2.1 ⊕ Wv 400 [285,299] 30 9.8±0.3 2 12.0±0.7 ⊕ Ba 400 [13,16] 13 13.6±0.7 5 10.8±1.8 ⊕ Table 4 Size (R) of subset Φ R for different values of σ using training sets with different sizes (N). N 100% 10% σ 5 8 10 5 8 10 R 1807 241 91 335 132 63 Unsupervised feature extraction via kernel subspace techniques A.R. Teixeira a A.M. Tomé a ⁎ E.W. Lang b a DETI/IEETA, Universidade de Aveiro, 3810-193 Aveiro, Portugal b CIML/Biophysics, University of Regensburg, D-93040 Regensburg, Germany ⁎ Corresponding author. Communicated by S. Fiori This paper provides a new insight into unsupervised feature extraction techniques based on kernel subspace models. The data projected onto kernel subspace models are new data representations which might be better suited for classification. The kernel subspace models are always described exploiting the dual form for the basis vectors which requires that the training data must be available even during the test phase. By exploiting an incomplete Cholesky decomposition of the kernel matrix, a computationally less demanding implementation is proposed. Online benchmark data sets allow the evaluation of these feature extraction methods comparing the performance of two classifiers which both have as input either the raw data or the new representations. Keywords Kernel PCA Feature extraction and low-rank decompositions 1 Introduction Unsupervised feature extraction methods try to generate representative features from raw data helping any classifier to learn a more robust solution and achieving a better generalization performance. Often original features are inappropriate, and even the number of features might be too large to conduct an efficient training of a classifier. Subspace techniques can be considered unsupervised feature generators which simultaneously provide dimension reduction and a more efficient data representation. Principal component analysis (PCA) is a subspace technique which is widely used in many fields of research like face recognition [1] and related computer vision tasks [2]. It computes an eigenvalue decomposition of the data covariance matrix. The related eigenvectors form an orthogonal representation and identify directions of largest variance in the data. A new data representation is obtained by projecting the given data onto the new eigenvectors. These projections, called features, are thus uncorrelated and preserve as much energy of the raw data as corresponds to the sum over the ordered eigenvalues, i.e. the new data variances, depending on the dimension reduction envisaged. It is generally assumed that the smallest eigenvalues represent the variance of the noise in the data. Thus a representation of the data with reduced dimensionality ignores the eigenvectors with related smallest eigenvalues and enables dimension reduction and denoising simultaneously. The PCA model thus assumes that the original data can be represented as linear combinations of these features. However, this linear representation is a limitation if it is to model highly complex data. Kernel PCA methods provide suitable non-linear extensions extracting non-linear principal components. KPCA is based on computing a standard linear PCA in a feature space, into which input data x is mapped via some nonlinear function ϕ ( x ) . To this end a canonical dot product is considered in feature space and replaced by using a kernel function k ( x , y ) = Φ T ( x ) Φ ( y ) which can be evaluated in input space. This so-called kernel trick can be carried out for any algorithm that can be formulated with dot products in feature space [3,4]. In a classification task, the new feature space representation provided by the non-linear kernel method most probably becomes linearly separable [5]. Several computer vision applications incorporate these techniques [6–8]. But KPCA can become quite cumbersome if not impossible with large data sets. It is well known that such techniques require O(N 3) operations and O(N 2) storage capacity, where N is the number of data points [9]. Also numerical instabilities arise if the kernel matrix is ill-conditioned. Such numerical instabilities can be avoided by resorting to multi-dimensional scaling techniques to reconstruct the feature vectors in interim space as proposed in [10]. Greedy KPCA offers an alternative in such cases. It builds upon the observation that in case of a full rank kernel matrix K one can approximate it by a low rank positive semi-definite matrix K ˜ via random sampling [11]. Further reduction of the computational complexity can be achieved by resorting to an incomplete Cholesky decomposition [12] combined with a symmetric pivoting scheme to guarantee numerical stability in case of smooth and gradually decaying eigenvalue spectra. It has been shown that a Cholesky factorization of the kernel matrix is numerically more stable and robust than approaches based on the Sherman–Morrision–Woodbury formula [13] which often run into numerical instabilities as the inverse of a matrix needs to be updated. In fact it has been reported that a Cholesky factorization avoids numerical instabilities even when the initial data matrix is not well conditioned [14]. Furthermore, the quality of greedy approaches can be controlled by applying an appropriate stopping criterion. In [11], the trace of the approximation error of the kernel matrix tr ( K − K ˜ ) was considered an upper bound and the algorithm was stopped when a certain tolerance ε was achieved. Similarly, the quality can be traced by iteratively optimizing the mapping error as discussed in [10]. The latter optimization showed excellent performance, however, it might occasionally suffer from multiple local optima. Alternatively, one can compute a low rank approximation to the kernel matrix and then invoke a Nyström extension of the eigenvectors to obtain a solution for the higher dimensional problem. The Nyström method is generally used for numerical solutions of eigenproblems [15]. The reduced-rank approximation of the kernel matrix can be obtained in either of the ways mentioned above and numerical instabilities due to ill-conditioning can be avoided by adding a jitter term K + σ I , σ > 0 [16]. But the Nyström method can also be employed as an alternative to estimate a low rank approximation of the kernel matrix as has been discussed in [16]. It has been shown also that this Nyström approximation is equivalent to a variational calculus based on the Rayleigh principle [17]. In this work KPCA and greedy KPCA subspace models to perform feature extraction are discussed: all proposed models and the related formalism are consistently described in matrix notation. Basis vector matrices are represented in dual coordinates like it is often considered in support vector machines [18] to exploit the dot product properties of kernels. The dual representation arises from the fact that the new coordinates can be solely expressed by the training data set. The greedy KPCA is based on an incomplete Cholesky decomposition [19] of the kernel matrix using a symmetric pivoting scheme. Hence, the incomplete Cholesky decomposition of the kernel matrix, when formed with the training data set, is exploited to reduce the complexity of several kernel models [20]. This method allows to compute the Nyström approximation of the eigenvectors of the kernel matrix corresponding to the largest eigenvalues. The symmetric pivoting scheme of the algorithm is used to select a subset of the training data set to obtain the dual form of the greedy KPCA model. Another issue to be discussed is the impact that centering of the data has on the models. The proposal is to perform the centering and simultaneously maintain the new representation of the training data set uncorrelated. Finally, the extracted features are used for classification. Numerical simulations compare the suitability of these feature extraction procedures via the performance of two simple classifiers: the nearest neighbor (NN) classifier and linear discriminant function (RL). As features either kernel features or principal component features were employed as well as a direct classification of the raw data. To further evaluate the impact of feature selection via non-linear projective subspace techniques, classification results are compared with the best results published in [21]. The greedy KPCA, furthermore, is used with a large benchmark data set (USPS data set of handwritten digits) which cannot be evaluated with standard KPCA using large training data sets. 2 Feature extraction and classification Classification algorithms assume that the objects to be classified are represented by points in a multidimensional space. However before executing a training algorithm to estimate the parameters of the classifier, a transformation of the original vectorial data is advantageous extracting appropriate features which ease the classification task. The classification scheme, including subspace methods as a pre-processing step, is illustrated in Fig. 1 . The system is composed of two parts: the subspace model and the classification. The present study will mainly focus on the feature generation step and discusses several appropriate subspace models. Any such subspace model is described by a matrix U whose columns form the basis vectors. The latter are used to generate the features to be used for classification by projecting the input data (x n ) onto them. Hence, the projections (z n ) constitute the new representation of the data. These projections thus results either as a simple linear combination of the input data, like with PCA projections, or they represent a superposition of non-linear components of the data, like with KPCA projections. In a subsequent classification task, the projections then form the input to any suitable classifier. While training the classifier, the basis vectors are estimated from a labeled training data set, and the projections are used to adapt the parameters of the classifier. Finally, the performance of the classifier can be evaluated employing projections of new data, taken from a test data set, onto those basis vectors computed from the training data set. 2.1 The subspace model for feature generation The basis vector matrix U describes the subspace model, and the number of columns represents the dimension of the new representation. Within a PCA model, the largest dimension of the input space coincides with the dimension of the data vectors. The subspace model U is obtained from an eigenvalue decomposition of the covariance matrix C ∝ UDU T of the data. In practice, the method is applied primarily to yield new representations with reduced dimensionality. Within a kernel PCA model, however, the nonlinear mapping function ϕ usually results in an increase of the dimension of the mapped data vectors. Fortunately, the mapping is never performed explicitly. Thus, kernel subspace techniques represent projective methods in a feature space created by a nonlinear transformation of the input data. The necessary steps will be summarized in the following employing a concise matrix notation and the representation of the subspace model in its dual form. Consider that the mapped training data set is given by Φ = [ ϕ ( x 1 ) , ϕ ( x 2 ) … ϕ ( x N ) ] Then the new representation of any training data point x n = [ x 1 n , x 2 n , … , x Dn ] T is obtained by computing the dot product between each mapped data vector ϕ ( x n ) and the basis vector matrix (1) z n = U T ϕ ( x n ) In order to avoid an explicit mapping of the data, the basis vector matrix U must be written in its dual form [3,22]: (2) U = Φ VD − 1 / 2 which expresses the fact that the eigenvectors can be written as a linear combination of the mapped training data vectors. Here V is a matrix of eigenvectors, and D is a diagonal matrix of corresponding eigenvalues, both resulting from the eigenvalue decomposition of the related kernel matrix K = Φ T Φ . Usually the columns of V are ordered in accord with the corresponding eigenvalues which decrease along the diagonal of matrix D, i.e. λ 1 > λ 2 > ⋯ > λ L > ⋯ > λ R > ⋯ > λ N Therefore the basis vector matrix U has dimension F×L, where F is the dimension of the feature space F , determined by the non-linear mapping, and L is the number of selected eigenvectors (and corresponding eigenvalues) of the kernel matrix. Notice that if the kernel matrix is computed without mapping the data, its elements are given by ( k ij = x i T x j ) and the number of nonzero eigenvalues is at most min(D,N), where D is the dimension of the input vector x n = [ x 1 n , … , x Dn ] T . It can be proven easily that the columns of matrix U also correspond to the eigenvectors of the related non-normalized correlation matrix S = Φ Φ T [23]. However, in kernel methods this correlation matrix is never computed to avoid an explicit mapping of the data. Rather, kernel methods rely on the so-called kernel trick [3,4] which provides an efficient implementation of dot products via their related elements of a suitable kernel matrix. Thus dot products in feature space are evaluated via kernel functions, like radial basis functions (RBF), using the data in input space. For instance, considering a pair of data vectors, the dot product of the mapped data is defined via (3) ϕ T ( x i ) ϕ ( x j ) ≔ k ( x i , x j ) = exp − ∥ x i − x j ∥ 2 2 σ 2 where the width parameter σ is to be assigned according to the range of values of the data set. In that way it is very easy to compute the N×N kernel matrix of dot products K, where each entry (i,j) is the result of the dot product between a pair (i,j) of examples of the training set. In a classification task, the training set is used to learn the basis vector matrix (U) and its projections Z, and the corresponding labels are used to train the classifier. Using an RBF kernel (Eq. (3)), the number of nonzero eigenvalues depends on the size of the training set but also on the value assigned to σ . Assuming only L ≤ N eigenvectors of the kernel matrix are chosen, the reduced N×L eigenvector matrix V and the corresponding L×L diagonal eigenvalue matrix D lead to a new, reduced representation of each training data point with dimension L. Hence, the training set can be represented by an L×N-dimensional matrix Z of projections onto the reduced set of L basis vectors U as (4) Z = U T Φ = D − 1 / 2 V T Φ T Φ = D − 1 / 2 V T K By replacing the kernel matrix by its eigenvalue decomposition K = V N D N V N T , where the matrix of eigenvectors and the matrix of eigenvalues both have dimension N×N, the previous equation can be simplified. As V T V N = [ I 0 ] where I is the L×L identity matrix and 0 is a L×(N−L) matrix of zeros, the L×N-dimensional matrix of projections in feature space read (5) Z = D 1 / 2 V T It can be shown easily that the new representations of the training data set are uncorrelated, i.e. ZZ T is a diagonal matrix. And also notice that a low-rank approximation K ˜ = VDV T for the kernel matrix can be obtained by computing Z T Z. This low-rank approximation results from the L leading eigenvalues and related eigenvectors. After learning the subspace model, the test data set is also projected onto the basis vectors U, i.e. z test = U T ϕ ( x test ) , thus forming the new representation of the test data. These projections then build the input to the classifier and the corresponding outputs, i.e. the classified test data, are used to evaluate the performance of both, the feature extraction method and the classification method. However, the following drawbacks of the proposed procedure during training and testing have to be pointed out: • Kernel methods, employing dual coordinates to represent the model (Eq. (2)) necessitate the storage of the training set even during the test phase to compute the projections of any new test point ϕ ( y ) onto the basis vectors of the model. • A large kernel matrix K can render its eigenvalue decomposition unfeasible in practical applications where often the manipulation of large data sets is required. These issues have been addressed in several kernel based algorithms and different solutions, and strategies have been presented in literature. Some approaches try to select a subset of data within the training set [24–29], others rely on low-rank approximations of the kernel matrices [19,30,7,16,31]. Most of these approximations are based on a similar criterion to optimize the solutions. 2.2 Low-rank approximations to the subspace model In large training data sets, the computation of the corresponding kernel matrix K becomes prohibitive. Consequently, its eigenvalue decomposition is often impractical to achieve in real data applications. However, in most of the cases, the kernel matrix is low rank and its eigenspectrum gradually decays to zero. Therefore only the R < N most significant eigenvalues and corresponding eigenvectors need to be computed to yield a good approximation of the kernel matrix. 2.2.1 The Nyström method based on a random subset selection Few papers [7,16,32] discuss the application of the Nyström method to compute a low rank approximation K ˜ = V R D R V R T of the kernel matrix K where only the R largest eigenvalues D R = diag ( λ 1 , … , λ r , … , λ R ) and corresponding eigenvectors, forming an N×R matrix V R , are computed. The Nyström method is based on the fact that the kernel matrix can be re-written in block notation as follows: (6) K = K R K RS K RS T K S Considering that the full matrix has dimension N×N, the upper left block matrix K R has dimension R×R, the upper right block matrix K RS has dimension R×S and the lower right block matrix K S has dimension S×S where S=N−R. Thus, the mapped training data set of dimension N is divided into two subsets of size R and S, respectively. The matrix K R represents the kernel matrix within subset Φ R (with R vectors), K RS is the kernel matrix comprising subsets Φ R and Φ S and K S is the kernel matrix of the subset Φ S . The Nyström extensions for the N×R eigenvector matrix V R can be obtained via (7) V R T = H T [ K R K RS ] where the R×R-dimensional matrix H is computed using eigenvalue decompositions of certain R×R matrices computed from the original kernel matrix or from the data set. Different approaches to build such reduced size matrices are discussed in the literature: • In [16], the data (or its related kernel matrix) is split into subsets (or the matrix into blocks) by randomly selecting the subset Φ R (or the rows/columns) to compute a low rank approximation to the kernel matrix. Alternatively, in [32] the data is clustered into R clusters, and the centroids are used to form the upper left block K R of the kernel matrix K. In both cases the eigenvalue decomposition of K R = B R Γ R B R T leads to (8) H = B R Γ R − 1 therefore substituting H yields non-orthogonal eigenvectors, i.e., V R T V R ≠ I [23]. • An alternative approach, leading to orthogonal eigenvectors instead, is proposed in [7]. The proposal adds a second step to the proposal of [16]. The data is transformed using the eigendecomposition of K R , and an R×R matrix is computed with the transformed data. The eigendecompositions of the R×R matrices of both steps are used to compute H. In the next section we will propose an equivalent solution which substitutes the random selection to form K R and the data transformation to compute the second R×R matrix. We discuss the properties of this approach and compute the corresponding matrix H. 2.2.2 Nyström method based on an incomplete Cholesky decomposition Instead of randomly choosing a subset of the training set, an incomplete Cholesky decomposition of the kernel matrix K can also be used as an alternative to form the block matrix K R in Eq. (7) [31]. In the following we discuss this decomposition and its connection with the Nyström approximation. The incomplete Cholesky decomposition of the kernel matrix K, using a symmetric pivoting scheme to obtain an R×N matrix C, reads (9) C = [ L L − T K RS ] The matrix L represents a triangular matrix corresponding to the Cholesky factorization of K R = L T L . Notice that the matrix L arises naturally from the incomplete Cholesky decomposition [20], while the concomitant symmetric pivoting scheme leads to the selection of the subset Φ R of the training set. Considering the R×N matrix C, the eigenvalue decomposition of its related R×R correlation matrix Q can be computed via (10) Q = CC T = E R D R E R T The R×R-dimensional matrix E R contains in its columns R eigenvectors and the R diagonal entries of D R represent the related eigenvalues. The latter are identical to the leading R eigenvalues of the related kernel matrix K ˜ = C T C = V R D R V R T . The result of this eigenvalue decomposition together with the Cholesky decomposition of K R lead to (11) H = L − 1 E R D R − 1 / 2 Substituting this result into Eq. (7), the N×R eigenvector matrix V R of the kernel matrix K ˜ can be written as (12) V R = K R K RS T L − 1 E R D R − 1 / 2 The eigenvectors, i.e. the columns of the N×R matrix V R , are orthogonal thus yielding V R T V R = I . Using the spectral theorem V R D R V R T to approximate the kernel matrix, it can be proven that the last block of the kernel matrix (Eq. (18)) is approximated by K S ˜ = K RS T K R − 1 K RS . This approximation holds also if H is computed as described by Eq. (8), but in that case the values of the eigenvalues have to be properly scaled (see [16] for more details). 2.2.3 Basis vectors and Cholesky decomposition In Eq. (12), the eigenvalues should be arranged in decreasing order, and the related eigenvectors, forming the columns of matrix E R , should be ordered accordingly. As before, L ≤ R eigenvalues and their related eigenvectors, forming the R×L eigenvector matrix E, can be selected to project the data onto only L directions. The projections of the training data set, in feature space then become (13) Z = U T Φ = D 1 / 2 V T = E T L − T Φ R T [ Φ R Φ S ] Consequently, the basis vector matrix U can be written as (14) U = Φ R L − 1 E = Φ R A Hence, the dual form of the basis vector matrix of the subspace model is written only employing a subset of the training data set. This substantially reduces the storage requirements and the computational load during testing the classifier. Also note that the L basis vectors form an orthogonal basis in feature space, i.e., U T U = I . As an alternative to Eq. (13), the projections of the training data set can be written as (15) Z = U T Φ = E T C Then it can be proven that the projections are uncorrelated even when the dimension of the data is reduced to L ≤ R , i.e, when the number of columns of E R is L ≤ R . But the projections Z can be used to compute the low-rank approximation of the kernel matrix only when the dimension of the new representation is R. To see this notice that E R E R T = I only holds when the eigenvector matrix has dimension R×R. In summary, the incomplete Cholesky decomposition with a symmetric pivoting scheme provides the means to compute matrix C and simultaneously identify the elements of subset Φ R . Furthermore, in [14,20] an implementation of the incomplete Cholesky decomposition of a kernel matrix (using the RBF kernel function), without explicitly computing the kernel matrix, is introduced. In the Appendix, an adaptation of this implementation to compute the basis vector matrix as described by Eq. (14) is presented. 2.3 Centering the data All previous deductions were conducted assuming that the data is centered. In input space, centering can be considered a pre-processing step which must be accomplished before computing the correlation matrix, and before projecting any new data vector onto its related eigenvectors. But in kernel methods, centering has to be integrated in the projection step. In the following, this centering procedure will be discussed in case of a complete and an incomplete training set, respectively. 2.3.1 KPCA with a complete training set In feature space, centering the mapped data is a more elaborate procedure performed mostly during computing the projections. To facilitate the exposition, let us consider a vector m = [ m 1 , … , m N ] T with N elements, all of which equal to 1/N, and a matrix M with N identical column vectors m. Then centering the training data set and projecting a new data point ϕ ( y ) onto the subspace directions, the following operations need to be integrated into the dot product: (16) ( Φ − Φ M ) T ( ϕ ( y ) − Φ m ) The first term removes the mean to the training data set, while the second subtracts the mean of the training set from new point ϕ ( y ) . The manipulation of the previous equation leads to the following vectors: • k 1 = Φ T ϕ ( y ) is a vector of dot products between the point and the training set. • k 2 = M T Φ T ϕ ( y ) is a vector with identical values in all entries. It is the mean of the dot products between the new point and the training set, • k 3 = Φ T Φ m is a vector containing the mean values of the N rows of the kernel matrix K, and finally • k 4 = M T Φ T Φ m is a vector with all entries equal to ( 1 / N 2 ) ∑ i ∑ j k ( i , j ) , where k(i,j) is the (i,j)-th entry of the kernel matrix. According to Eq. (4), the feature space projections of the input data point y then read (17) z y = D − 1 / 2 V T ( k 1 − k 2 − k 3 + k 4 ) = D − 1 / 2 V T ( I − M T ) ( k 1 − k 3 ) The summands related with k 3 and k 4 only depend on the training set and are present in every data point projected into the subspace U. Hence, they can be stored in advance and constitute a bias term that is present in every projection. It can be shown easily that projecting the complete training set Φ to obtain Z, the terms k 2, k 3 and k 4 arise from the centered kernel matrix (18) K c = ( I − M ) Φ T Φ ( I − M ) where I is an N×N identity matrix. Then, to accomplish uncorrelated projections of the training data set, in Eq. (4) the kernel matric K = Φ T Φ should be replaced by K c as given by Eq. (18), and the matrices V and D, determining the subspace model, should be obtained from the eigenvalue decomposition of K c . Finally notice that with an RBF kernel the dot products in feature space are always less than unity (see Eq. (3)) and that the contribution of the terms k 3 and k 4 in Eq. (17) depend on the parameter σ of the RBF kernel. 2.3.2 KPCA with a reduced training set As described in the Appendix, to avoid a computationally costly direct evaluation of the kernel matrix, an incomplete Cholesky decomposition can be performed having as input the complete training set. The outcome is a dimension reduced R×N matrix C with R ≤ N . A centered version of a low rank approximation of its related kernel matrix can then be obtained via (19) K ˜ c = ( I − M ) C T C ( I − M ) where the mean Cm is subtracted from every column of C. In that case, the eigenvectors E R must be computed from the correlation matrix Q c computed after centering the matrix C. Then, the term b = E T Cm must be subtracted from every data point projected into the subspace U (see Eq. (14)) according to (20) z y = U T ϕ ( y ) − b In the Appendix, an implementation of the greedy KPCA algorithm, as presented in this work, is given in MATLAB, and the centering of the data is added as an option to be selected by the user. 3 Numerical simulations The different subspace feature extraction methods discussed above are evaluated by comparing the performance of extracted features in a classification task employing two different simple classifiers. The goal is to compare the quality of the generated features, not the classification ability of the classifiers. The generalization errors of the latter allow to quantify the quality of the features extracted by projecting the data onto different subspace models. We carried out experiments on artificial and real world data sets available from public data repositories. We consider two groups of data: one consisting of 13 benchmarks and the other of the well-known USPS data set. To treat all generated features on an equal footing, two simple classifiers were used for classification: a nearest neighbor (NN) classifier and a linear discriminant function (RL). With a NN classifier, each element of the test set is classified according to its nearest neighbor in the training set. In case of linear discriminant functions, the weight vectors are learned within a training session by employing a training data set and by using the mean-squared-error criterion [33]. Each element in the test set is assigned to the class whose discriminant function has the largest value. 3.1 Subspace model parameters To establish a subspace model and compute its basis vectors, thereby using as kernel an RBF function to evaluate the dot products, it is needed to assign a value to the width parameter σ of the kernel. This parameter is often a variable of the experimental studies [34], or it is optimized applying a cross-validation strategy including training and test data sets [24]. The value of σ determines the sparsity of the kernel matrix, because the entries are in the range ]0,1]. Furthermore, the trace of the kernel matrix, corresponding to the sum of its eigenvalues, is always equal to tr(K)=N irrespective of the value of σ . For large σ , the first eigenvalue generally dominates the eigenvalue spectrum, while for smaller values a more gradual decrease of the eigenvalues results. The choice of sigma also interferes with the size R of the subset of basis vectors forming the subspace model of a greedy KPCA. In case of a gradual decrease of the eigenvalues, the incomplete Cholesky decomposition will automatically choose the complete training set, i.e. R=N. Experimentally we conclude that by choosing σ as the average of the distances of the training data to their mean x mean (21) σ = 1 N ∑ i = 1 N ∥ x i − x mean ∥ a smooth eigenspectrum decay is achieved. 3.2 Benchmark data sets A collection of benchmark data sets can be found on the following web site 1 1 Accessible at : Table 1 resumes the information of the 13 data sets. All data sets have 100 random partitions of pairs of training and test sets, except Splice and Image which have 20 partitions. These benchmark data sets will be used to extract features by applying the projective subspace techniques discussed above. Feeding these features into the two classifiers just mentioned and comparing the classification results enables us to quantify the suitability of the features extracted with the different subspace models. Especially feature degradation by greedy approximations may be quantified this way. Several other algorithms [21,24,29,31] have been applied already to each partition data set. All of them address binary classification problems and use various kinds of features. The second column of Table 1 summarizes the average and the standard deviation of the best generalization errors published in [21]. These data provide sort of a reference with which we compare our results. On the one hand this proves that our results are generally appropriate and competitive, and on the other hand it allows us to evaluate the usefulness of the features generated with the different projective subspace techniques. Using these training and test sets, algorithms and methods have been compared pairwise, and the difference between error percentages, achieved by any pair of algorithms/methods, has been evaluated statistically. A student t-test was performed, applying a 95% significance level, by considering the two hypotheses H 0 : μ 1 = μ 2 and H 1 : μ 1 ≠ μ 2 where μ 1 and μ 2 represent the average test errors achieved by two methods, respectively. This statistical test has been carried out in an attempt to reject ⊖ or accept ⊕ the null hypothesis (H0) with the significance level 95%. 3.2.1 Raw data sets First, both classifiers were applied to the raw data sets to provide sort of a base level where the features correspond to the observations themselves. The results of the classification lead us to organize the data sets into two groups (see table). In group 1, comprising the first eight data sets, at least one of the classifiers achieves an error rate comparable to the ones published in [21]. This means that for these data sets the observations themselves represent features informative enough to achieve a near optimal classification. In group 2, encompassing the remaining five data sets, the performance was far from the results presented in [21] indicating that the observations themselves represent suboptimal features which were not sufficiently informative. In group 1, the RL classifier achieves the better performance, with the exception of the thyroid data set. In the second group, the best results are split between the two classifiers. A student t-test is used to compare the best result, achieved with either an NN or an RL classifier, with the ones published. The null hypothesis H0 is rejected in the second group but is accepted in the first group, except for two data sets (see last column of Table 1). 3.2.2 Transformed data sets Next, both classifiers were applied to the features generated from the data sets via the projective subspace techniques described. In feature space, the number of basis vectors which can be computed is at most equal to the number of training examples (N). Both versions of KPCA are used to compute the subspace model: KPCA depends on the full training set while greedy KPCA only depends on a subset with R < N elements of the training set. Both subspace models are computed using centered versions of the kernel matrices as proposed before. Thus Eq. (17) is used to project a data point onto the KPCA model, and Eq. (20) is used to project the data onto a lower-dimensional greedy KPCA model. In both cases, the number of projections was varied from L=1 up L = min ( R ) in the training sets of each data set. The average error rate is computed using all test sets for each L and the minimum average error is reported. In Table 2 , average error rates and standard deviations are shown for both classifiers in case of a KPCA model to extract appropriate features. In addition, column I1 shows the result of the student t-test between the results of [21] and either the NN or the RL classifier. The null hypothesis H0 is accepted in all but the German data set indicating that the average error rate is in accord with the best results obtained in [21]. As only the simplest classifiers have been applied this hints to the excellent quality of the features extracted. It is also obvious that in group 1 there is no significant improvement of the classification result when using non-linear features of the data sets. In fact it can be seen that with the exception of the thyroid data set and the RL classifier as well as the Twonorm data set and the NN classifier, hardly any improvement could be reached compared to employing the bare observations as features. Only in the two cases mentioned, the transformation of the data in group 1 via a KPCA subspace model could improve the features extracted and render the classification results consistent internally and with the best classification result reported in the literature. The situation is completely different with the data sets from group 2, where the data transformation via a KPCA subspace model considerably improved the quality of the generated features and rendered them informative enough to compare favourably with the best classification results reported in the literature. In most cases, the minimum error rate is achieved using a number ( L > D ) of features higher than the dimension (D) of the raw data. One of the exceptions is the twonorm where L=1 and where a PCA model yields a similar performance if the data is projected onto only the leading eigenvector [31]. An additional t-test was performed to study how the number of projections affects the results. The error rates just discussed were compared to error rates achieved when the number of projections was L=D. Column I2 of Table 2 reports the t-test results: the null hypothesis H0 is rejected in group 2 and accepted in group 1. Furthermore, notice that the RL classifier yields, with the exception of the waveform set, the best classification only if the number of features is larger than the dimension of the raw data, i.e. when L > D . Table 3 shows the classification results obtained by employing greedy KPCA to extract the features and to compute the projections of the data onto the feature space coordinates. The column R shows the range of values for the size of the subset Φ R in the training sets. The average error rates obtained are similar to the ones computed with KPCA and corroborate that the computationally much less demanding greedy approximation extracts features of comparable quality and information content. The null hypothesis is now accepted for every data set (see column I1). The number of projections, i.e. features, used with both classifiers, however, was not always identical as the greedy KPCA algorithm was implemented using the same threshold for the approximation error (see Appendix) irrespective of the data set. But considering that greedy KPCA uses an approximation of the kernel matrix, some variation has to be expected. In fact, the average of the relative subset sizes (R/N) of the subspace models fluctuates strongly between the different data sets as shown in Fig. 2 . Small subset sizes mostly result from low dimensional data sets like the banana, titanic and thyroid data sets. Empirically, a small subset size correlates with a steep rise of the cumulative sum of eigenvalues of the kernel matrix, meaning that these eigenvalues decrease quickly. Thus the normalized cumulative sum of the eigenvalues, if the latter were arranged in decreasing order, (see (22)) can be used to illustrate the decrease characteristics of the eigenvalues of the kernel matrix: (22) S ( p ) = ∑ i = 1 p λ i ∑ i = 1 N λ i with λ 1 ≥ λ 2 ≥ ⋯ ≥ λ N Fig. 2 shows the normalized cumulative sum S(p) of the data sets of group 2. It can be seen that an abrupt increase on S(p) corresponds to a small value for R, and if S(p) converges slowly to the maximum value, then R has a higher value. The different decay profiles thus directly translate into different values for R/N. Therefore in group 2 kernel features lead to an improvement in the performance of both classifiers, either using KPCA or greedy KPCA algorithms to compute the basis vectors. And for both groups of data the performance of the classifier (either NN or RL) based on the features extracted in various ways is never worse than with the raw features, rather it is often much improved. 3.3 USPS data set The USPS data set is a benchmark data set used in many works. It is accessible at www.kernel-machines.org and is divided into a training data set with 7921 images and a test data set with 2007 images. The training data set is very large, so not every algorithm could be applied to the complete training set. Hence, two different training data sets were formed to comprise 10% and 100% of the available data, respectively. Each image consists of a handwritten digit comprising 16×16 pixels. Concatenating these pixels into a row vector, the resulting data vector in input space x n had dimension D=256. 3.3.1 The raw data set The best classification results obtained on this data set in literature report an error rate in the range 0.04–0.05 [4]. In a first attempt we considered the complete training set as features to be employed for training an NN or a RL classifier. Both classifiers were then used to classify the test data set. With an NN classifier an error rate equal to 0.056 was achieved, while a RL classifier yielded an error rate equal to 0.131. Note that especially with an NN classifier the raw data as features provide already enough information to achieve a near optimal classification. Generally, these error rates provide a baseline for comparison with results obtained employing more complex feature generation strategies. 3.3.2 PCA feature extraction First, the basis vector model was computed by principal component analysis (PCA) employing both the complete and the reduced-size training sets. For that, the 256×256-dimensional covariance matrices of the training data sets were computed, and the basis vectors of he subspace model corresponded to their eigenvectors. Arranging the latter according to their related eigenvalues in descending order reveals that the data from both training sets predominantly spread in only 50 of the 256 directions of the input space. The NN and RL classifiers were trained employing different numbers of extracted features as inputs z k . These numbers in each case corresponded to the number L of vectors taken to form the basis vector matrix. In the PCA model, L varies in the range L=1–100. Fig. 5 illustrates the classification error of the NN classifier upon varying the number of projections in input space which represent the features used for classification. The best performance yields an error rate of 0.052 and is achieved with the complete training data set using 50 projections roughly. With the reduced training data set the error rate is around 0.1. This result has to be expected as the covariance matrix exhibits approximately 50 significant eigenvalues, with the remaining eigenvalues being very close to zero. Increasing the number of PCA projections to L > 50 causes a slight increase of the error rate to 0.057. Fig. 6 shows the corresponding results for the RL classifier. Using PCA projections as features, the error rate of this classifier amounts to 0.14. This classifier yields similar error rates with both training sets indicating that the reduced feature set already contains enough information to achieve a good classification. But considering the best classification rates reported in the literature, these features turn out to be suboptimal still. 3.3.3 Kernel feature extraction As mentioned already, the training data set is too large for KPCA to be applied to the complete training data set. So results could be obtained only with the reduced training data set or by applying the greedy KPCA algorithm. The latter subspace model was computed without centering the data, thus eliminating the bias term of Eq. (20) and without centering C. Table 4 presents the size (R) of subset Φ R for the complete and the reduced training set, respectively, and for different values of the width parameter σ of the RBF kernel. Note that σ = 5 corresponds to the value computed by Eq. (21). For the reduced training data set (N=729), the KPCA subspace model, without centering, can be computed as well. Then the resulting eigenvalues λ i can be compared to the eigenvalues λ ˜ i of the greedy KPCA model. Fig. 3 shows the relative error defined as (23) er ( i ) = | λ i − λ ˜ i | λ i = 1 − λ ˜ i λ i This relative error falls into the same range for all σ values of the RBF kernel, and it is larger for the leading eigenvalues. Fig. 4 shows the difference between the cumulative sums of eigenvalues S(p) for different parameters σ of the RBF kernel when the greedy KPCA subspace model is employed to extract proper features from either the reduced training data set, N=729, or the complete training data set, N=7921. The relative value of the first eigenvalue (S(1)) is similar for both training sets but by comparing S(p) of both training sets, the same absolute level of S(p), corresponding to an identical amount of explained variance of the data, is achieved for different values of p. This comparison thus explains the values of the different subset sizes R of Table 4. In the greedy KPCA model, L varies in the range L=1 to L= R. Fig. 5 illustrates the classification rates achieved with the NN classifier upon varying the number of features extracted from the training data sets. If the features, i.e. the subspace projections of the data, were extracted from the complete training data set, the smallest error rates, amounting to 0.05, were achieved. Features extracted from the reduced training data set resulted in an error rate of 0.1 roughly. Hence, using either linear features (PCA) or kernel features (greedy KPCA), a similar error rate 0.05 is achieved with the NN classifier. Employing L > 50 projections, the error rate increases slightly except when the features were computed applying a width σ = 5 of the RBF kernel. Fig. 6 shows the classification results if the same features were classified with the RL classifier. Here the error rate does not have a so clear dependence on the size of the training set. Using as input features deduced from greedy KPCA subspace projections in feature space results in an error rate for classification of 0.09. It becomes roughly independent of the size of the kernel for a number of features beyond L > 100 . Note that this is about twice the number of features than in case of linear features extracted with PCA. But compared to the classification results obtained with linear features (PCA), the nonlinear features extracted with greedy KPCA yield a much lower error rate which is roughly one third of the PCA error rate. The results presented in [4], employing a training data set of 3000 examples, show a similar tendency: the linear SVM classifier performs better using nonlinear features computed with KPCA instead of the linear features computed with PCA. In [4] 2048 projections in KPCA feature space were calculated, yielding an improvement in error rate to 0.046 if a polynomial kernel is used. This small decrease in classification error indicates that the choice of the classifier is less critical than the feature generation procedure. In projective kernel subspace techniques, the number (R) of basis vectors has the highest value when σ = 5 , but the performance of the linear discriminant classifier RL does not change with increasing L after L=100. On the other hand, for other values of σ , the best performance is achieved using projections onto all the available basis vectors. All figures demonstrate that the RBF kernel with σ = 5 shows the best performance. However, when σ = 8 , in Fig. 6, a similar performance is visible when L reaches close to R. And notice that this represents the best tradeoff between performance and storage requirements during the testing phase as the subspace model is described using R=241 training examples. 4 Concluding remarks In this work kernel projective subspace techniques are concisely described using an algebraic approach which relies on the dual form of the basis vector model. The latter expresses the basis vectors in terms of the training data and consequently every data manipulation is always casted into the dot products as it has to be in kernel methods. This approach highlights the importance of the kernel matrix to estimate the subspace model. Furthermore, low-rank approximations of the kernel matrices, based on Nyström extensions, can be easily integrated in such a formalism. Furthermore, it was proven that the incomplete Cholesky decomposition with symmetric pivoting provides the means to achieve a computationally less demanding solution, namely the greedy KPCA. It has been shown also how an algorithm can be constructed, for an RBF kernel, to perform an incomplete Cholesky factorization and simultaneously identify the subset of data forming the dual basis vector model. Experiments show that these projective subspace techniques, cast into a greedy KPCA approach, achieve a performance which is similar to a full KPCA analysis. It is demonstrated also that the greedy KPCA model performs well on a very large data set like USPS data set. Although it is often assumed that extracting non-linear features always results in an increase of performance in classification, our results do not give this indication for all data sets. The reason is mostly related with the data characteristics, i.e. its information content relevant for classification. In some benchmark data sets the raw data taken as features already contained enough information to yield a good classification and any more sophisticated feature extraction procedures, as the ones discussed in this study, could not improve the performance of the classifiers any further. In other benchmark data sets as well as the USPS data set sophisticated feature extraction procedures substantially improved the classification results. Feature extraction also strongly depends on the parameters of the subspace model, especially on the parameter σ of the RBF kernel functions. Simulations showed that it is possible to estimate an appropriate value of this parameter thus achieving a good tradeoff between the eigenvalue decay and the number of non-zero eigenvalues of the kernel matrix and still resulting in a good classification performance. Acknowledgments A.R. Teixeira received a PhD Scholarship (SFRH/BD/28404/2006) supported by the Portuguese Foundation for Science and Technology (FCT). The work has been supported also by grants of DAAD. Appendix: Implementation of the Cholesky decomposition A very efficient implementation for the incomplete Cholesky decomposition algorithm exists (accessible in [30]) having as input: the training data set X, σ of the RBF kernel and a threshold to control approximation error of the decomposition. As described in [20], the matrix C is formed iteratively, starting with one row up to R when the error is less than the threshold. The error ε is approximated as ε ≈ tr ( K s − K rs T K r − 1 K rs ) . A similar approximation error was defined in other algorithms [28,35,27] to choose the subset of the training set. Using an RBF function, the trace is obtained as tr(K)=N where N denotes the size of the data set, then the threshold can be computed relative to the size. In the simulations a threshold equal to 0.01N was considered. The outputs of the algorithm are the index of the pivoting scheme and the matrix C. The former allow to identify the subset Φ R which will contribute to form R orthogonal basis vectors (see Eq. (14)). The main steps of the algorithm are given in Algorithm 1. Algorithm 1 Greedy KPCA: KPCA using Cholesky decomposition. % Given training data set X with size N % With c = 1 the model is computed with centered data otherwise not % The model is computed with L basis vectors % Define Parameters: σ (rbf), ε ( er ) % Perform a Cholesky Decomposition [30] % Inputs: X , σ ( rbf ) , ε ( er ) = 0.01 ⁎ N % Outputs: R × N matrix and vector with N elements: C, P % Compute the basis vectors % centering the data? c = 1 (yes) if c==1 mu=sum(C,2)/N; else mu=0; end C1=C-mu ⁎ ones(1,N); Q=C1 ⁎ C1’; % eigenvalues on decreasing order [E, D, dummy]=svd(Q); % L must be less or equal to R A=inv(C(1:R,1:R)) ⁎ E(:,1:L); Xr=X(:,P(1:R)); % The bias term of centering b=E(:,1:L)’ ⁎ mu; % Output parameters: R×L matrix A, L×1 vector b % Training subset with R elements Xr References [1] M.-H. Yang D.J. Kriegman N. Ahuja Detecting faces in images: a survey IEEE Transactions on Pattern Analysis and Machine Intelligence 24 1 2002 34 58 [2] B. Moghaddam Principal manifolds and probabilistic subspace for visual recognition IEEE Transactions on Pattern Analysis and Machine Intelligence 24 6 2002 780 788 [3] K.-R. Müller S. Mika G. Rätsch K. Tsuda B. Schölkopf An introduction to kernel-based algorithms IEEE Transactions on Neural Networks 12 2 2001 181 202 [4] B. Schölkopf A. Smola K.-R. Müller Nonlinear component analysis as a kernel eigenvalue problem Neural Computation 10 1998 1299 1319 [5] B. Schölkopf S. Mika C.J. Barges P. Knirsch K.-R. Müller G. Ratsch A.J. Smola Input space versus feature space in kernel-based methods IEEE Transactions on Neural Networks 10 5 1999 1000 1016 [6] J. Li X. Li D. Tao Kpca for semantic object extraction in images Pattern Recognition 41 2008 3244 3250 [7] C. Fowlkes S. Belongie F. Chung J. Malik Spectral grouping using the Nyström method IEEE Transactions on Pattern Analysis and Machine Intelligence 26 2 2004 214 225 [8] J. Yang X. Gao D. Zhang J.-y. Yang Kernel ICA: an alternative formulation and its application to face recognition Pattern Recognition 38 2005 1784 1787 [9] W.H. Press S.A. Teukolsky W.T. Vetterling B.P. Flannery Numerical Recipes in C. The Art of Scientific Computation 1994 Cambridge University Press [10] P. Cheng W. Li Ph. Ogunbona Greedy approximation of kernel {PCA} by minimizing the mapping error Digital Image Computing: Techniques and Applications 0 2009 303 308 , doi:ieeecomputersociety.org/10.1109/DICTA.2009.57 [11] A.J. Smola B. Schölkopf Sparse greedy matrix approximation for machine learning International Conference on Machine Learning 2000 Morgan Kaufmann San-Francisco, USA 911 918 [12] G.H. Golub C.F. van Loan Matrix Computations 1996 The John Hopkins University Press Baltimore [13] I.C. Choi C.L. Monma D.F. Shanno Further development of primal-dual interior point methods ORSA Journal on Computing 2 4 1990 304 311 [14] S. Fine K. Scheinberg Efficient SVM training using low-rank kernel representations Journal of Machine Learning Research 2 2001 243 264 [15] C.H.T. Baker The Numerical Treatment of Integral Equations 1977 Clarendon Press Oxford, UK [16] C.K. Williams M. Seeger Using the Nyström method to speed up kernel machines Advances in Neural Information Processing Systems, NIPS 13 2001 MIT Press 682 688 [17] C.K. Williams, On the extension of eigenvectors to new datapoints, in: Note, available at 〈 〉 , 2006, pp. 1–3. [18] B. Schölkopf A. Smola Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond 2002 MIT Press Cambridge, MA [19] H. Wang Z. Hu Y. Zhao An efficient algorithm for generalized discriminant analysis using incomplete Cholesky decomposition Pattern Recognition Letters 28 2007 254 259 [20] F.R. Bach M.I. Jordan Kernel independent component analysis Journal of Machine Learning Research 3 2002 1 48 [21] G. Rätsch T. Onoda K.R. Müller Soft margins for adaboost Machine Learning 42 3 2001 287 320 [22] A.R. Teixeira, A.M. Tomé, E.W. Lang, Greedy KPCA in biomedical signal processing, in: International Conference on Artificial Neural Networks—ICANN 07, Lecture Notes in Computer Science, vol. 4669, Porto, Portugal, 2007, pp. 486–495. [23] A.R. Teixeira A.M. Tomé E.W. Lang Exploiting low-rank approximations of kernel matrices in denoising applications IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2007) 2007 Thessaloniki Greece [24] Y. Xu D. Zhang F. Song J.-Y. Yang Z. Jing M. Li A method for speeding up feature extraction based on KPCA Neurocomputing 70 4–6 2007 1056 1061 [25] S.-W. Kim B.J. Oommen On using prototype reduction schemes to optimize kernel-based nonlinear subspace methods Pattern Recognition 37 2004 227 339 [26] D. Achlioptas F. McSherry B. Schölkopf Sampling techniques for kernel methods Advances in Neural Information Processing Systems 2002 MIT Press 335 342 [27] G. Baudat F. Anouar Feature vector selection and projection using kernels Neurocomputing 55 2003 21 38 [28] V. Franc V. Hlaváč Greedy algorithm for a training set reduction in the kernel methods 10th International Conference on Computer Analysis of Images and Patterns 2003 Springer Groningen, Holland 426 433 [29] G.C. Cawley N.L. Talbot Efficient leave-one-out cross-validation of kernel Fisher discriminant classifiers Pattern Recognition 36 2003 2585 2592 [30] F.R. Bach, Kernel independent component analysis, 2003, URL 〈 〉 . [31] A.R. Teixeira A.M. Tomé E.W. Lang Feature extraction using linear and non-linear subspace techniques C. Alippi M. Polycarpou Artificial Neural Networks—ICANN 2009 vol. II 2009 Springer-Verlag Cyprus 115 124 [32] K. Zhang J.T. Kwok Density-weighted Nyström method for computing large kernel eigensystems Neural Computation 21 2009 121 146 [33] R. Duda P. Hart D.G. Stork Pattern Classification 2001 John Wiley & Sons [34] S. Mika B. Schölkopf A. Smola K.R. Müller M. Scholz G. Rätsch Kernel PCA and de-noising in feature spaces Advances in Neural Information Processing 11 1999 MIT Press 532 536 [35] G.C. Cawley N.L.C. Talbot Efficient formation of a basis in a kernel induced feature space M. Verleysen European Symposium on Artificial Neural Networks 2002 d-side Bruges, Belgium 1 6 A.R. Teixeira is PhD student of Electrical Engineering at the University of Aveiro in Signal Processing group of IEETA. Her research interests include biomedical digital signal processing and principal and independent component analysis. A.M. Tomé is an Associate Professor of electrical engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as classification and pattern recognition applications. Elmar W. Lang is an Adjunct Professor of biophysics at the University of Regensburg, where he is heading the Neuro- and Bioinformatics Group. Currently, he serves as an Associate Editor of Neurocomputing and Neural Information Processing Letters and Reviews. His current research interests include biomedical signal and image processing, independent component analysis and blind source separation, neural networks for classification and pattern recognition, and stochastic process limits in queueing applications. "
    },
    {
        "doc_title": "Person identification using VEP signals and SVM classifiers",
        "doc_scopus_id": "79959476340",
        "doc_doi": "10.1109/IJCNN.2010.5596616",
        "doc_eid": "2-s2.0-79959476340",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Clinical conditions",
            "Data base",
            "Medical procedures",
            "Medical record",
            "Mental state",
            "Mental tasks",
            "Palm recognition",
            "Person identification",
            "Post-processing procedure",
            "Radial basis functions",
            "Real-time implementations",
            "Static classification",
            "SVM classifiers",
            "Visually evoked potentials"
        ],
        "doc_abstract": "This paper is focused on proving the concept that Visually Evoked Potential (VEP) signals registered during experiments with mental task execution can be used for discrimination of individuals. The viability of the VEP-based person identification was successfully tested for a data base of 13 persons. Among various classifiers tested, Support Vector Machine (SVM) with Radial Basis Function (RBF) exhibits the best performance. Our study revealed that the duration of the VEPs required for a reliable identification is crucial for real time implementation of the proposed biometry paradigm. A post processing procedure is formulated in order to overcome the problem of static classification. Our ultimate goal is not to compete with the conventional biometry (such as fingerprint, iris or palm recognition systems) but to design a VEP -based biometry modality as a supplement (\"a second opinion\") in clinical conditions. It could be used to verify a patient's identity in medical records, prior to medical procedures or to detect early in advance abnormal mental states of the patient. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Empirical mode decomposition - An introduction",
        "doc_scopus_id": "79959445738",
        "doc_doi": "10.1109/IJCNN.2010.5596829",
        "doc_eid": "2-s2.0-79959445738",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Basis functions",
            "Biomedical signal",
            "Empirical Mode Decomposition",
            "Essential component",
            "External stimulus",
            "Hilbert",
            "Hilbert Huang transforms",
            "Intrinsic mode functions",
            "Non-linear",
            "Nonstationary",
            "Orthogonal basis function",
            "Physiological process",
            "Spectral transform"
        ],
        "doc_abstract": "Due to external stimuli, biomedical signals are in general non-linear and non-stationary. Empirical Mode Decomposition in conjunction with a Hilbert spectral transform, together called Hilbert-Huang Transform, is ideally suited to extract essential components which are characteristic of the underlying biological or physiological processes. The method is fully adaptive and generates the basis to represent the data solely from these data and based on them. The basis functions, called Intrinsic Mode Functions (IMFs) represent a complete set of locally orthogonal basis functions whose amplitude and frequency may vary over time. The contribution reviews the technique of EMD and related algorithms and discusses illustrative applications. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sliding empirical mode decomposition",
        "doc_scopus_id": "79959429073",
        "doc_doi": "10.1109/IJCNN.2010.5596536",
        "doc_eid": "2-s2.0-79959429073",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Biomedical signal",
            "Data-driven",
            "Empirical Mode Decomposition",
            "Hilbert",
            "Hilbert Huang transforms",
            "Intrinsic mode functions",
            "Non-linear",
            "Non-stationary time series",
            "Nonstationary",
            "Orthogonal basis function",
            "Physiological process",
            "Real time",
            "Series analysis",
            "Spectral transform",
            "Time-periods"
        ],
        "doc_abstract": "Biomedical signals are in general non-linear and non-stationary which renders them difficult to analyze with classical time series analysis techniques. Empirical Mode Decomposition (EMD) in conjunction with a Hilbert spectral transform, together called Hilbert-Huang Transform, is ideally suited to extract informative components which are characteristic of underlying biological or physiological processes. The method is fully adaptive and generates a complete set of orthogonal basis functions, called Intrinsic Mode Functions (IMFs), in a purely data-driven manner. Amplitude and frequency of IMFs may vary over time which renders them different from conventional basis systems and ideally suited to study non-linear and non-stationary time series. However, biomedical time series are often recorded over long time periods. This generates the need for efficient EMD algorithms which can analyze the data in real time. No such algorithms yet exist which are robust, efficient and easy to implement. The contribution shortly reviews the technique of EMD and related algorithms and develops an on-line variant, called Sliding Empirical Mode Decomposition (SEMD), which is shown to perform well on large scale time series. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Brain status data analyzed by empirical mode decomposition",
        "doc_scopus_id": "79959416181",
        "doc_doi": "10.1109/IJCNN.2010.5596533",
        "doc_eid": "2-s2.0-79959416181",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Biomedical signal",
            "Empirical Mode Decomposition",
            "External stimulus",
            "Information contents",
            "Intelligent signal processing",
            "Intrinsic mode functions",
            "Neuromonitoring",
            "Non-linear",
            "Nonstationary",
            "Orthogonal basis function",
            "Oscillatory mode",
            "Time varying"
        ],
        "doc_abstract": "Due to external stimuli, biomedical signals are in general non-linear and non-stationary. Intelligent signal processing is crucial to unravel the information content buried in biomedical time series. Empirical Mode Decomposition is ideally suited to extract all pure oscillatory modes which are contained in the signal. These modes, called Intrinsic Mode Functions (IMFs), represent a complete set of locally orthogonal basis functions with time-varying amplitude and frequency. The contribution discusses the application of an online variant, called SEMD, to non-stationary biomedical time series recorded during neuromonitoring. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploratory matrix factorization for PET image analysis",
        "doc_scopus_id": "78650840905",
        "doc_doi": "10.1109/IEMBS.2010.5627704",
        "doc_eid": "2-s2.0-78650840905",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Alzheimer's disease",
            "Automatic feature extraction",
            "Classification rates",
            "Early diagnosis",
            "Matrix factorizations",
            "Nonnegative matrix factorization",
            "PET image analysis",
            "PET images",
            "Random forests",
            "Tree classifiers"
        ],
        "doc_abstract": "Features are extracted from PET images employing exploratory matrix factorization techniques such as nonnegative matrix factorization (NMF). Appropriate features are fed into classifiers such as a support vector machine or a random forest tree classifier. An automatic feature extraction and classification is achieved with high classification rate which is robust and reliable and can help in an early diagnosis of Alzheimer's disease. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Spatiotemporal ICA applied to retinotopic fMRI data",
        "doc_scopus_id": "78650840487",
        "doc_doi": "10.1109/IEMBS.2010.5627291",
        "doc_eid": "2-s2.0-78650840487",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "fMRI data",
            "Functional magnetic resonance imaging",
            "Independent components",
            "Parameter setting"
        ],
        "doc_abstract": "We use two spatiotemporal Independent Component Analysis algorithms, stJADE and stSOBI, to analyse data from a retinotopic functional magnetic resonance imaging experiment and compare their performance to the analysis of the same data with the spatial ICA done with JADE. This kind of experimental setting has the advantage that the activation in the brain can be estimated fairly easily and therefore can be used as well defined benchmark. We show that stSOBI can outperform sJADE and exhibits quite stable behaviour while stJADE critically depends on the quality of the chosen parameter settings for each subject. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Clustering evoked potential signals using subspace methods",
        "doc_scopus_id": "78650807610",
        "doc_doi": "10.1109/IEMBS.2010.5627971",
        "doc_eid": "2-s2.0-78650807610",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clustering techniques",
            "Ensemble averages",
            "Evoked potentials",
            "Evoked response",
            "Orthogonal subspaces",
            "Sub-space methods"
        ],
        "doc_abstract": "This work proposes a clustering technique to analyze evoked potential signals. The proposed method uses an orthogonal subspace model to enhance the single-trial signals of a session and simultaneously a subspace measure to group the trials into clusters. The ensemble averages of the signals of the different clusters are compared with ensemble averages of visually selected trials which are free of any artifact. Preliminary results consider recordings from an occipital channel where evoked response P100 wave is most pronounced. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Advances in EEG-based biometry",
        "doc_scopus_id": "77955351275",
        "doc_doi": "10.1007/978-3-642-13775-4_29",
        "doc_eid": "2-s2.0-77955351275",
        "doc_date": "2010-08-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Data base",
            "EEG signals",
            "Health disorders",
            "Human brain",
            "Mental tasks",
            "Person identification",
            "Potential applications",
            "Radial basis functions",
            "Static classification"
        ],
        "doc_abstract": "This paper is focused on proving the concept that the EEG signals collected during a perception or mental task can be used for discrimination of individuals. The viability of the EEG-based person identification was successfully tested for a data base of 13 persons. Among various classifiers tested, Support Vector Machine (SVM) with Radial Basis Function (RBF) exhibits the best performance. The problem of static classification that does not take into account the temporal nature of the EEG sequence was considered by an empirical post classifier procedure. The algorithm proposed has an effect of introducing a memory into the classifier without increasing its complexity. Control of a classified access into restricted areas security systems, health disorder identification in medicine, gaining more understanding of the cognitive human brain processes in neuroscience are some of the potential applications of EEG-based biometry. © 2010 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploratory matrix factorization for PET image analysis",
        "doc_scopus_id": "77954578837",
        "doc_doi": "10.1007/978-3-642-13769-3_56",
        "doc_eid": "2-s2.0-77954578837",
        "doc_date": "2010-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic classification",
            "Classification rates",
            "exploratory matrix factorization",
            "False negatives",
            "Matrix factorizations",
            "Nonnegative matrix factorization",
            "PET image analysis",
            "PET images",
            "Positron emission",
            "Random forests",
            "Tomographic images"
        ],
        "doc_abstract": "Features are extracted from PET images employing exploratory matrix factorization techniques, here non-negative matrix factorization (NMF). Appropriate features are fed into classifiers such as support vector machine or random forest. An automatic classification is achieved with high classification rate and only few false negatives. © 2010 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automated solvent artifact removal and base plane correction of multidimensional NMR protein spectra by AUREMOL-SSA",
        "doc_scopus_id": "77954690616",
        "doc_doi": "10.1007/s10858-010-9414-z",
        "doc_eid": "2-s2.0-77954690616",
        "doc_date": "2010-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Spectroscopy",
                "area_abbreviation": "CHEM",
                "area_code": "1607"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Strong solvent signals lead to a disappearance of weak protein signals close to the solvent resonance frequency and to base plane variations all over the spectrum. AUREMOL-SSA provides an automated approach for solvent artifact removal from multidimensional NMR protein spectra. Its core algorithm is based on singular spectrum analysis (SSA) in the time domain and is combined with an automated base plane correction in the frequency domain. The performance of the method has been tested on synthetic and experimental spectra including twodimensional NOESY and TOCSY spectra and a threedimensional 1H,13C-HCCH-TOCSY spectrum. It can also be applied to frequency domain spectra since an optional inverse Fourier transformation is included in the algorithm. © Springer Science+Business Media B.V. 2010.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Machine learning techniques to identify marker genes for diagnostic classification of microarrays",
        "doc_scopus_id": "85049053220",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85049053220",
        "doc_date": "2010-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2010 by Nova Science Publishers, Inc. All rights reserved.Intelligent and efficient mathematical and computational tools are needed to analyze and interpret the information content buried in large scale gene expression patterns made available by the recent development of microarray technology [28, 27]. Modern machine learning techniques like Support Vector Machines (SVM) or matrix decomposition techniques, like Principal Component Analysis (PCA), Singular Value Decomposition (SVD), Independent Component Analysis (ICA) and Nonnegative Matrix Factorization (NMF), provide new and efficient analysis tools which are currently explored in this area [76]. In this study we focus on classification tasks and apply knowledge-based as well as data-driven approaches to various microarray data sets. The data sets considered comprise the gene expression levels of either human breast cancer (HBC) cell lines or the famous leukemia data set or human peripheral blood cells differentiating from monocytes to macrophages under various environmental conditions. We study gene selection procedures either in gene space orin feature space and show that these tools are able to extract marker genes from these gene expression profiles without the need for extensive data bank search for appropriate functional annotations. With these marker genes corresponding test data sets can then easily be classified into related diagnostic categories.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Feature extraction using linear and non-linear subspace techniques",
        "doc_scopus_id": "70450174935",
        "doc_doi": "10.1007/978-3-642-04277-5_12",
        "doc_eid": "2-s2.0-70450174935",
        "doc_date": "2009-11-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Basis vector",
            "Benchmark data",
            "Dual form",
            "Feature extraction methods",
            "Feature extraction techniques",
            "Incomplete Cholesky decomposition",
            "Model-based",
            "Non-linear",
            "Subspace models"
        ],
        "doc_abstract": "This paper provides a new insight into unsupervised feature extraction techniques based on subspace models. In this work the subspace models are described exploiting the dual form of the basis vectors. In what concerns the kernel based model, a computationally less demanding model based on incomplete Cholesky decomposition is also introduced. An online benchmark data set allows the evaluation of the feature extraction methods comparing the performance of two classifiers having as input the raw data and the new representations. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Serial evolution",
        "doc_scopus_id": "71049127449",
        "doc_doi": "10.1007/978-3-642-02264-7_25",
        "doc_eid": "2-s2.0-71049127449",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Algorithmic optimization",
            "Biological evolution",
            "Computational costs",
            "Cross-over",
            "Evolution strategies",
            "Fitness functions",
            "Mathematical details",
            "Metaheuristic",
            "Parallel search",
            "Performance improvements",
            "Schema theorems",
            "Search spaces",
            "Sequential simulation"
        ],
        "doc_abstract": "Genetic algorithms (GA) represent an algorithmic optimization technique inspired by biological evolution. A major strength of this meta-heuristic is its ability to explore the search space in independent parallel search routes rendering the algorithm highly efficient if implemented on a parallel architecture. Sequential simulations of GAs frequently result in enormous computational costs. To alleviate this problem, we propose a serial evolution strategy which results in a much smaller number of necessary fitness function evaluations thereby speeding up the computation considerably. If implemented on a parallel architecture the savings in computational costs are even more pronounced. We present the algorithm in full mathematical detail and proof the corresponding schema theorem for a simple case without cross-over operations. A toy example illustrates the operation of serial evolution and the performance improvement over a canonical genetic algorithm. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic generation of biped walk behavior using genetic algorithms",
        "doc_scopus_id": "68749098693",
        "doc_doi": "10.1007/978-3-642-02478-8_101",
        "doc_eid": "2-s2.0-68749098693",
        "doc_date": "2009-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D simulations",
            "Adaptive behavior",
            "Automatic Generation",
            "Biped",
            "Biped Robot",
            "Biped walk",
            "Complex environments",
            "Degrees of freedom",
            "Humanoid",
            "Humanoid robot",
            "Joint trajectories",
            "Locomotion",
            "Offline",
            "Partial-Fourier",
            "RoboCup",
            "Walk forward",
            "Walking gait"
        ],
        "doc_abstract": "Controlling a biped robot with several degrees of freedom is a challenging task that takes the attention of several researchers in the fields of biology, physics, electronics, computer science and mechanics. For a humanoid robot to perform in complex environments, fast, stable and adaptive behaviors are required. This paper proposes a solution for automatic generation of a walking gait using genetic algorithms (GA). A method based on partial Fourier series was developed for joint trajectory planning. GAs were then used for offline generation of the parameters that define the gait. GAs proved to be a powerful method for automatic generation of humanoid behaviors resulting on a walk forward velocity of 0.51m/s which is a good result considering the results of the three best teams of RoboCup 3D simulation league for the same movement. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "How to apply nonlinear subspace techniques to univariate biomedical time series",
        "doc_scopus_id": "67949115716",
        "doc_doi": "10.1109/TIM.2009.2016385",
        "doc_eid": "2-s2.0-67949115716",
        "doc_date": "2009-08-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Electroencephalogram (EEG)",
            "Electrooculogram (EOG)",
            "Kernel principal component analysis (KPCA)",
            "Local singular spectrum analysis (SSA)",
            "Removing artifacts",
            "Subspace techniques"
        ],
        "doc_abstract": "In this paper, we propose an embedding technique for univariate single-channel biomedical signals to apply projective subspace techniques. Biomedical signals are often recorded as 1-D time series; hence, they need to be transformed to multidimensional signal vectors for subspace techniques to be applicable. The transformation can be achieved by embedding an observed signal in its delayed coordinates. We propose the application of two nonlinear subspace techniques to embedded multidimensional signals and discuss their relation. The techniques consist of modified versions of singular-spectrum analysis (SSA) and kernel principal component analysis (KPCA). For illustrative purposes, both nonlinear subspace projection techniques are applied to an electroencephalogram (EEG) signal recorded in the frontal channel to extract its dominant electrooculogram (EOG) interference. Furthermore, to evaluate the performance of the algorithms, an experimental study with artificially mixed signals is presented and discussed. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Minimum determinant constraint for non-negative matrix factorization",
        "doc_scopus_id": "67149144290",
        "doc_doi": "10.1007/978-3-642-00599-2_14",
        "doc_eid": "2-s2.0-67149144290",
        "doc_date": "2009-06-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Exact solution",
            "Illustrative examples",
            "Nonnegative matrix factorization",
            "Optimal solutions",
            "Sparsity constraints"
        ],
        "doc_abstract": "We propose a determinant criterion to constrain the solutions of non-negative matrix factorization problems and achieve unique and optimal solutions in a general setting, provided an exact solution exists. We demonstrate with illustrative examples how optimal solutions are obtained using our new algorithm detNMF and discuss the difference to NMF algorithms imposing sparsity constraints. © Springer-Verlag Berlin Heidelberg 2009.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A matrix factorization classifier for knowledge-based microarray analysis",
        "doc_scopus_id": "58149141891",
        "doc_doi": "10.1007/978-3-540-85861-4_17",
        "doc_eid": "2-s2.0-58149141891",
        "doc_date": "2009-01-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (miscellaneous)",
                "area_abbreviation": "COMP",
                "area_code": "1701"
            },
            {
                "area_name": "Computational Mechanics",
                "area_abbreviation": "ENGI",
                "area_code": "2206"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "In this study we analyze microarray data sets which monitor the gene expression levels of human peripheral blood cells during differentiation from monocytes to macrophages. We show that matrix decomposition techniques are able to identify relevant signatures in the deduced matrices and extract marker genes from these gene expression profiles. With these marker genes corresponding test data sets can then easily be classified into related diagnostic categories. The latter correspond to either monocytes vs macrophages or healthy vs Niemann Pick C diseased patients. Our results demonstrate that these methods are able to identify suitable marker genes which can be used to classify the type of cell lines investigated. © 2009 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Knowledge-based gene expression classification via matrix factorization",
        "doc_scopus_id": "48249153778",
        "doc_doi": "10.1093/bioinformatics/btn245",
        "doc_eid": "2-s2.0-48249153778",
        "doc_date": "2008-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Motivation: Modern machine learning methods based on matrix decomposition techniques, like independent component analysis (ICA) or non-negative matrix factorization (NMF), provide new and efficient analysis tools which are currently explored to analyze gene expression profiles. These exploratory feature extraction techniques yield expression modes (ICA) or metagenes (NMF). These extracted features are considered indicative of underlying regulatory processes. They can as well be applied to the classification of gene expression datasets by grouping samples into different categories for diagnostic purposes or group genes into functional categories for further investigation of related metabolic pathways and regulatory networks. Results: In this study we focus on unsupervised matrix factorization techniques and apply ICA and sparse NMF to microarray datasets. The latter monitor the gene expression levels of human peripheral blood cells during differentiation from monocytes to macrophages. We show that these tools are able to identify relevant signatures in the deduced component matrices and extract informative sets of marker genes from these gene expression profiles. The methods rely on the joint discriminative power of a set of marker genes rather than on single marker genes. With these sets of marker genes, corroborated by leave-one-out or random forest cross-validation, the datasets could easily be classified into related diagnostic categories. The latter correspond to either monocytes versus macrophages or healthy vs Niemann Pick C disease patients. © 2008 The Author(s).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Feature extraction using low-rank approximations of the kernel matrix",
        "doc_scopus_id": "47749097600",
        "doc_doi": "10.1007/978-3-540-69812-8_40",
        "doc_eid": "2-s2.0-47749097600",
        "doc_date": "2008-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Basis vectors",
            "Data sets",
            "Eigen decomposition",
            "Eigenvectors",
            "Extension method",
            "Feature extraction and classification",
            "Heidelberg (CO)",
            "High dimensional spaces",
            "High-dimensional",
            "International conferences",
            "Kernel functions",
            "Kernel matrices",
            "Largest eigenvalues",
            "Low rank approximations",
            "M method",
            "Numerical simulations",
            "Subspace techniques",
            "Training sets"
        ],
        "doc_abstract": "In this work we use kernel subspace techniques to perform feature extraction. The projections of the data onto the coordinates of the high-dimensional space created by the kernel function are called features. The basis vectors to project the data depend on the eigendecomposition of the kernel matrix which might become very high-dimensional in case of a large training set. Nevertheless only the largest eigenvalues and corresponding eigenvectors are used to extract relevant features. In this work, we present low-rank approximations to the kernel matrix based on the Nyström method. Numerical simulations will then be used to demonstrate the Nyström extension method applied to feature extraction and classification. The performance of the presented methods is demonstrated using the USPS data set. © 2008 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hybridizing sparse component analysis with genetic algorithms for microarray analysis",
        "doc_scopus_id": "44649149032",
        "doc_doi": "10.1016/j.neucom.2007.09.017",
        "doc_eid": "2-s2.0-44649149032",
        "doc_date": "2008-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Cognitive Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2805"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Microarray analysis",
            "Nonnegative matrix factorization (NMF)"
        ],
        "doc_abstract": "Nonnegative matrix factorization (NMF) has proven to be a useful tool for the analysis of nonnegative multivariate data. However, it is known not to lead to unique results when applied to blind source separation (BSS) problems. In this paper we present an extension of NMF capable of solving the BSS problem when the underlying sources are sufficiently sparse. In contrast to most well-established BSS methods, the devised algorithm is capable of solving the BSS problem in cases where the underlying sources are not independent or uncorrelated. As the proposed fitness function is discontinuous and possesses many local minima, we use a genetic algorithm for its minimization. Finally, we apply the devised algorithm to real world microarray data. © 2008 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271597 291210 291735 291866 31 Neurocomputing NEUROCOMPUTING 2008-02-15 2008-02-15 2010-11-18T03:54:23 S0925-2312(08)00105-7 S0925231208001057 10.1016/j.neucom.2007.09.017 S300 S300.1 FULL-TEXT 2020-03-13T12:15:27.710055Z 0 0 20080601 20080630 2008 2008-02-15T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 0925-2312 09252312 71 71 10 12 10 11 12 Volume 71, Issues 10–12 53 2356 2376 2356 2376 200806 June 2008 2008-06-01 2008-06-30 2008 Neurocomputing for Vision Research Dacheng Tao a Xuelong Li b a Hong Kong Polytechnic University, Hong Kong, China b Birbeck College, University of London, London, UK Advances in Blind Signal Processing Deniz Erdogmus c Danilo Mandic d Toshihisa Tanaka e c Oregon Health and Science University, Beaverton, OR, USA d Imperial College, London, UK e Tokyo University of Agriculture and Technology, Tokyo, Japan Special papers: Advances in Blind Signal Processing article fla Copyright © 2008 Elsevier B.V. All rights reserved. HYBRIDIZINGSPARSECOMPONENTANALYSISGENETICALGORITHMSFORMICROARRAYANALYSIS STADLTHANNER K 1 Introduction 2 Matrix factorization and BSS 3 Sparse nonnegative BSS 3.1 Nonnegative matrix factorization 3.2 Sparseness measure 4 Genetic algorithm-based optimization 4.1 Fitness function 4.2 Genetic operators 4.3 Parallelization 4.4 Algorithm repetitions 5 sNMF simulation studies using toy data 5.1 Robustness to noise 5.2 Recovery of correlated sources 5.3 Comparison with geometric methods 6 sNMF simulation studies using microarray data 7 Conclusions Acknowledgments References 1972 HANDBOOKMATHEMATICALFORMULASGRAPHSMATHEMATICALTABLES BALDI 2002 P DNAMICROARRAYSGENEEXPRESSION BARBER 1996 469 483 C CHIAPETTA 2004 1090 1109 P CICHOCKI 2002 A ADAPTIVEBLINDSIGNALIMAGEPROCESSING CONSORTIUM 2000 25 29 T FIORI 2005 214 244 S GRUBER 2006 1485 1501 P HABL 2000 131 136 M PERSPECTIVESINNEUROSCIENCEARTIFICIALNEURALNETWORKSINMEDICINEBIOLOGY ANALYZINGBRAINTUMORRELATEDEEGSIGNALSICAALGORITHMS HOYER 2004 1457 1469 P HYVARINEN 2001 A INDEPENDENTCOMPONENTANALYSIS HYVARINEN 1998 A PROCEEDINGSICPR1998 IMAGEFEATUREEXTRACTIONBYSPARSECODINGINDEPENDENTCOMPONENTANALYSIS LEE 1999 788 791 D LEE 2003 R76.1 R76.21 S LESS 2001 D ADVANCESINNEURALINFORMATIONPROCESSING13NIPS2000 ALGORITHMSFORNONNEGATIVEMATRIXFACTORIZATION LEWICKI 2000 337 365 M LI 2004 1193 1234 Y MACQUEEN 1967 J PROCEEDINGSFIFTHBERKELYSYMPOSIUMMATHEMATICALSTATISTICSPROBABILITY METHODSFORCLASSIFICATIONANALYSISMULTIVARIATEOBSERVATIONS MAKEIGH 1995 S ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS8NIPS1995 INDEPENDENTCOMPONENTANALYSISELECTROENCEPHALOGRAPHICDATA MICHALEWICZ 1999 Z GENETICALGORITHMSDATASTRUCTURESEVOLUTIONPROGRAMS OHLSHAUSEN 1996 333 339 B PLUMBLEY 2005 161 197 M PUNTONET 1998 141 164 C QUACKENBUSH 2001 418 427 J RUDERMAN 1994 517 548 D SAIDI 2004 6677 6683 S SARELA 2005 233 272 J STADLTHANNER 2003 103 110 K STADLTHANNER 2006 497 522 K THEIS 2003 419 439 F VIGARIO 1997 R ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS10NIPS1997 INDEPENDENTCOMPONENTANALYSISFORIDENTIFICATIONARTIFACTSINMAGNETOENCEPHALOGRAPHICRECORDINGS YANG 2004 27 32 K STADLTHANNERX2008X2356 STADLTHANNERX2008X2356X2376 STADLTHANNERX2008X2356XK STADLTHANNERX2008X2356X2376XK 2020-03-08T12:15:15.435Z S0925231208001057 DAAD-GRICES Acções Integradas Luso - Alemãs Corporate Technology, Munich DFG GRK 638 DFG Deutsche Forschungsgemeinschaft DAAD DAAD Deutscher Akademischer Austauschdienst Siemens AG Financial support by Siemens AG, Corporate Technology, Munich (project Biomarker ), the DFG (GRK 638: Nonlinearity and Nonequilibrium in Condensed Matter), the DAAD-GRICES Acções Integradas Luso - Alemãs (project GEVD-MP) and the DAAD Acciones Integradas Hispano - Alemanas (project Microarrays) is gratefully acknowledged. The PXE data set was kindly provided by PD Dr. Langmann, Institute of Clinical Chemistry (Director: Prof. Dr. G. Schmitz), University Clinic Regensburg. item S0925-2312(08)00105-7 S0925231208001057 10.1016/j.neucom.2007.09.017 271597 2010-12-22T11:14:37.841455-05:00 2008-06-01 2008-06-30 true 2182076 MAIN 21 69673 849 656 IMAGE-WEB-PDF 1 si99 141 10 13 si98 362 15 63 si97 213 14 30 si96 208 14 29 si95 178 15 22 si94 181 15 22 si93 182 15 22 si92 172 15 22 si91 321 21 38 si90 173 15 20 si9 258 14 38 si89 224 16 30 si88 261 13 41 si87 226 14 29 si86 242 16 33 si85 226 16 31 si84 209 12 31 si83 211 13 30 si82 138 12 11 si81 291 13 49 si80 239 16 33 si8 152 13 11 si79 220 13 30 si78 116 9 9 si77 128 13 10 si76 274 15 77 si75 321 21 38 si74 365 19 50 si73 362 19 50 si72 364 19 50 si71 359 19 50 si70 321 21 38 si7 140 12 14 si69 155 12 14 si68 354 19 48 si67 149 10 14 si66 251 19 29 si65 231 13 30 si64 256 13 30 si63 251 19 29 si62 245 19 29 si61 247 13 30 si60 248 13 30 si6 155 12 14 si59 149 10 14 si58 141 10 13 si57 263 16 33 si563 102 5 13 si562 121 12 13 si561 109 14 5 si560 111 14 6 si56 241 16 27 si559 109 14 5 si558 111 14 6 si557 109 14 5 si556 111 14 6 si555 109 14 5 si554 111 14 6 si553 333 14 54 si552 351 14 57 si551 183 10 26 si550 201 10 29 si55 196 13 38 si549 155 12 14 si548 152 13 11 si547 116 9 9 si546 155 12 14 si545 116 9 9 si544 12095 186 530 si543 156 17 14 si542 432 14 85 si541 228 13 30 si540 116 9 9 si54 144 10 15 si539 144 10 15 si538 168 18 11 si537 156 17 14 si536 312 17 60 si535 312 17 60 si534 356 17 71 si533 220 13 30 si532 364 17 72 si531 168 18 11 si530 156 17 14 si53 127 9 11 si529 155 12 14 si528 380 13 71 si527 266 13 47 si526 140 12 14 si525 155 12 14 si524 356 13 75 si523 248 16 31 si522 265 16 33 si521 256 16 31 si520 247 16 30 si52 140 12 14 si519 260 16 31 si518 265 16 33 si517 140 12 14 si516 155 12 14 si515 2262 69 229 si514 431 16 72 si513 152 13 11 si512 152 13 11 si511 152 13 11 si510 140 12 14 si509 140 12 14 si508 140 12 14 si51 140 12 14 si507 140 12 14 si506 140 12 14 si505 155 12 14 si504 152 13 11 si503 140 12 14 si502 369 17 74 si501 168 18 11 si500 156 17 14 si50 152 13 11 si5 140 12 14 si499 155 12 14 si498 306 12 71 si497 155 12 14 si496 155 12 14 si495 306 12 71 si494 140 12 14 si493 140 12 14 si492 155 12 14 si491 207 15 24 si490 306 12 71 si49 140 12 14 si489 152 13 11 si488 140 12 14 si487 155 12 14 si486 152 13 11 si485 140 12 14 si484 142 11 12 si483 434 16 91 si482 140 12 14 si481 218 16 23 si480 213 15 25 si48 155 12 14 si479 2233 69 221 si478 979 20 285 si477 283 15 77 si476 212 21 19 si475 191 11 30 si474 152 11 13 si473 149 10 14 si472 141 10 13 si471 140 12 14 si470 284 15 63 si47 152 13 11 si469 139 10 12 si468 2314 69 229 si467 152 13 11 si466 155 12 14 si465 152 13 11 si464 152 11 13 si463 141 10 13 si462 149 10 14 si461 141 10 13 si460 301 13 58 si46 207 15 24 si459 392 15 106 si458 139 10 12 si457 261 16 30 si456 805 16 192 si455 284 15 63 si454 139 10 12 si453 141 10 13 si452 309 21 40 si451 140 12 14 si450 181 18 19 si45 152 13 11 si449 156 17 14 si448 140 12 14 si447 284 15 63 si446 321 21 38 si445 140 12 14 si444 140 12 14 si443 306 21 40 si442 299 21 40 si441 140 12 14 si440 362 15 63 si44 140 12 14 si439 416 15 73 si438 321 21 38 si437 152 13 11 si436 213 14 30 si435 228 18 23 si434 228 18 23 si433 299 23 38 si432 378 23 57 si431 228 18 23 si430 310 15 68 si43 155 12 14 si429 152 13 11 si428 228 18 23 si427 208 14 29 si426 140 12 14 si425 156 17 14 si424 523 24 120 si423 2676 107 254 si422 215 17 25 si421 228 18 23 si420 321 21 38 si419 173 15 20 si418 185 12 25 si417 152 13 11 si42 155 12 14 si416 228 18 23 si415 173 15 20 si414 228 18 23 si413 312 21 37 si412 664 44 87 si411 173 15 20 si410 270 16 33 si41 140 12 14 si409 270 16 33 si408 308 18 39 si407 155 12 14 si406 321 21 38 si405 270 16 33 si404 502 17 111 si403 228 18 23 si402 270 16 33 si401 557 21 112 si400 321 21 38 si40 177 15 17 si4 202 12 37 si399 140 12 14 si398 274 15 77 si397 228 18 23 si396 173 15 20 si395 213 15 25 si394 325 15 92 si393 173 15 20 si392 144 10 15 si391 116 9 9 si390 224 16 30 si39 199 13 35 si389 242 16 33 si388 224 16 30 si387 242 16 33 si386 293 16 54 si385 140 12 14 si384 151 19 9 si383 166 14 21 si382 1600 69 243 si381 239 16 33 si380 140 12 14 si379 210 15 25 si378 239 16 33 si377 321 21 38 si376 155 12 14 si375 321 21 38 si374 354 19 48 si373 129 12 19 si372 350 23 49 si371 350 23 49 si38 308 13 56 si370 321 21 38 si37 277 13 43 si369 397 19 79 si368 198 18 21 si367 871 22 235 si366 155 12 14 si365 350 23 49 si364 428 16 82 si363 2760 69 277 si362 155 12 14 si361 152 13 11 si360 191 11 30 si36 258 19 33 si359 150 10 14 si358 855 49 130 si357 155 12 14 si356 425 19 78 si355 615 49 86 si354 168 18 11 si353 156 17 14 si352 290 15 44 si351 224 16 30 si350 609 18 109 si35 278 18 38 si349 155 12 14 si348 248 18 25 si347 233 17 26 si346 237 18 25 si345 155 12 14 si344 237 18 25 si343 485 18 83 si342 221 17 26 si341 237 18 25 si340 155 12 14 si34 155 12 14 si339 239 16 33 si338 220 13 30 si337 233 13 33 si336 300 17 48 si335 226 16 31 si334 301 15 62 si333 261 11 50 si332 168 18 11 si331 168 18 11 si330 168 18 11 si33 144 10 14 si329 226 14 29 si328 226 16 31 si327 226 16 31 si326 291 13 49 si325 239 16 33 si324 390 13 91 si323 261 13 41 si322 209 12 31 si321 170 13 17 si320 211 13 30 si32 149 10 14 si319 354 17 58 si318 258 14 38 si317 734 47 106 si316 362 15 98 si315 321 18 43 si314 170 13 17 si313 138 12 11 si312 210 18 23 si311 1086 39 243 si310 362 15 98 si31 152 11 13 si309 321 18 43 si308 156 17 14 si307 168 18 11 si306 876 50 140 si305 554 16 133 si304 611 16 145 si303 700 22 187 si302 396 18 80 si301 691 22 177 si300 156 17 14 si30 139 10 12 si3 152 13 11 si299 168 18 11 si298 168 17 21 si297 206 17 25 si296 658 22 156 si295 676 22 166 si294 168 17 21 si293 431 19 80 si292 301 15 62 si291 220 13 30 si290 489 15 146 si289 140 12 14 si288 140 12 14 si287 156 17 14 si286 210 15 25 si285 159 13 18 si284 140 12 14 si283 152 13 11 si282 127 9 11 si281 152 13 11 si280 140 12 14 si29 141 10 13 si28 213 14 30 si279 249 14 33 si278 236 13 34 si277 149 10 14 si276 368 19 49 si275 152 13 11 si274 140 12 14 si273 144 10 15 si272 249 14 33 si271 236 13 34 si270 152 13 11 si27 238 18 25 si269 140 12 14 si268 266 16 33 si267 368 19 49 si266 149 10 14 si265 251 19 29 si264 470 14 100 si263 155 12 14 si262 471 18 108 si261 333 18 68 si260 249 14 33 si26 208 14 29 si259 152 13 11 si258 600 41 114 si257 140 12 14 si256 155 12 14 si255 543 14 145 si254 542 14 146 si253 503 40 92 si252 155 12 14 si251 152 13 11 si250 149 10 14 si25 238 18 25 si249 141 10 13 si248 144 10 15 si247 127 9 11 si246 168 18 11 si245 152 13 11 si244 174 17 17 si243 610 20 104 si242 1702 20 399 si241 155 12 14 si240 147 12 14 si24 332 21 38 si239 142 12 13 si238 168 18 11 si237 596 18 132 si236 168 18 11 si235 156 17 14 si234 152 13 11 si233 140 12 14 si232 155 12 14 si231 152 13 11 si230 155 12 14 si23 155 12 14 si229 152 13 11 si228 140 12 14 si227 949 18 257 si226 120 9 8 si225 138 10 12 si224 1305 52 230 si223 120 9 8 si222 159 13 18 si221 150 13 17 si220 159 13 18 si219 241 16 27 si218 128 13 10 si217 152 13 11 si216 155 12 14 si215 128 13 10 si214 237 16 32 si213 323 16 59 si212 164 10 17 si211 253 16 29 si22 284 15 63 si210 2067 37 358 si21 321 21 38 si209 144 10 15 si208 150 10 14 si207 232 16 29 si206 323 16 59 si205 150 10 14 si204 120 9 8 si203 144 10 15 si202 156 17 14 si201 140 12 14 si200 156 17 14 si20 127 9 11 si2 155 12 14 si199 361 17 72 si198 168 18 11 si197 140 12 14 si196 168 18 11 si195 181 15 17 si194 217 13 34 si193 144 10 15 si192 1047 34 195 si191 1384 48 269 si190 228 18 22 si19 149 10 14 si189 155 12 14 si188 221 16 28 si187 1779 45 329 si186 168 18 11 si185 156 17 14 si184 155 12 14 si183 145 12 15 si182 145 12 15 si181 155 12 14 si180 145 12 15 si18 251 19 29 si179 155 12 14 si178 145 12 15 si177 175 12 18 si176 145 12 15 si175 175 12 18 si174 155 12 14 si173 155 12 14 si172 140 12 14 si171 152 13 11 si170 152 13 11 si17 251 19 29 si169 152 13 11 si168 716 21 148 si167 168 18 11 si166 168 18 11 si165 156 17 14 si164 152 13 11 si163 140 12 14 si162 255 20 29 si161 140 12 14 si160 152 13 11 si16 245 19 29 si159 152 13 11 si158 152 13 11 si157 152 13 11 si156 150 10 14 si155 155 12 14 si154 140 12 14 si153 152 13 11 si152 269 12 54 si151 145 12 15 si150 175 12 18 si15 149 10 14 si149 152 13 11 si148 140 12 14 si147 155 12 14 si146 301 13 59 si145 155 12 14 si144 140 12 14 si143 175 13 17 si142 152 13 11 si141 234 12 47 si140 155 12 14 si14 141 10 13 si139 175 12 18 si138 145 12 15 si137 298 13 48 si136 155 12 14 si135 326 13 67 si134 145 12 15 si133 234 12 47 si132 175 12 18 si131 282 12 52 si130 155 12 14 si13 258 14 36 si129 261 11 50 si128 191 11 30 si127 191 11 30 si126 173 15 18 si125 173 14 19 si124 162 14 18 si123 113 12 6 si122 161 14 17 si121 137 10 13 si120 146 11 14 si12 155 12 14 si119 145 10 15 si118 133 10 14 si117 362 15 63 si116 213 14 30 si115 208 14 29 si114 309 21 40 si113 362 15 63 si112 213 14 30 si111 208 14 29 si110 310 21 40 si11 521 17 121 si109 362 15 63 si108 213 14 30 si107 208 14 29 si106 306 21 40 si105 362 15 63 si104 213 14 30 si103 208 14 29 si102 299 21 40 si101 152 11 13 si100 149 10 14 si10 155 12 14 si1 152 13 11 fx1 8893 151 113 fx1 2004 93 70 fx2 13001 151 113 fx2 2354 93 70 fx3 9670 151 113 fx3 1949 93 70 fx4 10761 151 113 fx4 2137 93 70 fx5 9408 151 113 fx5 1974 93 70 fx6 9648 151 113 fx6 2071 93 70 gr1 80757 878 552 gr1 856 92 58 gr10 39312 608 383 gr10 869 94 59 gr11 47488 600 387 gr11 832 93 60 gr12 44465 532 386 gr12 999 92 67 gr13 35560 580 389 gr13 2217 92 62 gr14 36048 590 389 gr14 2185 92 61 gr15 25410 577 389 gr15 1703 93 63 gr16 31327 588 389 gr16 2058 94 62 gr17 27956 560 389 gr17 1897 94 65 gr18 28614 562 389 gr18 1830 92 64 gr2 91673 800 501 gr2 1804 93 58 gr3 78168 788 498 gr3 1289 93 59 gr4 42145 362 500 gr4 1626 91 125 gr5 39259 333 386 gr5 2656 93 108 gr6 124036 1015 384 gr6 1230 92 35 gr7 36217 607 383 gr7 847 94 59 gr8 74453 645 386 gr8 1658 94 56 gr9 67197 669 387 gr9 1322 93 54 NEUCOM 11019 S0925-2312(08)00105-7 10.1016/j.neucom.2007.09.017 Elsevier B.V. Fig. 1 Illustration of NMF and sNMF. (a) Scatterplot of two random nonnegative sources used to constitute the rows of the matrix S . (b) Scatterplot of the mixtures X obtained by multiplying S by a nonnegative 2 × 2 mixing matrix A . Note that all data points remain in the first quadrant. (c) One of the infinitely many possible NMF factorizations of X . Requiring only nonnegativity of A and S is insufficient to solve the BSS problem uniquely up to scaling and permutation indeterminacies. (d) Scatterplot of noisy mixtures X noise which were generated by adding random Gaussian noise to X . (e) Scatterplot of noisy sources S noise = A - 1 X noise . The Gaussian noise added to X leads to negative entries in S noise even if the original mixing matrix can be recovered and generally exacerbates the detection of nil entries. Fig. 2 Top: the original source signals s 1 and s 2 . Bottom: the pseudo source signals s 1 pseu and s 2 pseu . Even if the number of zero elements in s 2 pseu is lower than in the original source s 2 , the sparseness measure σ assigns to it a higher value than to the original source. Fig. 3 Flowchart of the regional parallelization scheme used in the GA of sNMF. The actions of the individual slave processes are highlighted in gray, those of the master process in white. Fig. 4 The scatterplots of the original observation matrix (upper left) as well as of the noisy matrices X noise { i } , i = 1 , 2 , 3 , for which sNMF can recover the sources. Fig. 5 The first row of the matrix X (top) and X noise { 3 } (bottom). Fig. 6 Top: the noisy recovered sources S ^ { 3 } as used in the computation of CC 1 . Middle: the denoised sources S ^ { 3 } as used in the computation of CC 2 . Bottom: the original sources. Fig. 7 Top: CTEs between original and estimated mixing matrix for the different noise levels of the observations. Bottom: the average correlation coefficients between the original and the recovered sources for both sNMF and fastICA. For SNRs larger than 18dB sNMF leads to better results than fastICA. For smaller SNRs fastICA is superior but also fails to recover source s 1 (cf. Table 5). Fig. 8 Top: the original sources s i . Note that s 3 was obtained from s 2 by adding a linear function. Bottom: the rows x i of the mixture matrix X as provided to the algorithms. Fig. 9 Top: the estimates s i sparse of the sources as obtained by sNMF. Bottom: the estimates s i fICA of the sources as obtained by the fastICA algorithm. Note that fastICA fails to recover the third source. Fig. 10 Results obtained from fastICA. Top: CTEs between the original mixing matrix and the estimated mixing matrices. Bottom: correlation coefficients between the original sources and the estimated sources. fastICA leads to unsatisfying recoveries of the sources and the mixing matrix (i.e. correlation coefficient between second original and recovered source < 0.95 , CTE > 1 ) if the correlation coefficient between the first and the second original source is larger than 0.3. Fig. 11 Illustration of geometric NMF. Three random nonnegative sources, each consisting of 1000 datapoints, were generated and mixed by a random nonnegative 3 × 3 mixing matrix. The source matrix contained scalar multitudes of the three unit vectors of R 3 . Top: scatter plot of the mixtures (gray dots). The black lines indicate the cone spanned by the columns of the mixing matrix A . The black points are the mixtures projected onto the standard simplex. Bottom: the observations X projected onto the standard simplex (i.e. the black dots of the top figure seen under a different angle). The edges of the obtained polytope (encircled by black diamonds) are those points of X which contain scalar multitudes of the columns of the mixing matrix A . They can easily be detected by convex hull algorithms like Qhull [2,5]. Fig. 12 Scatterplot and polytope obtained if S does not contain M unit vectors spanning R M and if all nonzero elements of S are larger than 0.65. Top: scatterplot of the observations X (gray dots) and their projections onto the standard simplex (black dots). Bottom: blow-up of the datapoints projected onto the standard simplex (i.e. the black points in the figure at the top). The triangular GHI corresponds to the projection of the cone defined by the columns of A onto the standard simplex. The edges of this triangle are empty as no nonzero datapoints smaller than 0.65 appeared in the columns of S and only maximally one element per column was zero. Hence, all datapoints reside within the polytope ABCDEF and the data could not only be confined by the cone defined by the columns of the original mixing matrix A , but also by another matrix with columns related with the points JKL. Grassmann clustering-based NMF spuriously detects the latter matrix while sNMF correctly identifies the original mixing matrix A . Fig. 13 The first four of the eight PXE data sets. Abscissa: genes. Ordinate: measured mRNA levels in arbitrary units. Fig. 14 The second four of the eight PXE data sets. Abscissa: genes. Ordinate: measured mRNA levels in arbitrary units. Fig. 15 The first four sources as obtained from sNMF. Abscissa: genes. Ordinate: mRNA levels in arbitrary units. Fig. 16 The second four sources as obtained from sNMF. Abscissa: genes. Ordinate: mRNA levels in arbitrary units. Fig. 17 The first four sources obtained when fastICA was applied to the PXE data. Fig. 18 The second four sources obtained when fastICA was applied to the PXE data. Table 1 The sparsenesses σ and σ τ ( τ = 0 ) of the original and the pseudo sources σ ( s ) σ τ ( s ) s 1 0.76 0.90 s 2 0.62 0.80 s 1 pseu 0.76 0.90 s 2 pseu 0.69 0.72 Note that contradicting the fact that the number of zero elements of the pseudo source signal s 2 pseu is lower than that of the original source signal s 2 , the sparseness reaches a higher value for the second pseudo source than for the second original source. Table 2 The SNR ¯ { i } between the matrix X and the matrices X noise { i } SNR ¯ { 1 } SNR ¯ { 2 } SNR ¯ { 3 } SNR ¯ { 4 } 28.4dB 22.3dB 18.9dB 16.2dB Table 3 Parameters used by sNMF when applied to data sets X noise { i } , i = 1 , … , 4 Parameter Value Weighting factor λ 0.01 Parameter defining noise threshold τ cf. Table 4 Number of bits in Gray coding b 60 Number of individuals of overall population N ind 400 Number of subpopulations N pop 8 Number of individuals per subpopulation N subsize 50 Selection pressure μ 1.1 Number of offsprings N off 50 Mutation probability p mut 0.001 Generations between migrations T mig 100 Total number of migrations N mig 20 Fraction of individuals (per subpop.) allowed to migrate μ mig 0.2 Number of elitist individuals N elitist 1 Number of sNMF repetitions N rep 5 Table 4 The values τ { i } as used when the observations X noise { i } were analyzed by sNMF τ { 1 } τ { 2 } τ { 3 } τ { 4 } 0.08 0.19 0.2 0.33 Table 5 Correlation coefficients CC 1 , CC 2 and CC fastICA between the original and the estimated sources s 1 s 2 s 3 X noise { 1 } CC 1 1.00 1.00 1.00 CC 2 1.00 1.00 1.00 CC fastICA 1.00 1.00 1.00 X noise { 2 } CC 1 0.95 0.98 0.99 CC 2 0.98 0.99 0.99 CC fastICA 0.96 0.98 0.99 X noise { 3 } CC 1 0.91 0.95 0.98 CC 2 0.95 0.97 0.98 CC fastICA 0.96 0.94 0.99 X noise { 4 } CC 1 0.75 0.87 0.97 CC 2 0.78 0.90 0.97 CC fastICA 0.89 0.98 0.93 Table 6 Results obtained by sNMF and fastICA algorithm c 1 c 2 c 3 CTE sNMF 1.00 1.00 1.00 0.39 fastICA 1.00 1.00 0.75 5.19 Displayed are the correlation coefficients c i between the ith original source and its corresponding estimate as well as the cross-talking error (CTE) between the estimated and the original mixing matrix. Table 7 Correlation coefficients C i between the i th estimated and the corresponding original source as well as the CTE between the original and the estimated mixing matrix for both Grassmann clustering-based NMF and sNMF C 1 C 2 C 3 CTE Grassmann NMF 0.70 −0.40 0.71 8.24 sNMF 1.00 1.00 1.00 0.62 While sNMF perfectly recovers the sources and the mixing matrix Grassmann clustering-based NMF fails to solve the given BSS problem. Table 8 Sparsenesses σ τ = 0 of the estimated sources Source 1 2 3 4 5 6 7 8 σ τ = 0 0.996 0.971 0.983 0.995 0.984 0.954 0.989 0.985 Table 9 Number of genes related with calcium ion binding (#(cib)) for each of the eight estimated sources obtained by sNMF Source 1 2 3 4 5 6 7 8 # (cib) 0 13 7 0 4 35 9 6 Only genes which are rated exclusively as having a calcium ion binding molecular function in the Gene Ontology [11] database were considered. Most genes related with calcium ion binding are clustered into source 6. Table 10 Number of genes related with calcium ion binding (#(cib)) for each of the 16 clusters formed during the analysis of the PXE data by fastICA Cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #(cib) 9 8 3 2 5 1 0 4 1 22 2 4 12 3 0 1 Only genes which are rated exclusively as having a calcium ion binding molecular function in the Gene Ontology [11] database were considered. Most genes related with calcium ion binding are clustered into source 10. Hybridizing sparse component analysis with genetic algorithms for microarray analysis K. Stadlthanner a ⁎ F.J. Theis a E.W. Lang a A.M. Tomé b C.G. Puntonet c J.M. Górriz c a Institute of Biophysics, Computational Intelligence Group, University of Regensburg, Eupener Str. 114, 52066 Aachen, Germany b Dept. de Electrónica e Telecomunicações/IEETA, Universidade de Aveiro, 3810 Aveiro, Portugal c Dept. Arqitectura y Tecnología de Computadores, Universidad de Granada, 18371 Granada, Spain ⁎ Corresponding author. Nonnegative matrix factorization (NMF) has proven to be a useful tool for the analysis of nonnegative multivariate data. However, it is known not to lead to unique results when applied to blind source separation (BSS) problems. In this paper we present an extension of NMF capable of solving the BSS problem when the underlying sources are sufficiently sparse. In contrast to most well-established BSS methods, the devised algorithm is capable of solving the BSS problem in cases where the underlying sources are not independent or uncorrelated. As the proposed fitness function is discontinuous and possesses many local minima, we use a genetic algorithm for its minimization. Finally, we apply the devised algorithm to real world microarray data. Keywords Sparse nonnegative matrix factorization Blind source separation Gene microarray analysis 1 Introduction Environmental stimuli cause the induction or repression of genes in living cells via a corresponding up- or down-regulation of the amount of messenger RNA (mRNA). Different experimental conditions may thus result in different characteristic expression patterns. The expression of each gene thereby relies on the specific processing of a number of regulatory inputs. Recently, high throughput methods like microarrays have become available and allow to measure whole genome wide gene expression profiles [4]. Intelligent data analysis tools are needed to unveil the information hidden in those microarray data sets [34]. Besides traditional supervised approaches, matrix factorization techniques like principal component analysis (PCA), independent component analysis (ICA), sparse component analysis (SCA) [19,9] or nonnegative matrix factorization (NMF) [22] might be useful to go beyond simple clustering and decompose such data sets into component profiles which might be indicative of underlying biological processes [23,36,7]. Without any hypothesis, such unsupervised approaches might be able to discover novel biological mechanisms and reveal genetic regulatory networks in large data sets when little a priori knowledge is available. Unsupervised analysis methods for microarray data analysis can be divided into three categories: Clustering approaches: They group genes with similar behavior under similar experimental conditions making it possible to analyze data within each group separately. It is supposed that genes within a cluster are functionally related. In general no attempt is made to model the underlying biology. Note that clusters are disjoint but genes may participate in several biological processes. Model-based approaches: They generate a model explaining the interactions among the biological entities participating in gene regulatory networks. Parameters of the model are trained on expression data sets. With complex models not enough data may be available to estimate the parameters. Also algorithms are often of prohibitive complexity and computational load. Matrix decomposition methods: They decompose any given data matrix into a product of two matrices with desired properties. The latter are imposed as constraints on the matrix decomposition procedure. These techniques can equivalently be regarded as expanding the given data vectors into a basis of desired property. Famous among such projection methods are the following: • PCA projects data into a space spanned by mutually orthogonal principal components (PCs). Each PC captures the maximum information, i.e. variance, that is not already present in the previous components. PCA can be used for data compression as well and it is the optimal dimension-reduction technique in the sum-of-squared-error sense. Dimension reduction of expression data can be used for visualization, filtering of noise or simplifying subsequent computations. With microarray data, the PCs are called eigenarrays. • ICA decomposes the data into statistically independent components (ICs). A common application of ICA is in blind source separation (BSS) problems, for example of EEG [29,16], MEG [43] and fMRI [44,21] data. ICA can also be used to reduce noise [42,37,14] or artifacts [38,39] if generated from independent sources. Usually a linear superposition of the underlying unknown source signals is assumed but nonlinear ICA algorithms also exist. With microarray data, ICA extracts expression modes, the ICs, each of which represents a linear influence of a hidden cellular variable. Each retrieved IC is considered to be indicative of a putative biological process, which can be characterized by the functional annotations of genes that are predominant within the IC. Each component thus defines corresponding groups of induced and repressed genes. Samples and genes can be visualized by projecting them to particular expression modes or to their influences, respectively. A projection to expression modes helps to highlight particular biological functions, to reduce noise, and to compress the data in a biologically sensible way. • NMF has been suggested by [22] to provide a decomposition of images into parts which are amenable to a more intuitive interpretation. It decomposes a given data matrix into a product of two strictly nonnegative matrices. When applied to microarrays this constraint seems natural as the raw fluorescence intensities measured can have only nonnegative values, of course. Thus this technique alleviates some of the problems which arise with PCA and ICA both of which yield negative entries in their component expression profiles. The latter have no obvious interpretation. In most applications so far this fact is gently ignored and the negative entries are turned positive by simply considering only the absolute values of the entries to the component profiles. In the following we give a more detailed account of the NMF method. 2 Matrix factorization and BSS In the field of modern data analysis mathematical transforms of the observed data are often used to unveil hidden causes. Especially in situations where different observations of the same process are available matrix factorization techniques have proven useful [24]. Thereby, the M × T observation matrix X is decomposed into a M × N matrix W and a N × T matrix H (1) X = WH . Here, it is assumed that M observations, consisting of T samples, constitute the rows of X and that M ⩽ N . Obviously, the decomposition is highly nonunique and can only be solved uniquely if additional conditions constraining the row vectors of H or the column vectors of W are given. One application of matrix factorization is linear BSS, where the observed signals X can be considered a weighted superposition of N underlying source signals. If the source signals form the rows of the N × T matrix S , then each element a ij of the mixing matrix A represents the weight with which the jth source signal contributes to the ith observed signal. Thus the matrix X of observed signals can be decomposed as (2) X = AS . In BSS now, given only the matrix X , a matrix factorization as in (1) is sought such that A and S are essentially equal to W and H for any given constraints, i.e. they are identical up to inherent scaling and permutation indeterminacies. Note that in the sequel we will confine ourselves to the symmetric BSS problem where the number of source signals to be recovered equals the number of observed signals, i.e. we will present an algorithm for the case M = N . 3 Sparse nonnegative BSS 3.1 Nonnegative matrix factorization A natural constraint to many real world problems is reflected in NMF where the source matrix S , the mixing matrix A as well as the observation matrix X are assumed to be strictly nonnegative. Although NMF has been shown to yield quite intuitive data decompositions in the field of image and text analysis [22], it cannot solve the BSS problem uniquely up to scaling and permutation indeterminacies, hence additional constraints are needed. With microarrays a naturally arising additional constraint seems to be the assumption that the source signals are sparsely represented, i.e. the signal vectors s n , which form the rows of the source signal matrix S , have many close-to-zero entries. Such sparse coding concepts have since long been discussed in the vision community [35,31,20,25] and have already been exploited successfully in NMF-based image analysis methods [17] as well as in other BSS algorithms [26]. Considering gene expression profiles, it may be expected that only a small number of genes are highly up- or down-regulated within a single process if underlying source signals should be indicative of ongoing biological processes in cells. An extension to standard NMF is thus presented in this paper. It is only assumed that the row vectors of S contain several close-to-zero components, while their exact sparseness need not be known. The proposed algorithm, called sparse NMF (sNMF), tries to find a NMF factorization of the data matrix, for which the source matrix S has the largest number of close-to-zero components. As will be shown in several simulations, this approach is capable of solving the BSS problem uniquely up to the usual scaling and permutation indeterminacies inherent in BSS. The basic idea is to estimate the original source matrix S and mixing matrix A , respectively, by determining two estimates A ^ , S ^ , of the nonnegative matrices A and S such that (1) A ^ and S ^ are both nonnegative; (2) the rows of the matrix S ^ are as sparsely encoded as possible, i.e. contain as many close-to-zero components as possible; (3) the reconstruction error F ( A ^ , S ^ ) = ∥ X - A ^ S ^ ∥ 2 of the estimated observations is as small as possible. Fig. 1 illustrates this approach. Fig. 1(a) shows the scatter plot of two random nonnegative sparse sources which constitute the rows of the matrix S . Obviously, all points in the scatterplot lie in the first quadrant of the coordinate system or, in other words, reside within a conic hull with cone lines defined by those columns of S which contain at least one null entry. By left multiplying S by a nonnegative mixing matrix A , the mixture matrix X is obtained whose scatterplot is depicted in Fig. 1(b). The points in this scatterplot are now contained in a conic hull whose cone lines are defined by the images of those points which were lying on the axis of the coordinate system in Fig. 1(a). The mixture matrix X can now be decomposed into two nonnegative matrices W and H by means of NMF. However, this decomposition is not unique, i.e. apart from the original mixing and source matrix an infinite number of matrix pairs W and H exists. They are also nonnegative and perfectly factorize X but which do not equal the original source and mixing matrix due to scaling and permutation indeterminacies. As illustrated in Fig. 1(c) the conic hulls of such matrices H also reside in the first quadrant but their cone lines do not coincide with the axes of the coordinate system. Hence, our approach to recover the original mixing and source matrix (except for scaling and permutation indeterminacies) is to select among all possible nonnegative factorizations of X , the one for which the cone lines of H coincide with the axes of the coordinate system or for which H contains as many null entries as possible. However, data from real life experiments are always corrupted by noise. Hence, only an approximate factorization of X is usually feasible. Furthermore, additional noise also blurs the null entries in the sources (see Fig. 1(d) and (e)) such that they may even contain small negative elements. To cope with this problem, close-to-zero elements have to be set to zero when the sparseness of the estimated sources is determined. To solve the sNMF problem algorithmically, we propose to estimate two nonnegative matrices A ^ and S ^ which minimize the following cost function: (3) E ( A ^ , S ^ ) = 1 2 M ∥ n ( X ) - n ( A ^ S ^ ) ∥ 2 - λ M ∑ m = 1 M σ τ ( s ^ m ) , where the function n ( .. ) is used to normalize the row vectors of X and A ^ S ^ , i.e. (4) n ( X ) = X ^ such that x ^ ij = x ij ∑ k = 1 T x ik 2 . Thus it holds that (5) 0 ⩽ 1 2 M ∥ n ( X ) - n ( A ^ S ^ ) ∥ 2 ⩽ 1 . In these equations σ τ denotes an appropriate sparseness measure given below, λ > 0 represents a Lagrange weighting factor, and s ^ m denotes the mth row of the matrix S ^ . Recall that we assume A to be a full rank square matrix throughout this paper. Hence, S ^ can be determined straightforwardly as (6) S ^ = A ^ - 1 X once a good estimate A ^ of the original mixing matrix A has been obtained. Eventually, this means that the above optimization problem depends only on A ^ . 3.2 Sparseness measure For the sparse nonnegative BSS problem at hand, we define the sparseness σ τ of a row vector s as the ratio of the number of its zero elements and the total number of components. However, measurements in real world experiments are always corrupted by noise. Thus small nonzero entries of a measurement vector should be set to zero as well. Hence, a nonnegative threshold is used to define the minimum value which avoids any component of s n being zero. This threshold is defined as a fraction τ ∈ [ 0 , 1 ] of the maximal component s n max of s n . Formally this leads to the following sparseness measure σ τ : (7) 0 ⩽ σ τ ( s m ) = number of elements of s m ⩽ τ · s m max number of elements of s m ⩽ 1 , where s m max is the maximum value of s m and τ ∈ [ 0 , 1 ] . Thus, the regularizing sparseness term, i.e. the second summand in the above given cost function, has values also in the range [ 0 , 1 ] only. The Lagrange parameter λ is used to balance the factorization of the data matrix with the sparseness requirement. As both terms in the cost function are normalized, results should be robust against varying sizes of X and S as soon as an appropriate λ has been obtained. In the literature various measures σ ( s ) have been proposed to estimate the sparseness of a signal without counting its nil entries explicitly. These measures are usually based on L n norm considerations and are computationally less demanding than the sparseness measure in (7). A prominent example of such a measure is the normalized ratio of the L 1 and L 2 norm of an T-dimensional vector s [17] according to (8) σ ( s ) = T - ∑ i = 1 T | s i | / ∑ i = 1 T s i 2 T - 1 , where s i is the ith component of s . As will be shown in the following, however, such estimates of the sparseness of a signal are inadequate for the BSS task at hand. To illustrate this point, consider the nonnegative BSS model (9) X = AS , X , S ∈ R M × T , A ∈ R M × M in which A , S and X are all nonnegative and the rows of S contain many close-to-zero components. In sNMF, given only X , the matrices A and S need to be recovered by searching for two nonnegative matrices A ^ and S ^ such that X = A ^ D - 1 P - 1 PD S ^ and the rows of S ^ are encoded as sparse as possible. Thereby the matrices P and D represent a permutation matrix and a diagonal scaling matrix, respectively. Obviously, such an approach can only succeed if among all the possible nonnegative factorizations of X , i.e. (10) { ( A ^ , S ^ ) , A ^ ∈ R M × M , S ^ ∈ R M × T , A ^ , S ^ nonnegative | A ^ S ^ = X } , no pair of matrices ( A ^ , S ^ ) ≠ ( A , S ) exists with the rows of S ^ being more sparse than the rows of S . But this seems to happen occasionally if the sparseness of the matrix S ^ is only estimated by the sparseness measure σ instead of being determined precisely by means of σ τ . To provide an illustrative example, two nonnegative random sources s 1 and s 2 consisting of 1000 data points were generated with 90% of the components of the first source signal and 80% of the components of the second source signal being equal to zero. These two sources were normalized and then used to constitute the rows of the source matrix S (cf. Fig. 2 ). The matrix of observations X was obtained by mixing the sources with the following mixing matrix: (11) A = 5 1 6 1 . Note that the first source signal is dominating in both mixtures according to x 1 , i = a 11 s 1 , i + a 12 s 2 , i , (12) x 2 , i = a 21 s 1 , i + a 22 s 2 , i . Thus the following alternative factorization of the observation matrix X is feasible. First, the original mixing matrix A can be replaced by the pseudo mixing matrix (13) A pseu = 0 1 1 1 . Correspondingly, the original source matrix S then has to be replaced by the matrix S pseu , the row vectors of which constitute the following pseudo source signals (see Fig. 2): s 1 pseu = s 1 , (14) s 2 pseu = 5 s 1 + s 2 . Obviously, these matrices also factorize X , i.e. X = A pseu S pseu still holds. But despite the fact that the number of zero elements in the second pseudo source signal s 2 pseu is about 8% lower than that of the original source signal s 2 , its sparseness estimate σ ( s 2 pseu ) is higher than that of the original source σ ( s 2 ) (cf. Table 1 ). Hence, a smaller value of the cost function is obtained with the matrices A and S than with the matrices A pseu and S pseu if the sparseness measure σ τ is used in (7). Accordingly, the matrices A and S would be recovered correctly up to the usual scaling and pertmutation indeterminacies inherent in the BSS model. In contrast, the higher sparseness estimate σ ( s 2 pseu ) of the second pseudo source signal compared to the original source signal s 2 leads to a smaller value of the cost function with the matrices A pseu and S pseu than with the matrices A and S . Accordingly, a sparse BSS algorithm which only estimates the sparseness by means of σ would fail to recover the original source signal matrix S and mixing matrix A , respectively, except for scaling and pertmutation indeterminacies. Note that the same conclusion holds with other L n -norm-based sparsity criteria. 4 Genetic algorithm-based optimization 4.1 Fitness function As mentioned above, the cost or fitness function defined in Eq. (3) is discontinuous such that it cannot by optimized by techniques based on gradient descent. Furthermore, it possesses many local minima which suggests to use a genetic algorithm (GA) [30,8] for its minimization. GAs are stochastic global search and optimization methods inspired by natural biological evolution. The core of a GA is a population of potential solutions, named individuals, to a given optimization problem as well as a set of operators borrowed from natural genetics. At each generation of a GA, a new set of approximations is created by the process of selecting individuals according to their level of fitness in the problem domain. Their reproduction is guided by the genetically motivated operators. This process leads to the evolution of individuals within the population which better solve the optimization problem than the individuals from which they were created. Finally, this process should lead to an optimal solution of the optimization problem even if many suboptimal solutions exist, i.e. if the target function to be optimized has many local minima. For the minimization of the fitness function in Eq. (3) the M 2 elements of the solution matrix A ^ have to be determined. Taking advantage of the scaling indeterminacy inherent in the linear mixture model (2) we may assume that the columns of the original mixing matrix A are normalized such that the diagonal elements of A are a ii = 1 ∀ i = 1 , … , M . Accordingly, the N ind individuals used in the GA to minimize (3) consists M 2 - M parameters. However, these parameters (also called genes in the context of GAs) were not stored directly as real valued numbers but were transformed to binary strings of length b using Gray coding [13]. Hence, each individual consisted of b ( M 2 - M ) binary numbers. As the original mixing matrix is known to have only nonnegative entries, it seems self-evident to confine the genes to be nonnegative, too. However, we allow the genes to be negative throughout the optimization procedure as we have observed in our experiments that otherwise the GA often fails to find the global minimum of the fitness function. In every generation of the GA, the fitness of each individual for the optimization task has to be computed in order to determine the number of offsprings that will be allowed to produce. These function values are not used directly as fitness values as otherwise the fittest individuals often produce too many offsprings such that the needed diversity in the population is lost and the algorithm converges prematurely to a suboptimal solution. Hence, we use a linear scaling procedure to transform fitness function values to fitness values. In order to compute the fitness function values for every individual, a matrix A ^ - is generated with its off elements consisting of the genes as stored in the individual and with unit diagonal elements. As during the evaluation of the target function these matrices have to be inverted, the matrices which are singular to machine precision are replaced by nonsingular nonnegative random matrices (also with ones on their diagonal). Accordingly, the corresponding individuals in the population are adjusted. Next, the matrices A ^ = { a mn } 1 ⩽ m ⩽ M , 1 ⩽ n ⩽ M and S ^ = { s mt } 1 ⩽ m ⩽ M , 1 ⩽ t ⩽ T are needed in order to evaluate the fitness function (3). For this purpose, the inverse W - ^ of A ^ - is computed and the matrices S ^ and A ^ are then obtained by setting the negative elements of the matrices S ^ - = { s mt ,- } 1 ⩽ m ⩽ M , 1 ⩽ t ⩽ T , S ^ - = W ^ - X and A ^ - = { a mn ,- } 1 ⩽ m ⩽ M , 1 ⩽ n ⩽ M , respectively, to zero: a ^ mn = H ( a ^ mn ,- ) a mn ,- , (15) s ^ mt = H ( s ^ mt ,- ) s mt ,- . Here, H denotes the Heaviside step function [3] (16) H ( x ) = 0 , x < 0 , 1 , x ⩾ 0 . After inserting the matrices S ^ and A ^ into (3) the resulting fitness function value is assigned to the corresponding individual. The individuals are then arranged in an ascending order according to their fitness function values and their fitness values F ( p ( i ) ) , i = 1 , … , N ind , are determined by (17) F ( p ( i ) ) = 2 - μ + 2 ( μ - 1 ) p ( i ) - 1 N ind - 1 , where p ( i ) is the position of individual i in the ordered population. The scalar parameter μ , which is usually chosen to be between 1.1 and 2.0, denotes the selective pressure towards the fittest individuals. 4.2 Genetic operators Stochastic universal sampling (SUS) [30] is used to determine the absolute number of offsprings an individual may produce. Thereby, an arc R i of length F ( p ( i ) ) is assigned to the ith individual, i = 1 , … , N ind , on a circle of circumference C = ∑ i = 1 N ind F ( x ( i ) ) . Starting from a randomly selected position, 2 N off marker points are allocated on the circle, whereas the distance between two consecutive marker points is C / 2 N off and N off is the total number of offsprings to be created. The ith individual may then produce as many offsprings as there are marker points in its corresponding arc R i on the circle. The offsprings are created in a two step procedure. In the first step, two individuals, which are eligible for reproduction according to the SUS criterion, are chosen at random and are used to create a new individual by uniform crossover, i.e. each bit of the new individual is created by copying, each time with a probability of 50%, the corresponding bit of the first or the second parent individual. In the second step, called mutation, the actual offsprings are obtained by flipping with a low probability p mut each bit of the new individuals. The role of mutation is often seen as providing a guarantee that the probability of searching any given parameter set will never be zero and acting as a safety net to recover good genetic material that may be lost through the action of selection and crossover. The last step of the GA applied to each generation is the replacement of the parent individuals by their offsprings. We use an elitist reinsertion scheme meaning that a fixed number N elitist of the fittest individuals is deterministically allowed to propagate through successive generations. Hence, only the N ind - N elitist less fittest parent individuals are replaced by their fittest offsprings which ensures that the best solution found so far remains in the population. In order to keep the algorithm from converging prematurely we make use of the concept of multiple populations. Thereby, a number N pop of populations, each consisting of N subsize individuals, are evolving independently in parallel and are only allowed to exchange their fittest individuals after every T mig th generation. Hence, as long as not all populations have converged to the same solution they will regain some diversity after every T mig th iteration step. We use the complete net structure scheme for the exchange of individuals which means that every population is exchanging a fraction μ mig of its fittest individuals with all other populations. 4.3 Parallelization A general problem of the proposed sNMF-GA algorithm is its tendency to converge prematurely to suboptimal solutions. This problem can be overcome to a large extent, however, if the regional parallelization model is used, i.e. if instead of one large population several smaller subpopulations are used; it only exchanges individuals from time to time. Another reason to parallelize the GA in sNMF-GA is that the evaluation of the fitness function may become very time consuming. The problem arises as in every evaluation of the fitness function the sparseness of the provided matrix S ^ has to be determined. This is done by comparing each entry of S ^ with a small, user defined threshold (cf. (7)), i.e. if S ^ is an M × T matrix, MT “if” statements have to be evaluated. As modern microarray chips are capable of detecting the expression of 50000 genes in parallel several hundreds of thousands of “if” comparisons need to be carried out per individual in real life applications. Hence, the implementation of the sNMF-GA algorithm is designed such that it can be run on several computers in parallel. As the implementation of the sNMF-GA algorithm is fully written in the C programming language, the routines of the MPICH2 [18] implementation of the Message Passing Interface standard [1] for the communication between computers can be used advantageously. As mentioned above, the regional scheme (see Fig. 3 ) is used for the parallelization of the GA. In order to determine the emigrants of each subpopulation, SUS is used for ranking, i.e. fitter emigrants have a higher chance to participate in the migration process than poorly performing ones. For the migration process the complete net structure scheme is used and the immigrants are reinserted into the individual subpopulation by replacing randomly selected individuals. As the migrants contain only M 2 - M (Gray encoded) numbers corresponding to the number of off-elements of the mixing matrix, the time needed for the data transfer between the master and the slave processes is negligible compared with the time the individual slaves need for the T mig generations of their GAs. Hence, the execution time of the GA can be reduced by a factor close to 1 / N nod if a cluster consisting of N nod nodes is available. 4.4 Algorithm repetitions Despite the use of the mutation operator and multiple populations, the algorithm failed in many experiments to recover the source and mixing matrix after its first run. In order to keep the computational load of the algorithm bearable, this problem could not be overcome by simply increasing the number N ind of individuals and N pop of populations to arbitrarily large values. But satisfying results could still be obtained by applying the algorithm repeatedly. This approach reflects recent efforts towards more robust NMF algorithms using multilayer techniques [10]. The algorithm is normally provided with the observation matrix X in its first run which is then decomposed into first estimates of the source matrix S ^ ( 1 ) and the mixing matrix A ^ ( 1 ) , i.e. X ≈ A ^ ( 1 ) S ^ ( 1 ) . In order to make use of the suboptimal results already achieved, during the next run the matrix S ^ ( 1 ) is provided to the algorithm instead of the matrix X . The matrix S ^ ( 1 ) is then factorized into the matrices A ^ ( 2 ) and S ^ ( 2 ) , which means that the matrix X can now be factorized as X ≈ A ^ ( 1 ) A ^ ( 2 ) S ^ ( 2 ) . This procedure is repeated N rep times until the newly determined mixing matrix A ( N rep ) differs only marginally from the identity matrix. With this procedure the final estimates of the mixing matrix A ^ and of the source matrix S ^ are determined as (18) A ^ = ∏ j = 1 N rep A ^ ( j ) and (19) S ^ = S ^ ( N rep ) , respectively, as the matrix X can be factorized as X = ∏ j = 1 N rep A ^ ( j ) S ^ ( N rep ) . 5 sNMF simulation studies using toy data 5.1 Robustness to noise The data obtained from microarray experiments are often corrupted by noise. Hence, the sensitivity to noise of the proposed sNMF algorithm is investigated in this section. Furthermore, the results are compared with those obtained by fastICA as this algorithm is most often used when microarray data are analyzed by means of BSS. For the investigation three nonnegative random sources s n with sparsenesses σ τ = 0 of 0.9, 0.8 and 0.7, respectively, were generated and were used to constitute the source matrix S . In order to generate a matrix X of observations, this matrix of source signals was multiplied by the following mixing matrix: (20) A = 0.253490 0.706450 0.518140 0.644120 0.442550 0.227250 0.074437 0.267830 0.878450 . Additionally, the matrix N ∈ R 3 × 1000 was created whose elements were randomly selected from a normal distribution. By means of this matrix four noisy versions X noise ,- { i } of X were generated as follows: (21) X noise ,- { i } = X + η { i } N , i = 1 , … , 4 whereas the weighting factors η { i } were given by (22) η { i } = 0.01 i . As the data obtained from microarray experiments are strictly nonnegative, the matrices X noise { i } were generated from the matrices X noise ,- { i } by multiplying the negative entries of X noise ,- { i } by - 1 (see Figs. 4 and 5 ). The average signal to noise ratios ( SNR ¯ { i } ) between the rows of the noisy matrices X noise { i } and the original matrix X of observations are collected in Table 2 . These observations X noise { i } were fed into the sNMF algorithm whereas the parameters as shown in Table 3 were used. The majority of these parameters are universal in the sense that they need not be adapted when the sNMF algorithm is applied to different data sets. An exception is the number of subpopulations N pop which depends on the number of elements M 2 of the mixing matrix A to be recovered. As a rule of thumb we suggest the following choice of N pop : (23) N pop ( M ) = 1 for M = 2 , 8 9 M 2 for M > 2 whereas ⌈ x ⌉ denotes the ceiling of x. Here the factor 8 9 originates from our observation that if A contains M 2 = 9 elements at least eight subpopulations are needed to find the global minimum of the target function (3) reliably. Furthermore, both the number of migrations N mig and the number of repetitions N rep need to be adjusted according to the data set to be analyzed. Here, N mig should be chosen sufficiently large such at the end of a single sNMF run all subpopulations are dominated by the same fittest individual. On the other hand N rep should be set such that at least during the last three runs of the sNMF algorithm no new best solution is found (Table 4 ). Eventually, the threshold τ in the sparseness measure σ τ has to be chosen. For the simulation at hand different values τ { i } , i = 1 , 2 , … , 4 , were used depending on the noise level with which the data sets X { i } were corrupted. These τ { i } 's were determined as follows: first, the matrices S ^ { i } , i = 1 , … , 4 , were computed by multiplying the inverse of the original mixing matrix A by the matrices X noise { i } , i.e. (24) S ^ { i } = A - 1 X noise { i } . Next, the difference Δ S { i } between the original source matrix and S ^ { i } was computed: (25) Δ S { i } = S ^ { i } - S . This matrix Δ S { i } contains the noise which appears in the sources if the noisy mixture X noise { i } is fed into sNMF instead of the original mixture X . Furthermore, the maximum element Δ s max { i } of Δ S { i } was determined. If this element was found in the qth row of Δ S { i } the parameter τ { i } was set to (26) τ { i } = Δ s max { i } s ^ q , max { i } , where s ^ q , max { i } is the maximum value of the qth row of S ^ { i } . Thus, τ { i } should be sufficiently large such that the entries in S ^ { i } which correspond to null entries in S are correctly detected by the sparseness measure σ τ { i } . After the τ { i } 's were determined the matrices X noise { i } were fed into sNMF and the source matrix S ^ { i } as well as the mixing matrix A ^ { i } were estimated. The quality of these estimates was evaluated using the following criteria: (1) The cross talking error (CTE) between the original and the estimated mixing matrix was determined (cf. Fig. 7) according to [40] (27) CTE = ∑ i = 1 n ∑ j = 1 n ∥ p ij ∥ max k ∥ p ik ∥ - 1 + ∑ j = 1 n ∑ i = 1 n ∥ p ij ∥ max k ∥ p kj ∥ - 1 , where P = ( p ij ) = A ^ - 1 A and A ^ the calculated estimate of A . (2) The correlation coefficients CC 1 between the rows of the matrix S ^ { i } and the original source matrix S were computed (cf. second row in Table 5 and Fig. 7). (3) In sNMF, elements of the kth row, k = 1 , 2 , 3 , of the estimated source matrix S ^ { i } which are smaller than τ { i } s k , max { i } are treated as null elements (here s k , max { i } denotes the maximum value of the kth row of the matrix S ^ { i } ). Hence, these elements were set to zero in S ^ { i } and the correlation coefficients CC 2 between the so-obtained and the original source matrix S were determined (cf. third row in Table 5 and Fig. 7). (4) For comparison, the data sets X noise { i } were also analyzed by fastICA. As before the results were evaluated by computing the cross-talk error ( CTE fastICA ) between the original and the estimated mixing matrix. Furthermore, the correlation coefficients ( CC fastICA ) between the original and the estimated source matrix have been determined (last row of Table 5, see also Fig. 7). As can be seen in Fig. 7 the original mixing matrix A was well recovered by sNMF if the observation matrix was corrupted by low levels of noise (e.g. for data sets X noise { 1 } and X noise { 2 } ). For higher levels, however, A could hardly be recovered. This is in contrast to fastICA, which even recovered A well if higher levels of noise were present. Still, sNMF lead to better estimates of the recovered sources than fastICA if it was applied to the data sets X noise { i } , i = 1 , 2 , 3 (see third row of Table 5 and Fig. 6 ). At first glance, this may seem surprising as the recovery of the sources by both sNMF and fastICA is solely based on the estimated mixing matrix. Hence, a poorer estimation of A by sNMF should also lead to poorer recoveries of the sources. However, recall that in sNMF any negative entries of the estimated sources are deliberately set to zero (cf. (15)). Furthermore note, that such negative entries only appear if the original mixing matrix A ^ was not recovered perfectly since the original sources are nonnegative. Thus, setting the negative entries in S ^ - to zero compensates (at least to some extent) the deficiencies of the estimation of A and leads to the comparatively better recovered sources (Figs. 7 and 8 ). For data set X noise { 4 } both sNMF and fastICA have problems in recovering the source s 1 . In summary these simulations show that sNMF is capable of recovering the original sources and to some extent also the original mixing matrices if the observations are corrupted by noise of moderate level. For such noise levels, sNMF better recovers the sources than the fastICA. This is impressive as sNMF is partly based on counting null entries which easily get blurred in the presence of noise. 5.2 Recovery of correlated sources In this section it is shown that the proposed method is capable of solving the BSS problem even if the underlying sources are correlated. This case is particularly interesting as the overwhelming majority of BSS algorithms only work if the underlying sources are uncorrelated or even statistically independent. For the simulation, three sources s i , i = 1 , 2 , 3 were generated as follows. The first and the second sources were generated as nonnegative random vectors where 90% and 80%, respectively, of the elements were randomly set to zero. The third source was generated from the second source by adding a linear function, i.e. (28) s 3 ( n ) = s 2 ( n ) + 0.001 ( n - 1 ) , where s i ( n ) denotes the nth element of the source s i and n = 1 , … , 1000 . This procedure leads to a nonvanishing correlation coefficient of c = 0.65 between the second and the third sources, while the sources s 1 and s 2 as well as s 1 and s 3 were uncorrelated. As before, these sources were used to constitute the source matrix S . The observation matrix X was generated by multiplying the source matrix S with the following mixing matrix: (29) A = 0.4554 0.5833 0.3739 0.8916 0.3988 0.8736 0.9042 0.0604 0.1326 . With sNMF the mixing matrix as well as the sources were recovered almost perfectly as can be seen in Table 6 and Fig. 9 . In contrast, such a perfect recovery seems to be impossible by ICA-based BSS. To show this, fastICA was used again to recover the sources s i , i = 1 , 2 , 3 , and the mixing matrix A . This algorithm also succeeded in recovering the sources s 1 and s 2 almost perfectly, but it failed to recover the third source s 3 (cf. Fig. 9). Accordingly, the CTE between the estimated and the original mixing matrix is more than five times higher than the CTE achieved with the sparse nonnegative BSS approach (cf. Table 6). To see how easily fastICA fails if the underlying sources are correlated a second simulation was carried out. For this simulation three nonnegative random sources with sparsenesses σ τ = 0 of 0.9, 0.8 and 0.7, respectively, were generated. In order to achieve correlation a certain number of randomly chosen elements of the second source were replaced by the corresponding elements of the first source. By means of this procedure nine variants s 2 { i } , i = 1 , … , 9 , of the second source were generated which had correlation coefficients with the first source of (30) CC { i } ( s 1 , s 2 { i } ) = 0.1 · ( i - 1 ) , i = 1 , … , 9 . Each of these source matrices was multiplied by the mixing matrix (31) A = 0.2535 0.7064 0.5181 0.6441 0.4426 0.2273 0.0744 0.2678 0.8785 such that nine observation matrices X { i } were created. As shown in Fig. 10 fastICA fails to recover the source and the mixing matrix satisfactorily if the correlation coefficient between the first and the second original source is larger than 0.3. For correlation coefficients larger than 0.7 the second source is not estimated at all, i.e. all three estimated sources are higher correlated with the first and the third original source than with the second original source. In contrast, sNMF successfully recovered the source matrices S { i } and the mixing matrix A regardless of the correlation between the first and the second original source. The correlation coefficients between the original and the estimated sources were always close to 1.00 and the CTE between the original and estimated mixing matrix was about 0.04. Hence, sNMF is capable of solving BSS problems in cases where other well-established BSS methods like fastICA principally fail. 5.3 Comparison with geometric methods In the literature a multitude of different algorithms can be found which try to solve the BSS problem by exploring the geometry of the space of observations [12]. Most of these methods are based on uncorrelatedness or independence assumptions on the sources [32] and often require that the probability function p of the sources is symmetrical, i.e. p ( y ) = p ( - y ) for y in R [33,41]. Obviously, the latter condition can never be met with NMF resulting in a failure of these algorithms in this context. Furthermore, as shown in Section 5.2 sNMF can also solve the BSS problem if the underlying sources are correlated. An intuitive approach to solve the nonnegative BSS problem geometrically is the following. First, note that as A and S are nonnegative all observations lie within a cone in the scatterplot of X whose cone lines are defined by the columns of A . If furthermore S contains M columns q 1 , … , q M consisting of the unit vectors (or scalar multiples of them) spanning R M then the corresponding columns of X lie on the cone lines, or, in other words, contain scalar multiples of the columns of A (see Fig. 11 top). Hence, A can be recovered if the columns q 1 , … , q M of X can be detected. This detection is achieved by projecting X onto the standard simplex where the columns q 1 , … , q M of X constitute the edges of the resulting polytope. These edges can be determined automatically by computing the convex hull of projected data by means of, e.g. the QHull algorithm [6,2] (see Fig. 11 bottom). Once the estimate A ^ of the mixing matrix has been determined the corresponding source matrix S ^ can easily be computed as (32) S ^ = A ^ - 1 X . However, problems arise when the data are corrupted by noise as in this case the convex hull is defined by more than just M edges and the original edges are blurred. To cope with this problem, Gruber et al. suggest to use Grassmann clustering [15] to determine the correct convex hull. In this approach, several subsets of the observations are formed at random and for each of these sets the convex hull is computed. The obtained facets of these convex hulls are then clustered in a projective space by an algorithm similar to k-means clustering [28]. Finally, the intersection points of the resulting centeroids are determined and are then used to estimate the mixing matrix A . Obviously, further difficulties appear if the columns of the source matrix S do not contain the M unit vectors or any vectors close to them. In such a case, the columns of X do not fully occupy the cone defined by A in the scatter plot of the observations. In particular, the areas close to the edges of the cone are not filled with any data points such that the polytope, which is observed after projection onto the standard simplex, does no longer contain the edges related with the columns of the mixing matrix A (points G, H and I in Fig. 12 ). Accordingly, convex hull algorithms like Qhull can no longer be used to recover the mixing matrix A . Grassmann clustering-based NMF may help in such scenarios, however, reasonable results can only be expected if the polytope on the simplex does at least to some extent adumbrate the edges related with the columns of A (i.e. the points G, H and I in Fig. 12). In other cases, the sparsity of the sources can be exploited to recover the mixing matrix A . To demonstrate this, three nonnegative random sources consisting of 1290 data points were generated which had sparsenesses of 0.107 0.109 and 0.114, respectively. The sources were used to constitute the rows of the source matrix S . The sources were created such that in each column of S only one null entry appeared. Furthermore, all nonvanishing elements of S were in the range [ 0.65 , 1.00 ] . These sources were mixed by the random nonnegative mixing matrix (33) A = 0.6109 0.7904 0.1368 0.4038 0.1487 0.8514 0.8587 0.5075 0.7062 . The scatterplot of the resulting mixtures X as well as the polytope obtained after projecting onto the standard simplex are depicted in Fig. 12. As can be seen, the edges G, H and I corresponding to the columns of A are clearly cut off and all projected data points reside within the polytope ABCDEF. Inspection of this polytope without any knowledge about the original source and mixing matrix allows two conclusions: first, the points G, H, and I correspond to the original mixing matrix but the edges of the triangle GHI do not contain any data points because of the limited range of the sources. Second, the points J, K, L also seem to be related with the columns of the original mixing matrix and the edges of the triangle JKL are again cut off because of the particular structure of the sources. As the line segments [ DE ] , [ CB ] and [ FA ] , which lie on the triangle GHI are shorter than the line segments [ AB ] , [ CD ] and [ EF ] on JKL, Grassmann clustering erroneously identifies the points J, K and L as those points which belong to the original source matrix. In contrast, sNMF manages to recover the original mixing matrix by taking advantage of the sparseness of the sources. Again, all parameters are chosen as given in Table 3 except the Lagrange parameter which has to be set to λ = 0.0002 . Most notably, both matrices, the one defined by the points J, K, and L as well as the other defined by the points G, H and I, lead to a nonnegative factorization of X , but if the points G, H and I are used to reconstruct A the corresponding sources are more sparse. Hence, sNMF leads to a perfect recovery of the sources and the mixing matrix while the estimates obtained by Grassmann clustering hardly resemble the original data (see Table 7 ). 6 sNMF simulation studies using microarray data So far the performance of sNMF has only been investigated by means of artificial data. In contrast, this section deals with the sNMF analysis of microarray data. As before, the results will be compared with those achieved by fastICA. The data sets to be analyzed were recorded during an investigation of Pseudo-Xanthoma Elasticum (PXE), an inherited connective tissue disorder characterized by progressive calcification and fragmentation of elastic fibers in the skin, the retina, and the cardiovascular system. During the investigations M = 8 microarray experiments have been carried out. In the first and the second experiment the PXE fibroblasts were incubated in bovine serum albumin (BSA) whereas the incubation time was 3h in the first and 24h in the second experiment. In the third experiment the PXE fibroblasts were incubated for 3h in an environment with a high concentration of the Transcription Growth Factor beta and in the fourth experiment the cells were incubated for 24h in an environment which was rich in Interleukin 1 beta. The same experiments were then repeated with a control group of normal fibroblasts. In order to detect the expression of the individual genes an Affymetrix HG-U133 plus 2.0 microarray chip was used. This in situ chip is capable of detecting 54675 genes in parallel and makes use of the probe pair strategy. Hence, each measured expression value was accompanied by a detection call. If in all experiments the detection call of a particular gene was “absent” the gene was removed from all data sets. After this procedure only 10530 genes remained (see Figs. 13 and 14 ) in each data set. These data sets were used to constitute the 8 × 10 530 observation matrix X which was then decomposed into the matrices A ^ and S ^ by the proposed sNMF algorithm. For the GA the number of sub-populations was increased to N pop = 56 (thus the overall number of individuals N ind increased to 2800), the total number of migrations to N mig = 25 and the number of algorithm repetitions to N rep = 8 . This number of repetitions seemed to be sufficient as it was observed that after the fifth of the N rep = 8 repetitions of the algorithm the resulting matrices A ^ and S ^ did not change any further. For the sparseness measure σ τ several values τ have been tried and it turned out that the best results were obtained if this parameter was set to 0.30 . The values of the remaining parameters of the GA remained as listed in Table 3. Note that as the overall number of individuals was large ( N ind = 2800 ) the algorithm was run in parallel on a cluster of 28 computers and took 5h of computation time. After the eight repetitions the following estimate of the mixing matrix A ^ was obtained: (34) A ˜ = 1.9879 1.5171 1.6557 6.2249 0.8153 0.5777 2.1567 0.4715 2.0143 1.7504 0.9985 4.1203 1.0451 0.6593 1.5290 0.9634 3.5696 0.5377 1.0782 3.4697 0.2301 0.1243 1.0611 0.2914 1.9490 1.4348 0.8186 4.0773 0.5377 0.4795 0.9305 0.4145 0.7278 0.6451 0.5221 2.4486 1.3107 0.3698 0.4669 0.1856 0.9855 0.9942 0.6677 2.9247 1.1428 1.1557 0.6835 0.4150 2.5548 0.9079 2.1846 4.2192 1.0214 0.2973 1.3055 0.6379 1.9814 1.0337 0.8580 3.7670 0.5493 0.7602 0.6348 1.2538 . The corresponding sources are depicted in Figs. 15 and 16 . As the value of τ used by sNMF is rather large the obtained sources are very sparse as can be seen in Table 8 . Hence, the sNMF algorithm uses only the expressions of a small portion of genes to reconstruct the mixing matrix X . Here, it is assumed that these genes are the most important representatives of their corresponding cellular processes. This coincides with the notion often found in genetics which says that the most highly expressed genes are the most typical for a specific cellular process [23]. Thus, the high value of τ was not only used to deal with noise, but it was also used deliberately to obtain sources with only a few nonzero entries, i.e. only few active genes. After the sNMF analysis each row of the matrix S should ideally consist of an expression pattern indicative of a specific biological process. It must be noted, however, that at least 100 of such processes are occurring simultaneously in a biological cell while the number of available observations and hence the number of estimated sources was only eight. Hence, each estimated source is expected to contain signatures from various cellular processes. But despite these highly overcomplete settings the algorithm succeeded in grouping the majority of genes which are related with calcium ion binding (344 in total) and hence with the disease picture of PXE into the sixth estimated source (see Table 9 ). Furthermore, the calcium ion binding related genes in the sixth source seem to be specific for only one biological process as maximally 14% of them could be found in any of the remaining sources. Note that for the assignment of genes to molecular functions the Gene Ontology database [11] was queried. In [27] the same PXE data set was analyzed by means of the fastICA algorithm. In this analysis the following procedure was carried out: first the data matrix X was fed into fastICA whereas the Gaussian nonlinearity was used. As can be seen in Figs. 17 and 18 the ICs extracted had both positive and negative entries. Note that this actually means that the cellular processes would need to produce negative amounts of mRNA. Furthermore, there are no null entries in the sources which would mean that all genes participate at all processes. Hence, in [27] a further postprocessing step was necessary. In this step two clusters were formed for each source. For this purpose the maximum ( s max ) and the minimum expression level ( s min ) found in the source was determined. The first cluster then consisted of all those genes which had expression levels higher than 0.26 s max while the second consisted of the genes with expression levels lower than 0.26 s min . Finally, the molecular function of the genes in each cluster was determined by means of the Gene Ontology [11] database. In this case, at most 22 genes related with calcium ion binding could be grouped into one cluster as can be seen in Table 10 . Furthermore, other clusters (e.g. clusters 1, 2 and 13) also contained significant amounts of genes related with calcium ion binding. sNMF thus achieved a better grouping of calcium ion binding genes than fastICA. These results are rather preliminary, though. Further cooperation with biologists is necessary to investigate if the calcium ion binding genes appearing in the sixth source as obtained by sNMF are indeed related with PXE. 7 Conclusions In this paper we have presented a new approach to solve the BSS problem which is based on a nonnegativity constraint for the observations, the mixing matrix and the sources with an additional sparseness constraint concerning the encoding of the source signals. As the cost function considered has many local minima we have used a GA for its minimization. Further, we have discussed which sparseness measure is eligible for our approach and compared the devised algorithm to ICA and geometric BSS methods. In these comparisons we have demonstrated that our approach is also able to solve the BSS problem when the underlying sources are statistically dependent or correlated. Finally, we have applied the proposed algorithm to analyze real world microarray data. The obtained results suggest that our approach is better suited to analyze microarray data than ICA, however, further investigations in cooperation with biologists are necessary to confirm these preliminary results. Acknowledgments Financial support by Siemens AG, Corporate Technology, Munich (project Biomarker), the DFG (GRK 638: Nonlinearity and Nonequilibrium in Condensed Matter), the DAAD-GRICES Acções Integradas Luso - Alemãs (project GEVD-MP) and the DAAD Acciones Integradas Hispano - Alemanas (project Microarrays) is gratefully acknowledged. The PXE data set was kindly provided by PD Dr. Langmann, Institute of Clinical Chemistry (Director: Prof. Dr. G. Schmitz), University Clinic Regensburg. References [1] The Message Passing Interface (MPI) standard 〈 www.mpi-forum.org 〉 . [2] QHull 〈 〉 . [3] M. Abramowitz I.A. Stegun Handbook of Mathematical Formulas, Graphs and Mathematical Tables ninth ed. 1972 Dover New York [4] P. Baldi W. Hatfield DNA Microarrays and Gene Expression 2002 Cambridge University Press Cambridge [5] C. Barber D. Dobkin H. Huhdanpaa The quickhull algorithm for convex hulls ACM Trans. Math. Software 22 4 1996 469 483 [6] C.B. Barber, D.P. Dopkin, H. Huhdanpaa, The quickhull algorithm for convex hull, Technical Report gcg53, The Geometry Center, University of Minnesota, Minneapolis, 1993. [7] P. Chiapetta M.C. Roubaud B. Torrésani Blind source separation and the analysis of microarray data J. Comput. Biol. 11 2004 1090 1109 [8] A. Chipperfield, P. Fleming, H. Pohlheim, C. Fonseca, Genetic algorithm toolbox, University of Sheffield, 1994. [9] A. Cichocki S.-I. Amari Adaptive Blind Signal and Image Processing 2002 Wiley New York [10] A. Cichocki, R. Zdunek, Multilayer nonnegative matrix factorization using projected gradient approaches, in: ICONIP 2006, Hong Kong, 2006. [11] T.G.O. Consortium Gene ontology: tool for the unification of biology Nat. Genet. 25 2000 25 29 [12] S. Fiori S.-I. Amari Editorial: special issue on geometric methods in neural networks and learning Neurocomputing 67 2005 214 244 [13] F. Gray, Pulse code communications, U.S. Patent 2632058, March 1953. [14] P. Gruber K. Stadlthanner M. Böhm F.J. Theis E.W. Lang A.M. Tomé A.R. Teixeira C.G. Puntonet J.M.G. Saéz Denoising using local projective subspace methods Neurocomputing 69 2006 1485 1501 [15] P. Gruber, F.J. Theis, Grassmann clustering, in: Proceedings of the European Signal Processing Conference (EUSIPCO), Florence, Italy. [16] M. Habl C. Bauer C. Ziegaus E.W. Lang Analyzing brain tumor related EEG signals with ICA algorithms Perspectives in Neuroscience: Artificial Neural Networks in Medicine and Biology 2000 Springer Berlin 131 136 [17] P.O. Hoyer Non-negative matrix factorization with sparseness constraints J. Mach. Learn. Res. 5 2004 1457 1469 [18] 〈 〉 , MPI-2: Extensions to the Message-Passing Interface 〈 www.mpi-forum.org 〉 . [19] A. Hyvärinen J. Karhunen E. Oja Independent Component Analysis 2001 Wiley New York [20] A. Hyvärinen E. Oja P. Hoyer J. Hurri Image feature extraction by sparse coding and independent component analysis Proceedings of the ICPR 1998 1998 [21] I.R. Keck, F.J. Theis, P. Gruber, E.W. Lang, K. Specht, C.G. Puntonet, 3D spatial analysis of fMRI data on a word perception task, in: C.G. Puntonet, A. Prieto (Eds.), Lecture Notes in Computer Science, vol. 3195, Springer, Berlin, 2004. [22] D.D. Lee H.S. Seung Learning the parts of objects by non-negative matrix factorization Nature 401 1999 788 791 [23] S.-I. Lee S. Batzoglou Application of independent component analysis to microarrays Genome Biol. 4 2003 R76.1 R76.21 [24] D.D. Less H.S. Seung Algorithms for non-negative matrix factorization Advances in Neural Information Processing 13 (NIPS’2000) 2001 MIT Press Cambridge, MA [25] M.S. Lewicki T.J. Sejnowski Learning overcomplete representations Neural Comput. 12 2000 337 365 [26] Y. Li A. Cichocki S. Amari Analysis of sparse representation and blind source separation Neural Comput. 16 2004 1193 1234 [27] D. Lutter, K. Stadlthanner, F.J. Theis, E.W. Lang, A.M. Tomé, B. Becker, T. Vogt, Analyzing gene expression profiles with ICA, in: Proceedings of the Fourth IASTED International Conference on Biomedical Engineering, Innsbruck, Austria, 2006, pp. 25–30. [28] J.B. MacQueen Some methods for classification and analysis of multivariate observations Proceedings of the Fifth Berkely Symposium on Mathematical Statistics and Probability 1967 University of California Press Berkely, CA [29] S. Makeigh A. Bell T.-P. Jung T. Sejnowski Independent component analysis of electroencephalographic data Advances in Neural Information Processing Systems 8 (NIPS’1995) 1995 MIT Press Cambridge, MA [30] Z. Michalewicz Genetic Algorithms + Data Structures = Evolution Programs 1999 Springer Berlin [31] B.A. Ohlshausen D.J. Field Natural image statistics and efficient coding Network Comput. Neural Syst. 7 1996 333 339 [32] M. Plumbley Geometrical methods for non-negative ica: manifolds, lie groups and toral subalgebras Neurocomputing 67 2005 161 197 (special issue on Geometric Methods in Neural Networks and Learning) [33] C.G. Puntonet A. Prieto Neural net approach for blind separation of sources based on geometric properties Neurocomputing 18 1998 141 164 [34] J. Quackenbush Computational analysis of microarray data Nature 2 2001 418 427 [35] D. Ruderman The statistics of natural images Network Comput. Neural Syst. 5 1994 517 548 [36] S.A. Saidi C.M. Holland D.P. Kreil D.J.C. MacKay D.S. Charnock-Jones C.G. Print S.K. Smith Independent component analysis for gene arrays Oncogene 23 2004 6677 6683 [37] J. Särelä H. Valpola Denoising source separation J. Mach. Learn. Res. 6 2005 233 272 [38] K. Stadlthanner F. Theis E.W. Lang A.M. Tomé W. Gronwald H.R. Kalbitzer A matrix pencil approach to the blind source separation of artifacts in 2D NMR spectra Neural Inf. Process. Lett. Rev. 1 2003 103 110 [39] K. Stadlthanner F. Theis E.W. Lang A.M. Tomé W. Gronwald H.R. Kalbitzer Separation of water artefacts in 2D NOESY protein spectra using congruent matrix pencils Neurocomputing 69 2006 497 522 [40] S.-I. Amari, A. Cichocki, H.H. Yang, A new learning algorithm for blind signal separation, Advances in Neural Information Processing Systems (NIPS) 8, 1996. [41] F.J. Theis A. Jung C.G. Puntonet E.W. Lang Linear geometric ica: fundamentals and algorithms Neural Comput. 15 2003 419 439 [42] A.M. Tomé, A.R. Teixeira, E.W. Lang, K. Stadlthanner, A. Rocha, Blind source separation using time-delayed signals, in: Proceedings of the International Joint Conference on Neural Networks, IJCNN’2004, vol. CD, Budapest, Hungary, 2004. [43] R. Vigario V. Jousmäki M. Hämäläinen R. Hari E. Oja Independent component analysis for identification of artifacts in magnetoencephalographic recordings Advances in Neural Information Processing Systems 10 (NIPS’1997) 1997 MIT Press Cambridge, MA [44] K. Yang J.C. Rajapakse ICA gives higher-order functional connectivity of brain Neural Inf. Process. Lett. Rev. 2 2004 27 32 Kurt Stadlthanner received his Diploma and Ph.D. degrees in Physics from the University of Regensburg in 2003 and 2007, respectively. Furthermore he obtained a Ph.D. degree in Computer Science from the University of Granada in 2006. He worked as a visiting researcher at the Department of Architecture and Computer Technology (University of Granada, Spain). Currently, he is working as a research scientist at the Philips Research Laboratories in Aachen (Germany). His scientific interests are in the fields of biological data processing and analysis by means of blind source separation and support vector machines. Fabian J. Theis obtained M.Sc. degrees in Mathematics and Physics at the University of Regensburg in 2000. He also received a Ph.D. degree in Physics from the same university in 2002 and a Ph.D. in Computer Science from the University of Granada in 2003. In 2006 he was awarded the Heinz Maier-Leibnitz Prize by the German Research Foundation. He worked as a visiting researcher at the Department of Architecture and Computer Technology (University of Granada, Spain), at the RIKEN Brain Science Institute (Wako, Japan) and at FAMU-FSU (Florida State University, USA). Currently, he is working for the Max-Planck-Institute for Dynamics and Self-Organisation at Göttingen, Germany, where is heading a Junior Research Group at the Department of Nonlinear Dynamics. His research interests include biostatistics and statistical machine learning with special focus on statistical signal processing using information-theoretic methods for analyzing large-scale biomedical data sets. Elmar W. Lang received his Physics Diploma with excellent grade in 1977 and his Ph.D. in Physics (summa cum laude) in 1980 and habilitated in Biophysics in 1988 at the University of Regensburg. He is apl. Professor of Biophysics at the University of Regensburg. He is currently an Associate Editor of Neurocomputing and Neural Information Processing—Letters and Reviews. His current research interests focus mainly on machine learning and include biomedical signal processing, independent component analysis and blind source separation, neural networks for classification and pattern recognition as well as stochastic process limits in queuing applications. Ana M. Tomé received her Ph.D. in Electrical Engineering from University of Aveiro in 1990. Currently, she is an Associate Professor of Electrical Engineering in the Department of Electronics and Telecommunications/IEETA of the University of Aveiro where she teaches courses on Digital Signal Processing for Electronics and Computer Engineering Diplomas. Her research interests include digital and statistical signal processing, independent component analysis and blind source separation as well as classification and pattern recognition applications of neural networks. Carlos G. Puntonet received a B.Sc. degree in 1982, an M.Sc. degree in 1986 and his Ph.D. degree in 1994, all from the University of Granada, Spain. These degrees are in Electronics Physics. Currently, he is an Associate Professor at the “Departamento de Arquitectura y TecnologÃa de Computadores” at the University of Granada. His research interests lie in the fields of signal processing, linear and nonlinear independent component analysis and blind separation of sources, artificial neural networks and optimization methods. J.M. Górriz received the B.Sc. in Physics and Electronic Engineering from the University of Granada, Spain, and the Ph.D. from the Universities of Cádiz and Granada, Spain, in 2000, 2001, 2003 and 2006, respectively. He is currently an Assistant Professor at the University of Granada. His present interests lie in the field of statistical signal processing and its application to speech and image processing. "
    },
    {
        "doc_title": "ICA analysis of retina images for glaucoma classification",
        "doc_scopus_id": "61849131687",
        "doc_doi": "10.1109/iembs.2008.4650253",
        "doc_eid": "2-s2.0-61849131687",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Classification rates",
            "Classification system",
            "Heidelberg",
            "K-nearest neighbor classification",
            "Optic nerve",
            "Scanning lasers",
            "Tomographs",
            "Visual fields"
        ],
        "doc_abstract": "Glaucoma represent one of the most frequent causes of partial loss of the visual field. It comes along with an ongoing destruction of the optic nerve caused by an increased pressure of the eye liquid. The disease becomes obvious from investigations of the retina with scanning laser microscopes. In this report an image analysis and classification system based on independent component analysis and k-nearest-neighbor classification is proposed. The method is tested with 120 selected retina images collected with the Heidelberg Retina Tomograph and achieves a classification rate of 91%. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Spatiotemporal group ICA applied to fMRI datasets",
        "doc_scopus_id": "61849111220",
        "doc_doi": "10.1109/iembs.2008.4650250",
        "doc_eid": "2-s2.0-61849111220",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Activation patterns",
            "Exploratory data analysis",
            "Frontal cortex",
            "Group analysis",
            "Group differences",
            "Independent component analysis(ICA)",
            "Prefrontal cortex",
            "Spatiotemporal process"
        ],
        "doc_abstract": "Exploratory data analysis techniques such as Independent component analysis (ICA) do not depend on a priori hypotheses and are able to detect unknown, yet structured spatiotemporal processes In neurolmaglng data. We present fMRI data of two different subject-groups (young and old), which performed a modified Wisconsin Card Sorting Test (WCST). Spatiotemporal ICA and SPM-generated brain maps of the subject data are compared. For the group analysis a singular value decomposition approach was used. Spatiotemporal ICA reveals a frontoparietal network being activated while subjects performed different variants of the WCST. Contrary to the SPM analysis, ICA analysis revealed significant differences between young and old subjects as well as significant within-group differences. While young subjects showed with increasing task demands (A>B>C) increasing activation of the right lateral prefrontal cortex and of the medial orbito-frontal cortex, old subjects showed no such gradient in activation pattern and appeared to be more distributed. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of unsupervised and supervised gene selection methods",
        "doc_scopus_id": "61849110360",
        "doc_doi": "10.1109/iembs.2008.4650389",
        "doc_eid": "2-s2.0-61849110360",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Chemically modified",
            "Efficient analysis",
            "Expressed genes",
            "Feature extraction techniques",
            "Gene expression profiles",
            "Independent component analysis(ICA)",
            "Matrix decomposition",
            "Regulatory process"
        ],
        "doc_abstract": "Modern machine learning methods based on matrix decomposition techniques like Independent Component Analysis (ICA) provide new and efficient analysis tools which are currently explored to analyze gene expression profiles. These exploratory feature extraction techniques yield informative expression modes (ICA) which are considered indicative of underlying regulatory processes. Their most strongly expressed genes represent marker genes for classification of the tissue samples under investigation. Comparison with supervised gene selection methods based on statistical scores or support vector machines corroborate these findings. The method is applied to macrophages loaded/de-loaded with chemically modified low density lipids. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Subspace techniques to remove artifacts from EEG: A quantitative analysis",
        "doc_scopus_id": "61849106041",
        "doc_doi": "10.1109/iembs.2008.4650185",
        "doc_eid": "2-s2.0-61849106041",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Characteristic bands",
            "Correlation coefficient",
            "Electro-encephalogram (EEG)",
            "Projective subspace",
            "Single channel recording",
            "Singular spectrum analysis",
            "Subspace techniques",
            "Time and frequency domains"
        ],
        "doc_abstract": "In this work we discuss and apply projective subspace techniques to both multichannel as well as single channel recordings. The single-channel approach is based on singular spectrum analysis(SSA) and the multichannel approach uses the extended infomax algorithm which is implemented in the opensource toolbox EEGLAB. Both approaches will be evaluated using artificial mixtures of a set of selected EEG signals. The latter were selected visually to contain as the dominant activity one of the characteristic bands of an electroencephalogram (EEG). The evaluation is performed both in the time and frequency domain by using correlation coefficients and coherence function, respectively. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "KPCA denoising and the pre-image problem revisited",
        "doc_scopus_id": "44449135904",
        "doc_doi": "10.1016/j.dsp.2007.08.001",
        "doc_eid": "2-s2.0-44449135904",
        "doc_date": "2008-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Statistics, Probability and Uncertainty",
                "area_abbreviation": "DECI",
                "area_code": "1804"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Denoising",
            "Fixed-point algorithm",
            "Kernel principal component analysis (KPCA)",
            "Pre-images"
        ],
        "doc_abstract": "Kernel principal component analysis (KPCA) is widely used in classification, feature extraction and denoising applications. In the latter it is unavoidable to deal with the pre-image problem which constitutes the most complex step in the whole processing chain. One of the methods to tackle this problem is an iterative solution based on a fixed-point algorithm. An alternative strategy considers an algebraic approach that relies on the solution of an under-determined system of equations. In this work we present a method that uses this algebraic approach to estimate a good starting point to the fixed-point iteration. We will demonstrate that this hybrid solution for the pre-image shows better performance than the other two methods. Further we extend the applicability of KPCA to one-dimensional signals which occur in many signal processing applications. We show that artefact removal from such data can be treated on the same footing as denoising. We finally apply the algorithm to denoise the famous USPS data set and to extract EOG interferences from single channel EEG recordings. © 2007 Elsevier Inc. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272384 291210 291718 291874 31 Digital Signal Processing DIGITALSIGNALPROCESSING 2007-08-15 2007-08-15 2010-10-06T23:00:19 S1051-2004(07)00112-1 S1051200407001121 10.1016/j.dsp.2007.08.001 S300 S300.1 HEAD-AND-TAIL 2015-05-15T04:43:22.69227-04:00 0 0 20080701 20080731 2008 2007-08-15T00:00:00Z rawtext articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype volfirst volissue affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 1051-2004 10512004 18 18 4 4 Volume 18, Issue 4 10 568 580 568 580 200807 July 2008 2008-07-01 2008-07-31 2008 Multidimensional signal processing article fla Copyright © 2007 Elsevier Inc. All rights reserved. KPCADENOISINGPREIMAGEPROBLEMREVISITED TEIXEIRA A GRUBER 2006 1485 1501 P MULLER 2001 181 202 K SCHOLKOPF 1998 1299 1319 B KWOK 2004 1517 1525 J SCHOLKOPF 1999 1000 1016 B EPHRAIM 1995 251 266 Y GHIL 2002 3.1 3.41 M GOWER 1968 582 585 J KIM 2005 1351 1365 K GOLYANDINA 2001 N ANALYSISTIMESERIESSTRUCTURESSARELATEDTECHNIQUES TEIXEIRA 2006 125 138 A BISHOP 1995 C NEURALNETWORKSFORPATTERNRECOGNITION TEIXEIRAX2008X568 TEIXEIRAX2008X568X580 TEIXEIRAX2008X568XA TEIXEIRAX2008X568X580XA item S1051-2004(07)00112-1 S1051200407001121 10.1016/j.dsp.2007.08.001 272384 2010-11-08T13:15:44.797509-05:00 2008-07-01 2008-07-31 true 372524 MAIN 13 58769 849 656 IMAGE-WEB-PDF 1 Digital Signal Processing 18 (2008) 568–580 www.elsevier.com/locate/dsp KPCA denoising and the pre-image problem revisited A.R. Teixeira a , A.M. Tomé a,∗ , K. Stadlthanner b ,E.W.Lang b a DETI/IEETA, Universidade de Aveiro, 3810-193 Aveiro, Portugal b Institute of Biophysics, University of Regensburg, D-93040 Regensburg, Germany Available online 15 August 2007 Abstract Kernel principal component analysis (KPCA) is widely used in classification, feature extraction and denoising applications. In the latter it is unavoidable to deal with the pre-image problem which constitutes the most complex step in the whole processing chain. One of the methods to tackle this problem is an iterative solution based on a fixed-point algorithm. An alternative strategy considers an algebraic approach that relies on the solution of an under-determined system of equations. In this work we present a method that uses this algebraic approach to estimate a good starting point to the fixed-point iteration. We will demonstrate that this hybrid solution for the pre-image shows better performance than the other two methods. Further we extend the applicability of KPCA to one-dimensional signals which occur in many signal processing applications. We show that artefact removal from such data can be treated on the same footing as denoising. We finally apply the algorithm to denoise the famous USPS data set and to extract EOG interferences from single channel EEG recordings. © 2007 Elsevier Inc. All rights reserved. Keywords: Kernel principal component analysis (KPCA); Pre-image; Time series analysis; Denoising 1. Introduction The objective of noise reduction techniques is to improve noisy signals. Projective subspace techniques can be used favorably to get rid of most of the noise contributions to multidimensional signals [1]. Whereas signals are considered to live in a sub-manifold only, noise is assumed to fill the multidimensional space evenly. The goal of subspace methods thus is to project the noisy signal onto the signal-plus-noise, or simply, signal subspace. This way part of the noise on the signal can be removed. Hence an estimate of the clean multidimensional signal can be made by removing or nullifying the components of the signal in the noise subspace, retaining only the components in the signal subspace. The decomposition of the space into (two) subspaces can be done using either singular value decomposition (SVD) or principal component analysis (PCA). Both techniques estimate those directions, corresponding to the L largest eigenvalues of the data covariance/scatter matrix or singular values of the data matrix, which can be associated with the eigenvectors spanning the signal subspace. The remaining orthogonal directions then can be associated with the noise subspace. Reconstructing the multidimensional signal using only those L dominant components can result in a substantial noise reduction of the recorded signals. Note that this approach is most appropriate if the underlying signal represents the main contribution to the recorded signal. More recently those algorithms are applied in feature * Corresponding author. E-mail address: ana@ieeta.pt (A.M. Tomé). 1051-2004/$ – see front matter © 2007 Elsevier Inc. All rights reserved. doi:10.1016/j.dsp.2007.08.001 A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 569 space created by a nonlinear mapping of the data. These generically nonlinear signal processing techniques like kernel principal component analysis (KPCA) are often used for denoising in image applications [2,3]. KPCA also represents a projective subspace technique. It transforms a signal nonlinearly into feature space and applies a linear principal component analysis to the transformed signals. But to recover the noise-reduced signal in input space after denoising in feature space, the nonlinear mapping must be reverted, i.e. the pre-image in input space must be estimated. In this work the KPCA methodology is shortly reviewed, following a matrix manipulation approach which is especially convenient when signal reconstruction and pre-image estimation are considered. Two methods of computing the pre-image [4,5], discussed in the literature, are summarized before a particularly suitable way of finding the starting point of the fixed-point iterative [5] method is suggested. This modification renders the algorithm much more efficient and fast. Furthermore, we adapt KPCA methods to denoise one-dimensional signals as well as to extract artefact-related features interfering with the signal of interest. Various signal processing applications rely on one-dimensional signals like biomedical signals, speech recordings, climatic and meteorological time series, just to mention a few. Clearly projective subspace techniques are only available for multidimensional signals. Hence, time series analysis techniques often rely on embedding one dimensional sensor signals in the space of their time-delayed coordinates [6–8], also called embedding space. Then resulting multidimensional signal can now be analyzed using KPCA. We apply the method to extract prominent artifacts like electro-oculograms (EOG) in electro-encephalograms (EEG). Note that in this example, the artifact-related contributions to the recorded EEG signals are considered “the signal” and the actual EEG signal is considered a “sort of a broadband noise.” Consequently, we can use the projective subspace techniques referred to above to separate the dominating artifacts from the “pure” EEG signals. Again the influence of the pre-image estimation on the performance of the algorithm is discussed. 2. KPCA denoising In practice, the goal of projective subspace techniques is to describe the data with reduced dimensionality by extracting meaningful components while still retaining the structure of the raw data. KPCA relies on a nonlinear mapping of given data to a higher dimensional space, called feature space. Then KPCA can simultaneously retain the nonlinear structure of the data while denoising is achieved with better performance because the projections are accomplished in the higher-dimensional feature space. 2.1. Kernel-PCA In KPCA a set of multidimensional signals x k ,k= 1,...,K, is envisaged to be mapped through a nonlinear function φ(x k ) into a feature space yielding the mapped data set Phi1 =[φ(x 1 ),φ(x 2 ),...,φ(x K )]. In feature space then a linear PCA is performed estimating the eigenvectors and eigenvalues of a matrix of outer products, called a scatter matrix which for zero mean data is given by C =Phi1Phi1 T . It can be shown that these eigenvectors and eigenvalues are related to those of a matrix of inner products, called a kernel matrix K = Phi1 T Phi1. Using the kernel trick [2], the centered kernel matrix can be expressed as K c = parenleftbigg I − 1 K j K j T K parenrightbigg Phi1 T Phi1 parenleftbigg I − 1 K j K j T K parenrightbigg = parenleftbigg I − 1 K j K j T K parenrightbigg K parenleftbigg I − 1 K j K j T K parenrightbigg , (1) where j K =[1,1,...,1] T is a vector with dimension K ×1, and I is a K ×K identity matrix. Notice that each element k(i,j) ≡ k(x i ,x j ) of the kernel matrix depends on the inner product φ T (x i )φ(x j ) which can be computed using only the data x k in input space. For instance, if a radial basis function (RBF) kernel is used, k(i,j) is calculated according to k(i,j) ≡ k(x i ,x j ) = exp parenleftbigg − bardblx i − x j bardbl 2 2σ 2 parenrightbigg , (2) where σ 2 is a free parameter related to the width of the kernel. It can be chosen according to any suitable data spread criterion. Because the eigenvalues of the scatter matrix C coincide with the eigenvalues of the kernel matrix K, the eigende- composition of K provides the necessary information to compute the projection of a vector of the input space y in c j the feature space. Considering the matrix V, the columns of which represent the L eigenvectors of the kernel matrix, 570 A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 and D, a diagonal matrix with the corresponding LlessorequalslantK eigenvalues of both matrices, the image φ(y j ) of a point in input space, can be projected onto the L directions spanned by the eigenvectors of the scatter matrix via z j = D −1/2 V T parenleftbigg I − 1 K j K j T K parenrightbigg Phi1 T φ(y j ), (3) where Phi1 T φ(y j ) represents a vector the components of which can be computed using the kernel trick by k y j = bracketleftbig k(x 1 ,y j ),k(x 2 ,y j ),...,k(x K ,y j ) bracketrightbig T . (4) There are many applications (for instance classification) where the projections provide necessary and sufficient infor- mation to characterize the problem. However, in denoising applications, for example, it is needed to reconstruct any data point in feature space from its noisy version employing the L principal components. Finally, the position of the corresponding point in input space is of interest, hence the pre-image of the denoised data sample in feature space needs to be estimated [4,5]. 2.2. Reconstruction and pre-image Consider the reconstructed point in feature space: ˆ φ(y j ) =Phi1 parenleftbigg I − 1 K j K j T K parenrightbigg VD −1/2 z j =Phi1g. (5) In order to avoid to work with the mapped data setPhi1, pre-image estimation methods described in literature combine the reconstruction in feature space and the estimation of its pre-image in input space in one step. This goal is achieved by using the Euclidian or L2-norm. The square of the Euclidian distance can be written using dot products which in turn can be substituted by kernel evaluations. Considering a point p in input space, the distance of its image φ(p) in feature space to the reconstructed point ˆ φ(y j ) is defined by ˜ d (2) = vextenddouble vextenddouble φ(p) − ˆ φ(y j ) vextenddouble vextenddouble 2 = parenleftbig φ(p) − ˆ φ(y j ) parenrightbig T parenleftbig φ(p) − ˆ φ(y j ) parenrightbig . (6) Substituting the expression given in Eq. (5) to compute the reconstructed point ˆ φ(y j ), the dot product can be replaced by kernel values ˜ d (2) = k(p,p) − 2g T k p + g T Kg, (7) where k p represents a vector whose entries are computed as the dot product of φ(p) with images Phi1 of the set of training data {x k } according to Eq. (4) identifying y j ≡ p. Both pre-image estimation methods use the definition of the Euclidian distance within different strategies, and consequently the input space point p must be chosen accordingly. In the following we will discuss these different strategies. 2.2.1. Distance method Recent work [4] to estimate the pre-image of a given point in feature space is based on the fact that it is possible to compute the coordinates of a new point if we know its distances to a set of known points [9]. Hence, the distances of the reconstructed point ˆ φ(y j ) to every point in the set Phi1 of images of the training data x k are computed. So in Eq. (7), the point p is chosen as an element of the training set, i.e., p ≡ x k . Then by computing the distances to all mapped points of the training set x k ,k= 1,...,K, the following distance vector is obtained ˜ d (2) = diag(K) − 2g T K + g T Kg. (8) With certain kernels, especially isotropic kernels and polynomial kernels with odd powers [4], it is possible to evaluate the distance in input space knowing the corresponding distance in feature space. If, for example, an RBF kernel is considered, there is a distinct relation between an input space distance d (2) and the corresponding feature space distance. Once the vector of distances in feature space can be computed as ˜ d (2) = 2 parenleftbigg j K − exp parenleftbigg − d (2) 2σ 2 parenrightbiggparenrightbigg (9) the corresponding vector of distances in input space is then given by A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 571 d (2) =−2σ 2 ln parenleftbig j K − 0.5 ˜ d (2) parenrightbig . (10) Consider next a subset S of neighbors of the reconstructed point ˆ φ(y j ), i.e., choose from the training set those S points with smallest distance ˜ d (2) , and select the corresponding points Q =[q 1 ,q 2 ,...,q S ] in input space. The coordinates of the subset of points may be taken as the columns of the eigenvector matrix E of their covariance matrix. After centering the set of neighbors by Q c = Q parenleftbigg I − 1 S j T S j S parenrightbigg (11) the columns of W = E T Q c represent the new coordinates of the points Q c . Their distance to the origin is obtained as d (2) 0 =[bardblw 1 bardbl 2 ,bardblw 2 bardbl 2 ,...,bardblw S bardbl 2 ]. Then, the coordinates of the point ˜p are given by W T ˜p =− 1 2 parenleftbig d (2) − d (2) 0 parenrightbig . (12) The pre-image p of the reconstructed point ˆ φ(y j ) is finally obtained as p = E˜p + 1 S Qj S = E˜p + p 0 (13) where p 0 represents the mean of the selected neighbors. This method is usually applied considering that the number S of neighbors is less than the dimension M of the points in input space [4]. In that case M–S components of the point ˜p vanish. Hence, the covariance matrix of the set of points Q can have at most S nonzero eigenvalues. Also note that the SVD of Eq. (12) represents the minimum norm solution. In that case, the second term of Eq. (13) representing the mean of the neighbors, might constitute the dominant term in the solution to the estimation of the pre-image of the reconstructed point ˆ φ(y j ). 2.2.2. Fixed point method The central idea of this method [5] consists in computing the unknown pre-image p which minimizes the Euclidian distance in feature space by setting to zero the gradient of Eq. (6) ∂ ˜ d (2) ∂p = ∂k(p,p) ∂p − 2 ∂g T k p ∂p = 0. (14) Substituting the RBF kernel, the first term of the previous equation is zero as k(p,p) = 1. Hence the zeros of the gradient are obtained by K summationdisplay i=1 g i (x i − p)exp parenleftbigg bardblx i − pbardbl 2 σ 2 parenrightbigg = X(g♦k p ) − pg T k p = 0, (15) where ♦ represents the Hadamard product. The zeroes can thus be computed iteratively by the fixed point algorithm p t+1 = X(g♦k p t ) g T k p t . (16) The iterative procedure stops when |p t+1 − p t | is less than a threshold and/or a maximum number of iterations t is achieved. An equivalent iteration scheme results starting with polynomial or sigmoidal kernels. However it has been reported that the convergence of the resulting procedure could not be achieved with polynomial kernels [4]. Because of this the following discussion is restricted to RBF kernels. 2.2.3. Modified fixed point method The fixed-point iteration can be started with any p 0 chosen randomly, but in that case it often results in a slow convergence. Alternatively, it can be started with the given noisy point in the input space. However this is an option only when the signal-to-noise ratio is high [10]. Hence, in the following we propose a modified iteration scheme which yields superior results: Motivation. Note that the denominator of Eq. (16) is formed by the dot product between the reconstructed point ˆ φ(y j ) and φ(p t ). Thus an appropriate starting point in input space can be chosen using the nearest neighborhood 572 A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 strategy borrowed from the distance method. This will avoid the numerical instability of having a very small or negative denominator. Implementation. Note that with an RBF kernel the identification of the closest neighbors can be achieved with the dot products avoiding thus the computation of Euclidian distances in feature space. Computing the vector r of dot products of ˆ φ(y j ) with the training set Phi1 yields r = g T K. (17) As the dot product of every mapped data point with itself is normalized to one, the closest neighbors are obtained by identifying the set S of maximal dot products ( ˆ φ T (y j )φ(x i )),i = 1,...,K.TheS closest neighbors, i.e., the ones that exhibit the largest dot products, are chosen. Selecting the corresponding points Q =[q 1 ,q 2 ,...,q S ] in input space, the fixed-point iteration should start with p 0 = (1/S)Qj S . (18) This strategy is more efficient than starting with the value computed by the distance method (see Eq. (13)) as suggested in [11]. The modified fixed point (mFP) algorithm will comprise the following initialization steps: the identification of S neighbors using Eq. (17) and the computation of the starting point of the fixed-point iteration (Eq. (16) with Eq. (18)). 3. KPCA of one-dimensional signals The projective subspace techniques discussed so far are clearly not available for one-dimensional time series to suppress noise contributions. But many signal processing applications rely on one-dimensional signals, like the bio- medical signals we are going to discuss. The transformation of one-dimensional signals to multidimensional signals can be effected by a technique called embedding. It is used, for example, in singular spectrum analysis (SSA) methods [7,12]. After embedding, projective techniques can be applied resulting in a noise-reduced multidimensional signal and, by reverting the embedding process, finally in a one-dimensional signal. However, other goals like feature extraction and classification can be accomplished also with this type of analysis. Following this strategy, one-dimensional sensor signals are embedded in the space of their time-delayed coordinates [8] to form a trajectory matrix, whose column vectors span the so called embedding space. To revert the embedding process, the last step is to transform the “denoised trajectory matrix,” whose columns are formed by the pre-images of the reconstructed (= denoised) vectors, into a one-dimensional time series with N samples in the time domain. 3.1. Embedding The embedding transformation can thus be regarded as a mapping which transforms a one-dimensional time series x = (x[0],x[1],...,x[N − 1]) to a multidimensional sequence of K = N − M + 1 lagged vectors x k = bracketleftbig x[k − 1 + M − 1],...,x[k − 1] bracketrightbig T ,k= 1,...,K (19) with M<Nbeing the corresponding embedding dimension. The lagged vectors then constitute the columns of the trajectory matrix X =[x 1 ,...,x K ], which represents a Toeplitz matrix. The further processing of this data matrix X can be performed by KPCA considering each column a point in a space of dimension M (input space). Note that the latter space is often also called feature space but it is not to be confused with the space resulting from the nonlinear transformation of the input data effected by KPCA. In the following we will use the term feature space only for this latter space. 3.2. Diagonal averaging After applying the described steps of KPCA to each column of the trajectory matrix (X), a new matrix of denoised data is obtained ˆ X. So, each φ(x k ),k = 1,...,K, is projected in feature space onto L principal directions; it is then reconstructed using these projections, and finally its pre-image p k in input space is estimated following one of the described methods. The denoised points p then form the columns of ˆ X, the “denoised trajectory matrix.” But k in general this matrix does not posses the characteristic Toeplitz structure anymore, i.e., the elements along each A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 573 descending diagonal of ˆ X will not be identical like in case of the original trajectory matrix X. This can be cured, however, by replacing the entries in each diagonal by their average, obtaining a Toeplitz matrix X r . This procedure assures that the Frobenius norm of the difference (X r − ˆ X) attains its minimum value among all possible solutions to get a matrix with all diagonals equal [12]. The noise-reduced one-dimensional signal, ˆx[n], is then obtained by reverting the embedding of matrix X r , i.e., by forming the signal with an entry of each descendent diagonal. 4. Numerical simulations The algorithms were implemented in MATLAB using the toolbox provided by Franc [13], where basic pattern recognition tools and kernel methods can be found. In the following the methods discussed above will first be applied to the USPS data set to illustrate the denoising performance of the modified KPCA concerning the estimation of pre-images. The second example refers to the extraction of EOG artifacts from single channel EEG recordings. 4.1. Denoising a USPS data set In this section the USPS data set 1 consisting of 16 × 16 handwritten digits is used. Thus the input data vector, x k has dimension 256 and is formed by row concatenation of the original image after adding white Gaussian noise N(0,σ 2 r ) to it. Notice that adding noise to each digit with fixed variance, the signal-to-noise ratio (SNR) differs for each digit. Table 1, first column, shows the SNRs for the digits represented in the second row of Fig. 1 after adding noise with variance σ 2 r = 0.25 to the original digits represented in the first row of Fig. 1. Each type of digit is denoised separately, i.e., a kernel matrix was computed for 300 randomly chosen examples of the digit. Each time the kernel matrix was created using an RBF kernel with σ = max i (bardblx i − x c bardbl), i = 1,...,K, with K = 300 and x c the mean of the data set. Then, denoising was achieved by projecting each mapped digit Phi1(x k ) Table 1 SNR for different pre-image estimation methods σ 2 r = 0.25 Digit SNR Image mFP Mean Distance L 06.202 8.910 8.686 8.766 8 1 −0.194 2.670 4.294 3.787 4 22.473 4.000 4.305 4.203 8 33.603 7.166 6.083 6.086 16 41.925 4.575 4.421 3.596 26 53.943 7.330 5.890 5.752 16 63.690 7.353 7.084 6.184 16 71.482 4.535 4.054 3.365 16 84.098 7.575 6.604 6.729 8 94.068 7.369 5.809 5.334 16 Fig. 1. Examples of each original, noisy and denoised digits using the different pre-image methods using S = 10 neighbors. First line—original digit, second line—noisy digit; the following lines the denoised digits using respectively the modified fixed point (mFP), mean and distance algorithms. 1 574 A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 Fig. 2. Eigenspectra of kernel matrices for: left—digit 1, right—digit 5. Fig. 3. SNR versus number of neighbors in the pre-image methods. Top trace—modified fixed point (mFP) algorithm; bottom trace—SNR of the noisy digit; dotted trace—mean of neighbors; full trace—distance algorithm. onto the leading L eigenvectors corresponding to the largest eigenvalues. The eigenspectra of the kernel matrices are different. The number L of leading eigenvalues in each case was chosen according to the leveling off of the latter (see Fig. 2). The last column of Table 1 shows the number L of directions obtained for each digit. Finally, to yield a denoised version ˆx k of the noisy digit x k , the pre-image ˆx k of the reconstructed image of a digit ˆ Phi1(x k ) was estimated employing one of the following methods • Distance: the distance method as proposed in [4], see Eq. (13). • Mean: the mean of the nearest neighbors only within the distance method which corresponds to considering p 0 as the pre-image. • mFP: the fixed point method initialized with p 0 . Figure 1 provides an illustration of the results of KPCA denoising for different digits using the different pre-image estimation methods discussed. Table 1 shows the SNRs between the original digit and the denoised version using S = 10 nearest neighbors. These values do not show any consistent tendency that could indicate a clear preference to any of the pre-image methods. But it is obvious that the modified fixed point (mFP) method yields better results in most cases and both the mean and the distance methods always yield very similar results. Considering the dependence on S, it turns out that the distance method, proposed in [4], is the most sensitive concerning the number of nearest neighbors selected. Figure 3 illustrates in a more detailed manner this dependence of the SNR obtained with the mFP, the mean and the distance methods on the number of nearest neighbors. A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 575 Fig. 4. Denoised digit using different values for S: top—modified fixed point (mFP) algorithm; middle—mean of neighbors; bottom—distance method. Table 2 SNR as function of the number of nearest neighbors S for noise variance σ 2 = 0.25 and the pre-image estimation methods discussed S SNR Mean Variance Noisy image 2.322 3.026 mFP 1 4.5164 2.4323 10 4.5287 2.3575 100 4.4144 2.4711 Mean 1 2.1092 2.4853 10 4.7978 1.8510 100 4.2323 1.7979 Distance 1 2.1092 2.4853 10 4.7942 1.8258 100 3.3670 1.8589 The results obtained with the method of mean nearest neighbors are always close to the ones resulting from the distance method. And both methods show a decline of the SNR with increasing S. On the other hand, the mod- ified fixed point is not dependent on the starting point (number of neighbors). Figure 4 illustrates the influence of the number of neighbors on the SNRs obtained. Naturally when the number of neighbors increases the solu- tion provided by the mean of neighbors encompasses a smoothing of the background thus leading to a higher noise level. The methods are also evaluated using the whole set of digits. The mean SNRs and their respective variances using different S are collected in Table 2. The results obtained with the modified fixed point method of course do not depend on S while in the distance method there is a difference of ≈1.5 dB between S = 10 and 100. But considering S = 1, both the mean and the distance methods correspond to choosing, within the given data set, the digit whose image is closest to ˆ φ(y j ). Then the SNR is similar to the one of the original noisy digit. However, considering S = 1in the modified fixed point method, a reliable solution is achieved with the algorithm converging faster than when it is initialized randomly. 4.2. EEG data Biomedical signals are often contaminated with artifacts which severely distort the signals to be investigated. As an example we will study the removal of prominent EOG artifacts from EEG recordings. A segment of the signal of 12 s of duration containing high-amplitude EOG artifacts was considered and divided into 4 sub-segments with N = 384 samples. KPCA was applied separately to each sub-segment. The one-dimensional signal was embedded in M = 11 dimensions, and the number L = 6 principal components for reconstruction was the same for all subsegments. The pre-image estimated in input space then obviously corresponds to the embedded, multidimensional version of the one-dimensional EOG contaminating the original EEG recording. This one-dimensional EOG can be obtained after reverting the embedding process. Finally the extracted EOG signal is subtracted from the original EEG recording to yield a corrected version of the EEG. The first experiment illustrates the impact of the method to estimate the pre- image upon the results and the second experiment illustrates the performance of the KPCA method when compared with local SSA. 576 A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 4.2.1. Estimation of the pre-image Visual inspection of the extracted signals confirmed that the results strongly depend on the method to estimate the pre-image, corroborating results obtained with the USPS data set. Further experiments showed that the performance of the distance method is strongly dependent on the number S of neighbors, yielding in some cases unreliable solutions (see Fig. 8, for example). The results of the modified fixed point algorithm are illustrated in Fig. 5 and are independent of the number of neighbors used to estimate the starting point. The rate of convergence can be improved considerably when using p 0 as the starting point instead of using a random initialization (see Fig. 6). To provide a global overview of the performance of the methods, correlation coefficients between a reference signal and signals resulting from changing either the method of estimating the pre-image or varying the number of neighbors S are calculated. Figure 7 shows the results considering as reference the signal obtained with the modified fixed point method initialized with the best match in the training set (S = 1). The correlation coefficient for mFP equals cc(mFP) = 1 whatever is the number of neighbors. Note that with S ={3,6,7,12} neighbors, the distance method does not yield a reliable solution. Figure 8 illustrates this for one such solution where we can confirm that the extracted signal does not correspond to the EOG component. Furthermore, note that if S>M, the correlation coefficients are small. If S<20, the result of the mean method is very close to the result of the modified fixed point Fig. 5. A segment of the EEG signal processed with KPCA (reconstructed with L = 6 principal components and using mFP to estimate the pre-image (top—the original EEG, middle—the extracted EOG artifact, bottom—the corrected EEG). Fig. 6. The number of iterations needed for denoising data vector x , k = 1,...,K = 374 using the modified fixed point (full line) or the fixed k point with random initialization (dotted line). A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 577 Fig. 7. Correlation coefficients between the signal obtained with mFP (S = 1) and all the signals obtained changing the pre-image method and/or varying S. Fig. 8. A segment of the EEG signal processed with KPCA (reconstructed with L = 6 principal components) using the distance method (S = 12) to estimate the pre-image (top—the original EEG, middle—the extracted EOG artifact, bottom—the corrected EEG). algorithm (mFP). This is corroborated by estimates of the power spectral densities of both the corrected EEG signals and the extracted EOG as obtained with the mean and modified fixed point algorithms, respectively. 4.2.2. KPCA compared to local SSA In previous work [14] a modification of the well known singular spectrum analysis (SSA) was proposed for denois- ing and feature extraction from one-dimensional signals. The method is called local singular spectrum analysis (local SSA). The method is briefly summarized in appendix but consists in a local PCA decomposition performed in input space while KPCA performs a PCA after transforming the input data nonlinearly to a feature space. The algorithm is applied to the same segments of the EEG recordings, contaminated by large EOG artifacts, to which also KPCA has been applied. Using an embedding dimension M = 41 and an optimal number of clusters q = 6, local SSA achieves an artifact separation (not shown here) which is upon visual inspection indistinguishable from the results of the KPCA analysis shown in Fig. 8a. Comparing the power spectral densities resulting from both the local SSA and the modified KPCA, Fig. 9 shows that the low frequency content of the corrected EEG is affected differently by both methods. The modified KPCA seems to preserve more spectral information in the very low frequency regime (f lessorequalslant3 Hz) but yields 578 A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 Fig. 9. Power spectral densities (psd) resulting from: left—local SSA (q = 6), right—kernel-PCA. similar results at higher frequencies: the correlation coefficient (cc) between the extracted EOGs resulting from both the KPCA and the local SSA is cc = 0.999 and the one between the corrected EEGs is cc = 0.833. The latter value results from the fact that local SSA also extracts the 50 Hz line interference while the KPCA does not. 5. Conclusions In this work we considered the estimation of the pre-image within KPCA. This becomes important in applications like denoising. We compared the two methods of pre-image estimation discussed in the literature and suggested two simple modifications yielding a hybrid approach which proofed very effective in practical applications with RBF kernels: (a) the mean of neighbors; (b) the mean of nearest neighbors as starting point to fixed point (mFP). The comparison on a denoising task revealed that the solution obtained by the distance method strongly depends on the number S of nearest neighbors chosen and sometimes does not yield reliable results. If S is smaller than the dimension of the data space, the solution can often be closely approximated by simply choosing the mean of the nearest neighbors which speeds up computation considerably. Concerning the fixed point algorithm a random initialization as suggested in the literature often results in very slow convergence. Initializing the algorithm with the mean of the nearest neighbors considerably speeds up convergence and yields a very robust algorithm. The methodology used for denoising can as well be applied to the problem of extracting prominent artifacts from one-dimensional signals like single channel EEG recordings. To this end, we adapt kernel principal component analysis to deal with one- dimensional signals by embedding them into the space of their delayed coordinates. We demonstrate the performance of the proposed KPCA algorithms to remove high-amplitude EOG interferences from EEG signals. The result of the artifact removal strongly depends on the way the pre-image is estimated. We show that again the fixed point algorithm initialized with the mean of the nearest neighbors (mFP) of each mapped data point is the most robust and reliably extracts the EOG artifacts. In a previous work [14,15] local SSA was proposed for denoising and feature extraction from one-dimensional signals. We present a short comparison to this method and show that the low frequency content of the corrected EEG is affected differently by both methods. Acknowledgments A.R. Teixeira received a Ph.D. Scholarship (SFRH/BD/28404/2006) supported by the Portuguese Foundation for Science and Technology (FCT). This work was also supported by grants from DAAD and CRUP which is gratefully acknowledged. Appendix A For convenience the main steps of local SSA are reviewed [15]. A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 579 • After embedding, the column vectors x k ,k= 1,...,K, of the trajectory matrix are clustered using any cluster- ing algorithm (like k-means [16]). The set of indices of the columns of X is subdivided into q disjoint subsets c 1 ,c 2 ,...,c q . Thus sub-trajectory matrix X (c i ) is formed with N c i columns of the matrix X which belong to the subset of indices c i . • A covariance matrix is computed in each cluster. And the eigenvalue decomposition of each the covariance ma- trices is computed. The denoising can be achieved by projecting the multidimensional signal into the subspace spanned by the eigenvectors corresponding to the L c i <Mlargest eigenvalues. • The clustering is reverted by forming an estimate ˆ X of the reconstructed, noise-free trajectory matrix using the columns of the extracted sub-trajectory matrices, ˆ X c i ,i= 1,...,q, according to the contents of subsets c i . Then the corresponding one dimensional signal is obtained as explained in Section 3.2. References [1] P. Gruber, K. Stadlthanner, M. Böhm, F.J. Theis, E.W. Lang, A.M. Tomé, A.R. Teixeira, C.G. Puntonet, J.M. Górriz Saéz, Denoising using local projective subspace methods, Neurocomputing 69 (2006) 1485–1501. [2] K.-R. Müller, S. Mika, G. Rätsch, K. Tsuda, B. Schölkopf, An introduction to kernel-based algorithms, IEEE Trans. Neural Networks 12 (2) (2001) 181–202. [3] B. Schölkopf, A. Smola, K.-R. Müller, Nonlinear component analysis as a kernel eigenvalue problem, Neural Comput. 10 (1998) 1299–1319. [4] J.T. Kwok, I.W. Tsang, The pre-image problem in kernel methods, IEEE Trans. Neural Networks 15 (6) (2004) 1517–1525. [5] B. Schölkopf, S. Mika, C.J. Barges, P. Knirsch, K.-R. Müller, G. Ratsch, A.J. Smola, Input space versus feature space in kernel-based methods, IEEE Trans. Neural Networks 10 (5) (1999) 1000–1016. [6] Y. Ephraim, H.L. Van Trees, A signal subspace approach for speech enhancement, IEEE Trans. Acoust. Speech Signal Process. 3 (4) (1995) 251–266. [7] M. Ghil, M.R. Allen, M.D. Dettinger, K. Ide, et al., Advanced spectral methods for climatic time series, Rev. Geophys. 40 (1) (2002) 3.1–3.41. [8] C.H. You, S.N. Koh, S. Rahardja, Signal subspace speech enhancement for audible noise reduction, in: ICASPP 2005, vol. I, Philadelphia, USA, 2005, pp. 145–148. [9] J.C. Gower, Adding a point to vector diagram in multivariate analysis, Biometrika 55 (1968) 582–585. [10] T. Takahashi, T. Kurita, Robust de-noising by kernel PCA, in: J.R. Dorronsoro (Ed.), ICANN2002, LNCS, vol. 2415, Springer-Verlag, Madrid, 2002, pp. 739–744. [11] K.I. Kim, M.O. Franz, B. Schölkopf, Itereative kernel component analysis for image modeling, IEEE Trans. Pattern Anal. Machine Intel. 27 (9) (2005) 1351–1365. [12] N. Golyandina, V. Nekrutkin, A. Zhigljavsky, Analysis of Time Series Structure: SSA and Related Techniques, Chapman & Hall, London, 2001. [13] V. Franc, V. Hlaváˇc, Stastical pattern recognition toolbox for Matlab, 2004. [14] A.R. Teixeira, A.M. Tomé, E.W. Lang, P. Gruber, A. Martins da Silva, On the use of clustering and local singular spectrum analysis to remove ocular artifacts from electroencephalograms, in: IJCNN2005, Montréal, Canada, 2005, pp. 2514–2519. [15] A.R. Teixeira, A.M. Tomé, E.W. Lang, P. Gruber, A. Martins da Silva, Automatic removal of high-amplitude artifacts from single-channel electroencephalograms, Comp. Methods Programs Biomed. 83 (2) (2006) 125–138. [16] C.M. Bishop, Neural Networks for Pattern Recognition, Oxford Univ. Press, Oxford, 1995. A.R. Teixeira received the diploma degree in mathematics applied to technology from the University of Porto, Porto, Portugal, in 2003, and the M.Sc. degree in electronics and telecommunications at the University of Aveiro, Aveiro, Portugal, in 2005. Currently, she is doing the Ph.D. in electrical engineering in the Signal Processing Lab of IEETA/DETI at the University of Aveiro. Her research interests include biomedical digital signal processing and principal and independent component analysis. A.M. Tomé received the Ph.D. degree in electrical engineering from the University of Aveiro, Aveiro, Portugal, in 1990. Currently, she is an Associate Professor of electrical engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as classification and pattern recognition applications. K. Stadlthanner received the diploma degree in physics from the University of Regensburg in 2003. He also received the Ph.D. degree in computer science from the University of Granada, Spain, in 2006 and the Ph.D. degree in biophysics from the University of Regensburg, Germany, in 2007. He is currently working in a research laboratory with Phillips AG, Aachen. His scientific interests are in the fields of biological signal processing and image analysis by means of machine learning techniques and neural networks. E.W. Lang received the diploma degree in physics in 1977, the Ph.D. degree in physics in 1980, and habilitated in biophysics in 1988 from the University of Regensburg, Regensburg, Germany. He is an Adjunct Professor of biophysics at the University 580 A.R. Teixeira et al. / Digital Signal Processing 18 (2008) 568–580 of Regensburg, where he is heading the Neuro- and Bioinformatics Group. Currently, he serves as an Associate Editor of Neu- rocomputing and Neural Information Processing—Letters and Reviews. His current research interests include biomedical signal and image processing, independent component analysis and blind source separation, neural networks for classification and pattern recognition, and stochastic process limits in queueing applications. YDSPR 762 S1051-2004(07)00112-1 10.1016/j.dsp.2007.08.001 Elsevier Inc. KPCA denoising and the pre-image problem revisited A.R. Teixeira a A.M. Tomé a ⁎ K. Stadlthanner b E.W. Lang b a DETI/IEETA, Universidade de Aveiro, 3810-193 Aveiro, Portugal b Institute of Biophysics, University of Regensburg, D-93040 Regensburg, Germany ⁎ Corresponding author. Kernel principal component analysis (KPCA) is widely used in classification, feature extraction and denoising applications. In the latter it is unavoidable to deal with the pre-image problem which constitutes the most complex step in the whole processing chain. One of the methods to tackle this problem is an iterative solution based on a fixed-point algorithm. An alternative strategy considers an algebraic approach that relies on the solution of an under-determined system of equations. In this work we present a method that uses this algebraic approach to estimate a good starting point to the fixed-point iteration. We will demonstrate that this hybrid solution for the pre-image shows better performance than the other two methods. Further we extend the applicability of KPCA to one-dimensional signals which occur in many signal processing applications. We show that artefact removal from such data can be treated on the same footing as denoising. We finally apply the algorithm to denoise the famous USPS data set and to extract EOG interferences from single channel EEG recordings. Keywords Kernel principal component analysis (KPCA) Pre-image Time series analysis Denoising References [1] P. Gruber K. Stadlthanner M. Böhm F.J. Theis E.W. Lang A.M. Tomé A.R. Teixeira C.G. Puntonet J.M. Górriz Saéz Denoising using local projective subspace methods Neurocomputing 69 2006 1485 1501 [2] K.-R. Müller S. Mika G. Rätsch K. Tsuda B. Schölkopf An introduction to kernel-based algorithms IEEE Trans. Neural Networks 12 2 2001 181 202 [3] B. Schölkopf A. Smola K.-R. Müller Nonlinear component analysis as a kernel eigenvalue problem Neural Comput. 10 1998 1299 1319 [4] J.T. Kwok I.W. Tsang The pre-image problem in kernel methods IEEE Trans. Neural Networks 15 6 2004 1517 1525 [5] B. Schölkopf S. Mika C.J. Barges P. Knirsch K.-R. Müller G. Ratsch A.J. Smola Input space versus feature space in kernel-based methods IEEE Trans. Neural Networks 10 5 1999 1000 1016 [6] Y. Ephraim H.L. Van Trees A signal subspace approach for speech enhancement IEEE Trans. Acoust. Speech Signal Process. 3 4 1995 251 266 [7] M. Ghil M.R. Allen M.D. Dettinger K. Ide Advanced spectral methods for climatic time series Rev. Geophys. 40 1 2002 3.1 3.41 [8] C.H. You, S.N. Koh, S. Rahardja, Signal subspace speech enhancement for audible noise reduction, in: ICASPP 2005, vol. I, Philadelphia, USA, 2005, pp. 145–148 [9] J.C. Gower Adding a point to vector diagram in multivariate analysis Biometrika 55 1968 582 585 [10] T. Takahashi, T. Kurita, Robust de-noising by kernel PCA, in: J.R. Dorronsoro (Ed.), ICANN2002, LNCS, vol. 2415, Springer-Verlag, Madrid, 2002, pp. 739–744 [11] K.I. Kim M.O. Franz B. Schölkopf Itereative kernel component analysis for image modeling IEEE Trans. Pattern Anal. Machine Intel. 27 9 2005 1351 1365 [12] N. Golyandina V. Nekrutkin A. Zhigljavsky Analysis of Time Series Structure: SSA and Related Techniques 2001 Chapman & Hall London [13] V. Franc, V. Hlaváč, Stastical pattern recognition toolbox for Matlab, 2004 [14] A.R. Teixeira, A.M. Tomé, E.W. Lang, P. Gruber, A. Martins da Silva, On the use of clustering and local singular spectrum analysis to remove ocular artifacts from electroencephalograms, in: IJCNN2005, Montréal, Canada, 2005, pp. 2514–2519 [15] A.R. Teixeira A.M. Tomé E.W. Lang P. Gruber A. Martins da Silva Automatic removal of high-amplitude artifacts from single-channel electroencephalograms Comp. Methods Programs Biomed. 83 2 2006 125 138 [16] C.M. Bishop Neural Networks for Pattern Recognition 1995 Oxford Univ. Press Oxford A.R. Teixeira received the diploma degree in mathematics applied to technology from the University of Porto, Porto, Portugal, in 2003, and the M.Sc. degree in electronics and telecommunications at the University of Aveiro, Aveiro, Portugal, in 2005. Currently, she is doing the Ph.D. in electrical engineering in the Signal Processing Lab of IEETA/DETI at the University of Aveiro. Her research interests include biomedical digital signal processing and principal and independent component analysis. A.M. Tomé received the Ph.D. degree in electrical engineering from the University of Aveiro, Aveiro, Portugal, in 1990. Currently, she is an Associate Professor of electrical engineering with the DETI/IEETA of the University of Aveiro. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation, as well as classification and pattern recognition applications. K. Stadlthanner received the diploma degree in physics from the University of Regensburg in 2003. He also received the Ph.D. degree in computer science from the University of Granada, Spain, in 2006 and the Ph.D. degree in biophysics from the University of Regensburg, Germany, in 2007. He is currently working in a research laboratory with Phillips AG, Aachen. His scientific interests are in the fields of biological signal processing and image analysis by means of machine learning techniques and neural networks. E.W. Lang received the diploma degree in physics in 1977, the Ph.D. degree in physics in 1980, and habilitated in biophysics in 1988 from the University of Regensburg, Regensburg, Germany. He is an Adjunct Professor of biophysics at the University of Regensburg, where he is heading the Neuro- and Bioinformatics Group. Currently, he serves as an Associate Editor of Neurocomputing and Neural Information Processing—Letters and Reviews. His current research interests include biomedical signal and image processing, independent component analysis and blind source separation, neural networks for classification and pattern recognition, and stochastic process limits in queueing applications. "
    },
    {
        "doc_title": "Routes to identify marker genes for microarray classification",
        "doc_scopus_id": "57649243989",
        "doc_doi": "10.1109/IEMBS.2007.4353368",
        "doc_eid": "2-s2.0-57649243989",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Gene regulation pathways",
            "Leukemia",
            "Monocyte"
        ],
        "doc_abstract": "Support vector machines are applied to extract marker genes from various microarray data sets: Breast Cancer, Leukemia and Monocyte - Macrophage Differentiation to ease classification of related pathologies or characterize related gene regulation pathways. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Greedy kernel PCA applied to single-channel EEG recordings",
        "doc_scopus_id": "57649229037",
        "doc_doi": "10.1109/IEMBS.2007.4353576",
        "doc_eid": "2-s2.0-57649229037",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Kernel technique",
            "Multivariate signals",
            "Reduced data sets"
        ],
        "doc_abstract": "In this work, we propose the correction of univariate, single channel EEGs using a kernel technique. The EEG signal is embedded in its time-delayed coordinates obtaining a multivariate signal. A kernel subspace technique is used for denoising and artefact extraction. The proposed kernel method follows a greedy approach to use a reduced data set to compute a new basis onto which to project the mapped data in feature space. The pre-image of the reconstructed multivariate signal is computed and the embedding is reverted. The resultant signal is the high amplitude artifact which must be subtracted from the original signal to obtain a corrected version of the underlying signal. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "How to extract marker genes from microarray data sets",
        "doc_scopus_id": "57649174930",
        "doc_doi": "10.1109/IEMBS.2007.4353266",
        "doc_eid": "2-s2.0-57649174930",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Gene expression profiles (GEP)",
            "Matrix factorization",
            "Mononcytes"
        ],
        "doc_abstract": "In this study we focus on classification tasks and apply matrix factorization techniques like principal component analysis (PCA), independent component analysis (ICA) and non-negative matrix factorization ( NMF) to a microarray data set. The latter monitors the gene expression levels (GEL) of mononcytes and macrophages during and after differentiation. We show that these tools are able to identify relevant signatures in the deduced matrices and extract marker genes from these gene expression profiles (GEPs) without the need for extensive data bank search for appropriate functional annotations. With these marker genes corresponding test data sets can then easily be classified into related diagnostic categories. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sparse nonnegative matrix factorization with genetic algorithms for microarray analysis",
        "doc_scopus_id": "51749107029",
        "doc_doi": "10.1109/IJCNN.2007.4370971",
        "doc_eid": "2-s2.0-51749107029",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Gene expression profiling",
            "Joint conference",
            "Non-Negative Matrix Factorization"
        ],
        "doc_abstract": "Nonnegative Matrix Factorization (NMF) has proven to be a useful tool for the analysis of nonnegative multivariate data. Gene expression profiles naturally conform to assumptions about data formats raised by NMF. However, it is known not to lead to unique results concerning the component signals extracted. In this paper we consider an extension of the NMF algorithm which provides unique solutions whenever the underlying component signals are sufficiently sparse. A new sparseness measure is proposed most appropriate to suitably transformed gene expression profiles. The resulting fitness function is discontinuous and exhibits many local minima, hence we use a genetic algorithm for its optimization. The algorithm is applied to toy data to investigate its properties as well as to a microarray data set related to Pseudo-Xanthoma Elasticum (PXE). ©2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring matrix factorization techniques for classification of gene expression profiles",
        "doc_scopus_id": "51149109776",
        "doc_doi": "10.1109/WISP.2007.4447571",
        "doc_eid": "2-s2.0-51149109776",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Gene expression profiles",
            "Gene expression profiling",
            "Independent component analysis",
            "Marker genes",
            "Sparse nonnegative matrix factorization"
        ],
        "doc_abstract": "In this study we focus on diagnostic classification tasks and the extraction of related marker genes from gene expression profiles. We apply ICA and sparse NMF to various microarray data sets. The latter monitor the gene expression levels of either human breast cancer (HBC) cell lines [1] or the famous leucemia data set [2] under various environmental conditions. We show that these matrix decomposition techniques are able to identify relevant signatures in the deduced matrices and extract marker genes from these gene expression profiles. With these marker genes corresponding test data sets can be classified into related diagnostic categories. ©2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Single-channel electroencephalogram analysis using non-linear subspace techniques",
        "doc_scopus_id": "51149104778",
        "doc_doi": "10.1109/WISP.2007.4447577",
        "doc_eid": "2-s2.0-51149104778",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "EEG",
            "EOG",
            "KPCA",
            "Local SSA",
            "Removing artifacts",
            "Subspace techniques"
        ],
        "doc_abstract": "In this work, we propose the correction of univariate, single channel EEGs using projective subspace techniques. The biomedical signals which often represent one dimensional time series, need to be transformed to multi-dimensional signal vectors for the latter techniques to be applicable. The transformation can be achieved by embedding an observed signal in its delayed coordinates. We propose the application of two non-linear subspace techniques to the obtained multidimensional signal. One of the techniques consists in a modified version of Singular Spectrum Analysis (SSA) and the other is kernel Principal Component Analysis (KPCA) implemented using a reduced rank approximation of the kernel matrix. Both nonlinear subspace projection techniques are applied to an electroencephalogram (EEG) signal recorded in the frontal channel to extract its prominent electrooculogram (EOG) interference. ©2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploiting low-rank approximations of kernel matrices in denoising applications",
        "doc_scopus_id": "48149085332",
        "doc_doi": "10.1109/MLSP.2007.4414330",
        "doc_eid": "2-s2.0-48149085332",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Algebraic frameworks",
            "Computational burden",
            "De-noising",
            "Eigen-decomposition",
            "Eigenvectors",
            "IEEE Signal Processing Society",
            "Kernel matrices",
            "Kernel methods",
            "Largest eigenvalues",
            "Low-rank approximations",
            "Machine-learning",
            "Numerical simulations",
            "Training data"
        ],
        "doc_abstract": "The eigendecomposition of a kernel matrix can present a computational burden in many kernel methods. Nevertheless only the largest eigenvalues and corresponding eigenvectors need to be computed. In this work we discuss the Nyström low-rank approximations of the kernel matrix and its applications in KPCA denoising tasks. Furthermore, the low-rank approximations have the advantage of being related with a smaller subset of the training data which constitute then a basis of a subspace. In a common algebraic framework we discuss the different approaches to compute the basis. Numerical simulations concerning the denoising are presented to compare the discussed approaches. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Blind matrix decomposition techniques to identify marker genes from microarrays",
        "doc_scopus_id": "38149124168",
        "doc_doi": "10.1007/978-3-540-74494-8_81",
        "doc_eid": "2-s2.0-38149124168",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Blind matrix decomposition",
            "Gene expression data sets",
            "Marker genes"
        ],
        "doc_abstract": "Exploratory matrix factorization methods like PCA, ICA and sparseNMF are applied to identify marker genes and classify gene expression data sets into different categories for diagnostic purposes or group genes into functional categories for further investigation of related regulatory pathways. Gene expression levels of either human breast cancer (HBC) cell lines [6] or the famous leucemia data set [10] are considered. © Springer-Verlag Berlin Heidelberg 2007.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploiting blind matrix decomposition techniques to identify diagnostic marker genes",
        "doc_scopus_id": "38149066889",
        "doc_doi": "10.1007/978-3-540-74695-9_9",
        "doc_eid": "2-s2.0-38149066889",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Blind matrix decomposition",
            "Diagnostic marker",
            "Human breast cancer (HBC)"
        ],
        "doc_abstract": "Exploratory matrix factorization methods like ICA and LNMF are applied to identify marker genes and classify gene expression data sets into different categories for diagnostic purposes or group genes into functional categories for further investigation of related regulatory pathways. Gene expression levels of either human breast cancer (HBC) cell lines [5] mediating bone metastasis or cell lines from Niemann Pick C patients monitoring monocyte - macrophage differentiation are considered. © Springer-Verlag Berlin Heidelberg 2007.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Greedy KPCA in biomedical signal processing",
        "doc_scopus_id": "38149027209",
        "doc_doi": "10.1007/978-3-540-74695-9_50",
        "doc_eid": "2-s2.0-38149027209",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical signals",
            "High-amplitude artifact",
            "Kernel Principal Component Analysis(KPCA)",
            "Multi-dimensional signal vectors"
        ],
        "doc_abstract": "Biomedical signals are generally contaminated with artifacts and noise. In case artifacts dominate, the useful signal can easily be extracted with projective subspace techniques. Then, biomedical signals which often represent one dimensional time series, need to be transformed to multi-dimensional signal vectors for the latter techniques to be applicable. In this work we propose the application of a greedy kernel Principal Component Analysis(KPCA) which allows to decompose the multidimensional vectors into components, and we will show that the one related with the largest eigenvalues correspond to an high-amplitude artifact that can be subtracted from the original. © Springer-Verlag Berlin Heidelberg 2007.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Blind matrix decomposition via genetic optimization of sparseness and nonnegativity constraints",
        "doc_scopus_id": "38149008287",
        "doc_doi": "10.1007/978-3-540-74690-4_81",
        "doc_eid": "2-s2.0-38149008287",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Genetic optimization",
            "Local minimums",
            "Matrix decomposition",
            "Microarray data",
            "Multivariate data",
            "Non-negativity constraints",
            "Nonnegative matrix factorization",
            "Target functions"
        ],
        "doc_abstract": "Nonnegative Matrix Factorization (NMF) has proven to be a useful tool for the analysis of nonnegative multivariate data. However, it is known not to lead to unique results when applied to nonnegative Blind Source Separation (BSS) problems. In this paper we present first results of an extension to the NMF algorithm which solves the BSS problem when the underlying sources are sufficiently sparse. As the proposed target function is discontinuous and possesses many local minima, we use a genetic algorithm for its minimization. Application to a microarray data set will be considered also. © Springer-Verlag Berlin Heidelberg 2007.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Nonlinear projective techniques to extract artifacts in biomedical signals",
        "doc_scopus_id": "84862632240",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84862632240",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Artificial data",
            "Biomedical signal",
            "Electro-oculogram",
            "Electroencephalogram signals",
            "Feature vectors",
            "Multidimensional signals",
            "One-dimensional time series",
            "Projective subspace",
            "Random noise signals",
            "Singular spectrum analysis",
            "Subspace projection",
            "Undistorted signals"
        ],
        "doc_abstract": "Biomedical signals are generally contaminated with artifacts and noise. In case the artifacts dominate, the useful signal can easily be extracted with projective subspace techniques. Then, biomedical signals which often represent one dimensional time series, need to be transformed to multidimensional signal vectors for the latter techniques to be applicable. The transformation can be achieved by embedding an observed signal in its delayed coordinates. Using this embedding we propose to cluster the resulting feature vectors and apply a singular spectrum analysis (SSA) locally in each cluster to recover the undistorted signals. We also compare the reconstructed signals to results obtained with kernel-PCA. Both nonlinear subspace projection techniques are applied to artificial data to demonstrate the suppression of random noise signals as well as to an electroencephalogram (EEG) signal recorded in the frontal channel to extract its prominent electrooculogram (EOG) interference.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extraction and separation of high-amplitude artifacts in electroencephalograms from epileptic patients",
        "doc_scopus_id": "33847202260",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33847202260",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Artifacts",
            "Artifacts removal",
            "Electrooculogram",
            "Singular spectrum analysis (SSA)"
        ],
        "doc_abstract": "Electroencephalogram (EEG) recordings are often distorted by high-amplitude artifacts which hamper its correct visual inspection. In this work we present a method which can be applied separately to each channel to extract high-amplitude components. The method is called local singular spectrum analysis (SSA) and is a principal component analysis in clusters formed after embedding the signals in their time-delayed coordinates. The extracted signal can be subtracted from the original channel resulting in a corrected EEG version. The algorithm is applied to real EEG segments containing paroxysmal epileptiform activity contaminated by artifactual activity. The extracted artifact as well as the corrected EEG will be presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Analyzing gene expression profiles with ICA",
        "doc_scopus_id": "33847182099",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33847182099",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Gene expression",
            "Gene transcript",
            "Microarray technology",
            "Superposition"
        ],
        "doc_abstract": "High-throughput genome-wide measurements of gene transcript levels have become available with the recent development of microarray technology. Intelligent and efficient mathematical and computational analysis tools are needed to read and interpret the information content buried in those large scale gene expression patterns at various levels of resolution. But the development of such methods is still in its infancy. Modern machine learning and data mining techniques based on information theory, like independent component analysis (ICA), consider gene expression patterns as a superposition of independent expression modes which are considered putative independent biological processes. We focus on two widely used ICA algorithms to blindly decompose gene expression profiles into independent component profiles representing underlying biological processes. These exploratory methods will be capable of detecting similarity, locally or globally, in gene expression patterns and help to group genes into functional categories - for example, genes that are expressed to a greater or lesser extent in response to a drug or an existing disease.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic removal of high-amplitude artefacts from single-channel electroencephalograms",
        "doc_scopus_id": "33746877580",
        "doc_doi": "10.1016/j.cmpb.2006.06.003",
        "doc_eid": "2-s2.0-33746877580",
        "doc_date": "2006-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Electrooculogram (EOG)",
            "Embedding",
            "Paroxysmal epileptiform activity",
            "Singular spectrum analysis (SSA)"
        ],
        "doc_abstract": "In this work, we present a method to extract high-amplitude artefacts from single channel electroencephalogram (EEG) signals. The method is called local singular spectrum analysis (local SSA). It is based on a principal component analysis (PCA) applied to clusters of the multidimensional signals obtained after embedding the signals in their time-delayed coordinates. The decomposition of the multidimensional signals in each cluster is achieved by relating the largest eigenvalues with the large amplitude artefact component of the embedded signal. Then by reverting the clustering and embedding processes, the high-amplitude artefact can be extracted. Subtracting it from the original signal a corrected EEG signal results. The algorithm is applied to segments of real EEG recordings containing paroxysmal epileptiform activity contaminated by large EOG artefacts. We will show that the method can be applied also in parallel to correct all channels that present high-amplitude artefacts like ocular movement interferences or high-amplitude low frequency baseline drifts. The extracted artefacts as well as the corrected EEG will be presented. © 2006 Elsevier Ireland Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2006-07-31 2006-07-31 2010-03-28T00:03:04 S0169-2607(06)00128-3 S0169260706001283 10.1016/j.cmpb.2006.06.003 S300 S300.1 FULL-TEXT 2015-05-14T05:25:39.534626-04:00 0 0 20060801 20060831 2006 2006-07-31T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0169-2607 01692607 83 83 2 2 Volume 83, Issue 2 7 125 138 125 138 200608 August 2006 2006-08-01 2006-08-31 2006 Section I: Methodology article fla Copyright © 2006 Elsevier Ireland Ltd. All rights reserved. AUTOMATICREMOVALHIGHAMPLITUDEARTEFACTSSINGLECHANNELELECTROENCEPHALOGRAMS TEIXEIRA A 1 Introduction 2 Background 3 Singular spectrum analysis and denoising 3.1 Embedding and SSA analysis 3.2 Local SSA 4 The parameters of the local SSA algorithm 4.1 Embedding dimension and number of clusters 4.2 MDL criterion 5 Results and discussion 5.1 Artificial signals 5.1.1 Parameters of SSA 5.1.2 Parameters of local SSA 5.2 EEG analysis 5.2.1 SSA and local SSA in EEG analysis 5.2.1.1 Analysis in frequency domain 5.2.2 Multichannel EEG analysis 6 Conclusions Acknowledgements References GRATTON 1998 44 53 G HE 2004 407 412 P VIGARIO 1997 395 404 R JUNG 2000 1745 1758 T JUNG 2000 163 178 T JOYCE 2004 313 325 C URRESTARAZU 2004 1071 1078 E ZHOU 2005 409 414 W WALLSTROM 2004 105 119 G GHIL 2002 3.1 341.341 M JAMES 2003 89 104 C TOME 2005 400 421 A LIAVAS 2001 1689 1695 A TEIXEIRA 2005 2514 2519 A IJCNN2005IEEE USECLUSTERINGLOCALSINGULARSPECTRUMANALYSISREMOVEOCULARARTIFACTS BISHOP 1995 C NEURALNETWORKSFORPATTERNRECOGNITION LEONOWICZ 2004 Z INTERNATIONALWORKSHOPCOMPUTATIONALPROBLEMSELECTRICALENGINEERING MODELORDERSELECTIONCRITERIACOMPARATIVESTUDYAPPLICATIONS DELORME 2004 9 21 A OPPENHEIM 1975 A DIGITALSIGNALPROCESSING YIOU 2000 254 290 P TEIXEIRAX2006X125 TEIXEIRAX2006X125X138 TEIXEIRAX2006X125XA TEIXEIRAX2006X125X138XA item S0169-2607(06)00128-3 S0169260706001283 10.1016/j.cmpb.2006.06.003 271322 2010-09-19T17:51:09.658821-04:00 2006-08-01 2006-08-31 true 1807896 MAIN 14 52263 849 656 IMAGE-WEB-PDF 1 si7 765 40 147 si61 1850 49 454 si6 3056 127 283 si58 570 18 123 si56 352 18 56 si55 1379 34 389 si5 341 14 105 si47 743 17 305 si36 523 18 142 si33 197 15 30 si27 341 14 105 si2 277 14 53 si18 736 34 159 si14 419 19 98 si12 151 13 18 si99 201 14 39 si98 171 14 38 si97 248 11 55 si96 278 11 62 si95 210 13 29 si94 248 11 55 si93 248 11 55 si92 227 11 54 si91 202 14 28 si90 198 13 27 si9 181 11 39 si89 278 11 62 si88 315 11 70 si87 202 14 28 si86 188 14 39 si85 201 14 39 si84 171 14 38 si83 227 11 54 si82 278 11 62 si81 210 13 29 si80 171 14 38 si8 368 18 112 si79 248 11 55 si78 227 11 54 si77 278 11 62 si76 468 13 124 si75 368 14 78 si74 198 13 27 si73 202 14 28 si72 198 13 27 si71 202 14 28 si70 771 14 234 si69 278 11 62 si68 315 11 70 si67 1265 39 398 si66 350 14 130 si65 335 14 72 si64 166 14 15 si63 378 14 91 si62 257 18 42 si60 252 14 55 si59 142 15 9 si57 142 15 9 si54 148 17 11 si53 214 11 45 si52 143 14 17 si51 136 10 14 si50 219 13 46 si49 335 14 108 si48 378 14 91 si46 210 16 30 si45 135 14 13 si44 150 14 14 si43 323 14 71 si42 144 15 13 si41 210 13 29 si40 129 10 12 si4 358 13 109 si39 144 15 13 si38 182 16 21 si37 210 19 30 si35 129 10 12 si34 136 11 13 si32 282 12 89 si31 206 14 41 si30 136 11 13 si3 241 11 51 si29 170 14 23 si28 136 11 13 si26 198 13 27 si25 469 14 124 si24 188 18 20 si23 188 18 20 si22 144 15 13 si21 188 18 20 si20 136 11 13 si19 144 15 13 si17 131 11 12 si16 207 11 46 si15 131 12 10 si132 140 11 16 si131 125 11 9 si130 114 8 12 si13 255 11 52 si129 117 8 12 si128 210 13 29 si127 210 13 29 si126 307 14 71 si125 196 10 31 si124 239 11 54 si123 295 11 68 si122 183 14 39 si121 117 8 12 si120 126 11 13 si119 108 7 9 si118 118 11 13 si117 110 7 9 si116 183 14 39 si115 199 14 40 si114 239 11 54 si113 126 11 13 si112 108 7 9 si111 118 11 13 si110 183 14 39 si11 189 11 42 si109 199 14 40 si108 167 15 21 si107 167 15 21 si106 248 11 55 si105 227 11 54 si104 248 11 55 si103 227 11 54 si102 248 11 55 si101 227 11 54 si100 188 14 39 si10 109 11 8 si1 633 14 202 gr12 210737 782 748 gr12 2571 93 89 gr3 25828 267 370 gr3 2279 90 125 gr1 30542 341 484 gr1 1816 88 125 gr13 213227 789 747 gr13 2353 93 88 gr5 37789 349 500 gr5 1964 87 125 gr2 33598 349 472 gr2 1915 93 125 gr4 37283 348 500 gr4 2074 87 125 gr7 34664 243 703 gr7 1069 43 125 gr6 58773 422 717 gr6 1707 74 125 gr9 31539 215 682 gr9 967 39 125 gr11 118555 374 760 gr11 2472 62 125 gr8 28855 267 357 gr8 2199 93 125 gr10 202166 723 686 gr10 2869 93 88 COMM 2547 S0169-2607(06)00128-3 10.1016/j.cmpb.2006.06.003 Elsevier Ireland Ltd Fig. 1 Three artificial signals and their respective frequency contents. Top: sinusoid, middle: funny curve and bottom: sawtooth. Fig. 2 The artificial signals with additive Gaussian noise. Left: SNR = 20 dB and right: SNR = 5 dB. Fig. 3 Relation between the mean square error (MSE) and the embedding dimension (M) for different signal periodicities (P). The actual values of the MSEs for a given period (P) were centered around zero to ease visual comparison. Fig. 4 Signals x ˆ [ n ] extracted with local SSA using the parameters: SNR = 5 dB, M = 11 . Left: q = 1 (SSA), middle: q = 3 and right: q = 5 . Fig. 5 Signals x ˆ [ n ] extracted with local SSA using the parameters: SNR = 5 dB, M = 36 . Left: q = 1 (SSA), middle: q = 3 and right: q = 5 . Fig. 6 (a–f) Funny curve: eigenvalue plot and MDL selection (dotted line) in each of the three clusters. Top M = 11 where in (c) all directions are selected and bottom M = 36 . Fig. 7 (a and b) Extracted artefact and corrected EEG signal obtained with local SSA using six clusters. Top: original EEG, middle: extracted EOG and bottom: corrected EEG. Fig. 8 Power spectral density(dB/Hz) vs. frequency (Hz) of original signal (full line), corrected EEG signal using local SSA—six clusters ( □ ), four clusters( • ) and SSA ( △ ). Fig. 9 (a–c) Energy along the segment: original EEG ( ⋆ ), corrected EEG signal using local SSA—six clusters ( □ ), corrected EEG signal using local SSA—four clusters ( • ), corrected EEG using SSA ( △ ). Fig. 10 (a–c) First segment of EEG signal recordings using Cz as reference electrode. The extracted artefacts and the corrected signals are shown only for the frame indicated. Fig. 11 Detailed view of the first segment around 53 s shown on an expanded scale. Left: original EEG and right: corrected EEG. Fig. 12 (a–c) Second segment of EEG signals recordings using Cz as reference electrode. The extracted artefacts and the corrected signals are shown only for the frame indicated. Fig. 13 (a–c) Third segment of EEG signals recordings using Cz as reference electrode. The extracted artefacts and the corrected signals are shown only for the frame indicated. Table 1 MSE between original and extracted periodic waves when SNR = 5 dB using different numbers of clusters (q) Sinusoid Funny Sawtooth P = 26 a P = 24 a P = 24 a q = 1 q = 3 q = 5 q = 1 q = 3 q = 5 q = 1 q = 3 q = 5 M = 11 0.041 0.031 0.317 0.310 0.091 0.045 0.177 0.076 0.206 M = 36 0.012 0.006 0.009 0.758 0.041 0.031 0.219 0.052 0.025 The MDL is used to choose the number of directions. Note that q = 1 corresponds to plain SSA. a Period. Table 2 Subspace dimension k and number of samples N c i in each cluster Label Dimension (k) N c i (SSA) 1 9 1624 Local SSA ( q = 4 ) 1 7 101 2 7 956 3 7 132 4 8 435 Local SSA ( q = 6 ) 1 6 128 2 10 112 3 5 754 4 7 208 5 8 324 6 10 98 Table 3 Number of clusters in each processed channel (“–” indicates not processed) EEG channels Number of clusters [-6pt] First segment Second segment Third segment Fp2–Cz 6 10 8 F4–Cz 3 2 6 C4–Cz 3 – 7 P4–Cz 3 – 8 O2–Cz 7 – – F8–Cz 6 6 3 T4–Cz 5 6 – T6–Cz 3 – 10 Fp1–Cz 6 5 7 F3–Cz 6 7 7 P3–Cz 3 – – O1–Cz 3 – – F7–Cz 3 8 4 T3–Cz 6 – – T5–Cz 6 – – Automatic removal of high-amplitude artefacts from single-channel electroencephalograms A.R. Teixeira a A.M. Tomé a ⁎ E.W. Lang b P. Gruber b A. Martins da Silva c a Departamento Electrónica, Telecomunicações e Informática/IEETA, Universidade Aveiro, 3810-193 Aveiro, Portugal b Institute of Biophysics, University of Regensburg, D-93040 Regensburg, Germany c HGSA and ICBAS/IBMC, University of Porto, 4099-001 Porto, Portugal ⁎ Corresponding author. Tel.: +351 234 370512. In this work, we present a method to extract high-amplitude artefacts from single channel electroencephalogram (EEG) signals. The method is called local singular spectrum analysis (local SSA). It is based on a principal component analysis (PCA) applied to clusters of the multidimensional signals obtained after embedding the signals in their time-delayed coordinates. The decomposition of the multidimensional signals in each cluster is achieved by relating the largest eigenvalues with the large amplitude artefact component of the embedded signal. Then by reverting the clustering and embedding processes, the high-amplitude artefact can be extracted. Subtracting it from the original signal a corrected EEG signal results. The algorithm is applied to segments of real EEG recordings containing paroxysmal epileptiform activity contaminated by large EOG artefacts. We will show that the method can be applied also in parallel to correct all channels that present high-amplitude artefacts like ocular movement interferences or high-amplitude low frequency baseline drifts. The extracted artefacts as well as the corrected EEG will be presented. Keywords Singular spectrum analysis (SSA) Embedding Principal component analysis Electrooculogram (EOG) Electroencephalogram (EEG) 1 Introduction Electroencephalographic recordings can be contaminated by eye blinking, head movements, muscle activity, heart beat and line noise. These artefacts pose a problem to the interpretation of recorded EEG signals, because in many cases they constitute the most prominent signals in terms of amplitudes. In particular, ocular activity represents the major source of artefacts in electroencephalogram (EEG) signals, especially when recorded from frontal channels. Such ocular artefacts are labeled electrooculograms (EOG). In fact, especially when measured at frontal locations of the scalp close to the eyes, the EOG signal amplitude can be several times larger than the brain-generated scalp potentials. Eye movements and blinks are very frequent and inevitably will occur during EEG recordings while objects perform various tasks. To reduce the presence of such disturbing eye movement activity in EEG recordings, the subject is often asked to suppress eye blinking or to fixate the eyes onto a given target. However, this goal is never fully accomplished either because of the nature of the task to be examined or because the subject is not willing or able to cooperate. Consequently, in studies related with single-trial event related potentials (ERP), data from frontal channels are often discarded at all. In others it is common to discard segments of the recorded EEG which are contaminated with ocular activities. This artefact rejection is commonly done either by visual inspection of the recordings or by setting some automatic detection criterion like eliminating the segments which achieve an amplitude higher than a predefined threshold. The detection can be accomplished either on-line or off-line (after the recording session). Naturally, the performance depends on the criterion used. A fixed threshold, for example, will generally not accomplish the variability that characterizes most of the signals during recording sessions. The advantage of on-line detection lies in the possibility of additional recordings (at least in some evoked potential response tests) to compensate for the loss of data due to artefacts. Even though, discarding segments of EEG can lead to a significant loss of information which in some cases compromises the significance of the study either because it has not enough data or the artefact-free trials represent a biased measure of the recording session. A detailed review of reduction strategies in evoked potential studies can be found in Ref. [1]. In continuous recording sessions, like the ones resulting from long monitoring sessions studying epilepsy, huge artefacts are also present and constitute a serious problem for the visual inspection of the recordings. Because of their large amplitudes, artefacts resulting from eye movements are often masking the onset of focal seizures. 2 Background The availability of digital EEG recordings allows the investigation of procedures trying to remove artefact components from the recorded brain signals. The primary goal will be to remove such superimposed artefacts without distorting the underlying brain signals. A variety of automatic procedures have been proposed in the literature to correct or remove ocular artefacts from EEG recordings. Some techniques are based on regression analysis, adaptive filtering techniques [2], principal component analysis, and more recently independent component analysis (ICA) [3–5] or other blind source separation techniques [6]. The traditional method is regression analysis which basically consists in the subtraction of the scaled EOG channel (or horizontal and vertical EOG recording channels) from the EEG signal. The most recent works use independent component analysis: Jung et al. [5] used the INFOMAX-algorithm, Urrestarazu et al. [7] and Zhou and Gotman [8] applied the joint approximate diagonalization of eigen-matrices algorithm (JADE), in Ref. [6], an approximate joint diagonalization of time-delayed correlation matrices (SOBI) was used while in Ref. [3], the fast fixed point algorithm (FASTICA) was applied. In all the works but [7], the EOG channels were included in the processed data set of signals though Vigário [3] had argued that the computation of the independent components can be achieved without the inclusion of EOG signals. One important issue in ICA methods applied to EEG recordings is the identification of components related with ocular artefacts. Hence, in order to reconstruct the data without artefacts, it is needed to eliminate the components related with the artefacts. Most of the works did not give any emphasis to the task of identifying artefact-related components which seems to be achieved mostly in a visual/manual manner. Despite the variety of methods applied, it is not possible to conclude about their performance once they use distinct databases, different measures and goals. In Ref. [4], the authors conclude that ICA performed better than PCA, where some remnants of the electrooculogram were still visible in the corrected data. In Ref. [9] the performance of regression methods, principal component analysis and an independent component analysis method were compared using real and simulated data and the authors conclude that ICA distorts the power of the EEG signal in the range of 5–20 Hz. In this work, we will present a method based on singular spectrum analysis [10,11] to remove artefacts from EEG recordings. SSA as well as local SSA, the modification we are proposing in this work, considers univariate signals, hence in case of EEGs needs as input single channel recordings only, contrary to the methods discussed above. This is a definite advantage as artefacts appear different in different channels, some even may not contain artefacts at all in certain segments. Hence, artefacts can be processed more specifically in each channel if needed. Another advantage concerns the identification of artefact related components in projection methods which generally can become very tedious in methods like ICA. With local SSA there is a natural assignment of high-amplitude artefacts to signal components associated with the largest eigenvalues of the decomposition. Further local SSA does not need a proper reference signal, a separately recorded EOG signal, for example, as regression and adaptive filtering methods do. Also note that these reference signals never provide a pure reference to the artefact, as EOG signals, for example, always contain EEG contaminations also. Last but not least, the proposed method is simple to implement and robust. However, the method discussed cannot take advantage of available topographic information as methods can do which process all channels together. The outline of the paper is as following: in next section, the SSA methodology is briefly explained, then the local SSA method is detailed. The implementation of the algorithm depends on the assignment of some parameters; Section 4 will discuss robust strategies to choose their values. In Section 5 simulations with artificial signals help to clarify the influence of these parameters on the performance of the algorithm. The method is then applied to single channel EEG recordings. It splits the recorded signal into two components: artefacts and undistorted EEG signals. In fact, the method identifies high-amplitude artefacts as the “signal” and the underlying, undistorted EEG signal as the “noise” component. Finally, the method is also applied in parallel to a set of channels containing paroxysmal epileptiform activity with high-amplitude artefacts to extract the latter simultaneously from all channels and reveal the undistorted EEG signals. 3 Singular spectrum analysis and denoising In many signal processing applications sensor signals are contaminated with noise. The latter is generally assumed to be additive and non-correlated with the sensor signals. The general purpose of SSA analysis is the decomposition of a time series into additive components which can be interpreted as “trends”, “oscillatory” and “noise” components. The SSA strategy is widely used in climatic, meteorologic and geophysics data analysis [10,11]. SSA relies on the embedding of a sensor signal in the high-dimensional space of its time-delayed coordinates thereby creating what is called a signal trajectory matrix. The multidimensional signal obtained after embedding is projected onto the most significant directions computed using singular value decomposition (SVD) or principal component analysis (PCA) techniques. An embedding strategy was also used in conjunction with FastICA to decompose recordings from a single EEG channel into distinct components [12]. Embedding multidimensional signals in a feature space spanned by delayed coordinates followed by a diagonalization of time-delayed correlation matrices is also a technique to blindly extract filtered versions of underlying source signals [13]. In this work, after embedding the signal in delayed coordinates, we introduce into SSA a clustering step which groups together similar columns of the trajectory matrix. After having applied PCA to each cluster, the multidimensional data are projected locally into a subspace spanned by k eigenvectors associated with the k most significant eigenvalues. The choice of the number k of components is based on an application of the minimum description length (MDL) criterion [14]. In next sections, we will describe the main steps of the basic SSA analysis and then introduce the additional steps of our modified version of singular spectrum analysis, called local SSA (note that this term is also used by Yiou et al. [20] in a different context). Finally, the MDL principle to choose the number of significant eigenvalues, thus estimating the dimension of the “signal subspace”, is presented. 3.1 Embedding and SSA analysis Embedding can be regarded as a mapping that transforms a one-dimensional time series x = ( x [ 0 ] , x [ 1 ] , … , x [ N − 1 ] ) into a multidimensional sequence of lagged vectors. Let M ∈ N denote a window length with M < N . The embedding procedure forms L = N − M + 1 multivariate vectors, x l , l = 1 , … , L , which constitute the columns of the trajectory matrix (1) X = x [ M − 1 ] x [ M ] ⋯ x [ N − 1 ] x [ M − 2 ] x [ M − 1 ] ⋯ x [ N − 2 ] x [ M − 3 ] x [ M − 2 ] ⋯ x [ N − 3 ] ⋮ ⋮ ⋯ ⋮ x [ 0 ] x [ 1 ] ⋯ x [ N − M ] Note that the trajectory matrix has identical entries along its diagonals (Toeplitz matrix). The multidimensional signal vectors can be centered in the embedding space by computing (2) X c = X I − 1 L j L j L T where j L = ( 1 , … , 1 ) T is an L × 1 vector of unit values, and I represents an L × L identity matrix. With the centered trajectory matrix X c an eigendecomposition of the M × M covariance matrix S = < X c X c T > is computed. In SSA analysis the strategy to choose the eigenvectors of S in order to project and reconstruct the multidimensional signal is called grouping [10]. The choice of eigenvectors depends on the goal of the analysis [11]. For example, denoising can be achieved by projecting the M-dimensional signal onto the k < M eigenvectors corresponding to the k largest eigenvalues. Then a “noise-free” signal can be obtained after reconstruction. Therefore, considering the matrix U with k eigenvectors corresponding to its columns, the denoised multidimensional signal is obtained via (3) X ˆ = U U T X c + X 1 L j L j L T Notice that it cannot in general be expected that X ˆ possesses identical elements along each descending diagonal with identical values like in case of X (Eq. (1)). However, this can be accomplished by replacing the entries in each diagonal by their average along the diagonal to form the matrix X ˆ d . This procedure assures that the Frobenius norm of the difference between the original matrix, X ˆ , and the transformed matrix, X ˆ d , has minimum value among all possible solutions to get a matrix with all diagonals equal. Finally, the denoised one-dimensional signal is obtained by reverting the embedding procedure, i.e. by taking a sample from each diagonal of the matrix X ˆ d . 3.2 Local SSA In SSA, any time series like an EEG recording is considered a superposition of underlying informative signal components. For instance, if the signal is assumed to be contaminated with a non-correlated additive gaussian noise, the following model is considered: x [ n ] = y [ n ] + r [ n ] , where r [ n ] represents additive Gausssian white noise. The embedding step turns this univariate sequence into a multivariate signal vector. With respect to a projective subspace denoising, this reduction of the noise level of the recorded signal becomes a non-linear operation thereby. The clustering step, introduced in local SSA after the embedding step, serves to approximate this non-linear processing by a locally linear processing (the PCA or SVD) by choosing directions of maximum variance in sub-groups of column vectors of the trajectory matrix. The necessary modifications of plain SSA are explained in detail in Ref. [15]. Basically it introduces a clustering step into the SSA technique and uses an MDL criterion to choose the signal-related, uncorrelated components in each cluster. For convenience we summarize the necessary steps of the whole procedure: • After embedding, the multivariate signals x l , l = 1 , … , L , representing the columns of the trajectory matrix X , are grouped together into k clusters using any appropriate clustering algorithm like k-means [16]. After clustering, the set of indices { c l } indexing the columns of X is subdivided into q < L disjoint subsets c 1 , c 2 , … , c q . Thus, any sub-trajectory matrix X ( c i ) is formed with those columns of the trajectory matrix X which belong to the subset c i of indices. • A covariance matrix C ( c i ) = 〈 X ( c i ) ( X ( c i ) ) T 〉 is computed in each cluster and its eigenvectors and eigenvalues are determined. To achieve denoising, the data is projected onto the eigenvectors which correspond to the k largest eigenvalues. Applying an MDL criterion (see next section), the number k of significant directions is estimated and can be different in each cluster. After denoising a sub-trajectory matrix X ˆ ( c i ) is reconstructed within each cluster. The further processing is very similar to the one described by Eq. (3). • The clustering is reverted, i.e. each column of the extracted sub-trajectory matrix X ˆ c i will be assigned to a column of X ˆ according to the contents of subset c i . • The reconstructed one-dimensional signal x ˆ [ n ] is obtained by reverting the embedding, i.e. by averaging over the entries of the corresponding descending diagonals of the reconstructed trajectory matrix X ˆ . 4 The parameters of the local SSA algorithm The implementation of the algorithm as described in the last section requires the assignment of the following parameters: the embedding dimension (M) and the number (q) of clusters to split the columns of the trajectory matrix. A third parameter (k), representing the signal subspace in each cluster, can be assigned automatically using an MDL criterion. In the following we discuss heuristics to estimate the embedding dimension M and the number of clusters q. Furthermore, we discuss an MDL criterion to estimate the signal subspace dimension k. It turns out that the parameters of the model are easy to estimate and well justified. 4.1 Embedding dimension and number of clusters In SSA applications the choice of an embedding dimension M has to be considered. If no further knowledge is available, M should be chosen approximately half of the segment length (N) [10]. To extract periodic signal components, M should be close to their periodicity [10]. A more general strategy is followed in Ref. [12] where a lower bound is suggested according to the frequency resolution contained in every column of the trajectory matrix, i.e. M > f s / f r where f s represents the sampling frequency and f r is the minimum frequency which is to be extracted. In local SSA the number q of clusters has to be assigned also. Obviously, the number of samples N constitutes a natural upper bound. A more practical heuristic chooses q according to the number of vectors resulting in each cluster. In particular, the cardinality of each cluster cannot be lower than the embedding dimension M. 4.2 MDL criterion The determination of the number of significant directions is based on the application of a maximum likelihood estimation of the parameter vector of the covariance matrix C ( c i ) of each cluster. This parameter vector is given by θ k = ( λ 1 > λ 2 > ⋯ > λ k , σ 2 , u 1 , u 2 , … , u k ) representing k ( M + 1 ) + 1 parameters in total. The parameters λ i , i = 1 , … , k represent the k ≤ M largest eigenvalues of the covariance matrix, u i the corresponding eigenvectors and σ 2 corresponds to the mean of the M − k discarded eigenvalues. Using the maximum likelihood estimate of θ ˆ , then k will be the value that minimizes the following expression (4) MDL ( k ) = − ln ⁡ f ( X c i | θ k ˆ ) + 1 2 K ln ⁡ N , k = 1 , … , M − 1 where N is the number of observations available to estimate the covariance matrix and f ( X c i | θ ˆ ) denotes the conditional probability density parameterized by θ ˆ . This log likelihood function L ( θ ˆ ) = ln ⁡ f ( X c i | θ ˆ ) represents the accuracy of the representation of the data with the parameter vector θ ˆ and depends on the ( M − k ) discarded eigenvalues (5) L ( θ k ˆ ) = N ( M − k ) ln ⁡ ∏ i = k + 1 M λ i 1 / ( M − k ) 1 / ( M − k ) ∑ i = k + 1 M λ i , k = 1 , … , M − 1 The negative log-likelihood − L ( θ ˆ ) is recognized to be a standard measure of the training error. However, it has been reported that the simple maximization of this term tends to result in the phenomenon of over-fitting. Thus, the second term in Eq. (4) was added as a regularization term to penalize complexity. The value of K is related with the number k ( M + 1 ) + 1 of parameters represented by the parameter vector θ k . However, the actual number of degrees of freedom is reduced by k ( k + 1 ) / 2 because of the normalization and orthogonality constraints imposed onto the eigenvectors of the covariance matrix. Considering real valued signals, the value of K is computed via [14] ( k = 1 , … , M − 1 ) (6) K = [ k ( M + 1 ) + 1 ] − 1 2 k ( k + 1 ) = k M − 1 2 k ( k − 1 ) + 1 To elucidate the impact of the choice of parameters onto the performance of the algorithm, the next section first discusses some toy examples containing essential features of EEG data. 5 Results and discussion The results present and discuss artificial signals as well as EEG recordings. The aim of our experimental study with artificial signals is to demonstrate the performance of plain SSA and local SSA algorithms in dependence on the choice of the their parameters: embedding dimension M, number of cluster q and signal subspace dimension k. The latter is assigned by applying an MDL criterion. Then results obtained by applying local SSA to segments of a frontal EEG signal recording are presented. The influence of the number of clusters on the performance of the algorithm is illustrated as well. Finally, the analysis is extended to a multichannel EEG recording where the algorithm is applied in parallel to a subset of EEG channels. We can see that both the extracted signal and the corrected EEG signal provide useful information helping a visual inspection of problematic segments with high-amplitude artefacts. 5.1 Artificial signals In order to achieve conclusive insight into the choice of the free parameters of the algorithm some experiments were realized using artificial signals (see Fig. 1 ). The latter are represented by x [ n ] = y [ n ] + r [ n ] , n = 0 , … , 499 , where y [ n ] is a zero-mean periodic signal and r [ n ] is a Gaussian white noise uncorrelated with the signal. The variance of the latter was chosen such that the ratio of energies (SNR) between y [ n ] and r [ n ] resulted to either 20 dB (high SNR) or 5 dB (low SNR) (see Fig. 2 ). Note that using Gaussian noise is justified only by its simplicity. A more realistic approach would model colored noise with a frequency distribution similar to real EEGs. The periodic signals were selected with distinct frequency characteristics: (a) a narrow band signal (sinusoid), (b) a signal with energy only in a low frequency band (sawtooth) and (c) a signal whose contents spreads all over the frequency band (funny curve). These situations are sufficiently general to reflect essential features of real EEG recordings yet simple enough to ease a systematic investigation of the effect of the parameters of the method. The latter is described in Section 3.2 where the extracted signal represents a periodic wave, x ˆ [ n ] ≃ y [ n ] . The residual might approximate Gaussian noise, i.e. r ˆ [ n ] = x [ n ] − x ˆ [ n ] . 5.1.1 Parameters of SSA The heuristic rules proposed in the literature to choose the embedding dimension M point towards a minimum value that is related with the period of the signal to be extracted. The experiments confirm this heuristic under the following circumstances: (a) the extracted signal is wide-band and (b) the SNR is high, i.e. the extracted signal has an amplitude larger than the residual signal. Fig. 3 shows the dependence of the centered mean square error (MSE) on the embedding dimension (M) considering different periods of y [ n ] and a SNR = 20 dB. However, decreasing the value of the signal-to-noise ratio to SNR = 5 dB, a similar result, i.e. a clearcut drop in the MSE above a critical embedding dimension could not be verified. And even choosing an embedding dimension higher than the respective period of the signal under consideration, (Fig. 5, left column) did not help in extracting the wide-band signals correctly. 5.1.2 Parameters of local SSA The simulations show that the local SSA algorithm works better if the embedding dimension is chosen higher than the minimal dimension given by the above mentioned heuristics when the SNR is low (5 dB). With a low SNR, i.e. when the amplitude of r [ n ] is close to the amplitude y [ n ] of the periodic signal, local SSA performs better than plain SSA with the exception of narrow band signals like the sinusoid. Table 1 shows the MSE error for all cases using embedding dimensions lower ( M = 11 ) and higher ( M = 36 ) than the period of the respective waves considered. The extracted periodic waves are shown in Figs. 4 and 5 and the best results were obtained with local SSA with an embedding dimension of M = 36 . We verified that having a high embedding dimension favors the spread of the random noise signal into more directions and the MDL criterion yields a better estimate of the signal subspace. The poor results of Fig. 4(right column) mostly result because here the MDL criterion overestimates the signal subspace dimension by selecting all the possible directions in some (or all) clusters. Fig. 6 illustrates the phenomenon for the funny curve when M = 11 (top) and M = 36 (bottom). The following observations can be made: • With M = 11 , the logarithm of the eigenvalues decreases linearly, hence there is no grouping neither a clear gap, so all directions were selected. In fact, an experimental study with artificial data shows that the MDL criterion provides a more reliable estimation of the signal subspace if the number of samples to estimate the covariance matrix is large [15]. • The small eigenvalues to be associated with the noise signals must be clustered together and must be separated by a gap from the larger eigenvalues to obtain a reliable estimate of the signal subspace dimension. This confirms observations reported in other studies as well [17,14]. • For an embedding dimension larger than the period of the waves, like it is the case with M = 36 in Fig. 6, the performance of the algorithm increases with the number of clusters. However, the cardinality of each cluster constitutes a natural upper bound to the number of clusters. 5.2 EEG analysis The EEG signals were chosen from a database of epileptic patients recorded on long-term EEG monitoring sessions. The EEG signals were recorded using 19 electrodes placed according to the 10–20 system and mounted with a common ground reference at Fz. The signals are filtered and digitalized at a sampling rate of 128 Hz and stored as European data format (EDF), using an EEG Galileo recording system. Monopolar brain signals using the Cz electrode as reference were visualized using EEGLAB [18] and processed using SSA and local SSA methodologies. 5.2.1 SSA and local SSA in EEG analysis The first EEG example shows the results of the analysis of 13 s of a frontal channel (Fp1–Cz) recording with high-amplitude eye movements. The one-dimensional signal was embedded using M = 41 in all three experiments performed. We tested different numbers of clusters q in the clustering step of the local SSA algorithm and the signal subspace dimension was estimated using an MDL criterion. Table 2 exhibits the subspace dimension k assigned by MDL both for plain SSA and local SSA (with q = 4 and q = 6 ). A visual inspection of the extracted EOG using the algorithm and the corrected EEG (the difference between original EEG and the extracted EOG) by a clinical expert revealed no significant difference between the different versions computed for different numbers of clusters. It was also observed that the 50 Hz interference is consistently removed together with the EOG signal in all cases. Fig. 7 illustrates the difference between using either the MDL criterion to select the subspace dimension or using only one direction corresponding to the largest eigenvalue to reconstruct the EOG signal. In the second case the extracted signal corresponds to the EOG artefact but the corrected EEG still contains some remnants of the original EOG signal. This demonstrates that with EOG artefacts the dominant PC is not sufficient to represent the large amplitude artefact as it was the case with the water artefact in case of 2D NMR spectra [13]. 5.2.1.1 Analysis in frequency domain In the frequency domain the power spectral density computed by the Welch method [19] was considered. EEG studies usually concentrate on the frequency content of the following set of frequency bands: theta waves (3.5–7.5 Hz), alpha waves (7.5–13 Hz) and beta waves (13–25 Hz). We also compared instantaneous measures of the energy in some of those bands just to evaluate the differences between the corrected EEG and their original counterparts. The energy was estimated in segments of 2 s, windowed with a Hamming window, corresponding to a frequency resolution of 0.5 Hz with an overlap between adjacent windows of 50%. The power spectral density (psd) of all the signals confirms that the 50 Hz line noise is also present in the extracted EOG signal (see Fig. 8 ). We also verify that the logarithm of the power spectral density of the corrected EEG (residual signal) exhibits a drop in the low frequency band (corresponding to theta and alpha bands < 10 Hz), whereas substantial spectral density builds up in the original signal due to the presence of the low frequency EOG artefact. The beta band is very similar to the original EEG except in close proximity to the 50 Hz line noise. Comparing the log psd of the corrected EEG, calculated with different numbers of clusters, it is seen that the variation of the latter influences solely the low frequency band and that the decrease in psd diminishes with increasing q (see Fig. 8). Thus, low frequency signals are less affected at higher q-values. Nonetheless the figures shown demonstrate that the high-amplitude, low frequency artefact is extracted efficiently. Note that with q = 6 only below 3 Hz the psd is diminished strongly. But small amplitude transitory signals in this frequency range might not be affected as the local SSA method proposed relies on the fact that components related to the largest eigenvalues of the decomposition correspond to a high energy content. This is why, for example, the 50 Hz noise is removed. It represents a small amplitude signal which is present persistently in the recordings. Thus, enough energy is accumulated and this artefact becomes associated with a component corresponding to a large eigenvalue of the local SSA decomposition. These observations are corroborated by instantaneous energies estimated in the characteristic bands for the original signal versus the corrected EEG signal (see Fig. 9 ). The energy in the beta band is preserved in the corrected EEG while it is altered in the other bands. Again with an increasing number of clusters the difference in estimated energies between original and corrected signals is decreasing. 5.2.2 Multichannel EEG analysis The signals of the set of channels recorded along the monitoring session suffer from distinct forms of distortion. In particular, the high-amplitude interference arising from ocular movements are more visible in frontal channels, while electrode artefacts show up in various channels spread over the scalp. We will present results using three data segments (with N = 1280 samples) recorded from a patient which suffered from a partial complex seizure from the right temporal focus followed by a secondary generalization. The three segments correspond to EEG signals preceding the epileptic seizure onset and are corrupted by high-amplitude artefacts: the first segment starts 28 min before seizure onset, the second 24 min before and the last segment starts at seizure onset as verified by simultaneous video recordings of concomitant body movements. The analysis is performed in parallel in more than one channel using an embedding dimension of M = 41 . The number of clusters is automatically assigned in each channel using the given heuristics which aim to prevent over-fitting due to the MDL criterion but simultaneously uses the maximum number of clusters consistent with these heuristics. The simulations start with a maximal number of clusters q max ⁡ , checking afterwards if all clusters end up with a cardinality higher than M, in which case the signal subspace dimension in each cluster is chosen as k ≤ ( M / 2 ) . If both criteria are not met, then the number q of clusters is decreased and the process is repeated until a reliable decomposition in each cluster is achieved. Each channel is processed separately and Table 3 presents the number of clusters in each processed signal for the three processed segments. • Segment 1: All channels are processed one after the other by the algorithm (see Fig. 10 ). The corrected EEG x ˆ [ n ] (Fig. 10(b)) clearly exhibits the high-amplitude components of the original signals in an undistorted way. In most of the channels, an instantaneous frequency analysis (spectrogram) of x ˆ [ n ] reveals that the frequency contents is mainly in the low frequency range ( < 10 Hz) and also around 50 Hz. The corrected EEG (see Fig. 10(c)) mainly possesses the high frequency ( > 10 Hz) contents of the original signal. However, in T4 and T6 bursts of theta (3–7 Hz) waves and sharp slow waves can be seen to occur around 53 s. This region is zoomed out for convenience in Fig. 11 . The bursts of spikes is now also clearly visible in the frontal channels. • Segment 2: This segment shows typical eye movement artefacts most visible in the frontal channels. Then only these frontal channels and channel T4, monitoring temporal cortex, were processed. In Fig. 12 we can see 4 s of this analysis: the extracted signal is only related with the EOG artefact and the 50 Hz line noise (Fig. 12(b)) and the corrected EEG has the lower amplitude components of the signal. In T4 a burst of spikes (after 5 min 51 s) can be seen while in other channels (F4 and F8) single spikes also occur during the same period. Comparing the corrected T4 channel (Fig. 12(c)) with the corresponding channel before the seizure onset (Fig. 13(a or c)) we can verify that both exhibit a pronounced burst of spike waves. The paroxysmal activity in T4 before the seizure initiation indicates the possible origin of the the epileptogenic focus. It is now possible to compare the corrected signal recorded in the T4 channel (Fig. 12(c)) in this segment with the corresponding signal in the segment preceding the seizure onset (Fig. 13(a or c)). As both are substantially similar this can be taken as an indication of the epileptogenic focus. • Segment 3: This segment precedes the onset of a partial complex seizure followed by a generalization and shows paroxysmal epileptiform activity in the temporal right regions. The frontal channels show ocular artefacts. In addition, channels C4, P4 and T6 show electrode artefacts (a drift in baseline) of low frequency and high-amplitude. It can be verified in Fig. 13(b) that those artefacts as well as the 50 Hz could be removed. Furthermore, channels Fp1 and Fp2 of the corrected EEG (Fig. 13(c)) show spike waves that were masked by the high-amplitude artefacts in the original signal. The focus starts in T4 and T6 with spikes, shows bilateralization and later generalization. Note that channel T4, contrary to channel T6, has not been processed and shows bursts of spikes which show up along all the data segment. 6 Conclusions In this work, we presented a modified SSA method, called local SSA, to remove, from EEG recordings, high-amplitude and low frequency artefacts which accumulate enough energy to be associated with large eigenvalues of the eigendecomposition. We process the raw data to which no digital filtering has been applied. The method needs the information contained within a single channel only, hence can be applied to each channel separately. Thus, only channels which contain such artefacts need to be processed. Our results confirm that local SSA shows good performance in removing artefacts like eye or head movements, baseline drifts and line noise. In summary, with the method proposed we can separate EEG signal recordings into two components: artefacts and undistorted EEG. It has to be pointed out that local SSA does not require any user intervention to select the components of the reconstruction as in conventional ICA methods, for example. Furthermore, the user can choose to process a subset of channels keeping others unprocessed which also allows a comparison of the outcomes of the algorithm with non-processed channels. Although this is ongoing work, we present a method that is intended to help a visual inspection of the EEG recordings by an experienced clinician, hence might be useful in some critical segment analysis like the onset of ictal seizures. Acknowledgements This work was supported by grants from the DAAD and the CRUP as well as the DFG which is gratefully acknowledged. References [1] G. Gratton Dealing with artifacts: the EOG contamination of the event-related brain potential Behav. Res. Methods, Instrum. Comput. 30 1 1998 44 53 [2] P. He G. Wilson C. Russel Removal of ocular artifacts from electroencephalogram by adpative filtering Med. Biol. Eng. Comput. 42 2004 407 412 [3] R.N. Vigário Extraction of ocular artefacts from EEG using independent component analysis Electroencephalogr. Clin. Neurophysiol. 103 1997 395 404 [4] T.-P. Jung S. Makeig M. Westerfield J. Townsend E. Courchesne T.J. Sejnowski Removal of eye activity artifacts from visual event-related potentials in normal and clinical subjects Clin. Neurophysiol. 111 2000 1745 1758 [5] T.-P. Jung S. Makeig C. Humphries T.-W. Lee M.J. Mckeown V. Iragui T.J. Sejnowski Removing electroencephalographic artifacts by blind source separation Psychophysiology 37 2000 163 178 [6] C.A. Joyce I.F. Gorodniysky M. Kutas Automatic removal of eye movement and blink artifacts from EEG data using blind component separation Psychophysiology 41 2004 313 325 [7] E. Urrestarazu J. Iriarte M. Alegre M. Valencia C. Viteri J. Artieda Independent component analysis removing artifacts in ictal recordings Epilepsia 45 9 2004 1071 1078 [8] W. Zhou J. Gotman Removing eye-movement artifacts from the EEG during the intracarotid amobarbital procedure Epilepsia 46 3 2005 409 414 [9] G.L. Wallstrom R.E. Kass A. Miller J.F. Cohn N.A. Fox Automatic correction of ocular artifacts in the EEG: a comparison of regression and component-based methods Int. J. Psychophysiol. 53 2004 105 119 [10] N. Golyandina, V. Nekrutkin, A. Zhigljavsky, Analysis of Time Series Structure: SSA and Related Techniques, Chapman & HALL/CRC, 2001. [11] M. Ghil M. Allen M.D. Dettinger K. Ide Advanced spectral methods for climatic time series Rev. Geophys. 40 1 2002 3.1 341.341 [12] C.J. James D. Lowe Extracting multisource brain activity from a single electromagnetic channel Artif. Intell. Med. 28 2003 89 104 [13] A.M. Tomé A. Teixeira E.W. Lang K. Stadlthanner A. Rocha R. Almeida dAMUSE—a new tool for denoising and blind source separation Digital Signal Process. 15 4 2005 400 421 [14] A.P. Liavas P.A. Regalia On the behavior of information theoretic criteria for model order selection IEEE Trans. Signal Process. 49 8 2001 1689 1695 [15] A.R. Teixeira A.M. Tomé E.W. Lang P. Gruber On the use of clustering and local singular spectrum analysis to remove ocular artifacts from IJCNN2005, IEEE Montréal, Canada 2005 2514 2519 [16] C.M. Bishop Neural Networks for Pattern Recognition 1995 Oxford University Press Oxford [17] Z. Leonowicz J. Karvanen T. Tanaka J. Rezmer Model order selection criteria: comparative study and applications International Workshop Computational Problems of Electrical Engineering 2004 [18] A. Delorme S. Makeig EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics J. Neurosci. Methods 134 2004 9 21 [19] A.V. Oppenheim R.W. Schafer Digital Signal Processing 1975 Prentice-Hall [20] P. Yiou D. Sornette M. Gill Data-adaptive wavelets and multi-scale SSA Physica D 142 2000 254 290 "
    },
    {
        "doc_title": "Denoising using local projective subspace methods",
        "doc_scopus_id": "33745200994",
        "doc_doi": "10.1016/j.neucom.2005.12.025",
        "doc_eid": "2-s2.0-33745200994",
        "doc_date": "2006-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Cognitive Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2805"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Autocorrelation",
            "Multi-dimensional data",
            "Projective subspace denoising embedding",
            "Projective subspace methods"
        ],
        "doc_abstract": "In this paper we present denoising algorithms for enhancing noisy signals based on Local ICA (LICA), Delayed AMUSE (dAMUSE) and Kernel PCA (KPCA). The algorithm LICA relies on applying ICA locally to clusters of signals embedded in a high-dimensional feature space of delayed coordinates. The components resembling the signals can be detected by various criteria like estimators of kurtosis or the variance of autocorrelations depending on the statistical nature of the signal. The algorithm proposed can be applied favorably to the problem of denoising multi-dimensional data. Another projective subspace denoising method using delayed coordinates has been proposed recently with the algorithm dAMUSE. It combines the solution of blind source separation problems with denoising efforts in an elegant way and proofs to be very efficient and fast. Finally, KPCA represents a non-linear projective subspace method that is well suited for denoising also. Besides illustrative applications to toy examples and images, we provide an application of all algorithms considered to the analysis of protein NMR spectra. © 2006 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271597 291210 291735 291866 31 Neurocomputing NEUROCOMPUTING 2006-02-03 2006-02-03 2010-11-15T09:20:10 S0925-2312(05)00341-3 S0925231205003413 10.1016/j.neucom.2005.12.025 S300 S300.3 FULL-TEXT 2015-05-15T03:05:57.461859-04:00 0 0 20060801 20060831 2006 2006-02-03T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor confloc contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol vol volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 0925-2312 09252312 69 69 13 15 13 14 15 Volume 69, Issues 13–15 10 1485 1501 1485 1501 200608 August 2006 2006-08-01 2006-08-31 2006 Blind Source Separation and Independent Component Analysis Selected papers from the ICA 2004 meeting, Granada, Spain Blind Source Separation and Independent Component Analysis Granada, Spain C.G. Puntonet E.W. Lang Special papers: Blind Source Separation and Independent Component Analysis article fla Copyright © 2006 Elsevier B.V. All rights reserved. DENOISINGUSINGLOCALPROJECTIVESUBSPACEMETHODS GRUBER P 1 Introduction 2 Feature space embedding 2.1 Embedding using delayed coordinates 2.2 Clustering 2.3 Principal component analysis and independent component analysis 3 Denoising algorithms 3.1 Local ICA denoising 3.1.1 Embedding and clustering 3.1.2 Decomposition and denoising 3.1.3 Reconstruction 3.1.4 Parameter estimation 3.2 Denoising using delayed AMUSE 3.2.1 Embedding 3.2.2 Generalized eigenvector decomposition 3.2.3 Implementation of the GEVD 3.3 Kernel PCA based denoising 4 Applications and simulations 4.1 Denoising with Local ICA applied to toy examples 4.1.1 Discussion of an MDL based subspace selection 4.1.2 Comparisons between LICA and LPCA 4.1.3 LICA denoising with multi-dimensional data sets 4.2 Denoising with dAMUSE applied to toy examples 4.3 Denoising of protein NMR spectra 4.3.1 Local ICA denoising 4.3.2 Kernel PCA denoising 4.3.3 Denoising using delayed AMUSE 5 Conclusions Acknowledgements References BELOUCHRANI 1997 434 444 A CICHOCKI 2002 A ADAPTIVEBLINDSIGNALIMAGEPROCESSING COMON 1994 287 314 P DIAMANTARAS 1996 K PRINCIPALCOMPONENTNEURALNETWORKSTHEORYAPPLICATIONS EFFERN 2000 257 266 A FISHLER 2000 2242 2247 E FREEMAN 1997 R SPINCHOREOGRAPHY GHARIEB 2003 252 274 R GHIL 2002 1 41 M HERAULT 1986 206 211 J NEURALNETWORKSFORCOMPUTINGPROCEEDINGSAIPCONFERENCE SPACETIMEADAPTIVESIGNALPROCESSINGBYNEURALNETWORKMODELS HYVARINEN 2001 A INTELLIGENTSIGNALPROCESSING HYVARINEN 1997 1483 1492 A JAIN 1988 A ALGORITHMSFORCLUSTERINGDATA KWOK 2003 J PROCEEDINGSINTERNATIONALCONFERENCEMACHINELEARNINGICML03 PREIMAGEPROBLEMINKERNELMETHODS LIAVAS 2001 1689 1695 A MA 2000 1187 1192 C MIKA 1999 S MOSKVINA 2003 932 942 V PARRA 2003 1261 1269 L PEARSON 1901 559 572 K SANDBERG 1997 477 485 I SCHOELKOPF 1998 1299 1319 B STADLTHANNER 2004 K PROCEEDINGSIJCNN2004 KERNELPCADENOISINGARTIFACTFREEPROTEINNMRSPECTRA STADLTHANNER 2003 103 110 K STADLTHANNER 2003 167 172 K PROCEEDINGSFOURTHINTERNATIONALSYMPOSIUMINDEPENDENTCOMPONENTANALYSISBLINDSOURCESEPARATIONICA2003 BLINDSOURCESEPARATIONWATERARTIFACTSINNMRSPECTRAUSINGAMATRIXPENCIL TAKENS 1981 366 381 F TOME 2000 A PROCEEDINGSINTERNATIONALJOINTCONFERENCENEURALNETWORKSIJCNN2000 BLINDSOURCESEPARATIONUSINGAMATRIXPENCIL TOME 2001 424 428 A PROCEEDINGSTHIRDINTERNATIONALCONFERENCEINDEPENDENTCOMPONENTANALYSISSIGNALSEPARATIONICA2003 ITERATIVEEIGENDECOMPOSITIONAPPROACHBLINDSOURCESEPARATION TOME 2002 A PROCEEDINGSEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO2002 ONLINESOURCESEPARATIONTEMPORALLYCORRELATEDSIGNALS TONG 1991 499 509 L VETTER 2002 290 294 R PROCEEDINGSIASTEDINTERNATIONALCONFERENCESIGNALPROCESSINGPATTERNRECOGNITIONAPPLICATIONSSPPRA02CRETE AUTOMATICNONLINEARNOISEREDUCTIONUSINGLOCALPRINCIPALCOMPONENTANALYSISMDLPARAMETERSELECTION VITANYI 2000 446 464 P GRUBERX2006X1485 GRUBERX2006X1485X1501 GRUBERX2006X1485XP GRUBERX2006X1485X1501XP item S0925-2312(05)00341-3 S0925231205003413 10.1016/j.neucom.2005.12.025 271597 2010-12-25T18:15:35.491277-05:00 2006-08-01 2006-08-31 true 728275 MAIN 17 69786 849 656 IMAGE-WEB-PDF 1 si99 342 16 56 si98 244 10 66 si97 134 9 13 si96 134 9 13 si95 156 15 15 si94 260 19 35 si93 1158 47 208 si92 413 19 73 si91 159 13 17 si90 110 7 9 si89 123 12 9 si88 123 12 9 si87 234 11 55 si86 192 13 41 si85 146 17 12 si84 146 17 12 si83 5505 162 382 si82 239 13 38 si81 110 7 9 si80 344 15 92 si79 344 15 92 si78 531 16 156 si77 117 11 12 si76 282 14 75 si75 975 19 268 si74 222 16 31 si73 222 16 31 si72 170 18 16 si71 134 9 13 si70 195 16 22 si69 558 19 136 si68 222 16 31 si67 322 15 107 si66 251 18 33 si63 212 15 26 si62 156 16 16 si61 145 11 14 si60 504 15 160 si59 136 16 14 si58 130 12 10 si57 1102 40 320 si56 145 13 16 si55 648 47 141 si54 136 16 14 si53 212 15 26 si52 414 15 129 si51 145 11 14 si50 625 16 173 si49 583 19 159 si48 159 13 17 si47 145 11 14 si46 389 15 144 si45 575 16 186 si44 3161 144 334 si43 207 16 24 si42 282 14 75 si41 232 12 44 si40 750 16 207 si39 444 15 133 si38 322 15 107 si37 645 19 174 si36 132 11 10 si35 138 11 14 si34 138 11 14 si33 268 11 55 si32 132 11 10 si31 145 11 14 si243 124 12 12 si242 1230 45 207 si241 146 11 12 si240 344 20 49 si239 159 13 22 si238 117 7 10 si237 1097 45 251 si236 229 12 55 si235 234 15 29 si234 159 13 22 si233 179 16 16 si232 299 15 86 si231 207 15 29 si230 396 13 77 si229 718 45 111 si228 234 15 29 si227 132 9 10 si226 136 11 14 si225 127 9 11 si224 1126 47 212 si223 1079 42 210 si222 229 15 31 si221 134 9 13 si220 766 17 230 si219 136 11 14 si218 229 15 31 si217 313 15 86 si216 229 15 31 si215 214 15 28 si214 396 13 77 si213 318 13 51 si212 310 13 49 si211 248 16 51 si210 295 13 60 si209 326 13 65 si208 567 21 112 si207 488 16 103 si206 476 13 94 si205 138 11 13 si204 173 16 17 si203 343 13 76 si202 234 12 48 si201 407 16 62 si200 163 14 14 si199 280 14 53 si198 157 14 18 si197 1608 107 259 si196 361 16 100 si195 308 13 89 si194 124 11 11 si193 406 19 99 si192 170 8 37 si191 117 7 10 si190 114 7 9 si189 1823 44 393 si188 277 16 47 si187 1005 45 200 si186 166 18 18 si185 1368 43 266 si184 179 16 16 si183 146 11 12 si182 258 16 31 si181 310 17 54 si180 819 17 210 si179 136 11 14 si178 163 12 31 si177 217 13 34 si176 319 13 65 si175 280 16 38 si174 653 45 111 si173 488 16 108 si172 228 14 33 si171 141 9 14 si170 826 23 170 si169 254 13 43 si168 217 13 34 si167 154 12 13 si166 525 20 106 si165 154 12 13 si164 500 16 134 si163 154 12 13 si162 134 9 13 si161 163 14 14 si160 891 20 256 si159 133 11 14 si158 255 16 36 si157 596 19 161 si156 545 16 100 si155 162 13 20 si154 131 11 14 si153 681 19 140 si152 272 12 53 si151 163 14 14 si150 284 16 40 si149 422 19 95 si148 145 11 14 si147 884 37 168 si146 527 16 143 si145 142 14 13 si144 128 9 12 si143 499 19 106 si142 545 16 100 si141 125 10 10 si140 174 13 31 si139 174 13 31 si138 133 11 14 si137 150 13 17 si136 649 19 181 si135 596 16 130 si134 257 11 52 si133 174 13 31 si132 548 16 104 si131 514 19 114 si130 288 16 42 si129 319 17 77 si128 295 16 65 si127 110 7 9 si126 252 13 59 si125 110 7 9 si124 548 16 104 si123 557 16 108 si122 899 19 227 si121 344 11 77 si120 494 19 104 si118 160 13 17 si117 145 11 14 si116 142 14 13 si115 164 13 18 si114 132 13 9 si113 134 9 13 si112 146 12 15 si111 223 13 40 si110 2348 99 332 si109 138 11 14 si108 132 11 10 si107 129 10 10 si106 268 11 55 si105 1620 32 328 si104 342 16 56 si103 374 17 61 si102 172 14 20 si101 196 14 25 si100 177 16 16 si9 488 15 101 si8 451 15 101 si7 571 19 129 si65 156 16 16 si64 145 11 14 si6 171 10 25 si5 517 13 106 si4 520 13 98 si30 487 15 84 si3 254 13 52 si29 504 15 93 si28 127 9 11 si27 606 16 104 si26 498 16 83 si25 195 16 26 si24 382 14 68 si23 198 14 27 si22 204 16 21 si21 195 16 26 si20 215 16 25 si2 277 13 56 si19 451 16 79 si18 129 12 19 si17 124 12 12 si16 310 16 57 si15 124 12 12 si14 234 12 48 si13 258 13 52 si12 525 13 106 si119 142 14 13 si11 529 13 106 si10 512 13 98 si1 351 13 65 fx6 17068 151 113 fx6 4411 93 70 gr5 54808 492 583 gr5 2571 94 111 gr10 20872 590 565 gr10 1389 93 89 gr9 90028 694 768 gr9 2554 93 103 fx3 15228 150 113 fx3 4166 93 70 fx8 10833 151 113 fx8 2085 93 70 gr3 77937 400 384 gr3 3683 94 90 fx9 10914 151 113 fx9 2025 93 70 gr6 42793 476 482 gr6 2258 93 94 gr7 33041 426 456 gr7 2098 93 100 fx1 15177 151 113 fx1 4109 93 70 fx2 15938 151 113 fx2 4549 93 70 gr8 48742 484 495 gr8 2356 93 95 fx5 16411 151 113 fx5 4166 93 70 fx7 15154 151 113 fx7 4151 93 70 gr1 58696 208 742 gr1 2212 35 125 gr2 51711 255 739 gr2 1925 43 125 gr4 53642 471 562 gr4 2620 93 111 fx4 17640 150 113 fx4 4524 93 70 NEUCOM 10195 S0925-2312(05)00341-3 10.1016/j.neucom.2005.12.025 Elsevier B.V. Fig. 1 Comparison between MDL and threshold denoising of an artificial signal with known SNR = 0 . The feature space dimension was M = 40 and the number of clusters was K = 35 . The (MDL achieved an SNR = 8.9 dB and the Threshold criterion an SNR = 10.5 dB ). Fig. 2 Comparison between LPCA and LICA based denoising. Here the mean square error of two signals x , y with L samples is ( 1 / L ) ∑ i | | x i - y i | | 2 . For all noise levels a complete parameter estimation was done in the sets { 10 , 15 , … , 60 } for M and { 20 , 30 , … , 80 } for K. Fig. 3 Comparison of LPCA and LICA based denoising upon an image infested with Gaussian noise. Also note an improvement in denoising power if both are applied consecutively (Local PCA SNR = 8.8 dB , LICA SNR = 10.6 dB , LPCA and LICA consecutively SNR = 12.6 dB ). All images where denoised using a fixed number of clusters K = 20 and a delay radius of M = 4 , which results in a 49-dimensional feature space. Fig. 4 Artificial signals (left column) and their frequency contents (right column). Fig. 5 Comparison of output signals resulting after the first step (second column) and the second step (last column) of dAMUSE. Fig. 6 The graph shows a 1D slice of a proton 2D NOESY NMR spectrum of the polypeptide P11 before and after removing the water artifact with the GEVD-MP algorithm. The 1D spectrum corresponds to the shortest evolution period t 1 . Fig. 7 The figure shows the corresponding artifact free P11 spectra after the denoising algorithms have been applied. The LICA algorithm was applied to all water components with M, K chosen with the MDL estimator ( γ = 32 ) between 20 and 60 and 20 and 80, respectively. The second graph shows the denoised spectrum with a KPCA based algorithm using a Gaussian kernel: (a) LICA denoised spectrum of P11 after the water artifact has been removed with the algorithm GEVD-MP; (b) KPCA denoised spectrum of P11 after the water artifact has been removed with the algorithm GEVD-MP. Fig. 8 The graph uncovers the differences of the LICA and KPCA denoising algorithms. As a reference the corresponding 1D slice of the original P11 spectrum is displayed on top. From top to bottom the three curves show: The difference of the original and the spectrum with the GEVD-MP algorithm applied, the difference between the original and the LICA denoised spectrum and the difference between the original and the KPCA denoised spectrum. To compare the graphs in one diagram the three graphs are translated vertically by 2, 4 and 6, respectively. Fig. 9 1D slice of a 2D NOESY spectrum of the polypeptide P11 in aqueous solution corresponding to the shortest evolution period t 1 . The chemical shift ranges from - 1 to 10ppm roughly. The insert shows the region of the spectrum between 10 and 9ppm roughly. The upper trace corresponds to the denoised baseline and the lower trace shows the baseline of the original spectrum: (a) Original (noisy) spectrum; (b) Reconstructed spectrum with the water artifact removed with the matrix pencil algorithm; (c) Result of the KPCA denoising of the reconstructed spectrum. Fig. 10 Comparison of denoising of the P11 protein spectrum: (a) 1D slice of the NOESY spectrum of the protein P11 spectrum reconstructed with the algorithm GEVD-MP; (b) Corresponding protein spectrum reconstructed with the algorithm dAMUSE. Table 1 Number of output signals correlated with noise or source signals after step 1 and step 2 of the algorithm dAMUSE SNR (dB) NM 1st step 2nd step Total Sources Noise Sources Noise 20 12 6 0 6 0 6 15 12 5 2 6 1 7 10 12 6 2 7 1 8 5 12 6 3 7 2 9 0 12 7 4 8 3 11 Table 2 Parameter values for the embedding dimension of the feature space of dAMUSE ( M dAMUSE ) , the number ( K ) of sampling intervals used per delay in the trajectory matrix, the number N pc of principal components retained after the first step of the GEVD and the half-width ( σ ) of the Gaussian filter used in the algorithms GEVD-MP and dAMUSE Parameter N IC M dAMUSE N pc N w ( GEVD ) P11 256 3 148 49 Parameter N w ( dAMUSE ) σ SNR GEVD - MP SNR dAMUSE P11 46 0.3 18,6dB 22,9dB Denoising using local projective subspace methods P. Gruber a K. Stadlthanner a M. Böhm a F.J. Theis a E.W. Lang a ⁎ A.M. Tomé b A.R. Teixeira b C.G. Puntonet c J.M. Gorriz Saéz c a Institute of Biophysics, Neuro- and Bioinformatics Group, University of Regensburg, 93040 Regensburg, Germany b Departamento de Electrónica e Telecomunicações/IEETA,Universidade de Aveiro, 3810 Aveiro, Portugal c Departamento Arqitectura y Técnologia de Computadores,Universidad de Granada, 18371 Granada, Spain ⁎ Corresponding author. In this paper we present denoising algorithms for enhancing noisy signals based on Local ICA (LICA), Delayed AMUSE (dAMUSE) and Kernel PCA (KPCA). The algorithm LICA relies on applying ICA locally to clusters of signals embedded in a high-dimensional feature space of delayed coordinates. The components resembling the signals can be detected by various criteria like estimators of kurtosis or the variance of autocorrelations depending on the statistical nature of the signal. The algorithm proposed can be applied favorably to the problem of denoising multi-dimensional data. Another projective subspace denoising method using delayed coordinates has been proposed recently with the algorithm dAMUSE. It combines the solution of blind source separation problems with denoising efforts in an elegant way and proofs to be very efficient and fast. Finally, KPCA represents a non-linear projective subspace method that is well suited for denoising also. Besides illustrative applications to toy examples and images, we provide an application of all algorithms considered to the analysis of protein NMR spectra. Keywords Local ICA Delayed AMUSE Projective subspace denoising embedding 1 Introduction The interpretation of recorded signals is often hampered by the presence of noise. This is especially true with biomedical signals which are buried in a large noise background most often. Statistical analysis tools like principal component analysis (PCA), singular spectral analysis (SSA), independent component analysis (ICA) etc. quickly degrade if the signals exhibit a low signal to noise ratio (SNR). Furthermore due to their statistical nature, the application of such analysis tools can also lead to extracted signals with a larger SNR than the original ones as we will discuss below in case of nuclear magnetic resonance (NMR) spectra. Hence in the signal processing community many denoising algorithms have been proposed [5,12,18,39] including algorithms based on local linear projective noise reduction. The idea is to project noisy signals in a high-dimensional space of delayed coordinates, called feature space henceforth. A similar strategy is used in SSA [9,20] where a matrix composed of the data and their delayed versions is considered. Then, a singular value decomposition (SVD) of the data matrix or a PCA of the related correlation matrix is computed. Noise contributions to the signals are then removed locally by projecting the data onto a subset of principal directions of the eigenvectors of the SVD or PCA analysis related with the deterministic signals. Modern multi-dimensional NMR spectroscopy is a very versatile tool for the determination of the native 3D structure of biomolecules in their natural aqueous environment [7,10]. Proton NMR is an indispensable contribution to this structure determination process but is hampered by the presence of the very intense water (H2O) proton signal. The latter causes severe baseline distortions and obscures weak signals lying under its skirts. It has been shown [26,29] that blind source separation (BSS) techniques like ICA can contribute to the removal of the water artifact in proton NMR spectra. ICA techniques extract a set of signals out of a set of measured signals without knowing how the mixing process is carried out [2,13]. Considering that the set of measured spectra X is a linear combination of a set of independent component (ICs) S , i.e. X = AS , the goal is to estimate the inverse of the mixing matrix A , using only the measured spectra, and then compute the ICs. Then the spectra are reconstructed using the mixing matrix A and those ICs contained in S which are not related with the water artifact. Unfortunately the statistical separation process in practice introduces additional noise not present in the original spectra. Hence denoising as a post-processing of the artifact-free spectra is necessary to achieve the highest possible SNR of the reconstructed spectra. It is important that the denoising does not change the spectral characteristics like integral peak intensities as the deduction of the 3D structure of the proteins heavily relies on the latter. We propose two new approaches to this denoising problem and compare the results to the established Kernel PCA (KPCA) denoising [19,25]. The first approach Local ICA (LICA) concerns a local projective denoising algorithm using ICA. Here it is assumed that the noise can, at least locally, be represented by a stationary Gaussian white noise. Signals usually come from a deterministic or at least predictable source and can be described as a smooth function evaluated at discrete time steps small enough to capture the characteristics of the function. That implies, using a dynamical model for the data, that the signal embedded in delayed coordinates resides within a sub-manifold of the feature space spanned by these delayed coordinates. With local projective denoising techniques, the task is to detect this signal manifold. We will use LICA to detect the statistically most interesting submanifold. In the following we call this manifold the signal+noise subspace since it contains all of the signal plus that part of the noise components which lie in the same subspace. Parameter selection within LICA will be effected with a minimum description length (MDL) criterion [40,6] which selects optimal parameters based on the data themselves. For the second approach we combine the ideas of solving BSS problems algebraically using a generalized eigenvector decomposition (GEVD) [28] with local projective denoising techniques. We propose, like in the algorithm for multiple unknown signals extraction (AMUSE) [37], a GEVD of two correlation matrices i.e, the simultaneous diagonalization of a matrix pencil formed with a correlation matrix and a matrix of delayed correlations. These algorithms are exact and fast but sensitive to noise. There are several proposals to improve efficiency and robustness of these algorithms when noise is present [2,8]. They mostly rely on an approximative joint diagonalization of a set of correlation or cumulant matrices like the algorithm second order blind identification (SOBI) [1]. The algorithm we propose, called delayed AMUSE (dAMUSE) [36], computes a GEVD of the congruent matrix pencil in a high-dimensional feature space of delayed coordinates. We show that the estimated signal components correspond to filtered versions of the underlying uncorrelated source signals. We also present an algorithm to compute the eigenvector matrix of the pencil which involves a two step procedure based on the standard eigenvector decomposition (EVD) approach. The advantage of this two step procedure is related with a dimension reduction between the two steps according to a threshold criterion. Thereby estimated signal components related with noise only can be neglected thus performing a denoising of the reconstructed signals. As a third denoising method we consider KPCA based denoising techniques [19,25] which have been shown to be very efficient outperforming linear PCA. KPCA actually generalizes linear PCA which hitherto has been used for denoising. PCA denoising follows the idea that retaining only the principal components with highest variance to reconstruct the decomposed signal, noise contributions which should correspond to the low variance components can deliberately be omitted hence reducing the noise contribution to the observed signal. KPCA extends this idea to non-linear signal decompositions. The idea is to project observed data non-linearly into a high-dimensional feature space and then to perform linear PCA in feature space. The trick is that the whole formalism can be cast into dot product form hence the latter can be replaced by suitable kernel functions to be evaluated in the lower-dimensional input space instead of the high-dimensional feature space. Denoising then amounts to estimating appropriate pre-images in input space of the non-linearly transformed signals. The paper is organized as follows: Section 1 presents an introduction and discusses some related work. In Section 2 some general aspects about embedding and clustering are discussed, before in Section 3 the new denoising algorithms are discussed in detail. Section 4 presents some applications to toy as well as to real world examples and Section 5 draws some conclusions. 2 Feature space embedding In this section we introduce new denoising techniques and propose algorithms using them. At first we present the signal processing tools we will use later on. 2.1 Embedding using delayed coordinates A common theme of all three algorithms presented is to embed the data into a high-dimensional feature space and try to solve the noise separation problem there. With the LICA and the dAMUSE we embed signals in delayed coordinates and do all computations directly in the space of delayed coordinates. The KPCA algorithm considers a non-linear projection of the signals to a feature space but performs all calculations in input space using the kernel trick. It uses the space of delayed coordinates only implicitly as intermediate step in the non-linear transformation since for that transformation the signal at different time steps is used. Delayed coordinates are an ideal tool for representing the signal information. For example in the context of chaotic dynamical systems, embedding an observable in delayed coordinates of sufficient dimension already captures the full dynamical system [30]. There also exists a similar result in statistics for signals with a finite decaying memory [24]. Given a group of N sensor signals, x [ l ] = [ x 0 [ l ] , … , x N - 1 [ l ] ] T sampled at time steps l = 0 , … , L - 1 , a very convenient representation of the signals embedded in delayed coordinates is to arrange them componentwise into component trajectory matrices X i , i = 0 , … , N - 1 [20]. Hence embedding can be regarded as a mapping that transforms a one-dimensional time series x i = ( x i [ 0 ] , x i [ 1 ] , … , x i [ L - 1 ] ) into a multi-dimensional sequence of lagged vectors. Let M be an integer (window length) with M < L . The embedding procedure then forms L - M + 1 lagged vectors which constitute the columns of the component trajectory matrix. Hence given sensor signals x [ l ] , registered for a set of L samples, their related component trajectory matrices are given by (1) X i = x i [ M - 1 ] x i [ M ] … x i [ L - 1 ] = x i [ M - 1 ] x i [ M ] ⋯ x i [ L - 1 ] x i [ M - 2 ] x i [ M - 1 ] ⋯ x i [ L - 2 ] ⋮ ⋮ ⋱ ⋮ x i [ 0 ] x i [ 1 ] ⋯ x i [ L - M ] and encompass M delayed versions of each signal component x i [ l - m ] , m = 0 , … , M - 1 collected at time steps l = M - 1 , … , L - 1 . Note that a trajectory matrix has identical entries along each diagonal. The total trajectory matrix of the set X will be a concatenation of the component trajectory matrices X i computed for each sensor, i.e (2) X = [ X 1 , X 2 , … , X N ] T . Note that the embedded sensor signal is also formed by a concatenation of embedded component vectors, i.e. x [ l ] = [ x 0 [ l ] , … , x N - 1 [ l ] ] . Also note that with LICA we deal with single column vectors of the trajectory matrix only, while with dAMUSE we consider the total trajectory matrix. 2.2 Clustering In our context clustering of signals means rearranging the signal vectors, sampled at different time steps, by similarity. Hence for signals embedded in delayed coordinates, the idea is to look for K disjoint sub-trajectory matrices to group together similar column vectors of the trajectory matrix X . A clustering algorithm like k-means [15] is appropriate for problems where the time structure of the signal is irrelevant. If, however, time or spatial correlations matter, clustering should be based on finding an appropriate partitioning of { M - 1 , … , L - 1 } into K successive segments, since this preserves the inherent correlation structure of the signals. In any case the number of columns in each sub-trajectory matrix X ( j ) amounts to L j such that the following completeness relation holds: (3) ∑ j = 1 K L j = L - M + 1 . The mean vector m j in each cluster can be considered a prototype vector and is given by (4) m j = 1 L j Xc j = 1 L j X ( j ) [ 1 , … , 1 ] T , j = 1 , … , K , where c j is a vector with L j entries equal to one which characterizes the clustering. Note that after the clustering the set { k = 0 , … , L - M - 1 } of indices of the columns of X is split in K disjoint subsets K j . Each trajectory sub-matrix X ( j ) is formed with those columns of the matrix X , the indices of which belong to the subset K j of indices. 2.3 Principal component analysis and independent component analysis PCA [23] is one of the most common multi-variate data analysis tools. It tries to linearly transform given data into uncorrelated data (feature space). Thus in PCA [4] a data vector is represented in an orthogonal basis system such that the projected data have maximal variance. PCA can be performed by eigenvalue decomposition of the data covariance matrix. The orthogonal transformation is obtained by diagonalizing the centered covariance matrix of the data set. In ICA, given a random vector, the goal is to find its statistically ICs. In contrast to correlation-based transformations like PCA, ICA renders the output signals as statistically independent as possible by evaluating higher-order statistics. The idea of ICA was first expressed by Jutten and Hérault [11] while the term ICA was later coined by Comon [3]. With LICA we will use the popular FastICA algorithm by Hyvärinen and Oja [14], which performs ICA by maximizing the non-Gaussianity of the signal components. 3 Denoising algorithms 3.1 Local ICA denoising The LICA algorithm we present is based on a local projective denoising technique using an MDL criterion for parameter selection. The idea is to achieve denoising by locally projecting the embedded noisy signal into a lower-dimensional subspace which contains the characteristics of the noise free signal. Finally the signal has to be reconstructed using the various candidates generated by the embedding. Consider the situation, where we have a signal x i 0 [ l ] at discrete time steps l = 0 , … , L - 1 but only its noise corrupted version x i [ l ] is measured (5) x i [ l ] = x i 0 [ l ] + e i [ l ] , where e [ l ] are samples of a random variable with Gaussian distribution, i.e. x i equals x i 0 up to additive stationary white noise. 3.1.1 Embedding and clustering First the noisy signal x i [ l ] is transformed into a high-dimensional signal x i [ l ] in the M-dimensional space of delayed coordinates according to (6) x i [ l ] ≔ [ x i [ l ] , … , x i [ l - M + 1 mod L ] ] T which corresponds to a column of the trajectory matrix in Eq. (1). To simplify implementation, we want to ensure that the delayed signal, like the original signal, (trajectory matrix) is given at L time steps instead of L - M + 1 . This can be achieved by using the samples in round robin manner, i.e. by closing the end and the begin of each delayed signal and cutting out exactly L components in accord with the delay. If the signal contains a trend or its statistical nature is significantly different at the end compared to the beginning, then this leads to compatibility problems of the beginning and end of the signal. We can easily resolve this misfit by replacing the signal with a version where we add the signal in reverse order, hence avoiding any sudden change in signal amplitude which would be smoothed out by the algorithm. The problem can now be localized by selecting K clusters in the feature space of delayed coordinates of the signal { x i [ l ] | l = 0 , … , L - 1 } . Clustering can be achieved by a k-means cluster algorithm as explained in Section 2.2. But k-means clustering is only appropriate if the variance or the kurtosis of a signal do not depend on the inherent signal structure. For other noise selection schemes like choosing the noise components based on the variance of the autocorrelation, it is usually better to find an appropriate partitioning of the set of time steps { 0 , … , L - 1 } into K successive segments, since this preserves the inherent time structure of the signals. For other noise selection methods like choosing the noise components based on the variance of the autocorrelation it is usually better to find an appropriate partition of the set of time steps { 0 , … , L - 1 } into K successive segments, since this preserves the inherent time structure of the signal. Note that the clustering does not change the data but only changes its time sequence, i.e. permutes and regroups the columns of the trajectory matrix and separates it into K sub-matrices. 3.1.2 Decomposition and denoising After centering, i.e. removing the mean in each cluster, we can analyze the M-dimensional signals in these K clusters using PCA or ICA. The PCA case (Local PCA (LPCA)) is studied in [38] so in the following we will propose an ICA based denoising. Using ICA, we extract M ICs from each delayed signal. Like in all projection based denoising algorithms, noise reduction is achieved by projecting the signal into a lower-dimensional subspace. We used two different criteria to estimate the number p of signal+noise components, i.e. the dimension of the signal subspace onto which we project after applying ICA. • One criterion is a consistent MDL estimator of p MDL for the data model in Eq. (5) [38] (7) p MDL = argmin p = 0 , … , M - 1 MDL ( M , L , p , ( λ j ) , γ ) argmin p = 0 , … , M - 1 - ( ( M - p ) L ) ln Π j = p + 1 M λ j ( 1 / ( M - p ) ) ( 1 / ( M - p ) ) ∑ j = p + 1 M λ j + pM - p 2 2 + p 2 + 1 1 2 + ln γ - pM - ( p 2 / 2 ) + ( p / 2 ) + 1 p 1 2 ln 2 L + ∑ j = 1 p ln λ j - ln ∑ j = 1 M - 1 λ j , where λ j denote the variances of the signal components in feature space, i.e. after applying the de-mixing matrix which we estimate with the ICA algorithm. To retain the relative strength of the components in the mixture, we normalize the rows of the de-mixing matrix to unit norm. The variances are ordered such that the smallest eigenvalues λ j correspond to directions in feature space most likely to be associated with noise components only. The first term in the MDL estimator represents the likelihood of the m - p Gaussian white noise components. The third term stems from the estimation of the description length of the signal part (first p components) of the mixture based on their variances. The second term acts as a penalty term to favor parsimonious representations of the data for short time series, and becomes insignificant in the limit L → ∞ since it does not depend on L while the other two terms grow without bounds. The parameter γ controls this behavior and is a parameter of the MDL estimator, hence of the final denoising algorithm. By experience, good values for γ seem to be 32 or 64. • Based on the observations reported by [17] and our observations that, in some situations, the MDL estimator tends to significantly underestimate the number of noise components, we also used another approach: We clustered the variances of the signal components into two clusters using k-means clustering and defined p cl as the number of elements in the cluster which contains the largest eigenvalue. This yields a good estimate of the number of signal components, if the noise variances are not clustered well enough together but, nevertheless, are separated from the signal by a large gap. More details and simulations corroborating our observations can be found in Section 4.1.1. Depending on the statistical nature of the data, the ordering of the components in the MDL estimator can be achieved using different methods. For data with a non-Gaussian distribution, we select the noise component as the component with the smallest value of the kurtosis as Gaussian noise corresponds to a vanishing kurtosis. For non-stationary data with stationary noise, we identify the noise by the smallest variance of its autocorrelation. 3.1.3 Reconstruction In each cluster the centering is reversed by adding back the cluster mean. To reconstruct the noise reduced signal, we first have to reverse the clustering of the data to yield the signal x i e [ l ] ∈ R M by concatenating the trajectory sub-matrices and reversing the permutation done during clustering. The resulting trajectory matrix does not possess identical entries in each diagonal. Hence we average over the candidates in the delayed data, i.e. over all entries in each diagonal: (8) x i e [ l ] ≔ 1 M ∑ j = 0 M - 1 x i e [ l + j mod L ] j , where x i e [ l ] j stands for the jth component of the enhanced vector x i e at time step l. Note that the summation is done over the diagonals of the trajectory matrix so it would yield x i if performed on the original delayed signal x i . 3.1.4 Parameter estimation We still have to find optimal values for the global parameters M and K. Their selection again can be based on a MDL criterion for the detected noise e ≔ x - x e . Accordingly we apply the LICA algorithm for different M and K and embed each of these error signals e ( M , K ) in delayed coordinates of a fixed large enough dimension M ^ and choose the parameters M 0 and K 0 such that the MDL criterion estimating the noisiness of the error signal is minimal. The MDL criterion is evaluated with respect to the eigenvalues λ j ( M , K ) of the correlation matrix of e ( M , K ) such that (9) ( M 0 , K 0 ) = argmin M , K MDL ( M ^ , L , 0 , ( λ j ( M , K ) ) , γ ) . 3.2 Denoising using delayed AMUSE Signals with an inherent correlation structure like time series data can as well be analyzed using second-order BSS techniques only [22,35]. GEVD of a matrix pencil [33,37] or a joint approximative diagonalization of a set of correlation matrices [1] is then usually considered. Recently we proposed an algorithm based on a generalized eigenvalue decomposition in a feature space of delayed coordinates [35]. It provides means for BSS and denoising simultaneously. 3.2.1 Embedding Assuming that each sensor signal is a linear combination X = AS of N underlying but unknown source signals s i , a source signal trajectory matrix S can be written in analogy to Eqs. (1) and (2). Then the mixing matrix A is a block matrix with a diagonal matrix in each block: (10) A = a 11 I M × M a 12 I M × M ⋯ a 1 N I M × M a 21 I M × M a 22 I M × M ⋯ ⋯ ⋮ ⋮ ⋱ ⋮ a N 1 I M × M a N 2 I M × M ⋯ a NN I M × M . The matrix I M × M represents the identity matrix, and in accord with an instantaneous mixing model the mixing coefficient a ij relates the sensor signal x i with the source signal s j . 3.2.2 Generalized eigenvector decomposition The delayed correlation matrices of the matrix pencil are computed with one matrix X r obtained by eliminating the first k i columns of X and another matrix, X l , obtained by eliminating the last k i columns. Then, the delayed correlation matrix R x ( k i ) = X r X l T will be an NM × NM matrix. Each of these two matrices can be related with a corresponding matrix in the source signal domain: (11) R x ( k i ) = AR s ( k i ) A T = AS r S l T A T . Then the two pairs of matrices ( R x ( k 1 ) , R x ( k 2 ) ) and ( R s ( k 1 ) , R s ( k 2 ) ) represent congruent pencils [34] with the following properties: • Their eigenvalues are the same, i.e., the eigenvalue matrices of both pencils are identical: D x = D s . • If the eigenvalues are non-degenerate (distinct values in the diagonal of the matrix D x = D s ) , the corresponding eigenvectors are related by the transformation E s = A T E x . Assuming that all sources are uncorrelated, the matrices R s ( k i ) are block diagonal, having block matrices R mm ( k i ) = S ri S li T along the diagonal. The eigenvector matrix of the GEVD of the pencil ( R s ( k 1 ) , R s ( k 2 ) ) again forms a block diagonal matrix with block matrices E mm forming M × M eigenvector matrices of the GEVD of the pencils ( R mm ( k 1 ) , R mm ( k 2 ) ) . The uncorrelated components can then be estimated from linearly transformed sensor signals via (12) Y = E x T X = E x T AS = E s T S hence turn out to be filtered versions of the underlying source signals. As the eigenvector matrix E s is a block diagonal matrix, there are M signals in each column of Y which are a linear combination of one of the source signals and its delayed versions. Then the columns of the matrix E mm represent impulse responses of finite impulse response (FIR) filters. Considering that all the columns of E mm are different, their frequency response might provide different spectral densities of the source signal spectra. Then NM output signals y encompass M filtered versions of each of the N estimated source signals. 3.2.3 Implementation of the GEVD There are several ways to compute the generalized eigenvalue decomposition. We resume a procedure valid if one of the matrices of the pencil is symmetric positive definite. Thus, we consider the pencil ( R x ( 0 ) , R x ( k 2 ) ) and perform the following steps: Step 1: Compute a standard eigenvalue decomposition of R x ( 0 ) = V Λ V T , i.e, compute the eigenvectors v i and eigenvalues λ i . As the matrix is symmetric positive definite, the eigenvalues can be arranged in descending order ( λ 1 > λ 2 > ⋯ > λ NM ) . This procedure corresponds to the usual whitening step in many ICA algorithms. It can be used to estimate the number of sources, but it can also be considered a strategy to reduce noise much like with PCA denoising. Dropping small eigenvalues amounts to a projection from a high-dimensional feature space onto a lower-dimensional manifold representing the signal+noise subspace. Thereby it is tacitly assumed that small eigenvalues are related with noise components only. Here we consider a variance criterion to choose the most significant eigenvalues, those related with the embedded deterministic signal, according to (13) λ 1 + λ 2 + ⋯ + λ l λ 1 + λ 2 + ⋯ λ NM ⩾ TH . If we are interested in the eigenvectors corresponding to directions of high variance of the signals, the threshold TH should be chosen such that their maximum energy is preserved. Similar to the whitening phase in many BSS algorithms, the data matrix X can be transformed using (14) Q = Λ - 1 / 2 V T to calculate a transformed matrix of delayed correlations C ( k 2 ) to be used in the next step. The transformation matrix can be computed using either the l most significant eigenvalues, in which case denoising is achieved, or all eigenvalues and respective eigenvectors. Also note, that Q represents a l × NM matrix if denoising is considered. Step 2: Use the transformed delayed correlation matrix C ( k 2 ) = QR x ( k 2 ) Q T and its standard eigenvalue decomposition: the eigenvector matrix U and eigenvalue matrix D x . The eigenvectors of the pencil ( R x ( 0 ) , R x ( k 2 ) ) , which are not normalized, form the columns of the eigenvector matrix E x = Q T U = V Λ - 1 / 2 U . The ICs of the delayed sensor signals can then be estimated via the transformation given below, yielding l (or NM ) signals, one signal per row of Y : (15) Y = E x T X = U T QX = U T Λ - 1 / 2 V T X . The first step of this algorithm is therefore equivalent to a PCA in a high-dimensional feature space [9,39], where a matrix similar to Q is used to project the data onto the signal manifold. 3.3 Kernel PCA based denoising Kernel PCA has been developed by [19], hence we give here only a short summary for convenience. PCA only extracts linear features though with suitable non-linear features more information could be extracted. It has been shown [19] that KPCA is well suited to extract interesting non-linear features in the data. KPCA first maps the data x i into some high-dimensional feature space Ω through a non-linear mapping Φ : R n → R m , m > n and then performs linear PCA on the mapped data in the feature space Ω . Assuming centered data in feature space, i.e. ∑ k = 1 l Φ ( x k ) = 0 , to perform PCA in space Ω amounts to finding the eigenvalues λ > 0 and eigenvectors ω ∈ Ω of the correlation matrix R ¯ = 1 / l ∑ j = 1 l Φ ( x j ) Φ ( x j ) T . Note that all ω with λ ≠ 0 lie in the subspace spanned by the vectors Φ ( x 1 ) , … , Φ ( x l ) . Hence the eigenvectors can be represented via (16) ω = ∑ i = 1 l α i Φ ( x i ) . Multiplying the eigenequation with Φ ( x k ) from the left the following modified eigenequation is obtained (17) K α = l λ α with λ > 0 . The eigenequation now is cast in the form of dot products occurring in feature space through the l × l matrix K with elements K ij = ( Φ ( x i ) · Φ ( x j ) ) = k ( x i , x j ) which are represented by kernel functions k ( x i , x j ) to be evaluated in the input space. For feature extraction any suitable kernel can be used and knowledge of the non-linear function Φ ( x ) is not needed. Note that the latter can always be reconstructed from the principal components obtained. The image of a data vector under the map Φ can be reconstructed from its projections β k via (18) P ^ n Φ ( x ) = ∑ k = 1 n β k ω k = ∑ k = 1 n ( ω k · Φ ( x ) ) ω k which defines the projection operator P ^ n . In denoising applications, n is deliberately chosen such that the squared reconstruction error (19) e rec 2 = ∑ i = 1 l ∥ P ^ n Φ ( x i ) - Φ ( x i ) ∥ 2 is minimized. To find a corresponding approximate representation of the data in input space, the so-called pre-image, it is necessary to estimate a vector z ∈ R N in input space such that (20) ρ ( z ) = ∥ P ^ n Φ ( x ) - Φ ( z ) ∥ 2 = k ( z , z ) - 2 ∑ k = 1 n β k ∑ i = 1 l α i k k ( x i , z ) is minimized. Note that an analytic solution to the pre-image problem has been given recently in case of invertible kernels [16]. In denoising applications it is hoped that the deliberately neglected dimensions of minor variance contain noise mostly and z represents a denoised version of x . Eq. (20) can be minimized via gradient descent techniques. 4 Applications and simulations In this section we will first present results and concomitant interpretation of some experiments with toy data using different variations of the LICA denoising algorithm. Next we also present some test simulations using toy data of the algorithm dAMUSE. Finally we will discuss the results of applying the three different denoising algorithms presented above to a real world problem, i.e. to enhance protein NMR spectra contaminated with a huge water artifact. 4.1 Denoising with Local ICA applied to toy examples We will present some sample experimental results using artificially generated signals and random noise. As the latter is characterized by a vanishing kurtosis, the LICA based denoising algorithm uses the component kurtosis for noise selection. 4.1.1 Discussion of an MDL based subspace selection In the LICA denoising algorithm the MDL criterion is also used to select the number of noise components in each cluster. This works without prior knowledge of the noise strength. Since the estimation is based solely on statistical properties, it produces suboptimal results in some cases, however. In Fig. 1 we compare, for an artificial signal with a known additive white Gaussian noise, the denoising achieved with the MDL based estimation of the subspace dimension versus the estimation based on the noise level. The latter is done using a threshold on the variances of the components in feature space such that only the signal part is conserved. Fig. 1 shows that the threshold criterion works slightly better in this case, though the MDL based selection can obtain a comparable level of denoising. However, the smaller SNR indicates that the MDL criterion favors some over-modelling of the signal subspace, i.e. it tends to underestimate the number of noise components in the registered signals. In [17] the conditions, such as the noise not being completely white, which lead to a strong over-modelling are identified. Over-modelling also happens frequently, if the eigenvalues of the covariance matrix related with noise components, are not sufficiently close together and are not separated from the signal components by a gap. In those cases a clustering criterion for the eigenvalues seems to yield better results, but it is not as generic as the MDL criterion. 4.1.2 Comparisons between LICA and LPCA Consider the artificial signal shown in Fig. 1 with varying additive Gaussian white noise. We apply the LICA denoising algorithm using either an MDL criterion or a threshold criterion for parameter selection. The results are depicted in Fig. 2 . The first and second diagram of Fig. 2 compare the performance, here the enhancement of the SNR and the mean square error, of LPCA and LICA depending on the input SNR. Note that a source SNR of 0 describes a case where signal and noise have the same strength, while negative values indicate situations where the signal is buried in the noise. The third graph shows the difference in kurtosis of the original signal and the source signal in dependence on the input SNR. All three diagrams were generated with the same data set, i.e. the same signal and, for a given input SNR, the same additive noise. These results suggest that a LICA approach is more effective when the signal is infested with a large amount of noise, whereas a LPCA seems better suited for signals with high SNRs. This might be due to the nature of our selection of subspaces based on kurtosis or variance of the autocorrelation as the comparison of higher statistical moments of the restored data, like kurtosis, indicate that noise reduction can be enhanced if we are using a LICA approach. 4.1.3 LICA denoising with multi-dimensional data sets A generalization of the LICA algorithm to multi-dimensional data sets like images where pixel intensities depend on two coordinates is desirable. A simple generalization would be to look at delayed coordinates of vectors instead of scalars. However, this appears impractical due to the prohibitive computational effort. More importantly, this direct approach reduces the number of available samples significantly. This leads to far less accurate estimators of important aspects like the MDL estimation of the dimension of the signal subspace or the estimation of the kurtosis criterion in the LICA case. Another approach could be to convert the data to a 1D string by choosing some path through the data and concatenating the pixel intensities accordingly. But this can easily create unwanted artifacts along the chosen path. Further, local correlations are broken up, hence not all the available information is used. But a more sophisticated and, depending on the nature of the signal, very effective alternative approach can be envisaged. Instead of converting the multi-dimensional data into 1D data strings prior to applying the LICA algorithm, we can use a modified delay transformation using shifts along all available dimensions. This concept is similar to the multi-dimensional auto-covariances used in the multi-dimensional SOBI (mdSOBI) algorithm introduced in [31]. In the 2D case, for example, consider an n × n image represented by a matrix P = ( a ij ) i , j = 1 … n . Then the transformed data set consists of copies of P which are shifted either along columns or rows or both. For instance, a translation a ij → a i - 1 , j + 1 , ( i , j = 1 , … , n ) yields the following transformed image: (21) P - 1 , 1 = a n , 2 ... a n , n a n , 1 a 1 , 2 ... a 1 , n a 1 , 1 ⋮ ⋮ ⋮ a n - 1 , 2 ... a n - 1 , n a n - 1 , 1 . Then instead of choosing a single delay dimension, we choose a delay radius M and use all P ν with ∥ ν ∥ < M as delayed versions of the original signal. The remainder of the LICA based denoising algorithm works exactly as in the case of a 1D time series. In Fig. 3 we show that this approach using the MDL criterion to select the number of components compared between LPCA and LICA. In addition we see that the algorithm also works favorable if applied multiple times. 4.2 Denoising with dAMUSE applied to toy examples A group of three artificial source signals with different frequency contents was chosen: one member of the group represents a narrow-band signal, a sinusoid; the second signal encompasses a wide frequency range; and the last one represents a sawtooth wave whose spectral density is concentrated in the low frequency band (see Fig. 4 ). The simulations were designed to illustrate the method and to study the influence of the threshold parameter TH on the performance when noise is added at different levels. In what concerns noise we also try to find out if there is any advantage of using a GEVD instead of a PCA analysis. Hence the signals at the output of the first step of the algorithm (using the matrix Q to project the data) are also compared with the output signals. Results are collected in Table 1 . Random noise was added to the sensor signals yielding a SNR in the range of [ 0 , 20 ] dB . The parameters M = 4 and TH = 0.95 were kept fixed. As the noise level increases, the number of significant eigenvalues also increases. Hence at the output of the first step more signals need to be considered. Thus as the noise energy increases, the number ( l ) of signals or the dimension of matrix C also increases after the application of the first step (last column of Table 1). As the noise increases, an increasing number of ICs will be available at the output of the two steps. Computing, in the frequency domain, the correlation coefficients between the output signals of each step of the algorithm and noise or source signals we confirm that some are related with the sources and others with noise. Table 1 (columns 3–6) shows that the maximal correlation coefficients are distributed between noise and source signals to a varying degree. We can see that the number of signals correlated with noise is always higher in the first level. Results show that for low noise levels the first step (which is mainly a PCA in a space of dimension NM) achieves good solutions already. However, we can also see (for narrow-band signals and/or M low) that the time domain characteristics of the signals resemble the original source signals only after a GEVD, i.e. at the output of the second step rather than with a PCA, i.e. at the output of first step. Fig. 5 shows examples of signals that have been obtained in the two steps of the algorithm for SNR = 10 dB . At the output of the first level the three signals with highest frequency correlation were chosen among the eight output signals. Using a similar criterion to choose three signals at the output of the 2nd step (last column of Fig. 5), we can see that their time course is more similar to the source signals than after the first step (middle column of Fig. 5). 4.3 Denoising of protein NMR spectra In biophysics the determination of the 3D structure of biomolecules like proteins is of utmost importance. NMR techniques provide indispensable tools to reach this goal. As hydrogen nuclei are the most abundant and most sensitive nuclei in proteins, proton NMR spectra of proteins dissolved in water are recorded mostly. Since the concentration of the solvent is by magnitudes larger than the protein concentration, there is always a large proton signal of the water solvent contaminating the protein spectrum. This water artifact cannot be suppressed completely with technical means, hence it would be interesting to remove it during the analysis of the spectra. BSS techniques have been shown to solve this separation problem [27,28]. BSS algorithms are based on an ICA [2] which extracts a set of underlying independent source signals out of a set of measured signals without knowing how the mixing process is carried out. We have used an algebraic algorithm [32,33] based on second order statistics using the time structure of the signals to separate this and related artifacts from the remaining protein spectrum. Unfortunately due to the statistical nature of the algorithm unwanted noise is introduced into the reconstructed spectrum as can be seen in Fig. 6 . The water artifact removal is effected by a decomposition of a series of NMR spectra into their uncorrelated spectral components applying a generalized eigendecomposition of a congruent matrix pencil [37]. The latter is formed with a correlation matrix of the signals and a correlation matrix with delayed or filtered signals [34]. Then we can detect and remove the components which contain only a signal generated by the water and reconstruct the remaining protein spectrum from its ICs. But the latter now contains additional noise introduced by the statistical analysis procedure, hence denoising deemed necessary. The algorithms discussed above have been applied to an experimental 2D nuclear overhauser effect spectroscopy (NOESY) proton NMR spectrum of the polypeptide P11 dissolved in water. The synthetic peptide P11 consists of 24 amino acids only and represents the helix H11 of the human glutathion reductase [21]. A simple pre-saturation of the water resonance was applied to prevent saturation of the dynamic range of the analog digital converter (ADC). Every data set comprises 512 free induction decays (FIDs) S ( t 1 , t 2 ) ≡ x n [ l ] or their corresponding spectra S ^ ( 1 1 , ω 2 ) ≡ x ^ n [ l ] , with L = 2048 samples each, which correspond to N = 128 evolution periods t 1 ≡ [ n ] . To each evolution period belong four FIDs with different phase modulations, hence only FIDs with equal phase modulations have been considered for analysis. A BSS analysis, using both the algorithm GEVD using matrix pencil (GEVD-MP) [28] and the algorithm dAMUSE [36], was applied to all data sets. Note that the matrix pencil within GEVD-MP was conveniently computed in the frequency domain, while in the algorithm dAMUSE in spite of the filtering operation being performed in the frequency domain, the matrix pencil was computed in the time domain. The GEVD is performed in dAMUSE as described above to achieve a dimension reduction and concomitant denoising. 4.3.1 Local ICA denoising For denoising we first used the LICA denoising algorithm proposed above to enhance the reconstructed protein signal without the water artifact. We applied the denoising only to those components which were identified as water components. Then we removed the denoised versions of these water artifact components from the total spectrum. As a result, the additional noise is at least halved as can also be seen from Fig. 7 . On the part of the spectrum away from the center, i.e. not containing any water artifacts, we could estimate the increase of the SNR with the original spectrum as reference. We calculated a SNR of 17.3 dB of the noisy spectrum and a SNR of 21.6 dB with applying the denoising algorithm. We compare the result, i.e. the reconstructed artifact-free protein spectrum of our denoising algorithm to the result of a KPCA based denoising algorithm using a Gaussian kernel in Fig. 8 . The figure depicts the differences between the denoised spectra and the original spectrum in the regions where the water signal is not very dominating. As can be seen, the LICA denoising algorithm reduces the noise but does not change the content of the signal, whereas the KPCA algorithm seems to influence the peak amplitudes of the protein resonances as well. Further experiments are under way in our laboratory to investigate these differences in more detail and to establish an automatic artifact removal algorithm for multi-dimensional NMR spectra. 4.3.2 Kernel PCA denoising As the removal of the water artifact lead to additional noise in the spectra (compare Fig. 9 (a) and (b)) KPCA based denoising was applied. First (almost) noise free samples had to be created in order to determine the principle axes in feature space. For that purpose, the first 400 data points of the real and the imaginary part of each of the 512 original spectra were used to form a 400 × 1024 sample matrix X ( 1 ) . Likewise five further sample matrices X ( m ) , m = 2 , … , 6 , were created, which now consisted of the data points 401–800, 601–1000, 1101–1500, 1249–1648 and 1649–2048, respectively. Note that the region (1000–1101) of data points comprising the main part of the water resonance was nulled deliberately as it is of no use for the KPCA. For each of the sample matrices X ( m ) the corresponding kernel matrix K was determined by (22) K i , j = k ( x i , x j ) , i , j = 1 , … , 400 , where x i denotes the ith column of X ( m ) . For the kernel function a Gaussian kernel (23) k ( x i , x j ) = exp - ∥ x i - x j ∥ 2 2 σ 2 , where (24) 2 σ 2 = 1 400 × 399 ∑ i , j = 1 400 ∥ x i - x j ∥ 2 is the width parameter σ , was chosen. Finally the kernel matrix K was expressed in terms of its EVD (Eq. (17)) which lead to the expansion parameters α necessary to determine the principal axes of the corresponding feature space Ω ( m ) : (25) ω = ∑ i = 1 400 α i Φ ( x i ) . Similar to the original data, the noisy data of the reconstructed spectra were used to form six 400 × 1024 dimensional pattern matrices P ( m ) , m = 1 , … , 6 . Then the principal components β k of each column of P m were calculated in the corresponding feature space Ω ( m ) . In order to denoise the patterns only projections onto the first n = 112 principal axes were considered. This lead to (26) β k = ∑ i = 1 400 α i k k ( x i , x ) , k = 1 , … , 112 , where x is a column of P m . After reconstructing the image P ^ n Φ ( x ) of the sample vector under the map Φ (Eq. (18)), its approximate pre-image was determined by minimizing the cost function (27) ρ ( z ) = - 2 ∑ k = 1 112 β k ∑ i = 1 400 α i k k ( x i , z ) . Note that the method described above fails to denoise the region where the water resonance appears (data points 1001–1101) because then the samples formed from the original data differ too much from the noisy data. This is not a major drawback as protein peaks totally hidden under the water artifact cannot be uncovered by the presented BSS method. Fig. 9(c) shows the resulting denoised protein spectrum on an identical vertical scale as Figs. 9(a) and (b). The insert compares the noise in a region of the spectrum between 10 and 9ppm roughly where no protein peaks are found. The upper trace shows the baseline of the denoised reconstructed protein spectrum and the lower trace the corresponding baseline of the original experimental spectrum before the water artifact has been separated out. 4.3.3 Denoising using delayed AMUSE LICA denoising of reconstructed protein spectra necessitate the solution of the BSS problem beforehand using any ICA algorithm. A much more elegant solution is provided by the recently proposed algorithm dAMUSE, which achieves BSS and denoising simultaneously. To test the performance of the algorithm, it was also applied to the 2D NOESY NMR spectra of the polypeptide P11. A 1D slice of the 2D NOESY spectrum of P11 corresponding to the shortest evolution period t 1 is presented in Fig. 9(a) which shows a huge water artifact despite some pre-saturation on the water resonance. Fig. 10 shows the reconstructed spectra obtained with the algorithms GEVD-MP and dAMUSE, respectively. The algorithm GEVD-MP yielded almost artifact-free spectra but with clear changes in the peak intensities in some areas of the spectra. On the contrary, the reconstructed spectra obtained with the algorithm dAMUSE still contain some remnants of the water artifact but the protein peak intensities remained unchanged and all baseline distortions have been cured. All parameters of the algorithms are collected in Table 2 . 5 Conclusions We proposed two new denoising techniques and also considered KPCA denoising which are all based on the concept of embedding signals in delayed coordinates. We presented a detailed discussion of their properties and also discussed results obtained applying them to illustrative toy examples. Furthermore we compared all three algorithms by applying them to the real world problem of removing the water artifact from NMR spectra and denoising the resulting reconstructed spectra. Although all three algorithms achieved good results concerning the final SNR, in case of the NMR spectra it turned out that KPCA seems to alter the spectral shapes while LICA and dAMUSE do not. At least with protein NMR spectra it is crucial that denoising algorithms do not alter integrated peak intensities in the spectra as the latter form the bases for the structure elucidation process. In future we have to further investigate the dependence of the proposed algorithms on the situation at hand. Thereby it will be crucial to identify data models for which each one of the proposed denoising techniques works best and to find good measures of how such models suit the given data. Acknowledgements This research has been supported by the BMBF (project ModKog) and the DFG (GRK 638: Non-linearity and Non-equilibrium in Condensed Matter). We are grateful to W. Gronwald and H. R. Kalbitzer for providing the NMR spectrum of P11 and helpful discussions. References [1] A. Belouchrani K. Abed-Meraim J.-F. Cardoso E. Moulines A blind source separation technique using second-order statistics IEEE Trans. Signal Process. 45 2 1997 434 444 [2] A. Cichocki S.-I. Amari Adaptive Blind Signal and Image Processing 2002 Wiley New York [3] P. Comon Independent component analysis—a new concept? Signal Process. 36 1994 287 314 [4] K.I. Diamantaras S.Y. Kung Principal Component Neural Networks, Theory and Applications 1996 Wiley New York [5] A. Effern K. Lehnertz T. Schreiber T. Grunwald P. David C.E. Elger Nonlinear denoising of transient signals with application to event-related potentials Physica D 140 2000 257 266 [6] E. Fishler H. Messer On the use of order statistics for improved detection of signals by the MDL criterion IEEE Trans. Signal Process. 48 2000 2242 2247 [7] R. Freeman Spin Choreography 1997 Spektrum Academic Publishers Oxford [8] R.R. Gharieb A. Cichocki Second-order statistics based blind source separation using a bank of subband filters Digital Signal Process. 13 2003 252 274 [9] M. Ghil M.R. Allen M.D. Dettinger K. Ide Advanced spectral methods for climatic time series Rev. Geophys. 40 1 2002 1 41 [10] K.H. Hausser, H.-R. Kalbitzer, NMR in Medicine and Biology, Berlin, 1991. [11] J. Hérault C. Jutten Space or time adaptive signal processing by neural network models J.S. Denker Neural Networks for Computing, Proceedings of the AIP Conference 1986 American Institute of Physics New York 206 211 [12] A. Hyvärinen P. Hoyer E. Oja Intelligent Signal Processing 2001 IEEE Press NY [13] A. Hyvärinen, J. Karhunen, E. Oja, Independent Component Analysis, 2001. [14] A. Hyvärinen E. Oja A fast fixed-point algorithm for independent component analysis Neural Comput. 9 1997 1483 1492 [15] A.K. Jain R.C. Dubes Algorithms for Clustering Data 1988 Prentice-Hall New Jersey [16] J.T. Kwok I.W. Tsang The pre-image problem in kernel methods Proceedings of the International Conference on Machine Learning (ICML03) 2003 [17] A.P. Liavas P.A. Regalia On the behavior of information theoretic criteria for model order selection IEEE Trans. Signal Process. 49 2001 1689 1695 [18] C.T. Ma Z. Ding S.F. Yau A two-stage algorithm for MIMO blind deconvolution of nonstationary colored noise IEEE Trans. Signal Process. 48 2000 1187 1192 [19] S. Mika B. Schölkopf A. Smola K. Müller M. Scholz G. Rätsch Kernel PCA and denoising in feature spaces Adv. Neural Inf. Process. Syst. NIPS11 11 1999 [20] V. Moskvina K.M. Schmidt Approximate projectors in singular spectrum analysis SIAM J. Mat. Anal. Appl. 24 4 2003 932 942 [21] A. Nordhoff, Ch. Tziatzios, J.A.V. Broek, M. Schott, H.-R. Kalbitzer, K. Becker, D. Schubert, R.H. Schirme, Denaturation and reactivation of dimeric human glutathione reductase, Eur. J. Biochem. (1997) 273–282. [22] L. Parra P. Sajda Blind source separation vis generalized eigenvalue decomposition J. Mach. Learn. Res. 4 2003 1261 1269 [23] K. Pearson On lines and planes of closest fit to systems of points in space Philos. Mag. 2 1901 559 572 [24] I.W. Sandberg L. Xu Uniform approximation of multidimensional myoptic maps Trans. Circuits Syst. 44 1997 477 485 [25] B. Schoelkopf A. Smola K.-R. Mueller Nonlinear component analysis as a kernel eigenvalue problem Neural Comput. 10 1998 1299 1319 [26] K. Stadlthanner E.W. Lang A.M. Tomé A.R. Teixeira C.G. Puntonet Kernel-PCA denoising of artifact-free protein NMR spectra Proceedings of the IJCNN’2004 Budapest, Hungaria 2004 [27] K. Stadlthanner F.J. Theis E.W. Lang A.M. Tomé W. Gronwald H.-R. Kalbitzer A matrix pencil approach to the blind source separation of artifacts in 2D NMR spectra Neural Inf. Process.—Lett. Rev. 1 2003 103 110 [28] K. Stadlthanner, A.M. Tomé, F. Theis, E.W. Lang, W. Gronwald, H.R. Kalbitzer, Separation of water artifacts in 2D NOESY protein spectra using congruent matrix pencils, Neurocomputing, 69 (2006) 497–522. [29] K. Stadlthanner A.M. Tomé F.J. Theis W. Gronwald H.-R. Kalbitzer E.W. Lang Blind source separation of water artifacts in NMR spectra using a matrix pencil Proceedings of the Fourth International Symposium on Independent Component Analysis and Blind Source Separation, ICA’2003 2003 Nara Japan 167 172 [30] F. Takens On the numerical determination of the dimension of an attractor Dynamical Systems Turbulence Ann. Notes Math. 898 1981 366 381 [31] F.J. Theis, A. Meyer-Bäse, E.W. Lang, Second-order blind source separation based on multi-dimensional autocovariances, in: Proceedings of the ICA 2004, Lecture Notes on Computer Science, vol. 3195, Granada, Spain, 2004, pp. 726–733. [32] A.M. Tome Blind source separation using a matrix pencil Proceedings of the International Joint Conference on Neural Networks, IJCNN’2000 Como, Italy 2000 [33] A.M. Tomé An iterative eigendecomposition approach to blind source separation Proceedings of the Third International Conference on Independent Component Analysis and Signal Separation, ICA’2003 San Diego, USA 2001 424 428 [34] A.M. Tomé N. Ferreira On-line source separation of temporally correlated signals Proceedings of the European Signal Processing Conference, EUSIPCO2002 2002 Toulouse France [35] A.M. Tomé, A.R. Teixeira, E.W. Lang, K. Stadlthanner, A.P. Rocha, Blind source separation using time delayed signals, in: Proceedings of the International Joint Conference on Neural Networks, IJCNN’2004, vol. CD, Budapest, Hungary, 2004. [36] A.M. Tomé, A.R. Teixeira, E.W. Lang, K. Stadlthanner, A.P. Rocha, R. ALmeida, dAMUSE—A new tool for denoising and BSS. Digital Signal Processing, 2005. [37] L. Tong R. wen Liu V.C. Soon Y.-F. Huang Indeterminacy and identifiability of blind identification IEEE Trans. Circuits Syst. 38 5 1991 499 509 [38] R. Vetter, Extraction of efficient and characteristic features of multidimensional time series, Ph.D. Thesis, EPFL, Lausanne, 1999. [39] R. Vetter J.M. Vesin P. Celka P. Renevey J. Krauss Automatic nonlinear noise reduction using local principal component analysis and MDL parameter selection Proceedings of the IASTED International Conference on Signal Processing Pattern Recognition and Applications (SPPRA 02) Crete 2002 290 294 [40] P. Vitányi M. Li Minimum description length induction, bayesianism, and kolmogorov complexity IEEE Trans. Inf. Theory 46 2000 446 464 Peter Gruber was born in Bad Homburg, Germany, on April 12, 1976. He obtained a degree in Mathematics in 2002 at the University of Regensburg. He is currently working on his Ph.D. thesis at the Biophysics Department of the University of Regensburg. His research topics include statistical signal processing, linear and nonlinear independent component analysis and geometric measure theory. Kurt Stadlthanner received his Diploma degree in Physics from the University of Regensburg in 2003. He is currently a doctoral student at the Institute of Biophysics, Neuro- and Bioinformatics group, at the University of Regensburg. His scientific interests are in the fields of biological data processing and analysis by means of blind source separation and support vector machines. M. Bóhm received his physics diploma from the University of Regensburg in 2004. He is currently a doctoral student at the Institute of Biophysics, Neuro- and Bioinformatics Group, University of Regensburg. His scientific interests are in the fields of blind source separation, bio-inspired optimization and brain modelling. Fabian J. Theis obtained M.Sc. degrees in Mathematics and Physics at the University of Regensburg in 2000. He also received a Ph.D. degree in Physics from the same university in 2002 and a Ph.D. in Computer Science from the University of Granada in 2003. He worked as visiting researcher at the department of Architecture and Computer Technology (University of Granada, Spain), at the RIKEN Brain Science Institute (Wako, Japan) and at FAMU-FSU (Florida State University, USA). Currently, he is heading the 'signal processing & information theory’ group at the Institute of Biophysics, University of Regensburg, Germany. His research interests include statistical signal processing, linear and nonlinear independent component analysis, overcomplete blind source separation and biomedical data analysis. Elmar W. Lang received his physics diploma with excellent grade in 1977 and his Ph.D. in physics (summa cum laude) in 1980 and habilitated in Biophysics in 1988 at the University of Regensburg. He is apl. Professor of Biophysics at the University of Regensburg. He is currently associate editor of Neurocomputing and Neural Information Processing—Letters and Reviews. His current research interests focus mainly on machine learning and include biomedical signal processing, independent component analysis and blind source separation, neural networks for classification and pattern recognition as well as stochastic process limits in queuing applications. Ana M. Tomé received her Ph.D. in Electrical Engineering from University of Aveiro in 1990. Currently she is Associate Professor of Electrical Engineering in the Department of Electronics and Telecomunications/IEETA of the University of Aveiro where she teaches courses on Digital Signal Processing for Electronics and Computer Engineering Diploms. Her research interests include Digital and Statistical Signal Processing, Independent Component Analysis and Blind Source Separation as well as Classification and Pattern Recognition Applications of Neural Networks. Ana Rita Teixeira received her diploma degree in mathematics applied to technology from University of Porto in 2003. Currently, she is finishing the M.Sc. of electronics and telecommunications at the University of Aveiro. Her research interests include biomedical digital signal processing as well as principal and independent component analysis. Carlos G. Puntonet received a B.Sc. degree in 1982, an M.Sc. degree in 1986 and his Ph.D. degree in 1994, all from the University of Granada, Spain. These degrees are in electronics physics. Currently, he is an Associate Professor at the “Departamento de Arquitectura y Tecnología de Computadores” at the University of Granada. His research interests lie in the fields of signal processing, linear and nonlinear independent component analysis and blind separation of sources, artificial neural networks and optimization methods. J.M. Górriz received the B.Sc. in Physics and Electronic Engineering from the University of Granada, Spain and the Ph.D. from the University of Cádiz, Spain in 2000, 2001, and 2003, respectively. He is currently Assistant Professor at the University of Granada. He is actually developing a Ph.D. in Voice Activity Detection, Robust Speech Recognition and Optimization Strategies. His present interests are in Statistical Signal Processing and its applications to speech. "
    },
    {
        "doc_title": "Sparse nonnegative matrix factorization applied to microarray data sets",
        "doc_scopus_id": "33745711532",
        "doc_doi": "10.1007/11679363_32",
        "doc_eid": "2-s2.0-33745711532",
        "doc_date": "2006-07-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Multivariate data",
            "Nonnegative Matrix Factorization (NMF)",
            "Sparse nonnegative matrix",
            "Target function"
        ],
        "doc_abstract": "Nonnegative Matrix Factorization (NMF) has proven to be a useful tool for the analysis of nonnegative multivariate data. However, it is known not to lead to unique results when applied to nonnegative Blind Source Separation (BSS) problems. In this paper we present first results of an extension to the NMF algorithm which solves the BSS problem when the underlying sources are sufficiently sparse. As the proposed target function has many local minima, we use a genetic algorithm for its minimization. © Springer-Verlag Berlin Heidelberg 2006.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of simulated annealing to automatically assign decorrelated components in second-order blind source separation",
        "doc_scopus_id": "33646354420",
        "doc_doi": "10.1109/TBME.2005.863968",
        "doc_eid": "2-s2.0-33646354420",
        "doc_date": "2006-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "2-D NOESY NMR",
            "Denoising",
            "Generalized eigenvalue decomposition",
            "Matrix pencil"
        ],
        "doc_abstract": "In this paper, an automatic assignment tool, called BSS-AutoAssign, for artifact-related decorrelated components within a second-order blind source separation (BSS) is presented. The latter is based on the recently proposed algorithm dAMUSE, which provides an elegant solution to both the BSS and the denoising problem simultaneously. BSS-AutoAssign uses a local principal component analysis (PCA)to approximate the artifact signal and defines a suitable cost function which is optimized using simulated annealing. The algorithms dAMUSE plus BSS-AutoAssign are illustrated by applying them to the separation of water artifacts from two-dimensional nuclear overhauser enhancement (2-D NOESY) spectroscopy signals of proteins dissolved in water. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of KPCA to extract artifacts in one-dimensional biomedical signals",
        "doc_scopus_id": "38949142965",
        "doc_doi": "10.1109/MLSP.2006.275580",
        "doc_eid": "2-s2.0-38949142965",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Extract artifacts",
            "Kernel principal component analysis(KPCA)"
        ],
        "doc_abstract": "Kernel principal component analysis(KPCA) is a nonlinear projective technique that can be applied to decompose multi-dimensional signals and extract informative features as well as reduce any noise contributions. In this work we extend KPCA to extract and remove artifact-related contributions as well as noise from one-dimensional signal recordings. We introduce an embedding step which transforms the one-dimensional signal into a multi-dimensional vector. The latter is decomposed in feature space to extract artifact related contaminations. We further address the preimage problem and propose an initialization procedure to the fixed-point algorithm which renders it more efficient. Finally we apply KPCA to extract dominant Electrooculogram (EOG) artifacts contaminating Electroencephalogram (EEG) recordings in a frontal channel. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The generalized eigendecomposition approach to the blind source separation problem",
        "doc_scopus_id": "33646158189",
        "doc_doi": "10.1016/j.dsp.2005.06.002",
        "doc_eid": "2-s2.0-33646158189",
        "doc_date": "2006-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Statistics, Probability and Uncertainty",
                "area_abbreviation": "DECI",
                "area_code": "1804"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Correlation matrices",
            "Generalized eigendecomposition",
            "Matrix pencil",
            "Spectral factorization"
        ],
        "doc_abstract": "This paper proposes a novel formulation of the generalized eigendecomposition (GED) approach to blind source separation (BSS) problems. The generalized eigendecomposition algorithms consider the estimation of a pair of correlation matrices (a matrix pencil) using observed sensor signals. Each of various algorithms proposed in the literature uses a different approach to form the pencil. This study proposes a linear algebra formulation which exploits the definition of congruent matrix pencils and shows that the solution and its constraints are independent of the way the matrix pencil is computed. Also an iterative eigendecomposition algorithm, that updates separation parameters on a sample-by-sample basis, is developed. It comprises of: (1) performing standard eigendecompositions based on power and deflation techniques; (2) computing a transformation matrix using spectral factorization. Another issue discussed in this work is the influence of the length of the data segment used to estimate the pencil. The algorithm is applied to artificially mixed audio data and it is shown that the separation performance depends on the eigenvalue spread. The latter varies with the number of samples used to estimate the eigenvalues. © 2005 Elsevier Inc. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272384 291210 291718 291874 31 Digital Signal Processing DIGITALSIGNALPROCESSING 2005-07-14 2005-07-14 2010-10-08T01:00:40 S1051-2004(05)00097-7 S1051200405000977 10.1016/j.dsp.2005.06.002 S300 S300.1 HEAD-AND-TAIL 2015-05-15T04:43:22.393274-04:00 0 0 20060501 20060531 2006 2005-07-14T00:00:00Z rawtext articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype volfirst volissue affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 1051-2004 10512004 16 16 3 3 Volume 16, Issue 3 10 288 302 288 302 200605 May 2006 2006-05-01 2006-05-31 2006 Emerging Techniques and Applications article fla Copyright © 2005 Elsevier Inc. All rights reserved. GENERALIZEDEIGENDECOMPOSITIONAPPROACHBLINDSOURCESEPARATIONPROBLEM TOME A CARDOSO 1996 3017 3030 J TONG 1991 499 509 L MOLGEDEY 1994 3634 3637 L CHANG 2000 900 907 C STONE 2001 1557 1574 J HUNDLEY 2002 1505 1508 D DIAMANTARAS 1996 K PRINCIPALCOMPONENTNEURALNETWORKSTHEORYAPPLICATIONS PARLETT 1998 B CLASSICSAPPLMATH SYMMETRICEIGENVALUEPROBLEM MA 2000 1187 1192 C MARTINEZ 2003 228 235 D BELOUCHRANI 1997 434 444 A PARRA 2003 1261 1269 L HYVARINEN 1996 1483 1492 A CICHOCKI 2002 A ADAPTIVEBLINDSIGNALIMAGEPROCESSING HURRI J TOMEX2006X288 TOMEX2006X288X302 TOMEX2006X288XA TOMEX2006X288X302XA item S1051-2004(05)00097-7 S1051200405000977 10.1016/j.dsp.2005.06.002 272384 2010-11-08T02:38:10.204049-05:00 2006-05-01 2006-05-31 true 189111 MAIN 15 34371 849 656 IMAGE-WEB-PDF 1 Digital Signal Processing 16 (2006) 288–302 www.elsevier.com/locate/dsp The generalized eigendecomposition approach to the blind source separation problem Ana Maria Tomé ∗ Departamento Electrónica e Telecomunicações/IEETA, Universidade Aveiro, 3810-193 Aveiro, Portugal Available online 14 July 2005 Abstract This paper proposes a novel formulation of the generalized eigendecomposition (GED) approach to blind source separation (BSS) problems. The generalized eigendecomposition algorithms consider the estimation of a pair of correlation matrices (a matrix pencil) using observed sensor signals. Each of various algorithms proposed in the literature uses a different approach to form the pencil. This study proposes a linear algebra formulation which exploits the definition of congruent matrix pen- cils and shows that the solution and its constraints are independent of the way the matrix pencil is computed. Also an iterative eigendecomposition algorithm, that updates separation parameters on a sample-by-sample basis, is developed. It comprises of: (1) performing standard eigendecompositions based on power and deflation techniques; (2) computing a transformation matrix using spectral fac- torization. Another issue discussed in this work is the influence of the length of the data segment used to estimate the pencil. The algorithm is applied to artificially mixed audio data and it is shown that the separation performance depends on the eigenvalue spread. The latter varies with the number of samples used to estimate the eigenvalues. © 2005 Elsevier Inc. All rights reserved. Keywords: Blind source separation; Generalized eigendecomposition; Correlation matrices; Matrix pencil * Fax: +351 234 370545. E-mail address: ana@ieeta.pt. 1051-2004/$ – see front matter © 2005 Elsevier Inc. All rights reserved. doi:10.1016/j.dsp.2005.06.002 A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 289 1. Introduction Blind source separation addresses the problem of recovering a set of source signals (s) after they are linearly mixed by an unknown mixing matrix (A). The measured sensor signals (x) are assumed to follow a linear superposition model, i.e., x = As. The problem is to find a separation matrix (B) with ˆs = Bx such that s similarequalˆs, without knowing the mixing matrix (A) and the source signals (s). It is also assumed that the number (m) of sensor signals is at least as large as the number (n) of source signals, i.e., mgreaterorequalslantn holds. Most of the solutions for blind source separation comprise two steps as described by Cardoso and Laheld [1] and Tong et al. [2]. In a first step, called the whitening (sphering) phase, the sensor signals x are linearly transformed z = Wxsuch that the correlation matrix of the transformed signal vector z equals the identity matrix. This linear transformation is usually computed using a standard eigendecomposition of the correlation matrix of the zero mean sensor signals. During this phase the dimensionality m of the sensor signal vector can also be reduced to the dimension nlessorequalslantm of the underlying source vector. After the whitening step, the separation matrix, relating the whitened data z and the output y,is an orthogonal matrix (O) which can be estimated during the second step applying different strategies. For example, Tong et al. [2] propose the algorithms AMUSE or EFOBI where a standard eigendecomposition is performed during the second step of a matrix derived from time-delayed correlations of whitened sensor signals or of a corresponding fourth-order cumulant matrix. Hence the global separation matrix (B), corresponding to an estimate of the inverse of the mixing matrix A, is the product of the two matrices, i.e., B = WO, computed during the two steps of the method. Note, however, that B corresponds to A −1 only up to permutation and scaling indeterminacies [2]. But the problem can be addressed also as a generalized eigendecomposition (GED) approach. The solution then comprises the simultaneous diagonalization of a matrix pen- cil (R x1 ,R x2 ) formed with correlation matrices R xi , i = 1,2, computed with the sensor signals. These correlation matrices are calculated with different strategies. Most of them exploit temporal correlations of the data. Souloumiac [3] considers two segments of sen- sor signals with distinct energy; Molgedey and Schuster [4] and also Chang et al. [5,6] compute time-delayed correlation matrices, while Tomé [7], Stone [8], and Hundley et al. [9] consider filtered versions of the sensor signals to compute the correlation matrices. The latter works are of particular interest to the present study as they deal with solution methods of the GED which can be considered equivalent to a filtering of the sensor signals to compute correlation matrices of the pencil. For instance, Hundley et al. [10] perform a GED of a pencil formed with one of the correlation matrices computed with the derivatives of the sensor signals. Stone proposes a measure of temporal predictability that depends on the Rayleigh quotient. The latter, as defined in Ref. [11] or [12], is known to have stable points at the eigenvectors of the matrix pencil and it is known that generalized Rayleigh quotients are equivalent to a GED [11]. Finally, GED methods have also been applied to convolutive mixtures models [13] as well as to nonlinear mixtures [14]. Using GED methods, the transpose of the eigenvector matrix of the pencil forms an estimate of the separation matrix B. But simultaneous diagonalization methods turned out to be sensitive to additive noise. To resolve that issue it has been proposed to resort to 290 A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 an approximative joint diagonalization of a set of time-delayed correlation matrices [15] instead. In summary, the diagonalization of a matrix pencil based on second-order statistics is recognized to be a simple approach to blind signal separation problems in spite of its sensi- tivity to additive noise. Note that Parra and Sajda [16] recently reviewed GED approaches to BSS problems referring to assumptions concerning signal statistics different from the one discussed in this study. In the following a concise algebraic formulation of GED approaches based on matrix pencils is discussed. This approach can be applied to signals in either the time—or the frequency domain. The formulation unifies GED approaches which used different ways to compute the correlation matrices to be simultaneously diagonalized. Using the concept of congruent pencils [12] it is easy to deduce the constraints of the GED approach to the blind source separation problem. This concept and the use of block matrix operations provides a general framework to understand the GED approach when the number (m) of mixtures is equal or larger than the number (n) of sources, i.e., m greaterorequalslant n. This methodology will be applied to correlation matrices computed at the input and at the output of linear finite impulse response filters (FIR) but it is applicable also if time-delayed correlation matrices are considered. The implementation of the GED is another issue discussed in this letter. A very common approach to compute the generalized eigendecomposition [12] is to reduce the statement to a standard eigendecomposition problem. For that a matrix transformation is needed. In this letter we propose a transformation matrix that can be formulated using spectral factor- ization. This way it is possible to develop an iterative algorithm comprising two standard eigendecompositions in the main loop. Each eigendecomposition is computed using power and deflation techniques. Some experimental results using artificial signals prove the re- liability of the method as well as its limitations. Also the results show that the method outperforms the most widely used BSS algorithm, FastICA, in most situations or yields comparable results at least. Also note that with high dimensional problems the present approach seems to be much faster than FastICA [17]. 2. The generalized eigendecomposition The solution for the blind source separation problem is based on the generalized eigen- decomposition of a matrix pencil (R x1 ,R x2 ) computed with the sensor signals. The sensor pencil is related to a pencil computed with the source signals via the mixing matrix A as described by the following relations: R x1 = AR s1 A T and R x2 = AR s2 A T , (1) where (R s1 ,R s2 ) represents the source matrix pencil, (R x1 ,R x2 ) represents the sensor pen- cil, and A denotes the instantaneous mixing matrix. According to Parlett [12] two pencils related as described by Eq. (1) are called congruent pencils if A is an invertible matrix, i.e., A is a nonsingular square matrix. But in blind source separation problems the matrix A can be a rectangular m × n matrix, i.e., the number of sensor signals (m) is greater or equal than the number of source signals (n). Nevertheless, as shown next, the pseudo-inverse (or A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 291 the inverse if m = n) of the mixing matrix can be estimated, using the sensor pencil, if the source pencil has distinct eigenvalues and the corresponding eigenvectors form a diagonal matrix. Proposition 1. Let λ s 1 ,λ s 2 ,...,λ s n be the eigenvalues of the source pencil (R s1 ,R s2 ) and λ x 1 ,λ x 2 ,...,λ x m the eigenvalues of the corresponding sensor pencil (R x1 ,R x2 ). Then these two pencils also will be called congruent if nlessorequalslantm of the eigenvalues λ x i are equal to the eigenvalues λ s i , i = 1,...,n. Note that this proposition extends the definition of congruent pencils given by [12] to the situation of a larger dimension of the sensor pencil as compared to the source pencil. Of course, the size of the matrices in each pencil depends on the size of the mixing matrix. If A is a square matrix (m = n) then both pencils have matrices n × n otherwise the sensor pencil, (R x1 ,R x2 ), has m×m matrices and the source pencil, (R s1 ,R s2 ), has n×n matrices. Then the following situations need to be considered: • The mixing matrix A is square m = n. The eigenvalues of the pencil form the roots of the characteristic polynomial, χ(λ), χ(λ)= det(R x1 − λR x2 ) = 0( if A is an invertible matrix and so det(R x1 − λR x2 ) = det(A)det(R s1 − λR s2 )det(A T ), which has the same roots as the characteristic polynomial of the source matrix pencil ψ(λ)= det(R s1 − λR s2 ). (3) • The mixing matrix A has full column rank m>n. When A is a rectangular matrix (m > n), if A T A is an invertible matrix, the pencil (A T AR s1 A T A, A T AR s2 A T A) also has n eigenvalues equal to the source pencil as proved in the previous case. Then, the mixed pencil also has n eigenvalues identical to those of the source pencil. a50 Proposition 2. The eigenvectors (e s ) of the source pencil are related to the eigenvectors (e) of the mixed pencil by the mixing matrix (A), e s = αA T e. The generalized eigendecomposition statement of the sensor pencil reads R x1 E = R x2 ED, (4) where E is the eigenvector matrix of the sensor pencil. It will be an unique matrix (with the columns normalized to unit length) if the diagonal matrix D has distinct eigenvalues, λ i . Otherwise the eigenvectors which correspond to the same eigenvalue might be substituted by their linear combinations without affecting the previous equality. So, supposing that the diagonal elements of D are all distinct, Eq. (4) can be written as AR s1 A T E = AR s2 A T ED. (5) 292 A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 • If A is an invertible matrix, we can multiply both sides of the equality by A −1 and setting E s = A T E (6) leads to the eigendecomposition of the source pencil R s1 E s = R s2 E s D, (7) where E s is the eigenvector matrix of the source pencil. In Eq. (6) each column of E s is related to a column of E by the transpose of the mixing matrix. Then, the normalized eigenvectors for a particular eigenvalue are related by e s = αA T e where α is a constant that normalizes the eigenvectors to unit length. • When the mixing matrix is a m×n(m>n)matrix, Eq. (5) can be written using block matrix notation. Consider A and E divided into two blocks: A into A H , n × n, and A L , (m − n) × n; E into E H , n × m, and E L ,(m− n) × m. Therefore, performing matrix block operations, Eq. (5) can be written as A H R s1 Φ = A H R s2 ΦD, A L R s1 Φ = A L R s2 ΦD, (8) where Φ = A T H E H + A T L E L = A T E (9) is an n × m matrix. If A H is an invertible matrix the first equation in (8), the eigen- decomposition of the source pencil. Then Φ must have (m − n) columns of zeroes paired with the eigenvalues in D that do not belong to the eigenvalue decomposition of (R s1 ,R s2 ). a50 In what concerns BSS problems, the transpose of the eigenvector matrix E T forms the separation matrix (B), i.e., ˆs = Bx = E T x (10) if E s (the source eigenvector matrix) is a diagonal or a permutation matrix. Then ˆs will be an estimate of the source signals except for the usual scaling and ordering indetermi- nacies. When the number of mixtures (m) is larger than the number of sources (n), there are (m − n) zero amplitude signals in the vector s of Eq. (10), if a noise-free model is considered. To summarize, the GED approach to blind source separation is feasible if the congruent pencils have distinct eigenvalues and if the eigenvector matrix of the source pencil is the identity matrix (or a permutation). Then, the source matrix pencil should be diagonal with distinct quotients between its diagonal elements. Note that this is a much stronger con- straint than having distinct eigenvalues. In a practical situation, the diagonality constraint might cause a problem because, when working with estimates, we may have very small values off the main diagonal, which prevent the source eigenvector matrix from being an identity. A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 293 3. Computing the eigendecomposition of symmetric pencils There are several ways to compute the eigenvalues or the eigenvectors of a matrix pencil, if at least one of the matrices is symmetric positive definite [12]. A very common approach is to reduce the GED statement to the standard form, i.e., to the eigenvalue decomposition problem. Consider the problem of computing the eigenvalues and the eigenvectors of the pencil (R x1 ,R x2 ) R x2 E = R x1 ED. (11) The reduction of the previous equation to the standard form ˆ CZ = ZD, is achieved by solving the eigendecomposition of the matrix R x1 . Then R x1 = UDelta1U T and if R x1 is pos- itive definite, it can be decomposed further R x1 = UDelta1 1/2 U T UDelta1 1/2 U T = WW, (12) where U is the eigenvector matrix and Delta1 is the diagonal eigenvalue matrix. Substituting R x1 = WW in Eq. (11) and denoting Z = WE,leads to the standard eigendecomposition W −1 R x2 W −1 Z = ZD, (13) where the matrix ˆ C = W −1 R x2 W −1 is symmetric positive definite when R x2 is symmetric positive definite. The transformation matrix W −1 = UDelta1 −1/2 U T (14) should be computed with the non-zero eigenvalues and the corresponding eigenvectors. With the eigendecomposition solution of ˆ C, the eigenvalues of pencil (11) (D)arealso available, while the eigenvectors are computed by E = W −1 Z. There are other ways to decompose matrix R x1 [12, Chapter 15], however with the suggested decomposition the transformation matrix, W −1 , can be written using spectral factorization. Then, an iterative eigendecomposition algorithm [18] can be developed using power and deflation techniques in each standard eigendecomposition. Consequently an iterative blind source separation algorithm is achieved by including an updating of the matrix pencil in the iteration steps as will be described in Section 3.1. 3.1. Computation of the sensor pencil There are different suggestions to compute the mixed matrix pencil (R x1 ,R x2 ). It can be shown that a pencil of correlation matrices computed respectively with mixed data and filtered mixed data has a congruent pencil in source domain as described by Eq. (1). Let X be a m × N matrix containing a segment with N samples of each of m measured signals. The correlation matrix for X,am × m matrix, is calculated as 1 R x1 = N XX T . (15) 294 A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 Consider now that a FIR (finite impulse response) filter of order M(Mlessmuch N) is applied to the sensor signals. The filtered sample k of the sensor signal m, is a weighted sum of (M + 1) original samples, i.e., y(m,k)= M summationdisplay n=0 h(n)x(m,k − n), (16) where h(n) are the coefficients (or impulse response) of the filter. However, as the same filter is applied individually to all sensor signals, each one comprising N samples, the con- volution operation can be expressed by Y = XH T , where H is a N × N Toeplitz matrix with h(n) on the nth diagonal (h(0) on the main diagonal, h(1) on the next lower diag- onal and so on). So, Y will be a m × N matrix having in each row a filtered version of each row in X. Note that any FIR filtering can be expressed that way but the coefficients characterize the type of filter applied. For instance, the derivative discussed in Ref. [10] can be considered a filtering operation using an FIR filter with coefficients h(0) = 1 and h(1) =−1. Then the correlation matrix of the filtered data forms a m × m matrix defined by R x2 = 1 N XH T HX T = YY T . (17) Substituting X = AS, in Eqs. (15) and (17) the corresponding source domain pencil is ( 1 N SS T , 1 N SH T HS T ) which is congruent to the mixed pencil as defined in Eq. (1). Other second order techniques reported in Refs. [2,15] use time-delayed correlation ma- trices. In AMUSE [2], the R x2 is a time-delayed correlation matrix while R x1 is described by Eq. (15), while in Ref. [6] both matrices in the pencil can be time-delayed. The time- delay correlation matrix is the correlation of the delayed data with the original data, then R x2 for a delay d can be written as R x2 = 1 N−d XH T X T . Where H has only one nonzero diagonal which is the dth diagonal lower than the main diagonal (d negationslash= 0, if d = 0, H will be the identity matrix and (15) is obtained). Substituting X = AS, we can easily verify that the definition of congruent pencils also holds when time-delayed correlation matrices are considered. Both approaches to compute the pencil correspond to second order statistics techniques. In a GED of a pencil of time-delayed (TD) correlation matrices, the pair of matrices com- puted for different time-delays constitute a pencil whose eigenvalues are distinct. Hence, the time-structure of the source signals can be considered being different. In a GED of a pencil of correlation matrices computed with signals at the input and the output of an FIR filter also results in distinct eigenvalues. This is because the filter modifies the input source spectra resulting in a different energy content of the output spectra. 3.2. Iterative eigendecomposition In Ref. [16] the generalized eigendecomposition is solved using the commands available on MATLAB, here we describe an iterative procedure where the parameters (eigenvalues and eigenvectors) can be available with different number of samples and using vector and matrix operations. The correlation matrices can be updated on a sample-by-sample basis, A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 295 for instance the correlation matrix of the mixed signals in iteration k (after a segment with k samples) is obtained as R x1 (k) = (1 − 1/k)R x1 (k − 1) + (1/k)X(:,k)X(:,k) T . (18) The correlation matrix of the filtered mixed signal is computed in a similar way substi- tuting the vector X(:,k) by the vector of filtered signals, Y(:,k). Where the sample k of mixed signal m can be computed using Eq. (16). The generalized eigendecomposition of the pencil (R x1 (k),R x2 (k)) as described in the last section uses the standard eigendecom- position into two consecutive steps. However, using the transformation matrix (14) written as a spectral factorization W −1 = summationdisplay i 1 √ δ i u i u T i (19) both steps are included in the main loop (k). Both standard eigendecompositions are achieved using power method and deflation techniques (as resumed in Appendix A and described in Ref. [11]). To assure the numerical stability of the algorithm, a criterion must be used in order to include a pair (u i ,δ i ), i = 1,...,m, of eigenvectors and eigenvalues of the standard eigendecomposition of R x1 (k) into the summation of Eq. (19). The mean square error ε i = epsilon1 T i epsilon1 i , where epsilon1 i = R x1 (k)u i −δ i u i , was the criterion used, as described in Ref. [18] and summarized in Appendix A. Then, having an estimate of the transformation matrix available, the eigendecomposition of ˆ C (13) can also start. The computed eigenval- ues, at the end of the data segment, have numerical values comparable to those obtained by the eig command of MATLAB [18]. As both standard eigendecompositions are based on the power method and deflation techniques, the eigenvalues are always estimated in descending order of magnitude. 4. Numerical simulations We present numerical simulations concerning the evaluation of the GED approaches comparing them with one of the most widely used method (FastICA) [19]. The comparison is carried out using different strategies to compute the correlation matrices: time-delayed matrices (GED/TD) as it was described in Ref. [6] and at the input and at output of a fi- nite impulse response filter (GED/FIR). It has to be noticed that the FastICA algorithm exploits higher order statistical dependencies considering that each time instance of data is a random vector. Those experiments are conducted using artificially mixed signals also used in the cited works and evaluating the performance with the performance index para- meter [20]. A second set of simulations provide a more detailed study of GED/FIR method. The iterative algorithm for generalized eigendecomposition as described in Section 3.2 and resumed in Appendix A is used using different filters to compute the pencil. 4.1. Evaluation of GED approaches The numerical simulations are carried out using the source signals of the MATLAB demo of the algorithm FastICA [21]. A segment of N (1000) samples of the source 4 sig- nals (S) are mixed by a random matrix (A), i.e., X = AS. Then the separation matrix (B) 296 A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 is computed using GED approach (GED/TD and GED/FIR) and FastICA algorithm. The separated signals ˆ S = BX are then computed for each of the methods. However, the meth- ods are evaluated using the performance index parameter adapted from Amari’s works and described also in Ref. [20]. In this work we used the performance index to evaluate the de- gree of diagonalization of the matrix C = BA, i.e., of the product of the separation matrix by the mixing matrix. Ideally that product should be a diagonal matrix or a permutation, then for each row i of C p k i = max(|C i |) summationtext |C ij | , (20) where the absolute maximum of row i occurs in column k. Then the separated signal in the row i of ˆ S corresponds to source signal in row k of the sources S. However, the maximum of other row (j negationslash= i) should not occur also in column k otherwise p k i is not considered valid. The source signals are all considered separated if, after computing p k i for all rows of C,thevalueofk in each performance index is different. Table 1 presents mean values using a total of 100 trials where a random mixing matrix is used for each trial. It can be seen that the GED/TD method yields the lowest performance index for all the source signals while GED/FIR and FastICA have very similar values. It has to be noted that both FastICA and GED/FIR separate all the sources in every trial Table 1 Comparison of performance indices resulting from an application of the algorithms GED/FIR, GED/TD, and FastICA to blindly separate signals taken from the FastICA toolbox (source 1: sinusoid, source 2: funny curve, source 3: sawtooth, source 4: impulsive noise) Method Source 1 Source 2 Source 3 Source 4 GED/FIR 0.98 0.94 0.96 0.90 GED/TD 0.94 0.75 0.76 0.89 FastICA 0.95 0.95 0.95 0.89 Fig. 1. (a) Signals mixed by 4 × 4 mixing matrix. (b) Signals separated by FastICA algorithm. (c) Signals sepa- rated by GED/TD using two time-delayed correlation matrices (d 1 = 1; d 2 = 2). (d) Signals separated by GED/ FIR computing correlation matrices at input and at output of a FIR (h =[0.50.5]). A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 297 while GED/TD does not achieve a complete separation in 30% of the trials meaning that in those cases the maxima of p k i occurred in the same column for two different rows of C. Figure 1 shows results of the separation obtained in one of the trials. It can be seen that FastICA and GED/FIR separate all the signals. The correlation coefficients between the recovered and the corresponding source signals are approximately 0.99. The GED/TD also yields correlation coefficients of 0.99 roughly, except for the fourth recovered signal (see Fig. 1c). Its correlation coefficient involving source 3 (sawtooth wave) amounts to 0.88, but it has also a high correlation coefficient (0.51) with source 4 (impulsive noise). The simulations were repeated using different parameters for the GED approaches as there are other delays in GED/TD and other filters in GED/FIR. The best results of GED/TD were always achieved with very small delays (d 1 = 0, d 2 = 1, as is often used for AMUSE [20]). In that case the performance indices are very similar to the other two methods. The GED/FIR method is studied in more detail in Section 4.2. 4.2. Iterative solution of the GED using FIR filters The experiments reported here use audio signals (classic music played on the piano), sampled at 11,025 Hz and a random matrix is used to linearly mix the audio signals. The matrix pencil is computed at the input and at the output of linear finite impulse response filters (FIR) with 11 coefficients and distinct frequency response (see Fig. 2). The matrix pencil eigendecomposition is computed using the iterative algorithm described in Appen- dix A, for a data segment of 50,000 samples. However, the parameters (eigenvalues and Fig. 2. Frequency response magnitude of the used filters. 298 A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 eigenvectors) are saved in steps of 100 samples. The recovered signals are computed us- ing the different estimates of the eigenvector matrix and then the correlation coefficients between sources and recovered signals are calculated. The three audio source signals are mixed with a 3 × 3 mixing matrix, the generalized eigendecomposition is performed for the different filters. As the different filters shape the source spectra in different ways, each eigendecomposition results in different values for the estimated eigenvalues. The highest values are observed for the lowpass and stopband filter because these filters preserve the highest percentage of the energy of the source signals (up 1000 Hz). Figure 3 shows the evolution of the stored eigenvalues along the data segment for the different filters. For each filter, the sensor signals are separated using the estimated eigen- vector matrices. Computing the correlation coefficients between the sources and the recov- ered signals with the different parameter estimates, quite different values are obtained. In particular, taking apart the initial transient behavior, the best results were achieved with the eigenvector matrices corresponding to the most widely separated eigenvalues for any particular filter. For instance, using the stopband filter, the best results were achieved us- ing data segments with 5000 <N<12,000 samples (50 <k<120 in Fig. 3). For longer data segments, the recovered signals are correlated with more than one source signal as the estimated eigenvalues are nearly degenerate. Table 2 presents the corresponding correlation coefficients using the eigenvector ma- trix estimated for 5000 samples (k = 50) of the data. In this case there is a maximum (approximately one) in each row/column and all other coefficients are very small. At the end of the data segment, i.e., with 50,000 samples (k = 500), the maximum values of the Fig. 3. Evolution of the eigenvalues along the data segment. Square mixing matrix (m = 3,n= 3). A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 299 correlation coefficients are smaller and there are also other correlation coefficients on the same row/column that are significant. In this case, listening the recovered signal r 2 ,the source s 1 predominates but the source s 2 is also audible. Also with all other filters studied there appear data regimes with nearly coinciding eigenvalues, resulting in more than one correlation coefficient in each column/row which is significantly different from zero (see Tables 3–5). To achieve a good signal separation, however, well separated eigenvalues are a must. Hence it appears that the dependence of the eigenvalue spread on the number of samples (segment length) needs to be studied to identify suitable data segments which will guarantee a good signal separation. Similar results are achieved if the number of mixtures (m) is higher than the num- ber of sources (n). As the mixing matrix is rank deficient, using the proposed algorithm, the first standard eigendecomposition (of R x1 ) has (m − n) eigenvalues equal to zero. Consequently, the transformation matrix as well as the transformed matrix will be rank Table 2 Absolute values of the correlation coefficients between sources and recovered signals, using the stopband filter for k = 50 and 500 k = 50 s 1 s 2 s 3 k = 500 s 1 s 2 s 3 r 1 0.003 0.032 0.999 r 1 0.165 0.984 0.001 r 2 0.007 1.000 0.005 r 2 0.853 0.527 0.064 r 3 1.000 0.013 0.012 r 3 0.197 0.146 0.971 Table 3 Absolute values of the correlation coefficients between sources and recovered signals, using the lowpass filter for k = 150 and 500 k = 150 s 1 s 2 s 3 k = 500 s 1 s 2 s 3 r 1 0.015 0.523 0.852 r 1 0.003 0.999 0.002 r 2 0.006 0.990 0.137 r 2 0.997 0.047 0.048 r 3 0.999 0.023 0.043 r 3 0.152 0.067 0.987 Table 4 Absolute values of the correlation coefficients between sources and recovered signals, using the bandpass filter for k = 110 and 500 k = 110 s 1 s 2 s 3 k = 500 s 1 s 2 s 3 r 1 0.015 0.998 0.047 r 1 0.005 1.000 0.006 r 2 0.756 0.004 0.660 r 2 0.994 0.019 0.112 r 3 0.821 0.039 0.561 r 3 0.247 0.027 0.966 Table 5 Correlation magnitudes between sources and recovered signals, using the highpass filter for k = 260 and 500 k = 260 s 1 s 2 s 3 k = 500 s 1 s 2 s 3 r 1 0.000 0.999 0.006 r 1 0.004 1.000 0.003 r 2 0.906 0.026 0.413 r 2 0.999 0.024 0.004 r 0.834 0.019 0.559 r 0.020 0.007 0.999 3 3 300 A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 deficient also (see Appendix A). Then, using the proposed algorithm, only n eigenvectors of dimension m are calculated. As it should be expected, the corresponding eigenvalues are of the same order of magnitude as the ones of the previous experiments, hence a similar evolution along the data segment, as shown in Fig. 3, results. In what concerns the separation performance, the results are similar to those of the square case m = n.In summary, the eigenvalue spread within a particular experiment determines the separation performance. 5. Conclusions The formalization of the generalized eigendecomposition approach to blind source sep- aration problems using the definition of congruent pencils provides a complete and simple framework to understand the feasibility of the method. Parlett [12] presents the definition of congruent pencils in case of an square mixing matrices A concluding that with invertible mixing matrices the two pencils have identical eigenvalues. We extended that definition of congruent pencils and prove that when the mixing matrix is rectangular, m × n(m>n), then also n eigenvalues are identical. So the eigenvectors of the source and the sensor pencil are related (see (6) and (9)) via the mixing matrix. Furthermore, it turns out that the transpose of the eigenvector matrix (E) of the sensor pencil provides an estimate of the separation matrix (B) if the source pencil possesses a diagonal eigenvector matrix. In addition, in order to achieve an unique solution (E), the eigenvalues have to be dis- tinct. This linear algebra approach constitutes an alternative formulation to Chang et al.’s work [6] and complements Stone’s [8] and Tomé’s [18] work. Note that the work of Stone and also Hundley et al. [9] corresponds to considering a specific filter only, while the algebraic formulation given in this study demonstrates that the GED using congruent pencils can be used with any sort of filter. Further note that this formalism can equally well be applied in case of time-delayed correlation matrices. Hence it provides a rather general approach to solve BSS problems of correlated data using second order techniques only. Another aspect dealt with in this study concerns the implementation of the proposed algebraic approach using an iterative estimation of the separation matrix B.Thisimple- mentation needs to resort to rather simple mathematical operations instead of having to deal with rather complex algebraic manipulations common to most other generalized eigenvalue decomposition algorithms. Still another aspect is the fact that most works in blind source separation of signals do not consider the segment length as an issue. But our experiments convincingly show that the data segment size severely influences the separation performance of the algorithm, hence of the reliability of the solutions obtained. Thought some of the cited works also sug- gest that the spread of the eigenvalues should be important for the reliability of a solution, no other detailed discussion of the dependence of the eigenvalue spread on the number of samples has been given in the literature so far. Our experiments definitely confirm these suggestions and conclude that the number of samples taken needs to be considered care- fully to achieve a reliable separation of the observed sensor signals into their underlying source signals. A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 301 Acknowledgment This work was supported by the “FCT-Fundação para a Ciência e Tecnologia” funding the project POSI/34246/SRI/2000. Appendix A The power method constitutes a systematic and iterative approach to look for vec- tors in Rfractur m which can be the principal normalized eigenvector of an m × m symmetric positive definite matrix. The other eigenvectors/values are computed using a deflation tech- nique, i.e., computing a new matrix to proceed with a new estimated eigenvector/value pair (u i ,δ i ). Starting with the original matrix M 1 = M, each new matrix is computed as M i+1 = M i − δ i u i u T i [11]. If M = TRT T holds, where T is an m × n matrix and m>n, the matrix is rank deficient, i.e., (m−n) eigenvalues are equal to zero. In this case the algo- rithm will estimate n orthogonal eigenvectors in a space of dimension m. These techniques were applied to develop an iterative algorithm for the generalized eigendecomposition of a matrix pencil. Given initial estimates: • To the set of eigenvector u m (0) negationslash= u m−1 (0) negationslash=···negationslash= u 1 (0) of the matrix R x1 ∈Rfractur m×m . • To the set of eigenvectors z m (0) negationslash= z m−1 (0) negationslash=···negationslash= z 1 (0) of the transformed matrix ˆ C. for k = 1,2,... % On-line estimation of R x1 and R x2 after k samplesofdata • Estimate (u i ,δ i )—using power method and deflation technique • Compute the mean square error ε i , i = 1,...,m W −1 = 0, n = 0 for j = 1 to m if (δ j (k) > 0 and ε j (k) < tol) W −1 = W −1 + u j (k)(u j (k)) T / radicalbig δ j (k) n = n + 1 end • Transform the second matrix ˆ C = W −1 R x2 W −1 • Estimate the (z l ,λ l ), l = 1,...,m,ofmatrix ˆ C • λ l are the n eigenvalues of (R x1 ,R x2 ) • e l = W −1 z l are the eigenvectors (not normalized to unit vectors) % n = m if the matrices are not rank deficient end % Estimate the eigenvectors (u i ) and eigenvalues (δ i ) of a matrix M ∈Rfractur m×m estimated in iteration k % Using power method and deflation for i = 1,2,...,m u i (k) = Mu i (k − 1) u i (k) = u i (k)/bardblu i (k)bardbl 302 A.M. Tomé / Digital Signal Processing 16 (2006) 288–302 δ i (k) = u i (k) T Mu i (k) if i negationslash= m M = M − u i (k)δ i (k)u i (k) T end References [1] J.-F. Cardoso, B.H. Laheld, Equivariant adaptive source separation, IEEE Trans. Signal Process. 44 (12) (1996) 3017–3030. [2] L. Tong, R.-W. Liu, V.C. Soon, Y.-F. Huang, Indeterminacy and identifiability of blind identification, IEEE Trans. Circuits Syst. 38 (5) (1991) 499–509. [3] A. Souloumiac, Blind source detection using second order non-stationarity, in: International Conference on Acoustic, Speech, and Signal Processing, Detroit, USA, 1995, pp. 1912–1916. [4] L. Molgedey, H. Schuster, Separation of a mixture of independent signals using time delayed correlations, Phys. Rev. Lett. 72 (23) (1994) 3634–3637. [5] C. Chang, Z. Ding, F.Y. Sze, F.H. Chan, A matrix-pencil approach to blind source separation of non-white noise, in: International Conference on Audio, Speech, and Signal Processing, vol. IV, 1998, pp. 2485–2488. [6] C. Chang, Z. Ding, F.Y. Sze, F.H.Y. Chan, A matrix-pencil approach to blind source separation of colored nonstationary signals, IEEE Trans. Signal Process. 48 (3) (2000) 900–907. [7] A.M. Tomé, Blind source separation using a matrix pencil, in: International Joint Conference on Neural Networks, Como, Italy, 2000. [8] J.V. Stone, Blind source separation using temporal predictibility, Neural Comput. 13 (7) (2001) 1557–1574. [9] D.R. Hundley, M.J. Kirby, M. Anderle, Blind source separation using the maximum signal fraction approach, Signal Process. 82 (2002) 1505–1508. [10] D.R. Hundley, M.J. Kirby, M.G. Anderle, A solution procedure for blind source separation using the maxi- mum noise fraction approach: algorithms and examples, in: Third International Conference on Independent Component Analysis and Signal Separation, San Diego, USA, 2001, pp. 337–342. [11] K.I. Diamantaras, S. Kung, Principal Component Neural Networks, Theory and Applications, Wiley, 1996. [12] B.N. Parlett, The Symmetric Eigenvalue Problem, Classics Appl. Math., SIAM, 1998. [13] C.T. Ma, Z. Ding, S.F. Yau, A two-stage algorithm for mimo blind deconvolution of nonstationary colored signals, IEEE Trans. Signal Process. 48 (4) (2000) 1187–1192. [14] D. Martinez, A. Bray, Nonlinear blind source separation using kernels, IEEE Trans. Neural Networks 14 (1) (2003) 228–235. [15] A. Belouchrani, K. Abed-Meraim, J.-F. Cardoso, E. Moulines, A blind source separation technique using second-order statistics, IEEE Trans. Signal Process. 45 (2) (1997) 434–444. [16] L. Parra, P. Sajda, Blind source separation via generalized eigenvalue decomposition, J. Machine Learn. Res. 4 (2003) 1261–1269. [17] K. Stadlthanner, A.M. Tomé, F.J. Theis, W. Gronwald, K.R. Kalbitzer, E.W. Lang, Blind source separation of water artifacts in NMR spectra using a matrix pencil, in: Fourth International Symposium on Independent Component Analysis and Blind Source Separation, Nara, Japan, 2003, pp. 167–172. [18] A.M. Tomé, An iterative eigendecomposition approach to blind source separation, in: Third International Conference on Independent Component Analysis and Signal Separation, San Diego, USA, 2001, pp. 424– 428. [19] A. Hyvärinen, E. Oja, A fast fixed algorithm for independent component analysis, Neural Comput. 9 (1996) 1483–1492. [20] A. Cichocki, S.-I. Amari, Adaptive Blind Signal and Image Processing, Wiley, 2002. [21] J. Hurri, H. Gävert, J. Särelä, A. Hyvärinen, FastICA for Matlab 4×, available at: projects/. Ana M. Tomé received her PhD in electrical engineering from University of Aveiro in 1990. Currently she is an Associate Professor in the Department of Electronics and Telecommunications of University of Aveiro. Her research interests include digital signal processing, blind source separation problems, and pattern recognition. YDSPR 595 S1051-2004(05)00097-7 10.1016/j.dsp.2005.06.002 Elsevier Inc. The generalized eigendecomposition approach to the blind source separation problem Ana Maria Tomé ⁎ Departamento Electrónica e Telecomunicações/IEETA, Universidade Aveiro, 3810-193 Aveiro, Portugal ⁎ Fax: +351 234 370545. This paper proposes a novel formulation of the generalized eigendecomposition (GED) approach to blind source separation (BSS) problems. The generalized eigendecomposition algorithms consider the estimation of a pair of correlation matrices (a matrix pencil) using observed sensor signals. Each of various algorithms proposed in the literature uses a different approach to form the pencil. This study proposes a linear algebra formulation which exploits the definition of congruent matrix pencils and shows that the solution and its constraints are independent of the way the matrix pencil is computed. Also an iterative eigendecomposition algorithm, that updates separation parameters on a sample-by-sample basis, is developed. It comprises of: (1) performing standard eigendecompositions based on power and deflation techniques; (2) computing a transformation matrix using spectral factorization. Another issue discussed in this work is the influence of the length of the data segment used to estimate the pencil. The algorithm is applied to artificially mixed audio data and it is shown that the separation performance depends on the eigenvalue spread. The latter varies with the number of samples used to estimate the eigenvalues. Keywords Blind source separation Generalized eigendecomposition Correlation matrices Matrix pencil References [1] J.-F. Cardoso B.H. Laheld Equivariant adaptive source separation IEEE Trans. Signal Process. 44 12 1996 3017 3030 [2] L. Tong R.-W. Liu V.C. Soon Y.-F. Huang Indeterminacy and identifiability of blind identification IEEE Trans. Circuits Syst. 38 5 1991 499 509 [3] A. Souloumiac, Blind source detection using second order non-stationarity, in: International Conference on Acoustic, Speech, and Signal Processing, Detroit, USA, 1995, pp. 1912–1916 [4] L. Molgedey H. Schuster Separation of a mixture of independent signals using time delayed correlations Phys. Rev. Lett. 72 23 1994 3634 3637 [5] C. Chang, Z. Ding, F.Y. Sze, F.H. Chan, A matrix-pencil approach to blind source separation of non-white noise, in: International Conference on Audio, Speech, and Signal Processing, vol. IV, 1998, pp. 2485–2488 [6] C. Chang Z. Ding F.Y. Sze F.H.Y. Chan A matrix-pencil approach to blind source separation of colored nonstationary signals IEEE Trans. Signal Process. 48 3 2000 900 907 [7] A.M. Tomé, Blind source separation using a matrix pencil, in: International Joint Conference on Neural Networks, Como, Italy, 2000 [8] J.V. Stone Blind source separation using temporal predictibility Neural Comput. 13 7 2001 1557 1574 [9] D.R. Hundley M.J. Kirby M. Anderle Blind source separation using the maximum signal fraction approach Signal Process. 82 2002 1505 1508 [10] D.R. Hundley, M.J. Kirby, M.G. Anderle, A solution procedure for blind source separation using the maximum noise fraction approach: algorithms and examples, in: Third International Conference on Independent Component Analysis and Signal Separation, San Diego, USA, 2001, pp. 337–342 [11] K.I. Diamantaras S. Kung Principal Component Neural Networks, Theory and Applications 1996 Wiley [12] B.N. Parlett The Symmetric Eigenvalue Problem Classics Appl. Math. 1998 SIAM [13] C.T. Ma Z. Ding S.F. Yau A two-stage algorithm for mimo blind deconvolution of nonstationary colored signals IEEE Trans. Signal Process. 48 4 2000 1187 1192 [14] D. Martinez A. Bray Nonlinear blind source separation using kernels IEEE Trans. Neural Networks 14 1 2003 228 235 [15] A. Belouchrani K. Abed-Meraim J.-F. Cardoso E. Moulines A blind source separation technique using second-order statistics IEEE Trans. Signal Process. 45 2 1997 434 444 [16] L. Parra P. Sajda Blind source separation via generalized eigenvalue decomposition J. Machine Learn. Res. 4 2003 1261 1269 [17] K. Stadlthanner, A.M. Tomé, F.J. Theis, W. Gronwald, K.R. Kalbitzer, E.W. Lang, Blind source separation of water artifacts in NMR spectra using a matrix pencil, in: Fourth International Symposium on Independent Component Analysis and Blind Source Separation, Nara, Japan, 2003, pp. 167–172 [18] A.M. Tomé, An iterative eigendecomposition approach to blind source separation, in: Third International Conference on Independent Component Analysis and Signal Separation, San Diego, USA, 2001, pp. 424–428 [19] A. Hyvärinen E. Oja A fast fixed algorithm for independent component analysis Neural Comput. 9 1996 1483 1492 [20] A. Cichocki S.-I. Amari Adaptive Blind Signal and Image Processing 2002 Wiley [21] J. Hurri H. Gävert J. Särelä A. Hyvärinen FastICA for Matlab 4× available at: Ana M. Tomé received her PhD in electrical engineering from University of Aveiro in 1990. Currently she is an Associate Professor in the Department of Electronics and Telecommunications of University of Aveiro. Her research interests include digital signal processing, blind source separation problems, and pattern recognition. "
    },
    {
        "doc_title": "Separation of water artifacts in 2D NOESY protein spectra using congruent matrix pencils",
        "doc_scopus_id": "29444445090",
        "doc_doi": "10.1016/j.neucom.2005.02.008",
        "doc_eid": "2-s2.0-29444445090",
        "doc_date": "2006-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Cognitive Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2805"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Biomolecules",
            "Generalized eigenvalue decomposition (GEVD)",
            "Matrix pencil approach"
        ],
        "doc_abstract": "Multidimensional proton nuclear magnetic resonance spectra of biomolecules dissolved in aqueous solutions are usually contaminated by an intense water artifact. We discuss the application of a generalized eigenvalue decomposition (GEVD) method using a matrix pencil to solve the blind source separation (BSS) problem of removing the intense solvent peak and related artifacts. The method explores correlation matrices of the signals and their filtered versions in the frequency domain and implements a two-step algebraic procedure to solve the GEVD. Two-dimensional nuclear Overhauser enhancement spectroscopy (2D NOESY) of dissolved proteins is studied. Results are compared to those obtained with the SOBI [Belouchrani et al., IEEE Trans. Signal Process. 45(2) (1997) 434-444] algorithm which jointly diagonalizes several time-delayed correlation matrices and to those of the fastICA [Hyvärinen and Oja, Neural Comput. 9 (1996) 1483-1492] algorithm which exploits higher order statistical dependencies of random variables. © 2005 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271597 291210 291735 291866 31 Neurocomputing NEUROCOMPUTING 2005-05-12 2005-05-12 2010-11-15T09:20:10 S0925-2312(05)00047-0 S0925231205000470 10.1016/j.neucom.2005.02.008 S300 S300.1 FULL-TEXT 2015-05-15T03:05:57.461859-04:00 0 0 20060101 20060131 2006 2005-05-12T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast footnotes misctext primabst ref vitae alllist content subj ssids 0925-2312 09252312 69 69 4 6 4 5 6 Volume 69, Issues 4–6 10 497 522 497 522 200601 January 2006 2006-01-01 2006-01-31 2006 Regular papers article fla Copyright © 2005 Elsevier B.V. All rights reserved. SEPARATIONWATERARTIFACTSIN2DNOESYPROTEINSPECTRAUSINGCONGRUENTMATRIXPENCILS STADLTHANNER K 1 Introduction 2 NMR spectra 3 The generalized eigenvalue decomposition approach 4 Computing the eigendecomposition of symmetric pencils 5 Results and discussion 5.1 Simulated protein spectra 5.2 Experimental protein spectra 5.2.1 Spectra of the synthetic polypeptide P 11 5.2.2 Spectra of the protein RALH814 5.2.3 Spectra of the protein TmCSP 6 Conclusions References BELL 1995 1129 1159 A BELOUCHRANI 1997 434 444 A BINGHAM 2000 1 8 E CARDOSO 1996 161 164 J CHANG 2000 900 907 C CICHOCKI 2002 A ADAPTIVEBLINDSIGNALIMAGEPROCESSING CRAVEN 1995 40 46 C CROSS 1993 220 224 K DELFOSSE 1995 59 83 N DERICH 1993 229 232 M GHARIEB 2003 252 274 R GOLUB 1996 G MATRIXCOMPUTATIONS GORLER 1997 177 188 A HAUSSER 1989 K NMRINMEDICINEBIOLOGY HYVARINEN 2001 A INDEPENDENTCOMPONENTANALYSIS HYVARINEN 1996 1483 1492 A KURODA 1989 604 610 Y LECLERC 1994 64 67 J LEVITT 2002 M SPINDYNAMICS MARION 1989 425 430 D MOLGEDEY 1994 3634 3637 L NUZILLARD 1998 209 211 D PARRA 2003 1261 1269 L PIJNAPPLE 1992 122 134 W STADLTHANNER 2003 103 110 K STONE 2001 1557 1574 J THEIS 2002 1 21 F TONG 1991 499 509 L VANHAMME 1998 197 203 L ZHU 1997 286 289 G ZIEGAUS 2004 79 100 C STADLTHANNERX2006X497 STADLTHANNERX2006X497X522 STADLTHANNERX2006X497XK STADLTHANNERX2006X497X522XK item S0925-2312(05)00047-0 S0925231205000470 10.1016/j.neucom.2005.02.008 271597 2010-12-25T18:19:25.113944-05:00 2006-01-01 2006-01-31 true 589521 MAIN 26 42058 849 656 IMAGE-WEB-PDF 1 si99 140 12 14 si98 259 16 54 si97 746 18 171 si96 142 14 13 si95 142 12 14 si94 142 12 13 si93 486 15 109 si92 249 16 52 si91 211 12 38 si90 281 16 56 si9 194 14 24 si89 845 19 196 si88 224 15 35 si87 270 16 48 si86 140 12 14 si85 814 16 192 si84 810 16 199 si83 185 16 20 si82 199 16 24 si81 185 8 42 si80 140 12 14 si8 205 14 25 si79 140 12 14 si78 140 12 14 si77 382 16 67 si76 387 16 63 si75 489 18 107 si74 470 18 107 si73 140 12 14 si72 382 16 67 si71 382 16 67 si70 150 15 17 si7 124 12 12 si69 363 17 64 si68 155 12 14 si67 158 10 18 si66 155 12 14 si65 359 16 63 si64 161 12 17 si63 133 12 13 si62 140 13 13 si61 387 15 98 si60 356 17 114 si6 129 12 19 si59 330 17 57 si58 606 15 175 si57 2049 42 413 si56 133 12 13 si55 124 12 12 si54 270 19 63 si53 166 14 19 si52 549 15 170 si51 162 13 15 si50 611 16 131 si5 124 12 12 si49 174 18 16 si48 300 16 48 si47 1938 33 456 si46 156 14 23 si45 319 16 44 si44 124 12 12 si43 124 12 12 si42 323 16 44 si41 205 14 25 si40 194 14 24 si4 333 16 57 si39 382 16 67 si38 249 13 40 si37 341 13 81 si36 230 15 31 si35 170 8 37 si34 186 10 38 si33 140 12 14 si32 274 16 52 si31 249 16 52 si30 270 16 48 si3 391 16 83 si29 137 14 11 si28 131 13 11 si27 412 17 104 si26 215 12 38 si25 185 8 42 si24 140 12 14 si23 421 19 109 si223 558 16 145 si222 222 15 30 si221 194 14 24 si220 179 13 38 si219 205 14 25 si218 431 16 80 si217 205 14 25 si216 558 16 145 si215 194 14 24 si214 179 13 38 si213 205 14 25 si212 417 13 74 si211 590 16 145 si22 430 19 119 si210 344 13 66 si21 349 15 77 si209 608 16 165 si208 179 13 38 si207 275 16 38 si206 171 12 26 si205 462 13 105 si204 645 15 168 si203 455 13 97 si202 469 18 98 si201 179 13 38 si200 227 14 33 si20 278 13 49 si2 414 16 77 si199 240 16 30 si198 135 13 11 si197 1835 50 255 si196 342 16 53 si195 225 16 32 si194 527 19 123 si193 316 16 57 si192 651 16 150 si191 1161 41 252 si190 193 16 30 si19 198 13 40 si189 1049 19 269 si188 181 16 36 si187 658 19 137 si186 210 16 32 si185 207 16 33 si184 123 12 9 si183 881 20 207 si182 344 13 66 si181 333 16 62 si180 145 12 15 si18 293 13 49 si179 137 13 14 si178 1342 34 278 si177 369 16 60 si176 205 14 25 si175 155 12 14 si174 193 15 22 si173 166 10 19 si172 354 13 68 si171 931 34 191 si170 194 14 24 si17 203 13 39 si169 205 14 25 si168 194 14 24 si167 205 14 25 si166 135 13 11 si165 382 16 67 si164 274 16 52 si163 641 17 239 si162 294 16 53 si161 152 13 11 si160 706 16 155 si16 291 13 49 si159 663 16 155 si158 166 10 19 si157 133 12 13 si156 354 13 68 si155 263 13 59 si154 150 15 17 si153 155 12 14 si152 222 11 46 si151 133 12 13 si150 340 17 107 si15 199 13 40 si149 330 17 57 si148 358 16 76 si147 146 13 13 si146 540 17 130 si145 216 16 31 si144 205 14 25 si143 513 17 120 si142 614 18 145 si141 310 13 61 si140 1121 19 298 si14 268 13 49 si139 194 14 24 si138 346 13 73 si137 476 15 105 si136 253 16 52 si135 174 18 16 si134 115 12 8 si133 470 19 112 si132 213 17 24 si131 382 16 67 si130 264 14 45 si13 179 13 38 si129 249 16 52 si128 159 15 21 si127 166 12 15 si126 142 12 13 si125 387 16 63 si124 142 12 14 si123 249 16 52 si122 275 16 52 si121 166 12 15 si120 763 19 197 si12 400 18 66 si119 670 15 150 si118 691 15 153 si117 497 16 121 si116 394 16 80 si115 142 12 13 si114 472 16 117 si113 369 16 76 si112 140 12 14 si111 142 12 13 si110 140 12 14 si11 227 14 33 si109 167 14 16 si108 140 12 14 si107 259 16 48 si106 142 12 13 si105 128 9 10 si104 363 18 74 si103 167 14 16 si102 513 15 112 si101 351 18 72 si100 173 15 27 si10 124 12 12 si1 133 12 13 gr2 63762 597 538 gr2 2381 93 84 gr4 47858 591 540 gr4 1879 93 85 gr9 47030 594 535 gr9 1805 93 84 fx1 15924 151 113 fx1 4581 93 70 fx2 17082 151 113 fx2 4378 93 70 fx6 15860 151 113 fx6 4432 93 70 gr6 50861 585 543 gr6 1998 93 86 gr7 40111 592 541 gr7 1777 93 85 fx3 15726 151 113 fx3 3916 93 70 fx4 16066 151 113 fx4 4116 93 70 fx5 17211 151 113 fx5 4831 93 70 gr8 41195 587 540 gr8 1787 94 86 gr1 19881 514 461 gr1 1254 94 84 gr10 40665 596 535 gr10 1733 94 84 gr3 61594 593 532 gr3 2267 94 84 gr5 42243 592 539 gr5 1777 93 85 NEUCOM 10047 S0925-2312(05)00047-0 10.1016/j.neucom.2005.02.008 Elsevier B.V. Fig. 1 Algorithm description of the matrix pencil approach. Fig. 2 Top: 1D slice of a simulated 2D NOESY spectrum of TmCSP overlaid with an experimental water spectrum corresponding to the shortest evolution period t 2 . The chemical shift ranges from 10.771 ppm (left) to - 1.241 ppm (right). Only the real part of the complex quantity x ( ω 2 , t 1 ) is shown. Bottom: Reconstructed simulated TmCSP spectrum with the water resonance removed with the matrix pencil algorithm. Fig. 3 Top: Reconstructed simulated TmCSP spectrum with the water resonance removed with the fastICA algorithm. Bottom: Reconstructed simulated TmCSP spectrum with the water resonance removed with the SOBI algorithm. Fig. 4 Top: 1D slice of a 2D NOESY spectrum of the polypeptide P11 in aqueous solution corresponding to the shortest evolution period t 1 . The chemical shift ranges from - 1 to 10ppm roughly. Bottom: Reconstructed P11 spectrum with the water artifact removed with the matrix pencil algorithm. Fig. 5 Top: Reconstructed P11 spectrum with the water artifact removed with the fastICA algorithm. Bottom: Reconstructed P11 spectrum with the water artifact removed with the SOBI algorithm. Fig. 6 Top: 1D slice of a 2D NOESY spectrum of the protein RALH814 in aqueous solution corresponding to the shortest evolution period t 1 . Bottom: Reconstructed protein spectrum with the water artifact removed with the matrix pencil algorithm. Fig. 7 Top: Reconstructed spectrum of the protein RALH814 with the water artifact removed with the fastICA algorithm. Bottom: Reconstructed spectrum of the protein RALH814 with the water artifact removed with the SOBI algorithm. Fig. 8 Top: Reconstructed spectrum of the protein RALH814 obtained with the matrix pencil algorithm. The second correlation matrix R x 2 corresponds to a low-pass filtered version of R x 1 . Bottom: Reconstructed spectrum obtained with the matrix pencil algorithm using a bandpass filter centered at the water resonance instead. Fig. 9 Top: 1D slice of a 2D NOESY spectrum of the protein TmCSP in aqueous solution corresponding to the shortest evolution period t 1 . Bottom: Reconstructed TmCSP protein spectrum obtained with the matrix pencil algorithm. Fig. 10 Top: Reconstructed TmCSP protein spectrum obtained with the fastICA algorithm. Bottom: Reconstructed TmCSP protein spectrum obtained with the SOBI algorithm. Table 1 Dependence of the percentage p H 2 O of water signal left and the reconstruction measure G on the width of the Gaussian bandpass filter G (dB) p H 2 O ( % ) σ = 1 - 22.54 3.42 σ = 2 - 24.96 6.15 σ = 3 - 26.30 11.01 σ = 4 - 26.25 15.28 Separation of water artifacts in 2D NOESY protein spectra using congruent matrix pencils K. Stadlthanner a A.M. Tomé b F.J. Theis a E.W. Lang a * 1 2 W. Gronwald c H.R. Kalbitzer c a Institute of Biophysics, Neuro- and Bioinformatics Group, University of Regensburg, D-93040 Regensburg, Germany b Departamento de Electrónica e Telecomunicações/IEETA, Universidade de Aveiro, P-3810 Aveiro, Portugal c Institute of Biophysics, University of Regensburg, D-93040 Regensburg, Germany * Corresponding author. Tel.: +499419432599; fax: +499419432479. 1 Financial support by the German Science Foundation (DFG) is gratefully acknowledged. 2 Financial support by the German Ministry of Education and Science (BMBF), project ModKog, is gratefully acknowledged. Communicated by T. Heskes Multidimensional proton nuclear magnetic resonance spectra of biomolecules dissolved in aqueous solutions are usually contaminated by an intense water artifact. We discuss the application of a generalized eigenvalue decomposition (GEVD) method using a matrix pencil to solve the blind source separation (BSS) problem of removing the intense solvent peak and related artifacts. The method explores correlation matrices of the signals and their filtered versions in the frequency domain and implements a two-step algebraic procedure to solve the GEVD. Two-dimensional nuclear Overhauser enhancement spectroscopy (2D NOESY) of dissolved proteins is studied. Results are compared to those obtained with the SOBI [Belouchrani et al., IEEE Trans. Signal Process. 45(2) (1997) 434–444] algorithm which jointly diagonalizes several time-delayed correlation matrices and to those of the fastICA [Hyvärinen and Oja, Neural Comput. 9 (1996) 1483–1492] algorithm which exploits higher order statistical dependencies of random variables. Keywords Blind source separation Independent component analysis Generalized eigenvalue decomposition Matrix pencil 2D NOESY Proton NMR spectra 1 Introduction Blind source separation (BSS) addresses the problem of finding which signals contribute to any given sensor signal recorded. It is of interest if little or nothing is known about the source signals and the mixing process, hence the term blind. This is a widespread problem in signal processing, hence BSS techniques have many application areas in speech and image processing, biomedical signal processing and communications among others. BSS has a very close relationship to a recently developed new statistical data processing technique called independent component analysis (ICA). For recent authoritative reviews see [8,20,33]. In general, the problem is very ill-posed and needs to be regularized to become solvable. Two venues have been considered in the past. Either one assumes statistically independent source signals and exploits higher order correlations in the data or one exploits the correlation structure in the signals relying on second-order statistics only. In any case a linear mixing model x ⇒ = A s ⇒ + ε ⇒ is considered generally with x ⇒ = ( x 1 , … , x m ) T the sensor signal, s ⇒ = ( s 1 , … , s n ) T the unknown, underlying source signal, A the unknown m × n mixing matrix of full rank ( m ⩾ n ) and ε ⇒ = ( ε 1 , … , ε m ) some additive noise term, which usually is assumed to be Gaussian white noise. Further s ⇒ and ε ⇒ are assumed uncorrelated. Most solutions consider a two-step procedure. During a whitening step the sensor signals are linearly transformed (via a principal component analysis (PCA) for example) such that the covariance matrix becomes the identity matrix. The problem is thereby reduced to finding an orthogonal (or unitary in case of complex-valued signals) separating matrix using higher order statistics or second-order de-correlation algorithms. During this whitening step, the dimension of the sensor signal vector may be reduced if needed. This is especially convenient in over-complete settings with more sensors than sources, i.e. ( m > n ) . This way the number of sources can be estimated as usually the least ( m - n ) eigenvalues are very similar in magnitude and correspond to noise contributions. The latter eigenvalues can then be used favorably to estimate the noise variance [8,44]. Higher order de-correlation techniques have been intensely studied recently. Many algorithms have been proposed among which are the popular Infomax-algorithm [1] or its natural gradient version [47], the JADE-algorithm [4] or its neural variant nJADE [49,50] and the very efficient fastICA-algorithm [21]. Also geometric approaches like the efficient fastGeo-algorithm [37] have been developed very recently. Furthermore thinICA [11], an algorithm combining blind source extraction methods [12] with joint approximative diagonalization procedures has been proposed also. ThinICA generalizes deflation techniques to the robust and simultaneous extraction of a subset of independent components combining the information of several cumulant slices of the observation process. Second-order techniques instead exploit the temporal structure of the source signals. The blind identification of the mixing model can be converted to standard (EVD) or generalized (GEVD) eigenvalue decomposition and simultaneous or joint diagonalization problems. In algorithms-like AMUSE [44] and EFOBI [43] a simultaneous diagonalization is performed of a pair of (properly symmetrized) matrices derived from instantaneous and time-delayed covariances or fourth-order cumulants. Later, further GEVD solutions have been presented where the matrices have been computed in different ways: 1. Molgedey and Schuster [27] and Chang et al. [5] compute time-delayed correlation matrices much like in the algorithm AMUSE. 2. Souloumiac [34] considers two segments of time-dependent signals with distinct energies. 3. Lo et al. [25] considers different embedding spaces of chaotic signals. 4. Tomé [38], using the notion of matrix pencils, 3 and Stone [36] consider filtered versions of the sensor signals to form one of the correlation matrices of the pencil. Recently, Tomé [39] also presented an algebraic formulation of the matrix pencil method using block matrix operations in case of over-determined BSS problems with a rectangular ( m × n ) mixing matrix A with more rows than columns, i.e. m > n . Also iterative as well as on-line methods to compute the eigendecomposition of a symmetric positive definite pencil [6] have been presented [40,41]. 3 The latter is defined as the set of all n × n square matrices A , B obeying A - λ B = 0 with λ ∈ C [17]. Algorithms like SOBI [2] instead perform a joint approximative diagonalization of a set of time-delayed covariance matrices of whitened data to extract their average eigenstructure. Extensions to non-stationary sources have been considered as well [7]. Recently, Tomé and Lang [42] also considered the GEVD of a set of matrix pencils and discussed conditions to choose an appropriate subset of matrices to determine the joint diagonalizer. As indicated above, GEVD techniques can be presented using a more general framework by introducing the notion of a congruent matrix pencil [30]. In this work a matrix pencil ( R x 1 , R x 2 ) is computed using only the frequency information of the sensor signals. First a discrete Fourier transform is applied to each sensor signal, then the first correlation matrix R x 1 is computed as usual. The second correlation matrix R x 2 is computed with data filtered in the frequency domain. The method has been addressed in a recent short contribution [35] with some preliminary results illustrated with artificial data and nuclear magnetic resonance (NMR) spectra of simple solutes mainly. We will simply call this strategy to solve the BSS problem the matrix pencil approach and apply it to the separation of the intense water resonance and related artifacts from two-dimensional nuclear Overhauser enhancement spectroscopy nuclear magnetic resonance (2D NOESY NMR) spectra of proteins. The paper is organized as follows: Section 2 provides some motivation for using a BSS approach to the water resonance removal in proton NMR spectra of aqueous protein solutions. In Section 3 the matrix pencil approach using a concise algebraic formulation, based on the definition of congruent pencils, is shortly reviewed. This framework provides an unified point of view of the GEVD approaches as well as of the constraints imposed upon the solutions achieved. In Section 4 an algorithmic implementation using a two-step procedure is presented. In the results section, the matrix pencils are computed with correlation matrices formed with sensor signals and their filtered analogues in the frequency domain. Finally, the algorithm is applied to simulated and experimental protein spectra and results are compared with other source separation algorithms. Section 6 draws some conclusions. 2 NMR spectra Modern multi-dimensional NMR spectroscopy [14,19] is a versatile tool for the determination of the native 3D structure of biomolecules in their natural aqueous environment. Proton NMR detects the magnetization of the 1H nuclei in such probes. It is an indispensable contribution to this structure determination process but is hampered by the presence of the very intense water ( H 2 O ) proton signal. Since the latter is the most intense signal in 2D NOESY spectra, it causes much trouble with baseline distortions and t 1 noise (here t 1 denotes the evolution period where, after an excitation pulse, the spin system evolves in time under its own interactions). It also obscures weak signals lying under its skirts. Because of its intensity it furthermore causes severe dynamic range problems [15] due to a limited resolution of the analog–digital-converter (ADC). Sophisticated experimental protocols thus have been developed to suppress the water signal as far as possible. But all these procedures introduce spectral distortions that can neither be avoided nor removed and prevent the analysis of the spectral region close to the water resonance. Often, therefore, only a simple pre-saturation pulse is applied at the water resonance to reduce the dynamic range problem. It has to be noted, however, that even a long weak pulse on the water resonance can bleach nearby solute proton resonances and can also affect other signals through dipolar cross-relaxation or chemical exchange. As an alternative equivalent spectra of the molecules dissolved in heavy water ( D 2 O ) can be taken to avoid the solvent water resonance. This, however, raises additional problems not the least being that heavy water differs in its physico-chemical properties from light water sufficiently to put into jeopardy a direct comparison of both spectra. Hence in recent years post-processing algorithms have been developed to remove the water resonance (simply called water artifact) from the NMR spectra [9,10,13,22,23,26,45,48]. Note that because of magnetic field inhomogeneities and peak suppression techniques the water signal cannot be modeled analytically. This prevents maximum likelihood estimations of model parameters within a non-linear least-squares approach. Early algorithmic approaches use rather crude approximations causing changes in peak area and phase which render the structure determination process rather unreliable, hence are unacceptable in protein NMR. More sophisticated water suppression algorithms are based on second-order statistics of the signals within an unsupervised approach. They usually use a state-space approach in the time domain by modelling the signal subspace using second-order techniques-like singular value decomposition (SVD) [32] or some variant of singular spectral analysis (SSA) [48]. These approaches are in general computationally very demanding with multi-dimensional NMR data (however, see [46] for a computationally efficient matrix decomposition algorithm). Furthermore they have been applied so far to rather “simple” NMR spectra where the BSS techniques discussed above can yield rather perfect solutions. BSS techniques using independent component analysis methods recently became famous for their ability to blindly decompose complicated signals into their underlying component signals without using any prior knowledge. Hence it is interesting whether BSS techniques can contribute to the removal of the water artifact in proton NMR spectra of aqueous biomolecular solutions. Concerning structure determination, homonuclear 2D NOESY spectra are a must. They rely on the nuclear Overhauser effect, i.e. the change in the intensity of the resonance of one spin species upon disturbing the equilibrium population of an adjacent spin with which it has an appreciable magnetic dipole–dipole interaction. 2D NOESY spectra provide information about cross-relaxation rates which for protons mainly depend on magnetic dipolar interactions. The latter vary with distance as r - 6 hence allow distances to neighboring nuclei to be determined. Loosely speaking one can consider it an atomic ruler which allows the 3D-structure to be determined if enough NOEs are available experimentally. A two-dimensional NMR time domain signal, called free induction decay (FID), is modelled by a superposition of damped complex harmonic functions [14,15,24] (1) x ( t 1 , t 2 ) = ∑ i M i exp [ - j ( Ω 1 i + λ 1 i ) t 1 ] exp [ - j ( Ω 2 i + λ 2 i ) t 2 ] + ε ( t 1 , t 2 ) to which white Gaussian noise with amplitude ε ( t 1 , t 2 ) and variance σ ε 2 is added. M i = | M i | exp ( j Φ i ) represents a complex amplitude factor with Φ i the corresponding phase, Ω ki , k = 1 , 2 , i = 1 , … , N represents the resonance frequency of the ith (out of N) component signal in the kth frequency domain, λ ki represent corresponding decay constants related with dipolar relaxation rates, j = - 1 and t 1 and t 2 designate the evolution and detection periods of the respective pulse protocols. Signal processing is routinely performed by Fourier analysis, resulting in spectra made of sums of Lorentzian shaped resonance lines given by (2) x ( ω 1 , ω 2 ) = ∑ i M i 1 j Δ Ω 1 i + λ 1 i 1 j Δ Ω 2 i + λ 2 i + ε ( ω 1 , ω 2 ) , where Δ Ω ki = ω k - Ω ki , k = 1 , 2 gives the distance from the line center. Statistical independence of two signals requires their scalar product to be zero both in the time domain or in the frequency domain. Therefore non-overlapping resonance lines should be reasonably independent [29]. But because the range of chemical shifts, i.e. the spread of the proton resonances on the frequency scale, is rather limited compared to individual resonance line widths, statistical independence is hard to assure in general. Second-order techniques exploit some weaker conditions for the separation of sources assuming that they have temporal structure with different autocorrelation functions or equivalently different power spectra. Algebraic algorithms, exploiting the time structure of the signals based on second-order statistics, turn out to be very efficient computationally. Hence they seem especially well suited to solve the BSS problem in case of NMR spectra. In the following study FIDs x ( t 1 , j , t 2 ) recorded at fixed evolution times t 1 , j , j = 1 , … , m were sampled over time spans t 2 = t 0 + N Δ t with t 0 a dead-time delay, N the number of samples in the t 2 domain and Δ t the sampling interval. The FIDs have been Fourier transformed with respect to both time domains to obtain corresponding spectra x ( ω 1 , ω 2 ) which then could be corrected for any phase distortions. Data matrices X have then been formed after an inverse Fourier transform in the ω 1 domain with the jth row of X representing the 1-D spectrum x ( t 1 , j , ω 2 ) corresponding to the jth increment of the evolution time t 1 , j . Then each row is multiplied by gaussian function centered near the water peak forming a filtered version of the data. Correlation matrices computed with both versions of the data will form the sensor pencil ( R x 1 , R x 2 ) . 3 The generalized eigenvalue decomposition approach For convenience we shortly review a concise algebraic formulation of the generalized eigenvalue decomposition approach (GEVD) [8,20] using the notion of congruent matrix pencils [39]. Existing GEVD algorithms differ in the way the matrices comprising the sensor pencil ( R x 1 , R x 2 ) , i.e. the matrix pencil formed with correlation matrices of the sensor signals, are computed. But in whatever way they are computed, these sensor pencils share a common property: they are related to pencils computed with the source signals, called source pencils henceforth, by the mixing matrix A as described by the following relations: R x 1 = AR s 1 A H , (3) R x 2 = AR s 2 A H , where ( R s 1 , R s 2 ) is the source pencil, ( R x 1 , R x 2 ) is the sensor matrix pencil corrected for any noise contribution and A represents the instantaneous mixing matrix. According to Parlett [30] two pencils related as described by Eq. (3) are called congruent pencils if A is an invertible matrix. In overcomplete BSS problems, the matrix A can be an m × n matrix, i.e. the number of sensor signals ( m ) is greater than the number of source signals ( n ) . (We will not deal with the more difficult under-complete case). Nevertheless, as shown next, the inverse (or the pseudo-inverse) of the mixing matrix can still be estimated, using the sensor pencil, if the eigenvector matrix of the source pencil is diagonal and has distinct eigenvalues. In fact congruent pencils possess identical eigenvalues which form the roots of the characteristic polynomials χ x ( λ ) = det ( R x 2 - λ R x 1 ) = 0 , (4) χ s ( λ ) = det ( R s 2 - λ R s 1 ) = 0 . Note, that if A is a rectangular matrix ( m > n ) with A H A an invertible matrix, the pencil ( A H AR s 1 A H A , A H AR s 2 A H A ) forms a congruent pencil with the corresponding source signal pencil and also possesses the same eigenvalues. Hence the sensor signal pencil formed with ( m × m ) matrices shows n ⩽ m eigenvalues equal to the eigenvalues of the source signal pencil. The ( m - n ) eigenvalues left are related with noise signals. The GEVD statement of the sensor pencil now reads (5) R x 2 E = R x 1 E Λ , where E represents a unique eigenvector matrix if the diagonal matrix Λ has distinct eigenvalues λ i . The corresponding GEVD statement concerning the source pencil can be obtained easily by substituting Eq. (3) into Eq. (5) yielding (6) AR s 2 A H E = AR s 1 A H E Λ . The further treatment now depends on the structure of the mixing matrix: • Concerning complete ( m = n ) BSS problems corresponding to square mixing matrices A , multiplying both sides of Eq. (6) with A - 1 and using (7) E s = A H E the corresponding GEVD statement of the source pencil results (8) R s 2 E s = R s 1 E s Λ where E s represents its eigenvector matrix and the normalized eigenvectors corresponding to a particular eigenvalue are related by (9) e ⇒ s = α A H e ⇒ with α a normalizing constant. It can be seen from Eq. (7) that the eigenvector matrix E forms an estimate of the inverse of the ( n × n ) mixing matrix A only if the matrix E s corresponds to the identity matrix or a simple permutation matrix. This is true if both correlation matrices of the source pencil are diagonal. • With non-square mixing matrices corresponding to overcomplete BSS problems, Eq. (8) can be rewritten in block matrix notation if A and E are both divided into two blocks: A into A H , ( n × n ) and A L , ( ( m - n ) × n ) and similarly E into E H , ( n × m ) and E L , ( ( m - n ) × m ) . Then the GEVD statement can be reformulated as follows A H R s 2 Φ = A H R s 1 Φ Λ , (10) A L R s 2 Φ = A L R s 1 Φ Λ , (11) Φ = A H H E H + A L H E L = A H E , Φ is now an ( n × m ) matrix representing the eigenvector matrix of the source pencil having ( m - n ) columns of zeros paired with the corresponding eigenvalues in Λ which do not belong to the eigenvalue decomposition of the source pencil ( R s 1 , R s 2 ) . The eigenvector matrix E represents an estimate of the separation matrix only if the remaining n columns of the matrix Φ form the identity matrix. Using E T as an estimate of the separation matrix the n source signals are obtained as well as ( m - n ) signals with vanishing amplitudes if no noise is involved. In summary, the GEVD approach to BSS problems is feasible if it is possible to form a pair of congruent pencils as described by Eq. (3). The GEVD of the source related pencil must have a diagonal eigenvector matrix E s ∝ I and this condition is independent of the way of computing the correlation matrices of the sensor pencil. The steps 1–4 presented in the Introduction describe different alternatives to compute the matrix pencil. Recently Parra and Sajda [31] reviewed several procedures to solve the GEVD referring to other assumptions concerning the source signals’ statistical properties. It general, the sensor pencil ( R x 1 , R x 2 ) might be corrupted with noise. Then a correction step needs to precede the actual eigendecomposition to subtract out any additive white Gaussian noise. Naturally the correction depends on the method used to estimate the correlation matrices. For instance, if a correlation matrix R ˜ x 1 is computed with noisy data, then the expressions given above can be retained by redefining R x 1 = R ˜ x 1 - σ ε 2 I , where I is the identity matrix and σ ε 2 denotes the noise variance. With overcomplete data sets, the noise variance can be estimated from the ( n - m ) least important eigenvalues of an initial PCA applied to the data as has been mentioned above already. Further information about noise estimation techniques can be found in [6,8,16]. In this investigation the noise term may be neglected as SNR ratios are high with the Proton NMR spectra considered, hence noise is not of foremost importance. 4 Computing the eigendecomposition of symmetric pencils A very convenient approach to compute the eigenvalues and eigenvectors of a matrix pencil is to reduce the GEVD statement (12) R x 2 E = R x 1 E Λ to a standard eigenvalue decomposition (EVD) problem which is of the form (13) CZ = Z Λ . Note that a solution can be obtained only if one of the matrices of the pencil is positive definite and the other has non-degenerate eigenvalues [17]. The strategy that we will follow is first to solve the eigenvalue decomposition of the symmetric positive definite matrix R x 1 giving (14) R x 1 = VDV H = VD 1 / 2 V H VD 1 / 2 V H = WW . Substituting this result into the GEVD statement and defining Z = WE yields the transformed equation (15) W - 1 R x 2 W - 1 Z = Z Λ which is of the standard EVD form of a real symmetric matrix C = W - 1 R x 2 W - 1 if the matrix R x 2 is also symmetric positive definite and the transformation matrix W - 1 is obtained as (16) W - 1 = VD - 1 / 2 V H . While the eigenvalues of the matrix pencil are available from the solution of the EVD of the matrix C the corresponding eigenvectors are obtained via E = W - 1 Z . The main steps of the implemented matrix pencil algorithm will be presented in the results section (Fig. 1 ). The two-step algebraic approach to solve the GEVD has much in common with other methods proposed to solve the BSS problem. The first EVD corresponds to the whitening step of most other methods. Then these whitened data are transformed and a second matrix is computed relying either on second-order statistics only (like AMUSE and SOBI) or on higher order statistics (like EFOBI). Finally in a second step another EVD is performed. 5 Results and discussion In the following we discuss the application of the matrix pencil method to several experimental protein proton 2D NOESY NMR spectra. Pre-saturation of the water resonance has been applied in all cases. FIDs x ( t 1 , j , t 2 ) recorded at fixed evolution times t 1 , j j = 1 , … , m and N equidistant samples in the t 2 -domain were obtained and the discrete Fourier transform of each FID was computed. The final m × N matrix X then contained as many rows as there were different evolution times t 1 , j according to the experimental protocol with each row representing a single 1D spectrum. Typically m = 512 evolution periods have been considered and N = 2048 data points were sampled of each spectrum in the t 2 or ω 2 domain, respectively. Due to phase cycling only every fourth spectrum has been considered, yielding data matrices of size ( m × N = 128 × 2048 ) . The observed sensor signals are considered to emerge from a linear superposition of unknown but uncorrelated source signals according to a noiseless model defined in the frequency domain (17) X ( t 1 , ω 2 ) = AS ( t 1 , ω 2 ) , where S is a ( n × N ) matrix of uncorrelated source signals and A = { a ij } , i = 1 , … , m , j = 1 , … , n the unknown ( m × n ) mixing matrix. A matrix pencil ( R x 1 , R x 2 ) comprises two correlation matrices of zero mean data x ⇒ where the second correlation matrix R x 2 is formed with filtered versions of the signals used to compute R x 1 . We will henceforth loosely speak of R x 2 forming a filtered version of R x 1 . The first matrix of the pencil is computed as follows: (18) R x 1 = 1 N X ( t 1 , ω 2 ) X H ( t 1 , ω 2 ) with N = 2048 representing the number of samples in the ω 2 domain and X H the conjugate transpose of the matrix X . The second correlation matrix R x 2 of the pencil has been computed after filtering each single spectrum (each row of X ( t 1 , ω 2 ) ). This correlation matrix can be described as follows: (19) R x 2 = 1 N ( X ( t 1 , ω 2 ) ♢ H ) ( H H ♢ X H ( t 1 , ω 2 ) ) , where ♢ represents the Hadamard (i.e. element-wise) product and the filter matrix H has in each row a bandpass filter of Gaussian shape centered near the water resonance with a variance in the range of 1 ⩽ σ 2 ⩽ 4 [35]. Note that neither the exact shape of the filter function nor its position in the spectrum matters much. It is mainly important that the filtered spectrum has a spectral shape different from the original one (see the low-pass filter example in case of the protein RALH814 discussed below). Experiments with different filters yielded rather consistent results which did not differ much. Note that both matrices of the pencil are of dimension 128 × 128 . Substituting the mixing model into Eqs. (18) and (19), it can be easily verified that a pair of matrices results which is related according to Eq. (3). The box of Fig. 1 resumes the main steps of the matrix pencil approach. Besides the matrix pencil algorithm, the data have been analyzed also with the SOBI and the fastICA algorithm. The SOBI algorithm also considers the time structure of the data and only needs to assume uncorrelated source signals like the matrix pencil algorithm. The idea is to consider delayed versions of the original signals and to form a set of time autocorrelation matrices with these delayed signals. A joint approximative diagonalization of the set of matrices is then performed by minimizing the sum of squares of the off-diagonal elements of the matrices of the set. The joint diagonalizer thus obtained contains the set of eigenvectors spanning the signal space hence represents its eigenstructure in an average way. It is often claimed that joint diagonalization methods are more robust than a simultaneous diagonalization of a matrix pencil. As parameters one has the number of matrices to be diagonalized and the set of time delays to construct the set of matrices. The fastICA algorithm does not consider the time structure in the data. Rather it treats them as random variables and maximizes the non-Gaussianity of the corresponding marginal distributions. It approximates any given data distribution by a family of exponential distributions (20) p γ ( y ) = k 1 exp ( k 2 | y | γ ) , γ > 0 , where the exponent γ differentiates between sub-( γ > 2 ) and super-( γ < 2 ) Gaussian densities. It is convenient to choose differentiable approximations to G ( y ) = - log ( p γ ( y i ) ) which enters an expression for the negentropy as the measure of non-Gaussianity. In case of γ = 1 one may choose (21) G 1 ( y ) = a 1 - 1 log cosh ( a 1 y ) , 1 ⩽ a 1 ⩽ 2 , whereas in case of γ < 1 a possible choice is (22) G 2 ( y ) = - a 2 - 1 exp - a 2 y 2 2 , a 2 ≈ 1 . Note that sub-Gaussian densities are hardly met with natural signals, hence these approximations are sufficient for many practical applications. The algorithm can be run in either a symmetric version or as a deflation approach. It is to be noted further that the fastICA algorithm has been used with real data only, though a version of fastICA for complex-valued signals is available in the literature [3], hence either the real or the imaginary part of the complex NMR data has been used. This is because the complex fastICA algorithm needs to assume uncorrelated real and imaginary parts of the complex signals. The latter assumption is not met with 2D NOESY NMR data as has been corroborated by test simulations using the complex version of the fastICA algorithm which gave worse results than obtained with using only the real part of the data within normal fastICA. Further the fastICA algorithm has to assume statistically independent source signals which represents a much stronger assumption than uncorrelated source signals as is necessary with the matrix pencil algorithm. As parameters one has the chosen non-linearity and the method of estimating the independent components. 5.1 Simulated protein spectra As a toy example we consider simulated [18] noise- and artifact-free 2D NOESY spectra of the cold-shock protein (TmCSP) of the bacteria Thermotoga maritima comprising 66 amino acids. These theoretical spectra contain 134 protein resonances and have been overlaid with experimental 2D NOESY spectra of pure water taken with pre-saturation of the water resonance to simulate conditions corresponding to experimental protein proton 2D NOESY spectra to be analyzed later on. All together 128 spectra x ( t 1 , i , ω 2 , ) , 1 ⩽ i ⩽ 128 with different amplitudes corresponding to 128 different evolution periods have been created. To apply the SOBI algorithm, an inverse Fourier transformation of these spectra had to be performed to get 128 time domain signals x ( t 2 , t 1 , i ) . With these theoretical spectra it is possible to quantify the artifact removal as well as any distortions of the protein spectra. The ratio p H 2 O = | F | / | F BSS | measures the reduction in peak area of the water resonance before ( | F | ) and after ( | F BSS | ) separating out the water artifact with any of the BSS algorithms considered in this study. As a quantitative measure of any spectral distortions the following quantity has been considered: (23) G = 10 log ∑ k = 1 134 ( x ( k ) - x BSS ( k ) ) 2 ∑ k = 1 134 ( x ( k ) ) 2 with the components of the vector x ⇒ corresponding to the theoretical peak amplitudes and the components of x ⇒ BSS giving the corresponding amplitudes of the reconstructed spectrum. Hence the smaller G is the better both spectra coincide. A 1D TmCSP spectrum back-calculated with the algorithm RELAX [18] and overlaid with the experimental water spectrum is shown in Fig. 2 illustrating the realistically scaled rather intense water resonance at 5.2ppm. Note that frequencies are given as relative frequencies on a ppm scale as is usually done in NMR. The matrix pencil formed with these data has been treated in the manner given above to estimate the independent components (ICs) of the artificial TmCSP spectra and the corresponding de-mixing matrix. The second matrix of the pencil has been formed with a bandpass filtered version of the correlation matrix. The filter has been centered on the water resonance at 5.2ppm. Table 1 shows the results obtained for p H 2 O and G. Though narrow bandpass filter yield the best results concerning the removal of the water artifact they also distort the remaining protein spectrum slightly. Roughly 38–40 independent components had to be assigned to the water artifact. Fig. 2 presents the reconstructed protein spectrum as obtained with the matrix pencil using a bandpass filter with σ = 1 . The slight baseline distortions introduced thereby appear around 1ppm. Fig. 3 shows corresponding results obtained with the fastICA algorithm corresponding to p H 2 O = 7.55 % and G = - 28.6 dB . These results compare favorably to the results obtained with the matrix pencil algorithm. But the latter is roughly five times faster and, as will be seen later, the fastICA algorithm usually yields less convincing results with real protein spectra. Further 39 ICs have been assigned to the water artifact. Note that because of the experimental pulse program protocol water resonances in each single 1D spectrum look differently each time due to changing phase relations. Hence there is not a single IC that can be assigned to the water resonance. Rather ICs have been assigned to the water artifact solely by their spectral density, i.e. only ICs with spectral contributions around 5.2ppm, the position of the water resonance, have been considered as being related with the water artifact. With the SOBI algorithm five autocorrelation matrices with delays τ = 0 , 0.8 , 1.6 , … , 3.2 ms have been jointly diagonalized yielding G = - 16.23 dB . This rather poor result could not be improved using other delays. As the CPU time amounted to 5h already more than five autocorrelation matrices have not been considered though. This is to be compared with less than 1min for the matrix pencil algorithm and only a few minutes for the fastICA algorithm. 5.2 Experimental protein spectra These cases are more difficult as the water resonance appears among the protein resonances, hence overlaps considerably with part of the protein resonances and even hides some protein resonances completely. 5.2.1 Spectra of the synthetic polypeptide P 11 Next the algorithm has been applied to a 2D NOESY spectrum of an aqueous solution of a synthetic polypeptide P11 which is identical to the H11 helix of the human glutathione reductase and consists of 24 amino acids only [28]. The data have been analyzed in the frequency domain only. The second correlation matrix of the pencil has been created with filtering the complex valued spectra with a Gaussian bandpass filter h ( ω 2 ) centered at the water resonance at 4.8ppm. The half-width of the filter turned out not to be critical and has been chosen to σ = 1 . All 128 spectra x ( t 1 i , ω 2 ) , i = 1 , … , 128 have been considered, hence the de-mixing matrix had dimension 128 × 128 . Of the 128 estimated independent components again roughly 25 components had to be assigned to the water resonance. Setting these independent components deliberately to zero during the reconstruction process an almost perfect separation of the water artifact resulted as is demonstrated in Fig. 4 . Also all baseline artifacts could be removed as well which means that these distortions also could be separated into different independent components. Both the fastICA and the SOBI algorithm have been applied to the P11 spectra as well (see Fig. 5 ). For the fastICA algorithm the parameters mentioned above have been used. The SOBI algorithm was run with five autocorrelation matrices with delays corresponding to τ = 0 , 180 , … , 720 μ s . In both cases the separation quality was much less with rather severe remnants of the water artifact being left in the reconstructed spectra. This result was obtained irrespective of the adjustable parameters of these algorithms which have been scanned through a rather large range of possible values. 5.2.2 Spectra of the protein RALH814 As another data set 2D NOESY spectra of the protein RALH814 have been analyzed, too. RALH814 represents the C-terminal RAS-binding domain of the protein RALGEF and comprises 87 amino acid residues. The 128 × 2048 dimensional data matrix has been analyzed with the matrix pencil algorithm as described above. Again the second correlation matrix R x 2 of the matrix pencil corresponded to a bandpass ( σ = 1 ) filtered version of correlation matrix R x 1 . Fig. 6 shows an original protein spectrum (top trace) with the prominent water resonance and its reconstructed versions with the water resonance separated out with the matrix pencil. Fig. 7 again presents corresponding results obtained with the fastICA (parameters as given above) and the SOBI (five autocorrelation matrices, delays τ = 0 , 100 , … , 400 μ s ) algorithm for comparison. Though hardly visible on the figures presented, the fastICA algorithm seems to introduce some spectral distortions (small peaks around 0ppm and between 9 and 10ppm disappear) that have not been observed in the analysis with the matrix pencil method. This is of course an important issue concerning an automatized water resonance separation procedure, as any spectral distortions might result in false structure determinations using these 2D NOESY data. It is to be noted that an equally good separation of the water resonance could be obtained if the correlation matrix R x 2 has been calculated by estimating the corresponding expectations with the low frequency samples, those with chemical shifts below the water resonance, of the spectrum only as is shown in Fig. 8 . 5.2.3 Spectra of the protein TmCSP Next we present experimental 2D NOESY TmCSP spectra which contain many protein resonances. Note that the water resonance overlaps considerably with part of the protein spectrum with some protein resonances very close to or even hidden underneath the solvent resonance. Fig. 9 shows an original TmCSP protein spectrum with the prominent water resonance and its reconstructed version with the water resonance separated out by applying the matrix pencil algorithm in the frequency domain. Both correlation matrices had dimension ( 128 × 128 ) and all 2048 data points have been used to estimate the expectations within the correlation matrices. Again the second correlation matrix R x 2 of the matrix pencil corresponded to a bandpass ( σ = 1 ) filtered version of correlation matrix R x 1 . Some remnants of the intense water resonance are still visible in the reconstructed spectrum indicating that a complete separation into independent components has not been achieved yet. This is due to the limited number of independent components that could be estimated with the data available. These remnants of the water artifact, however, still occluded the protein peaks in that frequency range. The data have been analyzed with the fastICA and SOBI algorithms as well yielding less convincing results as is shown in Fig. 10 . Again the fastICA algorithm introduced some spectral distortions like inverted multiplets (at 10 , 5 and 6ppm), hardly visible on the figures presented, that have been observed in the analysis with the GEVD method using a matrix pencil as well but to a far lesser extent. Also large remnants of the water artifact remain with still strong baseline distortions around 4ppm. The SOBI algorithm used five autocorrelation matrices with delays τ = 0 , 100 , … , 400 μ s and yields rather respectable results also. 6 Conclusions Proton 2D NOESY spectra represent an indispensable ingredient to any determination of the three-dimensional conformation of native proteins, which forms the basis for understanding their function in living cells. Water is the most abundant molecule in biological systems, hence proton NMR protein spectra are generally contaminated by a large water resonance causing severe dynamic range problems. We have shown that ICA methods can be useful to separate out these water resonances and concomitant baseline distortions and obtain largely undistorted pure protein spectra. GEVDs using a matrix pencil represent an exact and easily applied second-order technique to effect such artifact removal from the spectra. We have tested this method with artificial protein spectra where no solute resonances appear close to the water resonance. Application of the method to protein spectra with resonances hidden in part by the water resonance showed a good separation quality with only little remaining spectral distortions in the frequency range of the removed water resonance. The reconstructed spectra also show very convincingly that any baseline distortions stemming from the intense water resonance can be automatically cured as well. This is a very important feature for any reliable determination of peak volumes as is necessary during processing the data to extract the structure of the protein. The small spectral distortions still remaining are due to a limited number of IC components estimated and due to the fact that the estimated ICs assigned to the protein still contain small contaminations of the water resonance to be removed because of an imperfect separation of the spectral energy into truly independent components. It is important to note that no noticeable spectral distortions have been introduced farther away from the water resonance contrary to the fastICA algorithm which seemed to introduce distortions also in other parts of the spectrum. This observation has to be analyzed in more detail however before any general conclusions can be drawn. In summary, the GEVD approach using congruent matrix pencils is an algebraically exact, very fast and easy to implement algorithm that outperforms other BSS approaches like fastICA and SOBI in all applications shown. Further investigations will have to improve the separation quality even further and will have to answer the question if solute resonances hidden underneath the water resonance can be made visible with these or related methods. References [1] A.J. Bell T.J. Sejnowski An information-maximization approach to blind source separation and blind deconvolution Neural Comput. 7 1995 1129 1159 [2] A. Belouchrani K. Abed-Meraim J.-F. Cardoso E. Moulines A blind source separation technique using second-order statistics IEEE Trans. Signal Process. 45 2 1997 434 444 [3] E. Bingham A. Hyvärinen A fast fixed-point algorithm for independent component analysis of complex valued signals Int. J. Neural Syst. 10 1 2000 1 8 [4] J.-F. Cardoso A. Souloumiac Jacobi angles for simultaneous diagonalization SIAM J. Matrix Anal. Appl. 17 1 1996 161 164 [5] C. Chang Z. Ding F.Y. Sze F.H.Y. Chan A matrix-pencil approach to blind source separation of colored nonstationary signals IEEE Trans. Signal Process. 48 3 2000 900 907 [6] S. Choi, A symmetric-definite pencil approach to source separation, in: Proceedings of the ITC’2002, vol. CD, 2002. [7] S. Choi, A. Cichocki, A. Belouchrani, Second order nonstationary source separation, J. VLSI Signal Process. (2001) 1–13. [8] A. Cichocki S.-I. Amari Adaptive Blind Signal and Image Processing 2002 Wiley New York [9] C.J. Craven J.P. Waltho The action of time-domain convolution filters for solvent suppression J. Magn. Reson. B 106 1995 40 46 [10] K.J. Cross Improved digital filtering technique for solvent suppression J. Magn. Reson. 101 1993 220 224 [11] S. Cruces, A. Chichocki, Combining blind source extarction with joint approximative diagonalization: thin algorithms for ICA, in: S.I. Amari, A. Cichocki, S. Makino, N. Murata (Eds.), Proceedings of the Fourth International Symposium on Independent Component Analysis and Blind Signal Separation, vol. 1, 2003, pp. 463–468, ISBN 4-9901531-1-1. [12] N. Delfosse P. Loubaton Adaptive blind separation of independent sources a deflation approach Signal Process. 45 1995 59 83 [13] M. Derich X. Hu Elimination of water signal by postprocessing J. Magn. Reson. 101 1993 229 232 [14] R.R. Ernst, G. Bodenhausen, A. Wokaun, Principles of Nuclear Magnetic Resonance in One and Two Dimensions, 1987. [15] R. Freeman, Spin Choreography, Spektrum, Academic Publishers, Oxford, 1997. [16] R.R. Gharieb A. Chichocki Second-order statistics based blind source separation using a bank of subband filters Digital Signal Process. 13 2003 252 274 [17] G.H. Golub C.F. Van Loan Matrix Computations third ed. 1996 John Hopkins Press Baltimore [18] A. Görler H.R. Kalbitzer RELAX, a flexible program for the back calculation of NOESY spectra based on a complete relaxation matrix formalism J. Magn. Reson. 124 1997 177 188 [19] K.H. Hausser H.R. Kalbitzer NMR in Medicine and Biology 1989 Springer Berlin [20] A. Hyvärinen J. Karhunen E. Oja Independent Component Analysis 2001 Wiley New York [21] A. Hyvärinen E. Oja A fast fixed algorithm for independent component analysis Neural Comput. 9 1996 1483 1492 [22] Y. Kuroda A. Wada T. Yamazaki K. Nagayama Postaquisition data processing method for suppression of the solvent signal J. Magn. Reson. 84 1989 604 610 [23] J.H.J. Leclerc Distortion-free suppression of the residual water peak in proton spectra by postprocessing J. Magn. Reson. B 103 1994 64 67 [24] M.H. Levitt Spin Dynamics 2002 Wiley New York [25] T. Lo, H. Leung, J. Litva, Separation of a mixture of chaotic signals, in: International Conference on Acoustics, Speech and Signal Processing, Atlanta, USA, 1996, pp. 1798–1801. [26] D. Marion M. Ikura A. Bax Improved solvent suppression in one- and two-dimensional NMR spectra by convolution of time-domain data J. Magn. Res. 84 1989 425 430 [27] L. Molgedey H. Schuster Separation of a mixture of independent signals using time delayed correlations Phys. Rev. Lett. 72 23 1994 3634 3637 [28] A. Nordhoff, Ch. Tziatzios, J.A.V. Broek, M. Schott, H.-R. Kalbitzer, K. Becker, D. Schubert, R.H. Schirme, Denaturation and reactivation of dimeric human glutathione reductase, Eur. J. Biochem. (1997) 273–282. [29] D. Nuzillard J.M. Nuzillard Application of blind source separation 1D and 2D nuclear magnetic resonance spectroscopy IEEE Signal Process. Lett. 5 1998 209 211 [30] Parlett, The Symmetric Eigenvalue Problem, SIAM's Classics in Applied Mathematics, SIAM, Philadelphia, PA, 1998. [31] L. Parra P. Sajda Blind source separation via generalized eigenvalue decomposition J. Mach. Learn. Res. 4 2003 1261 1269 [32] W.W.F. Pijnapple A. van den Boogaart R. de Beer D. van Ormondt SVD-based quantification of magnetic resonance signals J. Magn. Reson. 97 1992 122 134 [33] St. Roberts, R. Everson, Independent Component Analysis, Principles and Practice, Cambridge, 2001. [34] A. Souloumiac, Blind source detection using second order non-stationarity, in: Proceedings of the International Conference on Acoustic, Speech, and Signal Processing, Detroit, USA, 1995, pp. 1912–1916. [35] K. Stadlthanner F. Theis E.W. Lang A.M. Tomé W. Gronwald H.R. Kalbitzer A matrix pencil approach to the blind source separation of artifacts in 2D NMR spectra Neural Inform. Process. Lett. Rev. 1 3 2003 103 110 [36] J.V. Stone Blind source separation using temporal predictibility Neural Comput. 13 7 2001 1557 1574 [37] F.J. Theis A. Jung C.G. Puntonet E.W. Lang Linear geometric ICA fundamentals and algorithms Neural Comput. 15 2002 1 21 [38] A.M. Tomé, Blind source separation using a matrix pencil, in: Proceedings of the International Joint Conference on Neural Networks, Como, Italy, 2000. [39] A.M. Tomé, An iterative eigendecomposition approach to blind source separation, in: Proceedings of the Third International Conference on Independent Component Analysis and Signal Separation, San Diego, USA, 2001, pp. 424–428. [40] A.M. Tomé, Separation of a mixture of signals using linear filtering and second order statistics, in: Proceedings of the Tenth European Symposium on Artificial Neural Networks, Brugges, 2002, pp. 307–312. [41] A.M. Tomé, N. Ferreira, On-line source separation of temporally correlated signals, in: Proceedings of the European Signal Processing Conference, EUSIPCO2002, Toulouse, France, 2002. [42] A.M. Tomé, E.W. Lang, Approximate diagonalization approach to blind source separation with a subset of matrices, in: Proceedings of the Seventh International Symposium on Signal Processing and its Applications, vol. 2, Paris, France, 2003, pp. 105–108. [43] L. Tong R.-W. Liu V.C. Soon Y.-F. Huang Indeterminacy and identifiability of blind identification IEEE Trans. Circuits Syst. 38 5 1991 499 509 [44] L. Tong, V.C. Soon, Y.F. Huang, R. Liu, AMUSE: a new blind identification algorithm, in: Proceedings of the ISCAS’2000, 2000, pp. 1784–1787. [45] A. van den Boogaart, D. van Ormondt, W.W.F. Pijnapple, R. de Beer, M. Ala-Korpela, Removal of the water resonance from 1H magnetic resonance spectra, in: J.G. McWhriter (Ed.), Mathematics in Signal Processing, vol. III, Clarendon Press, Oxford, 1994. [46] L. Vanhamme R.D. Fierro S. Van Huffel R. de Beer Fast removal of residual water in proton spectra J. Magn. Reson. 132 1998 197 203 [47] H.H. Yang, S.-I. Amari, A stochastic natural gradient descent algorithm for blind signal separation, in: Neural Networks for Signal Processing, vol. IV, Proceedings of the IEEE Signal Processing Society Workshop, Kyoto, Japan, 1996, pp. 433–442. [48] G. Zhu D. Smith Y. Hua Post-aquisition solvent suppression by singular-value decomposition J. Magn. Reson. 124 1997 286 289 [49] Ch. Ziegaus, E.W. Lang, Neural implementation of the JADE algorithm, Lecture Notes in Computer Science, vol. 1607, Springer, Berlin, 1999, pp. 487–496. [50] Ch. Ziegaus E.W. Lang A neural implementation of the JADE algorithm (nJADE) using higher order neurons Neurocomputing 56 2004 79 100 Kurt Stadlthanner received his Diploma degree in Physics from the University of Regensburg in 2003. He is currently a doctoral student at the Institute of Biophysics, Neuro- and Bioinformatics group, at the University of Regensburg. His scientific interests are in the fields of biological data processing and analysis by means of blind source separation and support vector machines. Fabian J. Theis obtained M.Sc. degrees in Mathematics and Physics at the University of Regensburg in 2000. He also received a Ph.D. degree in Physics from the same university in 2002 and a Ph.D. in Computer Science from the University of Granada in 2003. He worked as visiting researcher at the department of Architecture and Computer Technology (University of Granada, Spain), at the RIKEN Brain Science Institute (Wako, Japan) and at FAMU-FSU (Florida State University, USA). Currently, he is heading the ‘signal processing & information theory’ group at the Institute of Biophysics, University of Regensburg, Germany. His research interests include statistical signal processing, linear and nonlinear independent component analysis, overcomplete blind source separation and biomedical data analysis. Elmar W. Lang received his physics diploma in 1977 and his Ph.D. in physics in 1980 and habilitated in Biophysics in 1988 at the University of Regensburg. He is apl. Professor of Biophysics at the University of Regensburg, where he is heading the Neuro- and Bioinformatics group. Currently he serves as associate editor of Neurocomputing and Neural Information Processing-Letters and Reviews. His current research interests include biomedical signal and image processing, independent component analysis and blind source separation, neural networks for classification and pattern recognition, and stochastic process limits in queuing applications. Ana M. Tomé is Ph.D. in Electrical Engineering from University of Aveiro in 1990. Currently she is Associate Professor of Electrical Engineering in the Department of Electronics and Telecomunications/IEETA of the University of Aveiro where she teaches courses on Digital Signal Processing for Electronics and Computer Engineering Diploms. Her research interests include Digital and Statistical Signal Processing, Independent Component Analysis and Blind Source Separation as well as Classification and Pattern Recognition Applications of Neural Networks. Wolfram Gronwald received his Ph.D. in Chemistry in 1994 at the Technical University of Braunschweig, has been a postdoctoral fellow at the University of Alberta, Canada in 1995–1997 and received his habilitation 2002 in Biophysics from the University of Regensburg. He is currently lecturer at the Institute of Biophysics, University of Regensburg. His current research interests include computer assisted investigations of biological macromolecules and multi-dimensional NMR spectroscopy. Hans Robert Kalbitzer graduated in medicine (1976) and physics (1981) at the University of Heidelberg and is currently professor for biophysics at the University of Regensburg. His research interest includes structural biology, NMR spectroscopy and software development (home page: "
    },
    {
        "doc_title": "An algorithm for automatic assignment of artifact-related independent components in biomedical signal analysis",
        "doc_scopus_id": "33750099755",
        "doc_doi": "10.1109/IJCNN.2005.1556289",
        "doc_eid": "2-s2.0-33750099755",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Artifact-related source signals",
            "Biomedical signal analysis",
            "Cost function",
            "Water resonance"
        ],
        "doc_abstract": "In this work an automatic assignment tool for estimated independent components within an independent component analysis is presented. The tool is applied to the problem of removing the water resonance and related artifacts from multi-dimensional proton NMR spectra. The algorithm uses local PCA to approximate the water artifact and defines a suitable cost function which is optimized using simulated annealing. The blind extraction of artifact-related source signals is effected by a recently developed algorithm called dAMUSE. ©2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of clustering and local singular spectrum analysis to remove ocular artifacts from electroencephalograms",
        "doc_scopus_id": "33746915846",
        "doc_doi": "10.1109/IJCNN.2005.1556298",
        "doc_eid": "2-s2.0-33746915846",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Minimum Description Length (MDL)",
            "Ocular artifacts",
            "Time-delayed coordinates"
        ],
        "doc_abstract": "We present a method based on singular spectrum analysis [4], [5] to remove ocular artifacts (EOG) from an Electroencephalogram (EEG). After embedding the EEG signals in a feature space of time-delayed coordinates, feature vectors are clustered and the principal components (PCs) are computed locally within each cluster. Then we assume that the EOG artifact is associated with the PCs belonging to the largest eigenvalues. We incorporate a Minimum Description Length (MDL) criterion to estimate the number of eigenvectors needed to represent the EOG artifact faithfully. The EOG signal thus extracted is then subtracted from the original EEG signal to obtain the corrected EEG signal we are interested in. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hybridizing sparse component analysis with genetic algorithms for blind source separation",
        "doc_scopus_id": "33745326586",
        "doc_doi": "10.1007/11573067_15",
        "doc_eid": "2-s2.0-33745326586",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "NMF algorithms",
            "Nonnegative Blind Source Separation (BSS) problems",
            "Nonnegative Matrix Factorization (NMF)",
            "Sparse component analysis"
        ],
        "doc_abstract": "Nonnegative Matrix Factorization (NMF) has proven to be a useful tool for the analysis of nonnegative multivariate data. However, it is known not to lead to unique results when applied to nonnegative Blind Source Separation (BSS) problems. In this paper we present first results of an extension to the NMF algorithm which solves the BSS problem when the underlying sources are sufficiently sparse. As the proposed target function has many local minima, we use a genetic algorithm for its minimization. © Springer-Verlag Berlin Heidelberg 2005.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A hybridization of simulated annealing and local PCA for automatic component assignment within ICA",
        "doc_scopus_id": "25144438963",
        "doc_doi": "10.1007/11494669_132",
        "doc_eid": "2-s2.0-25144438963",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Artifacts",
            "Automatic assignment tools",
            "Cost functions",
            "Noise contributions"
        ],
        "doc_abstract": "Independent component analysis (ICA) as well as blind source separation (BSS) often faces the problem of assigning the independent or uncorrelated components estimated with ICA or BSS techniques to underlying source signals, artifacts or noise contributions. In this work an automatic assignment tool is presented which uses a priori knowledge about the form of some of the signals to be extracted. The algorithm is applied to the problem of removing water artifacts from 2D NOESY NMR spectra. The algorithm uses local PCA to approximate the water artifact and defines a suitable cost function which is optimized using simulated annealing. The blind source separation of the water artifact from the remaining protein spectrum is done with the recently developed algorithm dAMUSE. © Springer-Verlag Berlin Heidelberg 2005.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "AutoAssign - An automatic assignment tool for independent components",
        "doc_scopus_id": "25144437861",
        "doc_doi": "10.1007/11492542_10",
        "doc_eid": "2-s2.0-25144437861",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "AutoAssign",
            "Automatic assignment tool",
            "NOESY NMR spectra",
            "Water artifact"
        ],
        "doc_abstract": "In this work an automatic assignment tool for estimated independent components within an independent component analysis is presented. The algorithm is applied to the problem of removing the water artifact from 2D NOESY NMR spectra. The algorithm uses local PCA to approximate the water artifact and defines a suitable cost function which is optimized using simulated annealing. The blind source separation of the water artifact from the remaining protein spectrum is done with the recently developed algorithm dAMUSE. © Springer-Verlag Berlin Heidelberg 2005.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DAMUSE - A new tool for denoising and blind source separation",
        "doc_scopus_id": "19944416634",
        "doc_doi": "10.1016/j.dsp.2005.01.004",
        "doc_eid": "2-s2.0-19944416634",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Biomedical applications",
            "Denoising",
            "Embedding signals",
            "Generalized eigenvalue decomposition"
        ],
        "doc_abstract": "In this work a generalized version of AMUSE, called dAMUSE is proposed. The main modification consists in embedding the observed mixed signals in a high-dimensional feature space of delayed coordinates. With the embedded signals a matrix pencil is formed and its generalized eigendecomposition is computed similar to the algorithm AMUSE. We show that in this case the uncorrelated output signals are filtered versions of the unknown source signals. Further, denoising the data can be achieved conveniently in parallel with the signal separation. Numerical simulations using artificially mixed signals are presented to show the performance of the method. Further results of a heart rate variability (HRV) study are discussed showing that the output signals are related with LF (low frequency) and HF (high frequency) fluctuations. Finally, an application to separate artifacts from 2D NOESY NMR spectra and to denoise the reconstructed artefact-free spectra is presented also. © 2005 Elsevier Inc. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272384 291210 291718 291874 31 Digital Signal Processing DIGITALSIGNALPROCESSING 2005-02-23 2005-02-23 2010-10-07T02:00:10 S1051-2004(05)00005-9 S1051200405000059 10.1016/j.dsp.2005.01.004 S300 S300.1 HEAD-AND-TAIL 2015-05-15T04:43:22.393274-04:00 0 0 20050701 20050731 2005 2005-02-23T00:00:00Z rawtext articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype volfirst volissue affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref vitae alllist content subj ssids 1051-2004 10512004 15 15 4 4 Volume 15, Issue 4 7 400 421 400 421 200507 July 2005 2005-07-01 2005-07-31 2005 STATISTICAL SIGNAL PROCESSING article fla Copyright © 2005 Elsevier Inc. All rights reserved. DAMUSEANEWTOOLFORDENOISINGBLINDSOURCESEPARATION TOME A CICHOCKI 2002 A ADAPTIVEBLINDSIGNALIMAGEPROCESSING BELOUCHRANI 1997 434 444 A GHARIEB 2003 252 274 R MOSKVINA 2003 932 942 V GHIL 2002 3.1 3.41 M TONG 1991 499 509 L PARLETT 1998 B SIAMSCLASSICSINAPPLIEDMATHEMATICS SYMMETRICEIGENVALUEPROBLEM VETTER 2000 578 582 R PINCIROLI 1988 477 480 F COMPUTERSINCARDIOLOGY ARESPIRATIONRELATEDEKGDATABASE MARTINEZ 2004 570 581 J OPPENHEIM 1975 A DIGITALSIGNALPROCESSING MARION 1989 425 430 D KURODA 1989 604 610 Y DERICH 1993 229 232 M CROSS 1993 220 224 K LECLERC 1994 64 67 J CRAVEN 1995 40 46 C ZHU 1997 286 289 G VANDENBOOGAART 1994 A REMOVALWATERRESONANCE1HMAGNETICRESONANCESPECTRA PIJNAPPLE 1992 122 134 W STONE 2001 1557 1574 J STADLTHANNER 2003 103 110 K TOMEX2005X400 TOMEX2005X400X421 TOMEX2005X400XA TOMEX2005X400X421XA item S1051-2004(05)00005-9 S1051200405000059 10.1016/j.dsp.2005.01.004 272384 2010-11-07T23:59:29.276845-05:00 2005-07-01 2005-07-31 true 392278 MAIN 22 39013 849 656 IMAGE-WEB-PDF 1 Digital Signal Processing 15 (2005) 400–421 www.elsevier.com/locate/dsp dAMUSE—A new tool for denoising and blind source separation A.M. Tomé a,∗ ,A.R.Teixeira a ,E.W.Lang b,1 , K. Stadlthanner b,2 , A.P. Rocha c ,R.Almeida c,3 a DETUA/IEETA, Universidade de Aveiro, P-3810 Aveiro, Portugal b Institute of Biophysics, University of Regensburg, D-93040 Regensburg, Germany c DMA/FCUP, Universidade do Porto, P-4000 Porto, Portugal Available online 10 March 2005 Abstract In this work a generalized version of AMUSE, called dAMUSE is proposed. The main modifica- tion consists in embedding the observed mixed signals in a high-dimensional feature space of delayed coordinates. With the embedded signals a matrix pencil is formed and its generalized eigendecom- position is computed similar to the algorithm AMUSE. We show that in this case the uncorrelated output signals are filtered versions of the unknown source signals. Further, denoising the data can be achieved conveniently in parallel with the signal separation. Numerical simulations using artificially mixed signals are presented to show the performance of the method. Further results of a heart rate variability (HRV) study are discussed showing that the output signals are related with LF (low fre- quency) and HF (high frequency) fluctuations. Finally, an application to separate artifacts from 2D NOESY NMR spectra and to denoise the reconstructed artefact-free spectra is presented also.  2005 Elsevier Inc. All rights reserved. Keywords: Embedding signals; Generalized eigenvalue decomposition; Blind source separation; Denoising; Biomedical applications * Corresponding author. E-mail addresses: ana@ieeta.pt (A.M. Tomé), elmar.lang@biologie.uni-regensburg.de (E.W. Lang), aprocha@fc.up.pt (A.P. Rocha). 1 Supported by the BMBF, project: ModKog. 2 Supported by the DFG, Graduate College 638. 3 Supported by the FCT, grant: SFRH/BD/5484/2001. 1051-2004/$ – see front matter  2005 Elsevier Inc. All rights reserved. doi:10.1016/j.dsp.2005.01.004 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 401 1. Introduction Blind source separation (BSS) methods deal with the separation of observed sensor signals into their underlying source signals knowing neither these source signals nor the mixing process. BSS methods based on a generalized eigenvalue decomposition (GEVD) of a matrix pencil rely on second order statistics only taking advantage of certain correla- tion structures present in the data. They are very efficient and easy to implement but they are sensitive to noise [1,2]. There are several proposals to improve efficiency and robust- ness of these algorithms when noise is present [1,3]. Considering noisy signals, there exist, beneath others, several projective denoising tech- niques, the first step of which consists in increasing the dimension of the data set by joining delayed versions of the signals [4–6]. Hence the sensor signals are embedded in a high-dimensional feature space of delayed coordinates. Then denoising is achieved by back-projection of the data into a lower-dimensional subspace corresponding to the dimen- sion of the underlying source signals. The idea behind is that a random noise spreads all space equally while a deterministic signal lives on a sub-manifold only. Projecting onto that sub-manifold thus removes some of the noise components reducing its amplitude thereby. A similar strategy is used in singular spectrum analysis (SSA) [7] where a matrix com- posed of the data and their time-delayed versions is considered. Then, a singular value decomposition (SVD) of the data matrix or a principal component analysis (PCA) of the related correlation matrix is computed. The data are then projected onto the principal di- rections of the eigenvectors of the SVD or PCA analysis. The SSA can be used to extract information from short and noisy time series, hence can provide insight into the underlying system that generates the series [8]. In this work we will join BSS techniques and denoising strategies to present a modified version of AMUSE [9], called dAMUSE [10], which solves the BSS problem, albeit a filtering indeterminacy remains, and in parallel provides an efficient denoising tool, hence alleviates the sensitivity of GEVD methods against noise. AMUSE is a BSS algorithm using second-order statistics and the time structure in the data. It is based on a GEVD, i.e., the simultaneous diagonalization of a matrix pencil formed with a correlation matrix of zero mean sensor data and a time-delayed correlation matrix. The proposed algorithm dAMUSE also comprises this simultaneous diagonal- ization but the matrix pencil is formed with correlation matrices computed in a high- dimensional feature space rather than the input space, i.e., after increasing the data set dimension by joining delayed versions of each sensor signal. In the following section we will show, considering a model of linearly mixed sensor signals, that the estimated uncorrelated signals correspond to filtered versions of the under- lying source signals. We will also present an implementation of this algorithm to compute the eigenvector matrix of the matrix pencil which involves a two step procedure based on the standard eigenvalue decomposition (EVD) approach. The advantage of this procedure lies in an optional dimension reduction between the two steps with a concomitant reduc- tion in the number of estimated underlying source signals. The assertion at this step is that part of the source components correspond to noise components only and can be removed advantageously. Hence dAMUSE elegantly combines blind source separation techniques with denoising techniques. 402 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 Finally, to illustrate the proposed method, some applications are discussed comprising simulations with artificially mixed signals of varying signal to noise ratios (SNR) [10], an artifact removal and concomitant denoising of 2D NOESY proton NMR spectra of aqueous solutions of proteins [11], and an analysis of ECG signals [12]. The outline of the paper will be the following: Section 2 presents a concise algebraic formulation of GEVD methods and the AMUSE algorithm generalized to time-delayed signals or their filtered counterparts in the canonically conjugated Fourier space. Section 3 will discuss implementation aspects of the algorithm, Section 4 will discuss applications to artificial and real world noisy sensor signals and Section 5 will present a short discussion. 2. The algorithm dAMUSE 2.1. Embedding Consider a group of N sensor signals vectorx i , i = 1,...,N, with each sensor signal vectorx i embedded in a high-dimensional feature space Ω of delayed coordinates. Then the ith trajectory matrix [7] X i = bracketleftbig vectorx T i (t l + (K − 1)Delta1t),...,vectorx T i (t l + Delta1t), vectorx T i (t l ) bracketrightbig T (1) is formed with the ith sensor signal and its (K − 1) delayed versions (k = 0,...,K− 1), vectorx T i (t l + kDelta1t) = parenleftbig x i (t 0 + kDelta1t),x i (t 0 + τ + kDelta1t),...,x i parenleftbig t 0 + (L − 1)τ + kDelta1t parenrightbigparenrightbig thus represents one signal or one of its (K − 1) delayed versions. The trajectory matrix then reads (set t 0 = 0 for simplicity) X i =       x i ((K − 1)Delta1t) x i (τ + (K − 1)Delta1t) ... x i ((L − 1)τ) x i ((K − 2)Delta1t) x i (τ + (K − 2)Delta1t) ... x i ((L − 1)τ − Delta1t) . . . . . . ... . . . x i (0)x i (τ) ... x i ((L − 1)τ − (K − 1)Delta1t)       , where t l = t 0 +lτ, l = 0,...,L−1, denotes discrete time, τ −1 represents the sampling rate and Delta1t = nτ, n ∈N, n lessmuch L, denotes a fixed delay which is an integer multiple (usually n = 1) of the sampling interval τ. Thus each row of the trajectory matrix of each signal contains L − n(K − 1) samples of the signal. The total trajectory matrix X of the whole set of N signals will be a concatenation of the component trajectory matrices X i computed for each sensor: X =[X 1 ,X 2 ,...,X N ] T . (2) Assuming that each sensor signal is a linear combination of M lessorequalslant N underlying but un- known and uncorrelated L-dim source signals (vectors i ), a source trajectory matrix S can be written in analogy to Eq. (1). To simplify notation, in the following we will only consider the case: N = M. Then the sensor signal trajectory matrix can be expressed as X = AS, A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 403 where the total NK × NK-dimensional mixing matrix A is given by a Kronecker prod- uct A = a ⊗ I K×K . Then, the mixing matrix is a block matrix with a diagonal matrix a ij = a ij I K×K in each block, i.e., A =       a 11 I K×K a 12 I K×K ... a 1N I K×K a 21 I K×K a 22 I K×K ... ... . . . . . . . . . . . . a N1 I K×K ... ... a NN I K×K       . (3) The matrix I K×K represents the identity matrix and the mixing coefficient a ij relates the sensor signal i with the source signal j of the corresponding component trajectory matri- ces. As we are dealing with an instantaneous mixing model, all delayed versions of a signal are related by the same coefficient. 2.2. A GEVD using matrix pencils Next a correlation matrix R x =〈XX H 〉 (H denotes the Hermitian conjugate) of the sensor data and its time-delayed counterpart R x (d) are computed to form a matrix pencil. Note that the delay δ d = d · τ, d ∈N forms an integer multiple of the sampling interval τ. The time-delayed correlation matrix of the matrix pencil is computed with one matrix X r obtained by eliminating the first d columns of X and another matrix, X l , obtained by eliminating the last d columns. Then, the time-delayed correlation matrix R x (d) = 〈X r X H l 〉 will be an NK × NK matrix. Each of these two correlation matrices can be related with a corresponding correlation matrix in the source signal domain R x (d) = AR s (d)A H = A〈S r S H l 〉A H . (4) Then the two pairs of matrices (R x (0),R x (d)) and (R s (0),R s (d)) represent congruent pencils [13] with the following properties: • Their eigenvalues are the same, i.e., the eigenvalue matrices of both pencils are identi- cal: D x = D s . • If the eigenvalues are non-degenerate (distinct values in the diagonal of the ma- trix D x = D s ), the corresponding eigenvectors are related by the transformation E s = A H E x . Assuming that all sources are uncorrelated, the matrices R s (d) are block diagonal, having block matrices R ii (d) =〈(S r ) i (S l ) H i 〉 along the diagonal R s (d) =       R 11 (d) 0 ... 0 0 R 22 (d) ... 0 . . . . . . . . . . . .       . (5) 00... R NN (d) 404 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 The eigenvector matrix of the GEVD of the pencil (R s (0),R s (d)) can be written as E s =       E 11 0 ... 0 0 E 22 ... 0 . . . . . . . . . . . . 00... E NN       , (6) where E ii is the K ×K eigenvector matrix of the GEVD of the pencil (R ii (0),R ii (d)).The estimated uncorrelated components Y can be estimated from linearly transformed sensor signals via Y = E H x X = E H x AS = E H s S (7) hence turn out to be filtered versions of the underlying source signals. As the eigenvector matrix E s (Eq. (6)) is a block diagonal matrix, there are K signals in each column of Y which are a linear combination of one of the source signals and its delayed versions. For instance, the block i depends on the source signal i via K summationdisplay k=1 (E ii ) k,j s i parenleftbig t l + (K − k)Delta1t parenrightbig . (8) Equation (8) defines a convolution operation between column j of E ii and source sig- nal vectors i . Hence, the columns of the matrix E ii represent impulse responses of finite impulse response (FIR) filters. Considering that all the columns of E ii are different, their frequency response will provide different spectral densities of the source signal spectra. Then each of the N output signals vectory i , i = 1,...,NK, encompasses K filtered versions of each of the N estimated source signals vectors i . Note that the frequency shaping introduced by this linear filtering operation depends on the length (number of delays K) of the filter and on the coefficients in each column of E ii . In summary, the algorithm dAMUSE yields uncorrelated component signals which are filtered versions of the underlying source signals. Hence similar to blind deconvolution methods in addition to scaling and permutation indeterminacies there appears a filtering indeterminacy here. 2.3. Implementation of the algorithm dAMUSE There are several ways to compute the eigenvalues and eigenvectors of a matrix pencil if one of the matrices is symmetric positive definite [14]. One of those methods is based on standard eigenvalue decompositions (EVD) applied in two consecutive steps. One of the correlation matrices of the pair is computed for delay zero (δ = 0) to ensure the symmetric positive definite condition. So, considering the pencil (R x (0),R x (d)) the following steps are proposed: Step 1. Compute a standard eigenvalue decomposition of R x (0) = VLambda1V H , i.e., the eigen- vectors (vectorv i ) and eigenvalues (λ i ). As the matrix is symmetric positive definite, the eigen- values can be organized in descending order (λ 1 >λ 2 > ···>λ NK ). In AMUSE (and other algorithms) this procedure can be used to estimate the number of sources related A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 405 to the signal, hence can be considered a strategy to reduce noise. Dropping small eigen- values amounts to a projection from a high-dimensional signal plus noise feature space onto a lower-dimensional manifold representing the signal subspace. Thereby it is tac- itly assumed that small eigenvalues are related with noise components only. In this work, we consider a variance criterium to choose the most significant eigenvalues accord- ing to λ 1 + λ 2 +···+λ l λ 1 + λ 2 +···+λ NK greaterorequalslantΘ. (9) If we are interested in the eigenvectors corresponding to directions of high variance of the signals, the threshold (Θ) should be chosen so that the maximum energy of the signals is preserved. A transformation matrix Q can then be computed using either the l most significant eigenvalues λ i or all of the eigenvalues and respective eigenvectors vectorv i . Considering that the matrix R x (0) can be approximated with l eigenvalues and eigenvectors, i.e., R x (0) similarequal V l Lambda1 1/2 l Lambda1 1/2 l V H l , the transformation matrix [14] is then computed by Q =Lambda1 −1/2 l V H l , (10) where Q is an l × NK matrix if a reduction in dimension (l lessorequalslantNK) is considered. Step 2. Compute the matrix C = QR x (d)Q H and its standard eigenvalue decomposition: the eigenvector matrix U and eigenvalue matrix D x . The eigenvectors (which are not normalized) of the pencil (R x (0),R x (d)) form the columns of the eigenvector matrix E x = Q H U = V l Lambda1 −1/2 l U. (11) Note that if the threshold Θ = 1, the corresponding result can be obtained with the command eig(R x (0),R x (d)) using the most recent version of MATLAB. What concerns the uncorrelated components of the sensor signals and their delayed versions, they can be estimated via the transformation Y = E H x X = U H QX = U H Lambda1 −1/2 l V H l X (12) and either l or NK estimated uncorrelated signals are obtained representing filtered ver- sions of the underlying source signal estimates. 3. Results The proposed algorithm was applied to artificially mixed signals, RR and QT sequences of electrocardiograms (ECG) and 2D NOESY NMR signals.The artificial mixtures allow the illustration of the method once every parameter of the model is known. The heart rate variability study (HRV), based on RR and QT sequences, illustrates a preliminary application to real data. The heart rate variability (HRV) spectrum is currently separated into three frequency bands: very low frequency band (VLF range below 0.04 Hz), low frequency band (LF range 0.04–0.15 Hz) and high frequency band (HF range 0.15– 406 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 0.4 Hz). Hence it is interesting to know if the extracted signals can be related with LF and HF fluctuations of cardiovascular signals. A similar strategy applied to RR and QT sequences was proposed in [15,16] where a principal component analysis (PCA) followed by a generalized eigendecomposition was used to extract inherent uncorrelated component signals. The application of dAMUSE to 2D NOESY NMR signals was motivated mainly by the necessity to denoise the reconstructed spectra after the water artifact has been separated from the protein spectra using BSS techniques. The proposed algorithm dAMUSE offers an elegant and efficient way to perform both the blind signal separation and the denoising simultaneously. 3.1. Artificially mixed signals Using artificial signals, the simulations were designed to illustrate the method and to study the influence of its parameters, especially the dimension of the embedding K and the threshold Θ, on the respective performance. As the separation process also involves a linear filtering operation, each uncorrelated component estimated has its maximum correlation with one of the source signals for a non-zero delay, besides the usual indeterminacy in order and amplitude. 3.1.1. Delays In the first experiment, two artificial source signals with very similar frequency contents were considered (see Fig. 1). The signals were randomly mixed and the algorithm was ap- plied for different numbers K of delays using Θ = 1, d = 1. Figure 2 shows the results obtained when K = 3. In that case six uncorrelated components corresponding to filtered versions of the two source signals are computed: three of them (Fig. 2a) are related with the first source signal, while the other three (Fig. 2b) are related with the second source sig- nal. We can see that these estimated components represent different frequency contents of each of the corresponding source signals which each comprise a high and a low frequency component. For instance, Fig. 2a shows that: (1) The first estimated component isolates the high frequency component HF of source signal 1. (2) The third estimated component isolates the low frequency component LF of source signal 1. (3) The second estimated component contains both frequencies of the source signal. Figures 2c and 2d represent the frequency response amplitude of the filters represented by the columns of the transformed mixing matrix E H x A which can be considered as containing the coefficients of the corresponding filter. These filters are then paired with the respective uncorrelated source signals. It can easily be verified that the frequency response confirms the frequency contents of each of the uncorrelated signals as the filters obtained by the algorithm represent a low-pass, a band-pass, and a high-pass filter (see Fig. 2d). Increasing the number K of delays, the uncorrelated components show either the low frequency or high frequency mode of the sensor signals. In these cases, the frequency A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 407 Fig. 1. Artificial signals comprising a superposition of a high-frequency and a low-frequency sinusoidal signal: (a) source signals, (b) spectral densities of the source signals. responses of the columns of E H x A do not show a clear band-pass characteristic anymore which includes the frequency contents of both source signals. Rather the filters look more similar to a multi-pass filter structure. Another problem is that the eigenvalues of the first standard eigendecomposition start to have very low values. In that case it is indicated to consider a threshold Θ<1 and also reduce the number of signals at the output of the first step from NK to l<NK. In a second experiment three artificially generated, narrow-band source signals with different frequency contents were chosen: one member of the group represents a narrow- band signal, a sinusoid; the second signal encompasses a wide frequency range; and the last one represents a sawtooth wave whose spectral density is concentrated in the low frequency band (see Fig. 3). For any number K of delays, the number of signals after the first step (or the dimension of matrix C)isl = 6 <NK, because the threshold Θ = 0.95 eliminates all very low eigenvalues. Even though, the number of output signals is still higher than the number of source signals. Thus only 3 output signals which each has a high correlation (in the frequency domain) with one of the source signals will be considered in the following. It can be verified easily that upon increasing the number K of delays, the estimated independent signals decrease their bandwidth (except for the sinusoid). For example, Fig. 4 shows that the spectrum of output signal 2 has less components when K increases. The effect is also visible in the spectrum of output signal 3, but here the time domain characteristics of the wave are less affected as is to be expected. This 408 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 Fig. 2. (a) Independent components correlated with source signal 1, (b) independent components correlated with source signal 2, (c) frequency response amplitude of filters related with components represented in (a), (d) fre- quency response amplitude of filters related with components represented in (b). artificially mixed setup shows that the generalized eigenvalue decomposition in high- dimensional space results in a filtering indeterminacy that depends on the number of delays K. However it has been verified that the frequency range of every source is covered by the reconstructed signals. And if the sources have a narrow band frequency contents, it is possible to separate that source from all others. 3.1.2. Signal-to-noise ratios and thresholds The second group of simulations consider the influence of noise on the performance of the algorithm. The set of three artificial source signals of the last section was linearly A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 409 Fig. 3. Artificial signals (left column) and their frequency contents (right column). Fig. 4. Frequency contents of the output signals considering different time-delays K to form the input data matrix, hence the embedding dimension of the feature space. 410 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 Fig. 5. Comparing AMUSE and dAMUSE using the MSE error: (a) sinusoid signal, (b) triangular signal. mixed and random noise was added, resulting in a signal-to-noise ratio SNR in the range of −5dBlessorequalslant SNR lessorequalslant 20 dB. The mixtures then have been analyzed with the algorithm dAMUSE. The parameters of the algorithm were chosen as follows: K = 4, Θ = 0.95, d = 1. In what concerns noise we also tried to figure out if there is any advantage of using a GEVD instead of a PCA analysis. Hence the signals at the out put of the first step of the algorithm (using the matrix Q to project the data) are also compared with the signals obtained after the second step. The first experiment compares the performance of algorithm dAMUSE with AMUSE (the pencil has 3 × 3 matrices and was computed also with d = 0 and 1), using the mean square error (MSE) to measure the deviations of the source signals from the recovered signals for the different noise levels. Only those filtered signals, recovered at the output of dAMUSE, which retain most of their frequency content compared to the original were used in the comparison (i.e., the sinusoid and sawtooth signals). Figure 5 shows that dAMUSE performs the better the higher the noise level, i.e., for SNRlessorequalslant20 dB for both signals. Fig- ure 5b, however, shows a better performance for AMUSE when SNR = 20 dB, but this is a natural consequence of the filtering process causing a lack of high-frequency components for the filtered version of the source signal obtained at the output of dAMUSE. The following results illustrate the role of the threshold parameter Θ selecting the di- mension of the signal subspace when noisy signals are considered. The parameters K = 4 and Θ = 0.95 were kept fixed. As the noise level increases, the number of significant eigenvalues also increases. Hence, at the output of the first step more signals need to be considered. Thus as the noise energy increases, the number of signals (l) or the dimension of matrix C after the application of the first step increases (see last column of Table 1). As a consequence, with increasing noise level an increasing number of independent compo- nents will be available at the output of the second step also. Computing, in the frequency domain, the correlation coefficients between either the output signals of each step of the algorithm and noise or the output signals and the source signals, we confirm, that some are A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 411 Table 1 Number of output signals correlated with either noise or source signals after each step of the algorithm dAMUSE SNR (dB) NK First step Second step Sources Noise Sources Noise Total 20 12 6 0 6 0 6 15 12 5 2 6 1 7 10 12 6 2 7 1 8 512 3 29 0 17 48 311 related with the sources and others with noise. Table 1 (columns 3–6) shows that the max- imal correlation coefficients are distributed between noise and source signals to a varying degree, but the number of signals correlated with noise (Table 1) is always higher in the first level. These results show that for a low noise level, i.e., a high SNR, the first step (which is mainly a principal component analysis in a space of dimension NK) already achieves good solutions. But when the noise level increases, hence the SNR decreases, the additional second step, representing the GEVD of the matrix pencil improves results considerably. This was corroborated also by adding high energy noise and taking only the five most significant eigenvalues, and corresponding eigenvectors, at the output of the first step. In these cases the source signals might not be recovered. 3.2. Heart rate variability study A second study to show the performance of the algorithm dAMUSE deals with real world signals. We use ECG signals of two young normal subjects from the POLY/MEDLAV database [17]. One lead of the recorded ECG signals was processed by a wavelet transform based automatic delineation system [18], and the RR and QT intervals (see Fig. 6 for an explanation) were measured between the marks obtained and form the non-uniform sequences which are processed by dAMUSE. The ECG signals of these two subjects were chosen because the corresponding respira- tion signals exhibit spectral contents in distinct bands: (1) ECG1 corresponds to a respiratory signal in a low frequency band LF = (0.05–0.1Hz). (2) ECG2 corresponds to a signal in a high frequency band HF = (0.2–0.4Hz). Each heart beat (Fig. 6) is characterized by two values: the time interval between consecu- tive R peaks (RR) on the one hand and the interval between the onset of the QRS complex and the end of the T wave (QT) on the other hand. Then two sequences, synchronized with the onset of the QRS complex, are constituted by the values of the RR intervals and the QT intervals. The algorithm dAMUSE was applied to [RR(i) and [RR(i),QT(i)] T sequences of time intervals (to apply the formalism given in Section 2, simply set τ = 1 and Delta1t = 1 and t 0 = 0 in Eq. (1)). Different numbers of delays were tested but the best results were achieved by K = 8, the other parameters of the algorithm are d = 1, and Θ = 0.95. The estimated independent component signals are not correlated in the time domain and suffer 412 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 Fig. 6. Schematic representation relevant waves and intervals in a cycle of the ECG signal. from an indeterminacy on its amplitude as it is expected from these methods. Then, in order to achieve a direct comparison, the ICs were normalized to unit variance and amplitudes in the range [−1,1]. However, in HRV studies, the frequency contents in distinct frequency bands are the relevant information [19]. Hence an analysis in frequency space is performed for each independent signal using the Welch method [20]. The results in the frequency domain are compared graphically and the correlation coefficients are also computed. 3.2.1. RR time series In a first experiment we considered the time intervals between the R-waves of the ECGs. For these experiments a trajectory matrix is formed with the input vectors vectorx i (t l ),using K = 8 delayed versions of RR time intervals. The number of different independent com- ponents estimated for both ECG signals studied is not the same as can been see in Figs. 7a and 7c). The sequence of RR time intervals contains a prominent high-frequency component if the corresponding respiratory signal contains a high frequency component as well, i.e., in ECG2 signal. Nevertheless, we can see that in both cases (ECG1 and ECG2) the com- ponents computed correspond to distinct frequency ranges. Table 2 also shows that those components, which have correlation coefficients equal to zero in the time domain, also have small values when the coefficients are computed in the frequency domain. 3.3. RR-QT time series For these experiments the input vectors vectorx i (t l ), i = 1,2 were used to form a concate- nated trajectory matrix using K = 8 delayed RR- and K = 8 delayed QT time intervals. The parameters of the algorithm (Θ = 0.95 and d = 1) are the same as before, but here the number of output signals is higher (6 and 4, respectively for ECG1 and ECG2). Among all uncorrelated components estimated those three components vectory j , j = 1,2,3, with spec- tral densities in distinct frequency ranges (see Fig. 8) are chosen which show the highest contributions to the reconstruction of the original RR and QT sequences. In Table 3 the correlations between the frequency contents of the uncorrelated compo- nents is shown. The results confirm that the correlation between the frequency contents of the uncorrelated signals estimated is very low. Also, two of those signals have a frequency content in the range of the LF and HF fluctuations considered in the HRV studies. A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 413 Fig. 7. ECG1: (a) independent components, (b) spectral contents of independent components; ECG2: (c) inde- pendent components, (d) spectral contents of independent components. Table 2 Correlation coefficients between spectra of independent components considering the RR sequence ECG1 ECG2 s 1 s 2 s 3 s 1 s 2 s 3 s 1 10.26 – 1 0.38 0.13 s 2 0.26 1 – 0.38 1 0.33 s –––.13 0.33 1 3 414 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 Fig. 8. ECG1: (a) independent components, (b) spectral densities of independent components; ECG2: (c) inde- pendent components, (d) spectral densities of independent components. Table 3 Correlation coefficients between the uncorrelated component spectra ECG1 ECG2 s 1 s 2 s 3 s 1 s 2 s 3 s 1 10.10 0.09 1 0.38 0.14 s 2 0.10 1 0.03 0.38 1 0.13 s 0.09 0.03 1 0.14 0.13 1 3 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 415 For both signals the LF and VLF components are very similar to the ones computed with sequences of RR intervals as can be seen comparing Figs. 7 and 8. In fact, the correlation coefficients computed between VLF or LF components using RR or (RR, QT) sequences are greaterorequalslant 0.95. The signal ECG1 now presents a third component, representing the high fre- quency band which was not found when the input corresponded to the RR sequence alone. This suggests, that the intervals RR and QT reflect different influences of the autonomous nervous system as also discussed in Ref. [16]. In spite of a third component appearing for ECG2, the dissimilarity found in shape and dispersion of the spectra with RR or (RR, QT) sequences as inputs reinforces the supposition of different effects. This hypothesis was al- ready stated in Ref. [21] when studying the dynamic relation between HRV and QTV using a very different approach; furthermore our results are consistent with the ones presented in Ref. [21]. 3.4. Water artifact removal and denoising of NMR spectra Because of magnetic field inhomogeneities and peak suppression techniques the water signal cannot be modelled analytically. This prevents maximum likelihood estimations of model parameters within a nonlinear least squares approach to be applied successfully. Hence, in recent years post-processing algorithms have been developed to remove the water resonance (simply called water artifact) from the NMR spectra [22–29]. Early al- gorithmic approaches use rather crude approximations, causing changes in peak area and phase which render the structure determination process rather unreliable, hence are unac- ceptable in protein NMR. More sophisticated water suppression algorithms are based on second order statistics of the signals within an unsupervised approach. They usually use a state-space approach in the time domain by modelling the signal subspace using second order techniques like singular value decomposition (SVD) [30] or some variant of singu- lar spectral analysis (SSA) [28]. With multidimensional NMR data, these approaches are computationally very demanding in general. Furthermore they have been applied so far to rather “simple” NMR spectra only, where the BSS techniques discussed above can yield rather perfect solutions. Blind source separation techniques using independent component analysis methods recently became famous for their ability to blindly decompose compli- cated signals into their underlying component signals without using any prior knowledge. Hence it is interesting whether BSS techniques can contribute to the removal of the water artifact in proton NMR spectra of aqueous biomolecular solutions. We show that the algorithm dAMUSE can be used elegantly to separate the water ar- tifact from a protein proton NMR spectrum and simultaneously perform denoising of the reconstructed spectrum. We demonstrate this combination of BSS and denoising with 2D NOESY proton NMR spectra of the polypeptide P11 [11]. Figure 9 shows a 1D slice (corresponding to the shortest evolution period of the NMR pulse sequence) of the original proton NMR spectrum of P11 and the corresponding artefact-free spectrum obtained with a GEVD using a matrix pencil in the frequency do- main [31]. The huge water artefact could be removed successfully but only at the expense of an increased noise level of the reconstructed protein spectrum. Hence denoising as a post-processing deemed necessary. But dAMUSE can achieve both goals in one step as we will demonstrate next. 416 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 Fig. 9. Proton NMR spectrum of the polypeptide P11 top: original; bottom: GEVD-MP [31]. The starting point for the application of dAMUSE to P11 were 128 complex time do- main signals taken from a 2D NOESY proton NMR experiment where each of the 128 signals consisted of 2048 samples taken with a sampling rate of τ −1 = (90 µs) −1 similarequal 11 kHz. For every signal the component trajectory matrix X i , i = 1,...,128 (cf. Eq. (2)), was formed by using only one time delay (K = 2) of size Delta1t = 2τ = 180 µs. Thus the result- ing overall trajectory matrix X had a size 256 × 2046. The data of the trajectory matrix is manipulated in the frequency domain to compute a filtered version X f of the data in X. The matrix ˆ X was determined by computing the discrete Fourier transform of each row of X. A filtered version of the matrix ˆ X was gen- erated by applying a gaussian shaped function to every row of ˆ X leading to a matrix ˆ X f of bandpass-filtered sensor signals. The Gaussian filter was centered in the region of the water peak and had a width σ = 1. The filtering operation is thus achieved by computing the Hadamard product between the rows of ˆ X and the frequency response function of a Gaussian bandpass filter [32]. Finally, an inverse discrete Fourier transform of each row of the matrix ˆ X f was calculated to obtain a filtered version of the time domain data ma- trix X f . The two matrices are used to compute the matrix pencil formed with R =〈XX H 〉 and R f =〈X f X H f 〉. A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 417 Fig. 10. Reconstructed spectrum of the polypeptide P11 using top: GEVD-MP [31]; bottom: GEVD-dAMUSE. Next the two step procedure explained in Section 2.3 was applied. Only the 95 largest eigenvalues of the first EVD of the 256 × 256 correlation matrix R were considered after the first step in order to reduce noise. Then, the 95 × 256 matrix Q was formed and the EVD problem of the matrix C = QR f Q H was solved. Its eigenvector matrix U and the transformation matrix Q lead to the eigenvector matrix E x = Q H U of the matrix pencil according to Section 2.3. Finally, those estimated components of Y = E H x X which showed a significant spectral density only at the resonance frequency of the water protons were set to zero and with the remaining ICs the artifact-free protein spectra were reconstructed. Note that if less than the 95 largest eigenvalues were used the separation of the water and the protein signals failed whereas considering more than 100 of the largest eigenvalues lead to a drastic increase in noise. Figure 10 compares the results obtained by the stan- dard GEVD-MP [31] and the dAMUSE algorithms corresponding to SNRs of 17.3 and 22.43 dB, respectively. 4. Conclusions In this work we propose dAMUSE, a modified version of the algorithm AMUSE. The new algorithm is also based on a generalized eigendecomposition of a congruent matrix 418 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 pencil. But the latter is formed with correlation matrices which are computed with sensor signals embedded in a high-dimensional feature space of time-delayed coordinates much in the spirit of methods used in singular spectrum analysis [8]. It was shown that the indepen- dent components obtained at the output represent filtered versions of the unknown source signals. An implementation of the algorithm dAMUSE was also proposed which differs from the implementation of the algorithm AMUSE (and other blind source separation al- gorithms). dAMUSE defines a variance criterion for choosing the number of signals at the output of the first step (which is very similar to a principal component decomposition), hence paves the way for a denoising of the signals in parallel to the BSS. The algorithm has a set of parameters, whose proper choice must be further studied, particularly the number of delays K used to build the data set and the optimal choice of the threshold parameter Θ. The choice of K naturally constrains the linear filtering operation that characterizes the method, as was shown in the simulations with artificial signals. It was shown that additional a priori knowledge of characteristics of the spectral density of the signals considered can help in choosing K appropriately. The simulations also reveal that noise reduction cannot be achieved completely by PCA alone. Then having at the output a high number of signals, it is also important to find an automatic procedure to choose the relevant signals, as in case of noisy signals some might be related with noise only. The choice of time-delayed matrices to compute the pencil was done similar to the algorithm AMUSE, as well as the values for time-delays (d) [1]. With the NMR data an alternative filtering in the frequency domain was used to estimate one of the matrices of the pencil as discussed in other works [33–35] and good results were also achieved. In what concerns the HRV study, blind source separation techniques were used to study similar time-series signals (RR and QT intervals) and to discuss the relation of the uncor- related components extracted with the LF and HF fluctuations [16] of ECG signals. This preliminary study achieves results corroborating an earlier study [21] in spite of the use of different processing steps concerning the pre-processing of the temporal sequences and the method to estimate the uncorrelated signals. The new algorithm seems to provide a promising tool to HRV studies. Nevertheless a validation against a conventional method as well as a large data set should be used to verify the reliability and the performance of the method. Concerning the study of 2D NOESY NMR spectra of dissolved proteins, dAMUSE combines in an elegant and efficient way blind source separation techniques with local projective denoising techniques. It offers a very fast and efficient way of removing the water artifact from the spectra and allows a denoising of the reconstructed artifact-free protein spectra to achieve noise levels at least comparable to those of the experimental spectra. Although the unknown source signals cannot be obtained straightforwardly, the method seems well suited to deal with narrow band signals where filtered versions of the signals do not differ in essential characteristics from their underlying originals. Hence the BSS problem can be solved using only the filtered signals. As in case of NMR spectra one has to deal with a large number of extracted independent components, an automatic assignment procedure is needed to assign extracted independent components to the dif- ferent signals (artifact, protein, noise). Corresponding investigations are under way in our laboratories. A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 419 Acknowledgments We thank W. Gronwald and H.R. Kalbitzer for providing the P11 data and for helpful discussions. References [1] A. Cichocki, S.-I. Amari, Adaptive Blind Signal and Image Processing, Wiley, New York, 2002. [2] A. Belouchrani, K. Abed-Meraim, J.-F. Cardoso, E. Moulines, A blind source separation technique using second-order statistics, IEEE Trans. Signal Process. 45 (2) (1997) 434–444. [3] R. Gharieb, A. Cichocki, Second-order statistics based blind source separation using a bank of subband filters, Digital Signal Process. 13 (2003) 252–274. [4] T. Schreiber, H. Kantz, Nonlinear projective filtering ii: Application to real time series, in: Proceedings of Nolta, 1998. [5] R. Vetter, J. Vesin, P. Celka, J.K.P. Renevey, Automatic nonlinear noise reduction using local principal component analysis and mdl parameter selection, in: Proceedings of the IASTED International Conference on Signal Processing Pattern Recognition and Applications (SPPRA ’02), Crete, 2002, pp. 290–294. [6] P. Gruber, F.J. Theis, K. Stadlthanner, E.W. Lang, A.M. Tomé, A.R. Teixeira, Denoising using local ICA and kernel-PCA, in: Proceedings of International Joint Conference on Neural Networks, IJCNN ’04, Budapest, 2004. [7] V. Moskvina, K.M. Schmidt, Approximate projectors in singular spectrum analysis, SIAM J. Math. Anal. Appl. 24 (4) (2003) 932–942. [8] M. Ghil, M. Allen, M.D. Dettinger, K. Ide, Advanced spectral methods for climatic time series, Rev. Geo- phys. 40 (1) (2002) 3.1–3.41. [9] L. Tong, R.-W. Liu, V.C. Soon, Y.-F. Huang, Indeterminacy and identifiability of blind identification, IEEE Trans. Circuits Syst. 38 (5) (1991) 499–509. [10] A.M. Tomé, A.R. Teixeira, E.W. Lang, K. Stadlthanner, A. Rocha, Blind source separation using time- delayed signals, in: International Joint Conference on Neural Networks, IJCNN ’04, vol. CD, Budapest, 2004. [11] K. Stadlthanner, E.W. Lang, A.M. Tomé, A. Teixeira, C.G. Puntonet, Kernel-PCA denoising of artifact-free protein NMR spectra, in: Proc. IJCNN ’04, Budapest, 2004. [12] A.R. Teixeira, A.P. Rocha, R. Almeida, A.M. Tomé, The analysis of heart rate variability using indepen- dent component signals, in: Second International Conference on Biomedical Engineering, BIOMED ’04, IASTED, Innsbruck, 2004, pp. 240–243. [13] A.M. Tomé, N. Ferreira, On-line source separation of temporally correlated signals, in: European Signal Processing Conference, EUSIPCO ’02, Toulouse, 2002. [14] B.N. Parlett, The symmetric eigenvalue problem, in: SIAM’s Classics in Applied Mathematics, SIAM, 1998. [15] R. Vetter, P. Celka, J.M. Vesin, U. Scherrer, Sub-signal extraction of RR time series using independent component analysis, in: International Conference of the IEEE Engineering Medicine and Biology Society, vol. 20, 1998, pp. 286–289. [16] R. Vetter, N. Virag, J.M. Vesin, P. Celka, U. Scherrer, Observer of autonomic outflow based on blind source separation of the ECG parameters, IEEE Trans. Biomed. Engineer. 47 (5) (2000) 578–582. [17] F. Pinciroli, G. Pozzi, R. Rossi, M. Piovosi, A. Capo, R. Olivieri, M.D. Torre, A respiration-related ekg database, in: I.C. Society (Ed.), Computers in Cardiology, 1988, pp. 477–480. [18] J.P. Martínez, R. Almeida, S. Olmos, A.P. Rocha, P. Laguna, A wavelet-based ECG delineator: Evaluation on standard databases, IEEE Trans. Biomed. Engineer. 51 (4) (2004) 570–581. [19] Heart rate variability—standards of measurement, physiological interpretation, and clinical use, Task Force of the European Society of Cardiology and the North American Society of Pacing and Electrophysiology, Technical report, 1996. [20] A.V. Oppenheim, R.W. Schafer, Digital Signal Processing, Prentice Hall, New York, 1975. 420 A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 [21] R. Almeida, E. Pueyo, J.P. Martinez, A.P. Rocha, P. Laguna, S. Olmos, Quantification of short term QT variability versus heart rate variability, in: Publication on Proceedings of 7th Portuguese Conference of Biomedical Engineering, BIOENG ’03, Lisbon, 2003. [22] D. Marion, M. Ikura, A. Bax, Improved solvent suppression in one- and two-dimensional NMR spectra by convolution of time-domain data, J. Magn. Reson. 84 (1989) 425–430. [23] Y. Kuroda, A. Wada, T. Yamazaki, K. Nagayama, Postaquisition data processing method for suppression of the solvent signal, J. Magn. Reson. 84 (1989) 604–610. [24] M. Derich, X. Hu, Elimination of water signal by postprocessing, J. Magn. Reson. 101 (1993) 229–232. [25] K.J. Cross, Improved digital filtering technique for solvent suppression, J. Magn. Reson. 101 (1993) 220– 224. [26] J.H.J. Leclerc, Distortion-free suppression of the residual water peak in proton spectra by postprocessing, J. Magn. Reson. B 103 (1994) 64–67. [27] C.J. Craven, J.P. Waltho, The action of time-domain convolution filters for solvent suppression, J. Magn. Reson. B 106 (1995) 40–46. [28] G. Zhu, D. Smith, Y. Hua, Post-aquisition solvent suppression by singular-value decomposition, J. Magn. Reson. 124 (1997) 286–289. [29] A. van den Boogaart, D. van Ormondt, W.W.F. Pijnapple, R. de Beer, M. Ala-Korpela, in: J.G. McWhriter (Ed.), Removal of the Water Resonance from 1 H Magnetic Resonance Spectra, in: Mathematics in Signal Processing, vol. III, Clarendon, Oxford, 1994. [30] W.W.F. Pijnapple, A. van den Boogaart, R. de Beer, D. van Ormondt, SVD-based quantification of magnetic resonance signals, J. Magn. Reson. 97 (1992) 122–134. [31] K. Stadlthanner, A.M. Tomé, F.J. Theis, W. Gronwald, H.R. Kalbitzer, E.W. Lang, On the use of independent component analysis to remove water artifacts of 2D NMR protein spectra, in: 7th Portuguese Conference on Biomedical Engineering, BIOENG ’03, Lisbon, 2003. [32] K. Stadlthanner, A. Tomé, F.J. Theis, W. Gronwald, H.R. Kalbitzer, E.W. Lang, Removing water artifacts from 2D protein spectra using GEVD with congruent pencils, in: 7th International Symposium on Signal Processing and its Applications, vol. 2, Paris, 2003, pp. 85–88. [33] A.M. Tomé, Separation of a mixture of signals using linear filtering and second order statistics, in: 10th European Symposium on Artificial Neural Networks, ESANN ’02, Brugges, 2002, pp. 307–312. [34] J.V. Stone, Blind source separation using temporal predictibility, Neural Comput. 13 (7) (2001) 1557–1574. [35] K. Stadlthanner, F. Theis, E.W. Lang, A.M. Tomé, W. Gronwald, H.R. Kalbitzer, A matrix pencil approach to the blind source separation of artifacts in 2D NMR spectra, Neural Inform. Process. Lett. Rev. 1 (2003) 103–110. Ana M. Tomé received her PhD in electrical engineering from University of Aveiro in 1990. Currently she is associate professor of electrical engineering in the Department of Electronics and Telecomunications/IEETA of the University of Aveiro, where she teaches courses on digital signal processing for electronics and computer engineering diplomas. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation as well as classification and pattern recognition applications of neural networks. Ana R. Teixeira received her diploma degree in mathematics applied to technology from Uni- versity of Porto in 2003. She is currently a MsC student of electronics and telecommunications of University of Aveiro. Her research interests include biomedical digital signal processing and princi- pal and independent component analysis. Elmar W. Lang received his physics diploma in 1977 and his PhD in physics in 1980 and ha- bilitated in biophysics in 1988 at the University of Regensburg. He is apl. professor of biophysics at the University of Regensburg, where he is heading the Neuro- and Bioinformatics Group. Currently he serves as associate editor of Neurocomputing and Neural Information Processing—Letters and A.M. Tomé et al. / Digital Signal Processing 15 (2005) 400–421 421 Reviews. His current research interests include biomedical signal and image processing, indepen- dent component analysis and blind source separation, neural networks for classification and pattern recognition, and stochastic process limits in queuing applications. Kurt Stadlthanner received his physics diploma from the University of Regensburg in 2003. He is currently a doctoral student at the Institute of Biophysics, Neuro- and Bioinformatics Group, University of Regensburg. His scientific interests are in the fields of biological data processing and analysis by means of blind source separation and support vector machines. Rute Almeida was born in Porto, Portugal, in 1979. She received a 4 years degree on Mathematics Applied to Technology in 2000, from Faculty of Sciences, University of Porto (FCUP). She was with the Autonomic Function Study Center from Hospital S. João between March and October 2000, working in methods for automatic delineation of the ECG. Since October 2001 she is a PhD student of the Applied Mathematics Department of FCUP, supported by a grant from Foundation for Science and Technology (Portugal) and European Social Fund. Her main research interests are in time-scale methods and the automatic analysis of the ECG, namely the study of ventricular repolarization. Ana Paula Rocha was born in Coimbra, Portugal, in 1957. She received the Applied Mathematics degree and the Doctor degree (PhD in applied mathematics, systems theory, and signal processing) from the Faculty of Sciences, University of Porto (FCUP), in 1980 and 1993, respectively. She is currently an auxiliar professor in the Department of Applied Mathematics at FCUP, Portugal. Her re- search interests are in biomedical signals and system analysis (EMG, cardiovascular systems analysis and autonomic nervous system characterization), time–frequency/time-scale signal analysis, point processes spectral analysis, data treatment and interpretation. YDSPR 553 S1051-2004(05)00005-9 10.1016/j.dsp.2005.01.004 Elsevier Inc. dAMUSE—A new tool for denoising and blind source separation A.M. Tomé a ⁎ A.R. Teixeira a E.W. Lang b 1 K. Stadlthanner b 2 A.P. Rocha c R. Almeida c 3 a DETUA/IEETA, Universidade de Aveiro, P-3810 Aveiro, Portugal b Institute of Biophysics, University of Regensburg, D-93040 Regensburg, Germany c DMA/FCUP, Universidade do Porto, P-4000 Porto, Portugal ⁎ Corresponding author. 1 Supported by the BMBF, project: ModKog. 2 Supported by the DFG, Graduate College 638. 3 Supported by the FCT, grant: SFRH/BD/5484/2001. In this work a generalized version of AMUSE, called dAMUSE is proposed. The main modification consists in embedding the observed mixed signals in a high-dimensional feature space of delayed coordinates. With the embedded signals a matrix pencil is formed and its generalized eigendecomposition is computed similar to the algorithm AMUSE. We show that in this case the uncorrelated output signals are filtered versions of the unknown source signals. Further, denoising the data can be achieved conveniently in parallel with the signal separation. Numerical simulations using artificially mixed signals are presented to show the performance of the method. Further results of a heart rate variability (HRV) study are discussed showing that the output signals are related with LF (low frequency) and HF (high frequency) fluctuations. Finally, an application to separate artifacts from 2D NOESY NMR spectra and to denoise the reconstructed artefact-free spectra is presented also. Keywords Embedding signals Generalized eigenvalue decomposition Blind source separation Denoising Biomedical applications References [1] A. Cichocki S.-I. Amari Adaptive Blind Signal and Image Processing 2002 Wiley New York [2] A. Belouchrani K. Abed-Meraim J.-F. Cardoso E. Moulines A blind source separation technique using second-order statistics IEEE Trans. Signal Process. 45 2 1997 434 444 [3] R. Gharieb A. Cichocki Second-order statistics based blind source separation using a bank of subband filters Digital Signal Process. 13 2003 252 274 [4] T. Schreiber, H. Kantz, Nonlinear projective filtering ii: Application to real time series, in: Proceedings of Nolta, 1998 [5] R. Vetter, J. Vesin, P. Celka, J.K.P. Renevey, Automatic nonlinear noise reduction using local principal component analysis and mdl parameter selection, in: Proceedings of the IASTED International Conference on Signal Processing Pattern Recognition and Applications (SPPRA '02), Crete, 2002, pp. 290–294 [6] P. Gruber, F.J. Theis, K. Stadlthanner, E.W. Lang, A.M. Tomé, A.R. Teixeira, Denoising using local ICA and kernel-PCA, in: Proceedings of International Joint Conference on Neural Networks, IJCNN '04, Budapest, 2004 [7] V. Moskvina K.M. Schmidt Approximate projectors in singular spectrum analysis SIAM J. Math. Anal. Appl. 24 4 2003 932 942 [8] M. Ghil M. Allen M.D. Dettinger K. Ide Advanced spectral methods for climatic time series Rev. Geophys. 40 1 2002 3.1 3.41 [9] L. Tong R.-W. Liu V.C. Soon Y.-F. Huang Indeterminacy and identifiability of blind identification IEEE Trans. Circuits Syst. 38 5 1991 499 509 [10] A.M. Tomé, A.R. Teixeira, E.W. Lang, K. Stadlthanner, A. Rocha, Blind source separation using time-delayed signals, in: International Joint Conference on Neural Networks, IJCNN '04, vol. CD, Budapest, 2004 [11] K. Stadlthanner, E.W. Lang, A.M. Tomé, A. Teixeira, C.G. Puntonet, Kernel-PCA denoising of artifact-free protein NMR spectra, in: Proc. IJCNN '04, Budapest, 2004 [12] A.R. Teixeira, A.P. Rocha, R. Almeida, A.M. Tomé, The analysis of heart rate variability using independent component signals, in: Second International Conference on Biomedical Engineering, BIOMED '04, IASTED, Innsbruck, 2004, pp. 240–243 [13] A.M. Tomé, N. Ferreira, On-line source separation of temporally correlated signals, in: European Signal Processing Conference, EUSIPCO '02, Toulouse, 2002 [14] B.N. Parlett The symmetric eigenvalue problem SIAM's Classics in Applied Mathematics 1998 SIAM [15] R. Vetter, P. Celka, J.M. Vesin, U. Scherrer, Sub-signal extraction of RR time series using independent component analysis, in: International Conference of the IEEE Engineering Medicine and Biology Society, vol. 20, 1998, pp. 286–289 [16] R. Vetter N. Virag J.M. Vesin P. Celka U. Scherrer Observer of autonomic outflow based on blind source separation of the ECG parameters IEEE Trans. Biomed. Engineer. 47 5 2000 578 582 [17] F. Pinciroli G. Pozzi R. Rossi M. Piovosi A. Capo R. Olivieri M.D. Torre A respiration-related ekg database I.C. Society Computers in Cardiology 1988 477 480 [18] J.P. Martínez R. Almeida S. Olmos A.P. Rocha P. Laguna A wavelet-based ECG delineator: Evaluation on standard databases IEEE Trans. Biomed. Engineer. 51 4 2004 570 581 [19] Heart rate variability—standards of measurement, physiological interpretation, and clinical use, Task Force of the European Society of Cardiology and the North American Society of Pacing and Electrophysiology, Technical report, 1996. [20] A.V. Oppenheim R.W. Schafer Digital Signal Processing 1975 Prentice Hall New York [21] R. Almeida, E. Pueyo, J.P. Martinez, A.P. Rocha, P. Laguna, S. Olmos, Quantification of short term QT variability versus heart rate variability, in: Publication on Proceedings of 7th Portuguese Conference of Biomedical Engineering, BIOENG '03, Lisbon, 2003 [22] D. Marion M. Ikura A. Bax Improved solvent suppression in one- and two-dimensional NMR spectra by convolution of time-domain data J. Magn. Reson. 84 1989 425 430 [23] Y. Kuroda A. Wada T. Yamazaki K. Nagayama Postaquisition data processing method for suppression of the solvent signal J. Magn. Reson. 84 1989 604 610 [24] M. Derich X. Hu Elimination of water signal by postprocessing J. Magn. Reson. 101 1993 229 232 [25] K.J. Cross Improved digital filtering technique for solvent suppression J. Magn. Reson. 101 1993 220 224 [26] J.H.J. Leclerc Distortion-free suppression of the residual water peak in proton spectra by postprocessing J. Magn. Reson. B 103 1994 64 67 [27] C.J. Craven J.P. Waltho The action of time-domain convolution filters for solvent suppression J. Magn. Reson. B 106 1995 40 46 [28] G. Zhu D. Smith Y. Hua Post-aquisition solvent suppression by singular-value decomposition J. Magn. Reson. 124 1997 286 289 [29] A. van den Boogaart D. van Ormondt W.W.F. Pijnapple R. de Beer M. Ala-Korpela J.G. McWhriter Removal of the Water Resonance from 1H Magnetic Resonance Spectra Mathematics in Signal Processing vol. III 1994 Clarendon Oxford [30] W.W.F. Pijnapple A. van den Boogaart R. de Beer D. van Ormondt SVD-based quantification of magnetic resonance signals J. Magn. Reson. 97 1992 122 134 [31] K. Stadlthanner, A.M. Tomé, F.J. Theis, W. Gronwald, H.R. Kalbitzer, E.W. Lang, On the use of independent component analysis to remove water artifacts of 2D NMR protein spectra, in: 7th Portuguese Conference on Biomedical Engineering, BIOENG '03, Lisbon, 2003 [32] K. Stadlthanner, A. Tomé, F.J. Theis, W. Gronwald, H.R. Kalbitzer, E.W. Lang, Removing water artifacts from 2D protein spectra using GEVD with congruent pencils, in: 7th International Symposium on Signal Processing and its Applications, vol. 2, Paris, 2003, pp. 85–88 [33] A.M. Tomé, Separation of a mixture of signals using linear filtering and second order statistics, in: 10th European Symposium on Artificial Neural Networks, ESANN '02, Brugges, 2002, pp. 307–312 [34] J.V. Stone Blind source separation using temporal predictibility Neural Comput. 13 7 2001 1557 1574 [35] K. Stadlthanner F. Theis E.W. Lang A.M. Tomé W. Gronwald H.R. Kalbitzer A matrix pencil approach to the blind source separation of artifacts in 2D NMR spectra Neural Inform. Process. Lett. Rev. 1 2003 103 110 Ana M. Tomé received her PhD in electrical engineering from University of Aveiro in 1990. Currently she is associate professor of electrical engineering in the Department of Electronics and Telecomunications/IEETA of the University of Aveiro, where she teaches courses on digital signal processing for electronics and computer engineering diplomas. Her research interests include digital and statistical signal processing, independent component analysis, and blind source separation as well as classification and pattern recognition applications of neural networks. Ana R. Teixeira received her diploma degree in mathematics applied to technology from University of Porto in 2003. She is currently a MsC student of electronics and telecommunications of University of Aveiro. Her research interests include biomedical digital signal processing and principal and independent component analysis. Elmar W. Lang received his physics diploma in 1977 and his PhD in physics in 1980 and habilitated in biophysics in 1988 at the University of Regensburg. He is apl. professor of biophysics at the University of Regensburg, where he is heading the Neuro- and Bioinformatics Group. Currently he serves as associate editor of Neurocomputing and Neural Information Processing—Letters and Reviews. His current research interests include biomedical signal and image processing, independent component analysis and blind source separation, neural networks for classification and pattern recognition, and stochastic process limits in queuing applications. Kurt Stadlthanner received his physics diploma from the University of Regensburg in 2003. He is currently a doctoral student at the Institute of Biophysics, Neuro- and Bioinformatics Group, University of Regensburg. His scientific interests are in the fields of biological data processing and analysis by means of blind source separation and support vector machines. Rute Almeida was born in Porto, Portugal, in 1979. She received a 4 years degree on Mathematics Applied to Technology in 2000, from Faculty of Sciences, University of Porto (FCUP). She was with the Autonomic Function Study Center from Hospital S. João between March and October 2000, working in methods for automatic delineation of the ECG. Since October 2001 she is a PhD student of the Applied Mathematics Department of FCUP, supported by a grant from Foundation for Science and Technology (Portugal) and European Social Fund. Her main research interests are in time-scale methods and the automatic analysis of the ECG, namely the study of ventricular repolarization. Ana Paula Rocha was born in Coimbra, Portugal, in 1957. She received the Applied Mathematics degree and the Doctor degree (PhD in applied mathematics, systems theory, and signal processing) from the Faculty of Sciences, University of Porto (FCUP), in 1980 and 1993, respectively. She is currently an auxiliar professor in the Department of Applied Mathematics at FCUP, Portugal. Her research interests are in biomedical signals and system analysis (EMG, cardiovascular systems analysis and autonomic nervous system characterization), time–frequency/time-scale signal analysis, point processes spectral analysis, data treatment and interpretation. "
    },
    {
        "doc_title": "The analysis of heart rate variability using independent component signals",
        "doc_scopus_id": "11144253585",
        "doc_doi": null,
        "doc_eid": "2-s2.0-11144253585",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Eigendecomposition",
            "Frequency bands",
            "Transformation matrix"
        ],
        "doc_abstract": "Assuming that the fluctuations in the RR or in the (RR, QT) intervals of an ECG signal correspond to measures or sensor signals of a linear mixing model the independent components or source signals are computed. To extract the independent signals we propose a modified version of the algorithm AMUSE. A preliminary experimental study show that the estimated independent components are in fact related with the Low Frequency (LF) and with the High Frequency (HF) fluctuations of heart rate variability (HRV).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Denoising using local ICA and Kernel-PCA",
        "doc_scopus_id": "10844278202",
        "doc_doi": "10.1109/IJCNN.2004.1380936",
        "doc_eid": "2-s2.0-10844278202",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Autocorrelation method",
            "Denoising algorithms",
            "FMRI",
            "Gaussian distribution",
            "Minimum description length (MDL)"
        ],
        "doc_abstract": "We present a denoising algorithm for enhancing noisy signals based on local independent component analysis (ICA). This is done by applying ICA to the signal in localized delayed coordinates. The components resembling the signals can be detected by various criteria depending on the nature of the signal. Estimators of kurtosis or the variance of the autocorrelation have been considered. The algorithm proposed can favorably be applied to the problem of denoising multidimensional data like images or fMRI data sets. In comparison to denoising algorithms using wavelets, Wiener filters and kernel PCA the local PCA and ICA algorithms perform considerably better. We provide applications of the algorithm to images and the analysis of protein NMR spectra.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Blind source separation using time-delayed signals",
        "doc_scopus_id": "10844254846",
        "doc_doi": null,
        "doc_eid": "2-s2.0-10844254846",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Cumulant matrices",
            "Data vectors",
            "Independent signals",
            "Mixed signals"
        ],
        "doc_abstract": "In this work a modified version of AMUSE, called dAMUSE, is proposed. The main modification consists in increasing the dimension of the data vectors by joining delayed versions of the observed mixed signals. With the new data a matrix pencil is computed and its generalized eigendecomposition is performed as in AMUSE. We will show that in this case the output (or independent) signals are filtered versions of the source signals. Some numerical simulations using artificially mixed signals as well as biological data (RR and QT intervals of Electrocardiogram) are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Kernel-PCA denoising of artifact-free protein NMR spectra",
        "doc_scopus_id": "10844223640",
        "doc_doi": "10.1109/IJCNN.2004.1380913",
        "doc_eid": "2-s2.0-10844223640",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Biomolecules",
            "Cost functions",
            "Free induction decay (FID)",
            "Three dimensional (3D) structures"
        ],
        "doc_abstract": "Multidimensional 1H NMR spectra of biomolecules dissolved in light water are contaminated by an intense water artifact. Generalized eigenvalue decomposition methods using congruent matrix pencils are used to separate the water artefact from the protein spectra. Due to the statistical separation process, however, noise is introduced into the reconstructed spectra. Hence Kernel - based denoising techniques are discussed to obtain noise- and artifact - free 2D NOESY NMR spectra of proteins.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Delayed AMUSE - A tool for blind source separation and denoising",
        "doc_scopus_id": "35048849480",
        "doc_doi": "10.1007/978-3-540-30110-3_37",
        "doc_eid": "2-s2.0-35048849480",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "De-noising",
            "Feature space",
            "Generalized eigen decomposition",
            "Matrix pencil",
            "Noise levels",
            "Output signal",
            "Source signals"
        ],
        "doc_abstract": "In this work we propose a generalized eigendecomposition (GEVD) of a matrix pencil computed after embedding the data into a high-dim feature space of delayed coordinates. The matrix pencil is computed like in AMUSE but in the feature space of delayed coordinates. Its GEVD yields filtered versions of the source signals as output signals. The algorithm is implemented in two EVD steps. Numerical simulations study the influence of the number of delays and the noise level on the performance. © Springer-Verlag 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Denoising using local ICA and a generalized eigendecomposition with time-delayed signals",
        "doc_scopus_id": "33750111744",
        "doc_doi": "10.1007/978-3-540-30110-3_125",
        "doc_eid": "2-s2.0-33750111744",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "De-noising algorithm",
            "Generalized eigen decomposition",
            "Generalized eigenvalue decompositions (GEVD)",
            "Independent component analysis(ICA)",
            "Minimum description length",
            "Protein NMR spectra",
            "Signal sub-space",
            "Time delayed signals"
        ],
        "doc_abstract": "We present denoising algorithms based on either local independent component analysis (ICA) and a minimum description length (MDL) estimator or a generalized eigenvalue decomposition (GEVD) using a matrix pencil of time-delayed signals. Both methods are applied to signals embedded in delayed coordinates in a high-dim feature space Ω and denoising is achieved by projecting onto a lower dimensional signal subspace. We discuss the algorithms and provide applications to the analysis of 2D NOESY protein NMR spectra. © Springer-Verlag 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Removing water artefacts from 2D protein NMR spectra using GEVD with congruent matrix pencils",
        "doc_scopus_id": "84904311389",
        "doc_doi": "10.1109/ISSPA.2003.1224821",
        "doc_eid": "2-s2.0-84904311389",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "FastICA algorithms",
            "Generalized eigenvalue decompositions (GEVD)",
            "H NMR spectra",
            "Light water",
            "Matrix pencil",
            "Protein NMR spectra"
        ],
        "doc_abstract": "Multidimensional1 H nmr spectra of biomolecules dissolved in light water are contaminated by an intense water artefact. We discuss the application of the generalized eigenvalue decomposition (GEVD) method using a matrix pencil to explore the time structure of the signals in order to separate out the water artefacts. Simulated as well as experimental 2D NOESY spectra of proteins are studied. Results are compared to those obtained with the FastICA algorithm. © 2003 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A generalized eigendecomposition approach using matrix pencils to remove artefacts from 2D NMR spectra",
        "doc_scopus_id": "35248880526",
        "doc_doi": "10.1007/3-540-44869-1_73",
        "doc_eid": "2-s2.0-35248880526",
        "doc_date": "2003-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "2D NMR spectra",
            "FastICA algorithms",
            "Generalized eigendecomposition",
            "Generalized eigenvalue decomposition",
            "H NMR spectra",
            "Light water",
            "Matrix pencil",
            "1H-NMR spectra",
            "2D NMR spectra",
            "FastICA algorithms",
            "Generalized eigen decomposition",
            "Generalized eigenvalue decompositions (GEVD)",
            "Light water",
            "Matrix pencil"
        ],
        "doc_abstract": "Multidimensional1 H nmr spectra of biomolecules dissolved in light water are contaminated by an intense water artefact. We discuss the application of the generalized eigenvalue decomposition (GEVD) method using a matrix pencil to explore the time structure of the signals in order to separate out the water artefacts. Simulated as well as experimental 2D NOESY spectra of proteins axe studied. Results axe compared to those obtained with the FastICA algorithm. © Springer-Verlag Berlin Heidelberg 2003.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Approximate diagonalization approach to blind source separation with a subset of matrices",
        "doc_scopus_id": "29444433457",
        "doc_doi": "10.1109/ISSPA.2003.1224826",
        "doc_eid": "2-s2.0-29444433457",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Diagonalization method",
            "Diagonalizations",
            "Generalized eigen decomposition",
            "Ideal model",
            "Matrix pencil",
            "Robust solutions",
            "Simultaneous diagonalization"
        ],
        "doc_abstract": "In blind source separation problems it is assumed that the approximate diagonalization of a matrix set achieves more robust solutions than the simultaneous diagonalization of a matrix pencil. In this work we will analyse approximate diagonalization methods using a generalized eigendecomposition (GED) of any pair of a given matrix set. The constraints of GHD solutions provide a criterion to choose a matrix subset even when none of the matrices follows an ideal model. We also present some numerical simulations comparing the performance of the solutions achieved by the referred approaches. © 2003 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On-line source separation of temporally correlated signals",
        "doc_scopus_id": "84960877722",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84960877722",
        "doc_date": "2002-03-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Correlated signals",
            "Matrix blocks"
        ],
        "doc_abstract": "© 2002 EUSIPCO.In this work we will show that the blind source separation problem can be addressed using a linear algebra approach. Making use of the definition of congruent pencils and matrix block operations the problem is completely characterized. We also show that it is possible to have an on-line implementation of the method.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Providing an environment to teach DSP algorithms",
        "doc_scopus_id": "84954213980",
        "doc_doi": "10.1109/DSPWS.2002.1231120",
        "doc_eid": "2-s2.0-84954213980",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "DSP algorithm",
            "DSP course",
            "Elective course",
            "Real problems",
            "Real-time DSP",
            "Software frameworks",
            "Time-consuming tasks"
        ],
        "doc_abstract": "© 2002 IEEE.Teaching DSP concepts and algorithms can be very frustrating because students don't realize how to apply the concepts to solve real problems. Most of the DSP courses avoid the implementation of real-time DSP algorithms because it is a time consuming task. To minimize this drawback we developed a hardware expansion for the DSKC3X. We also have implemented a software framework that allows students to easily debug real-time DSP's algorithms. We also present and discuss the syllabus of an elective course targeted to the Electronics and Telecomunications diploma at the University of Aveiro.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of backward adaptation in a perceptual audio coder",
        "doc_scopus_id": "0034229796",
        "doc_doi": "10.1109/89.848231",
        "doc_eid": "2-s2.0-0034229796",
        "doc_date": "2000-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Forward-adaptive quantization",
            "Perceptual audio coding"
        ],
        "doc_abstract": "Typically, perceptual audio coders have followed a subband or transform coding scheme with forward-adaptive quantization. In this letter we present an alternative scheme which uses backward-adaptive quantization. We discuss the effects of this strategy on perceptual coding and show that it can be successfully applied.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Blind Source Separation using a matrix pencil",
        "doc_scopus_id": "0033699187",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0033699187",
        "doc_date": "2000-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Blind source separation",
            "Independent component analysis (ICA)",
            "Linear filters"
        ],
        "doc_abstract": "The Blind Source Separation might be addressed as a generalized eigenvalue problem. A complete formulation using linear algebra statements shows that the required matrix pencil might be computed at the input and at the output of a simple linear filter. This method will be evaluated against FastICA algorithm.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Generalized eigen-decomposition approach to blind source problems",
        "doc_scopus_id": "0033346023",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0033346023",
        "doc_date": "1999-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Blind source separation problem",
            "Correlation matrices",
            "Generalized eigenvalue decomposition (GSVD)"
        ],
        "doc_abstract": "The Blind Source Separation problem was recently addressed as a generalized eigen-value (GSVD) problem. This work is concerned with the formalization of the method using linear algebra statements. This mathematical framework also reveals a new method to compute the two correlation matrices used in the approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Method to extract articulatory parameters from the speech signal using neural networks",
        "doc_scopus_id": "0031360762",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0031360762",
        "doc_date": "1997-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Articulatory mapping",
            "Kohonen neural networks"
        ],
        "doc_abstract": "In this paper we present a method that uses artificial neural networks for acoustic to articulatory mapping. An assembly of Kohonen neural nets is used, in the first stage a network maps cepstral values, each neuron contains a subnet in a second stage that maps the articulatory space. The method allows both the acoustic to articulatory mapping, ensuring smooth varying vocal tract shapes, and the study of the non uniqueness problem.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Practical course on real-time signal processing using C-50",
        "doc_scopus_id": "0031334840",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0031334840",
        "doc_date": "1997-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Electronics and telecommunications engineering",
            "Voice compression",
            "Voice decompression",
            "Voice scramblers",
            "Voice unscramblers"
        ],
        "doc_abstract": "An optional course offered in the last year of undergraduate Electronics and Telecommunications Engineering at Universidade de Aveiro combines digital signal processors evolution, real-time programming strategies, and fixed point implementation of algorithms. The optional course uses the TMS320C50 starter kit and comprises two hours of lectures and three hours of laboratory classes per week. The laboratory classes are divided into two groups, with the first group involved in music synthesis and in sending a binary word to another kit by frequency shift keying modulation. The second group is involved in projects on reading the binary word sent by the other kit, voice scramblers and unscramblers, and voice compression and decompression.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visualization and storage of biological signals in neurophysiology laboratory",
        "doc_scopus_id": "0031293497",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0031293497",
        "doc_date": "1997-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "In this paper, we describe an application to visualize biological events, particularly suited to neurophysiology environments. The application requirements are based on the polygraphic trace. The application was developed using Object-Oriented techniques supported by Borland C++ compiler and its ObjectWindows library and Resource Workshop.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantifying the objective quality of voxel based data visualizations produced by a ray caster: A proposal",
        "doc_scopus_id": "0030721073",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030721073",
        "doc_date": "1997-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Volume rendering (VR) method",
            "Voxel based data set"
        ],
        "doc_abstract": "A set of parameters to assess the objective quality of visualizations of a voxel based data set, produced using a ray caster, is proposed as a first step toward the evaluation of the overall quality of these visualizations. Results obtained using synthetic data and a simple implementation of a ray caster are presented. The final goal of this evaluation will be the computation of `confidence indices' that could offer the user a `guided visualization', i.e. allow him/her to decide what are the `best' visualizations of a data set.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Teaching the Theory of Signals and Systems a proposal for a curriculum",
        "doc_scopus_id": "8744247274",
        "doc_doi": null,
        "doc_eid": "2-s2.0-8744247274",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Filter banks",
            "Fourier integral",
            "Theory of signals and systems",
            "University of Aviero, Portugal"
        ],
        "doc_abstract": "The teaching of the theory of signals and systems in the Department of Electronics, University of Aviero, Portugal, is discussed. The area of signals comprises four mandatory one-semester courses and a number of optional one-semester courses. The basic courses include probabilities and stochastic processes, applied mathematics, and signal processing. The optional courses are comprised of speech processing, signal processing algorithms, signal and image reconstruction, DSP hardware, and neural networks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sleep data integration and analysis an object oriented approach",
        "doc_scopus_id": "0027885116",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027885116",
        "doc_date": "1993-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This work is about an environment in which it is possible to relate and treat sleep EEG, respiratory and cardiac data coming from different sources. Some data treatment is done on-line and full data treatment is allowed off-line. The on-line treatment includes Sleep Staging using an Artificial Neural Network (ANN). This paper will try to enhance the advantages of Object Oriented Programming in the package we developed and the consequent reliability of the system architecture.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automated analysis of EEG: A new approach",
        "doc_scopus_id": "0027807708",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027807708",
        "doc_date": "1993-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Automatic sleep electroencephalography analysis",
            "Macroanalysis"
        ],
        "doc_abstract": "In this paper we describe a modular application for automatic Sleep EEG analysis. The method used for event detected is described and some of its characteristics are discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information processing models for sleep staging",
        "doc_scopus_id": "0027677414",
        "doc_doi": "10.1016/0957-4174(93)90032-2",
        "doc_eid": "2-s2.0-0027677414",
        "doc_date": "1993-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Belief automation",
            "Frontend processor",
            "Human experts",
            "Information processing models",
            "Sleep staging"
        ],
        "doc_abstract": "Three different methods of automated sleep staging are described and compared in the paper. The interesting aspect of the comparison is that the inputs to the three different information processing models (an expert system, a belief automaton, and a neural network) are the outputs of the same fronted processor that excerts EEG features. We found out that the level of man-machine agreement is very similar among the systems (around 85%) approaching the level of agreement required among human experts. However, the similarity of performance in such a diversified set of approaches points out the need for enhancing the accuracy of the first level of analysis (EEG feature extraction). © 1993.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information processing models for automatic sleep scoring",
        "doc_scopus_id": "0024941209",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0024941209",
        "doc_date": "1989-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Automatic Sleep Scoring",
            "Belief Automaton",
            "Information Processing Models",
            "Knowledge Based System",
            "Man-Machine Agreement"
        ],
        "doc_abstract": "Three different methods of information processing are described (a knowledge-based system, an automaton based on the theory of evidence, and a neural network). Their performance is compared and analyzed. One factor that is discussed is the degradation of the man-machine agreement, which is very high in the input tokens, and only high in the final result. The man-machine agreement for five sleep records, from normal subjects spanning the ages from 13 to 70 years, is shown. The records were chosen because they represent the normal sleep patterns of each age group. The token data and the human scoring are the same for all cases, so that the relative performance of the methods can be compared.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Performance and training strategies in feedforward neural networks: An application to sleep scoring",
        "doc_scopus_id": "0024924867",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0024924867",
        "doc_date": "1989-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Feedforward Neural Networks",
            "Perceptrons",
            "Sleep Scoring"
        ],
        "doc_abstract": "A comparison is made of the performance of single- and multilayer perceptrons in the scoring of sleep stages under different training conditions. The input to the neural network is a set of feature vectors, and the sleep staging is the output. Performance is the degree of agreement with the human scorer. For this application the single-layer perceptron performed at the same level as the multilayer perceptron. The best strategy for training the network is the use of human a priori knowledge. The neural network performed at the same level as other much more difficult to implement pattern recognition schemes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "MULTI-PROCESSOR ELECTROENCEPHALOGRAM (EEG) ANALYZER.",
        "doc_scopus_id": "0020948165",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0020948165",
        "doc_date": "1983-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "CLINICIANS",
            "EEG ANALYSIS",
            "INTERCHANNEL COINCIDENCES",
            "MULTIPROCESSORS",
            "PATTERN ANALYZERS",
            "TEMPORAL PATTERNS"
        ],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    }
]