[
    {
        "doc_title": "Data-Driven Analysis of European Portuguese Nasal Vowel Dynamics in Bilabial Contexts",
        "doc_scopus_id": "85130029911",
        "doc_doi": "10.3390/app12094601",
        "doc_eid": "2-s2.0-85130029911",
        "doc_date": "2022-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Process Chemistry and Technology",
                "area_abbreviation": "CENG",
                "area_code": "1508"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Fluid Flow and Transfer Processes",
                "area_abbreviation": "CENG",
                "area_code": "1507"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.European Portuguese (EP) is characterized by a large number of nasals encompassing five phonemic nasal vowels. One notable characteristic of these sounds is their dynamic nature, involving both oral and nasal gestures, which makes their study and characterization challenging. The study of nasal vowels, in particular, has been addressed using a wide range of technologies: early descriptions were based on acoustics and nasalance, later expanded with articulatory data obtained from EMA and real-time magnetic resonance (RT-MRI). While providing important results, these studies were limited by the discrete nature of the EMA-pellets, providing only a small grasp of the vocal tract; by the small time resolution of the MRI data; and by the small number of speakers. To tackle these limitations, and to take advantage of recent advances in RT-MRI allowing 50 fps, novel articulatory data has been acquired for 11 EP speakers. The work presented here explores the capabilities of recently proposed data-driven approaches to model articulatory data extracted from RT-MRI to assess their suitability for investigating the dynamic characteristics of nasal vowels. To this end, we explore vocal tract configurations over time, along with the coordination of velum and lip aperture in oral and nasal bilabial contexts for nasal vowels and oral congeners. Overall, the results show that both generalized additive mixed models (GAMMs) and functional linear mixed models (FLMMs) provide an elegant approach to tackle the data from multiple speakers. More specifically, we found oro-pharyngeal differences in the tongue configurations for low and mid nasal vowels: vowel track aperture was larger in the pharyngeal and smaller in the palatal region for the three non-high nasal vowels, providing evidence of a raised and more advanced tongue position of the nasal vowels. Even though this work is aimed at exploring the applicability of the methods, the outcomes already highlight interesting data for the dynamic characterization of EP nasal vowels.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Remote collaboration in maintenance contexts using augmented reality: insights from a participatory process",
        "doc_scopus_id": "85122856560",
        "doc_doi": "10.1007/s12008-021-00798-6",
        "doc_eid": "2-s2.0-85122856560",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Adaptive capabilities",
            "Daily tasks",
            "Industry sectors",
            "Participatory process",
            "Problem-solving",
            "Remote collaboration",
            "Remote maintenance",
            "Shared understanding",
            "Team members",
            "User study"
        ],
        "doc_abstract": "© 2021, The Author(s), under exclusive licence to Springer-Verlag France SAS, part of Springer Nature.Problem solving in Industry 4.0 often requires collaboration among remote team members, which face increased complexity on their daily tasks and require mechanisms with adaptive capabilities to share and combine knowledge. Augmented Reality (AR) is one of the most promising solutions, allowing taking advantage from seamless integration of virtual and real-world objects, which can be used to provide a shared understanding of the task and context. In this regard, most research works, so far, have been devoted to explore and evolve the necessary technology. However, it is now important to revisit the subject of remote collaboration in relation with AR to understand how much of the collaborative effort can already be supported and identify gaps that should inform further research. In line with this mindset, we adopted a user-centered approach with partners from the industry sector, including participatory design and a focus group with domain experts to probe how AR could provide solutions to support their collaborative efforts. We focused on using tangible artifacts in the form of storyboards to create a shared understanding with target users in remote collaboration. Afterwards, we identify a set of requirements, which we materialize through the design and creation of a collaborative prototype based on sharing of enhanced AR annotations. Finally, we present and discuss the results from a case study on a maintenance context, which provides interesting insights that can be applied to other remote settings, thus facilitating the digitization of the industry sector.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring the Age Effects on European Portuguese Vowel Production: An Ultrasound Study",
        "doc_scopus_id": "85123516451",
        "doc_doi": "10.3390/app12031396",
        "doc_eid": "2-s2.0-85123516451",
        "doc_date": "2022-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Process Chemistry and Technology",
                "area_abbreviation": "CENG",
                "area_code": "1508"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Fluid Flow and Transfer Processes",
                "area_abbreviation": "CENG",
                "area_code": "1507"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.For aging speech, there is limited knowledge regarding the articulatory adjustments underlying the acoustic findings observed in previous studies. In order to investigate the age-related articulatory differences in European Portuguese (EP) vowels, the present study analyzes the tongue configuration of the nine EP oral vowels (isolated context and pseudoword context) produced by 10 female speakers of two different age groups (young and old). From the tongue contours automatically segmented from the US images and manually revised, the parameters (tongue height and tongue advancement) were extracted. The results suggest that the tongue tends to be higher and more advanced for the older females compared to the younger ones for almost all vowels. Thus, the vowel articulatory space tends to be higher, advanced, and bigger with age. For older females, unlike younger females that presented a sharp reduction in the articulatory vowel space in disyllabic sequences, the vowel space tends to be more advanced for isolated vowels compared with vowels produced in disyllabic sequences. This study extends our pilot research by reporting articulatory data from more speakers based on an improved automatic method of tongue contours tracing, and it performs an inter-speaker comparison through the application of a novel normalization procedure.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A vision for contextualized evaluation of remote collaboration supported by AR",
        "doc_scopus_id": "85118343204",
        "doc_doi": "10.1016/j.cag.2021.10.009",
        "doc_eid": "2-s2.0-85118343204",
        "doc_date": "2022-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Characterization collaborative process",
            "Collaboration efforts",
            "Collaborative process",
            "Distributed teams",
            "Evaluation toolkit",
            "Guideline",
            "Knowledge retention",
            "Remote collaboration",
            "Team members",
            "User study"
        ],
        "doc_abstract": "© 2021 Elsevier LtdRemote collaboration using Augmented Reality (AR) has potential to support physically distributed team-members that need to achieve a common goal by increasing knowledge retention, improving understanding and awareness of the problem and its context. In this vein, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Thus, characterization and evaluation of the collaborative process is paramount, but a particularly challenging endeavor, due to the multitude of aspects that define the collaboration effort. In this context, the work presented here contributes with a critical analysis, discussing current evaluation efforts, identifying limitations and opportunities. Then, we outline a conceptual framework to support researchers in conducting evaluations in a more structured manner. To instrument this vision, an evaluation toolkit is proposed to support contextual data collection and analysis in such scenarios and obtain an additional perspective on selected dimensions of collaboration. We illustrate the usefulness and versatility of the toolkit through a case study on remote maintenance, comparing two distinct methods: sharing of video and AR-based annotations. Last, we discuss the results obtained, showing the proposed vision allows to have an additional level of insights to better understand what happened, eliciting a more complete characterization of the work effort.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2021-10-13 2021-10-13 2022-02-25 2022-02-25 2022-06-01T12:50:43 S0097-8493(21)00220-X S009784932100220X 10.1016/j.cag.2021.10.009 S300 S300.2 FULL-TEXT 2022-06-09T18:22:28.119019Z 0 0 20220201 20220228 2022 2021-10-13T16:42:28.006423Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst pubtype ref specialabst 0097-8493 00978493 true 102 102 C Volume 102 22 413 425 413 425 202202 February 2022 2022-02-01 2022-02-28 2022 Special Section on Adv Graphics+Interaction; Edited by Nuno Rodrigues, Daniel Mendes, Luís Paulo Santos, Kadi Bouatouch article fla © 2021 Elsevier Ltd. All rights reserved. AVISIONFORCONTEXTUALIZEDEVALUATIONREMOTECOLLABORATIONSUPPORTEDBYAR MARQUES B 1 Introduction 2 Background and challenges for evaluation of AR-based remote collaboration 3 A vision for contextualized collaborative AR evaluation 3.1 Evaluation purpose 3.2 Team and collaborative tasks 3.3 Experimental setup and design 3.4 Contextualized data gathering 3.5 Analysis and report 4 Toolkit for distributed evaluations using AR 5 User study on a remote maintenance scenario 5.1 Experimental setup 5.1.1 Video chat tool 5.1.2 AR-based annotation tool 5.2 Experimental design 5.3 Tasks 5.4 Measurements 5.5 Procedure 5.6 Participants 6 Results and discussion 6.1 Overall total time and task time 6.2 Overview of the collaborative process 6.3 Participants preferences and opinion 6.4 Final remarks 7 Conclusions and future work CRediT authorship contribution statement Acknowledgments References KIM 2020 529 538 K CONFERENCEVIRTUALREALITY3DUSERINTERFACES REDUCINGTASKLOADEMBODIEDINTELLIGENTVIRTUALASSISTANTFORIMPROVEDPERFORMANCEINCOLLABORATIVEDECISIONMAKING KIM 2018 2947 2962 K KIM 2018 569 607 S GERVASI 2020 841 865 R LUKOSCH 2015 515 525 S BILLINGHURST 2015 73 272 M ENS 2019 81 98 B WANG 2021 102071 P BOTTANI 2019 284 310 E WANG 2016 1 22 X LEE 2020 343 352 G IEEECONFERENCEVIRTUALREALITY3DUSERINTERFACES AUSERSTUDYVIEWSHARINGTECHNIQUESFORONETOMANYMIXEDREALITYCOLLABORATIONS LUDWIG 2021 119 167 T GUREVICH 2015 527 562 P KIM 2018 6034 6056 S KIM 2020 321 335 S NEALE 2004 112 121 D PROCEEDINGS2004ACMCONFERENCECOMPUTERSUPPORTEDCOOPERATIVEWORK EVALUATINGCOMPUTERSUPPORTEDCOOPERATIVEWORKMODELSFRAMEWORKS HAMADACHE 2009 206 221 K GROUPWAREDESIGNIMPLEMENTATIONUSE STRATEGIESTAXONOMYTAILORINGYOURCSCWEVALUATION ANTUNES 2014 146 169 P BELEN 2019 181 R MARQUES 2021 1 17 B BAI 2012 450 460 Z DEY 2018 37 A ANTON 2018 77 88 D PIUMSOMBOON 2018 2974 2982 T ARAUJO 2004 139 150 R MARQUES 2021 1 18 B MARQUES 2021 1 6 B INFORMATIONVISUALIZATIONIV VISUALLYEXPLORINGACOLLABORATIVEAUGMENTEDREALITYTAXONOMY IZARD 2007 260 280 C BARNUM 2010 C USABILITYTESTINGESSENTIALSREADYSETTEST GUTWIN 1999 243 281 C KIM 2018 6034 6056 S HUANG 2019 428 438 W PATEL 2012 1 26 H PIUMSOMBOON 2019 T MADEIRA 2021 83 89 T HUMANSYSTEMSENGINEERINGDESIGNIII EXPLORINGANNOTATIONSHANDTRACKINGINAUGMENTEDREALITYFORREMOTECOLLABORATION MARQUESX2022X413 MARQUESX2022X413X425 MARQUESX2022X413XB MARQUESX2022X413X425XB 2024-02-25T00:00:00.000Z 2024-02-25T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 0 2022-06-06T22:43:45.902Z University of Aveiro UA Universidade de Aveiro FCT SFRH/BD/143276/2019 UID/CEC/00127/2019 FCT Fundação para a Ciência e a Tecnologia European Regional Development Fund ERDF European Regional Development Fund We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [UID/CEC/00127/2019]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [ UID/CEC/00127/2019 ]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. item S0097-8493(21)00220-X S009784932100220X 10.1016/j.cag.2021.10.009 271576 2022-06-01T12:26:47.516259Z 2022-02-01 2022-02-28 true 3678202 MAIN 13 63615 849 656 IMAGE-WEB-PDF 1 gr3 43612 433 376 gr9 52635 306 376 gr1 28469 346 376 gr14 28871 413 376 gr12 42537 501 376 gr11 27866 474 376 gr7 39516 303 376 gr4 24720 434 376 gr15 11763 198 373 gr6 62725 542 376 ga1 true 16462 245 267 gr5 43801 320 376 gr2 78591 583 376 gr13 87515 441 376 gr8 45540 303 376 gr10 16187 255 369 gr3 6255 163 142 gr9 28164 164 201 gr1 4691 164 178 gr14 4530 164 149 gr12 4835 164 123 gr11 4118 164 130 gr7 20550 164 204 gr4 4480 164 142 gr15 3721 117 219 gr6 7321 163 113 ga1 true 5480 164 179 gr5 9004 164 192 gr2 9859 163 105 gr13 10706 163 139 gr8 22695 164 203 gr10 5254 151 219 gr3 341412 1919 1667 gr9 491740 1357 1667 gr1 192280 1532 1667 gr14 182688 1833 1667 gr12 307513 2220 1667 gr11 187668 2101 1667 gr7 367677 1342 1667 gr4 189805 1924 1667 gr15 78304 880 1654 gr6 482554 2402 1667 ga1 true 110211 1085 1183 gr5 291127 1420 1667 gr2 654708 2584 1667 gr13 594548 1957 1667 gr8 432457 1343 1667 gr10 110543 1129 1635 CAG 3442 S0097-8493(21)00220-X 10.1016/j.cag.2021.10.009 Elsevier Ltd Fig. 1 Conceptual framework for helping researchers evaluate of AR-remote collaboration in a more structured manner. Fig. 2 Scenario of remote collaboration using an AR-based tool instrumented with the CAPTURE toolkit: 1 — On-site technician requiring assistance; 2- Expert using AR to provide remote guidance; 3 - Researcher(s) following the evaluation process; 4- Distributed multi-user data gathering; 5- Contextual data collection based on existing dimensions of collaboration [37]; 6- Evaluation data storage; 7- Visualization dashboard for analysis of the collaborative process. Fig. 3 CAPTURE toolkit — example of pre-defined scenes associated with post-task measurements. Top — questionnaire regarding the collaboration process; Bottom — questionnaire regarding the collaborative tool. Fig. 4 CAPTURE toolkit — example of pre-defined scenes associated with selected dimensions of collaboration. Top — characteristics of the Team; Bottom — characteristics of the Task. Fig. 5 CAPTURE architecture. The toolkit can be integrated into a collaborative tool via visual editor. All data collected during collaboration is stored in a central server, which can be analyzed during post-task analysis through the visualization dashboard. Fig. 6 Overview of the CAPTURE toolkit assets: ready to use scene prefabs and editable scripts, which researchers may modify according to the aspects of collaboration being considered for the evaluation. Fig. 7 Video Chat tool for remote collaboration. Fig. 8 AR-based Annotation tool for remote collaboration. Fig. 9 Illustration of some of the completion stages associated with the maintenance tasks used in the study: 1- replace interconnected components; 2- plug and unplug some energy modules; 3- remove a specific sensor; 4- integrate new components into the equipment. Fig. 10 Total time and task time with the two conditions (in minutes). C1: video chat tool; C2: AR-based annotation tool. Fig. 11 Overview of the collaborative process outcomes for all teams during a scenario of remote maintenance, including all the selected measures collected: easy to share ideas properly, as well as communicate, level of attentional allocation, information understanding, mental effort, enjoyment, spatial presence. Top — C1: video chat tool; Bottom — C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1 — Low; 7 — High. Fig. 12 Collaborative process for the same team during remote maintenance using the two tools: Top — C1: video chat tool; Bottom — C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1 — Low; 7 — High. Fig. 13 Participants total reaction cards regarding the collaborative tools. C1: video chat tool; C2: AR-based annotation tool. A larger font size means that the word was selected by more participants (higher frequency). Red — negative meaning; gray — neutral meaning; green — positive meaning [41]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 14 Participants emotional state before (top) and after (bottom) the tasks for each condition. C1: video chat tool; C2: AR-based annotation tool. Fig. 15 Participants satisfaction towards the tools. C1: video chat tool; C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1- Low; 7- High. Special Section on Adv Graphics+Interaction A vision for contextualized evaluation of remote collaboration supported by AR Bernardo Marques Conceptualization Methodology Software Formal analysis Investigation Resources Writing – original draft Writing – review & editing Visualization Project administration Funding acquisition ⁎ Samuel Silva Conceptualization Methodology Writing – original draft Writing – review & editing Visualization António Teixeira Conceptualization Methodology Writing – original draft Writing – review & editing Visualization Paulo Dias Conceptualization Resources Writing – original draft Writing – review & editing Visualization Supervision Funding acquisition Beatriz Sousa Santos Conceptualization Writing – original draft Writing – review & editing Visualization Supervision Funding acquisition IEETA, DETI, Universidade de Aveiro, Portugal IEETA, DETI, Universidade de Aveiro Portugal DETI, IEETA, Universidade de Aveiro ⁎ Corresponding author. Remote collaboration using Augmented Reality (AR) has potential to support physically distributed team-members that need to achieve a common goal by increasing knowledge retention, improving understanding and awareness of the problem and its context. In this vein, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Thus, characterization and evaluation of the collaborative process is paramount, but a particularly challenging endeavor, due to the multitude of aspects that define the collaboration effort. In this context, the work presented here contributes with a critical analysis, discussing current evaluation efforts, identifying limitations and opportunities. Then, we outline a conceptual framework to support researchers in conducting evaluations in a more structured manner. To instrument this vision, an evaluation toolkit is proposed to support contextual data collection and analysis in such scenarios and obtain an additional perspective on selected dimensions of collaboration. We illustrate the usefulness and versatility of the toolkit through a case study on remote maintenance, comparing two distinct methods: sharing of video and AR-based annotations. Last, we discuss the results obtained, showing the proposed vision allows to have an additional level of insights to better understand what happened, eliciting a more complete characterization of the work effort. Graphical abstract Keywords Remote collaboration Augmented Reality Characterization collaborative process Guidelines Evaluation toolkit User study 1 Introduction Collaboration has the potential to achieve more effective solutions for challenging problems [1]. It has evolved from simple co-located situations to more complex remote scenarios, encompassing several team members with different experiences, expertise’s and multidisciplinary backgrounds. Remote collaboration can be described as the process of joint and interdependent activities between physically distributed collaborators performed to achieve a common goal [2–4]. This activity has become essential in many situations, as is the case of industrial, medical, and educational domains, among others [5,6]. To address such activities, remote solutions have been growing in terms of scale, complexity, and interdisciplinarity, entailing not only the mastery of multiple domains of knowledge, but also a strong level of proficiency in each [5,6]. Scenarios of remote collaboration imply that collaborators establish a joint effort to align and integrate their activities in a seamless manner. To address this, and overcome the fact team-members do not share a common space/world, there is an increasing interest in using Augmented Reality (AR) in this context [7–10]. Remote collaboration mediated by AR combines the advantages of virtual environments and the seamless integration with the real-world objects and other collaborators by overlying responsive computer-generated information on top of the real-world environment [2,11,12], allowing to establish a common ground, analogous to their understanding of the physical space, i.e., serve as a basis for situation mapping, allowing identification of issues, and making assumptions and beliefs visible [13–16]. These solutions can be used to empower workers that require knowledge from professionals unavailable on-site [17]. Remote experts can provide guidance, highlight specific areas of interest or share real-time spatial information [9,10,14,18] in the form of visual communication cues, e.g., pointers, annotations, hand gestures, among others [3,9,17,19–22]. These solutions can better support analysis, discussion and resolution of complex problems and situations, given their ability to enhance alertness, awareness, and understanding of the situation [23]. In the past decade, the community has been particularly active in this domain, concentrating efforts on creating the enabling technology to support the design and creation of an AR-based shared understanding. As the field matures and with the growing number of prototypes, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Therefore, evaluating such scenarios becomes an essential, but difficult endeavor [23–26], given the lack of methods and frameworks to guide the characterization of the collaborative process [9,27–29]. This is substantiated by Bai et al. reporting that ”it can be hard to isolate the factors that are specifically relevant to collaboration” [30]. In fact, this is further evident in remote scenarios, since the logistics associated with carrying out evaluations in these multifaceted contexts is even more demanding due to a significant number of variables that may affect the way teams collaborate [9,27]. Ratcliffe et al. report that ”remote settings introduce additional uncontrolled variables that need to be considered by researchers, such as potential unknown distractions, (...) participants and their motivation, and issues with remote environmental spaces” [31]. Also, Dey et al. suggest the existence of ”opportunities for increased user studies in collaboration” and the need for ”a wider range of evaluation methods” [32]. In this vein, Ens et al. emphasize that ” frameworks for describing groupware and MR systems are not sufficient to characterize how collaboration occurs through this new medium” [9]. Additionally, Ratcliffe et al. suggest that ”the infrastructure for collecting and storing this (mass) of XR data remotely is currently not fully implemented, and we are not aware of any end-to-end standardized framework” [31]. As such, conducting thorough evaluations is paramount to retrieve the necessary data for more comprehensive analysis that help provide a better perspective on the different factors of collaboration supported by AR. Hence, integration of proper characterization and evaluation methods, covering different contexts of use and tasks are of utmost importance. In this paper, we analyze existing evaluation efforts on remote collaboration using AR to provide a high-level overview. Motivated by the challenges reported, we present a conceptual framework for supporting researchers in obtaining an additional perspective on several dimensions of collaboration. Then, we propose the CAPTURE toolkit, a first instantiation towards the vision proposed, aiming to provide a strategy that monitors data concerning the level of collaboration, behavior and performance of each intervening party, individual and as a team, as well as contextual data. To illustrate the advantages of the framework, the toolkit usefulness and versatility are demonstrated through a case study in a remote maintenance scenario, comparing two distinct methods: sharing of video and AR-based annotations. Then, the results obtained are discussed, showing that the proposed vision allows having an additional level of insights to better understand what happened, eliciting a more complete characterization of the collaborative work effort. The remainder of this paper is organized as follows. Section 2 overviews existing evaluation efforts on remote collaboration mediated by AR. Section 3 proposes our conceptual framework for essential aspects that must be addressed. Section 4 describes the CAPTURE toolkit and Section 5 applies it through a user study on a remote maintenance scenario. Section 6 presents and discusses the main results. Finally, concluding remarks and future research opportunities are drawn in Section 7. 2 Background and challenges for evaluation of AR-based remote collaboration This section reports existing evaluation efforts addressing collaborative AR user studies. The goal is to understand how evaluation has been conducted in such scenarios, provide a high-level overview, and identify existing challenges and gaps. According to Merino et al. ”as MR/AR technologies become more mature, questions that involve human aspects will gain focus in MR/AR research. Consequently, we expect that future MR/AR papers will elaborate on human-centered evaluations that involve not only the analysis of user performance and user experience, but also the analysis of other scenarios, like understanding the role of MR/AR in working places and in communication and collaboration” [26]. However, there is no standard methodology for characterization and evaluation, specifically tailored to assess how remote collaboration occurs through AR. The literature shows that studies that evaluate their solutions rely on single-user methods, mainly focused on the comparison of technological aspects or interaction mechanisms, which are not the most adequate for multifaceted solutions that aim to support distributed team collaboration [2,9,25,27,29,32]. Also, most studies focus exclusively on the performance of one collaborator, i.e., on-site, or remote. This means evaluation usually does not consider interaction, and communication among team-members, and is not conducted in distributed scenarios, as should be the case to establish experimental conditions closer to real scenarios, Likewise, focus is given to the technological aspects of the solution being used, as well as to quantifying the effectiveness in completing the tasks, which mostly lack difficulty, diversity and ecological validity [2,3,7,29,30,32]. Moreover, the majority of studies are formal, conducted in laboratories, collecting objective and subjective data at the end of the tasks through standard practices with fixed answers like scale-based questionnaires (e.g., System Usability Scale (SUS), NASA Task Load Index (TLX), among others) or direct observation [7,9,19,27,32–34]. Adding to these data, only a reduced set of studies include measurements collected during the collaborative process (e.g., task duration and error/accuracy), as well as explicit communication (e.g., spoken messages or gestural cues), ease of collaboration and others [26,30]. While this is the case, the collection of more contextual and behavioral data is often not considered or hindered due to the complexity it entails regarding acquisition, processing and analysis, and more important, the lack of guidelines to inform researchers on what dimensions of collaboration should be collected and how. Therefore, current frameworks are not tailored to characterize how collaboration mediated by AR occurs [9,25,28,31], falling short to retrieve the necessary amount of data for more comprehensive analysis. As a consequence, without the appropriate methods, the research community does not accumulate enough experience to improve the work effort [3,9,25,26,28–30,32,35]. Thus, as the field of remote collaboration using AR matures, evaluation needs to move beyond a simple assessment of how the technology works, as it becomes essential to understand different aspects of collaboration itself, including how teams work together, how communication happens, how AR is used to create a common ground, among others. This should provide a richer output of the evaluation stage, balancing the design against requirements and leading to a more informed refinement of the context of use and system features, e.g., in line with the life-cycle for Human-Centered Design (HCD) described in the principles and activities associated with the [ISO 9241-210]. 1 1 iso.org/standard/77520.html. Given the challenges and constraints involved in evaluating the way collaboration occurs through AR, we argue it is paramount to address a set of important topics, namely: 1- conduct more collaborative-centric evaluations, i.e., move beyond usability testing, which fails to obtain a more comprehensive understanding of the work effort. Equally important, 2- develop evaluation strategies including contextual data collection and visualization, i.e., collect a richer data set to better understand how AR contributes to the collaborative process, in order to shape more effective collaboration. 3 A vision for contextualized collaborative AR evaluation The area being addressed in this work is part of a complex phenomenon. To allow answering existing problems, it is necessary to systematize knowledge and perspectives, so that it can be applied transversely. For this, it is necessary the creation of evaluation frameworks, i.e., capitalize on the hierarchies and dimensions of collaboration from ontologies and taxonomies, as well as the development of tools that allow contextualizing the use of collaborative solutions. Taking into account the challenges and needs identified in the previous section, Fig. 1 structurally presents an evaluation framework of the collaborative process when using a given tool, with a proposal of several levels of information that must be considered for contextualization, derived through a HCD methodology. In this effort, we argue that the evaluation process must be addressed by the research community, namely the definition of the evaluation purpose, as well as the team characteristics and the details of the collaborative tasks. Also, carefully establish the experimental setup and design. Equally important, explore contextualized data gathering and analysis, which requires the creation of novel tools. This last, being the aspect this work further contributes. Next, we elaborate on these with more detail. 3.1 Evaluation purpose To begin, the scope must be defined, taking into account existing dimensions of collaboration to clarify what will be evaluated, so that relevant research questions are formulated in the design phase and answered in the evaluation analysis [24]. 3.2 Team and collaborative tasks Also important, determine the team-members’ characteristics, i.e., role structure, coupling level, life-span, technology literacy and multidisciplinarity. In this context, participants with different ages, perspectives, motivations, and multidisciplinary background should be considered, which might lead to more relevant insights. Moreover, understanding of VR/AR, as well as remote tools is a benefit for the adaptation, thus removing the ’wow factor’ that makes participants feel excitement or admiration towards such technologies. Besides, participants should only perform one role, i.e., on-site or remote, so that they are only exposed to a set of tasks, concerns and responsibilities. Furthermore, the collaborative tasks goals must be clearly established including which team-members will be accountable for achieving each completion stage. It is also important to consider if the tasks are performed indoor, outdoor, or mixed between the two; A balance must be kept between task complexity and duration. Tasks must be complex and long enough to encourage interaction through AR. However, longer tasks may cause fatigue or boredom, affecting the evaluation outcomes. Equally relevant, tasks can introduce deliberated drawbacks, i.e., incorrect, contradictory, vague or missing information, to force more complex situations and elicit collaboration. 3.3 Experimental setup and design Establish the experimental setup and design are equally key. When considering prototypes, evaluation under laboratory settings should be used. Afterwards, when considering more mature solutions, evaluation should be made in the field, with real stakeholders and domain experts, moving beyond typical laboratory settings to increase the ecological validity of the evaluations. Regarding the environment, two separated rooms in the same/ different building(s) should be used. Otherwise, participants must be separated by some kind of physical barrier when in the same room. Furthermore, an adaptation period must be provided so that participants can explore the technology possibilities before the tasks, individually and as a team. Besides, a proper amount of time must be defined for other aspects, e.g., presentation of the study, pre- and post-task questionnaires, team interview, and others. 3.4 Contextualized data gathering As well observed by Merino et al. [26], future works on Mixed and Augmented Reality (MR/AR) will elaborate on human-centered evaluations involving not only the analysis of user experience and performance, but also understanding the role of such technologies in working places, in communication and in collaboration. In this scope, contextual information helps inform the conditions in which the collaboration took place. It can also be used for understanding interaction and communication changes, namely if the surroundings affected the way teams collaborate, in such a way that they needed to adapt it. Also, it helps portrait the conditions in which team-members performed a given action, received information or requested assistance, which can be used to assess uncommon situations or identify patterns that can lead to new understanding of a given artifact, as well as identify new research opportunities. Without comprehending contextual information, it becomes difficult to assess important variables related to the collaborative process, which means the findings reported may be misleading or of limited value. Hence, these aspects have an important impact on how the studies must be prepared and how they were conducted, influencing situation understanding, team-members communication, performance, and usage of AR. Literature shows that a better evaluation process can be supported by improved data collection and data visualization tools [35,36]. In particular, the following factors are crucial and must be taken in account to better understand the real impact of each aspect in the collaborative effort: team, tasks, context and AR-based tool [28]. Through these, a wide range of information is provided when performing judgment over the results and establishing conclusions. Therefore, data collection while team-members collaborate, considering different forms of measurement according to the evaluation goals is paramount and should include: • pre-task measures like demographic questionnaires (e.g., age, gender, occupation, years of experience, etc.), information on participants background: if they knew each other, previous experience with VR/AR technologies and remote tools, among other aspects; • runtime measures may comprise: – performance metrics including overall duration of specific events, number and type of errors; number and type of interactions; frequency of using each feature of the tool; screenshots of the enhanced content; – behavior metrics including conversational analysis (e.g., frequency of conversational turns, number of questions or interruptions, and dialog length, duration of overlapping speech); physical movement around the environment; number of hand gestures; physiological variables and emotions; eye gaze; – collaboration metrics including the level of effectiveness; perception; interest; engagement; awareness; togetherness; mental stress; – researchers may collect audio (or video) and register interesting events including the type (e.g., guide, request, express, propose) and frequency of communication (e.g., never, sometimes, often, continuously), if the goals were accomplished, difficulties detected, if the participants requested assistance and how many types, among other relevant aspects. • post-task measures can encompass: – register usability towards the tools(s) used; – record collaboration metrics including the level of effectiveness; perception; interest; engagement; awareness; togetherness; mental stress, etc; – collect participants reactions, opinions and preferences through semi-structured interviews. 3.5 Analysis and report The use of more contextualized approaches will provide ground to improve how research is analyzed and reported. Hence, increasing the awareness of researchers about the different dimensions of collaboration and the need to improve how the nuances associated to the collaborative effort are described. In turn, a more systematic characterization can lead to a community setting that enables easier communication, understanding, reflection, comparison and refining, building on existing research while fostering harmonization of perspectives for the field. In this context, some noticeable recommendations are: • researchers can profit from the outcomes generated to improve the level of detail provided in their reports; • the collaborative context needs to be widely described, allowing the creation of a better understanding of the surrounding conditions, including relations between individuals, their interconnection as a team, how AR was used, the characteristics of the environment, and others; • the outcomes can help identify limitations and promising functionalities regarding AR, providing opportunities for future work in a technical level; • the insights obtained may also lead to improvements in individual behavior and team collaboration in specific procedures and tasks over longer periods of time. 4 Toolkit for distributed evaluations using AR Given the challenges in evaluating the way remote collaboration occurs, the absence of frameworks and tools, this section describes CAPTURE - Contextual dAta Platform for remoTe aUgmented Reality Evaluation, a first instantiation towards addressing the vision previously described, in particular the need to include more contextual data in the evaluation of the collaborative process (Fig. 2), following the conceptual model [28]. To inform the conceptualization/development, we conducted brainstorm sessions with domain experts (academics, including faculty members and researchers) sharing several years of expertise in HCI, VR/AR, Visualization and remote collaboration, who co-authored multiple publications, and projects on these subjects. Hence, the toolkit must support: • data gathering at distributed locations in synchronous and asynchronous manner; • explicit input on different dimensions of collaboration, following a taxonomy for Collaborative AR [38,39] and an evaluation ontology for remote scenarios [37]; • data collection regarding team interaction, custom logging and registration of interesting events according to the selected scenarios of remote collaboration; • easy instrumentation into remote tools by providing ready to use scripts and prefabs for non-experts in programming, i.e., each process can be configured via visual editors; • modularity to ensure adaptation to different goals; • data storage and aggregation via a centralized server; • post-task analysis through a visualization dashboard. To elaborate, for team-members, the CAPTURE toolkit provides native off-the-shelf modules to support explicit input and data gathering regarding (Figs. 2–4): • individual and team profile: demographic data, knowledge of other collaborators, participants background, emotional state [40], experience with AR and remote tools; • collaborative context: details on the task and the environment, like the number of completion stages, resources available or the amount of persons, movement and noise in the surrounding space; • list of events: task duration, augmented content shared and received, and other relevant occurrences; • pre-defined measures: characteristics associated to the collaborative process, including, but not limited to, easy to communicate or express ideas and the level of spatial presence, enjoyment, mental effort, information understanding, attention allocation or others (Fig. 3 - top). Also, the Microsoft reaction card methodology [41] to have a grasp on team-members reaction towards the tool used for shared understanding (Fig. 3 - bottom); • interaction with the collaborative tool: duration of the collaborative process and specific events, e.g., when creation of content is started or completed, number and type of interactions, frequency of using each feature, as well as captures of the augmented instructions being shared. Regarding pre-defined measures, the aspects of collaboration proposed are the result of carefully survey existing literature to create a list of important topics facing the lack of methodologies and frameworks. This list was presented to the experts, who had an important role in selecting, analyzing and filtering said topics of collaboration by voting about the ones they considered being more relevant. To elaborate, we took inspiration from the questionnaires used by [3,42–46], as well as the works by [11,19,21,22,47–50]. Nevertheless, other aspects of collaboration can be considered according to the evaluation scope due to the inherent flexibility provided by the CAPTURE toolkit implementation, as described below. As for the researcher(s), the toolkit provides native off-the-shelf modules to support explicit input regarding (Fig. 2 - 5): • Study: area of application, research context and study type; • Time: synchronicity, duration and predictability; • Team: distribution, role structure, size, life-span, turnover, multidisciplinarity, technology usage, homogeneity of abilities, and knowledge of others (Fig. 4 - top); • Task: scope and type of task, interdependence, amount of information and movement required to fulfill the task, number of completion stages, resources necessary to achieve the goal (Fig. 4 - bottom); • User Actuation: capacity to passive-view, interact/explore, share/create, as well as level of symmetry; • Communication: structure, mode, intent, frequency and duration; • Environment: amount of noise, level of brightness, number of persons in the environment, weather conditions and resources available; • Notes: interesting events, notes, comments or difficulties, as well as if the goals were achieved and the amount of physical movement conducted by the team-members. At the system level, CAPTURE consists of a Unity Package that can easily be added to existing collaborative solutions in Unity. All data gathered from the different team-members and researcher(s) during collaboration sessions is stored in a central server for post-evaluation analysis through a visualization dashboard (Fig. 5), which allows reviewing the work effort of a particular team or set of teams, as well as compare different tools, if that is the evaluation scope [51]. The modules of the proposed toolkit can be integrated into existing remote tools via visual editors, i.e., with minimal need for programming skills (Fig. 6). It is possible to drag and drop ready to use prefabs and editable scripts into Unity 3D projects, which can be modified according to the evaluation scope in the inspector module. Fig. 6 illustrates the example of the collaborative process script, which researchers can manually edit (set the number of elements, add relevant aspects of collaboration to be assessed, etc.) according to the evaluation scope. This dynamic approach allows researchers to re-use scripts over different evaluation sessions according to the collaborative effort being considered. For development, Unity 3D was used based on C# scripts. Communication between each instance is performed over Wi-Fi through calls to a PHP server. In short, the field needs to have more contextualized evaluation strategies, allowing to learn more regarding how technology address the collaborative process. All of this can support an effort towards systematized data, which may support the proposal of guidelines in the future, resulting from the experience and knowledge accumulated through the analysis from multiple research teams and different technology approaches with contextualized information. This effort will allow to use these recommendations to jump-start the quality of current and novel solutions right from the very beginning of its conceptualization, which have already been proven useful in remote scenarios. 5 User study on a remote maintenance scenario A user study was conducted to compare the collaborative process of distributed teams using two distinct tools when instrumented with CAPTURE: Video Chat and AR-based Annotations. These were proposed following a user-centered approach with partners from the industry sector to probe how AR could provide solutions to support their collaborative needs. 5.1 Experimental setup To create a common ground between distributed team- members, two distinct methods were provided: a video chat tool and an AR-based annotation tool. Next, a brief description of the main features of each tool is provided. To clarify, the hardware used was the same for both methods, only the characteristics of the tool changed. Also, both tools were developed using the Unity 3D game engine, based on C# scripts. Communication was provided over Wi-Fi through WebRTC calls to a dedicated server. To place the augmented content in the real-world environment, we used the Vuforia library. 5.1.1 Video chat tool The first method uses video chat features to provide support (Fig. 7). On-site participants can point a handheld device to the situation context, which is shared though live video stream with the remote expert. In this context, the face of the expert is visible at all times, while the on-site participant may change between showing the task context or his face using the back and front cameras of the device. Besides, team-members can share text messages using the chat to ensure important messages are kept visible. Using these features, team-members may communicate and discuss the content being captured to express the main difficulties, identify areas of interest or the remote expert to inform where to act and what to do. 5.1.2 AR-based annotation tool The second method uses AR-based annotations as additional layers of information (Fig. 8). On-site participants can point a handheld device to capture the situation context. Using audio communication and annotation features like drawing, placing pre-defined shapes or notes, as well as sorting annotations, the participant can edit the capture to illustrate difficulties, identify specific areas of interest or indicate questions. Then, the capture is sent to the remote expert to suggest instructions accordingly i.e., inform where to act, and what to do, using similar annotation features. Afterwards, the on-site participant receives the annotations. The handheld device can be placed on top of a surface to follow the instructions in a hands-free setting. At any time, it can be picked up to perform an augmentation of the annotations, by re-aligning with the real world. 5.2 Experimental design A within-group experimental design was used. The null hypothesis (H0) considered was that the two experimental conditions are equally usable and acceptable to conduct the selected maintenance tasks. The independent variable was the information display method provided during the collaborative process, with two levels corresponding to the experimental conditions: C1 — Video Chat and C2 — AR-based Annotations. For both experimental conditions, the tools used provided a similar level of user actuation for both team-members, having identical features to view (C1 and C2), create, share and interact with augmented content (C2). Performance measures and participants’ opinion were the dependent variables. Participants’ demographic data, as well as previous experience with AR and collaborative tools were registered as secondary variables. 5.3 Tasks We focused on a case study where an on-site participant using a handheld device had to perform a maintenance procedure while being assisted from a remote expert using a computer. The tasks require accomplishing the following steps (Fig. 9): 1- replace interconnected components, 2- plug and unplug some energy modules, 3- remove a specific sensor, as well as 4- integrate new components into the equipment. For each condition, different tasks were used to minimize bias, i.e., learning effect. Nevertheless, we defined these tasks based on feedback from our industry partners regarding their usual work activities and needs, while ensuring a similar level of difficulty and resources. Each task was a defined-problem with 4 completion stages, forcing team-members to communicate in a continuous way while acting alternately (reciprocal interdependence) in an indoor environment with controlled illumination conditions and reduced noise. Besides the participants and researchers, no other individuals were present. The on-site participant needed to use different hand tools to perform the procedures, although low physical movement was required. 5.4 Measurements All data was collected through the CAPTURE toolkit for all conditions, including standard measures found in literature like task performance based on the overall total time, i.e., time needed to complete the tasks, answer to questionnaires and participation in a brief interview, as well as task time, i.e., time required for successfully fulfill the task in a collaborative manner. Besides, novel measures, taking advantage of the toolkit off-the-shelf modules, i.e., information on selected dimensions of collaboration (e.g., time, team; task; user actuation, communication, environment); the overview of the collaborative process (e.g., easy to communicate or express ideas, level of spatial presence, enjoyment, mental effort, information understanding and attention allocation) at the end of the tasks; participants emotional state, before and after the task fulfillment; participants preferences and opinion, also at the end. Hence, the toolkit was integrated into an existing video chat tool, as well as an AR-based tool [52] using stabilized annotations, following prior work with partners from the Industry sector. 5.5 Procedure Participants were instructed on the experimental setup, the tasks and gave their informed consent. Then, they were introduced to both tools and a time for adaptation was provided. Participants would act as on-site technicians with condition C1 and then C2, always in this order, while a researcher was the remote counterpart to ensure the instructions were correctly transmitted. We used this approach to facilitate collaboration, as having participants also act as the remote counterpart would add an additional level of complexity, which we believe was not necessary. Since this role was ensured by one of the researchers, we recognize that it is not the same as having a participant, but still allows to have a granular view of the work effort, since not all collaborative processes are created equal. Hence, the researcher also followed the same procedure during the evaluation. We argue that the data collected from this role convey a variability in the way collaboration occurred and in what works or not, depending on the team-members, which demonstrates the ability of the measures used to have some granularity in the evaluation of how the collaborative process took place. Participants started with a demographic questionnaire. In the next stage, they completed the maintenance tasks while observed by a researcher who assisted them if necessary, and registered any relevant event. Immediately after completing the tasks using the conditions, participants answered a post-study questionnaire regarding the collaborative process, as well as their preferences towards the tool used. Then, a small interview was conducted to understand participants’ opinion regarding their collaboration with each condition. The data collection was conducted under the guidelines of the Declaration of Helsinki. Also, all measures were followed to ensure a COVID-19 safe environment during each session of the user study. 5.6 Participants We recruited 26 participants (9 female - 34.7%), whose ages ranged from 20 to 63 years old (M = 33.1, SD = 11.7). Participants had various professions, e.g., Master and Ph.D. students, Researchers and Faculty members from different fields, as well as Software Engineers, Front-End Developers and an Assembly Line Operator. With respect to individual and team profile, 14 participants had prior experience with AR and 24 with collaborative tools. With the exception of 1 team, all collaborators had knowledge of each other prior to the study. 6 Results and discussion This section presents and discusses the main results obtained from the analysis of the data collected through CAPTURE. 6.1 Overall total time and task time As for the total duration, sessions lasted 32 min on average (SD = 3.10) using condition C1 and 28 min on average (SD = 3.03) using condition C2 (Fig. 10). Regarding task duration, it lasted 16 min on average (SD = 2.68) using condition C1 and 12 min on average (SD = 2.66) using condition C2. Therefore, participants were quicker on average to perform the tasks when using condition C2, despite having a higher data variability when compared to condition C1. 6.2 Overview of the collaborative process Regarding condition C1, participants rated the collaborative process (Likert-type scale: 1 — Low; 7 — High) as following (Fig. 11 — top): express ideas (median = 4.5), attentional allocation (median = 4), information understanding (median = 5), mental effort (median = 5), enjoyment (median = 4), communication (median = 5), spatial presence (median = 5.5). As for condition C2, participants rated the collaborative process as following (Fig. 11 - bottom): express ideas (median = 6), attentional allocation (median = 7), information understanding (median = 7), mental effort (median = 2), enjoyment (median = 6), communication (median = 6), spatial presence (median = 5). Hence, it is possible to understand that for the majority of aspects of collaboration, i.e., easy to share ideas properly, level of attention allocation, level of information understanding, level of enjoyment and easy to communicate, condition C2 was rated higher by the participants. Regarding the level of mental effort, participants rated higher condition C1, possibly due to the diminished level of attentional allocation this condition had, which lead to some communication arguing in order to understand where to perform some activities. Therefore, these results suggest that the AR-based annotation tool was better in such aspects of collaboration when compared to the video alternative. In contrast, for condition C1 the level of spatial presence was higher. This might be associated to the fact that this condition supported live video sharing between team-members, which may have an impact on participants feeling of togetherness with their collaborative counterparts, since it was possible to see the remote expert at all times during the task duration. On the other side, condition C2 provided stabilized AR-based annotations on top of captures/images of the task context. This condition did not allow to see the remote expert during the task procedures, which may have affected participants reaction towards the level of spatial presence, although not with any major difference. In this context, a smaller data variability can also be observed for easy to share ideas properly, level of information understanding, level of mental effort, easy to communicate and level of spatial presence, when analyzing the box plots of condition C1 and C2, as illustrated by Fig. 11. Through the visualization dashboard of the CAPTURE toolkit, it is possible to analyze the collaborative process at the end of an evaluation session for a specific team, or set of different teams. In particular, it is possible to analyze the aspects of collaboration obtained from the use of different tools for the elements of the same team, as explored in this study, which is illustrated in Fig. 12, through a random selection. Naturally, following the results presented above, when using condition C2, the team had a better collaborative performance when compared to the results of condition C1. Nevertheless, by analyzing the elements of each team individually, such type of visualization allows to identify aspects of collaboration that could be useful to improve over time, or that may be relevant to update in the collaborative tool being used. For example, when using condition C2, the on-site participant rated the level of spatial presence lower. This fact may suggest that in order to improve the feeling of togetherness, the AR-based annotation tool might benefit from including video sharing in its features. 6.3 Participants preferences and opinion With respect to participants experience with the tools, 44 reaction cards were selected to characterize condition C1, including 5 neutral, 9 negative and 30 with positive meaning. Likewise, 46 were selected to characterize condition C2, including 3 neutral, 1 negative and 40 with positive meaning (Fig. 13) . The following top 10 reaction cards represent participants most selected expressions to characterize each condition: C1 — accessible, collaborative, helpful, flexible, simplistic, familiar, usable, unrefined, expected and time-consuming; C2 — helpful, empowering, collaborative, appealing, easy-to-use, engaging, flexible, novel, innovative and advanced. However, when analyzing participants emotional state, collected before and after the tasks, a clearer perspective is attained. To elaborate, regarding condition C1, participants emotional state before the study varied among joy (11 out of 26), surprise (3 out of 26), excitement (8 out of 26) and contempt (4 out of 26) (Fig. 14 - top). Then, after the study, it varied among joy (7 out of 26), surprise (1 out of 26), excitement (1 out of 26) and contempt (17 out of 26) (Fig. 14 - top). As for condition C2, participants emotional state before the study varied among joy (12 out of 26), surprise (3 out of 26), excitement (7 out of 26) and contempt (4 out of 26) (Fig. 14 - bottom). Then, after the study, it varied among joy (6 out of 26), surprise (4 out of 26) and excitement (6 out of 26) (Fig. 14 - bottom). Hence, it is possible to verify that for condition C1, there was a decrease in the number of participants feeling joy, surprise and excitement at the end of the study, which lead to a significant rise associated to the emotional state of contempt. Contrarily, regarding condition C2, there were no occurrences of contempt, while joy and surprise had higher number of participants expressing those feelings. As for excitement, although the number of participants that reported such feeling is lower, it is very close to the values reported at the beginning of the study. As such, condition C2 presents significant higher values for emotions correlated with positive connotation, e.g., joy, surprise and excitement when compared to condition C1, which only presents a higher value for contempt (neutral connotation). In addition, Fig. 15 presents participants satisfaction regarding the collaborative tools used through a box plot representation, which illustrates clearly that condition C2 was preferred when compared to condition C1, following the analysis statement of participants emotional state. The interviews conducted at the end of the study also emphasize that the majority of participants preferred condition C2, since it enabled seeing non-verbal cues aligned with the task context, which they mentioned contributed to express themselves better through the augmented features, while also having a greater perception of where to perform a given action. Next, some comments by the participants are presented to provide additional context to the statement previously made: • regarding the level of attentional allocation and information understanding with condition C1, one participant emphasized the following: ”although the video tool is more familiar and quicker to start collaborate, when I needed to express myself about the equipment components or the tools I should use, that’s when I started noticing the lack of support. This lead me to repeat the same ideas in different ways to properly explain the desired goal, and the same also happened to my colleague”; • as for the level of mental effort with condition C1, another participant outlined that ”besides the use of voice, the absence of support to highlight an area of interest or express myself when using the video tool makes me prefer the use of AR-based annotations, in particular for more complex procedures, even though it was a novelty to me and I needed to learn and adapt to it”; • concerning easy to share ideas properly with condition C2, a different participant reported that “the use of AR-based annotations allowed me to interact more naturally, while also better comprehend where to perform a given action”. In regards to attentional allocation with condition C2, the same participant commented that “having the handheld device displaying the annotations near the equipment, allowed me to perform the maintenance tasks easily when compared to the video, since in this last there was no content besides the text chat I could use to remember what to do, or to confirm my actions”; • with respect to the level of mental effort and spatial presence, an additional participant mentioned the following regarding condition C2: “since I’m familiarized with remote video tools in my daily activities, I was expecting that the absence of video would affect my collaboration with the remote expert. Nevertheless, since the AR-based tool focused more on the task itself, I was engaged in such a way, that not viewing the expert did not affect me at all”. Regarding additional comments/suggestions, some participants (7 out of 26) emphasize condition C2 could help create documentation for scenarios where identical tasks may occur. Actually, the AR-based tool already supports revisiting existing annotations, a feature identified as useful by industry partners during the design of a remote maintenance support platform [52]. Nevertheless, for the case study reported, such feature was not made available, since the tasks used did not imply repeating particular activities. Another topic raised by some participants (18 out of 26) was the possible inclusion of Head Mounted Displays (HMDs), which they consider may further enhance their performance, since it supports a hands-free setting. Likewise, the AR-based tool used already supports HMD, as described in prior work [53]. Since our goal was not to compare different set-ups, we decided not to include such type of device at this moment. In addition, 4 out of 26 participants referred to possible limitations regarding the use of mobile devices as means to answer a questionnaire, since they were used to doing so on computers. They reported that for questions using drop-down menus and multiple-choice options it was easy to select the desired answer. As for the ones requiring text entry, the process could be slower and tiring. Yet, they understood the usefulness/relevance due to the fact of monitoring real-life scenarios. For example, CAPTURE is ready to be used in industry contexts, in which most technicians may find themselves without a computer. Furthermore, having these target users answering relevant questions after the tasks provides more useful insights than having them filling the questionnaires at the end of a workday on a more suitable device for writing. In this vein, we argue a compromise was required and that the solution provided takes these constraints into consideration. Nevertheless, this also opens new opportunities to propose novel forms of providing input in such scenarios. Furthermore, following the possible inclusion of HMDs in such scenarios and their similar (or even worst) capacity to answer questionnaires, this is also an open topic. Although it is possible to create text with such devices, e.g., hand interaction, literature shows it may not be the best approach. An alternative may be to use a keyboard linked to the HMD device just for answering the existing questionnaires, or perhaps, support voice/sound data collection, and later convert that into text, either via automatic or semi-automatic means. Nevertheless, more than ease of filling out questionnaires, what really matters is evaluating and monitoring collaboration in the best way possible. Therefore, as mentioned, on-the-fly feedback is essential. The choice of the most adequate input form for collecting information from the questionnaires may depend on the hardware available/being used, or on the person designing the study. Overall, the idea is that the toolkit is flexible enough to support all these options. Last, a reduced number of participants (5 out of 26) suggested viewing the remote expert, not as a basic feature, but as an option for specific cases which may help increase empathy and trust during the collaboration process. 6.4 Final remarks To summarize the added value of our proposal, and how it compares to existing approaches, the conceptual framework instantiated through the CAPTURE toolkit allows to retrieve additional amounts of contextual data, as well as selected aspects of collaboration according to the evaluation scope, (usually ignored in existing evaluations found in literature), for more comprehensive analysis using the visualization dashboard. Another aspect that must be emphasized, is the capacity to adapt to the available data collection instruments. Although self-report was used to gather the emotional response, CAPTURE can adapt to support the inclusion of external sensors (e.g., biomedical devices), if necessary for different scenarios. With all things considered, it is possible to better understand the phenomenon, i.e., recognize when selected aspects of collaboration affect the work effort. By having these insights, it is possible to more easily identify key issues that need to be tackled to ensure a proper shared understanding is attained by distributed team-members in future sections of remote collaboration. By doing so, the research community can evolve from simple evaluations on how technology works, to more complex evaluations aimed to capture a better perspective on the different factors of collaboration supported by AR, which may lead to a more effective collaborative process over time. Hence, we have shown that a better characterization of the collaborative process can be successfully used to provide an additional perspective on the nuances of remote collaboration mediated by AR, which without contextual data would not be possible. Altogether, due to the flexibility and range of the proposed conceptual model, the instrumentation through the CAPTURE toolkit establishes itself as a general-purpose evaluation approach, providing data that otherwise would be difficult to obtain and analyze. While we must be prudent with generalizing our findings, we expect our insights to be valuable for future reproduction in other domains beside maintenance context. To finish, the continuous observation of contextual data in other tools and with other users may allow, in the future, to create guidelines, supported by experimental data, which can guide the initial development of novel collaborative solutions. 7 Conclusions and future work As a contribution, a critical analysis on collaborative user studies mediated by AR is presented, showing that most studies rely on single-user methods, not adapted to collaborative scenarios and that existing frameworks are not well suited to characterize how collaboration occurs. Motivated by these, we presented a conceptual framework to support researchers in designing better evaluations based on retrieving contextualized data for more comprehensive analysis of the collaborative process. To instantiate this framework, the CAPTURE toolkit was proposed to assist with more user centric evaluations, allowing to easily analyze the collaborative process of a particular team or comparison between a set of teams or different tools. During the analysis of the results obtained, it was possible to realize that the contextual data allowed us to understand participants ease to communicate and to share ideas, and the level of attention allocation, spatial presence or others. Also, measure emotional state, and reaction towards the tools used. In this vein, participants felt AR supports more natural interaction, which contributes to increase empathy, interest and collaboration. By having a grasp on these aspects, typically not reported in the literature, but which are very informative/valuable to understand where the focus of the work, it is possible to better define how research should progress and how the tools can evolve. Hence, conduct comparative analysis of distributed teams may benefit researchers in better understanding the collaborative phenomenon, when compared to how its being currently reported, designing novel methods and improve the collaborative effort. This reinforces, once again, the need to evolve and make these experiences more contextualized and better reported, so that the research community can move into a phase of producing guidelines for remote scenarios supported by AR. Later, we intend to support data/voice collection, both during the collaborative process among the remote team members, and as an additional data input during the post-task assessment. We envision it may be relevant for researchers having metrics that can be automatically calculated and brought for analysis through an updated version of the visualization dashboard, e.g., characteristics of the dialog, during synchronous collaboration (e.g., number of questions, interruptions, occurrences of specific words). One possible way being considered is supporting some form of synchronization so that all user-related events are synchronized with video/voice streams captured in the study. Furthermore, we plan to share the toolkit with the research community, which may elicit newer data gathering/visualization requirements. Also, conduct field studies with experts from the industry sector to demonstrate the framework use in real scenarios. Last, pursue the creation of guidelines to elicit more complete evaluations in such scenarios. CRediT authorship contribution statement Bernardo Marques: Conceptualization, Methodology, Software, Formal analysis, Investigation, Resources, Writing – original draft, Writing – review & editing, Visualization, Project administration, Funding acquisition. Samuel Silva: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Visualization. António Teixeira: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Visualization. Paulo Dias: Conceptualization, Resources, Writing – original draft, Writing – review & editing, Visualization, Supervision, Funding acquisition. Beatriz Sousa Santos: Conceptualization, Writing – original draft, Writing – review & editing, Visualization, Supervision, Funding acquisition. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [UID/CEC/00127/2019]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. References [1] Kim K. de Melo C.M. Norouzi N. Bruder G. Welch G.F. Reducing task load with an embodied intelligent virtual assistant for improved performance in collaborative decision making Conference on Virtual Reality and 3D User Interfaces 2020 IEEE VR 529 538 Kim [ K], de Melo [ CM], Norouzi [ N], Bruder [ G], Welch [ GF]. Reducing task load with an embodied intelligent virtual assistant for improved performance in collaborative decision making. Conference on Virtual Reality and 3D User Interfaces, IEEE VR 2020:529–538. [2] Kim K. Billinghurst M. Bruder G. Duh H.B. Welch G.F. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008–2017) IEEE Trans Vis Comput Graphics 24 2018 2947 2962 Kim [ K], Billinghurst [ M], Bruder [ G], Duh [ HB], Welch [ GF]. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017). IEEE Transactions on Visualization and Computer Graphics 2018a;24:2947–2962. [3] Kim S. Billinghurst M. Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration Comput Support Coop Work CSCW 27 3–6 2018 569 607 Kim [ S], Billinghurst [ M], Lee [ G]. The effect of collaboration styles and view independence on video-mediated remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal 2018b;27(3-6):569–607. [4] Gervasi R. Mastrogiacomo L. Franceschini F. A conceptual framework to evaluate human-robot collaboration Int J Adv Manuf Technol 108 3 2020 841 865 Gervasi [ R], Mastrogiacomo [ L], Franceschini [ F]. A conceptual framework to evaluate human-robot collaboration. The International Journal of Advanced Manufacturing Technology 2020;108(3):841–865. [5] Lukosch S. Billinghurst M. Alem L. Kiyokawa K. Collaboration in augmented reality Comput Support Coop Work CSCW 24 2015 515 525 Lukosch [ S], Billinghurst [ M], Alem [ L], Kiyokawa [ K]. Collaboration in Augmented Reality. In: Computer Supported Cooperative Work, CSCW 2015; vol. 24. 2015, p. 515–525. [6] Schneider M, Rambach J, Stricker D. Augmented reality based on edge computing using the example of remote live support, In: 2017 IEEE International Conference on Industrial Technology, 2017, pp. 1277–1282. [7] Billinghurst M. Clark A. Lee G. A survey of augmented reality Found Trends Human Computer Interact 8 2015 73 272 Billinghurst [ M], Clark [ A], Lee [ G]. A Survey of Augmented Reality. Foundations and Trends in HumanComputer Interaction 2015;8:73–272. [8] Jalo H, Pirkkalainen H, Torro O, Kärkkäinen H, Puhto J, Kankaanpää T. How Can Collaborative Augmented Reality Support Operative Work in the Facility Management Industry? In: Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, 2018, pp. 41–51. [9] Ens B. Lanir J. Tang A. Bateman S. Lee G. Piumsomboon T. Billinghurst M. Revisiting collaboration through mixed reality: The evolution of groupware Int J Human Computer Stud 131 2019 81 98 Ens [ B], Lanir [ J], Tang [ A], Bateman [ S], Lee [ G], Piumsomboon [ T], et al. Revisiting Collaboration through Mixed Reality: The Evolution of Groupware. International Journal of Human-Computer Studies 2019;131:81–98. [10] Wang P. Bai X. Billinghurst M. Zhang S. Zhang X. Wang S. He W. Yan Y. Ji H. Ar/MR remote collaboration on physical tasks: A review Robot Comput-Integr Manuf 72 2021 102071 Wang [ P], Bai [ X], Billinghurst [ M], Zhang [ S], Zhang [ X], Wang [ S], et al. Ar/mr remote collaboration on physical tasks: A review. Robotics and Computer-Integrated Manufacturing 2021;72:102071. [11] Aschenbrenner D, Rojkov M, Leutert F, Verlinden J, Lukosch S, Latoschik M, Schilling K. Comparing Different Augmented Reality Support Applications for Cooperative Repair of an Industrial Robot, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2018, pp. 69–74. [12] Bottani E. Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade IISE Trans 51 3 2019 284 310 Bottani [ E], Vignali [ G]. Augmented reality technology in the manufacturing industry: A review of the last decade. IISE Transactions 2019;51(3):284–310. [13] Wang X. Ong S. Nee A. A comprehensive survey of augmented reality assembly research Adv Manuf 4 1 2016 1 22 Wang [ X], Ong [ SK], Nee [ AYC]. A comprehensive survey of augmented reality assembly research. Advances in Manufacturing 2016;4(1):1–22. [14] Hall M, McMahon C, Bermell-Garcia P, Johansson A, Ravindranath R. Capturing synchronous collaborative design activities: A state-of-the-art technology review, In: Proceedings of International Design Conference, DESIGN 2018, 2018, pp. 347–358. [15] Lee G. Kang H. Lee J. Han J. A user study on view-sharing techniques for one-to-many mixed reality collaborations IEEE Conference on Virtual Reality and 3D User Interfaces 2020 IEEE VR 343 352 Lee [ G], Kang [ H], Lee [ J], Han [ J]. A user study on view-sharing techniques for one-to-many mixed reality collaborations. IEEE Conference on Virtual Reality and 3D User Interfaces, IEEE VR 2020;:343–352. [16] Ludwig T. Stickel O. Tolmie P. Sellmer M. Share-IT: Ad hoc remote troubleshooting through augmented reality Comput Support Coop Work (CSCW) 30 1 2021 119 167 Ludwig [ T], Stickel [ O], Tolmie [ P], Sellmer [ M]. shARe-IT: Ad hoc Remote Troubleshooting through Augmented Reality. Computer Supported Cooperative Work (CSCW) 2021;30(1):119–167. [17] Gurevich P. Lanir J. Cohen B. Design and implementation of TeleAdvisor: a projection-based augmented reality system for remote collaboration Comput Support Coop Work (CSCW) 24 6 2015 527 562 Gurevich [ P], Lanir [ J], Cohen [ B]. Design and implementation of teleadvisor: a projection-based augmented reality system for remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal 2015;24(6):527–562. [18] Zigart T, Schlund S. Evaluation of Augmented Reality Technologies in Manufacturing - A Literature Review, In: Proceedings of the AHFE 2020 International Conference on Human Factors and Ergonomics, 2020, pp. 75–82. [19] Fakourfar O, Ta K, Tang R, Bateman S, Tang A. Stabilized Annotations for Mobile Remote Assistance, In: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 2016, pp. 1548–1560. [20] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration KSII Trans Int Inf Syst 12 12 2018 6034 6056 Kim [ S], Billinghurst [ M], Lee [ C], Lee [ G]. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet and Information Systems 2018c;12(12):6034–6056. [21] Kim S, Lee G, Huang W, Kim H, Woo W, Billinghurst M. Evaluating the Combination of Visual Communication Cues for HMD-Based Mixed Reality Remote Collaboration, In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 2019, pp. 1–13. [22] Kim S. Lee G. Billinghurst M. Huang W. The combination of visual communication cues in mixed reality remote collaboration J Multimod User Interfaces 14 4 2020 321 335 Kim [ S], Lee [ G], Billinghurst [ M], Huang [ W]. The combination of visual communication cues in mixed reality remote collaboration. Journal on Multimodal User Interfaces 2020;14(4):321–335. [23] Neale D.C. Carroll J.M. Rosson M.B. Evaluating computer-supported cooperative work: Models and frameworks Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work CSCW ’04 2004 112 121 Neale [ DC], Carroll [ JM], Rosson [ MB]. Evaluating computer-supported cooperative work: Models and frameworks. In: Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work. CSCW 04; 2004, p. 112121. [24] Hamadache K. Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation Groupware: Design, Implementation, and Use 2009 206 221 Hamadache [ K], Lancieri [ L]. Strategies and taxonomy, tailoring your CSCW evaluation. In: Groupware: Design, Implementation, and Use. 2009, p. 206–221. [25] Antunes P. Herskovic V. Ochoa S.F. Pino J.A. Reviewing the quality of awareness support in collaborative applications J Syst Softw 89 2014 146 169 Antunes [ P], Herskovic [ V], Ochoa [ SF], Pino [ JA]. Reviewing the quality of awareness support in collaborative applications. Journal of Systems and Software 2014;89:146–169. [26] Merino L, Schwarzl M, Kraus M, Sedlmair M, Schmalstieg D, Weiskopf D. Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009–2019), In: IEEE International Symposium on Mixed and Augmented Reality, ISMAR, 2020. [27] Belen R.A.J. Nguyen H. Filonik D. Favero D.D. Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018 AIMS Electron Electr Eng 3 2019 181 Belen [ RAJ], Nguyen [ H], Filonik [ D], Favero [ DD], Bednarz [ T]. A systematic review of the current state of collaborative mixed reality technologies: 20132018. AIMS Electronics and Electrical Engineering 2019;3:181. [28] Marques B, Teixeira A, Silva S, Alves Ja, Dias P, Santos BS. A Conceptual Model for Data Collection and Analysis for AR-based Remote Collaboration Evaluation, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2020, pp. 1–2. [29] Marques B. Teixeira A. Silva S. ao Alves J. Dias P. Santos B.S. A critical analysis on remote collaboration mediated by augmented reality: Making a case for improved characterization and evaluation of the collaborative process Comput Graph 2021 1 17 Marques [ B], Teixeira [ A], Silva [ S], Alves [ J], Dias [ P], Santos [ BS]. A critical analysis on remote collaboration mediated by augmented reality: Making a case for improved characterization and evaluation of the collaborative process. Computers & Graphics 2021a;:1–17. [30] Bai Z. Blackwell A.F. Analytic review of usability evaluation in ISMAR Interact Comput 24 6 2012 450 460 Bai [ Z], Blackwell [ AF]. Analytic review of usability evaluation in ISMAR. Interacting with Computers 2012;24(6):450 – 460. [31] Ratcliffe J, Soave F, Bryan-Kinns N, Tokarchuk L, Farkhatdinov I. Extended Reality (XR) Remote Research: a Survey of Drawbacks and Opportunities, In: CHI Conference on Human Factors in Computing Systems, 2021, pp. 1–13. [32] Dey A. Billinghurst M. Lindeman R.W. Swan J.E. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014 Front Robotics AI 5 2018 37 Dey [ A], Billinghurst [ M], Lindeman [ RW], Swan [ JE]. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014. Frontiers in Robotics and AI 2018;5:37. [33] Anton D. Kurillo G. Bajcsy R. User experience and interaction performance in 2D/3D telecollaboration Future Gener Comput Syst 82 2018 77 88 Anton [ D], Kurillo [ G], Bajcsy [ R]. User experience and interaction performance in 2d/3d telecollaboration. Future Generation Computer Systems 2018;82:77–88. [34] Piumsomboon T. Lee G. Ens B. Thomas B. Billinghurst M. Superman vs giant: A study on spatial perception for a multi-scale mixed reality flying telepresence interface IEEE Trans Vis Comput Graphics 24 11 2018 2974 2982 Piumsomboon [ T], Lee [ G], Ens [ B], Thomas [ B], Billinghurst [ M]. Superman vs giant: A study on spatial perception for a multi-scale mixed reality flying telepresence interface. IEEE Transactions on Visualization and Computer Graphics 2018;24(11):2974–2982. [35] Araujo R.M. Santoro F.M. Borges M.R. A conceptual framework for designing and conducting groupware evaluations Int J Comput Appl Technol 19 3 2004 139 150 Araujo [ RM], Santoro [ FM], Borges [ MRS]. A conceptual framework for designing and conducting groupware evaluations. International Journal of Computer Applications in Technology 2004;19(3):139150. [36] Araujo R, Santoro F, Borges M. The CSCW lab ontology for groupware evaluation, In: 8th International Conference on Computer Supported Cooperative Work in Design, 2, 2004, pp. 148–153. [37] Marques B, Silva S, Dias P, Santos BS. An Ontology for Evaluation of Remote Collaboration using Augmented Reality, In: European Conference on Computer-Supported Cooperative Work, ECSCW: the International Venue on Practice-Centred Computing on the Design of Cooperation Technologies - Posters & Demos, Reports of the European Society for Socially Embedded Technologies, 2021, pp. 1–8. [38] Marques B. Silva S. Alves J. Araujo T. Dias P. Santos B.S. A conceptual model and taxonomy for collaborative augmented reality IEEE Trans Vis Comput Graphics 2021 1 18 Marques [ B], Silva [ SS], Alves [ J], Araujo [ T], Dias [ P], Santos [ BS]. A conceptual model and taxonomy for collaborative augmented reality. IEEE Transactions on Visualization & Computer Graphics 2021c;:1–18. [39] Marques B. Ara T. Silva S. Dias P. Visually exploring a collaborative augmented reality taxonomy Information Visualization, IV 2021 1 6 Marques [ B], Ara [ T], Silva [ S], Dias [ P]. Visually exploring a Collaborative Augmented Reality Taxonomy. In: Information Visualization, IV. 2021d, p. 1–6. [40] Izard C.E. Basic emotions, natural kinds, emotion schemas, and a new paradigm Perspect Psychol Sci 2 3 2007 260 280 Izard [ CE]. Basic Emotions, Natural Kinds, Emotion Schemas, and a New Paradigm. Perspectives on Psychological Science 2007;2(3):260–280. [41] Barnum C.M. Usability Testing Essentials: Ready, Set...Test! first ed. 2010 Morgan Kaufmann Publishers Inc. San Francisco, CA, USA Barnum [ CM]. Usability Testing Essentials: Ready, Set...Test! 1st ed.; San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.; 2010. [42] Gutwin C. Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware ACM Trans on Computer Human Int 6 3 1999 243 281 Gutwin [ C], Greenberg [ S]. The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Transactions on Computer-Human Interaction 1999;6(3):243281. [43] Nilsson S, Johansson B, Jonsson A. Using AR to support cross-organisational collaboration in dynamic tasks, In: IEEE International Symposium on Mixed and Augmented Reality, ISMAR, 2009, pp. 3–12. [44] Kim S, Lee G, Sakata N, Billinghurst M. Improving co-presence with augmented visual communication cues for sharing experience through video conference, In: ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings, 2014, pp. 83–92. [45] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Trans Int Inf Syst 12 2018 6034 6056 Kim [ S], Billinghurst [ M], Lee [ C], Lee [ G]. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet & Information Systems 2018d;12:6034–6056. [46] Huang W. Kim S. Billinghurst M. Alem L. Sharing hand gesture and sketch cues in remote collaboration J Vis Commun Image Represent 58 2019 428 438 Huang [ W], Kim [ S], Billinghurst [ M], Alem [ L]. Sharing hand gesture and sketch cues in remote collaboration. Journal of Visual Communication and Image Representation 2019;58:428–438. [47] Patel H. Pettitt M. Wilson J.R. Factors of collaborative working: A framework for a collaboration model Applied Ergon 43 1 2012 1 26 Patel [ H], Pettitt [ M], Wilson [ JR]. Factors of collaborative working: A framework for a collaboration model. Applied ergonomics 2012;43(1):1–26. [48] Johnson S, Gibson M, Mutlu B. Handheld or Handsfree? Remote Collaboration via Lightweight Head-Mounted Displays and Handheld Devices, In: Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, 2015, pp. 1825–1836. [49] Teo T, Lee G, Billinghurst M, Adcock M. Investigating the use of different visual cues to improve social presence within a 360 mixed reality remote collaboration, In: ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry, 2019. [50] Piumsomboon T. Dey A. Ens B. Lee G. Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality Front Robotics AI 6 2019 Piumsomboon [ T], Dey [ A], Ens [ B], Lee [ G], Billinghurst [ M]. The effects of sharing awareness cues in collaborative mixed reality. Frontiers Robotics AI 2019;6. [51] Marques B, Silva S, Dias P, Santos BS. A Toolkit to Facilitate Evaluation and Characterization of the Collaborative Process in Scenarios of Remote Assistance Supported by AR, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2021, pp. 336–337. [52] Marques B, Silva S, Rocha A, Dias P, Santos BS. Remote Asynchronous Collaboration in Maintenance scenarios using Augmented Reality and Annotations, In: IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), IEEE VR 2021, 2021, pp. 567–568. [53] Madeira T. Marques B. Alves J.a. Dias P. Santos B.S. Exploring annotations and hand tracking in augmented reality for remote collaboration Human Systems Engineering and Design III 2021 Springer International Publishing 83 89 Madeira [ T], Marques [ B], Alves [ J], Dias [ P], Santos [ BS]. Exploring annotations and hand tracking in augmented reality for remote collaboration. In: Human Systems Engineering and Design III. Springer International Publishing; 2021, p. 83–89. "
    },
    {
        "doc_title": "A critical analysis on remote collaboration mediated by Augmented Reality: Making a case for improved characterization and evaluation of the collaborative process",
        "doc_scopus_id": "85113835900",
        "doc_doi": "10.1016/j.cag.2021.08.006",
        "doc_eid": "2-s2.0-85113835900",
        "doc_date": "2022-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Collaboration process",
            "Collaborative process",
            "Comprehensive evaluation",
            "Design and Development",
            "Enabling technologies",
            "In-depth knowledge",
            "Mixed and augmented realities",
            "Remote collaboration"
        ],
        "doc_abstract": "© 2021 Elsevier LtdRemote Collaboration mediated by Mixed and Augmented Reality (MR/AR) shows great potential in scenarios where physically distributed collaborators need to establish a common ground to achieve a shared goal. So far, most research efforts have been devoted to creating the enabling technology, overcoming engineering hurdles and proposing methods to support its design and development. To contribute to more in-depth knowledge on how remote collaboration occurs through these technologies, it is paramount to understand where the field stands and how characterization and evaluation have been conducted. In this vein, this work reports the results of a literature review which shows that evaluation is frequently performed in ad-hoc manners, i.e., disregarding adapting the evaluation methods to collaborative AR. Most studies rely on single-user methods, which are not suitable for collaborative solutions, falling short of retrieving the necessary amount of contextualized data for more comprehensive evaluations. This suggests minimal support of existing frameworks and a lack of theories and guidelines to guide the characterization of the collaborative process using AR. Then, a critical analysis is presented in which we discuss the maturity of the field and a roadmap of important research actions is proposed, that may help address how to improve the characterization and evaluation of the collaboration process moving forward and, in consequence, improve MR/AR based remote collaboration.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2021-08-20 2021-08-20 2022-02-25 2022-02-25 2022-06-01T12:50:43 S0097-8493(21)00170-9 S0097849321001709 10.1016/j.cag.2021.08.006 S300 S300.2 FULL-TEXT 2022-06-01T12:26:47.504333Z 0 0 20220201 20220228 2022 2021-08-20T03:46:47.260845Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor highlightsabst orcid primabst pubtype ref specialabst 0097-8493 00978493 true 102 102 C Volume 102 61 619 633 619 633 202202 February 2022 2022-02-01 2022-02-28 2022 Technical Section article fla © 2021 Elsevier Ltd. All rights reserved. ACRITICALANALYSISREMOTECOLLABORATIONMEDIATEDBYAUGMENTEDREALITYMAKINGACASEFORIMPROVEDCHARACTERIZATIONEVALUATIONCOLLABORATIVEPROCESS MARQUES B 1 Introduction 2 Related work on user evaluation in collaborative AR 2.1 Previous surveys including user evaluation information 2.2 Summary 3 Method and overview of recent literature 3.1 Augmented reality vs mixed reality 3.2 Search process 3.3 Analysis process 3.4 Validity limitations 3.5 Results 3.5.1 User studies categorization 3.5.2 Study design 3.5.3 Study type 3.5.4 Task type 3.5.5 Evaluation methods and data type 3.5.6 Participants 3.6 Summary 4 Critical analysis 4.1 Main limitations Limitation 1: partial evaluation Limitation 2: lack of contextual information Limitation 3: failure situations are not contemplated Limitation 4: lack of theories and guidelines Limitation 5: minimal support in existing frameworks Limitation 6: limited reporting of outcomes 4.2 Maturity of the field 5 Charting out a roadmap for the characterization and evaluation of the collaborative process 5.1 Definition of dimensions of collaboration 5.2 Systematization of perspectives for the field 5.3 Creation of new paradigms, architectures and frameworks 5.4 Development of tools for improved data gathering 5.5 New and better outcomes to support the assessment 6 Conclusions and future work CRediT authorship contribution statement Acknowledgments References ALEM 2011 135 148 L RECENTTRENDSMOBILECOLLABORATIVEAUGMENTEDREALITYSYSTEMS HANDSONVIDEOTOWARDSAGESTUREBASEDMOBILEARSYSTEMFORREMOTECOLLABORATION LUKOSCH 2015 515 525 S COMPUTERSUPPORTEDCOOPERATIVEWORKCSCW2015 COLLABORATIONINAUGMENTEDREALITY THOMAS 1996 P CSCWREQUIREMENTSEVALUATION KIM 2018 2947 2962 K KIM 2018 569 607 S KIM 2020 321 335 S KIM 2020 313 319 S ARIAS 2000 84 113 E GRUDIN 2013 J ENCYCLOPEDIAHUMANCOMPUTERINTERACTION BILLINGHURST 2015 73 272 M ENS 2019 81 98 B BRUNO 2019 875 887 F ONG 2008 2707 2742 S WANG 2016 1 22 X PALMARINI 2018 215 228 R BILLINGHURST 2021 1 4 M ALTUG 2016 23 41 Y CHOI 2018 51 66 S BOTTANI 2019 284 310 E VANLOPIK 2020 K BELEN 2019 181 R ELVEZIO 2017 1 2 C BAI 2012 450 460 Z DESOUZACARDOSO 2020 106159 L ROLTGEN 2020 93 100 D FERNANDEZDELAMO 2018 148 155 I JETTER 2018 18 33 J HAMADACHE 2009 206 221 K GROUPWAREDESIGNIMPLEMENTATIONUSE STRATEGIESTAXONOMYTAILORINGYOURCSCWEVALUATION ANTUNES 2014 146 169 P PATEL 2012 1 26 H DUENSER 2008 A SIGGRAPH2008 ASURVEYEVALUATIONTECHNIQUESUSEDINAUGMENTEDREALITYSTUDIES DEY 2018 37 A GUTWIN 1999 243 281 C SPEICHER 2019 1 15 M PROCEEDINGS2019CHICONFERENCEHUMANFACTORSINCOMPUTINGSYSTEMS MIXEDREALITY MILGRAM 1994 282 292 P MILGRAM 1994 1321 1329 P ROKHSARITALEMI 2020 S BAI 2020 1 13 H PIUMSOMBOON 2019 T WANG 2020 P RHEE 2020 T TEO 2019 T PIUMSOMBOON 2019 T WANG 2019 P WALDOW 2019 246 262 K TEO 2018 406 410 T KIM 2018 6034 6056 S GUNTHER 2018 339 344 S PIUMSOMBOON 2018 T RYSKELDIEV 2018 177 185 B HOPPE 2018 328 337 A AKKIL 2018 524 542 D LEE 2017 G SIGGRAPHASIA2017MOBILEGRAPHICSINTERACTIVEAPPLICATIONS MIXEDREALITYCOLLABORATIONTHROUGHSHARINGALIVEPANORAMA KOMIYAMA 2017 R GUREVICH 2015 527 562 P TAIT 2015 563 589 M KIM 2015 1669 1674 S HUANG 2013 70 77 W PECE 2013 1319 1328 F BANNAI 2006 143 154 Y REGENBRECHT 2004 338 354 H ARAUJO 2004 139 150 R RATCLIFFE 2021 1 13 J CHICONFERENCEHUMANFACTORSINCOMPUTINGSYSTEMS EXTENDEDREALITYXRREMOTERESEARCHASURVEYDRAWBACKSOPPORTUNITIES GAINES 1991 3 22 B LALANNE 2009 153 160 D PROCEEDINGS2009INTERNATIONALCONFERENCEMULTIMODALINTERFACES FUSIONENGINESFORMULTIMODALINPUTASURVEY TEIXEIRA 2014 29 A SPEECHAUTOMATAINHEALTHCAREVOICECONTROLLEDMEDICALSURGICALROBOTSCHAPTER1 ACRITICALANALYSISSPEECHBASEDINTERACTIONINHEALTHCAREROBOTSMAKINGACASEFORINCREASEDUSESPEECHINMEDICALASSISTIVEROBOTS BILLINGHURST 2003 M SERENO 2020 1 20 M COLLAZOS 2019 4789 4818 C MEYER 2019 87 97 M AUGSTEIN 2019 27 58 M NICKERSON 2013 336 359 R TERUEL 2017 e1858 M ZOLLMANN 2020 1 20 S CHANDRASEKARAN 1999 20 26 B NOY 2001 1 25 N HERSKOVIC 2007 328 336 V GROUPWAREDESIGNIMPLEMENTATIONUSE EVALUATIONMETHODSFORGROUPWARESYSTEMS PEREIRA 2015 146 157 C PEREIRA 2016 1 196 C DYNAMICEVALUATIONFORREACTIVESCENARIOS MARQUESX2022X619 MARQUESX2022X619X633 MARQUESX2022X619XB MARQUESX2022X619X633XB 2024-02-25T00:00:00.000Z 2024-02-25T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 0 item S0097-8493(21)00170-9 S0097849321001709 10.1016/j.cag.2021.08.006 271576 2022-06-01T12:26:47.504333Z 2022-02-01 2022-02-28 true 1219570 MAIN 15 65990 849 656 IMAGE-WEB-PDF 1 gr3 31653 220 376 gr1 115848 553 567 ga1 true 21692 176 301 gr2 34838 257 376 gr3 7538 128 219 gr1 9119 164 168 ga1 true 8112 128 219 gr2 9782 149 219 gr3 214178 975 1666 gr1 774364 2446 2509 ga1 true 186053 780 1333 gr2 259932 1137 1666 CAG 3400 S0097-8493(21)00170-9 10.1016/j.cag.2021.08.006 Elsevier Ltd Fig. 1 Overview of the main results from the recent literature review on evaluation and AR-supported Remote Collaboration. In the first level are the categories considered for the systematic review, raging among the participants, application areas, collaboration details, study characteristics, task details, adaptation period and evaluation methods. Then, in the outer ring, the detailed topics of interest for each category are presented, respectively. For each, the number of publications covering it is illustrated, following the literature review analysis. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Positioning of Remote Collaboration mediated by AR between the Replication and Empiricism phases of the BRETAM model. Inspired by [95]. Fig. 3 Roadmap overview of the main topics that should be addressed regarding remote collaboration mediated by AR to make the field achieve the Theory, Automation and Maturity phases of the BRETAM model. Inspired by [97]. Table 1 Summary of evaluation surveys addressing Collaborative Augmented Reality (2008–2019). Year & Authors Pub. # Pubs. analyzed Aspects of Collaboration Main outcomes 2008 - Duenser et al. [41] 10 n/a Studies that evaluated collaboration between users using AR were quite underrepresented. Only 10 papers were reported, which were divided according to the study type in formal and informal studies. 2008 - Zhou et al. [42] not specified n/a A small number of examples of collaborative AR prototypes were starting to emerge, but few had been evaluated in formal user studies. 2012 - Bai et al. [29] 9 Communication, Awareness An increase in measurements of particular interest in AR collaborative systems included explicit communication (e.g., spoken and gestural messages), ease of collaboration and information gathering (basic awareness, eye gaze). 2015 - Billinghurst et al. [12] not specified Communication Besides the standard subjective measures, process measures may be more important than quantitative outcome measures. Process measures are typically gathered by transcribing interaction between users, like speech or gestures and performing a conversational analysis. In this context, very few studies have examined communication process measures. 2018 - Kim et al. [6] not specified n/a A reduce but increasing number of publications explicitly focused on ways to improve collaboration using AR. A mixture of qualitative and quantitative experimental measures were used, such as performance time and accuracy (quantitative), and subjective questionnaires (qualitative). 2018 - Dey et al. [43] 12 n/a Need to conduct more user studies regarding collaboration using AR, more use of field studies, and the use of a wider range of evaluation methods. There is an urge to improve the reporting quality of user studies, and education of researchers on how to conduct good AR user studies. 2019 - Ens et al. [14] 110 Time, Space, Symmetry, Artificiality, Focus, Scenario Review of the history of collaborative MR systems, and investigation on how common taxonomies and frameworks in CSCW and MR research could be applied to such systems. The authors emphasize that MR systems have been facing significant engineering hurdles and have only recently started to mature to focused on the nuances of supporting collaboration. 2019 - Belen et al. [25] 259 Task, Awareness, Presence, Social factors A total of 112 papers studied how MR affects the sense of presence and the perception of social awareness, situational awareness and task awareness during collaboration. A considerable amount of research studied how collaboration reduces cognitive workload through MR environments. 55 papers were categorized under user perception and cognition studies. Table 2 Summary of User studies on Remote Collaboration using AR or MR — Part 1. Legend: S — Subjective; O — Objective; HHD — Handheld Device; HMD — Head Mounted Display. ID Pub. Year Application area Collaboration details Task type Devices used (On-site User) Devices used (Remote User) Study type Data type Study design 1 [53] 2020 Assembly Hierarchy - Synchronous Lego Brick Assembly Projector, External Camera HMD, Hand Tracker Formal S Within-subjects 2 [54] 2020 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Controllers, 360°camera HMD, Controllers Formal S Between-subjects 3 [55] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers, Hand Tracker Formal O + S – 4 [56] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Puzzle Assembly See-through HMD, 360°camera HMD, Controllers Formal O + S – 5 [57] 2019 Assembly Hierarchy - Synchronous Lego Brick Assembly See-through HMD, Depth Sensors HMD, Hand Tracker Formal O + S Within-subjects 6 [58] 2019 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation See-through HMD See-through HMD, Formal S Within-subjects 7 [59] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Lego Brick Assembly See-through HMD, 360°camera HMD, Hand Tracker Formal O + S – 8 [60] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers Formal O + S – 9 [61] 2019 Social Presence Parallel - Synchronous Puzzle Assembly See-through HMD HMD, Controllers Formal S Within-subjects 10 [62] 2019 Assembly Hierarchy - Synchronous Lego Brick Assembly Projector, Camera HMD, Hand Tracker Formal O + S Within-subjects 11 [63] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Hand Tracker Formal O + S Within-subjects 12 [52] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker HMD, Hand Tracker Formal O + S Between-subjects 13 [64] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker See-through HMD, Hand Tracker Formal O + S Within-subjects 14 [6] 2018 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Within-subjects 15 [65] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Hand Tracker Formal O + S Within-subjects 16 [66] 2018 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 17 [67] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HMD, Hand Tracker HMD, Hand Tracker Informal, Formal S Between-subjects 18 [22] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer, Mouse and Keyboard Formal O + S Between-subjects 19 [68] 2018 Assembly Hierarchy - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Hand Tracker Formal O + S Between-subjects 20 [69] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 21 [70] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD HMD, Controllers Formal O + S Within-subjects 22 [71] 2018 Assistance Parallel - Synchronous Navigation, Object Selection and Manipulation HHD HHD Formal O + S – 23 [72] 2018 Assistance Parallel - Synchronous Navigation, Object Selection and Manipulation HMD, Controllers, Hand Tracker HMD, Controllers, Hand Tracker Formal O + S – 24 [73] 2018 Assembly Parallel - Synchronous Puzzle Assembly Projector, External Camera Computer, Gaze Tracker Informal, Formal O + S Within-subjects 25 [74] 2017 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers, Hand Tracker Formal O + S – 26 [75] 2017 Assembly Hierarchy - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Gaze Tracker Formal S Between-subjects 27 [76] 2017 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera, Body Tracker Projector, Optitrack Capture Tracker Informal S Within-subjects 28 [77] 2016 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker HMD, Controllers Informal O + S Between-subjects 29 [78] 2015 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Lego Brick Assembly Projector, External Camera Computer, Mouse and Keyboard Informal, Formal O + S Within-subjects 30 [79] 2015 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD Computer, Mouse and Keyboard – O + S Between-subjects 31 [80] 2015 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 32 [81] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD Computer, Mouse and Keyboard – O + S Between-subjects 33 [82] 2014 Assembly Parallel - Synchronous Puzzle Assembly HHD or See-through HMD Computer, Mouse and Keyboard Formal O + S Between-subjects 34 [83] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer with Touch screen Informal, Field S – 35 [84] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer, Mouse and Keyboard Informal, Field O + S Within-subjects 36 [85] 2013 Assembly Hierarchy - Synchronous Puzzle Assembly Monitor, External Camera HMD Formal O + S – 37 [86] 2013 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation HHD HHD Formal O + S – 38 [87] 2012 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera See-through HMD, external camera Informal S Between-subjects 39 [88] 2012 Assistance Hierarchy - Synchronous Airplane Cockpit HHD Computer, Mouse and Keyboard Formal O + S Within-subjects 40 [89] 2007 Education Parallel - Asynchronous Navigation, Object Selection and Manipulation Computer, External Camera Computer, External Camera, Gaze Tracker Informal S Between-subjects 41 [90] 2006 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HMD, External Tracker HMD, External Tracker Formal O – 42 [91] 2004 Social Presence Hierarchy - Synchronous Navigation, Object Selection and Manipulation Computer, Mouse and Keyboard Computer, Mouse and Keyboard Formal S Within-subjects Table 3 Summary of User studies in Remote Collaboration using AR or MR — Part 2. ID Pub. Evaluation methods # Participants (# Females) Participant role Participants knew each other Previous experience with AR/VR/MR Description experimental context Adaptation period Duration (min) Recording audio and video 1 [53] Questionnaires, Interview 34 (11) On-site or Remote Yes and No – – Yes 55 – 2 [54] Questionnaires, User Preference 40 (-) On-site or Remote – Yes Yes Yes 40 – 3 [55] Task Performance, Questionnaires, User Preference 32 (8) On-site or Remote – Yes – – – – 4 [56] Task Performance, Questionnaires, User Preference 10 (1) On-site or Remote – Yes Yes Yes – – 5 [57] Task Performance, Questionnaires, User Preference 10 (4) On-site or Remote – – – – – – 6 [58] Questionnaires, User Preference 8 (4) On-site or Remote – – Yes – 30 – 7 [59] Task Performance, Questionnaires, User Preference 14 (-) On-site – – Yes – 70 Yes 8 [60] Task Performance, Questionnaires, User Preference 24 (5) On-site – Yes Yes – 90 – 9 [61] Questionnaires, Interview 48 (24) On-site Yes Yes and No Yes – – – 10 [62] Task Performance, Questionnaires, User Preference 13 (5) On-site or Remote – No – Yes – – 11 [63] Task Performance, Questionnaires, User Preference 12 (3) On-site and Remote Yes Yes Yes – – – 12 [52] Task Performance, Questionnaires, Interview 32 (9) On-site and Remote Yes Yes and No Yes Yes 120 – 13 [64] Task Performance, Questionnaires 20 (5) On-site or Remote No – Yes Yes 35 – 14 [6] Task Performance, Questionnaires, User Preference 24 (7) On-site or Remote Yes – – – – Yes 15 [65] Task Performance, Questionnaires, User Preference 8 (2) On-site – Yes Yes Yes – – 16 [66] Questionnaires, Interview 24 (4) On-site or Remote Yes – Yes Yes – – 17 [67] Questionnaires, User Preference 38 (23) On-site or Remote – – Yes Yes 30 – 18 [22] Task Performance, Questionnaires 30 (4) On-site – – – – – 19 [68] Task Performance, Questionnaires, User Preference 10 (0) On-site – – – – – – 20 [69] Task Performance, Questionnaires, User Preference 8 (4) On-site or Remote – – Yes – 60 Yes 21 [70] Task Performance, Questionnaires, User Preference 16 (5) On-site or Remote – Yes – – – – 22 [71] Task Performance, Questionnaires, User Preference 40 (-) On-site or Remote – – Yes – – – 23 [72] Task Performance, Questionnaires, User Preference 28 (-) On-site and Remote – Yes – – – – 24 [73] Task Performance, Questionnaires, User Preference 24 (16) On-site or Remote – – Yes Yes – – 25 [74] Task Performance, Questionnaires, User Preference 8 (-) On-site and Remote – – – – – – 26 [75] Questionnaires, User Preference 8 (2) On-site or Remote Yes – – – – – 27 [76] Questionnaires, User Preference 8 (-) Remote – – – – – – 28 [77] Task Performance, Questionnaires 10 (-) On-site – – – – – – 29 [78] Task Performance, Questionnaires 13 (-) + 24 (-) On-site or Remote – – – – – – 30 [79] Task Performance, Questionnaires, User Preference 36 (15) On-site or Remote – – – – – Yes 31 [80] Task Performance, Questionnaires, Interview 24 (7) Remote – – Yes Yes 70 – 32 [81] Task Performance, Questionnaires, User Preference – Remote – – – – – Yes 33 [82] Task Performance, Questionnaires, User Preference 24 (7) On-site or Remote Yes – Yes Yes 90 – 34 [83] Questionnaires, User Preference 25 (-) + 11 (5) – – – – – 50 – 35 [84] Task Performance, Questionnaires, User Preference 20 (-) + 60 (29) On-site – – – – – – 36 [85] Task Performance, Questionnaires, User Preference 14 (-) On-site and Remote – – Yes – – – 37 [86] Task Performance, Questionnaires, User Preference 36 (-) On-site or Remote – – Yes – – – 38 [87] Interview 5 (-) On-site – – – – – Yes 39 [88] Task Performance, Questionnaires 48 (21) On-site Yes and No – Yes Yes – – 40 [89] Questionnaires, Interview 9 (3) On-site – – – Yes 20 – 41 [90] Task Performance 12 (2) On-site or Remote – – – – – – 42 [91] Questionnaires, User Preference 27 (8) On-site or Remote – – Yes – – – Table 4 Overview of common approaches and what is missing regarding the evaluation process of remote collaboration mediated by AR. Common: •synchronous hierarchy collaboration •within-subjects design •formal user studies •navigation, selection, manipulation and assembly tasks •focus on technological aspects or interaction mechanisms of the collaborative AR solution •subjective and objective data collection •use of single-user questionnaires, task performance and user preferences assessment •young participants from universities •participants act as on-site or remote team-members Missing: •conduct outdoor and field studies •explore complex/adequate tasks •contemplate failure situations •provide an adaptation period •address participants relationships, knowledge and motivations •better description of the collaborative process supported by AR •reporting of study average duration •data collection on dialog turns, interaction types, main features and visual complexity •contextualized information on the team, task, environment and collaborative tool •improve existing frameworks •use of video, audio recordings, and post-task interviews ☆ This article was recommended for publication by C Sandor. Technical Section A critical analysis on remote collaboration mediated by Augmented Reality: Making a case for improved characterization and evaluation of the collaborative process Bernardo Marques Conceptualization Methodology Validation Formal analysis Investigation Data curation Writing – original draft Writing – review & editing Visualization Project administration Funding acquisition ⁎ António Teixeira Conceptualization Methodology Writing – original draft Visualization Samuel Silva Methodology Investigation Writing – original draft Writing – review & editing João Alves Validation Formal analysis Investigation Data curation Writing – original draft Paulo Dias Conceptualization Investigation Data curation Writing – original draft Writing – review & editing Supervision Funding acquisition Beatriz Sousa Santos Conceptualization Methodology Investigation Writing – original draft Writing – review & editing IEETA, DETI, University of Aveiro, Aveiro, 3810-193, Portugal IEETA, DETI, University of Aveiro Aveiro 3810-193 Portugal IEETA, DETI, University of Aveiro, Aveiro, 3810-193, Portugal ⁎ Corresponding author. Remote Collaboration mediated by Mixed and Augmented Reality (MR/AR) shows great potential in scenarios where physically distributed collaborators need to establish a common ground to achieve a shared goal. So far, most research efforts have been devoted to creating the enabling technology, overcoming engineering hurdles and proposing methods to support its design and development. To contribute to more in-depth knowledge on how remote collaboration occurs through these technologies, it is paramount to understand where the field stands and how characterization and evaluation have been conducted. In this vein, this work reports the results of a literature review which shows that evaluation is frequently performed in ad-hoc manners, i.e., disregarding adapting the evaluation methods to collaborative AR. Most studies rely on single-user methods, which are not suitable for collaborative solutions, falling short of retrieving the necessary amount of contextualized data for more comprehensive evaluations. This suggests minimal support of existing frameworks and a lack of theories and guidelines to guide the characterization of the collaborative process using AR. Then, a critical analysis is presented in which we discuss the maturity of the field and a roadmap of important research actions is proposed, that may help address how to improve the characterization and evaluation of the collaboration process moving forward and, in consequence, improve MR/AR based remote collaboration. Graphical abstract Keywords Remote collaboration Augmented Reality Mixed Reality Evaluation and characterization collaborative process Critical analysis Roadmap proposal 1 Introduction Collaboration is essential in many situations, as is the case of industrial, medical, and educational domains, among others [1–4] and can be described as the process of joint and interdependent activities between co-located or remote collaborators performed to achieve a common goal [5–9]. Collaboration scenarios have evolved from simple co-located scenarios to more complex remote collaboration, encompassing several team members with different experiences, expertise’s and multidisciplinary backgrounds distributed by different geographic locations around the world. Therefore, the methods required to address such activities have been growing in terms of scale, complexity, and interdisciplinarity, entailing not only the mastery of multiple domains of knowledge, but also a strong level of proficiency in each [3,4,10]. Remote collaboration, implies that collaborators establish a joint effort to align and integrate their activities in a seamless manner. Technological support for remote collaboration has been addressed among other fields by Computer-Supported Cooperative Work (CSCW), focusing on conceptualizing, designing, and prototyping solutions for communication, cooperation, assistance, training, learning as well as knowledge sharing between distributed collaborators. One major issue of remote collaboration is the fact that collaborators do not share a common space/world, reason for the interest in using Augmented Reality (AR) in this context [11–15]. Collaboration using AR helps distributed collaborators establish a common ground, analogous to their understanding of the physical space, allowing to inform where to act, and what to do, e.g., making assumptions and beliefs visible by providing real-time spatial information, highlighting specific areas of interest, or sharing situated information associated with relevant objects in the on-site physical environment [16–20]. Remote AR-based solutions are well suited for overlaying responsive computer-generated information on top of the real-world environment, resulting in the creation of solutions that combine the advantages of virtual environments and the possibility for seamless interaction with the real-world objects and other collaborators [6,15,17,19,21–24]. A number of studies have focused on the use of virtual annotations to augment the shared understanding, using drawings, pointers, gaze, hand gestures and others on 2D images or live video streams [3,6,9,14,22,25]. As an alternative, recent studies started to explore the use of virtual replicas [26–28], as well as reconstructions of the physical environment [29,30], although these required the existence of 3D models and additional hardware, which may limit their adoption in some scenarios of application. Using such approaches to enhance the common ground can improve efficiency and accuracy of the performed tasks by enhancing the perception of the shared understanding [6,22,24,31,32], as well as collaboration times, knowledge retention, increased problem context and awareness [16,17,33–35]. While creating the means to support collaboration clearly motivated early research, advances in AR have been limited by new technical developments, which means most of the research efforts, so far, have been focused on creating the enabling technology and propose novel methods to support its design and development [14,36,37]. On the other hand, with the growing development of CSCW, the evaluation of these solutions during the collaborative effort become an essential, but difficult endeavor [31,38,39], given the novelty of the field and the lack of methods and theories [14,25] to guide the characterization of the collaborative process, i.e., describe the contributions of AR to the collaborative work effort. In addition, scenarios of remote collaboration are multifaceted [40], which means many aspects may affect the way teams collaborate, making it difficult to identify all variables related to the collaborative process. Therefore, the integration of proper characterization and evaluation methods and guidelines is of paramount importance. In this paper, we analyze the subject of remote collaboration supported by AR through a systematic review and investigate how characterization and evaluation of the collaborative process has been conducted during user studies to better understand their specificities, rather than focusing on the development of technology itself. In this context, we analyzed existing surveys that addressed collaborative user studies and evaluation in their reviews. Plus, we performed a literature review from 2000 to 2020 to provide a high-level overview of the field, allowing identification of strengths and weaknesses of existing methods. Based on the analyzes, we describe the challenges involved with evaluating these solutions and critically analyze the state of the field. As a result, a possible roadmap is proposed to facilitate and elicit characterization of the collaboration process using AR-based solutions, so that research and development can move forward and focus on the nuances of supporting collaboration, i.e., focus squarely on the human concerns that underlie collaboration, rather than creating the enabling technology that makes remote collaboration mediated by AR possible. The rest of the paper is organized as follows: Section 2 presents an overview based on survey papers to update, complement and fill gaps on the current information on the state-of-the-art. Next, Section 3 details the methodology adopted to conduct our literature review and describes a high-level overview of the reviewed papers. Then, Section 4 provides a critical analysis in which we discuss the challenges associated to the characterization and evaluation of the collaborative process. Afterwards, Section 5 propose a roadmap to address these challenges. Finally, Section 6 concludes by summarizing the main outcomes. 2 Related work on user evaluation in collaborative AR This section identifies and analyzes existing survey papers that cover relevant evaluation details, which are summarized in Table 1. Our goal was to understand how evaluation has been conducted in collaborative scenarios, allowing to compare and contrast different methods, as well as identify opportunities and limitations associated to the characterization of the collaborative process. From the list of prior surveys, the first six entries are rather general in scope, although the review of collaborative AR papers is also mentioned, despite being only a portion of the results reported [6,12,29,41–43]. While this is the case, the two last entries of the list [14,25] focus entirely on the subject of Collaborative AR and MR, including co-located and remote examples. Although these surveys primarily focused on the development of collaborative AR technology itself, some important outcomes regarding evaluation are also reported, as described below in detail. Besides, another publication [7] was considered in this analysis, that even though not strictly a survey, includes important information regarding evaluation of collaborative work in the context being addressed. 2.1 Previous surveys including user evaluation information Duenser et al. (2008) reported on user evaluation techniques used in AR research. Then, studies that evaluate collaboration between users using AR were quite underrepresented: from a total of 161 publications included in the survey, only 10 addressed collaborative AR. Besides reporting that 8 papers were formal and 2 informal user evaluation, the survey does not present further detail on the collaborative studies [41]. In addition, Zhou et al. (2008) presented one of the first overviews of the research conducted until that moment at the ISMAR conference and its predecessors. Although the research focus was on AR technologies, it also pointed out the significance of usability evaluation. The authors reported that a small number of collaborative AR prototypes were starting to emerge, but few had been evaluated in formal user studies. The authors also highlighted how the role of different displays would affect collaboration in the future and how the location of the task affected user behaviors in terms of verbal and non-verbal communication. Since collaboration and evaluation were not one of the focus of the survey, no further detail was provided [42]. In the same way, Bai et al. (2012) conducted an analytic review on usability evaluation at ISMAR. The authors suggested that while the design of usable systems were the main focus of collaborative AR research to that point, an increase in evaluation research was emerging. They also stated that measurements of particular interest in collaborative AR systems may include explicit communication (e.g., spoken and gestural messages), ease of collaboration and information gathering (e.g., basic awareness, eye gaze). The authors also reported that subjective answers may be collected via questionnaire and that direct observation was used to extract objective results. Moreover, signs of discomfort and enjoyment during collaboration were also taken into account by researchers [29]. Billinghurst et al. (2015) published a survey on AR, in which almost 50 years of research and development in the field were summarized. The authors state that in Collaborative AR studies, besides the standard subjective measures, process measures may be more important than quantitative outcome measures. Process measures are typically gathered by transcribing interaction between users, like speech or gestures and performing a conversational analysis. Measures that have been found to be significantly relevant include: frequency of conversational turns, duration of overlapping speech, number of questions, number of interruptions, turn completions and dialog length, among others. Besides, gesture and non-verbal behaviors can also be analyzed for characteristic features. The survey acknowledges that there have been very few user studies with collaborative AR environments and almost none that examined communication process measures [12]. Then again, Kim et al. (2018) revisited the trends presented at ISMAR conferences. According to their review, user evaluation and feedback has become one of the main categories for research presented at ISMAR, with 16.4% of publications reporting evaluation being conducted, showing a significant increase when compared to Zhou et al. 5.8% [42]. The authors extended Zhou et al. list of emerging research, including interactive collaborative systems for multiple remote or co-located users. A mixture of qualitative and quantitative experimental measures were used in studies that addressed collaboration, such as performance time and accuracy (quantitative), and subjective questionnaires (qualitative) [6]. Dey et al. (2018) conducted a systematic review of AR usability studies. A total of 291 papers have been reviewed. Among other things, over the years, there were few collaborative user studies, mostly directed towards remote collaboration. The authors reported 12 papers, in a total of 15 studies associated to the collaboration application area. One noticeable feature was the fact that there were no pilot studies reported, which is an area for potential improvement. Also, a reduced number (3 out of 15) of field studies was reported and all except one were performed indoors. Furthermore, a within-subjects design was used by 14 out of 15 studies, since these require fewer participants to achieve adequate statistical significance, with only 12 participants being recruited per study. Besides, roughly one-third of the participants were females in all studies. Hence, participant populations are dominated by mostly young, educated, male participants, suggesting that the field could benefit from more diversity. A majority of the studies, 8 out of 15 collected both objective (quantitative) and subjective (qualitative) data, while 5 studies were only based on subjective data, and 2 studies were based on only objective data. Aside from subjective feedback or ratings, task completion time and error/accuracy were also extensively used. Curiously, the NASA TLX was only used by one study. This analysis suggest the need of more user studies regarding collaboration using AR, particularly more field studies, and the use of a wider range of evaluation methods [43]. Although not strictly a survey, Kim et al. (2018) proposed a questionnaire including aspects regarding overall collaboration, namely the level of enjoyment and mental stress in communication with the partner, and whether collaboration was effective or not [7]. Moreover, the questionnaire included questions about who (presence of others — users’ feeling of togetherness with the collaborating partner), what (users’ activities — effectiveness in sending and receiving messages) and where (location of activities — whether seeing work space properly and asking the level of having a same focus with a partner). The questionnaire was based on previous work by Gutwin and Greenberg (1999), which suggested three types of experimental measurements are necessary to assess collaboration: product, process, and satisfaction. Product measures focus on assessing collaboration outcomes in terms of efficiency (e.g., task completion time) or quality (e.g., accuracy). Process measures assess user communication and patterns of collaboration and can be obtained by system log data, observation, and video/audio analysis. Satisfaction measures are adequate to assess participants’ subjective opinions on the quality of their collaboration and can be collected through interviews and questionnaires [44]. More recently, Ens et al. (2019) revisited collaboration through MR. A total of 110 papers employing MR technology and motivated by challenges in collaborative scenarios was reviewed, showing a rise in the number of papers published from 2012 and onward. The authors emphasize that MR systems have been facing significant engineering hurdles, being limited by the contemporary capabilities of technology, and have only recently started to mature to the point where researchers can focus squarely on the human concerns that underlie communication and collaboration, instead of focusing on creating the enabling technology. The vast majority of papers analyzed (106, or 95%) focused on synchronous collaboration. Moreover, 30 papers (27%) worked on a co-located setting, while 75 papers (68%) worked on a remote setting, and 6 papers (5%) support both settings. In the early years (up to 2005), most research addressed co-located work. Then, the paradigm changed, and from 2006 forward most work tackled remote collaboration. In addition, 45 papers (41%) focus on symmetric collaboration, while 63 (57%) on asymmetric, and 2 (2%) supported both types. The review states that existing methods are not sufficient to characterize how collaboration occurs. Finally, it also emphasizes the need to deepen the understanding of collaborative work through more user studies [14]. Finally, Belen et al. (2019) performed a systematic review of the current state of collaborative MR technologies published from 2013 to 2018. A total of 259 papers have been classified based on their application areas, types of display devices used, collaboration setups, user interaction and experience aspects. Regarding the collaboration setups used, 129 papers (50%) report works that used a remote setup, 103 papers (40%) used a collocated setup, and 27 (10%) used both settings. The type of user interaction and user experience were categorized, resulting in 55 papers categorized under user perception and cognition studies, which aim to lessen cognitive workload for task understanding and completion time and increase users’ perceptual (e.g., situational, social, and task) awareness and presence. Besides, a total of 112 papers studied how MR affects the sense of presence and the perception of social awareness, situational awareness and task awareness during collaboration. There was also a considerable amount of research on how collaboration reduces cognitive workload through MR environments. This review also showed that user interaction in a collaborative MR environment is an essential topic that requires further investigation [25]. 2.2 Summary Research is evolving from solving technical issues using AR and MR, towards more meaningful studies on collaboration. We were able to understand that evaluation is frequently done using single-user methods, which are not always applicable to groupware collaborative solutions. To clarify, by single-user methods, we are referring to the methodologies used in the collaborative studies. For example, focusing more on technological aspects of the solution being used than in the collaborative process; including tasks with low complexity that do not elicit real collaboration among participants; using only performance measures like task completion time and error/accuracy data, while other important dimensions are ignored; collecting participant data based only on standard practices with fixed answers, applying scales, questionnaires (e.g., System Usability Scale (SUS), NASA Task Load Index (TLX), among others), which are not thought for collaborative scenarios, thus ignoring detail on crucial aspects of collaboration. The majority of papers mentioned in the surveys informed on the tasks, types of devices used (although not specific to on-site or remote users), evaluation design, evaluation methods and number of participants, but lack detail on the participants’ role, if participants knew each other previous to the study, their previous experience with Virtual Reality (VR), AR or MR solutions, description on the experimental context, among other factors of collaboration. However, our review highlights some limitations included in previous surveys, namely the absence of information regarding specific characteristics of the collaborative context. These characteristics are important since collaboration may occur at many levels and depends on several factors that may impact directly the collaborative outcomes [40]. Contextual information helps inform the conditions in which the collaborative effort took place. Without comprehending the contextual information, it becomes difficult to assess the important variables related to the collaborative process, which means the results and findings reported may be misleading or of limited value in these scenarios, thus being an important subject to improve the characterization of the collaborative process. Hence, these aspects have an important impact on how the studies must be prepared and how they were conducted, influencing situation understanding, team-members communication, task performance, and even how AR-based tools were used among team-members, among others. Therefore, it is important to conduct thorough collaborative studies, allowing to retrieve the necessary amount of data for more comprehensive analysis that helps provide a perspective on the different factors of collaboration supported by AR. To sum up, the use of AR-based multi-site solutions creates challenges to the contextualization of the actions of each user and the problems/barriers they may face. Therefore, having a grasp of those aspects is paramount to ensure characterization is genuine. By doing so, researchers may be able to better assess a wide range of information, namely individual and team personalities, motivations, performances, behaviors, who completed the tasks and who provided instructions, how was the communication process, details of the surrounding environments, as well as duration and type of interactions with the collaborative technology, among other aspects when analyzing data and establishing conclusions. 3 Method and overview of recent literature To understand to what extent user evaluation is currently being reported covering collaborative AR and Mixed Reality (MR) research, we conducted an analysis of existing works through a systematic review. This section presents the research methods employed to carry out the review process, which was divided into: the search, i.e., describing how the collection of publications was performed and the review, i.e., explaining the process employed to ensure that the papers follow our review criterion. What differentiates our review from other surveys described in the previous section is the fact that we focus exclusively on evaluation and user studies in remote scenarios mediated by AR/MR to comprehend how the collaborative process has been captured and reported, rather than addressing the technology that made collaboration possible, which was the nucleus of the two only surveys that dedicated their efforts to the subject of Collaborative AR/MR, while the remaining ones are rather general in application scenario, although also addressing more technological aspects of AR/MR. Besides, by identifying relevant aspects that are missing from existing surveys regarding evaluation and user studies, we are able to include them in our analysis, leading to a discussion in which we critically analyze the field in light of the BRETAM model, thus providing a clearer understanding of how the characterization of the collaborative process has been achieved, which lead to the proposal of a roadmap of relevant research topics, aiming to help the community move the field forward. 3.1 Augmented reality vs mixed reality While older papers used the term remote collaboration supported by AR, more recent efforts described in literature are beginning to replace the term AR by MR. Next, we elaborate on the meaning of MR, and why this sudden change has started to emerge. Many researchers see MR as a synonym for AR [45]. Some consider MR a superset of AR, i.e., a real-world object can interact with a virtual one in real-time to assist individuals in practical scenarios [46–48]. Yet, others consider MR distinct from AR in the sense that MR enables walking into, and manipulating a scene, whereas AR does not, i.e., there is a separation of the real and virtual world content, which may lead to lower user immersion [49]. Although MR is increasingly gaining in popularity and relevance, the research community is still far from a shared understanding of what MR actually constitutes. Speicher et al. (2019) highlights that currently, there is no single definition for MR, since this concept can be considered different things for different individuals. In their survey, six partially competing notions were identified based on literature analysis and experts’ responses. Nevertheless, there is no universally agreed on, one-size-fits-all definition of MR. Moreover, the authors state that it is highly unrealistic to expect one single definition may appear in the future, which means discussions about MR become increasingly difficult. Therefore, it is extremely important to be clear and consistent in terminology while communicating one’s understanding of MR in order to avoid confusion and ensure constructive discussion [45]. Among the most important applications of MR are collaborative solutions, that may be used as decision-making tools for daily life problems [14,49]. In this context, Speicher et al. (2019) suggested that MR can be considered as a type of collaboration that describes the interaction between physically separated users exploring AR and VR [45]. This definition includes mapping of the environment of an on-site AR collaborator, i.e., capturing more dimensional information about the local scene, which is reconstructed in VR for the remote collaborator [45,50] and so provides unique capabilities to achieve a common goal, e.g., improved communication cues for more efficient and easier collaboration [8,46,51,52]. Given the aforementioned panorama, we decided to include both terms in our analysis. 3.2 Search process Our review was made as inclusive as possible. We collected papers from the Scopus database (since it covers most top journals and conferences on Collaborative AR) using the search terms: (“Augmented Reality” OR “Mixed Reality”) AND (“Remote Collaboration” OR “Remote Cooperation” OR “Remote Assistance” OR “Remote Guidance” OR “Distributed Collaboration”) AND (“User Evaluation” OR “User Study” OR “User Experiment”) The search for the terms was made in the Title, Abstract, and Keywords fields. All search results published in conferences and journals between 2000 and 2020 were taken into consideration. Only publications in the English language were considered as this is the current ’lingua franca’ of the academic research. 3.3 Analysis process We obtained a total of 64 publications. The search results were analyzed individually to identify whether or not it supported evaluation of remote scenarios supported by solutions using MR or AR. Only 42 publications satisfied the defined criteria. We started by filtering the initial collection of publications to meet our objectives. We removed articles that were incorrectly selected in the search process (false positives) and identified only those articles that included user evaluation. The reviews of each paper focused on the following attributes (Tables 2 and 3): application areas and keywords; type of collaboration; type of task; types of devices used (regarding on-site and remote users); type of study; type of data collected; evaluation design; evaluation methods; number of participants (number of female participants); participant role; participants’ familiarity with each other; previous experience with AR/VR/MR; experimental context description; adaptation period provided; study average duration (min); recording of audio and video. 3.4 Validity limitations A considerable amount of effort was invested on the selection and review process. Although the Scopus bibliographic database has been used to cover a wide range of publication venues and topics, there may be limitations with the described method. The search terms used might be limiting, as other papers could have used different keywords to describe “Remote Collaboration”, “Augmented Reality”, “Mixed Reality” or “Evaluation”. Therefore, it remains likely that there are papers which may have not been included in this review. 3.5 Results Next, a high-level overview of the reviewed papers is provided (Tables 2 and 3), following a similar structure as the one used by Dey et al. (2018) in their systematic review [43], which is extended to include relevant aspects missing from the surveys analyzed in the previous section, such as collaboration details, task type, study type, data type, study design, evaluation methods, participants characteristics, experimental context, adaptation period, and duration. 3.5.1 User studies categorization The papers (Tables 2 and 3) have the following distribution by application areas: assistance (25 papers, 59.5%); assembly (11 papers, 26.2%); co-design (3 papers, 7.1%); social presence (2 paper, 4.8%); education (1 paper, 2.4%), as presented in the orange bubbles in Fig. 1. Regarding the collaboration details, 30 papers (71.4%) explored collaboration using a synchronous hierarchy approach, i.e., each member has a specific function or expertise and all team members are present and could act in real-time, while 11 papers (26.2%) studied synchronous parallel approach, where all elements have the same level of expertise and could act in real time and only 1 paper (2.4%) studied asynchronous parallel approach, i.e., all elements have the same level of expertise in which collaboration would take place at different times, as shown in the dark blue bubbles in Fig. 1. 3.5.2 Study design As shown in Table 2, 16 papers (38.7%) used a within-subjects design, while 15 papers (35.7%) used a between-subjects design. There were no mentions of a mixed-factorial design. In addition, 11 (26.2%) papers did not mention the method used, as illustrated in the green bubbles in Fig. 1. 3.5.3 Study type We found that most papers (33, 78.6%) were formal user studies. On the opposite, 7 papers (16.6%) reported conducting informal studies. Only 2 papers (4.8%) conducted user studies in the field, which shows a lack of experimentation in real-world conditions, as exhibit in the green bubbles in Fig. 1. 3.5.4 Task type As expected, most papers (26 out of 42, 61.9%) explored navigation, object selection and manipulation, forcing participants to communicate and use collaborative tools to provide indications to achieve a concrete goal. Additionally, 12 papers (28.6%) focused on assembly tasks using Lego bricks, or puzzles like tangram, pentominoes, origami, among others. Only 1 paper (2.4%) reported the use of an airplane cockpit as case study, as presented in the red bubbles in Fig. 1. This shows that there is an opportunity for conducting more user studies exploring different, more complex case studies, or even combinations of different types. Moreover, just 14 papers (33.33%) claim to have provided an adaptation period before the performance of the tasks, as shown in the purple bubbles in Fig. 1. Finally, the bulk of the user studies were conducted in an indoor environment, but only 21 papers (50%) described the experimental context, although no clear pattern emerged. 3.5.5 Evaluation methods and data type In terms of data type, 30 papers (71.4%) collected subjective and objective data, 11 papers (26.2%) collected only subjective data, and just 1 (2.4%) only objective data. Concerning the evaluation methods, we found that the most popular method is filling out questionnaires (40 papers, 95.2%), followed by assessing task performance (31 papers, 73.8%) with error/accuracy measures and task completion time. Then, user preference (28 papers, 66.7%) and finally interviews (5 papers, 11.9%), as illustrated in the light blue bubbles in Fig. 1. Note that many papers used more than one evaluation method, so the percentages sum to more than 100%. Another essential point: only 13 papers (31%) mentioned the average duration of the user study (58.5 min). Some papers mentioned the duration of the task, but no clear information on the collaboration process is provided, like dialog length, frequency of conversational turns, among others. Besides, none of the papers report to have conducted gesture or non-verbal behaviors analysis. This is supported by the lack of audio or video recording, since only 6 papers (14.3%) acknowledge to store this type of data. 3.5.6 Participants Our review of the participants shows that the number of participants involved in the analyzed studies ranged from 5 to 48, with an average of 21. Also, a total of 31 out of 42 papers (73.4%) reported involving female participants in their experiments, with the ratio of female participants to male participants being 47.6% of total participants in those 31 papers. Hence, most of the studies were run with young participants, mostly university students, rather than a more representative cross section of the population. Equally important, 23 papers (54.8%) stated that participants would perform the role of the on-site or remote user during the studies. Moreover, in 5 papers (11.9%) the participants would perform the on-site and remote role. 11 papers (26.2%) only allowed the participants to perform the on-site user, while 3 papers (7.1%) only allowed to perform the remote role. In these cases, the counterpart would be performed by a monitor, as presented in the brown bubbles in Fig. 1. Most papers, 32 out of 42 (76.2%) made no mention if participants knew each other, with only 9 clearly stating that information. Likewise, the same percentage did not mention any type of previous experience the participants might have with AR or MR systems. 3.6 Summary Our review (Table 4 and Fig. 1) shows that the dominant type of collaboration is based on the hierarchy approach focused on synchronous communication between participants. Also, that assistance and assembly are the main areas of application, exploring navigation, selection and manipulation tasks in indoor environments, during approximately one hour. On average, studies involved 21 participants, mostly young university students. Moreover, ruffly half of the papers reported that the participants would perform the role of the on-site or remote user during the studies. Besides, most papers lack information regarding if participants knew each other prior to the study and if they had previous experience with MR systems. The majority of the studies conducted are formal studies, collecting objective and subjective data using evaluation methods like questionnaires, task performance and user preferences in that order respectively. As for collaborative measures, most works focus on effectiveness, only checking if participants were able to accomplish a given task collaboratively. Moreover, the evaluation design is distributed between within-subjects and between-subjects. Besides, interviews are not used often, as is also the case of recording audio and video during the studies. In addition, half of the times the experimental context is not described and only one third of the times studies referred the existence of an adaptation period. It is important to report this last fact, as it can affect the way the collaboration process was performed between collaborators, i.e., those that had an opportunity to use, adapt and comprehend the technology that helped create a shared understanding prior to the tasks will easily interact better with their respective counterpart, when compared to the ones that have only done the adjustment process during the task itself. Another observation is that single-user evaluation methods are applied to collaborative tasks, which mainly focus on the comparison of technological aspects or interaction mechanisms based on rather simpler procedures. We argue that collaborative tasks must be difficult and long enough to encourage interaction between collaborators and for the AR-based solution being used to provide enough contribution. In general, tasks can benefit from deliberate drawbacks, and constraints, i.e., incorrect, contradictory, vague or missing information, to force more complicated situations and elicit collaboration. For example, suggest the use of an object which does not exist in the environment of the other collaborator or suggest removing a red cable, which is green in the other collaborator context. Such situations help introduce different levels of complexity, which go beyond the standard approaches used, and elicit more realistic real-life situations where the surroundings are not always perfect. Likewise, multiple procedures may be applied to an evaluation, while also exploring different levels of complexity, contextual changes in the surroundings environments, as well as stress conditions. 4 Critical analysis This section describes the main limitations hindering a better understanding regarding how AR supports collaborative work in remote scenarios. Analysis was mostly based in the results from the literature review process, complemented by meetings with domain experts, and authors’ own experience creating and conducting evaluation studies in this domain [Refs omitted for review purposes]. The contributions presented in this paper were conducted in the scope of a larger multidisciplinary research line, with a total of nine individuals with several years of expertise (minimal of 6 years, and a maximum of 40 years of experience) in the areas of Human–Computer Interaction (HCI), Virtual and Augmented Reality (VR/AR), Information Visualization (IV), Multimodal Interaction (MMI), as well as remote collaboration in several scenarios of application. To this effect, face-to-face and remote meetings were conducted, as well as focus group and brainstorm sessions (sometimes with different combinations of experts according to their availability) over several months. To conclude the section, a global assessment of the field maturity is attempted, followed by a critical analysis on how that may affect the road ahead. 4.1 Main limitations As was aforementioned, the characterization and evaluation of the collaborative process in remote scenarios using AR-based solutions have been reported mainly using single-user methods focusing on technological aspects, thus lacking information and focus on the important dimensions of collaboration. As a result the following main limitations can be identified. Limitation 1: partial evaluation According to Merino et al. “designing appropriate evaluations that examine MR/AR is challenging, and suitable guidance to design and conduct evaluations of MR/AR are largely missing” [37]. This fact is further evident in scenarios of remote collaboration, since the logistics associated with carrying out evaluations is even more demanding due to a significant number of variables that must be considered. The existence of two or more collaborators makes it more difficult to evaluate the solution as a whole, given that it requires to perform multiple evaluations at the same time and that validation from all users is required [40]. As a consequence, there is a clear lack in addressing crucial aspects of collaboration like how was the relation and communication of the collaborators during the tasks (only 10 out of 42 papers reported such information and just 6 recorded audio or video during the studies), whether they had previous experience with AR/VR/MR technologies and were able to use the available solutions to their full potential (a topic just mentioned by 11 out of 42 papers), how the available information was used to support the accomplishment of the tasks, among other aspects. In this context, trying to apply conventional evaluation techniques to collaborative settings, without adapting them can lead to an incomplete vision of the process of collaboration and in turn to dubious results, falling short to retrieve the necessary amount of data for more comprehensive evaluations and characterizations of the collaborative process which may lead to an incomplete vision of the process of collaboration. Given the complex environments and situations collaborators may encounter, such methods alone provide insufficient information and rarely are good indicators for improving distributed solutions [31,38,92,93]. Limitation 2: lack of contextual information Remote collaboration represents high levels of data by involving different types of distributed collaborators, tasks and in encompassing dynamical environments with contextual data. Dey et al. revealed that “work needs to be done towards making AR-based remote collaboration akin to the real world with not only shared understanding of the task but also shared understanding of the other collaborators emotional and physiological states” [43]. Moreover, Ratcliffe et al. suggested that “remote settings introduce additional uncontrolled variables that need to be considered by researchers, such as potential unknown distractions, trust in participants and their motivation, and issues with remote environmental spaces” [94]. However, our analysis shows that half of the papers analyzed (21 out of 42) did not described the experimental context of collaborators, and that 76.2% (32 out of 42) did not report participants knowledge of each other. The same percentage of papers did not mention previous experience with AR or MR technologies, as illustrated in Table 3. By doing this, evaluation scenarios disregard information such as contextual or user related data, obtaining only superficial results. Limitation 3: failure situations are not contemplated Bai et al. stated that: ”as deeper insight is obtained into the affordances of AR collaboration, more complex activities should be supported” [29]. This is also corroborated by Ens et al. which highlighted that “as new capabilities emerge, (...) we expect to see this trend continue, with an initial focus on perfecting the systems, followed by deeper explorations of collaboration” [14]. Furthermore, this is also supported by our analysis from the selected data set, which shows that failure situations were not taken into account by any study. For example, in the case of failure to achieve the intended goals of the collaborative process, how can we understand what went wrong? Was it caused by problems in participants communication, by too much augmented information being displayed, by the actions of a particular collaborator that did not followed correctly some indications, or was it caused by an error in the AR-based solution being used? Limitation 4: lack of theories and guidelines Literature shows an absence of rules, guidelines and theories to guide the characterization of the collaborative process using solutions mediated by AR. For example, Dey et al. suggests that “opportunities for increased user studies in collaboration, more use of field studies, and a wider range of evaluation methods” [43]. Moreover, Ens et al. reported that “MR systems faced significant engineering hurdles, and have only recently started catching up to provide new theories and lessons for collaboration” [14]. A better evaluation strategy is required by researchers and developers to obtain a comprehensive description, given the challenges involved in evaluating many aspects that may influence the way collaboration occurs, e.g., relations between individuals, their interconnection as a team and how the use of AR/MR technologies affected the accomplishment of the tasks in relation to the collaborative effort. Limitation 5: minimal support in existing frameworks The constraints and challenges identified may change according to the maturity of the solution being used, the goal of the evaluation, the participants individual and group characteristics, among other parameters. In this context, existing frameworks are not sufficiently well suited to describe how collaboration mediated by AR/MR technologies happens, thus ignoring detail on crucial aspects of collaboration [7,14,29,36,39,43]. For example, Bai et al. emphasized that “it can be hard to isolate the factors that are specifically relevant to collaboration” [29]. Likewise, Ens et al. outlined that “frameworks for describing groupware and MR systems are not sufficient to characterize how collaboration occurs through this new medium” [14]. In addition, Ratcliffe et al. communicate that “the infrastructure for collecting and storing this (mass) of XR data remotely is currently not fully implemented, and we are not aware of any end-to-end standardized framework” [94]. Therefore, integration of proper characterization and evaluation methods and guidelines, covering different contexts of use and tasks, running in its intended (real or simulated) environment are of paramount importance. Limitation 6: limited reporting of outcomes There is now an opportunity to convince researchers to better document their work, and help improve evaluations and characterizations that are, in our view, a bottleneck in this research area. Currently, researchers struggle to analyze the state of the art, since much information on existing publications lack detail on the collaborative process as previously demonstrated. This may happen since most of the research efforts have been devoted on creating the enabling technology. 4.2 Maturity of the field To put in perspective the evolution of the field, as well as consider current limitations, this section concludes with the analysis of the status of the area according to the BRETAM model (Fig. 2) [95]. This model has been considered useful for the introduction of new knowledge, technology or products and adopted in several scenarios, including for example, in a multimodal interaction review [96]. According to the current panorama reported in this publication, we argue that it is possible to situate the field of remote collaboration mediated by AR between the Replication and Empiricism phases of the BRETAM model as illustrated in Fig. 2. We argue that remote collaboration mediated by AR has already passed the Breakthrough phase, which means research institutions worldwide can replicate the basic concepts, as demonstrated by the last few decades of research [25]. The Replication and Empiricism phases on the other hand imply increased ideas to generate enough experience, leading to empirical design rules. As such, these phases seem adequate to the overall panorama described in this publication, reinforcing the need to deepen the understanding and characterization of the collaborative process through methods, frameworks, guidelines and various user studies. In our view, remote collaboration mediated by AR has still not reached the Theory phase as it requires enough empirical experience to model the basis of success and failure, which cannot be performed without proper methods for the characterization and evaluation of the collaborative process [14,25]. Likewise, the Automation phase was also rejected, which implies automation of the scientific data-gathering and analysis, since existing systems are still limited by the contextual and multi-user data they are able to collect, thus not being sufficient to characterize how collaboration occurs [14]. As such, without fulfilling the previous phases, the field cannot be positioned into the Maturity phase, i.e., turn to cost reduction and quality improvements in what describes a mature technology [95]. 5 Charting out a roadmap for the characterization and evaluation of the collaborative process According to what was said in the critical analysis, it is important to address the main limitations to make the field achieve the Theory, Automation and Maturity phases of the BRETAM model [95]. Aiming at contributing to that, in this section we propose a first roadmap, to deal with the most pressing issues (Fig. 3), composed by five key topics: • definition of dimensions of collaboration to face the partial characterization of the collaborative process; • systematization of perspectives based on the acquired knowledge of the field, facing the lack of theories and guidelines; • creation of new paradigms, architectures and frameworks to answer the limited support to development and evaluation of existing ones; • enhanced support for data gathering, leading to better design, development and evaluation with distributed users supported by AR; • new and better outcomes from the evaluation to support the assessment, leading to the creation of new theories, as well as improve the lack of contextual information. 5.1 Definition of dimensions of collaboration First, it is important to identify dimensions that need to be taken into consideration when performing the characterization of the collaborative process. In practical terms, given a concrete application context and a problem, the research community is still not able to provide an overall definition of the collaborative AR system that addresses it. Although there are works that have presented some dimensions of collaboration, existing efforts are mostly oriented towards technology. As the field matures, it is normal new proposals emerge to address new aspects related to collaboration. A comprehensive set of dimensions must be defined to more thoroughly classify and discuss the contributions of the collaborative work effort, not only addressing the technological features being used, but also encompassing the characteristics of the context. For example, Ens et al. stated the following: “While somewhat useful, the dimensions we used are fairly technical, and focus mainly on mechanical aspects of the system or properties of the underlying technologies. (...) Perhaps additional dimensions with a greater focus on user experience would better allow for capturing the essence of collaborative scenarios“ [14]. Therefore, some of the existing dimensions might still not reflect the full scope of some categories by encompassing all possibilities. Therefore, this effort cannot be intended as a closed work, but should, instead, be taken as the grounds that might enable the community to elaborate, expand, and refine the field. This may be achieved by analyzing the literature regarding collaborative work supported by AR, in particular, existing categorization efforts [13,14,25,45,98–101]. Another possibility is to adopt a conceptual-to-empirical methodology by using a participatory design process, i.e., actively involving stakeholders in focus group and brainstorming sections. This entails going beyond Collaborative AR literature, considering other domains (e.g., CSCW, Groupware, Telerehabilitation, Remote Medicine, among others) that may be relevant to characterize the collaborative effort, to identify which dimensions should be taken into account when we move from asking what existing systems can do, to understanding what they would be able to do in particular contexts, i.e., the value of AR to the collaborative process. 5.2 Systematization of perspectives for the field Ens et al. report that when considering if it is possible to clearly describe distinct categories of collaborative MR research based on the existing dimensions, the answer is “to some extent, yes, however the result is not wholly satisfying (...) these dimensions do not suffice to describe all scenarios” [14]. Therefore, another area of research that needs to be addressed given the lack of theories and guidelines [14,43], is the need to bring new dimensions forward into conceptual models, guidelines, taxonomies and ontologies, that might foster harmonization of perspectives for the field, thus creating a common ground for systematization and discussion [100,102]. Through these, it would be possible to structure the characterization of the collaboration process, which can form the basis for analysis and comparison, fostering a more detailed understanding of the field, and in turn ensure that the research adds to the body of knowledge and provides enough context and evidence to enable a transparent account [103] and transferability [104]. These can also work as a knowledge repository for evaluation, allowing researchers to observe and compare a variety of results inside the same domain and make considerations and conclusions about specific nuances of collaboration. For example, the proposal of human-centered approaches, i.e., focusing on collaboration, instead of the technology, might bring forward a perspective that is not rapidly deprecated with the advancements of technology [105]. To create conceptual models and taxonomies, it is important to ensure the dimensions of collaboration contain categories and characteristics that are mutually exclusive and collectively exhaustive [106,107]; Moreover, a detailed explanation of these objects of interest must be included, following, for example, a similar approach to the one used by Zollmann et al. [108]. It is also relevant to include discussion and refinement over several iterations with domain experts, to verify if the established dimensions, categories and characteristics are well defined, need to be merged, or if new ones can be identified [106]. Regarding ontologies, literature shows that its design is considered a creative process and no two ontologies by different individuals would be the same, since the applications of the ontology and the designer’s understanding of the domain will undoubtedly affect the ontology design choices [109,110]. As such, one possible strategy is to adapt existing ontologies when they exist, or as an alternative when this is not the case, define and populate a new ontology considering relevant dimensions of collaboration as the core classes and establish their relations with each other based on the targeted application of the ontology. 5.3 Creation of new paradigms, architectures and frameworks According to Merino et al. “as MR/AR technologies become more mature, questions that involve human aspects will gain focus in MR/AR research. Consequently, we expect that future MR/AR papers will elaborate on human-centered evaluations that involve not only the analysis of user performance and user experience, but also the analysis of other scenarios, like understanding the role of MR/AR in working places and in communication and collaboration” [37]. However, there is no standard methodology for characterization and evaluation, specifically tailored to assess how remote collaboration occurs through AR/MR technologies. In this vein, without the appropriate paradigms, methods and mechanisms, the research community does not accumulate enough experience to improve collaboration between distributed collaborators [7,14,29,36,37,39,43,92]. Currently, there is too much focus on post-task evaluation. New paradigms must also consider continuous assessment, i.e., giving proper relevance to evaluation conducted during the accomplishment of open challenges, instead of pre-defined tasks, which fail short to mimic real scenarios of remote collaboration. As such, architectures and frameworks capable of supporting the new paradigm(s) must be created, to assist researchers conducting future user studies, while eliciting more characterization of the collaboration process in remote scenarios. Such frameworks must include support for: • defining the evaluation scope for individual and collective assessment by properly identifying which dimensions of collaboration will be evaluated; • detailing collaborative challenges to be performed, including specification of the users minimum level of knowledge, definition of each collaborator activity, as well as definition of the procedures; • defining the experimental setup and design, ensuring each dimension is defined in terms of the necessary variables and how they should be measured according to specific techniques; • conducting data gathering through the use of a distributed evaluation tools focusing in the dimensions proposed specifically for remote collaboration; • performing data analysis, including inspection of what happened during the tasks, to understand how the collaboration process occurred over time. 5.4 Development of tools for improved data gathering According to Marques et al. “it is essential the existence of holistic evaluation strategies that monitor the use and performance of the proposed solutions regarding the team, its members, and the technology, allowing adequate characterization and report of collaborative processes” [36]. To achieve this, the operationalization of data gathering should also deserve its own line of work due to its importance. It is paramount to conduct thorough collaborative user studies to provide new perspectives [14,36–38,111]. A better evaluation process can be supported by improved data collection and data visualization tools [92,112]. In this context, it is necessary to collect, process and analyze a multiplicity of data, e.g., context, history, user related information like actions, emotional state, as well as the results of processing the various components of the data gathering tools, aiming at obtaining a more comprehensive understanding. To accomplish this, tools must be designed and developed to allow researchers to run multiple evaluations at different locations simultaneously, following a distributed paradigm [36]. In this process, researchers should be able define measures, custom logging and register interesting events they detect, which can be later reviewed in post-task analysis, adapting and extending, for example the works by Pereira et al. [93,113,114]. Likewise, the following factors are crucial and must be taken in account to better understand the real impact of each aspect in the collaborative effort: team, collaborative tasks, context and AR-based solution [36]. These factors can help portrait the conditions in which collaborators performed a given action, received information or requested assistance. In addition, they can be used to assert uncommon situations or identify patterns that can lead to new understanding of a given artifact, as well as identify new research questions. Therefore, such tools are essential to help researchers when performing judgment over evaluation results. 5.5 New and better outcomes to support the assessment A better characterization of the collaborative process coupled with improved and specific evaluation tools and methods will provide ground to improve how research is reported. Thus, increasing the awareness of researchers about the different dimensions of collaboration and elicit better reporting, as researchers understand the need to improve how they describe the nuances associated to the collaborative effort of their work. Currently, in most cases, data relevant to characterize the collaborative context is not reported. To elaborate, most works focus only on individual performance, on the technological aspects of the AR-based solution or in quantifying effectiveness of tasks. It is important to consider a wide range of information, namely individual and team personalities, motivations, performances, behaviors, who completed the tasks and who provided instructions, how was the communication process, as well as duration and type of interactions with the collaborative AR-based technology, among other aspects when analyzing data and establishing conclusions. The reporting process must also integrate the context in which the collaborative effort took place, thus allowing the creation of a better understanding of the surrounding conditions, while contributing to support replication of such context, if they are relevant to other researchers, in future studies. Moreover, a complete definition of the data used to substantiate the usefulness of the results reported must be included, as well as the measures used, how was the data computed, based on what criteria, etc. This is essential to move towards replication and interpretability across contributions in the field. A more systematic reporting can, in turn, lead to a community setting that enables easier communication, understanding, reflection, comparison, refining, as well as building on existing research and foster harmonization of perspectives for the field. Furthermore, researchers can also compare their outcomes, as this is also a good opportunity for reflecting and refining. It is important to use what is learned during the studies and identify aspects which did not go according to what was expected or select additional ones which may improve on existing guidelines for future user studies. 6 Conclusions and future work Collaborative AR solutions can be powerful tools for analysis, discussion and support of complex problems and situations in remote scenarios. By bringing different and sometimes opposing points of view together, such solutions can lead to new insights, innovative ideas, and interesting artifacts. However, most research efforts have been devoted to creating the enabling technology for supporting the design and development of such solutions. Hence, the characterization and evaluation of the collaborative process is an essential, but a very difficult endeavor nowadays. This paper describes a critical analysis supported by surveys that addressed evaluation and user studies in scenarios of remote collaboration mediated by AR. In addition, a literature review on works ranging from 2000 to 2020 is also presented. Based on the limitations and challenges identified, we argue that remote collaboration mediated by AR is currently between the Replication and Empiricism phases of the BRETAM model. To contribute to an advance to Theory, Automation and Maturity phases, based in the critical analysis, we propose a roadmap for important research actions that need to be addressed to facilitate and elicit more characterization of the collaboration process using AR-based solutions in the future. Work is being continued through the creation of a conceptual model and taxonomy, as well as an initial architecture and framework aligned with the proposed roadmap. These can form the basis for a common ground, as well as the development of a framework for researchers who want to follow best practices in designing their own collaborative AR user studies in remote scenarios. CRediT authorship contribution statement Bernardo Marques: Conceptualization, Methodology, Validation, Formal analysis, Investigation, Data curation, Writing – original draft, Writing – review & editing, Visualization, Project administration, Funding acquisition. António Teixeira: Conceptualization, Methodology, Writing – original draft, Writing – review, Visualization. Samuel Silva: Methodology, Investigation, Writing – original draft, Writing – review & editing. João Alves: Validation, Formal analysis, Investigation, Data curation, Writing – original draft. Paulo Dias: Conceptualization, Investigation, Data curation, Writing – original draft, Writing – review & editing, Supervision, Funding acquisition. Beatriz Sousa Santos: Conceptualization, Methodology, Investigation, Writing – original draft, Writing – review & editing, Supervision Funding acquisition. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise. This research was developed in the scope of the Ph.D. grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT , in the context of the project [UID/CEC/00127/2019]. References [1] Alem L. Tecchia F. Huang W. HandsOnVideo: Towards a gesture based mobile AR system for remote collaboration Recent Trends of Mobile Collaborative Augmented Reality Systems 2011 135 148 Alem L, Tecchia F, Huang W. Handsonvideo: Towards a gesture based mobile ar system for remote collaboration. In: Alem L, Huang W, editors. Recent Trends of Mobile Collaborative Augmented Reality Systems.2011, p. 135–148,. [2] Johnson S, Gibson M, Mutlu B. Handheld or handsfree? Remote collaboration via lightweight head-mounted displays and handheld devices. In: Proceedings of the 18th ACM conference on computer supported cooperative work & social computing; 2015, p. 1825–36. [3] Lukosch S. Billinghurst M. Alem L. Kiyokawa K. Collaboration in augmented reality Computer Supported Cooperative Work, CSCW 2015 2015 515 525 Lukosch S, Billinghurst M, Alem L, Kiyokawa K. Collaboration in Augmented Reality. In: Computer Supported Cooperative Work, CSCW 2015 vol. 24.2015, p. 515–525,. [4] Schneider M, Rambach J, Stricker D. Augmented reality based on edge computing using the example of remote live support. In: 2017 IEEE international conference on industrial technology; 2017, p. 1277–82. [5] Thomas P.J. CSCW Requirements and Evaluation 1996 Springer-Verlag Berlin, Heidelberg Thomas PJ. CSCW Requirements and Evaluation. Berlin, Heidelberg: Springer-Verlag;1996. [6] Kim K. Billinghurst M. Bruder G. Duh H.B. Welch G.F. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017) IEEE Trans Vis Comput Graphics 24 2018 2947 2962 Kim K, Billinghurst M, Bruder G, Duh HB, Welch GF. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017). IEEE Transactions on Visualization and Computer Graphics2018a;24:2947–2962,. [7] Kim S. Billinghurst M. Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration Comput Support Coop Work: CSCW: An Int J 27 3–6 2018 569 607 Kim S, Billinghurst M, Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal2018b;27(3-6):569–607,. [8] Kim S. Lee G. Billinghurst M. Huang W. The combination of visual communication cues in mixed reality remote collaboration J Multimodal User Interf 14 4 2020 321 335 Kim S, Lee G, Billinghurst M, Huang W. The combination of visual communication cues in mixed reality remote collaboration. Journal on Multimodal User Interfaces2020a;14(4):321–335,. [9] Kim S. Billinghurst M. Kim K. Multimodal interfaces and communication cues for remote collaboration J Multimodal User Interf 14 4 2020 313 319 Kim S, Billinghurst M, Kim K. Multimodal interfaces and communication cues for remote collaboration. Journal on Multimodal User Interfaces2020b;14(4):313–319,. [10] Arias E. Eden H. Fischer G. Gorman A. Scharff E. Transcending the individual human mind-creating shared understanding through collaborative design ACM Trans Comput-Human Inter 7 2000 84 113 Arias E, Eden H, Fischer G, Gorman A, Scharff E. Transcending the Individual Human Mind-Creating Shared Understanding through Collaborative Design. ACM Transactions on Computer-Human Interaction2000;7:84–113,. [11] Grudin J. Poltrock S. The Encyclopedia of Human-Computer Interaction 2013 The Interaction Design Foundation Grudin J, Poltrock S. The Encyclopedia of Human-Computer Interaction. The Interaction Design Foundation;2013. [12] Billinghurst M. Clark A. Lee G. A survey of augmented reality Found Trends Human–Comput Interact 8 2015 73 272 Billinghurst M, Clark A, Lee G. A Survey of Augmented Reality. Foundations and Trends in Human–Computer Interaction2015;8:73–272,. [13] Jalo H, Pirkkalainen H, Torro O, Kärkkäinen H, Puhto J, Kankaanpää T. How can collaborative augmented reality support operative work in the facility management industry?. In: Proceedings of the 10th international joint conference on knowledge discovery, knowledge engineering and knowledge management; 2018. p. 41–51. [14] Ens B. Lanir J. Tang A. Bateman S. Lee G. Piumsomboon T. Billinghurst M. Revisiting collaboration through mixed reality: The evolution of groupware Int J Human-Comput Stud 131 2019 81 98 Ens B, Lanir J, Tang A, Bateman S, Lee G, Piumsomboon T,, others,Revisiting Collaboration through Mixed Reality: The Evolution of Groupware. International Journal of Human-Computer Studies2019;131:81–98,. [15] Bruno F. Barbieri L. Marino E. Muzzupappa M. D’Oriano L. Colacino B. An augmented reality tool to detect and annotate design variations in an industry 4.0 approach Int J Adv Manuf Technol 105 1 2019 875 887 Bruno F, Barbieri L, Marino E, Muzzupappa M, D’Oriano L, Colacino B. An augmented reality tool to detect and annotate design variations in an Industry 4.0 approach. The International Journal of Advanced Manufacturing Technology2019;105(1):875–887,. [16] Ong S.K. Yuan M.L. Nee A.Y.C. Augmented reality applications in manufacturing: A survey Int J Prod Res 46 10 2008 2707 2742 Ong SK, Yuan ML, Nee AYC. Augmented reality applications in manufacturing: A survey. International Journal of Production Research2008;46(10):2707–2742,. [17] Wang X. Ong S.K. Nee A.Y.C. A comprehensive survey of augmented reality assembly research Adv Manuf 4 1 2016 1 22 Wang X, Ong SK, Nee AYC. A comprehensive survey of augmented reality assembly research. Advances in Manufacturing2016;4(1):1–22,. [18] Palmarini R. Erkoyuncu J.A. Roy R. Torabmostaedi H. A systematic review of augmented reality applications in maintenance Robot Comput-Integr Manuf 49 2018 215 228 Palmarini R, Erkoyuncu JA, Roy R, Torabmostaedi H. A systematic review of augmented reality applications in maintenance. Robotics and Computer-Integrated Manufacturing2018;49:215–228,. [19] Hall M, McMahon CA, Bermell-Garcia P, Johansson A, Ravindranath R. Capturing synchronous collaborative design activities: A state-of-the-art technology review. In: Proceedings of international design conference, DESIGN 2018; 2018. p. 347–58. [20] Billinghurst M. Grand challenges for augmented reality Front Virtual Reality 2 2021 1 4 Billinghurst M. Grand Challenges for Augmented Reality. Frontiers in Virtual Reality2021;2:1–4,. [21] Altug Y. Mahdy A.M. A perspective on distributed and collaborative augmented reality Int J Recent Trends Human Comput Interact (IJHCI) 7 2016 23 41 Altug Y, Mahdy AM. A Perspective on Distributed and Collaborative Augmented Reality. International Journal of Recent Trends in Human Computer Interaction (IJHCI)2016;7:23–41,. [22] Choi S. Kim M. Lee J. Situation-dependent remote AR collaborations: Image-based collaboration using a 3D perspective map and live video-based collaboration with a synchronized VR mode Comput Ind 101 2018 51 66 Choi S, Kim M, Lee J. Situation-dependent remote AR collaborations: Image-based collaboration using a 3D perspective map and live video-based collaboration with a synchronized VR mode. Computers in Industry2018;101:51–66,. [23] Bottani E. Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade IISE Trans 51 3 2019 284 310 Bottani E, Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade. IISE Transactions2019;51(3):284–310,. [24] van Lopik K. Sinclair M. Sharpe R. Conway P. West A. Developing augmented reality capabilities for industry 4.0 small enterprises: Lessons learnt from a content authoring case study Comput Ind 117 2020 van Lopik K, Sinclair M, Sharpe R, Conway P, West A. Developing augmented reality capabilities for industry 4.0 small enterprises: Lessons learnt from a content authoring case study. Computers in Industry2020;117. [25] Belen R.A.J. Nguyen H. Filonik D. Favero D.D. Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018 AIMS Electron Electr Eng 3 2019 181 Belen RAJ, Nguyen H, Filonik D, Favero DD, Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018. AIMS Electronics and Electrical Engineering2019;3:181,. [26] Oda O, Elvezio C, Sukan M, Feiner S, Tversky B. Virtual replicas for remote assistance in virtual and augmented reality. In: Proceedings of the 28th annual acm symposium on user interface software & technology - UIST ’15; 2015. p. 405–15. [27] Elvezio C. Sukan M. Oda O. Feiner S. Tversky B. Remote collaboration in AR and VR using virtual replicas ACM SIGGRAPH 2017 2017 1 2 Elvezio C, Sukan M, Oda O, Feiner S, Tversky B. Remote collaboration in AR and VR using virtual replicas. ACM SIGGRAPH 20172017;:1–2,. [28] Barroso Ja, Fonseca L, Marques B, Dias P, Sousa BS. Remote collaboration using mixed reality: Exploring a shared model approach through different interaction methods. In: Proceedings of european conference on computer-supported cooperative work, ecscw 2020 posters; 2020. p. 1–6. [29] Bai Z. Blackwell A.F. Analytic review of usability evaluation in ISMAR Interact Comput 24 6 2012 450 460 Bai Z, Blackwell AF. Analytic review of usability evaluation in ISMAR. Interacting with Computers2012;24(6):450 – 460,. [30] Zillner J, Mendez E, Wagner D. Augmented reality remote collaboration with dense reconstruction. In: Adjunct proceedings - 2018 IEEE international symposium on mixed and augmented reality, ismar-adjunct 2018; 2018. p. 38–9. [31] Neale DC, Carroll JM, Rosson MB. Evaluating computer-supported cooperative work: Models and frameworks. In: Proceedings of the 2004 ACM conference on computer supported cooperative work, CSCW ’04; 2004. p. 112–21. [32] de Souza Cardoso L.F. Mariano F.C.M.Q. Zorzal E.R. A survey of industrial augmented reality Comput Ind Eng 139 2020 106159 de Souza Cardoso LF, Mariano FCMQ, Zorzal ER. A survey of industrial augmented reality. Computers & Industrial Engineering2020;139:106159,. [33] Röltgen D. Dumitrescu R. Classification of industrial augmented reality use cases Procedia CIRP 91 2020 93 100 Röltgen D, Dumitrescu R. Classification of industrial augmented reality use cases. Procedia CIRP2020;91:93 – 100,. [34] Fernández del Amo I. Erkoyuncu J.A. Roy R. Wilding S. Augmented reality in maintenance: An information-centred design framework Proc Manuf 19 2018 148 155 Fernández del Amo I, Erkoyuncu JA, Roy R, Wilding S. Augmented reality in maintenance: An information-centred design framework. Procedia Manufacturing2018;19:148 – 155,. [35] Jetter J. Eimecke J. Rese A. Augmented reality tools for industrial applications: What are potential key performance indicators and who benefits? Comput Hum Behav 87 2018 18 33 Jetter J, Eimecke J, Rese A. Augmented reality tools for industrial applications: What are potential key performance indicators and who benefits? Computers in Human Behavior2018;87:18–33,. [36] Marques B, Teixeira A, Silva S, Alves Ja, Dias P, Santos BS. A conceptual model for data collection and analysis for AR-based remote collaboration evaluation. In: IEEE international symposium on mixed and augmented reality, ISMAR; 2020. [37] Merino L, Schwarzl M, Kraus M, Sedlmair M, Schmalstieg D, Weiskopf D. Evaluating mixed and augmented reality: A systematic literature review (2009–2019). In: IEEE international symposium on mixed and augmented reality, ISMAR; 2020. p. 438–51. [38] Hamadache K. Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation Groupware: Design, Implementation, and Use 2009 206 221 Hamadache K, Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation. In: Groupware: Design, Implementation, and Use.2009, p. 206–221,. [39] Antunes P. Herskovic V. Ochoa S.F. Pino J.A. Reviewing the quality of awareness support in collaborative applications J Syst Softw 89 2014 146 169 Antunes P, Herskovic V, Ochoa SF, Pino JA. Reviewing the quality of awareness support in collaborative applications. Journal of Systems and Software2014;89:146–169,. [40] Patel H. Pettitt M. Wilson J.R. Factors of collaborative working: A framework for a collaboration model Applied Ergon 43 1 2012 1 26 Patel H, Pettitt M, Wilson JR. Factors of collaborative working: A framework for a collaboration model. Applied ergonomics2012;43(1):1–26,. [41] Duenser A. Grasset R. Billinghurst M. A survey of evaluation techniques used in augmented reality studies SIGGRAPH 2008 2008 Duenser A, Grasset R, Billinghurst M. A survey of evaluation techniques used in augmented reality studies. In: SIGGRAPH 2008.2008,. [42] Feng Zhou, Duh HB, Billinghurst M. Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR. In: International symposium on mixed and augmented reality; 2008. p. 193–202. [43] Dey A. Billinghurst M. Lindeman R.W. Swan J.E. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014 Front Robot AI 5 2018 37 Dey A, Billinghurst M, Lindeman RW, Swan JE. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014. Frontiers in Robotics and AI2018;5:37,. [44] Gutwin C. Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware ACM Trans Comput-Hum Interact 6 3 1999 243 281 Gutwin C, Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Trans Comput-Hum Interact1999;6(3):243–281,. [45] Speicher M. Hall B.D. Nebeling M. What is mixed reality? Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems 2019 1 15 Speicher M, Hall BD, Nebeling M. What is mixed reality? In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. CHI ’19;2019, p. 1–15,. [46] Kato H, Billinghurst M. Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings 2nd IEEE and ACM international workshop on augmented reality (IWAR’99); 1999. p. 85–94. [47] Milgram P. Takemura H. Utsumi A. Kishino F. Augmented reality: A class of displays on the reality-virtuality continuum Photon Indust Appl 1994 282 292 Milgram P, Takemura H, Utsumi A, Kishino F. Augmented reality: A class of displays on the reality-virtuality continuum. Photonics for industrial applications1994;:282–292,. [48] Milgram P. Kishino F. A taxonomy of mixed reality visual displays IEICE Trans Inform Syst 0916-8532 77 12 1994 1321 1329 Milgram P, Kishino F. A taxonomy of mixed reality visual displays. IEICE TRANSACTIONS on Information and Systems1994;77(12):1321–1329,. [49] Rokhsaritalemi S. Sadeghi-Niaraki A. Choi S.-M. A review on mixed reality: Current trends, challenges and prospects Appl Sci 10 2 2020 Rokhsaritalemi S, Sadeghi-Niaraki A, Choi SM. A review on mixed reality: Current trends, challenges and prospects. Applied Sciences2020;10(2). [50] Bai H. Sasikumar P. Yang J. Billinghurst M. A user study on mixed reality remote collaboration with eye gaze and hand gesture sharing Conf Human Factors Comput Syst - Proc 2020 1 13 Bai H, Sasikumar P, Yang J, Billinghurst M. A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing. Conference on Human Factors in Computing Systems - Proceedings2020;:1–13,. [51] Masai K, Kunze K, Sugimoto M, Billinghurst M. Empathy glasses. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems, CHI EA ’16; 2016. p. 1257–63. [52] Piumsomboon T. Dey A. Ens B. Lee G. Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality Front Robot AI 6 2019 Piumsomboon T, Dey A, Ens B, Lee G, Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality. Frontiers Robotics AI2019a;6. [53] Wang P. Bai X. Billinghurst M. Zhang S. Han D. Sun M. Wang Z. Lv H. Han S. Haptic feedback helps me? A VR-SAR remote collaborative system with tangible interaction Int J Human-Comput Interact 2020 Wang P, Bai X, Billinghurst M, Zhang S, Han D, Sun M,, others,Haptic feedback helps me? a VR-SAR remote collaborative system with tangible interaction. International Journal of Human-Computer Interaction2020;. [54] Rhee T. Thompson S. Medeiros D. Dos Anjos R. Chalmers A. Augmented virtual teleportation for high-fidelity telecollaboration IEEE Trans Vis Comput Graphics 2020 Rhee T, Thompson S, Medeiros D, Dos Anjos R, Chalmers A. Augmented virtual teleportation for high-fidelity telecollaboration. IEEE Transactions on Visualization and Computer Graphics2020;. [55] Teo T, Lee G, Billinghurst M, Adcock M. Investigating the use of different visual cues to improve social presence within a 360 mixed reality remote collaboration. In: Proceedings - VRCAI 2019: 17th ACM siggraph international conference on virtual-reality continuum and its applications in industry; 2019. [56] Teo T, Hayati A, Lee G, Billinghurst M, Adcock M. A technique for mixed reality remote collaboration using 360 panoramas in 3D reconstructed scenes. In: Proceedings of the ACM symposium on virtual reality software and technology, VRST; 2019. [57] Sasikumar P, Gao L, Bai H, Billinghurst M. Wearable remotefusion: A mixed reality remote collaboration system with local eye gaze and remote hand gesture sharing. In: Adjunct Proceedings of the 2019 IEEE international symposium on mixed and augmented reality, ismar-adjunct 2019; 2019. p. 393–4. [58] Mahmood T, Fulmer W, Mungoli N, Huang J, Lu A. Improving information sharing and collaborative analysis for remote geospatial visualization using mixed reality. In: Proceedings - 2019 IEEE international symposium on mixed and augmented reality, ISMAR 2019; 2019. p. 236–47. [59] Teo T. Lawrence L. Lee G. Billinghurst M. Adcock M. Mixed reality remote collaboration combining 360 video and 3D reconstruction Conf Human Factors Comput Syst 2019 Teo T, Lawrence L, Lee G, Billinghurst M, Adcock M. Mixed reality remote collaboration combining 360 video and 3D reconstruction. Conference on Human Factors in Computing Systems2019c;. [60] Piumsomboon T. Lee G. Irlitti A. Ens B. Thomas B. Billinghurst M. On the shoulder of the giant: A multi-scale mixed reality collaboration with 360 video sharing and tangible interaction Conf Human Factors Comput Syst 2019 Piumsomboon T, Lee G, Irlitti A, Ens B, Thomas B, Billinghurst M. On the shoulder of the giant: A multi-scale mixed reality collaboration with 360 video sharing and tangible interaction. Conference on Human Factors in Computing Systems2019b;. [61] Yoon B, Kim H-I, Lee G, Billinqhurst M, Woo W. The effect of avatar appearance on social presence in an augmented reality remote collaboration. In: 26th IEEE conference on virtual reality and 3d user interfaces, vr 2019 - proceedings; 2019. p. 547–56. [62] Wang P. Zhang S. Billinghurst M. Bai X. He W. Wang S. Sun M. Zhang X. A comprehensive survey of AR/MR-based co-design in manufacturing Eng Comput 2019 Wang P, Zhang S, Billinghurst M, Bai X, He W, Wang S,, others,A comprehensive survey of AR/MR-based co-design in manufacturing. Engineering with Computers2019;. [63] Lee G, Teo T, Kim S, Billinghurst M. A user study on MR remote collaboration using live 360 video. In: Proceedings of the 2018 ieee international symposium on mixed and augmented reality, ISMAR 2018; 2019. p. 153–64. [64] Waldow K. Fuhrmann A. Grünvogel S. Investigating the effect of embodied visualization in remote collaborative augmented reality Lecture Notes in Comput Sci 11883 LNCS 2019 246 262 Waldow K, Fuhrmann A, Grünvogel S. Investigating the effect of embodied visualization in remote collaborative augmented reality. Lecture Notes in Computer Science2019;11883 LNCS:246–262,. [65] Teo T. Lee G. Billinghurst M. Adcock M. Hand gestures and visual annotation in live 360 panorama-based mixed reality remote collaboration ACM Int Conf Proc Ser 2018 406 410 Teo T, Lee G, Billinghurst M, Adcock M. Hand gestures and visual annotation in live 360 panorama-based mixed reality remote collaboration. ACM International Conference Proceeding Series2018;:406–410,. [66] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration KSII Trans Internet Inform Syst 12 12 2018 6034 6056 Kim S, Billinghurst M, Lee C, Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet and Information Systems2018c;12(12):6034–6056,. [67] Congdon B, Wang T, Steed A. Merging environments for shared spaces in mixed reality. In: Proceedings of the ACM symposium on virtual reality software and technology, VRST; 2018. [68] Yamada S, Chandrasiri N. Evaluation of hand gesture annotation in remote collaboration using augmented reality. In: 25th IEEE conference on virtual reality and 3d user interfaces, vr 2018 - proceedings; 2018. p. 727–8. [69] Günther S. Avrahami D. Kratz S. Mühlhäuser M. Exploring audio, visual, and tactile cues for synchronous remote assistance ACM Int Conf Proc Ser 2018 339 344 Günther S, Avrahami D, Kratz S, Mühlhäuser M. Exploring audio, visual, and tactile cues for synchronous remote assistance. ACM International Conference Proceeding Series2018;:339–344,. [70] Piumsomboon T. Lee G. Hart J. Ens B. Lindeman R. Thomas B. Billinghurst M. Mini-me: An adaptive avatar for mixed reality remote collaboration Conf Human Factors Comput Syst 2018-April 2018 Piumsomboon T, Lee G, Hart J, Ens B, Lindeman R, Thomas B,, others,Mini-me: An adaptive avatar for mixed reality remote collaboration. Conference on Human Factors in Computing Systems2018;2018-April. [71] Ryskeldiev B. Cohen M. Herder J. Stream space: Pervasive mixed reality telepresence for remote collaboration on mobile devices J Inform Proc 26 2018 177 185 Ryskeldiev B, Cohen M, Herder J. Stream space: Pervasive mixed reality telepresence for remote collaboration on mobile devices. Journal of Information Processing2018;26:177–185,. [72] Hoppe A. Reeb R. van de Camp F. Stiefelhagen R. Interaction of distant and local users in a collaborative virtual environment Lecture Notes in Comput Sci 10909 LNCS 2018 328 337 Hoppe A, Reeb R, van de Camp F, Stiefelhagen R. Interaction of distant and local users in a collaborative virtual environment. Lecture Notes in Computer Science2018;10909 LNCS:328–337,. [73] Akkil D. Isokoski P. Comparison of gaze and mouse pointers for video-based collaborative physical task Interact Comput 30 6 2018 524 542 Akkil D, Isokoski P. Comparison of gaze and mouse pointers for video-based collaborative physical task. Interacting with Computers2018;30(6):524–542,. [74] Lee G. Teo T. Kim S. Billinghurst M. Mixed reality collaboration through sharing a live panorama SIGGRAPH Asia 2017 Mobile Graphics and Interactive Applications 2017 Lee G, Teo T, Kim S, Billinghurst M. Mixed reality collaboration through sharing a live panorama. SIGGRAPH Asia 2017 Mobile Graphics and Interactive Applications2017a;. [75] Lee G, Kim S, Lee Y, Dey A, Piumsomboon T, Norman M, Billinghurst M. Mutually shared gaze in augmented video conference. In: Adjunct proceedings of the 2017 ieee international symposium on mixed and augmented reality, ismar-adjunct 2017; 2017. p. 79–80. [76] Komiyama R. Miyaki T. Rekimoto J. Jackin space: Designing a seamless transition between first and third person view for effective telepresence collaborations ACM Int Conf Proc Ser 2017 Komiyama R, Miyaki T, Rekimoto J. Jackin space: Designing a seamless transition between first and third person view for effective telepresence collaborations. ACM International Conference Proceeding Series2017;. [77] Chenechal M, Duval T, Gouranton V, Royan J, Arnaldi B. Vishnu: Virtual immersive support for HelpiNg users an interaction paradigm for collaborative remote guiding in mixed reality. In: 2016 IEEE 3rd vr international workshop on collaborative virtual environments, 3dcve 2016; 2016. p. 9–12. [78] Gurevich P. Lanir J. Cohen B. Design and implementation of TeleAdvisor: a projection-based augmented reality system for remote collaboration Comput Support Coop Work: CSCW: An Int J 24 6 2015 527 562 Gurevich P, Lanir J, Cohen B. Design and implementation of teleadvisor: a projection-based augmented reality system for remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal2015;24(6):527–562,. [79] Tait M. Billinghurst M. The effect of view independence in a collaborative AR system Comput Support Coop Work: CSCW: An Int J 24 6 2015 563 589 Tait M, Billinghurst M. The effect of view independence in a collaborative AR system. Computer Supported Cooperative Work: CSCW: An International Journal2015;24(6):563–589,. [80] Kim S. Lee G. Ha S. Sakata N. Billinghurst M. Automatically freezing live video for annotation during remote collaboration Conf Human Factors Comput Syst 18 2015 1669 1674 Kim S, Lee G, Ha S, Sakata N, Billinghurst M. Automatically freezing live video for annotation during remote collaboration. Conference on Human Factors in Computing Systems2015;18:1669–1674,. [81] Tait M, Billinghurst M. View independence in remote collaboration using AR. In: ISMAR 2014 - IEEE international symposium on mixed and augmented reality - science and technology 2014, proceedings; 2014. p. 309–10. [82] Kim S, Lee G, Sakata N, Billinghurst M. Improving co-presence with augmented visual communication cues for sharing experience through video conference. In: ISMAR 2014 - IEEE international symposium on mixed and augmented reality - science and technology 2014, proceedings; 2014. p. 83–92. [83] Gauglitz S, Nuernberger B, Turk M, Höllerer T. In touch with the remote world: Remote collaboration with augmented reality drawings and virtual navigation. In: Proceedings of the ACM symposium on virtual reality software and technology, vrst; 2014. p. 197–205. [84] Gauglitz S, Nuernberger B, Turk M, Höllerer T. World-stabilized annotations and virtual scene navigation for remote collaboration. In: UIST 2014 - Proceedings of the 27th annual acm symposium on user interface software and technology; 2014. p. 449–60. [85] Huang W. Alem L. Tecchia F. HandsIn3D: Supporting remote guidance with immersive virtual environments Lecture Notes in Comput Sci 8117 LNCS PART 1 2013 70 77 Huang W, Alem L, Tecchia F. Handsin3d: Supporting remote guidance with immersive virtual environments. Lecture Notes in Computer Science2013;8117 LNCS(PART 1):70–77,. [86] Pece F. Steptoe W. Wanner F. Julier S. Weyrich T. Kautz J. Steed A. PanoInserts: Mobile spatial teleconferencing Conf Human Factors Comput Syst 2013 1319 1328 Pece F, Steptoe W, Wanner F, Julier S, Weyrich T, Kautz J,, others,Panoinserts: Mobile spatial teleconferencing. Conference on Human Factors in Computing Systems2013;:1319–1328,. [87] Poppe E, Brown R, Johnson D, Recker J. Preliminary evaluation of an augmented reality collaborative process modelling system. In: Proceedings of the 2012 international conference on cyberworlds; 2012. p. 77–84. [88] Gauglitz S, Lee C, Turk M, Höllerer T. Integrating the physical environment into mobile remote collaboration. In: MobileHCI’12 - proceedings of the 14th international conference on human computer interaction with mobile devices and services; 2012. p. 241–50. [89] Barakonyi I, Prendinger H, Schmalstieg D, Ishizuka M. Cascading hand and eye movement for augmented reality videoconferencing. In: IEEE symposium on 3D user interfaces, 3dui 2007; 2007. p. 71–78. [90] Bannai Y. Tamaki H. Suzuki Y. Shigeno H. Okada K. A tangible user interface for remote collaboration system using mixed reality Lecture Notes in Comput Sci 4282 2006 143 154 Bannai Y, Tamaki H, Suzuki Y, Shigeno H, Okada K. A tangible user interface for remote collaboration system using mixed reality. Lecture Notes in Computer Science2006;4282:143–154,. [91] Regenbrecht H. Lum T. Kohler P. Ott C. Wagner M. Wilke W. Mueller E. Using augmented virtuality for remote collaboration Presence: Teleoperat Virtual Environ 13 3 2004 338 354 Regenbrecht H, Lum T, Kohler P, Ott C, Wagner M, Wilke W,, others,Using augmented virtuality for remote collaboration. Presence: Teleoperators and Virtual Environments2004;13(3):338–354,. [92] Araujo R.M. Santoro F.M. Borges M.R.S. A conceptual framework for designing and conducting groupware evaluations Int J Comput Appl Technol 19 3 2004 139 150 Araujo RM, Santoro FM, Borges MRS. A conceptual framework for designing and conducting groupware evaluations. International Journal of Computer Applications in Technology2004;19(3):139–150,. [93] Pereira C, Teixeira A, e Silva MO. Live evaluation within ambient assisted living scenarios. In: Proceedings of the 7th international conference on pervasive technologies related to assistive environments, PETRA 2014; 2014. [94] Ratcliffe J. Soave F. Bryan-Kinns N. Tokarchuk L. Farkhatdinov I. Extended reality (XR) remote research: a survey of drawbacks and opportunities CHI Conference on Human Factors in Computing Systems 2021 1 13 Ratcliffe J, Soave F, Bryan-Kinns N, Tokarchuk L, Farkhatdinov I. Extended Reality (XR) Remote Research: a Survey of Drawbacks and Opportunities. In: CHI Conference on Human Factors in Computing Systems.2021, p. 1–13,. [95] Gaines B.R. Modeling and forecasting the information sciences Inform Sci 57–58 1991 3 22 Information Sciences-Past, Present, and Future Gaines BR. Modeling and forecasting the information sciences. Information Sciences1991;57-58:3 – 22,. Information Sciences-Past, Present, and Future. [96] Lalanne D. Nigay L. Palanque p. Robinson P. Vanderdonckt J. Ladry J.-F.c. Fusion engines for multimodal input: A survey Proceedings of the 2009 International Conference on Multimodal Interfaces ICMI-MLMI ’09 2009 Association for Computing Machinery New York, NY, USA 153 160 Lalanne D, Nigay L, Palanque p, Robinson P, Vanderdonckt J, Ladry JF. Fusion engines for multimodal input: A survey. In: Proceedings of the 2009 International Conference on Multimodal Interfaces. ICMI-MLMI ’09; New York, NY, USA: Association for Computing Machinery;2009, p. 153–160,. [97] Teixeira A. A critical analysis of speech-based interaction in healthcare robots: Making a case for the increased use of speech in medical and assistive robots Speech and Automata in Healthcare : Voice-Controlled Medical and Surgical Robots - Chapter 1 2014 29 Teixeira A. A critical analysis of speech-based interaction in healthcare robots: Making a case for the increased use of speech in medical and assistive robots. In: Speech and Automata in Healthcare : Voice-Controlled Medical and Surgical Robots - Chapter 1.2014, p. 29,. [98] Billinghurst M. Kato H. Collaborative augmented reality Commun ACM 45 2003 Billinghurst M, Kato H. Collaborative augmented reality. Communications of the ACM2003;45. [99] Wang X, Dunston PS. Groupware concepts for augmented reality mediated human-to-human collaboration. In: Joint international conference on computing and decision making in civil and building engineering; 2006. p. 1836–42. [100] Brockmann T, Krueger N, Stieglitz S, Bohlsen I. A framework for collaborative augmented reality applications. In: Proceedings of the nineteenth americas conference on information systems; 2013, p. 1–10. [101] Sereno M. Wang X. Besancon L. Mcguffin M.J. Isenberg T. Collaborative work in augmented reality: A survey IEEE Trans Vis Comput Graphics 2020 1 20 Sereno M, Wang X, Besancon L, Mcguffin MJ, Isenberg T. Collaborative work in augmented reality: A survey. IEEE Transactions on Visualization and Computer Graphics2020;:1–20,. [102] Collazos C.A. Gutiérrez F.L. Gallardo J. Ortega M. Fardoun H.M. Molina A.I. Descriptive theory of awareness for groupware development J Ambient Intell Humaniz Comput 10 12 2019 4789 4818 Collazos CA, Gutiérrez FL, Gallardo J, Ortega M, Fardoun HM, Molina AI. Descriptive theory of awareness for groupware development. Journal of Ambient Intelligence and Humanized Computing2019;10(12):4789–4818,. [103] Talkad Sukumar P, Avellino I, Remy C, DeVito MA, Dillahunt TR, McGrenere J, Wilson ML. Transparency in qualitative research: Increasing fairness in the chi review process. In: Extended abstracts of the 2020 chi conference on human factors in computing systems; 2020. p. 1–6. [104] Meyer M. Dykes J. Criteria for rigor in visualization design study IEEE Trans Vis Comput Graphics 26 1 2019 87 97 Meyer M, Dykes J. Criteria for rigor in visualization design study. IEEE transactions on visualization and computer graphics2019;26(1):87–97,. [105] Augstein M. Neumayr T. A human-centered taxonomy of interaction modalities and devices Interact Comput 31 2019 27 58 Augstein M, Neumayr T. A human-centered taxonomy of interaction modalities and devices. Interacting with Computers2019;31:27–58,. [106] Nickerson R.C. Varshney U. Muntermann J. A method for taxonomy development and its application in information systems Eur J Inform Syst 22 2013 336 359 Nickerson RC, Varshney U, Muntermann J. A method for taxonomy development and its application in information systems. European Journal of Information Systems2013;22:336–359,. [107] Teruel M.A. Navarro E. López-Jaquero V. Montero F. González P. A comprehensive framework for modeling requirements of CSCW systems J Softw: Evolu Process 29 5 2017 e1858 Teruel MA, Navarro E, López-Jaquero V, Montero F, González P. A comprehensive framework for modeling requirements of cscw systems. Journal of Software: Evolution and Process2017;29(5):e1858,. [108] Zollmann S. Grasset R. Langlotz T. Lo W.H. Mori S. Regenbrecht H. Visualization techniques in augmented reality: A taxonomy, methods and patterns IEEE Trans Vis Comput Graphics 2020 1 20 Zollmann S, Grasset R, Langlotz T, Lo WH, Mori S, Regenbrecht H. Visualization techniques in augmented reality: A taxonomy, methods and patterns. IEEE Transactions on Visualization and Computer Graphics2020;:1–20,. [109] Chandrasekaran B. Josephson J.R. Benjamins V.R. What are ontologies, and why do we need them? IEEE Intell Syst Appl 14 1 1999 20 26 Chandrasekaran B, Josephson JR, Benjamins VR. What are ontologies, and why do we need them? IEEE Intelligent Systems and Their Applications1999;14(1):20–26,. [110] Noy N.F. McGuinness D.L. Ontology development 101: A guide to creating your first ontology Stanford Knowl Syst Lab Tech Rep 15 2 2001 1 25 Noy NF, McGuinness DL. Ontology Development 101: A Guide to Creating Your First Ontology. Stanford Knowledge Systems Laboratory Technical Report2001;15(2):1–25,. [111] Herskovic V. Pino J.A. Ochoa S.F. Antunes P. Evaluation methods for groupware systems Haake J.M. Ochoa S.F. Cechich A. Groupware: Design, Implementation, and Use 2007 Springer, Berlin, Heidelberg 328 336 Herskovic V, Pino JA, Ochoa SF, Antunes P. Evaluation methods for groupware systems. In: Haake JM, Ochoa SF, Cechich A, editors. Groupware: Design, Implementation, and Use. Springer, Berlin, Heidelberg;2007, p. 328–336,. [112] de Araujo RM, Santoro FM, Borges MRS. The CSCW lab ontology for groupware evaluation. In: 8th international conference on computer supported cooperative work in design, Vol. 2; 200. p. 148–53. [113] Pereira C. Almeida N. Martins A.I. Silva S. Rosa A.F. Oliveira e Silva M. Teixeira A. Evaluation of complex distributed multimodal applications: Evaluating a TeleRehabilitation system when it really matters Lecture Notes in Comput Sci (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 2015 146 157 Pereira C, Almeida N, Martins AI, Silva S, Rosa AF, Oliveira e Silva M,, others,Evaluation of complex distributed multimodal applications: Evaluating a TeleRehabilitation system when it really matters. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)2015;:146–157,. [114] Pereira C. Teixeira A. Oliveira e Silva M. Dynamic evaluation for reactive scenarios (Ph.D. thesis) 2016 University of Aveiro (MAPi) 1 196 Pereira C, Teixeira A, Oliveira e Silva M. Dynamic Evaluation for Reactive Scenarios, Ph.D. Dissertation. University of Aveiro (MAPi). Ph.D. thesis;2016. "
    },
    {
        "doc_title": "Does Remote Expert Representation really matters: A comparison of Video and AR-based Guidance",
        "doc_scopus_id": "85129657701",
        "doc_doi": "10.1109/VRW55335.2022.00208",
        "doc_eid": "2-s2.0-85129657701",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Human computer interaction",
            "Human computer interaction design and evalution method",
            "Human-centered computing",
            "Human-computer-interaction designs",
            "Interaction paradigm",
            "Mixed/augmented reality",
            "Remote experts",
            "User study"
        ],
        "doc_abstract": "© 2022 IEEE.This work describes a user study aimed at understanding how the remote expert representation affects the sense of social presence in scenarios of remote guidance. We compared a traditional video chat solution with an Augmented Reality (AR) annotation tool. These were selected due to ongoing research with partners from the industry sector, following the insights of a participatory design process. A well defined-problem was used, i.e., a synchronous maintenance task with 4 completion stages that required a remote expert using a computer to guide 26 on-site participants wielding a handheld device. The results of the study are described and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Radar-Based Gesture Recognition Towards Supporting Communication in Aphasia: The Bedroom Scenario",
        "doc_scopus_id": "85125218898",
        "doc_doi": "10.1007/978-3-030-94822-1_30",
        "doc_eid": "2-s2.0-85125218898",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Aphasia",
            "Assistive technology",
            "Communication disorders",
            "Daily lives",
            "FMCW radar",
            "Gesture",
            "Gestures recognition",
            "In-bed scenario",
            "Input modalities",
            "Smart environment"
        ],
        "doc_abstract": "© 2022, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.Aphasia and other communication disorders affect a person’s daily life, leading to isolation and lack of self-confidence, affecting independence, and hindering the ability to express themselves easily, including asking for help. Even though assistive technology for these disorders already exists, solutions rely mostly on a graphical output and touch, gaze, or brain-activated input modalities, which do not provide all the necessary features to cover all periods of the day (e.g., night-time). In the scope of the AAL APH-ALARM project, we aim at providing communication support to users with speech difficulties (mainly aphasics), while lying in bed. Towards this end, we propose a system based on gesture recognition using a radar deployed, for example, in a wall of the bedroom. A first prototype was implemented and used to evaluate gesture recognition, relying on radar data and transfer learning. The initial results are encouraging, indicating that using a radar can be a viable option to enhance the communication of people with speech difficulties, in the in-bed scenario.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The angry versus happy recognition advantage: the role of emotional and physical properties",
        "doc_scopus_id": "85124243405",
        "doc_doi": "10.1007/s00426-022-01648-0",
        "doc_eid": "2-s2.0-85124243405",
        "doc_date": "2022-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Experimental and Cognitive Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3205"
            },
            {
                "area_name": "Developmental and Educational Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3204"
            },
            {
                "area_name": "Arts and Humanities (miscellaneous)",
                "area_abbreviation": "ARTS",
                "area_code": "1201"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.Facial emotional expressions are pivotal for social communication. Their fast and accurate recognition is crucial to promote adaptive responses to social demands, for the development of functional relationships, and for well-being. However, the literature has been inconsistent in showing differentiated recognition patterns for positive vs. negative facial expressions (e.g., happy and angry expressions, respectively), likely due to affective and perceptual factors. Accordingly, the present study explored differences in recognition performance between angry and happy faces, while specifically assessing the role of emotional intensity and global/regional low-level visual features. 98 participants categorized angry and happy faces morphed between neutral and emotional across 9 levels of expression intensity (10–90%). We observed a significantly higher recognition efficiency (higher accuracy and shorter response latencies) for angry compared to happy faces in lower levels of expression intensity, suggesting that our cognitive resources are biased to prioritize the recognition of potentially harmful stimuli, especially when briefly presented at an ambiguous stage of expression. Conversely, an advantage for happy faces was observed from the midpoint of expression intensity, regarding response speed. However, when compensating for the contribution of regional low-level properties of distinct facial key regions, the effect of emotion was maintained only for response accuracy. Altogether, these results shed new light on the processing of facial emotional stimuli, emphasizing the need to consider emotional intensity and regional low-level image properties in emotion recognition analysis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring Silent Speech Interfaces Based on Frequency-Modulated Continuous-Wave Radar",
        "doc_scopus_id": "85122850592",
        "doc_doi": "10.3390/s22020649",
        "doc_eid": "2-s2.0-85122850592",
        "doc_date": "2022-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Acoustic signals",
            "Ambient noise",
            "Contact less",
            "European portuguese",
            "Frequency-modulated-continuous-wave radars",
            "Health condition",
            "Lighting conditions",
            "Privacy concerns",
            "Silent speech",
            "Silent speech interfaces",
            "Algorithms",
            "Electromyography",
            "Noise",
            "Radar",
            "Speech"
        ],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.Speech is our most natural and efficient form of communication and offers a strong potential to improve how we interact with machines. However, speech communication can sometimes be limited by environmental (e.g., ambient noise), contextual (e.g., need for privacy), or health conditions (e.g., laryngectomy), preventing the consideration of audible speech. In this regard, silent speech interfaces (SSI) have been proposed as an alternative, considering technologies that do not require the production of acoustic signals (e.g., electromyography and video). Unfortunately, despite their plentitude, many still face limitations regarding their everyday use, e.g., being intrusive, non-portable, or raising technical (e.g., lighting conditions for video) or privacy concerns. In line with this necessity, this article explores the consideration of contactless continuous-wave radar to assess its potential for SSI development. A corpus of 13 European Portuguese words was acquired for four speakers and three of them enrolled in a second acquisition session, three months later. Regarding the speaker-dependent models, trained and tested with data from each speaker while using 5-fold cross-validation, average accuracies of 84.50% and 88.00% were respectively obtained from Bagging (BAG) and Linear Regression (LR) classifiers, respectively. Additionally, recognition accuracies of 81.79% and 81.80% were also, respectively, achieved for the session and speaker-independent experiments, establishing promising grounds for further exploring this technology towards silent speech recognition.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhanced Communication Support for Aphasia Using Gesture Recognition: The Bedroom Scenario",
        "doc_scopus_id": "85118133283",
        "doc_doi": "10.1109/ISC253183.2021.9562810",
        "doc_eid": "2-s2.0-85118133283",
        "doc_date": "2021-09-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Energy Engineering and Power Technology",
                "area_abbreviation": "ENER",
                "area_code": "2102"
            },
            {
                "area_name": "Renewable Energy, Sustainability and the Environment",
                "area_abbreviation": "ENER",
                "area_code": "2105"
            },
            {
                "area_name": "Transportation",
                "area_abbreviation": "SOCI",
                "area_code": "3313"
            },
            {
                "area_name": "Urban Studies",
                "area_abbreviation": "SOCI",
                "area_code": "3322"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Aphasia",
            "Assistive technology",
            "Communication support",
            "Gesture",
            "Gestures recognition",
            "In-bed scenario",
            "Language disorders",
            "Sensor",
            "Smart environment",
            "Speech disorders"
        ],
        "doc_abstract": "© 2021 IEEE.Citizens with speech and language disorders, such as Aphasia, often experience difficulties in expressing their needs. Assistive technologies for these disorders rely mostly on graphical interfaces activated by touch or gaze, which do not effectively cover all communication contexts throughout the day and may raise privacy concerns. In the scope of the AAL APH-ALARM project, our main aim is to extend communication support for users with speech and language difficulties (mainly aphasics) in the bedroom environment. We propose a system for supporting communication based on gesture recognition using non-invasive compact sensors worn by the user or deployed in the environment (e.g., bed). A first prototype was implemented using wrist-worn sensors and machine learning to recognize a small set of gestures. Initial results suggest that gesture recognition to enhance communication for people with speech and language impairments is viable, even when in bed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Slower access to visual awareness but otherwise intact implicit perception of emotional faces in schizophrenia-spectrum disorders",
        "doc_scopus_id": "85110287464",
        "doc_doi": "10.1016/j.concog.2021.103165",
        "doc_eid": "2-s2.0-85110287464",
        "doc_date": "2021-08-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Experimental and Cognitive Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3205"
            },
            {
                "area_name": "Developmental and Educational Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3204"
            }
        ],
        "doc_keywords": [
            "Emotions",
            "Facial Expression",
            "Fear",
            "Humans",
            "Perception",
            "Psychotic Disorders",
            "Schizophrenia"
        ],
        "doc_abstract": "© 2021 Elsevier Inc.Schizophrenia-spectrum disorders are characterized by deficits in social domains. Extant research has reported an impaired ability to perceive emotional faces in schizophrenia. Yet, it is unclear if these deficits occur already in the access to visual awareness. To investigate this question, 23 people with schizophrenia or schizoaffective disorder and 22 healthy controls performed a breaking continuous flash suppression task with fearful, happy, and neutral faces. Response times were analysed with generalized linear mixed models. People with schizophrenia-spectrum disorders were slower than controls in detecting faces, but did not show emotion-specific impairments. Moreover, happy faces were detected faster than neutral and fearful faces, across all participants. Although caution is needed when interpreting the main effect of group, our findings may suggest an elevated threshold for visual awareness in schizophrenia-spectrum disorders, but an intact implicit emotion perception. Our study provides a new insight into the mechanisms underlying emotion perception in schizophrenia-spectrum disorders.",
        "available": true,
        "clean_text": "serial JL 272616 291210 291726 291738 291782 31 Consciousness and Cognition CONSCIOUSNESSCOGNITION 2021-07-15 2021-07-15 2021-07-15 2021-07-15 2021-07-31T18:55:24 S1053-8100(21)00091-X S105381002100091X 10.1016/j.concog.2021.103165 S300 S300.1 FULL-TEXT 2021-07-31T20:08:33.791434Z 0 0 20210801 20210831 2021 2021-07-15T16:36:58.334189Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref 1053-8100 10538100 true 93 93 C Volume 93 13 103165 103165 103165 202108 August 2021 2021-08-01 2021-08-31 2021 Reviews article rev © 2021 Elsevier Inc. All rights reserved. SLOWERACCESSVISUALAWARENESSBUTOTHERWISEINTACTIMPLICITPERCEPTIONEMOTIONALFACESINSCHIZOPHRENIASPECTRUMDISORDERS GRAVE J 1 Introduction 2 Methods and materials 2.1 Sample size 2.2 Participants 2.3 Neuropsychological assessment 2.4 Stimuli 2.5 Suppressors 2.6 Display 2.7 Procedures 2.8 Statistical analysis 3 Results 3.1 Sample characterization 3.2 Neuropsychological characterization 3.3 Split-half reliability of response time 3.4 Response time 3.5 Accuracy 4 Discussion 5 Conclusion 6 Role of funding source CRediT authorship contribution statement Acknowledgements Appendix A Supplementary material References AICHERT 2013 329 336 D 2013 DIAGNOSTICSTATISTICALMANUALMENTALDISORDERSDSM5 AMORIM 2000 P BAKER 2016 1 10 D BATES 2010 D LME4MIXEDEFFECTSMODELINGR BATES 2015 1 48 D BECKER 2011 637 659 D BECKER 2014 189 194 D BEDWELL 2013 88 93 J BERKOVITCH 2021 513 523 L BERKOVITCH 2017 878 892 L BERKOVITCH 2018 835 848 L BEST 2017 713 723 M BORTOLON 2015 79 107 C BRYSBAERT 2019 16 M BUTLER 2009 1095 1107 P BUTLER 2003 199 209 P CALVO 2010 1274 1297 M CALVO 2008 471 494 M CALVO 2015 1081 1106 M CAPITAO 2014 1027 1036 L CARMEL 2010 2030 D CARUANA 2020 100181 N CARUANA 2021 183 198 N CARUANA 2019 135 151 N CAVACO 2013 189 198 S CHAN 2010 381 390 R CLARK 2013 513 519 C CONNOR 2004 R850 R852 C COUTURE 2006 S44 S63 S DAVIS 2011 647 655 F DEHAENE 2011 200 227 S DEHAENE 1998 14529 14534 S DELCUL 2006 1313 1323 A DERNTL 2017 621 627 B DONG 2018 200 208 D EDWARDS 2002 789 832 J ENGELL 2020 14 A FETT 2011 573 588 A FOX 2019 J RCOMPANIONAPPLIEDREGRESSION FRITH 2009 3453 3458 C GAYET 2014 460 S GOLDSMITH 2020 8 D GOMES 2018 886 895 N GOODHEW 2019 14 25 S GRAY 2013 537 544 K GREEN 2012 1216 1224 M GREEN 2015 620 631 M GREEN 2019 146 161 M GREEN 2011 700 708 M GUNTHER 2017 46 53 V GUPTA 2015 747 754 R GUPTA 2016 328 337 R HAJDUK 2019 139 153 M HARTMAN 2019 36 42 L HEDGE 2018 1166 1186 C HEDGER 2016 934 968 N HERZOG 2015 64 71 M HESSELMANN 2018 1641 1659 G HONG 2015 392 S HORAN 2013 151 172 W SOCIALCOGNITIONINSCHIZOPHRENIAEVIDENCETREATMENT SOCIALCOGNITIONFUNCTIONALOUTCOMEINSCHIZOPHRENIA JAHSHAN 2017 38 42 C JAVITT 2009 249 275 D JIANG 2007 349 355 Y KERI 2005 1445 1455 S KERR 2017 3002 J KIM 2015 314 321 D KOHLER 2003 1768 1774 C KOHLER 2010 1009 1019 C KORB 2017 166 180 S KRING 2014 443 454 A KUCHARSKAPIETURA 2013 335 343 K KUHARIC 2019 165 173 D KURYLO 2018 267 283 D LAERE 2018 945 955 E LAKENS 2021 1 31 D LEE 2011 1001 1008 J LEE 2015 1 14 J LEE 2016 165 173 S LEE 2010 291 297 S LEFEBVRE 2021 661 675 S LI 2010 1029 1039 H LINDEN 2010 997 1002 S LIU 2016 32594 Y LO 2015 1171 S LUDECKE 2020 D MAROSI 2019 8958 C MASHOUR 2020 776 798 G MATHERSUL 2009 278 291 D MATHIS 2012 108 113 K MATUSCHEK 2017 305 315 H MCCLEERY 2020 823 A MELNIKOFF 2018 280 293 D MENZEL 2018 74 83 C MEYER 2012 571 M MILES 1930 412 430 W MORRENS 2007 1038 1053 M NIEDENTHAL 2010 417 433 P NORTON 2009 1094 1098 D OHMAN 2001 466 478 A OHMAN 2012 17 32 A PARK 2011 18 23 S PEIRCE 2007 8 13 J PINHEIRO 2000 J MIXEDEFFECTSMODELSINSSPLUS PINKHAM 2014 14 19 A PINKHAM 2011 174 178 A PINKHAM 2014 168 1677 A PITTS 2012 287 303 M PONCIANO 1982 191 202 E POOL 2016 79 106 E POURNAGHDALI 2020 1071 1103 A PREMKUMAR 2008 14 19 P RAMANOEL 2014 1086 S RAUCH 2010 200 206 A ROMEROFERREIRO 2016 177 183 M RYCHLOWSKA 2017 1259 1270 M SANCHEZCUBILLO 2009 438 450 I SAVLA 2013 979 992 G SCHOEMAN 2009 163 176 R HANDBOOKNEUROPSYCHIATRICBIOMARKERSENDOPHENOTYPESGENES EMOTIONRECOGNITIONDEFICITSANEUROCOGNITIVEMARKERSCHIZOPHRENIALIABILITY SEEDORFF 2019 M SEYMOUR 2016 15 19 K SHASTEEN 2016 150 155 J SILVERSTEIN 2020 e200017 S SMITH 2009 1202 1208 F SOARES 2014 e114724 S STEIN 2019 1 38 T TRANSITIONSBETWEENCONSCIOUSNESSUNCONSCIOUSNESS BREAKINGCONTINUOUSFLASHSUPPRESSIONPARADIGMREVIEWEVALUATIONOUTLOOK STEIN 2011 167 T STEIN 2021 612 624 T STEIN 2014 566 574 T STEIN 2014 387 T SUSLOW 2013 T TAMMINGA 2014 S131 S137 C TAYLOR 2012 136 145 S THEINTERNATIONALSCHIZOPHRENIACONSORTIUM 2009 748 752 THEEUWES 2000 104 124 J TSUCHIYA 2005 1096 1101 N TSUCHIYA 2009 1224 1225 N UNDERWOOD 2016 131 138 R VANTWOUT 2007 227 235 M VANTWOUT 2007 758 764 M VETTER 2019 e43467 P WANG 2019 2565 H WEBB 2020 17427 A WEBB 2020 e0234513 A WILLENBOCKEL 2010 671 684 V WIRTH 2020 2463 2481 B WON 2019 564 S YANG 2014 724 E YANG 2007 882 886 E YANG 2018 e0206799 Y ZHOU 2020 102896 S GRAVEX2021X103165 GRAVEX2021X103165XJ HEFCE none 2022-07-15T00:00:00.000Z © 2021 Elsevier Inc. All rights reserved. 2021-07-21T00:10:10.006Z William James Center for Research, University of Aveiro Portuguese Foundation for Science and Technology SFRH/BD/129980/2017 UIBD/04810/2020 UID/IC/4255/2020 FCT Fundação para a Ciência e a Tecnologia This work was supported by the Portuguese Foundation for Science and Technology under a PhD grant to the first author [SFRH/BD/129980/2017] and within the R&D Units Center for Health Technology and Services Research [UID/IC/4255/2020] and William James Center for Research, University of Aveiro [UIBD/04810/2020]. The funding sources had no involvement in the study design; collection, analysis and interpretation of data; writing of the report; and decision to submit the article for publication. item S1053-8100(21)00091-X S105381002100091X 10.1016/j.concog.2021.103165 272616 2021-07-31T20:08:33.791434Z 2021-08-01 2021-08-31 true 1409216 MAIN 16 48122 849 656 IMAGE-WEB-PDF 1 gr2 7796 200 287 gr1 52274 270 691 gr3 11061 209 295 gr2 5010 153 219 gr1 10326 86 219 gr3 6850 155 219 gr2 52081 886 1271 gr1 382602 1196 3061 gr3 71934 925 1306 mmc1 mmc1.docx docx 59665 APPLICATION si1 820 YCCOG 103165 103165 S1053-8100(21)00091-X 10.1016/j.concog.2021.103165 Elsevier Inc. Fig. 1 Illustration of a trial in b-CFS task with fearful faces. Face stimuli were retrieved from the Karolinska Directed Emotional Faces (Lundqvist et al., 1998). Fig. 2 Means of response time (in sec) by group. Analysis with generalized linear-mixed models showed that response times were significantly longer in the schizophrenia group than the control group (p = .006). Error bars indicate standard error. Fig. 3 Means of response time (in sec) by group and emotion. Exploratory analysis with generalized linear-mixed models per group showed that, in the schizophrenia group and in the control group, response times were significantly shorter for happy faces than neutral (p < .001) and fearful ones (p < .001), with no difference between fearful and neutral faces (p > .050). Error bars indicate standard error. Table 1 Sociodemographic and clinical characteristics of people with schizophrenia and people without psychiatric disorder. SZ n = 23 CG n = 22 p-value 1 Age, M (SD) 28 (5.74) 27.81 (5.39) 0.913 Gender, n (%) Male 15 (65.22) 15 (68.18) 0.833 Female 8 (34.78) 7 (31.82) Education, n (%) First cycle 0 (0) 0 (0) 0.440 Second cycle 1 (4.35) 0 (0) Third cycle 1 (4.35) 3 (13.64) Secondary 15 65.22) 10 (45.45) Graduation 4 (17.39) 5 (22.73) Master or higher 2 (8.70) 4 (18.18) Occupation, n (%) Student 4 (17.39) 3 (13.64) 0.891 Employed 11 (47.83) 12 (54.55) Unemployed 8 (34.78) 7 (31.82) Handedness, n (%) Right 23 (100) 21 (95.45) 0.301 Left 0 (0) 1 (4.55) Ocular dominance, n (%) Right 13 (56.52) 13 (59.09) 0.862 Left 10 (43.48) 9 (40.91) First episode psychosis, n (%) Yes 11 (47.83) No 12 (52.18) Number of hospitalizations, M (SD) 1.39 (1.27) Duration of the disorder (years), M (SD) 3.39 (2.04) Age of onset, M (SD) 24.61 (5.69) Note: CG, control group; SZ, schizophrenia-spectrum group. 1 The comparisons between groups were performed using Chi-Square, Fisher, or independent sample t-tests. Slower access to visual awareness but otherwise intact implicit perception of emotional faces in schizophrenia-spectrum disorders Joana Grave Conceptualization Methodology Formal analysis Investigation Resources Data curation Writing – original draft Visualization a b ⁎ Nuno Madeira Conceptualization Methodology Resources Writing – review & editing Supervision c d Maria João Martins Conceptualization Methodology Investigation Resources Writing – review & editing d e Samuel Silva Conceptualization Methodology Software Resources Writing – review & editing f Sebastian Korb Conceptualization Methodology Software Formal analysis Resources Data curation Writing – review & editing g h Sandra Cristina Soares Conceptualization Methodology Formal analysis Investigation Data curation Writing – review & editing Supervision a b ⁎ a William James Center for Research, Department of Education and Psychology, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal William James Center for Research Department of Education and Psychology University of Aveiro Campus Universitário de Santiago 3810-193 Aveiro Portugal William James Center for Research, Department of Education and Psychology, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal b Center for Health Technology and Services Research, Department of Education and Psychology, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Center for Health Technology and Services Research Department of Education and Psychology University of Aveiro Campus Universitário de Santiago 3810-193 Aveiro Portugal Center for Health Technology and Services Research, Department of Education and Psychology, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal c Psychiatry Department, Centro Hospitalar e Universitário de Coimbra, Praceta Prof. Mota Pinto, 3000-075 Coimbra, Portugal Psychiatry Department Centro Hospitalar e Universitário de Coimbra Praceta Prof. Mota Pinto 3000-075 Coimbra Portugal Psychiatry Department, Centro Hospitalar e Universitário de Coimbra, Praceta Prof. Mota Pinto, 3000-075 Coimbra, Portugal d Institute of Psychological Medicine, Faculty of Medicine – University of Coimbra, Portugal, Rua Larga, 3004-504 Coimbra, Portugal Institute of Psychological Medicine Faculty of Medicine – University of Coimbra, Portugal Rua Larga 3004-504 Coimbra Portugal Institute of Psychological Medicine, Faculty of Medicine – University of Coimbra, Portugal, Rua Larga, 3004-504 Coimbra, Portugal e Ocupational Health and Safety Management Services, University of Coimbra Social Services, Rua Doutor Guilherme Moreira 12, 3000-210 Coimbra, Portugal Ocupational Health and Safety Management Services University of Coimbra Social Services Rua Doutor Guilherme Moreira 12 3000-210 Coimbra Portugal Ocupational Health and Safety Management Services, University of Coimbra Social Services, Rua Doutor Guilherme Moreira 12, 3000-210 Coimbra, Portugal f Department of Electronics, Telecommunication and Informatics (DETI)/Institute of Electronics and Informatics Engineering (IEETA), University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunication and Informatics (DETI)/Institute of Electronics and Informatics Engineering (IEETA) University of Aveiro Campus Universitário de Santiago 3810-193 Aveiro Portugal Department of Electronics, Telecommunication and Informatics (DETI)/Institute of Electronics and Informatics Engineering (IEETA), University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal g Department of Psychology, University of Essex, CO4 3SQ Colchester, United Kingdom Department of Psychology University of Essex CO4 3SQ Colchester United Kingdom Department of Psychology, University of Essex, CO4 3SQ Colchester, United Kingdom h Department of Cognition, Emotion, and Methods in Psychology, University of Vienna, Liebiggasse 5 1010, Vienna, Austria Department of Cognition, Emotion, and Methods in Psychology University of Vienna Liebiggasse 5 1010 Vienna Austria Department of Cognition, Emotion, and Methods in Psychology, University of Vienna, Liebiggasse 5 1010, Vienna, Austria ⁎ Corresponding authors at: Department of Education and Psychology, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal. Department of Education and Psychology University of Aveiro Campus Universitário de Santiago 3810-193 Aveiro Portugal Schizophrenia-spectrum disorders are characterized by deficits in social domains. Extant research has reported an impaired ability to perceive emotional faces in schizophrenia. Yet, it is unclear if these deficits occur already in the access to visual awareness. To investigate this question, 23 people with schizophrenia or schizoaffective disorder and 22 healthy controls performed a breaking continuous flash suppression task with fearful, happy, and neutral faces. Response times were analysed with generalized linear mixed models. People with schizophrenia-spectrum disorders were slower than controls in detecting faces, but did not show emotion-specific impairments. Moreover, happy faces were detected faster than neutral and fearful faces, across all participants. Although caution is needed when interpreting the main effect of group, our findings may suggest an elevated threshold for visual awareness in schizophrenia-spectrum disorders, but an intact implicit emotion perception. Our study provides a new insight into the mechanisms underlying emotion perception in schizophrenia-spectrum disorders. Keywords Schizophrenia Facial expression Continuous flash suppression Visual perception Implicit emotion perception 1 Introduction Schizophrenia-spectrum disorders, such as schizophrenia and schizoaffective disorder, are among the most severe and complex psychiatric disorders, characterized by a wide range of deficits in social cognition (Green, Horan, & Lee, 2015; Pinkham, 2014; Savla, Vella, Armstrong, Penn, & Twamley, 2013). Social cognition refers to the psychological mechanisms underlying social interactions, and involves the perception, interpretation, and use of social information (Pinkham, 2014). Social cognitive deficits in schizophrenia-spectrum disorders are significant predictors of poor functioning outcomes, such as social and community functioning (Couture, Penn, & Roberts, 2006; Fett et al., 2011; Horan, Lee, & Green, 2013), and remain quite stable despite medication (Kucharska-Pietura & Mortimer, 2013). One of the most studied social cognitive abilities is the perception of emotional stimuli, namely facial expressions (Green et al., 2015). Being able to accurately perceive others’ emotional state is crucial to engage in social interactions and to respond in accordance to social events (Frith, 2009). Deficits in emotional face processing in schizophrenia-spectrum disorders, namely in its explicit processing, are well established in the literature (Chan, Li, Cheung, & Gong, 2010; Kohler, Walker, Martin, Healey, & Moberg, 2010; Savla et al., 2013). These are related to structural and functional alterations in the brain regions dedicated to emotion perception (e.g., amygdala) (Green et al., 2015; Li, Chan, McAlonan, & Gong, 2010; Taylor et al., 2012), and have been considered neurocognitive markers of schizophrenia (Schoeman, Niehaus, Koen, & Leppänen, 2009). Despite inconsistent findings – in part due to the heterogeneity between studies (Edwards, Jackson, & Pattison, 2002) – difficulties are mostly encountered during the discrimination and recognition of threat-related faces, particularly fearful ones (Kohler et al., 2003; Kuharic et al., 2019; Lee, Lee, Kweon, Lee, & Lee, 2010; Norton, McBain, Holt, Ongur, & Chen, 2009; Premkumar et al., 2008; Romero-Ferreiro et al., 2016; Won et al., 2019). An impaired ability to detect social threat was also reported by Pinkham et al. (2014). Using a visual search task, the authors observed a decreased ability to detect threat in people with schizophrenia or schizoaffective disorder, compared to people without psychiatric disorders, when angry faces were presented among happy faces, but not when snakes were presented among flowers or mushrooms (Pinkham et al., 2014). Given the importance of emotional faces in the social world, this abnormal perception may explain some of the maladaptive threat appraisals that characterize psychotic experiences (Underwood, Kumari, & Peters, 2016) and that lead to social withdrawal, inappropriate social interactions, and exacerbated anxiety. Despite existing models suggesting that deficits in the early stages of visual perception are the first chain of events leading to social cognitive dysfunction and poor functioning outcomes (Green, Hellemann, Horan, Lee, & Wynn, 2012; Javitt, 2009; McCleery et al., 2020), the mechanisms underlying emotional face processing in schizophrenia-spectrum disorders are far from understood. Deficits in early visual perception are described quite often in schizophrenia using non-emotional visual masking (Butler et al., 2003; Del Cul, Dehaene, & Leboyer, 2006; Green et al., 2012; Green, Lee, Wynn, & Mathis, 2011; Herzog & Brand, 2015; McCleery et al., 2020), inattentional blindness (Pitts, Martínez, & Hillyard, 2012), and attentional blink paradigms (Mathis et al., 2012). There is also evidence for an abnormal magnocellular pathway – involved in early perception by projecting visual information from the thalamus directly to the subcortical regions, and sensible to low spatial frequency – in emotional faces (Bedwell et al., 2013; Butler et al., 2009; Jahshan, Wolf, Karbi, Shamir, & Rassovsky, 2017; Marosi, Fodor, & Csukly, 2019), namely fearful ones (Kim, Shim, Song, Im, & Lee, 2015; Lee, Park, Song, Choi, & Lee, 2015). For example, people with schizophrenia, compared to healthy individuals, showed decreases in event-related potential components, reflecting early visual perception, for fearful faces with low spatial frequency (Kim et al., 2015). Consistently, two studies using the Bubbles technique 1 1 In the Bubbles technique, participants are presented with partial visual information about an object. The visual information is displayed through an opaque field with randomly located Gaussian apertures, designated by bubbles. This allows to isolate the visual information that is used to recognize or categorize the objects (Clark et al., 2013; Lee et al., 2011). found that people with schizophrenia or schizoaffective disorder required more visual cues (i.e., more bubbles), used different strategies to collect visual information (e.g., underutilized the eye regions), and ignored information presented in the lower range of spatial frequencies (Clark, Gosselin, & Goghari, 2013; Lee, Gosselin, Wynn, & Green, 2011). However, these studies used explicit tasks, in which participants are asked to allocate their attention towards the facial expression. In contrast, in implicit tasks, limited visual information is provided and participants are not required to attend to the emotional content of the faces. Whether implicit emotion perception is altered in schizophrenia-spectrum disorders remains largely unknown, as the relevant literature is sparse and mixed (Lee, Kim, & Lee, 2016). This is surprising given the current debate on whether deficits in emotional face perception in schizophrenia-spectrum disorders are better explained by automatic, bottom-up factors – determined by salience or emotional content, and typically involved in implicit tasks (Connor, Egeth, & Yantis, 2004; Theeuwes, Atchley, & Kramer, 2000) – or top-down factors – driven by observers’ prior knowledges, expectations or goals, with attention being voluntarily allocated towards the stimulus, feature or location, and typically involved in explicit tasks (Connor et al., 2004; Theeuwes et al., 2000) –, as reviewed by Lee et al. (2016). Several studies found an abnormal implicit perception of emotional faces in people with schizophrenia (Derntl & Habel, 2017; Liu, Zhang, Zhao, Tan, & Luo, 2016; Park, Kim, Kim, Kim, & Lee, 2011; Rauch et al., 2010; Suslow et al., 2013) and healthy individuals with high social anhedonia (Günther et al., 2017). For example, Suslow et al. (2013) observed an increased activation of the right amygdala to masked angry and happy faces (shown for 33 ms), and a stronger negative priming for masked angry faces in people with schizophrenia, compared to people without psychiatric disorders, but only in the first part of a backward masking paradigm. Liu et al. (2016) demonstrated that, when asked to indicate the direction of an arrow presented with valid fearful (compared to neutral) facial cues (showed for 100 ms), heathy individuals, but not people with schizophrenia, exhibited shorter response times (RT) and decreased event-related potential components. The authors pointed that the lack of fearful bias in schizophrenia suggests bottom-up constraints in early visual perception (Liu et al., 2016). Other studies have instead showed that the ability to implicitly perceive emotional faces may be preserved in schizophrenia-spectrum disorders (Aichert et al., 2013; Kring, Siegel, & Barrett, 2014; Shasteen et al., 2016; van’t Wout, Aleman, et al., 2007). Using a Continuous Flash Suppression (CFS) paradigm, Kring et al. (2014) found that both people with schizophrenia or schizoaffective disorder and people without psychiatry disorders rated visible neutral faces as more/less trustworthy and warm when faces were presented concurrently with subliminal happy/angry faces. The authors argued that patients (and healthy controls controls) were able to perceive the emotional content even when stimuli were presented outside visual awareness. Similar findings were reported by Shasteen et al. (2016) using a priming paradigm. In this study, both people with schizophrenia or schizoaffective disorder and people without psychiatry disorders rated Chinese pictographs as more threatening when they were preceded by briefly presented and masked angry faces, compared to neutral faces. Interestingly, the authors failed to replicate their findings when bodily emotions, rather than emotional faces, were used (Hajdúk, Klein, Bass, Springfield, & Pinkham, 2019). The mixed findings regarding implicit emotion perception may be related to the clinical heterogeneity that characterizes schizophrenia (American Psychiatric Association, 2013), but also to the use of distinct implicit tasks that recruit distinct cognitive functions, such as response inhibition (Derntl & Habel, 2017), attention orienting and execution (Liu et al., 2016), and cognitive control (Aichert et al., 2013). Besides these complex cognitive functions, some implicit tasks require an explicit judgement of the stimulus, such as subjective ratings of trustworthiness, competence, interpersonally, and threat (Kring et al., 2014; Shasteen et al., 2016). Critics have pointed out that a strict separation into two types of psychological processes – Type 1 and Type 2 thinking – does not apply to most tasks (Melnikoff & Bargh, 2018). Nonetheless, the use of paradigms that minimize the recruitment of complex cognitive functions is critical to obtain a broader understanding on emotion perception in schizophrenia-spectrum disorders. Of relevance when investigating the bottom-up factors involved in early visual perception, is the access to visual awareness. According to the Global Neuronal Workspace (Dehaene & Changeux, 2011; Dehaene, Kerszberg, & Changeus, 1998; Mashour, Roelfsema, Changeux, & Dehaene, 2020), the access to visual awareness relies on top-down amplification by late and higher-level integrative processes, and bottom-up spread of sensory signals through the visual system. Thus, when a visual stimulus reaches a certain salience threshold at a sensory level, it triggers the activation of neuronal networks (e.g., parieto-frontal), leading to a top-down amplification. Recent studies using non-emotional stimuli have reported an elevated threshold for visual awareness in schizophrenia (Berkovitch et al., 2021; Berkovitch, Del Cul, Maheu, & Dehaene, 2018; Del Cul et al., 2006; Lefebvre et al., 2021), although its understanding is yet incomplete (Berkovitch et al., 2017). It has been proposed that this elevated threshold in schizophrenia decreases the information entering consciousness, leading to misinterpretations of stimuli and to exacerbated positive symptoms (Berkovitch et al., 2017). Considering the social cognitive deficits in schizophrenia, a disrupted access to visual awareness by stimuli with socioemotional relevance could impact the way these people perceive and make sense of their social world. Hence, studying how emotional faces enter visual awareness will help to understand the mechanisms underlying emotion deficits in schizophrenia-spectrum disorders. In the present study, we aimed to investigate the access to visual awareness by emotional faces in schizophrenia-spectrum disorders using a breaking-CFS paradigm (b-CFS; Jiang, Costello, & He, 2007; Stein, Hebart, & Sterzer, 2011), a variant of CFS (Tsuchiya & Koch, 2005). CFS is a binocularly-rivalry technique in which a stimulus presented to one eye is suppressed from visual awareness by competing noise patterns exhibited to the other eye. When compared to more conventional paradigms (e.g., binocular rivalry, brief affective prime), CFS allows for longer suppression times by manipulating the contrast of the stimulus and suppressors (Tsuchiya & Koch, 2005). In b-CFS, the main dependent variable is the time it takes for the suppressed stimulus to “break” into visual awareness and to be consciously detected (Jiang et al., 2007; Stein et al., 2011). Shorter suppression times reflect a privileged access to visual awareness (Gayet, Van Der Stigchel, & Paffen, 2014; Yang, Brascamp, Kang, & Blake, 2014). Importantly, b-CFS is less susceptible to the interferences of complex cognitive functions (Caruana & Seymour, 2021; Caruana, Stein, Watson, Williams, & Seymour, 2019), including Theory of Mind, the ability to perceive human mental stages and to infer about others’ dispositions, intentions, and beliefs (Pinkham, 2014). Participants diagnosed with schizophrenia or schizoaffective disorder and control participants without psychiatric disorders completed a b-CFS task with fearful, happy, and neutral faces, similar to previous studies in the general population (Capitão et al., 2014; Gray, Adams, Hedger, Newton, & Garner, 2013; Stein, Seymour, Hebart, & Sterzer, 2014; Tsuchiya, Moradi, Felsen, Yamazaki, & Adolphs, 2009; Yang, Yeh, & Keil, 2018). Fearful faces were used because they signal a potential yet unclear threat, thus enhancing visual attention to collect more information on the surroundings (Davis et al., 2011), and their processing is particularly hampered in schizophrenia (Kim et al., 2015; Kohler et al., 2003; Kuharic et al., 2019; J.S. Lee et al., 2015; S.J. Lee et al., 2010; Norton et al., 2009; Premkumar et al., 2008; Romero-Ferreiro et al., 2016; Won et al., 2019). On the other hand, happy faces convey enjoyment and affiliation, linked to reward and attachment – although they can also signal dominance and elicit rather negative feelings in the observer (Niedenthal, Mermillod, Maringer, & Hess, 2010; Rychlowska et al., 2017) –, and their processing is relatively preserved in schizophrenia (Kring et al., 2014; Lee et al., 2010; Premkumar et al., 2008; Romero-Ferreiro et al., 2016). We made the following hypotheses. First, we expected that people with schizophrenia-spectrum disorders would be overall slower to consciously detect the visual stimuli than people without psychiatric disorders. Second, and based on the assumption that deficits in fearful face perception in schizophrenia arise from bottom-up abnormalities (Kim et al., 2015; Lee et al., 2015; Liu et al., 2016), we expected that the detection of fearful faces would be overall slower than neutral (and possibly happy) faces in patients. In healthy individuals, we expected that detection would be overall faster for fearful than neutral (and possibly happy) faces, because of the previous reports that fearful faces gain preferential access to awareness in b-CFS (Capitão et al., 2014; Gray et al., 2013; Stein et al., 2014; Tsuchiya et al., 2009; Yang, Zald, & Blake, 2007). By exploring the access to visual awareness using emotional faces, which are relevant socioemotional stimuli, the present study aims to provide new evidence about the mechanisms underlying implicit emotion perception of face stimuli in schizophrenia-spectrum disorders. 2 Methods and materials 2.1 Sample size We set the goal to test 20 participants per group, considering previous schizophrenia studies (Bedwell et al., 2013; Derntl & Habel, 2017; Kring et al., 2014; Seymour, Rhodes, Stein, & Langdon, 2016) and constrains related to time and clinical recruitment – commonly encountered in research with humans (Lakens, 2021). The recruitment and data collection were defined to start in February 2017 and to finish no later than January 2019. After reaching the minimum sample size, additional participants were recruited to prevent data loss, and data acquisition was fully stopped at the end of the timeframe. To confirm if the study was sufficiently powered, we conducted a retrospective power analysis using the PANGEA application (Westfall, 2016), considering a medium effect size of d = 0.45 based on the schizophrenia (Derntl & Habel, 2017; Kohler et al., 2010; Liu et al., 2016) and the CFS literature (Hedger, Gray, Garner, & Adams, 2016). The retrospective power analysis showed a 92.5% of power considering a 3 × 2 mixed factor design (within-between subjects), with participants nested in groups, and 72 repetitions per emotion and participant. 2.2 Participants Twenty-six people diagnosed with schizophrenia or schizoaffective disorder and 23 gender-, age-, and education-matched healthy individuals were recruited. Inclusion criteria included age between 18 and 40 years, native Portuguese speakers, and normal or corrected-to-normal visual acuity. Exclusion criteria consisted of the self-reported presence or history of medical disorders that may affect brain functioning, substance abuse in the past month, and substance dependence in the past six months. Patients were identified and referred by their psychiatrists in outpatient clinics of mental health hospitals located in the centre region of Portugal. They were previously diagnosed with schizophrenia or schizoaffective disorder by their psychiatrists based on the DSM-5 criteria (American Psychiatric Association, 2013). No structured clinical interview for the clinical group was performed in the present study. Patients were on a stable medication regiment for a minimum of six weeks and had not been hospitalised within the last two months. The duration of the disorder was no longer than 10 years. All patients were medicated with antipsychotics. Two patients were excluded: one for not having a diagnosis of schizophrenia or schizoaffective disorder and one for limited mobility in the right arm. The control group was recruited from the local community and nearby university population via social media advertisings. Additional exclusion criteria for these participants consisted of the presence or history of psychiatric disorder or treatment, as confirmed via the Portuguese version of the Mini International Neuropsychiatric Interview (MINI 5.0.0; Amorim, 2000), and the presence or history of psychotic disorders in first-degree biological relatives, as stated by the participants. The study was approved by local institutional review boards and all participants gave written informed consent before participation. The study was conducted in accordance with the Declaration of Helsinki and the standards of the American Psychological Association. Participants did not receive any reward for their participation. 2.3 Neuropsychological assessment We used the Trail Making Test (TMT) (Cavaco et al., 2013) to control for possible effects of neurocognitive functions. The TMT consists of two parts and is widely used to assess visuoperceptual abilities (e.g., visual search; TMT-A), working memory, and task-switching abilities (TMT-B) in the general population (Sánchez-Cubillo et al., 2009) and schizophrenia (Laere, Tee, & Tang, 2018). In our sample, the score was the time of completion of each part (in sec). Longer time of completion indicates worse performance. We used the Portuguese version of the Zung Self-rating Anxiety Scale (SAS) (Ponciano, Vaz Serra, & Relvas, 1982) to control for possible effects of anxiety in fear detection in b-CFS (Capitão et al., 2014). The SAS is a self-rating scale that covers a variety of anxiety symptoms over the period of one week. It contains 20 items in a 4-point Likert scale from 1 (a little of time) to 4 (most of the time). Items 5, 9, 13, 17, and 19 are reverse scored. Total scores range from 20 to 80, with higher scores indicating higher anxiety levels. In our sample, the SAS showed a Cronbach’s alpha of 0.76, revealing adequate levels of internal consistency. The SAS responses were missing for three patients. 2.4 Stimuli Male faces (facing forward) displaying fearful, happy, and neutral expressions were selected from six different identities from the Karolinska Directed Emotional Faces (AM05, AM08, AM10, AM11, AM17, AM28) (Lundqvist, Flykt, & Öhman, 1998), with comparable attractiveness and intensity, and matching of important low-level visual features across fear and happy conditions (e.g., presence of teeth). 2 2 Stimuli were adapted from a previous study conducted by our laboratory, in which only male faces were presented (Rocha et al., in prep). To improve suppression, each face was put into grayscale. The position of the eyes across all stimuli was aligned using GIMPS software A gray frame with a central oval-shaped opining was superimposed onto each face, and the oval edge of the superimposed frame was smooth with a Gaussian filter. The luminance was equalized using the SHINE toolbox (Willenbockel et al., 2010) in Matlab Final stimuli contained the oval face in one of the four quadrants, centred at a vertical and horizontal distance of 4.6° of visual angle related to a central black fixation cross. The gray background was set to the mean luminance resulting from normalization. 2.5 Suppressors For building suppressors, several Mondrian patterns were composed of randomly arranged, high contrasts and colourful circles with diameters between 1.5° and 5° of visual angles, animated at 10 Hz. 2.6 Display The task was programmed in, and displayed with, Psychopy software (Peirce, 2007). Stimuli and overlapping suppressors were presented centred on the screen area, covering about 18° by 18° of visual angle, and surrounded by a frame of black and white lines, of equal thickness, up to about 20° by 20° of visual angle (Fig. 1 ). Stimuli were displayed on a Dell E190S monitor (pixel resolution: 1280 × 1024, refresh rate: 60 Hz, display dimension: 14.8 × 11.8 in.). Red-blue anaglyph glasses were used to allow the presentation of different information to each eye (Engell & Quilliam, 2020; Gomes, Soares, Silva, & Silva, 2018; Korb, Osimo, Suran, Goldstein, & Rumiati, 2017; Zhou et al., 2020). In the present study, the red filter lens was used over the left eye and the blue filter lens over the right eye. Stimuli were always displayed to the left eye (red lens) and suppressors to the right eye (blue lens). 2.7 Procedures Participants were tested individually. After giving their informed consent, participants completed the sociodemographic questionnaire and the SAS. Prior to testing, ocular dominance was assessed with the Miles’ test (Miles, 1930) to control for the effects of ocular dominance. Participants were seated 50 cm away from the monitor, while wearing the anaglyph glasses, and asked to keep their gaze on the black fixation cross presented in the centre of the screen. Participants were instructed to indicate, as rapidly and accurately as possible, on which side of the screen (left/right) the face or any part of the face became visible, by pressing with their left and right index fingers either the ‘f’ or ‘j’ keys on a computer keyboard. Each trial started with a blank frame with the black fixation cross, displayed to both eyes for 2 sec. The frame was centrally presented. The face and suppressors were simultaneously presented, with the face appearing in one of the four quadrants (upper/lower and left/right; the order of the quadrants was random and different for each participant). The opacity of the face increased linearly from 0% to 60% over the course of 1 sec, and then stayed at 60% until 6 sec after stimulus onset (SO) or until the participant’s response. The opacity of the Mondrian patterns stayed at 100% until 1 sec, and then linearly decreased to 0% until 6 sec after SO or until the participant’s response (Fig. 1). Immediately after participant’s response, or 6 sec after SO in trials without response, a random white noise mask was presented to both eyes. The duration of the white noise mask varied randomly between 200 and 500 ms (in trials without response) or longer (in trials with response). The speed of access to visual awareness was analysed via RT (in sec), defined from the moment of SO to the moment of a response button press. The task was preceded by 36 practice trials, which did not contain any of the stimuli used in the main task. The main task comprised 216 trials (6 identities × 3 emotions × 4 quadrants × 3 repetitions). Stimuli were presented in a semi-random order, with one mandatory break in every 60 trials. The task lasted about 40 min. Lastly, participants completed the TMT-A and TMT-B, and were fully debriefed about the purpose of the study. 2.8 Statistical analysis Significant levels were set at α = 0.05 and effect sizes were computed whenever possible. Sociodemographic and neuropsychological data were analysed with parametric (i.e., independent samples t-tests for continuous variables) and non-parametric tests (i.e., Fisher exact-test and Pearson for categorical variables). We adjusted the degrees of freedom in case of violation of the assumption of equal variances in the independent samples t-test, assessed with Levene’s test. Analysis were performed in jamovi (The Jamovi Project, 2020), a software in R language (R Core Team, 2020). Regarding b-CFS, participants with less than 80% of trials in which the face was correctly detected within the 6 sec of stimuli presentation (valid trials) were excluded (one patient: 8.33%; one healthy control: 75.46%). For RT analysis, only valid trials were considered. Outliers (1.57%) were eliminated and calculated as follows: RT faster than 500 ms or slower than mean + 3 × standard deviation, computed per emotion and participant (Kerr, Hesselmann, Räling, Wartenburger, & Sterzer, 2017). The correlation between task performance and individual-differences measure, such as psychiatric diagnosis, depends on the reliability of these variables alone, which highlights the need to evaluate the reliability of individual-differences in experimental tasks (Goodhew & Edwards, 2019; Hedge, Powell, & Sumner, 2018). We thus computed the reliability estimates for the RT of each emotion using the average function of the package splithalf (Parsons, 2020) in R (R Core Team, 2020), with 5000 random splits of the data. RT analyses were performed with generalized linear mixed-effect models (GLMM) using the gmer function in the lmer4 package (Bates, Mächler, Bolker, & Walker, 2015) in R (R Core Team, 2020). GLMMs have several advantages in comparison to more conventional approaches (e.g., they consider multiple observations within each condition and all factors than might explain the data) and do not require an absence of heteroskedasticity (Bates, 2010; Pinheiro & Bates, 2000). Our first level consisted of 9175 observations, aggregated in our second level (n = 45). We considered a Gamma distribution (to account for positive skewness) and an identity link function (Lo & Andrews, 2015). Based on our design, the simpler model comprised untransformed RTs as the dependent variable, and group × emotion as fixed factors. Different by-actor (to ensure that the results were not driven by the specific pictures that were selected) and by-individual random effects were considered (Matuschek, Kliegl, Vasishth, Baayen, & Bates, 2017; Seedorff, Oleson, & McMurray, 2019). Fixed factors (ocular dominance, handedness, habituation during the experiment, age, SAS, TMT-A, TMT-B) were individually added to the model to improve fit. Continuous fixed factors were always centred. Models failing to converge were not considered as they would increase Type I error (Seedorff et al., 2019). Models were contrasted to the simpler model for fit using the anova function in the car package (Fox & Weisberg, 2019), and the best-fitting model was selected and described in the results section. Once the winning model was identified, we used the Anova function in the car package (Fox & Weisberg, 2019) to compute the Type-III Wald Chi-squared tests, the emmeans package (Lenth, 2020) to compute post-hoc comparisons with Tukey correction, and the performance package (Lüdecke, Makowski, Waggoner, Patil, & Ben-Shachar, 2020) to compute the Intraclass Coefficient (ICC) and the R-squared. Summary tables of the winning model, performed with the tab_model function in the sjPlot package (Lüdecke, 2021), are reported in Supplementary Materials. The accuracy was defined as the proportion of valid trials. Therefore, invalid trials included trials with incorrect responses (0.62%) and trials without response within the 6 sec (3.41%). We computed GLMM with a binominal distribution and a logit link function using the lmer4 package (Bates et al., 2015). Models were constructed, selected, and analysed following the same pathway as RT. 3 Results 3.1 Sample characterization The final sample consisted of 45 participants (15 women, 33.33%), with a mean age of 27.91 (SD = 5.51). Of those, 23 were diagnosed with schizophrenia or schizoaffective disorder and 22 had no diagnosis of psychiatric disorders. As expected, we found no significant differences in age, t(43) = 0.11, p = .913, d = 0.03, gender, χ 2 (1) = 0.04, p = .833, and education, χ 2 (4) = 3.76, p = .440. 3 3 Groups were statistically compared in terms of gender, age, and education for the following reasons: 1) a perfect person-by-person match was not always possible during the recruitment; 2) and two participants were excluded from the analysis. Of the clinical group, 22 (95.65%) were diagnosed with schizophrenia, while one (4.35%) was diagnosed with schizoaffective disorder. Sociodemographic and clinical data are described in Table 1 . 3.2 Neuropsychological characterization There was a significant difference between groups in TMT-A, t(43) = 6.15, p < .001, d = 1.83, with patients showing longer time of completion (M = 44.62, SD = 13.77) than controls (M = 24.72, SD = 6.52); and in TMT-B, t(43) = 3.43, p = .001, d = 1.02, again with patients showing longer time of completion (M = 98.36, SD = 30.80) than controls (M = 69.11; SD = 26.00). Despite patients (M = 31.55, SD = 6.28) reporting higher anxiety than controls (M = 29.86, SD = 4.39), the difference was not statistically significant, t(40) = 1.01, p = .316, d = 0.31. 3.3 Split-half reliability of response time The Spearman-brown corrected reliability estimates were 0.97, 95% CI = [0.95, 0.98] for fearful faces, 0.98, 95% CI = [0.97, 0.99] for happy faces, and 0.97, 95% CI = [0.95, 0.98] for neutral faces. These estimates indicate good reliability for RTs in all emotions. 3.4 Response time The winning model contained the intercept for individual and by-individual random slope for the effect of emotion (1 + emotion|individual) and the intercept for actor (1|actor) as random effects; group × emotion as fixed factors; and habituation as covariate. Ocular dominance, handedness, age, SAS, TMT-A, and TMT-B did not significantly improve the model (see Table A.1 for model comparison). The model showed a marginal R-squared of 0.31 and a conditional R-squared of 0.58. The ICC was of 0.39, which emphasises the need to account for random factors (Table A.2). Wald Chi-squared test revealed a significant main effect of group, χ2(1) = 7.42, p = .006). People with schizophrenia or schizoaffective disorder (M = 2.56, SE = 0.16) were slower to consciously detect the stimuli than healthy individuals (M = 2.08, SE = 0.17) (Fig. 2 ). There was a main effect of emotion, χ2(2) = 6.03, p = .049. RTs for happy faces (M = 2.24, SE = 0.14) were significantly shorter than for neutral (M = 2.36, SE = 0.13, p = .006) and fearful faces (M = 2.35, SE = 0.13, p < .001), with no difference between neutral and fearful faces (p = .931); and a main effect of habituation, χ2(1) = 532.59, p < .001), with decreases in RT as the task progressed. The interaction between group and emotion was not statistically significant, χ2(2) = 0.78, p = .677 (Fig. 3 ). Similar results were found after excluding the person with schizoaffective disorder (see Supplementary Materials). In exploratory analysis, each group was tested individually using the same factors as the winning model for the full sample. Models with by-individual random slope for the effect of emotion failed to converge, thus only the intercepts for individual (1|individual) and actor (1|actor) were included as random effects. In the control group’s model (Table A.3), there was a main effect of habituation, χ2(1) = 290.09, p < .001; and emotion, χ2(2) = 42.08, p < .001, with RTs for happy faces (M = 2.00, SE = 0.17) being significantly faster than neutral (M = 2.10, SE = 0.17, p < .001) and fearful faces (M = 2.09, SE = 0.17, p < .001), but with no difference between fearful and neutral faces (p = .816). Similarly, in the clinical group’s model (Table A.4), there was a main effect of habituation, χ2(1) = 264.78, p < .001; and emotion, χ2(2) = 41.38, p < .001, with RTs for happy faces (M = 2.48, SE = 0.18) being significantly faster than neutral (M = 2.62, SE = 0.18, p < .001) and fearful faces (M = 2.62, SE = 0.18, p < .001), but again with no difference between fearful and neutral (p = .963). 3.5 Accuracy The selected model for accuracy contained the intercept for individual (1|individual) and for actor (1|actor) as random effects; the group × emotion as fixed factors; and habituation as covariate (see Table A.5 for model comparison). Models with by-individual random slope for the effect of emotion failed to converge. The winning-model showed a marginal R-squared of 0.20 and a conditional R-squared of 0.49. The ICC by individual was of 0.34 and by actor was of 0.02 (Table A.6). Wald Chi-squared test revealed a main effect of group, χ2(1) = 12.42, p < .001. People with schizophrenia or schizoaffective disorder showed a lower proportion of valid trials (M = 0.97, SE = 0.01) than controls (M = 0.99, SE = 0.01). However, from most part, this was due to some trials without response. Indeed, when these trials were excluded from the analysis (9388 observations), no main effect of group was observed, χ2(1) = 0.01, p = .964, such that the proportion of correct responses was similar in the clinical (M = 0.99, SE = 0.01) and control groups (M = 0.99, SE = 0.01). Lastly, there was a main effect of habituation, χ2(1) = 126.65, p < .001), with higher accuracy as the task progressed. No main effect of emotion, χ2(2) = 4.65, p = .098, and no significant interaction between group and emotion, χ2(2) = 0.38, p = .829, were reported. Similar results were found after excluding the person with schizoaffective disorder (see Supplementary Materials). 4 Discussion In the present study, we investigated the access to visual awareness by emotional faces in schizophrenia-spectrum disorders by presenting fearful, happy, and neutral faces in a b-CFS paradigm. Emotional faces are important socioemotional cues whose explicit processing is compromised in schizophrenia and linked to social dysfunction (Couture et al., 2006; Kohler et al., 2010). Yet, results on the implicit processing of emotional faces are quite mixed, with tasks often requiring complex cognitive functions (Lee et al., 2016). Our study provides new insights about the mechanisms underlying implicit emotion perception in schizophrenia-spectrum disorders, by exploring how distinct facial expressions access to visual awareness in people with schizophrenia or schizoaffective disorder, compared to people without psychiatric disorders, while minimizing the recruitment of complex cognitive functions. As expected, people with schizophrenia-spectrum disorders took significantly longer to consciously detect a stimulus presented outside visual awareness than healthy individuals. This may be related to an elevated threshold for visual awareness observed in visual masking (Berkovitch et al., 2018, 2021; Del Cul et al., 2006) and visual detection paradigms with non-emotional stimuli (Lefebvre et al., 2021), as recently reviewed (Berkovitch et al., 2017). For example, in the study by Lefebvre et al. (2021), the target was presented at an initial duration of 400 ms, followed by decreases/increases of ~13 ms in duration based on participants’ response. The authors noted that people with schizophrenia, compared to people without psychiatric disorders, needed longer presentation times to consciously detect the stimulus. Moreover, the clinical group failed to activate the anterior cingulate cortex during the transition to visual awareness, and other brain regions (e.g., cuneus, visual cortex) during unconscious and conscious perception (Lefebvre et al., 2021). Based on the Global Neuronal Workspace (Dehaene & Changeux, 2011; Dehaene et al., 1998; Mashour et al., 2020), Lefebvre et al. (2021) concluded that this elevated threshold for visual awareness was due to impairments in the interplay between sensory pathways and higher-level deficits (as shown by alterations in the anterior cingulate). Although we cannot debate on the role of higher-level deficits, our results might in part be a reflection of sensory impairments in schizophrenia-spectrum disorders (Javitt, 2009). Therefore, people with schizophrenia might need an increased sensory salience (i.e., stimulus contrast) for the stimuli to be consciously detected. This is supported by evidence for an abnormal early visual perception in schizophrenia (Butler et al., 2003; Green et al., 2011, 2012; Herzog & Brand, 2015; Mathis et al., 2012; McCleery et al., 2020; Pitts et al., 2012). Nevertheless, the main effect of group should be interpreted with caution. It is possible that not only CFS-specific factors (i.e., differential processing that occurs specially under CFS) are at play, and future studies should attempt to control for non-CFS specific factors (Stein, 2019), which were not considered in the present study. The inclusion of a binocular control condition might be particularly relevant to rule out these non CFS-specific factors (Stein & Sterzer, 2014). In line with neurocognitive deficits in schizophrenia (Green, Horan, & Lee, 2019; Laere et al., 2018), the clinical group performed significantly worse in the TMT than the control group. Yet, we found that adding the TMT performance did not significantly improve the model. Groups did not statistically differ in sociodemographic characteristics (e.g., age, gender, education) and anxiety levels, and individual variations were taken into consideration in the model as random effects. Our findings therefore seem not to be modulated by these factors, but a more profound assessment is needed to examine the influence of factors other than the psychiatric diagnosis, such as IQ. There is also a current debate on whether post-perceptual factors, such as motor preparation, have an impact on suppression duration in b-CFS (Stein, 2019; Yang et al., 2014). This is particularly relevant because of the psychomotor slowing observed in schizophrenia-spectrum disorders (Morrens, Hulstijn, & Sabbe, 2007). Indeed, people with schizophrenia perform consistently slower in a variety of neurocognitive tasks, which is associated with alterations in the basal circuitry and peripheral inflammatory markers (Goldsmith et al., 2020). As such, the slower b-CFS (and TMT) performance in the clinical group might at least partly be linked to a delay in planning, programming, and executing the movements to complete the task. Future studies should attempt to separate the relative contribution of psychomotor slowing. A possible way to proceed is to conduct a simpler detection task, in which participants are instructed to indicate whether a stimulus (presented to both eyes in one of the four quadrants) appeared on the left or right side of the screen, by pressing the designated key on the keyboard, as rapidly and accurately as possible. This would allow to quantify the psychomotor slowing in a similar setting as the b-CFS task. The access to visual awareness by biologically relevant stimuli, such as human faces, was only recently explored in schizophrenia. By using b-CFS, Zhou et al. (2020) found that people with schizophrenia needed longer time to detect the presence of a face than people without psychiatric disorders, similar to the present study. Interestingly, the authors reported that in controls, but not in patients, self-face stimuli broke suppression significantly faster than famous faces, which may be related to bottom-up impairments in self-perception in schizophrenia (Zhou et al., 2020). However, others have reported an intact visual awareness for upright/inverted faces (Caruana et al., 2019), averted/direct eye gaze (Seymour et al., 2016), and angry/fearful faces with direct/averted eye gaze (Caruana & Seymour, 2021) in schizophrenia-spectrum disorders. Seymour et al. (2016) showed a similar b-CFS performance by people with schizophrenia and people without psychiatric disorders, with direct eye gaze breaking suppression significantly faster than averted gaze in both groups. Similarly, Caruana et al. (2019) reported a preserved perceptual prioritization of upright faces, compared to inverted faces, in schizophrenia-spectrum disorders. These mixed findings in the literature may be related to the use of distinct stimuli and methodology, namely suppressors, opacity manipulation, stimulus location, and low-level features (Pournaghdali & Schwartz, 2020). For instance, in the three studies showing similar suppression times (Caruana & Seymour, 2021; Caruana et al., 2019; Seymour et al., 2016), stimuli were viewed through a mirror stereoscope, while anaglyph glasses were used in our and Zhou et al. (2020) study. Anaglyph glasses are associated with some crosstalk between eyes because parts of the suppressed stimuli might bleed through the lenses (Baker, Kaestner, & Gouws, 2016; Carmel, Arcaro, Kastner, & Hasson, 2010). In our study, the elevated suppressor opacity presented to one eye would still overlay this effect during much of the trial. Thus, it seems unlikely that this potential crosstalk would increase the performance discrepancy between groups. Nevertheless, researchers should explore the use of distinct methods for inducing binocular rivalry in schizophrenia-spectrum disorders; see Hesselmann, Darcy, Rothkirch, and Sterzer (2018) for a detailed discussion. In contrast to our hypothesis, suppression times for fearful faces were not slower in people with schizophrenia-spectrum disorders and faster in people without psychiatric disorders, as reflected by the lack of a significant interaction between group and emotion. Instead, all participants, regardless of their group, were significantly faster at detecting happy faces than neutral and fearful ones. This suggests that the implicit perception of emotional faces is relatively intact in schizophrenia-spectrum disorders. Although these results do not support our hypothesis, similar findings are described in the literature (Aichert et al., 2013; Kring et al., 2014; Linden et al., 2010; Shasteen et al., 2016; van’t Wout, Aleman, et al., 2007). A previous study showed no differences between people with schizophrenia-spectrum disorders and people without psychiatric disorders in the interference of fearful faces during a gender identification task, suggesting a preserved automatic allocation of attention towards fearful faces (van’t Wout, Aleman, et al., 2007). Likewise, Linden et al. (2010) reported that, although people with schizophrenia performed worse than people without psychiatric disorders, they showed a similar interference of angry faces in face identification during a working memory task. Kring et al. (2014) observed that angry and happy faces suppressed from awareness affected the processing of non-suppressed neutral faces in people with schizophrenia and healthy individuals. The authors concluded that people with schizophrenia were able to perceive emotional cues from unseen faces, and that deficits in explicit emotional face perception may arise from the integration of a semantic context (i.e., affective label) with the perceptual information about the face (Kring et al., 2014). Our findings do not support the reports of an abnormal implicit perception of emotional faces in schizophrenia (Derntl & Habel, 2017; Liu et al., 2016; Park et al., 2011; Rauch et al., 2010; Suslow et al., 2013). Liu et al. (2016) showed that fearful faces presented for 100 ms failed to draw patients’ attention in an exogenous and automatic fashion – an attentional bias seen in healthy individuals. The authors concluded that fear processing is impaired by bottom-up factors in early visual perception in schizophrenia (Liu et al., 2016). Nevertheless, it is possible that these tasks do not solely reflect perceptual abnormalities because they require complex cognitive functions. Implicit emotional recognition speed (as assessed via RT) is linked to several cognitive factors: processing speed, impulsivity/inhibition, working-memory capacity, sensorimotor function, and sustained attention/vigilance (Mathersul et al., 2009). Along this line, Meyer and Lieberman (2012) reviewed that cognitive working memory load reduces the activation of brain areas involved in social cognitive processing (i.e., mentalizing network) in working memory for social information. An increased cognitive load may amplify the difference between people with schizophrenia and controls in social cognitive tasks. Therefore, by using a paradigm that minimizes the influence of high-level cognitive domains, we propose that the implicit perception of emotional faces is overall intact in schizophrenia-spectrum disorders, but that deficits arise when tasks become more demanding (Caruana & Seymour, 2021). Additionally, difficulties in recognizing and discriminating threat-related faces, namely fearful ones, are consistently reported in the schizophrenia literature (Kohler et al., 2003; Kuharic et al., 2019; Lee et al., 2010; Norton et al., 2009; Premkumar et al., 2008; Romero-Ferreiro et al., 2016; Won et al., 2019). These are observed irrespective of the antipsychotic treatment and illness severity (Kohler et al., 2010). Our outcomes suggest that these deficits, however, are most likely explained by top-down cognitive biases related to the judgment of faces, instead of bottom-up abnormalities at early perceptual stages (Caruana & Seymour, 2021; Caruana, Inkley, & El Zein, 2020; Kring et al., 2014; Shasteen et al., 2016). In a recent study by Caruana and Seymour (2021), angry and fearful faces with direct and averted gaze were presented using a b-CFS paradigm. The authors reported that, in the clinical and control groups, fearful faces were faster at breaking visual suppression than angry faces. Moreover, people with schizophrenia or schizoaffective disorder, compared to people without psychiatric disorders, performed significantly worse in a verbal comprehension task to evaluate first-order Theory of Mind inferences about the mental state of a character. Similar to our interpretation, these results point to a preserved implicit perception of fearful (and angry) faces in schizophrenia-spectrum disorders in early visual stages (Caruana & Seymour, 2021). Accordingly, a meta-analysis showed a more pronounced hypo-activation of the limbic system, with compensatory hyper-activation of the medial prefrontal cortex during explicit perception of fearful faces, compared to implicit perception, which might represent an inability to contextualize salient fearful faces (Dong et al., 2018). This is congruent with an increased tendency for paranoid patients to attribute anger to neutral faces (Pinkham, Brensinger, Kohler, Gur, & Gur, 2011), which stresses the role of prior knowledges, expectations, and goals in emotional face recognition in schizophrenia-spectrum disorders. In addition, RTs in the present study were significantly shorter for happy faces than neutral and fearful ones. Our findings are not consistent with the faster conscious detection of fearful faces in b-CFS (Capitão et al., 2014; Gray et al., 2013; Stein et al., 2014; Tsuchiya et al., 2009; Yang et al., 2007) – as reviewed by Hedger et al. (2016) – and with an overall preferential threat detection in healthy individuals (Öhman, Flykt, & Esteves, 2001; Öhman, Soares, Juth, Lindstörm, & Esteves, 2012; Soares, Lindström, Esteves, Öhman, & Nishijo, 2014). One possible explanation arises from stimuli preparation. For instance, Yang et al. (2007) normalized the stimuli’s RMS contrast – linked to the enhancement of high spatial frequencies (Ramanoel et al., 2014), which seems to play a role in fear recognition (Menzel, Redies, & Hayn-Leichsenring, 2018; Smith & Schyns, 2009) – and this might have inadvertently emphasized aspects relevant for detection. Concomitantly, Webb, Hibbard, and O’Gorman (2020) recommended caution when applying RMS contrast normalization since it seems to boost the salience of fearful faces, which may contribute to the fear advantage in b-CFS (Webb & Hibbard, 2020). Moreover, Yang et al. (2007) reported a similar fear advantage with inverted faces, thus highlighting the potential role of low-level properties (unaffected by inversion). Similar to our findings, other b-CFS studies have demonstrated that happy faces are significantly faster at breaking visual suppression than neutral and fearful faces (Yang et al., 2018) or angry faces (Hong, Yoon, & Peaco, 2015; Korb et al., 2017). As reviewed (Pool, Brosch, Delplanque, & Sander, 2016), a happy advantage is reported in paradigms such as dot-probe (Wirth & Wentura, 2020), letter-discrimination (Gupta, Young-Jin, & Lavia, 2016), inattentional blindness (Gupta & Srinivasan, 2015), and visual search (Becker, Anderson, Mortensen, Neufeld, & Neel, 2011; Calvo & Nummenmaa, 2008). It has been postulated that salient facial features (particularly in the mouth region) attract attention and facilitate the detection of happy faces (Calvo & Nummenmaa, 2008). This happy advantage is not merely explained by high local luminance in the teeth area – leading to high local contrast in the area around the mouth –, but by a combination of all features that characterize happy faces (Becker et al., 2011; Becker & Srinivasan, 2014; Calvo & Nummenmaa, 2008; Wirth & Wentura, 2020). Moreover, there is a consistent superior recognition of happy faces in the general population (Calvo & Nummenmaa, 2015), even outside of overt visual attention (Calvo, Nummenmaa, & Avero, 2010), and an intact recognition of happy faces in people with schizophrenia or schizoaffective disorder (Kring et al., 2014; Lee et al., 2010; Premkumar et al., 2008; Romero-Ferreiro et al., 2016). Taken together, happy faces are sometimes prioritized by the visual system, with evidence for a faster access to visual awareness in b-CFS, but further studies are needed to confirm this effect, not in the scope of the current work. Before drawing final conclusions, potential limitations need to be considered. First, it is possible that low-level features, such as the ones related to the teeth in happy faces (Yang et al., 2018), have played a role in suppression times (Gray et al., 2013; Jiang et al., 2007). To control for this effect, only happy and fearful faces with teeth exposure were selected. Stimuli were carefully prepared to avoid variations in size, colour, and luminance and, by including a random intercept for actor, random variations related to the selected faces were statistically controlled. Nevertheless, future studies should include a condition (e.g., inverted/upright faces) to control for the emotional content while keeping constant the low-level properties. Additionally, as previously stated, we did not add a binocular control task because of already extensive task duration. The lack of a control task does not allow to rule out the non CFS-specific factors and, therefore, we cannot assure that our findings reflect differential processing that occurred specially under CFS (Stein & Sterzer, 2014). We thus encourage researchers to include a control task whenever possible. Second, authors have argued that conscious experience should not be evaluated by dichotomous responses because stimuli that are only partially visible might not yet elicit a “yes” response (Pournaghdali & Schwartz, 2020; Stein, 2019). To account for this, we asked participants to answer as soon as the face or any part of the face become visible. However, it would be useful to include objective measures, such as eye-tracking (Vetter, Badde, Phelps, & Carrasco, 2019). Furthermore, the assumption that differences in detection speed in b-CFS reflect distinct unconscious processing has been recently challenged (Stein & Peelen, 2021). The authors proposed that the use of detection-discrimination paradigms would help to unravel the conscious and unconscious influences in visual perception by showing if stimuli that are more rapidly detected are consciously discriminated – supporting conscious processes – or not – supporting unconscious processes. Third, only male faces were used. Despite evidence that female faces break suppression significantly faster than male faces in healthy individuals (Wang, Tong, Shang, & Chen, 2019), this female advantage is not modulated by the facial expression (Hong et al., 2015). The explicit perception of face gender is relatively intact in schizophrenia (Bortolon et al., 2015), with no evidence for an abnormal implicit perception. Therefore, it is unlikely that our findings are limited to the use of male faces, but we advise researchers to use male and female faces in future studies to avoid this potential limitation. Fourth, all patients were medicated with antipsychotics, raising a potential effect of medication. Antipsychotics are relatively ineffective in treating social cognition (Kucharska-Pietura & Mortimer, 2013), and that deficits in visual abilities are also reported in unmedicated people with schizophrenia (Kéri, Kiss, Kelemen, Benedek, & Janka, 2005). One study revealed that the performance in a masking paradigm to investigate conscious access was not correlated with pharmacological treatment (Berkovitch et al., 2018). As such, and although we cannot fully exclude the effects of medication, it is possible that pharmacological treatment has little impact in our findings. Moreover, our clinical records included little information, which limited the analysis. Schizophrenia and schizoaffective disorder are very heterogeneous disorders, with patients exhibiting a wide range of symptomatology (American Psychiatric Association, 2013). For instance, one study showed that emotion perception is influenced by the predominance of positive or negative symptoms (van’t Wout, van Dijke, et al., 2007). We thus endorse researchers to adopt a more profound and independent clinical evaluation that allows subgroup comparisons. This evaluation would also include structured diagnostic interviews by clinicians other than the treating medical team to ascertain the diagnosis of all patients and the severity of psychotic symptoms. Fifth, the relatively modest sample size may not have been sufficiently large to reveal emotion-specific differences between groups, particularly considering the heterogeneity that characterizes schizophrenia-spectrum disorders. Yet, statistical power seems sufficient considering a medium effect of d = 0.45, based on subsequent power analysis. The inclusion of multiple observations per individual (i.e., three repetitions for each facial expression, stimulus location, and actor) can increase the power of experimental designs and decrease the variance within conditions (Brysbaert, 2019). Nevertheless, our findings should be considered preliminary before being replicated in a larger sample size. At last, our clinical sample included diagnosis of schizophrenia and schizoaffective disorder, characterized by the presence of affective symptoms together with psychotic symptoms (American Psychiatric Association, 2013). Despite focusing on a single diagnostic category might have improved the robustness of our findings, no categorical differences between schizophrenia and schizoaffective disorder are observed in brain structure and several domains of social/non-social cognition, including Theory of Mind, processing speed, and working memory (Hartman, Heinrichs, & Mashhadi, 2019). This is supported by our study, showing the same pattern of results after the exclusion of the person with schizoaffective disorder. Genome-wide association studies report an overlap of genetic aetiology between schizophrenia, schizoaffective, and bipolar disorder, and proposed a schizophrenia-bipolar continuum (not reflected in the traditional diagnostic categories) (The International Schizophrenia Consortium, 2009). This spectrum is supported by an overlapping of cognitive and perceptual abnormalities in schizophrenia and bipolar disorder (Berkovitch et al., 2021; Tamminga et al., 2014), although usually less severe in bipolar disorder. Future studies would therefore benefit from the inclusion of a clinical control group composed of people with bipolar disorder. Notwithstanding these limitations, our study is relevant to understand deficits in social domains in schizophrenia-spectrum disorders. An elevated threshold for visual awareness limits the information that reaches consciousness. This may be especially challenging in socioemotional contexts, where individuals are exposed to a wide range of biologically relevant stimuli – often in an implicit manner – that requires a rapid response. Furthermore, due to a slower access to visual awareness, people with schizophrenia-spectrum disorders may be less open to visual information that challenges their inferences, thus reinforcing their misperceptions about the social world (Berkovitch et al., 2017). Along this line, recent studies have stressed the role of remediation strategies that directly target basic visual perception in higher-level cognitive functions and even social cognition (Best & Bowie, 2017; Kurylo et al., 2018; Silverstein et al., 2020). A broader comprehension of the fundamental aspects of visual perception in schizophrenia-spectrum disorders will therefore enable the development of tailored interventions. 5 Conclusion We investigated implicit emotion perception in people with schizophrenia or schizoaffective disorder, compared to people without psychiatric disorders, by exploring access to visual awareness by emotional faces. People with schizophrenia-spectrum disorders were slower at consciously detecting the presence of a face stimulus than healthy individuals, which is congruent with an elevated threshold for visual awareness, although further evidence is needed to rule out the effects of non CFS-specific factors and psychomotor speed. However, there was no significant interaction between group and emotion. Instead, RTs for fearful faces did not differ from those for neutral faces, and happy faces were significantly faster at breaking visual suppression than neutral and fearful faces in all participants, regardless of the group. Our study suggests that the implicit perception of emotional faces is intact in schizophrenia-spectrum disorders, while the access to visual awareness might not be. Although they should be considered preliminary, our results contribute to the comprehension of visual consciousness in socioemotional contexts in schizophrenia-spectrum disorders. 6 Role of funding source This work was supported by the Portuguese Foundation for Science and Technology under a PhD grant to the first author [SFRH/BD/129980/2017] and within the R&D Units Center for Health Technology and Services Research [UID/IC/4255/2020] and William James Center for Research, University of Aveiro [UIBD/04810/2020]. The funding sources had no involvement in the study design; collection, analysis and interpretation of data; writing of the report; and decision to submit the article for publication. CRediT authorship contribution statement Joana Grave: Conceptualization, Methodology, Formal analysis, Investigation, Resources, Data curation, Writing – original draft, Visualization. Nuno Madeira: Conceptualization, Methodology, Resources, Writing – review & editing, Supervision. Maria João Martins: Conceptualization, Methodology, Investigation, Resources, Writing – review & editing. Samuel Silva: Conceptualization, Methodology, Software, Resources, Writing – review & editing. Sebastian Korb: Conceptualization, Methodology, Software, Formal analysis, Resources, Data curation, Writing – review & editing. Sandra Cristina Soares: Conceptualization, Methodology, Formal analysis, Investigation, Data curation, Writing – review & editing, Supervision. Declaration of competing interest The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: Nuno Madeira has been a consultant or advisory board member to Angelini, AstraZeneca, Ferrer and Janssen. All other authors have no conflicts of interest/competing interests to report. Acknowledgements We acknowledge the 26th European Congress of Psychiatry for publishing our preliminary findings in their conference proceedings. We would like to thank Ana Margarida Pinto, Diogo Carreiras, Julieta Azevedo, Filipa Gomes, Kelly Silva and Mélanie Silveira for their assistant during recruitment and data collection; and Pedro Bem-Haja and Cláudia Figueiredo for their support and critical comments during formal analysis. We gratefully acknowledge all of the participants. Finally, we acknowledge the anonymous reviewers’ insightful comments and suggestions, adding quality to this manuscript. Appendix A Supplementary material Supplementary data to this article can be found online at Appendix A Supplementary material The following are the Supplementary data to this article: Supplementary data 1 References Aichert et al., 2013 D.S. Aichert B. Derntl N.M. Wöstmann J.K. Groß S. Dehning A. Cerovecki U. Ettinger Intact emotion-cognition interaction in schizophrenia patients and first-degree relatives: Evidence from an emotional antisaccade task Brain and Cognition 82 3 2013 329 336 10.1016/j.bandc.2013.05.007 Aichert, D. S., Derntl, B., Wöstmann, N. M., Groß, J. K., Dehning, S., Cerovecki, A., Möller, H. J., Habel, U., Riedel, M., & Ettinger, U. (2013). Intact emotion-cognition interaction in schizophrenia patients and first-degree relatives: Evidence from an emotional antisaccade task. Brain and Cognition, 82, 329–336. American Psychiatric Association, 2013 American Psychiatric Association Diagnostic and Statistical Manual of Mental Disorders (DSM–5) 5th ed 2013 American Psychiatric Association American Psychiatric Association. (2013). Diagnostic and Statistical Manual of Mental Disorders (DSM–5) (5th ed). American Psychiatric Association. Amorim, 2000 P. Amorim Mini International Neuropsychiatric Interview (MINI): Validation of a short structured diagnostic psychiatric interview Braziliam Journal of Psychiatry 22 3 2000 10.1590/S1516-44462000000300003 Amorim, P. (2000). Mini International Neuropsychiatric Interview (MINI): Validation of a short structured diagnostic psychiatric interview. Braziliam Journal of Psychiatry, 22(3). Baker et al., 2016 D.H. Baker M. Kaestner A.D. Gouws Measurement of crosstalk in stereoscopic display systems used for vision research Journal of Vision 16 15 2016 1 10 10.1167/16.15.14 Baker, D. H., Kaestner, M., & Gouws, A. D. (2016). Measurement of crosstalk in stereoscopic display systems used for vision research. Journal of Vision, 16(15), 1–10. Bates, 2010 D.M. Bates Lme4: Mixed-effects modeling with R 2010 Springer Bates, D. M. (2010). Lme4: Mixed-effects modeling with R. Springer. Bates, Mächler, Bolker, & Walker, 2015 D.M. Bates M. Mächler B. Bolker S. Walker Fitting linear mixed-effects models using lme4 Journal of Statistical Software 67 1 2015 1 48 10.18637/jss.v067.i01 Bates, D. M., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. Becker et al., 2011 D.V. Becker U.S. Anderson C.R. Mortensen S.L. Neufeld R. Neel The face in the crowd effect unconfounded: Happy faces, not angry faces, are more efficiently detected in single- and multiple-target visual search tasks Journal of Experimental Psychology: General 140 4 2011 637 659 10.1037/a0024060 Becker, D. V, Anderson, U. S., Mortensen, C. R., Neufeld, S. L., & Neel, R. (2011). The face in the crowd effect unconfounded: Happy faces, not angry faces, are more efficiently detected in single- and multiple-target visual search tasks. Journal of Experimental Psychology: General, 140(4), 637–659. Becker and Srinivasan, 2014 D.V. Becker N. Srinivasan The vividness of the happy face Current Directions in Psychological Science 23 3 2014 189 194 10.1177/0963721414533702 Becker, D. V, & Srinivasan, N. (2014). The vividness of the happy face. Current Directions in Psychological Science, 23(3), 189–194. Bedwell et al., 2013 J.S. Bedwell C.C. Chan O. Cohen Y. Karbi E. Shamir Y. Rassovsky The magnocellular visual pathway and facial emotion misattribution errors in schizophrenia Progress in Neuro-Psychopharmacology and Biological Psychiatry 44 2013 88 93 10.1016/j.pnpbp.2013.01.015 Bedwell, J. S., Chan, C. C., Cohen, O., Karbi, Y., Shamir, E., & Rassovsky, Y. (2013). The magnocellular visual pathway and facial emotion misattribution errors in schizophrenia. Progress in Neuro-Psychopharmacology and Biological Psychiatry, 44, 88–93. Berkovitch et al., 2021 L. Berkovitch L. Charles A. Del Cul N. Hamdani M. Delavest S. Sarrazin J. Houenou Disruption of conscious access in psychosis is associated with altered structural brain connectivity The Journal of Neuroscience 41 3 2021 513 523 10.1523/JNEUROSCI.0945-20.2020 Berkovitch, L., Charles, L., Del Cul, A., Hamdani, N., Delavest, M., Sarrazin, S., Mangin, J. F., Guevara, P., Ji, E., D’Albis, M. A., Gaillard, R., Bellivier, F., Poupon, C., Leboyer, M., Tamouza, R., Dehaene, S., & Houenou, J. (2020). Disruption of conscious access in psychosis is associated with altered structural brain connectivity. The Journal of Neuroscience, 41(3), 513–523. Berkovitch et al., 2017 L. Berkovitch S. Dehaene R. Gaillard Disruption of conscious access in schizophrenia Trends in Cognitive Sciences 21 11 2017 878 892 10.1016/j.tics.2017.08.006 Berkovitch, L., Dehaene, S., & Gaillard, R. (2017). Disruption of conscious access in schizophrenia. Trends in Cognitive Sciences, 21(11), 878–892. Berkovitch et al., 2018 L. Berkovitch A. Del Cul M. Maheu S. Dehaene Impaired conscious access and abnormal attentional amplification in schizophrenia NeuroImage: Clinical 18 2018 835 848 10.1016/j.nicl.2018.03.010 Berkovitch, L., Del Cul, A., Maheu, M., & Dehaene, S. (2018). Impaired conscious access and abnormal attentional amplification in schizophrenia. NeuroImage: Clinical, 18, 835–848. Best and Bowie, 2017 M.W. Best C.R. Bowie A review of cognitive remediation approaches for schizophrenia: From top-down to bottom-up, brain training to psychotherapy Expert Review of Neurotherapeutics 17 7 2017 713 723 10.1080/14737175.2017.1331128 Best, M. W., & Bowie, C. R. (2017). A review of cognitive remediation approaches for schizophrenia: from top-down to bottom-up, brain training to psychotherapy. Expert Review of Neurotherapeutics, 17(7), 713–723. Bortolon et al., 2015 C. Bortolon D. Capdevielle S. Raffard Face recognition in schizophrenia disorder: A comprehensive review of behavioral, neuroimaging and neurophysiological studies Neuroscience and Biobehavioral Reviews 53 2015 79 107 10.1016/j.neubiorev.2015.03.006 Bortolon, C., Capdevielle, D., & Raffard, S. (2015). Face recognition in schizophrenia disorder: A comprehensive review of behavioral, neuroimaging and neurophysiological studies. Neuroscience and Biobehavioral Reviews, 53, 79–107. Brysbaert, 2019 M. Brysbaert How many participants do we have to include in properly powered experiments? A tutorial of power analysis with some simple guidelines Journal of Cognition 2 1 2019 16 10.5334/joc.72 Brysbaert, M. (2019). How many participants do we have to include in properly powered experiments? A tutorial of power analysis with some simple guidelines. Journal of Cognition, 2(1), 1–38. Butler et al., 2009 P.D. Butler I.Y. Abeles N.G. Weiskopf A. Tambini M. Jalbrzikowski M.E. Legatt D.C. Javitt Sensory contributions to impaired emotion processing in schizophrenia Schizophrenia Bulletin 35 6 2009 1095 1107 10.1093/schbul/sbp109 Butler, P. D., Abeles, I. Y., Weiskopf, N. G., Tambini, A., Jalbrzikowski, M., Legatt, M. E., Zemon, V., Loughead, J., Gur, R. C., & Javitt, D. C. (2009). Sensory contributions to impaired emotion processing in schizophrenia. Schizophrenia Bulletin, 35(6), 1095–1107. Butler et al., 2003 P.D. Butler L.A. DeSanti J. Maddox J.M. Harkavy-Friedman X.F. Amador R.R. Goetz J.M. Gorman Visual backward-masking deficits in schizophrenia: Relationship to visual pathway function and symptomatology Schizophrenia Research 59 2-3 2003 199 209 10.1016/S0920-9964(01)00341-3 Butler, P. D., DeSanti, L. A., Maddox, J., Harkavy-Friedman, J. M., Amador, X. F., Goetz, R. R., Javitt, D. C., & Gorman, J. M. (2002). Visual backward-masking deficits in schizophrenia: Relationship to visual pathway function and symptomatology. Schizophrenia Research, 59, 199–209. Calvo et al., 2010 M.G. Calvo L. Nummenmaa P. Avero Recognition advantage of happy faces in extrafoveal vision: Featural and affective processing Visual Cognition 18 9 2010 1274 1297 10.1080/13506285.2010.481867 Calvo, M. G. (2010). Recognition advantage of happy faces in extrafoveal vision: Featural and affective processing. Visual Cognition, 18(9), 1274–1297. Calvo and Nummenmaa, 2008 M.G. Calvo L. Nummenmaa Detection of emotional faces: Salient physical features guide effective visual search Journal of Experimental Psychology: General 137 3 2008 471 494 10.1037/a0012771 Calvo, M. G., & Nummenmaa, L. (2008). Detection of Emotional Faces: Salient Physical Features Guide Effective Visual Search. Journal of Experimental Psychology: General, 137(3), 471–494. Calvo and Nummenmaa, 2015 M.G. Calvo L. Nummenmaa Perceptual and affective mechanisms in facial expression recognition: An integrative review Cognition and Emotion 30 6 2015 1081 1106 10.1080/02699931.2015.1049124 Calvo, M. G., & Nummenmaa, L. (2015). Perceptual and affective mechanisms in facial expression recognition: An integrative review. Cognition and Emotion, 30(6), 1081–1106. Capitão et al., 2014 L.P. Capitão S.J.V. Underdown S. Vile E. Yang C.J. Harmer S.E. Murphy Anxiety increases breakthrough of threat stimuli in continuous flash suppression Emotion 14 6 2014 1027 1036 10.1037/a0037801 Capitão, L. P., Underdown, S. J. V., Vile, S., Yang, E., Harmer, C. J., & Murphy, S. E. (2014). Anxiety increases breakthrough of threat stimuli in continuous flash suppression. Emotion, 14(6), 1027–1036. Carmel, Arcaro, Kastner, & Hasson, 2010 D. Carmel M. Arcaro S. Kastner U. Hasson How to create and use binocular rivalry Journal of Visualized Experiments 45 2010 2030 10.3791/2030 Carmel, D., Arcaro, M., Kastner, S., & Hasson, U. (2010). How to create and use binocular rivalry. Journal of Visualized Experiments, 45, 1–10. Caruana et al., 2020 N. Caruana C. Inkley M. El Zein Gaze direction biases emotion categorisation in schizophrenia Schizophrenia Research: Cognition 21 2020 100181 10.1016/j.scog.2020.100181 Caruana, N., Inkley, C., & El Zein, M. (2020). Gaze direction biases emotion categorisation in schizophrenia. Schizophrenia Research: Cognition, 21, 100181. Caruana and Seymour, 2021 N. Caruana K. Seymour Bottom-up processing of fearful and angry facial expressions is intact in schizophrenia Cognitive Neuropsychiatry 26 3 2021 183 198 10.1080/13546805.2021.1902794 Caruana, N., & Seymour, K. (2021). Bottom-up processing of fearful and angry facial expressions is intact in schizophrenia. Cognitive Neuropsychiatry, 1–16. Caruana et al., 2019 N. Caruana T. Stein T. Watson N. Williams K. Seymour Intact prioritisation of unconscious face processing in schizophrenia Cognitive Neuropsychiatry 24 2 2019 135 151 10.1080/13546805.2019.1590189 Caruana, N., Stein, T., Watson, T., Williams, N., & Seymour, K. (2019). Intact prioritisation of unconscious face processing in schizophrenia. Cognitive Neuropsychiatry, 24(2), 135–151. Cavaco et al., 2013 S. Cavaco A. Goncalves C. Pinto E. Almeida F. Gomes I. Moreira A. Teixeira-Pinto Trail making test: Regression-based norms for the portuguese population Archives of Clinical Neuropsychology 28 2 2013 189 198 10.1093/arclin/acs115 Cavaco, S., Gonçalves, A., Pinto, C., Almeida, E., Gomes, F., Moreira, I., Fernandes, J., & Teixeira-Pinto, A. (2013). Trail making test: Regression-based norms for the portuguese population. Archives of Clinical Neuropsychology, 28(2), 189–198. Chan et al., 2010 R.C.K. Chan H. Li E.F.C. Cheung Q. Gong Impaired facial emotion perception in schizophrenia: A meta-analysis Psychiatry Research 178 2 2010 381 390 10.1016/j.psychres.2009.03.035 Chan, R. C. K., Li, H., Cheung, E. F. C., & Gong, Q. (2010). Impaired facial emotion perception in schizophrenia : A meta-analysis. Psychiatry Research, 178(2), 381–390. Clark et al., 2013 C.M. Clark F. Gosselin V.M. Goghari Aberrant patterns of visual facial information usage in schizophrenia Journal of Abnormal Psychology 122 2 2013 513 519 10.1037/a0031944 Clark, C. M., Gosselin, F., & Goghari, V. M. (2013). Aberrant patterns of visual facial information usage in schizophrenia. Journal of Abnormal Psychology, 122(2), 513–519. Connor et al., 2004 C.E. Connor H.E. Egeth S. Yantis Visual attention: Bottom-up versus top-down Current Biology 14 19 2004 R850 R852 10.1016/j.cub.2004.09.041 Connor, C. E., Egeth, H. E., & Yantis, S. (2004). Visual attention: Bottom-up versus top-down. Current Biology, 14(19), R850–R852. Couture, Penn, & Roberts, 2006 S.M. Couture D.L. Penn D.L. Roberts The functional significance of social cognition in schizophrenia: A review Schizophrenia Bulletin 32 Suppl. 1 2006 S44 S63 10.1093/schbul/sbl029 Couture, S. M., Penn, D. L., & Roberts, D. L. (2006). The functional significance of social cognition in schizophrenia: A review. Schizophrenia Bulletin, 32(Suppl~. 1), S44–S63. Davis et al., 2011 F.C. Davis L.H. Somerville E.J. Ruberry A.B.L. Berry L.M. Shin P.J. Whalen A tale of two negatives: Differential memory modulation by threat-related facial expressions Emotion 11 3 2011 647 655 10.1037/a0021625 Davis, F. C., Somerville, L. H., Ruberry, E. J., Berry, A. B. L., Shin, L. M., & Whalen, P. J. (2011). A tale of two negatives: Differential memory modulation by threat-related facial expressions. Emotion, 11(3), 647–655. Dehaene & Changeux, 2011 S. Dehaene J. Changeux Experimental and theoretical approaches to conscious processing Neuron 70 2 2011 200 227 10.1016/j.neuron.2011.03.018 Dehaene, S., & Changeus, J. P. (2011). Experimental and theoretical approaches to conscious processing. Neuron, 70, 200–227. Dehaene et al., 1998 S. Dehaene M. Kerszberg J. Changeus A neuronal model of a global workspace in effortful cognitive tasks Proceedings of the National Academy of Sciences of the United States of America 95 24 1998 14529 14534 10.1073/pnas.95.24.14529 Dehaene, S., Kerszberg, M., & Changeus, J. (1998). A neuronal model of a global workspace in effortful cognitive tasks. Proceedings of the National Academy of Sciences of the United States of America, 95(24), 14529–14534. Del Cul et al., 2006 A. Del Cul S. Dehaene M. Leboyer Preserved subliminal processing and impaired conscious access in schizophrenia Archives of General Psychiatry 63 2006 1313 1323 10.1001/archpsyc.63.12.1313 Del Cul, A., Dehaene, S., & Leboyer, M. (2006). Preserved subliminal processing and impaired conscious access in schizophrenia. Archives of General Psychiatry, 63, 1313–1323. Derntl and Habel, 2017 B. Derntl U. Habel Angry but not neutral faces facilitate response inhibition in schizophrenia patients European Archives of Psychiatry and Clinical Neuroscience 267 7 2017 621 627 10.1007/s00406-016-0748-8 Derntl, B., & Habel, U. (2017). Angry but not neutral faces facilitate response inhibition in schizophrenia patients. European Archives of Psychiatry and Clinical Neuroscience, 267, 621–627. Dong et al., 2018 D. Dong Y. Wang X. Jia Y. Li X. Chang M. Vandekerckhove D. Yao Abnormal brain activation during threatening face processing in schizophrenia: A meta-analysis of functional neuroimaging studies Schizophrenia Research 197 2018 200 208 10.1016/j.schres.2017.11.013 Dong, D., Wang, Y., Jia, X., Li, Y., Chang, X., Vandekerckhove, M., Luo, C., & Yao, D. (2018). Abnormal brain activation during threatening face processing in schizophrenia: A meta-analysis of functional neuroimaging studies. Schizophrenia Research, 197, 200–208. Edwards et al., 2002 J. Edwards H.J. Jackson P.E. Pattison Emotion recognition via facial expression and affective prosody in schizophrenia: A methodological review Clinical Psychology Review 22 6 2002 789 832 10.1016/S0272-7358(02)00130-7 Edwards, J., Jackson, H. J., & Pattison, P. E. (2002). Emotion recognition via facial expression and affective prosody in schizophrenia: A methodological review. Clinical Psychology Review, 22(6), 789–832. Engell and Quilliam, 2020 A.D. Engell H.M. Quilliam Faces under continuous flash suppression capture attention faster than object, but without a face-evoked steady-state visual potentials: Is curvilinearity responsible for the behavioral effect? Journal of Vision 20 6 2020 14 10.1167/jov.20.6.14 Engell, A. D., & Quilliam, H. M. (2020). Faces under continuous flash suppression capture attention faster than object, but without a face-evoked steady-state visual potentials: Is curvilinearity responsible for the behavioral effect? Journal of Vision, 20(6), 14. Fett et al., 2011 A. Fett W. Viechtbauer M.G. Dominguez D.L. Penn J. van Os L. Krabbendam The relationship between neurocognition and social cognition with functional outcomes in schizophrenia: A meta-analysis Neuroscience and Biobehavioral Reviews 35 3 2011 573 588 10.1016/j.neubiorev.2010.07.001 Fett, A. K. J., Viechtbauer, W., Dominguez, M. de G., Penn, D. L., van Os, J., & Krabbendam, L. (2011). The relationship between neurocognition and social cognition with functional outcomes in schizophrenia: A meta-analysis. Neuroscience and Biobehavioral Reviews, 35(3), 573–588. Fox and Weisberg, 2019 J. Fox S. Weisberg An R companion to applied regression 3rd ed. 2019 Saga Publishing Fox, J., & Weisberg, S. (2019). An R companion to applied regression (3rd editio). Saga Publishing. Frith, 2009 C. Frith Role of facial expressions in social interactions Philosophical Transactions of the Royal Society B: Biological Sciences 364 1535 2009 3453 3458 10.1098/rstb.2009.0142 Frith, C. (2009). Role of facial expressions in social interactions. Philosophical Transactions of the Royal Society B: Biological Sciences, 364, 3453–3458. Gayet et al., 2014 S. Gayet S. Van Der Stigchel C.L.E. Paffen Breaking continuous flash suppression: Competing for consciousness on the pre-semantic battlefield Frontiers in Psychology 5 2014 460 10.3389/fpsyg.2014.00460 Gayet, S., Van Der Stigchel, S., & Paffen, C. L. E. (2014). Breaking continuous flash suppression: Competing for consciousness on the pre-semantic battlefield. Frontiers in Psychology, 5, 460. Goldsmith et al., 2020 D.R. Goldsmith N. Massa B.D. Pearce E.C. Wommack A. Alrohaibani N. Goel E. Duncan Inflammatory markers are associated with psychomotor slowing in patients with schizophrenia compared to healthy controls Npj Schizophrenia 6 1 2020 8 10.1038/s41537-020-0098-4 Goldsmith, D. R., Massa, N., Pearce, B. D., Wommack, E. C., Alrohaibani, A., Goel, N., Cuthbert, B., Fargotstein, M., Felger, J. C., Haroon, E., Miller, A. H., & Duncan, E. (2020). Inflammatory markers are associated with psychomotor slowing in patients with schizophrenia compared to healthy controls. Npj Schizophrenia, 6(1), 1–8. Gomes et al., 2018 N. Gomes S.C. Soares S. Silva C.F. Silva Mind the snake: Fear detection relies on low spatial frequencies Emotion 18 6 2018 886 895 10.1037/emo0000391 Gomes, N., Soares, S. C., Silva, S., & Silva, C. F. (2018). Mind the snake: Fear detection relies on low spatial frequencies. Emotion, 18(6), 886–895. Goodhew & Edwards, 2019 S.C. Goodhew M. Edwards Translating experimental paradigms into individual-differences research: Contributions, challenges, and practical recommendations Consciousness and Cognition 69 2019 14 25 10.1016/j.concog.2019.01.008 Goodhew, S. C., & Edwards, M. (2019). Translating experimental paradigms into individual-differences research: Contributions, challenges, and practical recommendations. Consciousness and Cognition, 69(January), 14–25. Gray et al., 2013 K.L.H. Gray W.J. Adams N. Hedger K.E. Newton M. Garner Faces and awareness: Low-level, not emotional factors determine perceptual dominance Emotion 13 3 2013 537 544 10.1037/a0031403 Gray, K. L. H., Adams, W. J., Hedger, N., Newton, K. E., & Garner, M. (2013). Faces and awareness: Low-level, not emotional factors determine perceptual dominance. Emotion, 13(3), 537–544. Green et al., 2012 M.F. Green G. Hellemann W.P. Horan J. Lee J.K. Wynn From perception to functional outcome in schizophrenia: Modeling the role of ability and motivation Archives of General Psychiatry 69 12 2012 1216 1224 10.1001/archgenpsychiatry.2012.652 Green, M. F., Hellemann, G., Horan, W. P., Lee, J., & Wynn, J. K. (2012). From perception to functional outcome in schizophrenia: Modeling the role of ability and motivation. Archives of General Psychiatry, 69(12), 1216–1224. Green, Horan, & Lee, 2015 M.F. Green W.P. Horan J. Lee Social cognition in schizophrenia Nature Reviews. Neuroscience 16 10 2015 620 631 10.1038/nrn4005 Green, M. F., Horan, W. P., & Lee, J. (2015). Social cognition in schizophrenia. Nature Publishing Group, 16, 620–631. Green et al., 2019 M.F. Green W.P. Horan J. Lee Nonsocial and social cognition in schizophrenia: Current evidence and future directions World Psychiatry 18 2 2019 146 161 10.1002/wps.v18.210.1002/wps.20624 Green, M. F., Horan, W. P., & Lee, J. (2019). Nonsocial and social cognition in schizophrenia: current evidence and future directions. World Psychiatry, 18(2), 146–161. Green et al., 2011 M.F. Green J. Lee J.K. Wynn K.I. Mathis Visual masking in schizophrenia: Overview and theoretical implications Schizophrenia Bulletin 37 4 2011 700 708 10.1093/schbul/sbr051 Green, M. F., Lee, J., Wynn, J. K., & Mathis, K. I. (2011). Visual masking in schizophrenia: Overview and theoretical implications. Schizophrenia Bulletin, 37(4), 700–708. Günther et al., 2017 V. Günther J. Zimmer A. Kersting K.T. Hoffmann D. Lobsien T. Suslow Automatic processing of emotional facial expressions as a function of social anhedonia Psychiatry Research: Neuroimaging 270 2017 46 53 10.1016/j.pscychresns.2017.10.002 Günther, V., Zimmer, J., Kersting, A., Hoffmann, K. T., Lobsien, D., & Suslow, T. (2017). Automatic processing of emotional facial expressions as a function of social anhedonia. Psychiatry Research: Neuroimaging, 270, 46–53. Gupta and Srinivasan, 2015 R. Gupta N. Srinivasan Only irrelevant sad but not happy faces are inhibited under high perceptual load Cognition and Emotion 29 4 2015 747 754 10.1080/02699931.2014.933735 Gupta, R., & Srinivasan, N. (2015). Only irrelevant sad but not happy faces are inhibited under high perceptual load. Cognition and Emotion, 4, 747–754. Gupta et al., 2016 R. Gupta H. Young-Jin N. Lavia Distrated by pleasure: Effects of positive versus negative valence on emotional capture under load Emotion 16 3 2016 328 337 10.1037/emo0000112 Gupta, R., Young-Jin, H., & Lavia, N. (2016). Distrated by pleasure: Effects of positive versus negative valence on emotional capture under load. Emotion, 16(3), 328–337. Hajdúk et al., 2019 M. Hajdúk H.S. Klein E.L. Bass C.R. Springfield A.E. Pinkham Implicit and explicit processing of bodily emotions in schizophrenia Cognitive Neuropsychiatry 25 2 2019 139 153 10.1080/13546805.2019.1706465 Hajdúk, M., Klein, H. S., Bass, E. L., Springfield, C. R., & Pinkham, A. E. (2019). Implicit and explicit processing of bodily emotions in schizophrenia. Cognitive Neuropsychiatry, 25(2), 139–153. Hartman, Heinrichs, & Mashhadi, 2019 L.I. Hartman R.W. Heinrichs F. Mashhadi The continuing story of schizophrenia and schizoaffective disorder: One condition or two? Schizophrenia Research: Cognition 16 2019 36 42 10.1016/j.scog.2019.01.001 Hartman, L. I., Heinrichs, R. W., & Mashhadi, F. (2019). The continuing story of schizophrenia and schizoaffective disorder: One condition or two? Schizophrenia Research: Cognition, 16(November 2018), 36–42. Hedge et al., 2018 C. Hedge G. Powell P. Sumner The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences Behavior Research Methods 50 3 2018 1166 1186 10.3758/s13428-017-0935-1 Hedge, C., Powell, G., & Sumner, P. (2018). The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences. Behavior Research Methods, 50(3), 1166–1186. Hedger et al., 2016 N. Hedger K.L.H. Gray M. Garner W.J. Adams Are visual threats prioritized without awareness? A critical review and meta-analysis involving 3 behavioral paradigms and 2696 observers Psychological Bulletin 142 9 2016 934 968 10.1037/bul0000054 Hedger, N., Gray, K. L. H., Garner, M., & Adams, W. J. (2016). Are visual threats prioritized without awareness? A critical review and meta-analysis involving 3 behavioral paradigms and 2696 observers. Psychological Bulletin, 142(9), 934–968. Herzog and Brand, 2015 M.H. Herzog A. Brand Visual masking & schizophrenia Schizophrenia Research: Cognition 2 2 2015 64 71 10.1016/j.scog.2015.04.001 Herzog, M.H., & Brand, A. (2015). Visual masking & schizophrenia. Schizophrenia Research: Cognition, 2(2), 64–71. doi:10.1016/j.scog.2015.04.001. Hesselmann et al., 2018 G. Hesselmann N. Darcy M. Rothkirch P. Sterzer Investigating masked priming along the “vision-for-perception” and “vision-for-action” dimensions of unconscious processing Journal of Experimental Psychology 147 11 2018 1641 1659 10.1037/xge0000420 Hesselmann, G., Darcy, N., Rothkirch, M., & Sterzer, P. (2018). Investigating masked priming along the “vision-for-perception” and “vision-for-action” dimensions of unconscious processing. Journal of Experimental Psychology, 147(11), 1641–1659. Hong, Yoon, & Peaco, 2015 S.W. Hong K.L. Yoon S. Peaco Sex differences in perception of invisible facial expressions Frontiers in Psychology 6 2015 392 10.3389/fpsyg.2015.00392 Hong, S. W., Lira Yoon, K., & Peaco, S. (2015). Sex differences in perception of invisible facial expressions. Frontiers in Psychology, 6(APR), 1–8. Horan, Lee, & Green, 2013 W.P. Horan J. Lee M.F. Green Social cognition and functional outcome in schizophrenia D.L. Roberts D.L. Penn Social cognition in schizophrenia: From evidence to treatment 2013 Oxford University Press 151 172 Horan, W. P., Lee, J., & Green, M. F. (2013). Social cognition and functional outcome in schizophrenia. In D. L. Roberts & D. L. Penn (Eds.), Social cognition in schizophrenia: From evidence to treatment (pp. 151–172). Jahshan et al., 2017 C. Jahshan M. Wolf Y. Karbi E. Shamir Y. Rassovsky Probing the magnocellular and parvocellular visual pathways in facial emotion perception in schizophrenia Psychiatry Research 253 2017 38 42 10.1016/j.psychres.2017.03.031 Jahshan, C., Wolf, M., Karbi, Y., Shamir, E., & Rassovsky, Y. (2017). Probing the magnocellular and parvocellular visual pathways in facial emotion perception in schizophrenia. Psychiatry Research, 253, 38–42. Javitt, 2009 D.C. Javitt When doors of pereption close: Bottom-up models of disrupted cognition in schizophrenia Annual Review of Clinical Psychology 5 2009 249 275 10.1146/annurev.clinpsy.032408.153502 Javitt, D. C. (2009). When doors of pereption close: Bottom-up models of disrupted cognition in schizophrenia. Annual Review of Clinical Psychology, 5, 249–275. Jiang, Costello, & He, 2007 Y. Jiang P. Costello S. He Processing of invisible stimuli: Advantage of upright faces and recognizable words in overcoming interocular suppression Psychological Science 18 4 2007 349 355 10.1111/j.1467-9280.2007.01902.x Jiang, Y., Costello, P., & He, S. (2007). Processing of invisible stimuli: Advantage of upright faces and recognizable words in overcoming interocular suppression. Psychological Science, 18(4), 349–356. Kéri et al., 2005 S. Kéri I. Kiss O. Kelemen G. Benedek Z. Janka Anomalous visual experiences, negative symptoms, perceptual organization and the magnocellular pathway in schizophrenia: A shared construct? Psychological Medicine 35 10 2005 1445 1455 10.1017/S0033291705005398 Kéri, S., Kiss, I., Kelemen, O., Benedek, G., & Janka, Z. (2005). Anomalous visual experiences, negative symptoms, perceptual organization and the magnocellular pathway in schizophrenia: A shared construct? Psychological Medicine, 35(10), 1445–1455. Kerr, Hesselmann, Räling, Wartenburger, & Sterzer, 2017 J.A. Kerr G. Hesselmann R. Räling I. Wartenburger P. Sterzer Choice of analysis pathway dramatically affects statistical outcomes in breaking continuous flash suppression Scientific Reports 7 2017 3002 10.1038/s41598-017-03396-3 Kerr, J. A., Hesselmann, G., Räling, R., & Wartenburger, I. (2017). Choice of analysis pathway dramatically affects statistical outcomes in breaking continuous flash suppression. Scientific Reports, 7, 3002. Kim et al., 2015 D.W. Kim M. Shim M.J. Song C.H. Im S.H. Lee Early visual processing deficits in patients with schizophrenia during spatial frequency-dependent facial affect processing Schizophrenia Research 161 2–3 2015 314 321 10.1016/j.schres.2014.12.020 Kim, D. W., Shim, M., Song, M. J., Im, C. H., & Lee, S. H. (2015). Early visual processing deficits in patients with schizophrenia during spatial frequency-dependent facial affect processing. Schizophrenia Research, 161(2–3), 314–321. Kohler et al., 2003 C.G. Kohler T.H. Turner W.B. Bilker C.M. Brensinger S.J. Siegel S.J. Kanes R.C. Gur Facial emotion recognition in schizophrenia: Intensity effects and error pattern American Journal of Psychiatry 160 10 2003 1768 1774 10.1176/appi.ajp.160.10.1768 Kohler, C. G., Turner, T. H., Bilker, W. B., Brensinger, C. M., Siegel, S. J., Kanes, S. J., Gur, R. E., & Gur, R. C. (2003). Facial emotion recognition in schizophrenia: Intensity effects and error pattern. American Journal of Psychiatry, 160(10), 1768–1774. Kohler et al., 2010 C.G. Kohler J.B. Walker E.A. Martin K.M. Healey P.J. Moberg Facial emotion perception in schizophrenia: A meta-analytic review Schizophrenia Bulletin 36 5 2010 1009 1019 10.1093/schbul/sbn192 Kohler, C. G., Walker, J. B., Martin, E. A., Healey, K. M., & Moberg, P. J. (2010). Facial emotion perception in schizophrenia: A meta-analytic review. Schizophrenia Bulletin, 36(5), 1009–1019. Korb, Osimo, Suran, Goldstein, & Rumiati, 2017 S. Korb S.A. Osimo T. Suran A. Goldstein R.I. Rumiati Face proprioception does not modulate access to visual awareness of emotional faces in a continuous flash suppression paradigm Consciousness and Cognition 51 2017 166 180 10.1016/j.concog.2017.03.008 Korb, S., Osimo, S. A., Suran, T., Goldstein, A., & Rumiati, R. I. (2017). Face proprioception does not modulate access to visual awareness of emotional faces in a continuous flash suppression paradigm. Consciousness and Cognition, 51(October 2016), 166–180. Kring et al., 2014 A.M. Kring E.H. Siegel L.F. Barrett Unseen affective faces influence person perception judgments in schizophrenia Clinical Psychological Science 2 4 2014 443 454 10.1177/2167702614536161 Kring, A. M., Siegel, E. H., & Barrett, L. F. (2014). Unseen affective faces influence person perception judgments in schizophrenia. Clinical Psychological Science, 2(4), 443–454. Kucharska-Pietura and Mortimer, 2013 K. Kucharska-Pietura A. Mortimer Can antipsychotics improve social cognition in patients with schizophrenia? CNS Drugs 27 5 2013 335 343 10.1007/s40263-013-0047-0 Kucharska-Pietura, K., & Mortimer, A. (2013). Can antipsychotics improve social cognition in patients with schizophrenia? CNS Drugs, 27(5), 335–343. Kuharic et al., 2019 D.B. Kuharic P. Makaric I. Kekin I.L. Lovrencic A. Savic D. Ostojic M.R. Kuzman Differences in facial emotional recognition between patients with the first-episode psychosis, multi-episode schizophrenia, and healthy controls Journal of the International Neuropsychological Society 25 2 2019 165 173 10.1017/S1355617718001029 Kuharic, D. B., Makaric, P., Kekin, I., Lovrencic, I. L., Savic, A., Ostojic, D., Silic, A., Brecic, P., Bajic, Z., & Kuzman, M. R. (2018). Differences in facial emotional recognition between patients with the first-episode psychosis, multi-episode schizophrenia, and healthy controls. Journal of the International Neuropsychological Society, 00, 1–9. Kurylo et al., 2018 D.D. Kurylo R. Waxman S.M. Silverstein B. Weinstein J. Kader I. Michalopoulos Remediation of perceptual organisation in schizophrenia Cognitive Neuropsychiatry 23 5 2018 267 283 10.1080/13546805.2018.1493986 Kurylo, D. D., Waxman, R., Silverstein, S. M., Weinstein, B., Kader, J., & Michalopoulos, I. (2018). Remediation of perceptual organisation in schizophrenia. Cognitive Neuropsychiatry, 23(5), 267–283. Laere, Tee, & Tang, 2018 E. Laere S.F. Tee P.Y. Tang Assessment of cognition in schizophrenia using Trail Making Test: A meta-analysis Psychiatry Investigation 15 10 2018 945 955 10.30773/pi.2018.07.22 Laere, E., Tee, S. F., & Tang, P. Y. (2018). Assessment of cognition in schizophrenia using trail making test: A meta-analysis. Psychiatry Investigation, 15(10), 945–955. Lakens, 2021 D. Lakens Sample size justification PsyArXiv 2021 1 31 10.31234/osf.io/9d3yf Lakens, D. (2021). Sample size justification. PsyArXiv, 1–31. Lee et al., 2011 J. Lee F. Gosselin J.K. Wynn M.F. Green How do schizophrenia patients use visual information to decode facial emotion? Schizophrenia Bulletin 37 5 2011 1001 1008 10.1093/schbul/sbq006 Lee, J., Gosselin, F., Wynn, J. K., & Green, M. F. (2011). How do schizophrenia patients use visual information to decode facial emotion? Schizophrenia Bulletin, 37(5), 1001–1008. Lee et al., 2015 J.S. Lee G. Park M.J. Song K.H. Choi S.H. Lee Early visual processing for low spatial frequency fearful face is correlated with cortical volume in patients with schizophrenia Neuropsychiatric Disease and Treatment 12 2015 1 14 10.2147/NDT.S97089 Lee, J. S., Park, G., Song, M. J., Choi, K. H., & Lee, S. H. (2015). Early visual processing for low spatial frequency fearful face is correlated with cortical volume in patients with schizophrenia. Neuropsychiatric Disease and Treatment, 12, 1–14. Lee et al., 2016 S.A. Lee C.Y. Kim S.H. Lee Non-conscious perception of emotions in psychiatric disorders: The unsolved puzzle of psychopathology Psychiatry Investigation 13 2 2016 165 173 10.4306/pi.2016.13.2.165 Lee, S. A., Kim, C. Y., & Lee, S. H. (2016). Non-conscious perception of emotions in psychiatric disorders: The unsolved puzzle of psychopathology. Psychiatry Investigation, 13(2), 165–173. Lee et al., 2010 S.J. Lee H.K. Lee Y.S. Kweon C.T. Lee K.U. Lee Deficits in facial emotion recognition in schizophrenia: A replication study with Korean subjects Psychiatry Investigation 7 2010 291 297 10.4306/pi.2010.7.4.291 Lee, S. J., Lee, H. K., Kweon, Y. S., Lee, C. T., & Lee, K. U. (2010). Deficits in facial emotion recognition in schizophrenia: A replication study with Korean subjects. Psychiatry Investigation, 7, 291–297. Lefebvre et al., 2021 S. Lefebvre E. Very R. Jardri M. Horn A. Yrondi C. Delmaire D. Pins The neural correlates of the visual consciousness in schizophrenia: An fMRI study European Archives of Psychiatry and Clinical Neuroscience 271 4 2021 661 675 10.1007/s00406-020-01167-2 Lefebvre, S., Very, E., Jardri, R., Horn, M., Yrondi, A., Delmaire, C., Rascle, C., Dujardin, K., Thomas, P., & Pins, D. (2020). The neural correlates of the visual consciousness in schizophrenia: An fMRI study. European Archives of Psychiatry and Clinical Neuroscience, 0123456789. Lenth, 2020 Lenth, R. (2020). Estimated marginal means, aka Least-squares means. R package version 1.5.0. Li et al., 2010 H. Li R.C.K. Chan G.M. McAlonan Q. Gong Facial emotion processing in schizophrenia: A meta-analysis of functional neuroimaging data Schizophrenia Bulletin 36 5 2010 1029 1039 10.1093/schbul/sbn190 Li, H., Chan, R. C. K., McAlonan, G. M., & Gong, Q.-Y. (2010). Facial emotion processing in schizophrenia: A meta-analysis of functional neuroimaging data. Schizophrenia Bulletin, 36(5), 1029–1039. Linden et al., 2010 S.C. Linden M.C. Jackson L. Subramanian C. Wolf P. Green D. Healy D.E.J. Linden Emotion-cognition interactions in schizophrenia: Implicit and explicit effects of facial expression Neuropsychologia 48 4 2010 997 1002 10.1016/j.neuropsychologia.2009.11.023 Linden, S. C., Jackson, M. C., Subramanian, L., Wolf, C., Green, P., Healy, D., & Linden, D. E. J. (2010). Emotion-cognition interactions in schizophrenia: Implicit and explicit effects of facial expression. Neuropsychologia, 48(4), 997–1002. Liu, Zhang, Zhao, Tan, & Luo, 2016 Y. Liu D. Zhang Y. Zhao S. Tan Y. Luo Deficits in attentional processing of fearful facial expressions in schizophrenic patients Scientific Reports 6 2016 32594 10.1038/srep32594 Liu, Y., Zhang, D., Zhao, Y., Tan, S., & Luo, Y. (2016). Deficits in attentional processing of fearful facial expressions in schizophrenic patients. Scientific Reports, 6(February), 32594. Lo and Andrews, 2015 S. Lo S. Andrews To transform or not to transform: Using generalized linear mixed models to analyse reaction time data Frontiers in Psychology 6 2015 1171 10.3389/fpsyg.2015.01171 Lo, S., & Andrews, S. (2015). To transform or not to transform: using generalized linear mixed models to analyse reaction time data. Frontiers in Psychology, 6, 1171. Lüdecke, 2021 Lüdecke, D. (2021). sjPlot: Data visualization for statistics in social sciences. Lundqvist et al., 1998 Lundqvist, D., Flykt, A., & Öhman, A. (1998). The Karolinska directed emotional faces (KDEF). Department of Neurosciences Karolinska Hospital. Lüdecke, Makowski, Waggoner, Patil, & Ben-Shachar, 2020 D. Lüdecke D. Makowski P. Waggoner I. Patil M.S. Ben-Shachar performance: Assessment of regression models performance CRAN 2020 10.5281/zenodo.3952174 Lüdecke, D., Makowski, D., Waggoner, P., Patil, I., & Ben-Shachar, M. S. (2020). Assessment of Regression Models Performance. CRAN. Marosi et al., 2019 C. Marosi Z. Fodor G. Csukly From basic perception deficits to facial affect recognition impairments in schizophrenia Scientific Reports 9 2019 8958 10.1038/s41598-019-45231-x Marosi, C., Fodor, Z., & Csukly, G. (2019). From basic perception deficits to facial affect recognition impairments in schizophrenia. Scientific Reports, 9, 8958. Mashour et al., 2020 G.A. Mashour P. Roelfsema J.P. Changeux S. Dehaene Conscious processing and the Global Neuronal Workspace hypothesis Neuron 105 5 2020 776 798 10.1016/j.neuron.2020.01.026 Mashour, G. A., Roelfsema, P., Changeux, J. P., & Dehaene, S. (2020). Conscious processing and the Global Neuronal Workspace hypothesis. Neuron, 105(5), 776–798. Mathersul et al., 2009 D. Mathersul D.M. Palmer R.C. Gur R.E. Gur N. Cooper E. Gordon L.M. Williams Explicit identification and implicit recognition of facial emotions: II. Core domains and relationships with general cognition Journal of Clinical and Experimental Neuropsychology 31 3 2009 278 291 10.1080/13803390802043619 Mathersul, D., Palmer, D. M., Gur, R. C., Gur, R. E., Cooper, N., Gordon, E., & Williams, L. M. (2009). Explicit identification and implicit recognition of facial emotions: II. Core domains and relationships with general cognition. Journal of Clinical and Experimental Neuropsychology, 31(3), 278–291. Mathis et al., 2012 K.I. Mathis J.K. Wynn C. Jahshan G. Hellemann A. Darque M.F. Green An electrophysiological investigation of attentional blink in schizophrenia: Separating perceptual and attentional processes International Journal of Psychophysiology 86 1 2012 108 113 10.1016/j.ijpsycho.2012.06.052 Mathis, K. I., Wynn, J. K., Jahshan, C., Hellemann, G., Darque, A., & Green, M. F. (2012). An electrophysiological investigation of attentional blink in schizophrenia: Separating perceptual and attentional processes. International Journal of Psychophysiology, 86, 108–113. Matuschek et al., 2017 H. Matuschek R. Kliegl S. Vasishth H. Baayen D. Bates Balancing Type I error and power in linear mixed models Journal of Memory and Language 94 2017 305 315 10.1016/j.jml.2017.01.001 Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing Type I error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. McCleery et al., 2020 A. McCleery J.K. Wynn J. Lee E.A. Reavis J. Ventura K.L. Subotnik K.H. Nuechterlein Early visual processing is associated with social cognitive performance in recent-onset schizophrenia Frontiers in Psychiatry 11 2020 823 10.3389/fpsyt.2020.00823 McCleery, A., Wynn, J. K., Lee, J., Reavis, E. A., Ventura, J., Subotnik, K. L., Green, M. F., & Nuechterlein, K. H. (2020). Early visual processing is associated with social cognitive performance in recent-onset schizophrenia. Frontiers in Psychiatry, 11, 823. Melnikoff and Bargh, 2018 D.E. Melnikoff J.A. Bargh The mythical number two Trends in Cognitive Sciences 22 4 2018 280 293 10.1016/j.tics.2018.02.001 Melnikoff, D. E., & Bargh, J. A. (2018). The mythical number two. Trends in Cognitive Sciences, 22(4), 280–293. Menzel, Redies, & Hayn-Leichsenring, 2018 C. Menzel C. Redies G.U. Hayn-Leichsenring Low-level image properties in facial expressions Acta Psychologica 188 2018 74 83 10.1016/j.actpsy.2018.05.012 Menzel, C., Redies, C., & Hayn-Leichsenring, G. U. (2018). Low-level image properties in facial expressions. Acta Psychologica, 188(May), 74–83. Meyer and Lieberman, 2012 M.L. Meyer M.D. Lieberman Social working memory: Neurocognitive networks and directions for future research Frontiers in Psychology 3 2012 571 10.3389/fpsyg.2012.00571 Meyer, M. L., & Lieberman, M. D. (2012). Social working memory: Neurocognitive networks and directions for future research. Frontiers in Psychology, 3, 571. Miles, 1930 W.R. Miles Ocular dominance in human adults The Journal of General Psychology 3 3 1930 412 430 10.1080/00221309.1930.9918218 Miles, W. R. (1930). Ocular dominance in human adults. The Journal of General Psychology, 412–430. Morrens et al., 2007 M. Morrens W. Hulstijn B. Sabbe Psychomotor slowing in schizophrenia Schizophrenia Bulletin 33 4 2007 1038 1053 10.1093/schbul/sbl051 Morrens, M., Hulstijn, W., & Sabbe, B. (2007). Psychomotor slowing in schizophrenia. Schizophrenia Bulletin, 33(4), 1038–1053. Niedenthal et al., 2010 P.M. Niedenthal M. Mermillod M. Maringer U. Hess The Simulation of Smiles (SIMS) model: Embodied simulation and the meaning of facial expression Behavioral and Brain Sciences 33 6 2010 417 433 10.1017/S0140525X10000865 Niedenthal, P. M., Mermillod, M., Maringer, M., & Hess, U. (2010). The Simulation of Smiles (SIMS) model: Embodied simulation and the meaning of facial expression. Behavioral and Brain Sciences, 33(6), 417–433. Norton et al., 2009 D. Norton R. McBain D.J. Holt D. Ongur Y. Chen Association of impaired facial affect recognition with basic facial and visual processing deficits in schizophrenia Biological Psychiatry 65 12 2009 1094 1098 10.1016/j.biopsych.2009.01.026 Norton, D., McBain, R., Holt, D. J., Ongur, D., & Chen, Y. (2009). Association of impaired facial affect recognition with basic facial and visual processing deficits in schizophrenia. Biological Psychiatry, 65(12), 1094–1098. Öhman et al., 2001 A. Öhman A. Flykt F. Esteves Emotion drives attention: Detecting the snake in the grass Journal of Experimental Psychology: General 130 3 2001 466 478 10.1037//0096-3445.130.3.466 Öhman, A., Flykt, A., & Esteves, F. (2001). Emotion drives attention: Detecting the snake in the grass. Journal of Experimental Psychology: General, 130(3), 466–478. Öhman et al., 2012 A. Öhman S.C. Soares P. Juth B. Lindstörm F. Esteves Evolutionary derived modulations of attention to two common fear stimuli: Serpents and hostile humans Journal of Cognitive Psychology 24 1 2012 17 32 10.1080/20445911.2011.629603 Öhman, A., Soares, S. C., Juth, P., Lindstörm, B., & Esteves, F. (2012). Evolutionary derived modulations of attention to two common fear stimuli: Serpents and hostile humans. Journal of Cognitive Psychology, 24(1), 17–32. Park et al., 2011 S. Park J. Kim C. Kim J.H. Kim K. Lee Sustained attention in the context of emotional processing in patients with schizophrenia Psychiatry Research 187 1–2 2011 18 23 10.1016/j.psychres.2010.11.007 Park, S. H., Kim, J. J., Kim, C. H., Kim, J. H., & Lee, K. H. (2011). Sustained attention in the context of emotional processing in patients with schizophrenia. Psychiatry Research, 187, 18–23. Parsons, 2020 Parsons, S. (2020). Splithalf: Robust estimates of split half reliability [R package version 0.7.2]. Peirce, 2007 J.W. Peirce PsychoPy: Psychophysics software in Python Journal of Neuroscience Methods 162 1–2 2007 8 13 10.1016/j.jneumeth.2006.11.017 Peirce, J. W. (2007). PsychoPy: Psychophysics software in Python. Journal of Neuroscience Methods, 162(1–2), 8–13. Pinheiro and Bates, 2000 J. Pinheiro D.M. Bates Mixed effects models in S and S-PLUS 2000 Springer Pinheiro, J., & Bates, D. M. (2000). Mixed effects models in S and S-PLUS. Springer. Pinkham, 2014 A.E. Pinkham Social cognition in schizophrenia Journal of Clinical Psychiatry 75 Suppl. 2 2014 14 19 10.4088/JCP.13065su1.04 Pinkham, A. E. (2014). Social cognition in schizophrenia. Journal of Clinical Psychiatry, 75(Suppl. 2), 14–19. Pinkham et al., 2011 A.E. Pinkham C. Brensinger C. Kohler R.E. Gur R.C. Gur Actively paranoid patients with schizophrenia over attribute anger to neutral faces Schizophrenia Research 125 2–3 2011 174 178 10.1016/j.schres.2010.11.006 Pinkham, A. E., Brensinger, C., Kohler, C., Gur, R. E., & Gur, R. C. (2011). Actively paranoid patients with schizophrenia over attribute anger to neutral faces. Schizophrenia Research, 125(2–3), 174–178. Pinkham et al., 2014 A.E. Pinkham N.J. Sasson S. Kelsven C.E. Simpson K. Healey C. Kohler An intact threat superiority effect for nonsocial but not social stimuli in schizophrenia Journal of Abnormal Psychology 123 1 2014 168 1677 10.1037/a0035639 Pinkham, A. E., Sasson, N. J., Kelsven, S., Simpson, C. E., Healey, K., & Kohler, C. (2014). An intact threat superiority effect for nonsocial but not social stimuli in schizophrenia. Journal of Abnormal Psychology, 123(1), 168–1677. Pitts, Martínez, & Hillyard, 2012 M.A. Pitts A. Martínez S.A. Hillyard Visual processing of contour patterns under conditions of inattentional blindness Journal of Cognitive Neuroscience 24 2 2012 287 303 10.1162/jocn_a_00111 Pitts, M. A., Martínez, A., & Hillyard, S. A. (2012). Visual processing of contour patterns under conditions of inattentional blindness. Journal of Cognitive Neuroscience, 24(2), 287–303. Ponciano et al., 1982 E. Ponciano A. Vaz Serra J. Relvas Aferição da Escala de Auto-avaliação de Ansiedade, de Zung, numa amostra da população portuguesa: Resultados da aplicação numa amostra da população normal Psiquiatria Clínica 3 1982 191 202 Ponciano, E., Vaz Serra, A., & Relvas, J. (1982). Aferição da Escala de Auto-avaliação de Ansiedade, de Zung, numa amostra da população portuguesa: Resultados da aplicação numa amostra da população normal. Psiquiatria Clínica, 3, 191–202. Pool et al., 2016 E. Pool T. Brosch S. Delplanque D. Sander Attentional bias for positive emotional stimuli: A meta-analytic investigation Psychological Bulletin 142 1 2016 79 106 10.1037/bul0000026 Pool, E., Brosch, T., Delplanque, S., & Sander, D. (2016). Attentional bias for positive emotional stimuli: A meta-analytic investigation. Psychological Bulletin, 142(1), 79–106. Pournaghdali and Schwartz, 2020 A. Pournaghdali B.L. Schwartz Continuous flash suppression: Known and unknowns Psychonomic Bulletin & Review 27 2020 1071 1103 10.3758/s13423-020-01771-2 Pournaghdali, A, & Schwartz, B.L. (2020). Continuous flash suppression: Known and unknowns. Psychonomic Bulletin & Review, 27, 1071–1103. doi:10.3758/s13423-020-01771-2. Premkumar et al., 2008 P. Premkumar M.A. Cooke D. Fannon E. Peters T.M. Michel I. Aasen V. Kumari Misattribution bias of threat-related facial expressions is related to a longer duration of ilness and poor executive function in schizophrenia and schizoaffective disorder European Psychiatry 23 1 2008 14 19 10.1016/j.eurpsy.2007.10.004 Premkumar, P., Cooke, M. A., Fannon, D., Peters, E., Michel, T. M., Aasen, I., Murray, R. M., Kuipers, E., & Kumari, V. (2008). Misattribution bias of threat-related facial expressions is related to a longer duration of ilness and poor executive function in schizophrenia and schizoaffective disorder. European Psychiatry, 23, 14–19. R Core Team, 2020 R Core Team (2020). R: A language and enviroment for statistical computing. R Foundation for Statistical Computing. Ramanoel et al., 2014 S. Ramanoel L. Kauffmann N. Guyader A. Chauvin C. Pichat M. Dojat C. Peyrin Effect of RMS contrast normalization on the retinotopic processing of spatial frequencies during scene categorization Journal of Vision 14 2014 1086 10.1167/14.10.1086 Ramanoel, S., Kauffmann, L., Guyader, N., Chauvin, A., Pichat, C., Dojat, M., & Peyrin, C. (2014). Effect of RMS contrast normalization on the retinotopic processing of spatial frequencies during scene categorization. Journal of Vision, 14(10), 1086–1086. Rauch et al., 2010 A.V. Rauch M. Reker P. Ohrmann A. Pedersen J. Bauer U. Dannlowski T. Suslow Increased amygdala activation during automatic processing of facial emotion in schizophrenia Psychiatry Research: Neuroimaging 182 3 2010 200 206 10.1016/j.pscychresns.2010.03.005 Rauch, A. V., Reker, M., Ohrmann, P., Pedersen, A., Bauer, J., Dannlowski, U., Harding, L., Koelkebeck, K., Konrad, C., Kugel, H., Arolt, V., Heindel, W., & Suslow, T. (2010). Increased amygdala activation during automatic processing of facial emotion in schizophrenia. Psychiatry Research: Neuroimaging, 182, 200–206. Romero-Ferreiro et al., 2016 M.V. Romero-Ferreiro L. Aguado J. Rodriguez-Torresano T. Palomo R. Rodriguez-Jimenez J.L. Pedreira-Massa Facial affect recognition in early and late-stage schizophrenia patients Schizophrenia Research 172 1-3 2016 177 183 10.1016/j.schres.2016.02.010 Romero-Ferreiro, M. V., Aguado, L., Rodriguez-Torresano, J., Palomo, T., Rodriguez-Jimenez, R., & Pedreira-Massa, J. L. (2016). Facial affect recognition in early and late-stage schizophrenia patients. Schizophrenia Research, 172, 177–183. Rychlowska et al., 2017 M. Rychlowska R.E. Jack O.G.B. Garrod P.G. Schyns J.D. Martin P.M. Niedenthal Functional smiles: Tools for love, sympathy, and war Psychological Science 28 9 2017 1259 1270 10.1177/0956797617706082 Rychlowska, M., Jack, R. E., Garrod, O. G. B., Schyns, P. G., Martin, J. D., & Niedenthal, P. M. (2017). Functional smiles: Tools for love, sympathy, and war. Psychological Science, 28(9), 1259–1270. Sánchez-Cubillo et al., 2009 I. Sánchez-Cubillo J.A. Periáñez D. Adrover-Roig J.M. Rodríguez-Sánchez M. Ríos-Lago J. Tirapu F. Barceló Construct validity of the Trail Making Test: role of task-switching, working memory, inhibition/interference control, and visuomotor abilities Journal of the International Neuropsychological Society 15 3 2009 438 450 10.1017/S1355617709090626 Sánchez-Cubillo, I., Periáñez, J.A., Adrover-Roig, D., Rodríguez-Sánchez, J.M., Ríos-Lago, M., Tirapu, J., & Barceló, F. (2009). Construct validity of the Trail Making Test: role of task-switching, working memory, inhibition/interference control, and visuomotor abilities. Journal of the International Neuropsychological Society, 15(3), 438–450. doi:10.1017/S1355617709090626. Savla et al., 2013 G.N. Savla L. Vella C.C. Armstrong D.L. Penn E.W. Twamley Deficits in domains of social cognition in schizophrenia: A meta-analysis of the empirical evidence Schizophrenia Bulletin 39 5 2013 979 992 10.1093/schbul/sbs080 Savla, G. N., Vella, L., Armstrong, C. C., Penn, D. L., & Twamley, E. W. (2013). Deficits in domains of social cognition in schizophrenia: A meta-analysis of the empirical evidence. Schizophrenia Bulletin, 39(5), 979–992. Schoeman et al., 2009 R. Schoeman D.J.H. Niehaus L. Koen J.M. Leppänen Emotion recognition deficits as a neurocognitive marker of schizophrenia liability M.S. Ritsner The handbook of neuropsychiatric biomarkers, endophenotypes and genes 2009 Springer 163 176 Schoeman, R., Niehaus, D. J. H., Koen, L., & Leppänen, J. M. (2009). Emotion recognition deficits as a neurocognitive marker of schizophrenia liability. In M. S. Ritsner (Ed.), The Handbook of Neuropsychiatric Biomarkers, Endophenotypes and Genes (pp. 163–176). Springer. Seedorff, Oleson, & McMurray, 2019 M. Seedorff J. Oleson B. McMurray Maybe maximal: Good enough mixed models optimize power while controlling Type I error PsyArXiv 2019 10.31234/osf.io/xmhfr Seedorff, M., Oleson, J., & McMurray, B. (2019). Maybe maximal: Good enough mixed models optimize power while controlling Type I error. PsyArXiv. Seymour et al., 2016 K. Seymour G. Rhodes T. Stein R. Langdon Intact unconscious processing of eye contact in schizophrenia Schizophrenia Research: Cognition 3 2016 15 19 10.1016/j.scog.2015.11.001 Seymour, K., Rhodes, G., Stein, T., & Langdon, R. (2016). Intact unconscious processing of eye contact in schizophrenia. Schizophrenia Research: Cognition, 3, 15–19. Shasteen et al., 2016 J.R. Shasteen A.E. Pinkham S. Kelsven K. Ludwig K. Payne D.L. Penn Intact implicit processing of facial threat cues in schizophrenia Schizophrenia Research 170 1 2016 150 155 10.1016/j.schres.2015.11.029 Shasteen, J. R., Pinkham, A. E., Kelsven, S., Ludwig, K., Payne, K., & Penn, D. L. (2016). Intact implicit processing of facial threat cues in schizophrenia. Schizophrenia Research, 170(1), 150–155. Silverstein et al., 2020 S. Silverstein A.R. Seitz A.O. Ahmed J.L. Thompson V. Zemon M. Gara P.D. Butler Development and evaluation of a visual remediation intervention for people with schizophrenia Journal of Psychiatry and Brain Science 5 2020 e200017 10.20900/jpbs.20200017 Silverstein, S., Seitz, A. R., Ahmed, A. O., Thompson, J. L., Zemon, V., Gara, M., & Butler, P. D. (2020). Development and evaluation of a visual remediation intervention for people with schizophrenia. Journal of Psychiatry and Brain Science, 5, 1–35. Smith & Schyns, 2009 F.W. Smith P.G. Schyns Smile trough four fear and sadness: Transmitting and identifying facial expression signals over a range of viewing distances Psychological Science 20 10 2009 1202 1208 10.1111/j.1467-9280.2009.02427.x Smith, F. W., & Schyns, P. G. (2009). Smile trough four fear and sadness: Transmitting and identifying facial expression signals over a range of viewing distances. Psychological Science, 20(10), 1202–1208. Soares, Lindström, Esteves, Öhman, & Nishijo, 2014 S.C. Soares B. Lindström F. Esteves A. Öhman H. Nishijo The hidden snake in the grass: Superior detection of snakes in challenging attentional conditions PLoS One 9 12 2014 e114724 10.1371/journal.pone.0114724 Soares, S. C., Lindström, B., Esteves, F., & Öhman, A. (2014). The hidden snake in the grass: Superior detection of snakes in challenging attentional conditions. PLoS ONE, 9(12), e114724. Stein, 2019 T. Stein The breaking continuous flash suppression paradigm: Review, evaluation, and outlook G. Hesselmann M. Rothkirch M. Overgaard Transitions between consciousness and unconsciousness 2019 Routledge/Taylor & Francis Group 1 38 T.SteinThe breaking continuous flash suppression paradigm: Review, evaluation, and outlookG.HesselmannM.RothkirchM.OvergaardTransitions between consciousness and unconsciousness2019Routledge/Taylor & Francis Group138 Stein, Hebart, & Sterzer, 2011 T. Stein M.N. Hebart P. Sterzer Breaking Continuous Flash Suppression: A new measure of unconscious processing during interocular suppression? Frontiers in Human Neuroscience 5 2011 167 10.3389/fnhum.2011.00167 Stein, T., Hebart, M. N., & Sterzer, P. (2011). Breaking Continuous Flash Suppression: A new measure of unconscious processing during interocular suppression? Frontiers in Human NNuroscience, 5, 167. Stein and Peelen, 2021 T. Stein M.V. Peelen Dissociating conscious and unconscious influences on visual detection effects Nature Human Behaviour 5 5 2021 612 624 10.1038/s41562-020-01004-5 Stein, T., & Peelen, M. V. (2021). Dissociating conscious and unconscious influences on visual detection effects. Nature Human Behaviour. Stein et al., 2014 T. Stein K. Seymour M.N. Hebart P. Sterzer Rapid fear detection relies on high spatial frequencies Psychological Science 25 2 2014 566 574 10.1177/0956797613512509 Stein, T., Seymour, K., Hebart, M. N., & Sterzer, P. (2014). Rapid fear detection relies on high spatial frequencies. Psychological Science, 25(2), 566–574. Stein & Sterzer, 2014 T. Stein P. Sterzer Unconscious processing under interocular suppression: Getting the right measure Frontiers in Psychology 5 2014 387 10.3389/fpsyg.2014.00387 Stein, T., & Sterzer, P. (2014). Unconscious processing under interocular suppression: Getting the right measure. Frontiers in Psychology, 5(MAY), 1–5. Suslow et al., 2013 T. Suslow C. Lindner U. Dannlowski K. Walhöfer M. Rödiger B. Maisch H. Kugel Automatic amygdala response to facial expression in schizophrenia: Initial hyperresponsivity followed by hyporesponsivity BMC Neuroscience 14 1 2013 10.1186/1471-2202-14-140 Suslow, T., Lindner, C., Dannlowski, U., Walhöfer, K., Rödiger, M., Maisch, B., Bauer, J., Ohrmann, P., Lencer, R., Zwitserlood, P., Kersting, A., Heindel, W., Arolt, V., & Kugel, H. (2013). Automatic amygdala response to facial expression in schizophrenia: Initial hyperresponsivity followed by hyporesponsivity. BMC Neuroscience, 14, 140. Tamminga et al., 2014 C.A. Tamminga G. Pearlson M. Keshavan J. Sweeney B. Clementz G. Thaker Bipolar and schizophrenia network for intermediate phenotypes: Outcomes across the psychosis continuum Schizophrenia Bulletin 40 Suppl. 2 2014 S131 S137 10.1093/schbul/sbt179 Tamminga, C. A., Pearlson, G., Keshavan, M., Sweeney, J., Clementz, B., & Thaker, G. (2014). Bipolar and schizophrenia network for intermediate phenotypes: Outcomes across the psychosis continuum. Schizophrenia Bulletin, 40(Suppl. 2), S131-137. Taylor et al., 2012 S.F. Taylor J. Kang I.S. Brege I.F. Tso A. Hosanagar T.D. Johnson Meta-analysis of functional neuroimaging studies of emotion perception and experience in schizophrenia Biological Psychiatry 71 2 2012 136 145 10.1016/j.biopsych.2011.09.007 Taylor, S. F., Kang, J., Brege, I. S., Tso, I. F., Hosanagar, A., & Johnson, T. D. (2012). Meta-analysis of functional neuroimaging studies of emotion perception and experience in schizophrenia. Biological Psychiatry, 71(2), 136–145. The International Schizophrenia Consortium, 2009 The International Schizophrenia Consortium Common polygenic variation contributes to risk of schizophrenia and bipolar disorder Nature 460 7256 2009 748 752 10.1038/nature08185 The International Schizophrenia Consortium. (2009). Common polygenic variation contributes to risk of schizophrenia and bipolar disorder. Nature, 460, 748–752. The Jamovi Project, 2020 The Jamovi Project. (2020). jamovi. (Version 1.2) [Computer Software]. Theeuwes et al., 2000 J. Theeuwes P. Atchley A.F. Kramer On the time course of top-down and bottom-up control of visual attention Attention and Performance 18 2000 104 124 Theeuwes, J., Atchley, P., & Kramer, A. F. (2000). On the time course of top-down and bottom-up control of visual attention. Attention and Performance, 18, 104–124. Tsuchiya and Koch, 2005 N. Tsuchiya C. Koch Continuous flash suppression reduces negative afterimages Nature Neuroscience 8 8 2005 1096 1101 10.1038/nn1500 Tsuchiya, N., & Koch, C. (2005). Continuous flash suppression reduces negative afterimages. Nature Neuroscience, 8(8), 1096–1101. Tsuchiya, Moradi, Felsen, Yamazaki, & Adolphs, 2009 N. Tsuchiya F. Moradi C. Felsen M. Yamazaki R. Adolphs Intact rapid detection of fearful faces in the absence of the amygdala Nature Neuroscience 12 10 2009 1224 1225 10.1038/nn.2380 Tsuchiya, N., Moradi, F., Felsen, C., Yamazaki, M., & Adolphs, R. (2009). Intact rapid detection of fearful faces in the absence of the amygdala. Nature Neuroscience, 12(10), 1224–1225. Underwood et al., 2016 R. Underwood V. Kumari E. Peters Cognitive and neural models of threat appraisal in psychosis: A theoretical integration Psychiatry Research 239 2016 131 138 10.1016/j.psychres.2016.03.016 Underwood, R., Kumari, V., & Peters, E. (2016). Cognitive and neural models of threat appraisal in psychosis: A theoretical integration. Psychiatry Research, 239, 131–138. van 't Wout et al., 2007 M. van 't Wout A. Aleman R.P.C. Kessels W. Cahn E.H.F. de Haan R.S. Kahn Exploring the nature of facial affect processing deficits in schizophrenia Psychiatry Research 150 3 2007 227 235 10.1016/j.psychres.2006.03.010 van’t Wout, M., Aleman, A., Kessels, R. P. C., Cahn, W., de Haan, E. H. F., & Kahn, R. S. (2007). Exploring the nature of facial affect processing deficits in schizophrenia. Psychiatry Research, 150(3), 227–235. van't Wout et al., 2007 M. van't Wout A. van Dijke A. Aleman R.P.C. Kessels W. Pijpers R.S. Kahn Fearful faces in schizophrenia: The relationship between patients characteristics and facial affect recognition The Journal of Nervous and Mental Disease 195 9 2007 758 764 10.1097/NMD.0b013e318142cc31 van’t Wout, M., van Dijke, A., Aleman, A., Kessels, R. P. C., Pijpers, W., & Kahn, R. S. (2007). Fearful faces in schizophrenia: The relationship between patients characteristics and facial affect recognition. The Journal of Nervous and Mental Disease, 195(9), 758–764. Vetter, Badde, Phelps, & Carrasco, 2019 P. Vetter S. Badde E.A. Phelps M. Carrasco Emotional faces guide the eyes in the absence of awareness ELife 8 2019 e43467 10.7554/eLife.43467 Vetter, P., Badde, S., Phelps, E. A., & Carrasco, M. (2019). Emotional faces guide the eyes in the absence of awareness. ELife, 8, 1–15. Wang, Tong, Shang, & Chen, 2019 H. Wang S. Tong J. Shang W. Chen The role of gender in the preconscious processing of facial trustworthiness and dominance Frontiers in Psychology 10 2019 2565 10.3389/fpsyg.2019.02565 Wang, H., Tong, S., Shang, J., & Chen, W. (2019). The role of gender in the preconscious processing of facial trustworthiness and dominance. Frontiers in Psychology, 10(November). Webb & Hibbard, 2020 A.L.M. Webb P.B. Hibbard Suppression durations for facial expressions under breaking continuous flash suppression: Effects of faces’ low-level image properties Scientific Reports 10 1 2020 17427 10.1038/s41598-020-74369-2 Webb, A. L. M., & Hibbard, P. B. (2020). Suppression durations for facial expressions under breaking continuous flash suppression: effects of faces’ low-level image properties. Scientific Reports, 10(1), 1–11. Webb et al., 2020 A.L.M. Webb P.B. Hibbard R. O’Gorman Contrast normalisation masks natural expression-related differences and artifically enhances the perceived salience of fear expressions PLoS One 15 6 2020 e0234513 10.1371/journal.pone.0234513 Webb, A. L. M., Hibbard, P. B., & O’Gorman, R. (2020). Contrast normalisation masks natural expression-related differences and artifically enhances the perceived salience of fear expressions. PloS One, 15(6), e0234513. Westfall, 2016 Westfall, J. (2016). Power analysis for general anova designs. [Shiny App]. Willenbockel et al., 2010 V. Willenbockel J. Sadr D. Fiset G.O. Horne F. Gosselin J.W. Tanaka Controlling low-level image properties: The SHINE toolbox Behavior Research Methods 42 3 2010 671 684 10.3758/BRM.42.3.671 Willenbockel, V., Sadr, J., Horne, G. O., Gosselin, F., & Tanaka, J. (2010). Controlling low-level image properties: the SHINE toolbox. Behavior Research Methods, 42(3), 671–684. Wirth and Wentura, 2020 B.E. Wirth D. Wentura It occurs after all: Attentional bias towards happy faces in the dot-probe task Attention, Perception, & Psychophysics 82 5 2020 2463 2481 10.3758/s13414-020-02017-y Wirth, B. E., & Wentura, D. (2020). It occurs after all: Attentional bias towards happy faces in the dot-probe task. Attention, Perception, & Psychophysics, 82, 2463–2481. Won et al., 2019 S. Won W.L. Lee S. Kim J.J. Kim B.J. Lee J. Yu Y. Chung Distinct differences in emotional recognition according to severity of psychotic symptoms in early-stage schizophrenia Frontiers in Psychiatry 10 2019 564 10.3389/fpsyt.2019.00564 Won, S., Lee, W. K., Kim, S. W., Kim, J. J., Lee, B. J., Yu, J. C., Lee, K. Y., Lee, S. H., Kim, S. H., Kang, S. H., Kim, E., & Chung, Y. C. (2019). Distinct differences in emotional recognition according to severity of psychotic symptoms in early-stage schizophrenia. Frontiers in Psychiatry, 10, 564. Yang et al., 2014 E. Yang J. Brascamp M.S. Kang R. Blake On the use of continuous flash suppression for the study of visual processing outside of awareness Frontiers in Psychology 5 2014 724 10.3389/fpsyg.2014.00724 Yang, E., Brascamp, J., Kang, M. S., & Blake, R. (2014). On the use of continuous flash suppression for the study of visual processing outside of awareness. Frontiers in Psychology, 5, 724. Yang et al., 2007 E. Yang D.H. Zald R. Blake Fearful expressions gain preferential access to awareness during continuous flash suppression Emotion 7 4 2007 882 886 10.1037/1528-3542.7.4.882 Yang, E., Zald, D. H., & Blake, R. (2007). Fearful expressions gain preferential access to awareness during continuous flash suppression. Emotion, 7(4), 882–886. Yang, Yeh, & Keil, 2018 Y. Yang S. Yeh A. Keil Can emotional content be extracted under interocular suppression? PLoS One 13 11 2018 e0206799 10.1371/journal.pone.0206799 Yang, Y. H., & Yeh, S. L. (2018). Can emotional content be extracted under interocular suppression? PLoS ONE, 13(11), e0206799. Zhou et al., 2020 S. Zhou Y. Xu N. Wang S. Zhang H. Geng H. Jia Deficits of subliminal self-face processing in schizophrenia Consciousness and Cognition 79 2020 102896 10.1016/j.concog.2020.102896 Zhou, S., Xu, Y., Wang, N., Zhang, S., Geng, H., & Jia, H. (2020). Deficits of subliminal self-face processing in schizophrenia. Consciousness and Cognition, 79, 102896. "
    },
    {
        "doc_title": "Visually exploring a Collaborative Augmented Reality Taxonomy",
        "doc_scopus_id": "85118430803",
        "doc_doi": "10.1109/IV53921.2021.00024",
        "doc_eid": "2-s2.0-85118430803",
        "doc_date": "2021-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Co-located collaboration",
            "Collaboration taxonomy",
            "Collaborative augmented realities",
            "Collaborative Work",
            "Common ground",
            "Harmonisation",
            "Remote collaboration",
            "Systematisation",
            "User study",
            "Visualization tools"
        ],
        "doc_abstract": "© 2021 IEEE.Augmented Reality (AR) has been explored with the objective to assist in scenarios of co-located or remote collaboration. To help understand how well collaborative work can be addressed with AR, it is important to foster harmonization of perspectives and create a common ground for systematization and discussion. In this vein, understand relationships among existing dimensions of collaboration, as well as identify research opportunities, is of paramount importance and thus tools that allow visually exploring information associated with Collaborative AR may be most valuable. In this paper, we present a first effort towards the creation of such an interactive visualization tool for exploration and analysis of collaborative AR research. It allows visualize data of selected papers organized according to a human-centered taxonomy on collaborative AR. In order to get insights into whether the structure was understood and if the representation was clear and efficient to use, we evaluated the proposed tool through a user study with 40 participants. Results suggest the tool has potential towards the creation of a shared understanding and identification of existing patterns, trends and opportunities within the field of collaborative AR.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Remote asynchronous collaboration in maintenance scenarios using augmented reality and annotations",
        "doc_scopus_id": "85105968777",
        "doc_doi": "10.1109/VRW52623.2021.00166",
        "doc_eid": "2-s2.0-85105968777",
        "doc_date": "2021-03-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Asynchronous collaboration",
            "Collaborative approach",
            "Maintenance scenario",
            "Maintenance tasks",
            "Remote experts",
            "Shared understanding",
            "Spatial informations",
            "User study"
        ],
        "doc_abstract": "© 2021 IEEE.This paper presents an Augmented Reality (AR) remote collaborative approach making use of different stabilized annotation features, part of an ongoing research with partners from the industry. It enables a remote expert to assist an on-site technician during asynchronous maintenance tasks. To foster the creation of a shared understanding, the on-site technician uses mobile AR, allowing the identification of issues, while the remote expert uses a computer to share annotations and provide spatial information about objects, events and areas of interest. The results of a pilot user study to evaluate asynchronous collaborative aspects while using the approach are also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Toolkit to Evaluate and Characterize the Collaborative Process in Scenarios of Remote Collaboration Supported by AR",
        "doc_scopus_id": "85126382153",
        "doc_doi": "10.1109/ISMAR-Adjunct54149.2021.00074",
        "doc_eid": "2-s2.0-85126382153",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Case-studies",
            "Characterization collaborative process",
            "Collaborative process",
            "Data collection",
            "Data filtering",
            "Distributed evaluation",
            "Evaluation toolkit",
            "Remote collaboration",
            "Remote maintenance",
            "Visual editors"
        ],
        "doc_abstract": "© 2021 IEEE.Remote collaboration using Augmented Reality (AR) has enormous potential to support collaborators that need to achieve a common goal. However, there is a lack of tools for evaluating these multifaceted contexts, involving many aspects that may influence the way collaboration occurs. Therefore, it is essential to develop solutions to monitor AR-supported collaboration in a more structured manner, allowing adequate portrayal and report of such efforts. As a contribute, we describe CAPTURE, a toolkit to instrument AR-based tools via visual editors, enabling rapid data collection and filtering during distributed evaluations. We illustrate the use of the toolkit through a case study on remote maintenance and report the results obtained, which can elicit a more complete characterization of the collaborative process moving forward.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Rassper: Radar-based silent speech recognition",
        "doc_scopus_id": "85119301325",
        "doc_doi": "10.21437/Interspeech.2021-1413",
        "doc_eid": "2-s2.0-85119301325",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Ambient noise",
            "European portuguese",
            "Health condition",
            "Lighting conditions",
            "Privacy concerns",
            "Public places",
            "Silent speech",
            "Silent speech interfaces",
            "Silent speech recognition",
            "Ultrasound probes"
        ],
        "doc_abstract": "Copyright © 2021 ISCA.Speech is our most natural and efficient way of communication and offers a strong potential to improve how we interact with machines. However, speech communication can sometimes be limited by environmental (e.g., ambient noise), contextual (e.g., need for privacy in a public place), or health conditions (e.g., laryngectomy), hindering the consideration of audible speech. In this regard, silent speech interfaces (SSI) have been proposed (e.g., considering video, electromyography), however, many technologies still face limitations regarding their everyday use, e.g., the need to place equipment in contact with the speaker (e.g., electrodes/ultrasound probe), and raise technical (e.g., lighting conditions for video) or privacy concerns. In this context, the consideration of technologies that can help tackle these issues, e.g, by being contactless and/or placed in the environment, can foster the widespread use of SSI. In this article, continuous-wave radar is explored to assess its potential for SSI. To this end, a corpus of 13 words was acquired, for 3 speakers, and different classifiers were tested on the resulting data. The best results, obtained using Bagging classifier, trained for each speaker, with 5-fold cross-validation, yielded an average accuracy of 0.826, an encouraging result that establishes promising grounds for further exploration of this technology for silent speech recognition.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Designing and Deploying an Interaction Modality for Articulatory-Based Audiovisual Speech Synthesis",
        "doc_scopus_id": "85116375726",
        "doc_doi": "10.1007/978-3-030-87802-3_4",
        "doc_eid": "2-s2.0-85116375726",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Articulatory",
            "Audio-visual speech",
            "Audiovisual speech synthesis",
            "Face contacts",
            "Face to face",
            "Interaction modality",
            "Multi-modal",
            "Multimodal Interaction",
            "Remote communication",
            "Speech interaction"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.Humans communicate with each other in a multimodal way. Even with several technologies mediating remote communication, face-to-face contact is still our main and most natural way to exchange information. Despite continuous advances in interaction modalities, such as speech interaction, much can be done to improve its naturalness and efficiency, particularly by considering the visual cues transmitted by facial expressions through audiovisual speech synthesis (AVS). To this effect, several approaches have been proposed, in the literature, mostly based in data-driven methods. These, while presenting very good results, rely on models that work as black boxes without a direct relation with the actual process of producing speech and, hence, do not contribute much to our understanding of the underpinnings of the synergies between the audio and visual outputs. In this context, the authors proposed a first proof of concept for an articulatory-based approach to AVS, supported on the articulatory phonology framework, and argued that this research needs to be challenged and informed by fast methods to translate it to interactive applications. In this article, we describe further evolutions of the pronunciation module of the AVS core system along with the proposal of a set of interaction modalities to enable its integration in applications to enable a faster translation into real scenarios. The proposed modalities are designed in line with the W3C recommendations for multimodal interaction architectures making it easy to integrate with any applications that consider it.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Assessing Velar Gestures Timing in European Portuguese Nasal Vowels with RT-MRI Data",
        "doc_scopus_id": "85116328665",
        "doc_doi": "10.1007/978-3-030-87802-3_3",
        "doc_eid": "2-s2.0-85116328665",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Articulatory data",
            "Articulatory synthesis",
            "Dynamic nature",
            "European portuguese",
            "Gestural timing",
            "Nasal vowels",
            "Real- time",
            "Real-time magnetic resonance",
            "Resonance imaging data",
            "Vowel production"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.European Portuguese (EP) nasal vowels are characterised by their dynamic nature entailing a gradual variation from an oral into a nasal configuration. The analysis of velar dynamics assumes a particular relevance for improving our understanding of nasal vowel production with an impact, e.g., on articulatory synthesis. Following on previous work, considering EMA and real-time magnetic resonance imaging (RT-MRI), at 14 fps, this study revisits the work regarding the characterisation of EP nasal vowels by analysing gesture timings considering articulatory data obtained from RT-MRI of the vocal tract at a higher frame rate (50 fps) and a larger number of speakers. The analysis, considering eleven EP speakers, characterises the duration of opening and closing velar gestures and explores synchronisation with the previous oral gesture (start-to-release lag) and the potential influence of vowel height in this regard.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Conceptual Model and Taxonomy for Collaborative Augmented Reality",
        "doc_scopus_id": "85112654469",
        "doc_doi": "10.1109/TVCG.2021.3101545",
        "doc_eid": "2-s2.0-85112654469",
        "doc_date": "2021-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Collaboration process",
            "Collaborative augmented realities",
            "Collaborative Work",
            "Conceptual frameworks",
            "Real-world objects",
            "Research efforts",
            "Seamless integration",
            "Shared understanding"
        ],
        "doc_abstract": "IEEETo support the nuances of collaborative work, many researchers have been exploring the field of Augmented Reality (AR),aiming to assist in co-located or remote scenarios. Solutions using AR allow taking advantage from seamless integration of virtualobjects and real-world objects, thus providing collaborators with a shared understanding or common ground environment. However,most of the research efforts, so far, have been devoted to experiment with technology and mature methods to support its design anddevelopment. Therefore, it is now time to understand were do we stand and how well can we address collaborative work with AR, tobetter characterize and evaluate the collaboration process. In this paper, we perform an analysis of the different dimensions that shouldbe taken into account when analysing the contributions of AR to the collaborative work effort. Then, we bring these dimensions forwardinto a conceptual framework and propose an extended human-centered taxonomy for the categorization of the main features of Collaborative AR. Our goal is to foster harmonization of perspectives for the field, which may help create a common ground forsystematization and discussion. We hope to influence and improve how research in this field is reported by providing a structured list ofthe defining characteristics. Finally, some examples of the use of the taxonomy are presented to show how it can serve to gatherinformation for characterizing AR-supported collaborative work, and illustrate its potential as the grounds to elicit further",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Multidisciplinary User-Centered Approach to Designing an Information Platform for Accessible Tourism: Understanding User Needs and Motivations",
        "doc_scopus_id": "85112160466",
        "doc_doi": "10.1007/978-3-030-78092-0_9",
        "doc_eid": "2-s2.0-85112160466",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Development process",
            "Focus groups",
            "Information platform",
            "People with disabilities",
            "Potential users",
            "Technological solution",
            "Tourism activities",
            "User-centered approach"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.The present work aims to expand knowledge on user needs and the motivations of people with disabilities (PwD) in order to create a technological solution for addressing barriers in the accessible tourism market. For this, a user-centered design was followed, putting PwD at the center of the development process. This was obtained by identifying personas and scenarios contributing to a better depiction of the potential users, clearly describing their requirements and accessibility needs. For obtaining the characterization of personas, a comprehensive study in the area of accessible tourism, involving various tourism stakeholders was performed. The methods applied to collect the data were questionnaires, interviews and two focus groups with PwD. Two personas are presented in this article, illustrating the needs and motivations of two groups of PwD. In addition, two scenarios concerning the personas were also elaborated, showing how a technological solution can help the integration in tourism activities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Promoting Social Skills in Technology-Mediated Communication Contexts: First Results on Adopting the Social Compass Curriculum",
        "doc_scopus_id": "85112134284",
        "doc_doi": "10.1007/978-3-030-80091-8_55",
        "doc_eid": "2-s2.0-85112134284",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.Communication is a vital part of our daily lives. It allows us to express ourselves, our needs and feelings, enabling the establishment of relationships with others. Unfortunately, the ability to communicate can often be partially or, even, fully hindered due to a wide range of conditions. Since communication acts as a crucial step for integration, difficulties associated with communication can act as obstacles to those that experience them, making them struggle to feel included in society. Taking in mind the importance of being able to communicate and the multiple facets of technology, but also other dimensions such as social interaction, and building on previous work regarding assistive communication technologies for school kids, this work addresses the promotion of social skills in technology-mediated contexts. To this end, a tool designed to motivate and promote face to face interaction as well as the employment of the user’s social skills during them, is proposed as an instrument to support and potentially teach and improve the child’s social skills. The proposed approach has already been submitted to first evaluations by end-users and professionals in the field of speech therapy with very promising results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Conceptual Model for Data Collection and Analysis for AR-based Remote Collaboration Evaluation",
        "doc_scopus_id": "85099568572",
        "doc_doi": "10.1109/ISMAR-Adjunct51615.2020.00016",
        "doc_eid": "2-s2.0-85099568572",
        "doc_date": "2020-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Collaborative process",
            "Conceptual model",
            "Data collection",
            "Enabling technologies",
            "Holistic evaluations",
            "Novel methods",
            "Remote collaboration",
            "Team performance"
        ],
        "doc_abstract": "© 2020 IEEE.A significant effort has been devoted to the creation of the enabling technology and in the proposal of novel methods to support remote collaboration using Augmented Reality (AR), given the novelty of the field. As the field progresses to focus on the nuances of supporting collaboration and with the growing number of prototypes mediated by AR, the characterization and evaluation of the collaborative process becomes an essential, but difficult endeavor. Evaluation is particularly challenging in this multifaceted context involving many aspects that may influence the way collaboration occurs. Therefore, it is essential the existence of holistic evaluation strategies that monitor the use and performance of the proposed solutions regarding the team, its members, and the technology, allowing adequate characterization and report of collaborative processes. As a contribute, we propose a conceptual model for multi-user data collection and analysis that monitors several collaboration aspects: individual and team performance, behaviour and level of collaboration, as well as contextual data in scenarios of remote collaboration using AR-based solutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Data-driven critical tract variable determination for European Portuguese",
        "doc_scopus_id": "85094103354",
        "doc_doi": "10.3390/info11100491",
        "doc_eid": "2-s2.0-85094103354",
        "doc_date": "2020-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Critical variables",
            "Data driven",
            "Dynamic aspects",
            "Imaging data",
            "Tract variables",
            "Unsupervised data",
            "Visual analysis",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Technologies, such as real-time magnetic resonance (RT-MRI), can provide valuable information to evolve our understanding of the static and dynamic aspects of speech by contributing to the determination of which articulators are essential (critical) in producing specific sounds and how (gestures). While a visual analysis and comparison of imaging data or vocal tract profiles can already provide relevant findings, the sheer amount of available data demands and can strongly profit from unsupervised data-driven approaches. Recent work, in this regard, has asserted the possibility of determining critical articulators from RT-MRI data by considering a representation of vocal tract configurations based on landmarks placed on the tongue, lips, and velum, yielding meaningful results for European Portuguese (EP). Advancing this previous work to obtain a characterization of EP sounds grounded on Articulatory Phonology, important to explore critical gestures and advance, for example, articulatory speech synthesis, entails the consideration of a novel set of tract variables. To this end, this article explores critical variable determination considering a vocal tract representation aligned with Articulatory Phonology and the Task Dynamics framework. The overall results, obtained considering data for three EP speakers, show the applicability of this approach and are consistent with existing descriptions of EP sounds.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Serious games for stroke telerehabilitation of upper limb-a review for future research",
        "doc_scopus_id": "85097394134",
        "doc_doi": "10.5195/ijt.2020.6326",
        "doc_eid": "2-s2.0-85097394134",
        "doc_date": "2020-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Rehabilitation",
                "area_abbreviation": "MEDI",
                "area_code": "2742"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020, University Library System, University of Pittsburgh. All rights reserved.Maintaining appropriate home rehabilitation programs after stroke, with proper adherence and remote monitoring is a challenging task. Virtual reality (VR)-based serious games could be a strategy used in telerehabilitation (TR) to engage patients in an enjoyable and therapeutic approach. The aim of this review was to analyze the background and quality of clinical research on this matter to guide future research. The review was based on research material obtained from PubMed and Cochrane up to April 2020 using the PRISMA approach. The use of VR serious games has shown evidence of efficacy on upper limb TR after stroke, but the evidence strength is still low due to a limited number of randomized controlled trials (RCT), a small number of participants involved, and heterogeneous samples. Although this is a promising strategy to complement conventional rehabilitation, further investigation is needed to strengthen the evidence of effectiveness and support the dissemination of the developed solutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Understanding public speakers’ performance: First contributions to support a computational approach",
        "doc_scopus_id": "85087533947",
        "doc_doi": "10.1007/978-3-030-50347-5_30",
        "doc_eid": "2-s2.0-85087533947",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Audio-visual data",
            "Computational approach",
            "Professional life",
            "Video database"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Communication is part of our everyday life and our ability to communicate can have a significant role in a variety of contexts in our personal, academic, and professional lives. For long, the characterization of what is a good communicator has been subject to research and debate by several areas, particularly in Education, with a focus on improving the performance of teachers. In this context, the literature suggests that the ability to communicate is not only defined by the verbal component, but also by a plethora of non-verbal contributions providing redundant or complementary information, and, sometimes, being the message itself. However, even though we can recognize a good or bad communicator, objectively, little is known about what aspects – and to what extent—define the quality of a presentation. The goal of this work is to create the grounds to support the study of the defining characteristics of a good communicator in a more systematic and objective form. To this end, we conceptualize and provide a first prototype for a computational approach to characterize the different elements that are involved in communication, from audiovisual data, illustrating the outcomes and applicability of the proposed methods on a video database of public speakers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Contributions to a quantitative unsupervised processing and analysis of tongue in ultrasound images",
        "doc_scopus_id": "85087274917",
        "doc_doi": "10.1007/978-3-030-50516-5_15",
        "doc_eid": "2-s2.0-85087274917",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic analysis",
            "Computational approach",
            "Electromagnetic articulography",
            "Speech motor control",
            "Speech production",
            "Temporal coherence",
            "Tongue segmentation",
            "Ultrasound imaging"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Speech production studies and the knowledge they bring forward are of paramount importance to advance a wide range of areas including Phonetics, speech therapy, synthesis and interaction. Several technologies have been considered to study static and dynamic features of the articulators and speech motor control, such as electromagnetic articulography (EMA), real-time magnetic resonance (RTMRI) and ultrasound (US) imaging. While the latest advances in RTMRI provide a wealth of data of the full vocal tract, it is an expensive resource that requires specialized facilities. In this sense, US is a more affordable alternative for several contexts, enabling the acquisition of larger datasets, but demands adequate computational approaches for processing and analysis. While the literature is prolific in proposing methods for tongue segmentation from US, the noisy nature of the images and the specificities of the equipment often dictate a poor performance on novel datasets, a matter that needs to be assessed, before large data acquisition, to devise suitable acquisition and processing methods. In the scope of a line of research studying speech changes with age, this work describes the first results of an automatic tongue segmentation method from US, along with a characterization of the main challenges posed by the image data. Even though improvements are still needed, particularly to ensure temporal coherence, at its current stage, this method can already provide the required data for an automatic analysis of maximum tongue height, a relevant parameter to assess speech changes on vowel production.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enabling Multimodal Emotionally-Aware Ecosystems Through a W3C-Aligned Generic Interaction Modality",
        "doc_scopus_id": "85086143464",
        "doc_doi": "10.1007/978-3-030-49289-2_11",
        "doc_eid": "2-s2.0-85086143464",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Affective Computing",
            "Daily lives",
            "Emotional state",
            "Fast tracks",
            "Interactive system",
            "Key resources",
            "Life experiences",
            "Multi-modal"
        ],
        "doc_abstract": "© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2020.Emotions play a key role in our life experiences. In interactive systems, the user’s emotional state can be relevant to provide increased levels of adaptation to the user, but can also be paramount in scenarios where such information might enable us to help users manage and express their emotions (e.g., anxiety), with a positive impact on their daily life and on how they interact with others. However, although there is a clear potential for emotionally-aware applications, they still have a long road to travel to reach the desired potential and availability. This is mostly due to the still low translational nature of the research in affective computing, and to the lack of straightforward, off-the-shelf methods for easy integration of emotion in applications without the need for developers to master the different concepts and technologies involved. In light of these challenges, we advance our previous work and propose an extended conceptual vision for supporting emotionally-aware interactive ecosystems and a fast track to ensure the desired translational nature of the research in affective computing. This vision then leads to the proposal of an improved iteration of a generic affective modality, a key resource to the accomplishment of the proposed vision, enabling off-the-shelf support for emotionally-aware applications in multimodal interactive contexts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Stop Anxiety: Tackling Anxiety in the Academic Campus Through an mHealth Multidisciplinary User-Centred Approach",
        "doc_scopus_id": "85086141395",
        "doc_doi": "10.1007/978-3-030-49289-2_9",
        "doc_eid": "2-s2.0-85086141395",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Health facilities",
            "Iterative approach",
            "Management techniques",
            "Mobile Health (M-Health)",
            "Mobile Technology",
            "Multi-disciplinary teams",
            "Proof of concept",
            "Quality of life"
        ],
        "doc_abstract": "© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2020.Anxiety-related disorders have a strong impact on our quality of life. With their epidemiological prevalence, across the population, highly exceeding the capacity for treatment in health facilities, new ways of delivering therapies are needed. The wide availability of mobile technologies, e.g., smartphones, has provided an accessible and ubiquitous platform for delivering psychological therapies and many mobile health (mHealth) systems have been proposed to support users in managing their levels of anxiety. However, many of the available tools provide features without evidence-based support of their adequateness and effectiveness. Furthermore, several tools are designed without specifically considering the users’ needs and motivations, resulting in poor adherence or a lack of motivation for systematic use, hindering any positive effects. Considering the need to more closely focus on the motivations of the target users, this article describes how the efforts of a multidisciplinary team are contributing to support a user-centred approach to design and develop a tool to support anxiety management in the context of an Academic Campus. As a first materialization of this ongoing work, a proof-of-concept application is proposed (StopAnxiety), developed by adopting an iterative approach, and already providing a set of clinician-approved anxiety management techniques.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards automatic determination of critical gestures for European Portuguese sounds",
        "doc_scopus_id": "85081601133",
        "doc_doi": "10.1007/978-3-030-41505-1_1",
        "doc_eid": "2-s2.0-85081601133",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic determination",
            "Conservative approaches",
            "Critical articulator",
            "Data-driven approach",
            "Data-driven methods",
            "Real time",
            "Speech production",
            "Vocal-tract data"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Technologies, such as electromagnetic midsagittal articulography (EMA) and real-time magnetic resonance (RT-MRI), can contribute to improve our understanding of the static and dynamic aspects of speech, namely by providing information regarding which articulators are essential (critical) in producing specific sounds and how (gestures). Previous work has successfully demonstrated the possibility to determine critical articulators considering vocal tract data obtained from RT-MRI. However, these works have adopted a conservative approach by considering vocal tract representations analogous to the flash points obtained with EMA data, i.e., landmarks fixed over the articulators, e.g., tongue. To move towards a data-driven method able to determine gestural scores, e.g., driving articulatory speech synthesis, one important step is to move into a representation aligned with Articulatory Phonology and Task Dynamics. This article advances towards this goal by exploring critical articulators determination considering a vocal tract representation aligned with this framework is adopted and presents first results considering 50 Hz RTMRI data for two speakers of European Portuguese.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "\"Threat-unrelated\" properties: An ill-defined concept. A reply to \"the danger of interpreting detection differences between image categories\" (Gayet, Stein, &amp; Peelen, 2019)",
        "doc_scopus_id": "85069855070",
        "doc_doi": "10.1037/emo0000632",
        "doc_eid": "2-s2.0-85069855070",
        "doc_date": "2019-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Psychology (all)",
                "area_abbreviation": "PSYC",
                "area_code": "3200"
            }
        ],
        "doc_keywords": [
            "Animals",
            "Birds",
            "Fear",
            "Female",
            "Humans",
            "Male",
            "Snakes"
        ],
        "doc_abstract": "© 2018 American Psychological Association.In our previous work (Gomes, Soares, Silva, & Silva, 2018), we evidenced that snakes (vs. birds), an archetypal threat-related stimulus, have an advantage in accessing visual awareness during continuous flash suppression. This advantage was observed when the images were filtered in low spatial frequency (LSF) but not in high spatial frequency and interpreted as supporting the role of a subcortical pathway (superior colliculus-pulvinar) to the amygdala in threat detection, thought to be sensitive to LSF but not to high spatial frequency information. Recently Gayet, Stein, and Peelen (2019), using stimuli without differences in the threat they represented (bicycles and cars) but with an analogous perimeter-Area ratio as snakes and birds (respectively), evidenced that bicycles had a faster access to visual awareness than cars. As in our study, this advantage relied on LSF information. The authors argued that the images' perimeter-Area ratio (a characteristic intrinsically related with stimuli shape referred to by the authors as threat-unrelated properties), but not the threat itself, may have driven the detection advantage observed in our study and the association with the action of the subcortical pathway to the amygdala. We propose that the results obtained by Gayet et al. do not undermine ours and discuss the relevance of higher perimeter-Area ratios (such as images with elongated shapes; e.g., snakes) as a threat-related property required for a fast-initial snake processing. We argue that stimuli shape is a prime feature for the fast-initial threat processing, relevant to grab early visual attention, an effect that might explain the obtained results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards european portuguese conversational assistants for smart homes",
        "doc_scopus_id": "85071088633",
        "doc_doi": "10.4230/OASIcs.SLATE.2019.5",
        "doc_eid": "2-s2.0-85071088633",
        "doc_date": "2019-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Conversational Assistants",
            "Device integration",
            "Green homes",
            "Home devices",
            "Low costs",
            "Smart devices",
            "Smart environment",
            "Smart homes"
        ],
        "doc_abstract": "© Maksym Ketsmur, António Teixeira, Nuno Almeida, and Samuel Silva.Nowadays, smart environments, such as Smart Homes, are becoming a reality, due to the access to a wide variety of smart devices at a low cost. These devices are connected to the home network and inhabitants can interact with them using smartphones, tablets and smart assistants, a feature with rising popularity. The diversity of devices, the user’s expectations regarding Smart Homes, and assistants’ requirements pose several challenges. In this context, a Smart Home Assistant capable of conversation and device integration can be a valuable help to the inhabitants, not only for smart device control, but also to obtain valuable information and have a broader picture of how the house and its devices behave. This paper presents the current stage of development of one such assistant, targeting European Portuguese, not only supporting the control of home devices, but also providing a potentially more natural way to access a variety of information regarding the home and its devices. The development has been made in the scope of Smart Green Homes (SGH) project.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Contribute for an ontology for smart homes and their conversational assistants",
        "doc_scopus_id": "85070062478",
        "doc_doi": "10.23919/CISTI.2019.8760934",
        "doc_eid": "2-s2.0-85070062478",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Conversational assistants",
            "Green homes",
            "Knowledge base",
            "Semantic information",
            "Smart homes",
            "User need"
        ],
        "doc_abstract": "© 2019 AISTI.Despite the improved capabilities of recent Assistants, control and access to the information regarding Smart Homes stills limited. More information is needed on what should be a Smart Home Assistant and how-to have the structured semantic information to support their answers and actions, i.e., how to structure the knowledge and make it simple to use by current approaches to Assistants, based commonly in intentions and entities. As contribute to increase Assistants capabilities in Smart Homes environments, based on analyses of the domain and user enquiries, we propose the basis for an ontology to support both, the interaction and the Smart Home knowledge base. The proposed ontology is being used to support the creation of an enhanced version of a Conversational Assistant for the Smart Green Homes project with novel functionalities, aligned with advanced user needs and expectations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The am4i architecture and framework formultimodal interaction and its application to smart environments",
        "doc_scopus_id": "85067542443",
        "doc_doi": "10.3390/s19112587",
        "doc_eid": "2-s2.0-85067542443",
        "doc_date": "2019-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Adaptation",
            "Devices",
            "Multi-devices",
            "Multi-Modal Interactions",
            "Smart environment"
        ],
        "doc_abstract": "© 2019 by the authors. Licensee MDPI, Basel, Switzerland.Technologies, such as smart sensors, actuators, and other kinds of devices, are often installed in our environments (e.g., our Homes) and available to integrate our daily lives. Despite their installation being motivated by the pursuit of automation and increased efficiency, making these environments usable, acceptable and enjoyable in a sustainable, energy efficient way is not only a matter of automation. Tackling these goals is a complex task demanding the combination of different perspectives including building and urban Architecture, Ubiquitous Computing and Human-Computer Interaction (HCI) to provide occupants with the means to shape these environments to their needs. Interaction is of paramount relevance in the creation of adequate relations of users with their environments, but it cannot be seen independently from the ubiquitous sensing and computing or the environment’s architecture. In this regard, there are several challenges to HCI, particularly in how to integrate this multidisciplinary effort. Although there are several solutions to address some of these challenges, the complexity and dynamic nature of the smart environments and the diversity of technologies involved still present many challenges,� particularly for its development. In general, the development is complex, and it is hard to create a dynamic environment providing versatile and adaptive forms of interaction. To participate in the multidisciplinary effort, the development of interaction must be supported by tools capable of facilitating co-design by multidisciplinary teams. In this article, we address the development of interaction for complex smart environments and propose the AM4I architecture and framework,� a novel modular approach to design and develop adaptive multiplatform multilingual multi-device multimodal interactive systems. The potential of the framework is demonstrated by proof-of-concept applications in two different smart environment contexts, non-residential buildings and smart homes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and Development for Individuals with ASD: Fostering Multidisciplinary Approaches Through Personas",
        "doc_scopus_id": "85064929398",
        "doc_doi": "10.1007/s10803-019-03898-1",
        "doc_eid": "2-s2.0-85064929398",
        "doc_date": "2019-05-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Developmental and Educational Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3204"
            }
        ],
        "doc_keywords": [
            "Autism Spectrum Disorder",
            "Humans",
            "Personality",
            "Psychotherapy",
            "User-Computer Interface"
        ],
        "doc_abstract": "© 2019, Springer Science+Business Media, LLC, part of Springer Nature.Developing technologies to support individuals with ASD is a growing field of research facing numerous challenges. First, while the individual with ASD is central, the motivations of others, such as parents, are often taken as the motivations of the individual. Second, the desirable cross-disciplinary pollination for improved intervention can often face difficulties due to a lack of a common language among disciplines. Thirdly, the literature often lacks enough information to allow a clear understanding of the targeted contexts and goals not enabling an assessment of outcomes and building on past advances. To tackle these challenges, we propose that families of Personas and scenarios are used throughout the design and development process, and as dissemination resources, and provide illustrative examples.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Heuristic evaluation in visualization: An empirical study : ition paper",
        "doc_scopus_id": "85062881054",
        "doc_doi": "10.1109/BELIV.2018.8634108",
        "doc_eid": "2-s2.0-85062881054",
        "doc_date": "2019-02-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Empirical studies",
            "Heuristic evaluation",
            "heuristics sets",
            "Potential problems",
            "Usability inspection",
            "Usability problems",
            "Usability tests",
            "Visualization application"
        ],
        "doc_abstract": "© 2018 IEEE.Heuristic evaluation is a usability inspection method that has been adapted to evaluate visualization applications through the development of specific sets of heuristics. This paper presents an empirical study meant to assess the capacity of the method to anticipate the usability issues noticed by users when using a visualization application. The potential usability problems identified by 20 evaluators were compared with the issues found for the same application by 46 users through a usability test, as well as with the fixes recommended by the experimenters observing those users during the test. Results suggest that using some heuristics may have elicited potential problems that none of the users noticed while using the application; on the other hand, users encountered unpredicted usability issues.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring critical articulator identification from 50Hz RT-MRI data of the vocal tract",
        "doc_scopus_id": "85074701958",
        "doc_doi": "10.21437/Interspeech.2019-2897",
        "doc_eid": "2-s2.0-85074701958",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Copyright © 2019 ISCAThe study of the static and dynamic aspects of speech production can profit from technologies such as electromagnetic midsagittal articulography (EMA) and real-time magnetic resonance (RTMRI). These can improve our knowledge on which articulators and gestures are involved in producing specific sounds and foster improved speech production models, paramount to advance, e.g., articulatory speech synthesis. Previous work, by the authors, has shown that critical articulator identification could be performed from RTMRI data of the vocal tract, with encouraging results, by extending the applicability of an unsupervised statistical identification method previously proposed for EMA data. Nevertheless, the slower time resolution of the considered RT-MRI corpus (14 Hz), when compared to EMA, potentially influencing the ability to select the most suitable representative configuration for each phone - paramount for strongly dynamic phones, e.g., nasal vowels -, and the lack of a richer set of contexts - relevant for observing coarticulation effects -, were identified as limitations. This article addresses these limitations by exploring critical articulator identification from a faster RTMRI corpus (50 Hz), for European Portuguese, providing a richer set of contexts, and testing how fusing the articulatory data of two speakers might influence critical articulator determination.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the role of oral configurations in European Portuguese nasal vowels",
        "doc_scopus_id": "85074699107",
        "doc_doi": "10.21437/Interspeech.2019-2232",
        "doc_eid": "2-s2.0-85074699107",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Copyright © 2019 ISCAThe characterisation of nasal vowels is not only a question of studying velar aperture. Recent work shows that oropharyngeal articulatory adjustments enhance the acoustics of nasal coupling or, at least, magnify differences between oral/nasal vowel congeners. Despite preliminary studies on the oral configurations of nasal vowels, for European Portuguese, a quantitative analysis is missing, particularly one to be applied systematically to a desirably large number of speakers. The main objective of this study is to adapt and extend previous methodological advances for the analysis of MRI data to further investigate: how velar changes affect oral configurations; the changes to the articulators and constrictions when compared with oral counterparts; and the closest oral counterpart. High framerate RT-MRI images (50fps) are automatically processed to extract the vocal tract contours and the position/configuration for the different articulators. These data are processed by evolving a quantitative articulatory analysis framework, previously proposed by the authors, extended to include information regarding constrictions (degree and place) and nasal port. For this study, while the analysis of data for more speakers is ongoing, we considered a set of two EP native speakers and addressed the study of oral and nasal vowels mainly in the context of stop consonants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mind the snake: Fear detection relies on low spatial frequencies.",
        "doc_scopus_id": "85038622081",
        "doc_doi": "10.1037/emo0000391",
        "doc_eid": "2-s2.0-85038622081",
        "doc_date": "2018-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Psychology (all)",
                "area_abbreviation": "PSYC",
                "area_code": "3200"
            }
        ],
        "doc_keywords": [
            "Adolescent",
            "Adult",
            "Amygdala",
            "Animals",
            "Awareness",
            "Fear",
            "Female",
            "Humans",
            "Male",
            "Models, Neurological",
            "Snakes",
            "Superior Colliculi",
            "Visual Cortex",
            "Young Adult"
        ],
        "doc_abstract": "© 2017 American Psychological Association.The privileged processing of threat stimuli, even in the absence of visual awareness, has been associated with a subcortical superior colliculus (SC) - pulvinar pathway to the amygdala, bypassing the visual cortex. However, this has been heavily disputed by studies showing that cortical activity cannot be ruled out in fear processing. A recent study using continuous flash suppression (CFS) showed that rapid detection of fear faces relies on high spatial frequencies, which involve cortical visual areas. In the present study, we also used CFS and manipulated spatial frequency in order to assess if more primitive fear stimuli - snakes - follow a similar trend. Our results show an advantage of snakes in accessing awareness based on low spatial frequency information, arguing in favor of the role of a SC-pulvinar pathway to the amygdala.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Unconscious influence over executive control: Absence of conflict detection and adaptation",
        "doc_scopus_id": "85049441258",
        "doc_doi": "10.1016/j.concog.2018.06.021",
        "doc_eid": "2-s2.0-85049441258",
        "doc_date": "2018-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Experimental and Cognitive Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3205"
            },
            {
                "area_name": "Developmental and Educational Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3204"
            }
        ],
        "doc_keywords": [
            "Adaptation, Psychological",
            "Adolescent",
            "Attention",
            "Conflict (Psychology)",
            "Executive Function",
            "Female",
            "Flicker Fusion",
            "Humans",
            "Male",
            "Photic Stimulation",
            "Reaction Time",
            "Unconscious (Psychology)",
            "Young Adult"
        ],
        "doc_abstract": "© 2018 Elsevier Inc.Executive control and its modulation of attentional mechanisms allow us to detect and adapt to conflicting information. According to recent studies, executive control functions may be modulated by unconsciously perceived information, although the available evidence is not consistent. In this study, we used a Flanker Task and employed Chromatic Flicker Fusion, a suppression technique that has been proposed as more adequate to elicit executive control functions, to assess conflict and conflict adaptation effects. Our results showed that, when suppressed, flankers did not evoke conflict related effects on performance. However, in trials where most flankers were incongruent, longer response times in congruent trials were observed, consistent with orienting responses. Our results help to support earlier theories regarding the inherent limitations of unconsciously perceived information, though future studies should further investigate why and under which conditions is the executive control system modulated by unconscious information.",
        "available": true,
        "clean_text": "serial JL 272616 291210 291726 291738 291782 31 Consciousness and Cognition CONSCIOUSNESSCOGNITION 2018-07-07 2018-07-07 2018-07-07 2018-07-07 2020-01-22T07:05:01 S1053-8100(18)30239-3 S1053810018302393 10.1016/j.concog.2018.06.021 S300 S300.4 FULL-TEXT 2020-01-22T07:31:58.738198Z 0 0 20180801 20180831 2018 2018-07-07T04:51:58.232656Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref 1053-8100 10538100 true 63 63 C Volume 63 9 110 122 110 122 201808 August 2018 2018-08-01 2018-08-31 2018 Regular Articles article fla © 2018 Elsevier Inc. All rights reserved. UNCONSCIOUSINFLUENCEOVEREXECUTIVECONTROLABSENCECONFLICTDETECTIONADAPTATION SILVA F 1 Introduction 1.1 Executive control of attention 1.2 The role of awareness in executive control processing 1.3 The present study 2 Method 2.1 Participants 2.2 Stimuli and apparatus 2.3 Procedure 3 Results 3.1 Response time 3.2 Accuracy 4 General discussion 4.1 Limitations and future directions 5 Conclusion References ALMEIDA 2008 15214 15218 J ANSORGE 2011 351 365 U ANSORGE 2014 268 287 U ATAS 2016 866 881 A BAHRAMI 2010 224 233 B BONNEH 2001 798 Y BOTVINICK 2001 624 M BOY 2010 11134 11139 F BRAVER 2012 106 113 T BREITMEYER 2015 234 250 B BREITMEYER 2006 B VISUALMASKINGTIMESLICESTHROUGHCONSCIOUSUNCONSCIOUSVISION BRUCHMANN 2010 1800 1809 M DENBUSSCHE 2010 86 97 E CALLEJAS 2005 27 37 A CALLEJAS 2004 225 227 A CARTER 2007 367 379 C CHO 2006 19878 19883 R CLAYSON 2011 1953 1961 P COHEN 2000 421 423 J CORBETTA 2002 215 229 M CUSHMAN 1983 1557 1561 W CUSTERS 2010 47 50 R DEPISAPIA 2012 639 649 N DEHAENE 2001 1 37 S DESENDER 2012 K DESENDER 2013 K DIEDE 2016 1255 1266 N DIEDE 2017 824 835 N DOSTILIO 2012 332 340 K DUNCAN 1980 272 J EGNER 2005 1784 1790 T EIMER 2003 7 26 M ERIKSEN 1974 143 149 B FAN 2014 J FAN 2009 209 220 J FAN 2005 471 479 J FOGELSON 2014 601 S GAYET 2014 489 507 S GHINESCU 2010 197 205 R GOMES 2017 227 234 N GRATTON 1992 480 G GRICE 1985 495 501 G HASEGAWA 2014 1347 K HOSHIYAMA 2006 57 61 M HOUTMAN 2012 278 285 F HUBERHUBER 2018 169 175 C JIANG 2013 J JIANG 2015 102 111 J JIANG 2007 657 662 Y KAHNEMAN 1973 D ATTENTIONEFFORT KANG 2011 13535 13545 M KAPADIA 1995 843 856 M KERBY 2014 D KIEFER 2010 464 489 M KOECHLIN 2003 1181 1185 E KOIVISTO 2018 M KOPP 1996 282 294 B KURATOMI 2013 315 328 K LAMME 2003 12 18 V LARSON 2009 663 670 M LAVIE 1995 451 468 N LEE 1988 323 347 B LI 2008 95 107 W LOGAN 2001 393 434 G LOGAN 1979 166 174 G LOGOTHETIS 1996 621 N LU 2012 928 938 S MACK 1998 A INATTENTIONALBLINDNESS MACKIE 2013 301 312 M MANSOURI 2009 141 152 F MARIEN 2012 399 415 H MARZOUKI 2007 34 45 Y MCCORMICK 1997 168 P MELCHER 2005 723 729 D MEYER 1971 227 234 D MILLER 2000 59 E MOUTOUSSIS 2002 9527 9532 K MOZER 2005 M MUDRIK 2011 764 770 L MULCKHUYSE 2010 299 309 M MURPHY 1993 723 S NIEUWENHUIS 2006 1260 1272 S NOTEBAERT 2009 275 279 W OHMAN 2005 953 958 A PANADERO 2015 35 45 A PASHLER 1994 220 H PEREMEN 2014 Z POSNER 1980 3 25 M POSNER 1994 7398 7403 M POSNER 2012 M POSNER 2008 31 61 M TOPICSININTEGRATIVENEUROSCIENCE ATTENTIONORGANSYSTEM POSNER 2004 M ATTENTIONCOGNITIVECONTROL RAIZADA 2001 431 466 R RAYMOND 1992 849 J RIC 2012 222 226 F RIDDERINKHOF 2004 443 447 K SAENZ 2002 631 632 M SAKURABA 2012 3949 3953 S SANDBERG 2010 1069 1078 K SANGER 2014 J SCHOUPPE 2014 N SILVA 2007 D MANUALINVENTARIODEESTADOTRACODEANSIEDADESTAI SILVERSTEIN 2015 216 227 B SIMONS 1997 261 267 D SNODGRASS 2006 43 79 M SOARES 2017 139 147 S SOKOLOV 1963 545 580 E SOKOLOV 1990 142 150 E SPIELBERGER 1970 C MANUALFORSTATETRAITANXIETYINVENTORY STROOP 1935 643 J TAYLOR 1977 404 D TOMCZAK 2014 M TSUCHIYA 2005 1096 1101 N TSUCHIYA 2006 6 N TULVING 1990 301 306 E ULLSPERGER 2005 467 472 M VANDENBUSSCHE 2009 452 477 E VANGAAL 2010 e11508 S VANOPSTAL 2011 1860 1864 F VANVEEN 2001 1302 1308 V WANG 2013 161 169 B WANG 2016 274 282 X WINKIELMAN 2005 121 135 P WU 2015 1110 Q WU 2018 2267 2282 T YEUNG 2004 931 959 N ZABELINA 2013 581 585 D ZERNICKI 1987 239 247 B ZOU 2016 8408 8413 J ZYSSET 2001 29 36 S SILVAX2018X110 SILVAX2018X110X122 SILVAX2018X110XF SILVAX2018X110X122XF 2020-07-07T00:00:00.000Z 2020-07-07T00:00:00.000Z © 2018 Elsevier Inc. All rights reserved. 2019-03-30T17:13:31.062Z S1053810018302393 FCT - Foundation for Science and Technology UID/CEC/00127/2013 Fundação para a Ciência e a Tecnologia Fundação para a Ciência e a Tecnologia CINTESIS UID/IC/4255/2013 FCT Fundação para a Ciência e a Tecnologia FEDER and CENTRO 2020 funds CENTRO-01-0145-FEDER-000010 CENTRO 2020 Programa Operacional Competitividade e Internacionalização – COMPETE2020 Programa Operacional Temático Factores de Competitividade National Funds China National Funds for Distinguished Young Scientists This article was supported by FEDER through the operation POCI-01-0145-FEDER-007746 funded by the Programa Operacional Competitividade e Internacionalização – COMPETE2020 and by National Funds through FCT - Fundação para a Ciência e a Tecnologia within CINTESIS, R&D Unit (reference UID/IC/4255/2013); and by FEDER and CENTRO 2020 funds through project SOCA – Smart Open Campus (ref. CENTRO-01-0145-FEDER-000010) and by research unit IEETA, funded by National Funds through the FCT - Foundation for Science and Technology, in the context of project UID/CEC/00127/2013. item S1053-8100(18)30239-3 S1053810018302393 10.1016/j.concog.2018.06.021 272616 2020-01-22T07:31:58.738198Z 2018-08-01 2018-08-31 true 657156 MAIN 13 45031 849 656 IMAGE-WEB-PDF 1 gr1 12901 144 690 gr2 20215 165 689 gr3 18052 319 605 gr4 21672 339 617 gr1 4566 46 219 gr2 4931 53 219 gr3 3679 116 219 gr4 4304 120 219 gr1 391653 587 2803 gr2 397310 685 2857 gr3 140451 1413 2678 gr4 156946 1501 2732 YCCOG 2648 S1053-8100(18)30239-3 10.1016/j.concog.2018.06.021 Elsevier Inc. Fig. 1 Schematic example of chromatic flicker fusion. Flanker arrows alternate between red and green isoluminant colors at frequency of 30 Hz, fusing with the background. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Progression of a single trial. Starting with a blank frame for 200 ms, there’s a linear ramping up of stimuli and background for 200 ms, followed by the presentation of the stimuli for 1200 ms. The participant is asked to give its response as to the orientation of the target (center) arrow. Fig. 3 Average response time across all conditions, blocks and congruency levels, compared to the absence condition (no flankers). Error bars represent 1SEM. Statistical differences to the absence condition are represented with “*” (*p ≤ 0.05 and **p ≤ 0.001). Fig. 4 Average accuracy across all conditions, blocks and congruency levels compared to the absence condition (no flankers). Error bars represent 1 SEM. Statistical differences to the absence condition are represented with “*” (*p ≤ 0.05 and **p ≤ 0.001). Table 1 Mean response time (ms) per condition, block and congruency levels and the corresponding standard deviation (below and between parenthesis). Condition and block Masked Absence Visible Congruency Block A Block B – Block A Block B Congruent 897.069 (52.723) 899.141 (46.585) – 907.277 (53.718) 907.925 (53.397) Incongruent 895.716 (50.158) 892.971 (43.048) – 974.268 (57.665) 955.772 (51.521) Total 896.386 (50.864) 894.134 (43.272) 890.650 (45.754) 939.247 (53.802) 945.822 (51.264) Table 2 Mean accuracy (%) per condition, block and congruency levels and the corresponding standard deviation (below and between parenthesis). Condition and block type Masked Absence Visible Congruency Block A Block B – Block A Block B Congruent 98.89 (1.58) 98.97 (2.06) – 99.17 (1.56) 99.31 (1.75) Incongruent 98.47 (2.05) 98.40 (1.56) – 94.85 (6.08) 96.85 (2.69) Total 98.68 (1.50) 98.52 (1.38) 98 (1.87) 97.02 (3.31) 97.34 (2.21) Unconscious influence over executive control: Absence of conflict detection and adaptation Fábio Silva a d Joana Dias b Samuel Silva c Pedro Bem-Haja b Carlos F. Silva b Sandra C. Soares b d e ⁎ a Portuguese Catholic University, Institute of Health Sciences, Lisbon, Portugal Portuguese Catholic University Institute of Health Sciences Lisbon Portugal b CINTESIS.UA, Department of Education and Psychology, University of Aveiro, Portugal CINTESIS.UA Department of Education and Psychology University of Aveiro Portugal c Department of Electronics, Telecommunication and Informatics (DETI)/Institute of Electronics and Informatics Engineering (IEETA), University of Aveiro, Portugal Department of Electronics, Telecommunication and Informatics (DETI)/Institute of Electronics and Informatics Engineering (IEETA) University of Aveiro Portugal d William James Center for Research, ISPA University Institute, Portugal William James Center for Research ISPA University Institute Portugal e Department of Clinical Neuroscience, Division for Psychology, Karolinska Institutet, Sweden Department of Clinical Neuroscience Division for Psychology Karolinska Institutet Sweden ⁎ Corresponding author at: University of Aveiro, Department of Education and Psychology, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal. University of Aveiro Department of Education and Psychology Campus Universitário de Santiago 3810-193 Aveiro Portugal Executive control and its modulation of attentional mechanisms allow us to detect and adapt to conflicting information. According to recent studies, executive control functions may be modulated by unconsciously perceived information, although the available evidence is not consistent. In this study, we used a Flanker Task and employed Chromatic Flicker Fusion, a suppression technique that has been proposed as more adequate to elicit executive control functions, to assess conflict and conflict adaptation effects. Our results showed that, when suppressed, flankers did not evoke conflict related effects on performance. However, in trials where most flankers were incongruent, longer response times in congruent trials were observed, consistent with orienting responses. Our results help to support earlier theories regarding the inherent limitations of unconsciously perceived information, though future studies should further investigate why and under which conditions is the executive control system modulated by unconscious information. Keywords Executive control Attention Conflict Conflict adaptation Chromatic flicker fusion Flanker task 1 Introduction 1.1 Executive control of attention We are constantly exposed to an endless plethora of incoming information that overwhelms our senses. Importantly, the number and diversity of stimuli competing for a prioritized processing is exponential in modern times. It is, therefore, crucial that our cognitive system functions to properly deal with this abundant external stimulation by constantly managing the diverse and simultaneous sources of distraction. Failures in this function (i.e., not properly ignoring distractor stimuli or failing to override habitual responses), which are linked with certain pathologies (e.g., in schizophrenia; Cho, Konecky, & Carter, 2006), can pose critical hazards in our daily life (e.g., not attending to a sudden road danger due to a ringing telephone; Logan & Gordon, 2001; Mansouri, Tanaka, & Buckley, 2009). The attention system, with its separable, yet interconnected brain networks –alerting, orienting and executive control – modulates the priority and controls the flow of information that is processed and attended (Fan et al., 2009; Mackie, Van Dam, & Fan, 2013; Posner & Fan, 2008). While the alerting network is associated with an increase in vigilance regarding possible impending stimuli, the orienting network is activated in response to stimuli that signal high behavioral significance and may then require urgent action. Finally, and most relevant to the present study, the executive control network functions to detect and resolve conflict between different sources of stimulation (Fan, Mccandliss, Fossella, Flombaum, & Posner, 2005; Mackie et al., 2013). According to recent theories (Fan, 2014; Wu et al., 2018), the general purpose of this network is to cope (reduce) information uncertainty, where conflict is viewed as a special case of increased uncertainty. Consequently, the executive control network plays a vital role in orchestrating the attention system, by either inhibiting or facilitating the prioritization of some stimuli over others (Corbetta & Shulman, 2002). Therefore, constant adjustments in perceptual selection, response biasing and the maintenance of contextual or goal-directed information reflect important functions of the executive control system, which are particularly critical in the face of difficult, novel or complex tasks (i.e., when a high degree of cognitive management is demanded) (Botvinick, Braver, Barch, Carter, & Cohen, 2001). These functions are mainly coordinated by pre-frontal areas, particularly the lateral pre-frontal cortex (lPFC) and the posterior medial frontal cortex (pMFC), operating in concert to coordinate the attentional system and implement more efficient responses to conflicting information (Koechlin, 2003; Miller, 2000; Ridderinkhof, Ullsperger, Crone, & Nieuwenhuis, 2004). Despite the enormous amount of information to which we are constantly exposed, the processing capacity of our attentional system is limited (Kahneman, 1973) and bound to be affected by the analysis of multiple information presented simultaneously. The direct consequence is cognitive conflict, which occurs when a specific feature of a stimulus is incompatible or incongruent with that of a second stimulus attribute, thus generating conflict or interference (Duncan, 1980; Zysset, Müller, Lohmann, & von Cramon, 2001). The anterior cingulate cortex (ACC), in conjunction with the adjacent pMFC, plays an important role as a monitoring system for conflict (Carter & van Veen, 2007; Cohen, Botvinick, & Carter, 2000). The ACC appears to detect and initiate neural responses following errors and decreased performance, as shown by the error-related negativity (ERN) – an event related potential in the ACC that is sensible to deviations from optimal performance (Braver, 2012). This, in turn, will be conveyed to other areas of the prefrontal cortex (e.g., lPFC) that will subsequently implement counter-conflict measures, leading to what is known as conflict adaptation (Carter & van Veen, 2007; Yeung, Botvinick, & Cohen, 2004). The effect of conflict adaptation is, then, the product of coordinated actions of the executive control system that result in a tendency to emphasize goal-directed stimuli (i.e., relevant to the task at hand) and to devalue distractors or conflicting stimuli (Botvinick et al., 2001; Egner & Hirsch, 2005; Ullsperger, Bylsma, & Botvinick, 2005). Importantly, the expectations regarding the utility of the information will determine its importance and prioritization in processing and, consequently, its behavioral impact. This expectation is learned throughout previously conflicting experiences (Gratton, Coles, & Donchin, 1992; Wang, Zhao, Xue, & Chen, 2016). In laboratory settings, conflict can be evoked by the use of different tasks, such as the Stroop task (Stroop, 1935), priming paradigms (Meyer & Schvaneveldt, 1971; Tulving & Schacter, 1990), dual-tasks (Pashler, 1994) and the Eriksen flanker task (Eriksen & Eriksen, 1974). In these tasks, whenever the target is surrounded or preceded by congruent distractor stimuli (i.e., stimuli corresponding, or matching the target, therefore being associated with the same response), participants tend to be faster (i.e., exhibit shorter response times) and more accurate in identifying the target. A reverse effect is observed when the distractors are incongruent or incompatible with the target (i.e., stimuli that differ from the target, pointing to a different response) (Boy, Husain, & Sumner, 2010; Hasegawa & Takahashi, 2014; Larson, Kaufman, & Perlstein, 2009). The overall conflict caused by the distractors is assessed by comparing the differences between congruent and incongruent trials. Regarding adaptation effects, these can be viewed in terms of immediate adaptation (trial-by-trial), where the previous trial’s congruency modulates the next one, or long-term adaptation (block-wise), where the previous group of trial’s congruency modulates the current trial (Ansorge, Kunde, & Kiefer, 2014; Gratton et al., 1992; Kuratomi & Yoshizaki, 2013; Logan & Zbrodoff, 1979). Such adaptation effects (specifically long-term, which are more relevant to the current study) are formally analyzed by comparing compatibility indexes. These indexes are calculated by subtracting the mean performance measure (i.e., response times, accuracies or error rates) of incongruent trials, with the corresponding measure for congruent trials, in a given block of trials. This final value represents the degree of attendance given to distractors in that respective block (Hasegawa & Takahashi, 2014; Nieuwenhuis et al., 2006). Although previous results are consistent in showing how our information management abilities, provided by executive control mechanisms, help us to detect and adapt to information that proves to be either disruptive or helpful to our current goals, such findings are mainly based on information that we consciously experience and perceive. Yet, the question of whether these mechanisms solely rely in conscious processing has only recently been put to test, as alluded in the following section. 1.2 The role of awareness in executive control processing Until recently, goal-directed (or top-down) processing, such as that involved in executive control, was assumed to be limited to conscious processing, with unconscious processing (i.e., without awareness) being mostly associated with low-level operations (i.e., stimulus-driven or automatic), having a limited capability in undertaking high-level processes (i.e., complex computations) (Dehaene & Naccache, 2001; Kiefer & Martens, 2010; Posner, Snyder, & Solso, 2004). Attention is a paradigmatic example of a cognitive process that is highly intertwined with conscious processing (Posner, 1994). Indeed, the absence of attention is associated with the absence of awareness (i.e., conscious perception), as reflected by several attentional phenomena, such as inattentional blindness (Mack & Rock, 1998), change blindness (Simons & Levin, 1997) and attentional blink (Raymond, Shapiro, & Arnell, 1992). Recent research has challenged the assumption that unconscious processing is restricted to automatic and less complex processing (Ansorge et al., 2014; Custers & Aarts, 2010; Silverstein, Snodgrass, Shevrin, & Kushwaha, 2015). Several studies seem to support, instead, a less restricted role of unconscious processing, from as basic as the conceptual integration of an object with its background (e.g., Mudrik, Breska, Lamy, & Deouell, 2011), to the selection and use of a strategy without intention or conscious awareness regarding the meaning of the cues (i.e., distractors) provided in the tasks (e.g., Ghinescu, Schachtman, Stadler, Fabiani, & Gratton, 2010; Ric & Muller, 2012). This has been mainly demonstrated by using the masked priming paradigm (see below), with the results concluding that processes associated with goal pursuit can be activated and altered without awareness (Marien, Custers, Hassin, & Aarts, 2012; Van den Bussche, Van den Noortgate, & Reynvoet, 2009). The results from electrophysiological and neuroimaging studies are consistent with such claims, i.e., that the access to higher cognitive processing is not exclusively restricted to conscious information (Desender, Van Lierde, & Van den Bussche, 2013), by showing that masked information increases the activity related to the executive control networks without any reported awareness by the participants (De Pisapia, Turatto, Lin, Jovicich, & Caramazza, 2012; D’Ostilio & Garraux, 2012). However, evidence regarding unconscious modulation of the executive control system remains unclear (Ansorge et al., 2014; Desender & Van den Bussche, 2012). Specifically, concerning unconscious conflict detection and the subsequent adaptation, researchers have tackled different approaches and interpretations and provided an overall positive picture regarding these effects (Diede & Bugg, 2017; Hasegawa & Takahashi, 2014; Jiang, Zhang, & van Gaal, 2015; Panadero, Castellanos, & Tudela, 2015; Van Opstal, Calderon, Gevers, & Verguts, 2011; Wang, Xiang, & Li, 2013). Nonetheless, some studies still raise doubts regarding such possibility, by showing either partial or no adaptation to conflict (e.g., Ansorge, Fuchs, Khalid, & Kunde, 2011; Atas, Desender, Gevers, & Cleeremans, 2016; Schouppe, de Ferrerre, Van Opstal, Braem, & Notebaert, 2014), or even failing to observe conflict detection (e.g., Bruchmann, Herper, Konrad, Pantev, & Huster, 2010; Wu et al., 2015). To study unconscious processes, several new methods were designed and implemented based on traditional tasks, such as inattentional blindness (Mack & Rock, 1998), motion induced blindness (Bonneh, Cooperman, & Sagi, 2001) and binocular rivalry (Logothetis, Leopold, & Sheinberg, 1996). However, for the study of executive control related processes, the most widely used has been the masked priming paradigm (Wu et al., 2015). In this technique, the stimuli (distractors) are made invisible through their very brief presentation (around 10 ms) followed by an immediate random visual pattern precluding its conscious perception (Breitmeyer, 2015; Breitmeyer & Öğmen, 2006). However, one of the main downsides of this technique is that it does not enable a more prolonged unconscious exposure, such as the continuous flash suppression (CFS; Tsuchiya & Koch, 2005) or the chromatic flicker fusion (CFF; Hoshiyama, Kakigi, Takeshima, Miki, & Watanabe, 2006) techniques. In the CFS technique, masks are flashed continuously at high speeds (∼10 Hz) to the dominant eye, and a stimulus is presented to the other eye, resulting on a suppressed access to consciousness of this stimulus (Tsuchiya & Koch, 2005; Tsuchiya, Koch, Gilroy, & Blake, 2006). The CFF technique, on the other hand, takes advantage of the principle of visual processing overload, where two opponent colors alternate at a very high frequency (>25 Hz), hence fusing the colors and rendering the image implanted there invisible (Cushman & Levinson, 1983; Hoshiyama et al., 2006). Importantly, and aside from the specificities of the technical designs and core processes explored by each technique to render the stimuli invisible, one relevant difference regards their respective neural modulation. While on the CFS technique, the suppressed stimuli appear to be mainly processed in the visual cortex, not dispersing to other neural areas (Fogelson, Kohler, Miller, Granger, & Tse, 2014), this is not the case with the CFF technique. This latter technique allows neural dissipation to a larger extent, allowing information to be decodable further up the processing stream, specifically in the frontal and temporal regions (Fogelson et al., 2014), potentially critical for executive control functions. 1.3 The present study Currently, available research is not yet consensual regarding the impact of unconscious processing in executive control functions, particularly of conflict and adaptation to conflict. A recent study by Wu et al. (2015), which consisted in measuring the impact on executive control of unconscious stimuli suppressed by CFS, showed that unconsciously processed information failed to evoke conflict. According to the authors, one of the possible explanations for this lack of conflict may be related to the deep suppression associated with CFS, not allowing information to reach frontal areas, and therefore preventing executive processing of the information (Peremen & Lamy, 2014; Wu et al., 2015). In the present study, in line with this rationale, we considered CFF, instead, due to its less restricted suppressive aspect, enabling suppressed information to reach higher cortical functions. Thus, by means of a Flanker Task, we hypothesized that under CFF the suppressed flankers would be able to reach executive control related neural areas, evoking conflict response patterns, i.e., longer RTs and lower accuracy for incongruent flankers. In addition, by presenting two blocks with different congruent to incongruent ratios, we assessed possible adaptation effects (only block-wise effects, given its more pronounced effects; Snodgrass & Shevrin, 2006). We expected such effects to be reflected in a lower compatibility index in the block with mostly incongruent trials, as compared to the block with a neutral balance of congruency. 2 Method 2.1 Participants Thirty-seven university students, from the University of Aveiro, volunteered to participate. However, eight participants were excluded due to software issues and one due to distraction or confusion regarding the instructions, which was reflected in a high number of errors and trials with no responses. The final sample consisted of 29 participants (15 males and 14 females, with ages ranging between 18 and 24, M = 21.52, SD = 1.48). All participants reported having either normal or corrected to normal eyesight. Moreover, none of the participants reported major physical or psychological problems. The study was approved by the Ethics Committee of the Institute of Health Sciences of the Portuguese Catholic University and conducted in accordance with the Declaration of Helsinki. Participants received course credits for their participation. 2.2 Stimuli and apparatus We used an Eriksen Flanker Task (Eriksen & Eriksen, 1974), with arrows as stimuli. A central gray arrow (target) was presented with one of two orientations (right or left). The target was either shown isolated or surrounded by four other arrows (flankers), two on each side, with the same or opposite orientation as the target arrow. The masking (suppression) of the flankers was achieved by using the Chromatic Flicker Fusion technique (CFF) and consisted of alternating low contrast red and green arrows (opponent colors) (Moutoussis & Zeki, 2002). The background, where the stimuli appeared, was a red and green checkerboard pattern with a black mesh, enabling easier flicker fusion (Hoshiyama et al., 2006). The red/green flankers alternated with each other in a sequence animated at 30 Hz, above the chromatic flicker fusion threshold, which is around 25 Hz (Jiang, Zhou, & He, 2007), fusing with the background, and creating a yellowish solid background allover (see Fig. 1 ; Fogelson et al., 2014; Zou, He, & Zhang, 2016). The target arrow is presented in every trial, regardless of condition or block, and is not affected by CFF. A blue fixation cross located at the center of the screen acts as a reference location to where subjects are told to focus their attention throughout the task. Despite the a priori computation of isoluminant shades of the considered red and green colors, a requirement to minimize the flickering and enable fusion, slight variations in display calibration or the participant’s color perception might require fine-tuning to ensure optimal settings. Therefore, a calibration task was introduced to determine the exact luminance levels for each participant, thus ensuring the best possible masking of the flankers. To this effect, stimuli structurally similar to the experimental stimuli, in this case plain squares – replaced the arrows to avoid revealing the content that would be masked –, were displayed under CFF, but enabling the choice of different luminance levels for the green color. The calibration task is explained in more detail in the procedure bellow. The experimental task was performed in a 21 in. Dell LCD monitor with a 1920 × 1080 display resolution, a size of 476.16 × 267.84 mm and a refresh rate of 60 Hz. The brightness and contrast were kept at their intermediate levels (50%) throughout the study and the lab’s ambient light conditions were the same for all participants. Headphones (Sony MDR-XD150) were used to introduce each trial by the sound of a beep. The distance from the participant point of view to the screen was set to 50 cm, and this distance, along with the display’s characteristics, were considered for proper dimensioning of the stimuli to conform to the visual angle dimensions, i.e., stimuli frame occupying 16° and the arrows (flankers included) 2.8° wide. Each trial started with a 200 ms fixation period followed by a 200 ms ramping up of the background during which the target (and flankers, if present) were introduced and kept for 1200 ms (see Fig. 2 ). 2.3 Procedure Upon arriving at the laboratory, participants were provided with the informed consent. Afterwards, besides filling in some information regarding their sex, age, and vision problems, participants filled in an anxiety inventory, the Portuguese version of the State-Trait Anxiety Inventory (STAI Y-1; Silva & Spielberger, 2007; Spielberger, Gorsuch, & Lushene, 1970). This inventory allowed the assessment of the anxiety level of the participants before the experiment, given the influence of anxiety in attentional regulation (Sänger, Bechtold, Schoofs, Blaszkewicz, & Wascher, 2014). However, no participant was excluded, since the scores were within the normal range for the Portuguese population. The experiment started with a calibration phase. Participants were presented with a visible square target suppressed through CFF. This task, known as heterochromatic flicker photometry (Lee, Martin, & Valberg, 1988), allowed participants to set their optimal fusion conditions for the experiment by freely adjusting the luminance of the green color to the value for which the colors seemed to blend and the flickering was minimal. The individual luminance level thus determined was considered throughout their respective experimental task. The calibration phase was essential to prevent the awareness of the suppressed stimuli under CFF, since it reduced the flickering sensation evoked by the technique and created a more solid impression of a fused color between the stimuli and the background (Jiang et al., 2007; Lee et al., 1988). However, subjective awareness of the flankers was still assessed at the end of the experimental task, with the participants verbally questioned as to whether they could identify anything surrounding the target in the two blocks where the flanker stimuli were masked. This was implemented to further ensure the adequate initial calibration of the task and any possible software glitches that might have led to an incomplete suppression. In the pre-task phase, participants undertook a 20-trial practice block, allowing them to get accustomed to the task, and during which only trials with the target arrow were shown. Any clarifications or instructions were further provided, if necessary. The experimental task consisted of 3 different conditions: the masked, the absence and the visible condition. The masked condition was the one where the CFF suppression was implemented, thus granting a masked effect to the flankers that surrounded the target. In the absence condition, only the target arrow (i.e., with no flankers surrounding it) was shown. Finally, in the visible condition, the flankers were made visible around the target (i.e., the CFF technique was not implemented). The task required that the participants would simply report the orientation of the central arrow (target) as fast and accurately as possible by pressing, on a regular keyboard, the letter “z” to indicate left and the letter “m” to indicate right. Each condition, except for the absence condition, contained two types of blocks - A and B. Both blocks included 100 trials but differed in the ratio of congruent to incongruent trials (i.e., trials where flankers were compatible or incompatible with the target arrow). In block A (“neutral block”), both compatible and incompatible trials had an equal weight, i.e., a 1:1 proportion between them (50 congruent to 50 incongruent trials). In block B, amongst the 100 trials, 80 were incongruent and 20 congruent, thus exhibiting a 1:5 proportion of compatible to incompatible trials, assuming the role of “mostly incongruent” block. In every block, both the orientation of the target and the congruency of the flankers were randomly distributed for each participant. The experiment began with the masked condition, with block A being the first to be presented, followed by block B. This order was implemented to prevent a possible incompatible block bias to be carried to the neutral block (Hasegawa & Takahashi, 2014). Next, the absence condition was presented, introduced between the masked and visual conditions. This condition helped to establish baseline and control values for further comparisons and aided the mitigation of any possible adaptation effects. Specifically, the inclusion of the absence block was aimed at preventing any possible bias that block B (mostly incongruent) of the masked condition might have generated and that would be carried to the block A (neutral) of the following condition (visible condition). Finally, the last two blocks were related to the visible conditions (again block A, followed by block B). In sum, excluding the practice trials, each participant was presented with 500 trials. One pause in-between and within each block was introduced to allow participants to take a break and restart the experiment whenever they felt ready. 3 Results 3.1 Response time Trials with incorrect responses (i.e., indicating an incorrect orientation of the target arrow) were eliminated, as well as outliers (±2SD from each participants’ mean). Mean RTs were submitted to a repeated measures analysis of variance (ANOVA) with the condition type (visible vs masked), block type (A – neutral vs B – mostly incongruent) and congruency levels (congruent vs incongruent) as within-subject variables. As a measure for effect size of ANOVAs, we reported the partial eta square (η 2 p ). In addition to the ANOVAs, we also used t-tests for specific comparisons between variables, assuming Cohen’s D as an effect size measure. Significance levels were determined at p ≤ 0.05. The gathered RT data is represented in the table below (Table 1 ). The results revealed a main effect of condition (F(1,28) = 64.396, p < 0.001, η 2 p  = 0.697), with participants being, on average, 40 ms slower in the visible than in the masked condition. A main effect of congruency was also observed (F(1,28) = 130.840, p < 0.001, η 2 p  = 0.824), showing that, overall, congruent trials led to faster RT compared to incongruent trials. Critically, the results showed that congruency effects were modulated by the condition (F(1,28) = 155.529, p < 0.001, η 2 p  = 0.847), reflecting the fact that the condition type (masked vs visible) significantly impacted the congruency effects on performance. Finally, and most importantly, a three-way interaction between condition, block and congruency levels, was also found (F(1,28) = 8.995, p = 0.006, η 2 p  = 0.243). In a more in-depth analysis, within the visible condition, congruent trials of block A resulted in faster RTs, compared to incongruent trials (66.99 ms faster in congruent trials), t(28) = −12.793, p < 0.001, d = −2.375. Block B demonstrated a similar RT pattern, with congruent trials producing faster RTs than incongruent ones (47.84 ms faster in congruent trials), t(28) = −10.733, p < 0.001, d = −1.994. In the masked condition, however, the results revealed no effects related to congruency in block A, with congruent trials not displaying a significant difference when compared to incongruent ones, t(28) = 0.489, p = 0.629, d = 0.091. Yet, in block B, a significant effect of congruency was observed, t(28) = 2.047, p = 0.050, d = 0.379, with slower RTs in congruent, compared to incongruent trials (6.17 ms slower in congruent trials). By specifically comparing the masked condition with the absence condition, the results showed that only the mean RTs from congruent trials in block B showed a significant difference (around 8.49 ms), t(28) = 2.143, p = 0.041, d = 0.397. Congruent and incongruent trials from block A and incongruent trials from block B did not display significant differences relative to the absence condition (p > 0.05). This contrasts with the differences shown in the visible condition, in which, notwithstanding the block type or congruency levels of trials, there was always a significant difference from the absence condition (p ≤ 0.05; see Fig. 3 ). We also performed comparisons across conditions (masked vs visible), with the results revealing that only congruent trials failed to show significant differences between conditions in block A (t(28) = −1.658, p = 0.108, d = −0.308) and block B (t(28) = −1.741, p = 0.093, d = −0.323). Incongruent trials of both block A (t(28) = −9.783, p < 0.001, d = −1.818) and block B (t(28) = −10.748, p < 0.001, d = −1.996) were significantly different between masked and visible conditions. When addressing the compatibility index in the visible condition, aimed at the analysis of adaptation effects, a significant reduction in this index in block B (mostly incongruent), compared to the block A (neutral), was observed (t(28) = 4.864, p < 0.001, d = 0.903). The compatibility index analysis in the masked condition was not analyzed due to the lack of regular conflict observed in this condition – a requirement for further adaptations effects to take place. 3.2 Accuracy Given the lack of a normal distribution, accuracy data and possible effects derived from the independent variables were explored with Wilcoxon Signed-Ranks tests. Effect sizes were calculated as matched pairs rank-biserial correlations (“mpRBC”) (Kerby, 2014; Tomczak & Tomczak, 2014). The gathered accuracy data is represented in the table below (Table 2 ). The results showed that, in the visible condition, a higher accuracy was evident in the congruent trials, compared to the incongruent ones, in both block A (4.32% difference; Z = −3.985, p < 0.001, mpRBC = 0.425) and block B (2.46% difference; Z = 3.537, p < 0.001, mpRBC = 0.324). Regarding the masked condition, in block A, no effect of congruency was observed, with congruent trials revealing no significant differences from incongruent trials, Z = −1.213, p = 0.225, mpRBC = −0.669. In block B, a similar pattern of results, showing no congruency related differences, was obtained, Z = −1.281, p = 0.200, mpRBC = −0.241. In the masked condition, only the incongruent trials of both blocks A and B showed no significant difference from the absence condition (Z = −1.189, p = 0.235, mpRBC = −0.248, and Z = −1.271, p = 0.204, mpRBC = 0.037, respectively). Significant differences were, nonetheless, observed when comparing the absence condition with congruent trials from masked condition in both blocks A (Z = −3.087, p = 0.002, mpRBC = −0.064) and B (Z = −2.625, p = 0.009, mpRBC = −0.124). As for the visible condition, the mean accuracy values within this condition, with the exception for the incongruent trials in block B (Z = −1.514, p = 0.130, mpRBC = −0.411), always showed significant differences (p ≤ 0.05) from the absence condition (see Fig. 4 ). Across conditions, the results revealed that the condition type (masked vs visible) significantly affected accuracy in incongruent trials of both blocks A (Z = −3.623, p < 0.001, mpRBC = 0.179) and B (Z = −2.246, p = 0.025, mpRBC = 0.103), similarly to the RT results. The accuracy in congruent trials was not significantly different between both conditions in blocks A (Z = −0.642, p = 0.521, mpRBC = −0.880) and B (Z = −0.632, p = 0.527, mpRBC = −0.899). Overall, the visible condition showed 1% less accuracy than the visible condition (Z = −3.584, p < 0.001, mpRBC = 0.451), mainly due to the higher error rates in incongruent trials. Adaptation analysis, partially contrasting with the RTs results, revealed a tendency towards the reduction of the compatibility index from block A to block B in the visible condition, but only with a marginally significant difference (Z = −1.708, p = 0.088, mpRBC = −0.411). Again, in the masked condition, no adaptation related analysis was performed due to the same reasons explained earlier. 4 General discussion In the present study, we assessed whether the awareness of flankers, manipulated by means of CFF suppression, was necessary in allowing them to reach the executive control system and evoke conflict and subsequent adaptation. Consistent with previous studies (e.g., Egner & Hirsch, 2005; Van Veen, Cohen, Botvinick, Stenger, & Carter, 2001), our results showed that when visible, flankers did indeed generate conflict, as seen by the increased mean RTs and lower accuracy on incongruent trials, as opposed to congruent ones. In addition, and also in line with previous literature (e.g., Clayson & Larson, 2011; Kuratomi & Yoshizaki, 2013), our results showed conflict adaptation effects in the visible condition that were reflected in the reduction of the compatibility index found in block B (where most trials were incongruent), compared to the neutral block (block A). Contrarily to our prediction, however, such effects were not observed in the masked condition. As in Wu et al. (2015) study, our results showed that masked flankers did not result in a clear conflict pattern in task performance (i.e., conflict). Nevertheless, in our study, the results indicated that the neutral block’s (block A) accuracy for the congruent trials was significantly different from the absence condition, whereas in block B (mostly incongruent) this difference was seen both in RTs and accuracy of the congruent set of trials. Furthermore, again in the masked condition, RTs in block B also displayed differences between congruency levels, where congruent trials resulted in longer RTs, compared to incongruent trials. This difference was the opposite to what was expected (i.e., congruent trials leading to faster RTs than incongruent ones) and stemmed largely from the abnormally high mean RTs that accompanied congruent flankers. A possible explanation for this finding may rely on a deeper processing given to the congruent flankers when these were suppressed by CFF. In fact, such RT pattern is also seen in masked prime paradigms when the stimulus onset asynchrony (SOA; time between cue and the target) is long (over 300 ms). The underlying mechanism seems to rely on an unconscious inhibition that blocks sensorimotor activation from that cue (distractor), dismissing signals that do not lead to an immediate response (Boy et al., 2010; Eimer & Schlaghecken, 2003). Despite sharing this pattern, however, since we used flankers instead of primes (i.e., distractors presented at the same time as the target), this is an unlikely explanation of our results. A further plausible explanation for these findings (i.e., longer RTs in congruent trials compared to incongruent ones) relies on the possibility that the lower proportion of congruent trials may have activated the orienting network of the attentional system (Callejas, Lupiáñez, & Tudela, 2004; Posner, 1980, 2012). Indeed, in the block of trials where this effect was observed, most of the arrow flankers displayed the opposite orientation to that of the central arrow (1:5 congruent to incongruent ratio). Hence, congruent trials may have resulted in orienting responses due to their unexpected occurrence, compared to what was expected in this block of trials (i.e., flankers incongruent with the target). This “post-unexpectedness” slowing has already been seen in similar flanker studies (e.g., Houtman, Castellar, & Notebaert, 2012; Notebaert et al., 2009), where the less expected the stimuli resulted in more attention oriented towards them. This proposition is based on the phenomenon known as the orienting response, which results from the disparity between a predefined model of the stimulus that emerged as a result of experience, and the new unexpected stimulus (Posner, 1994; Sokolov, 1963, 1990; Zernicki, 1987). Orienting responses can take place independently of consciousness (Lamme, 2003; Lu, Cai, Shen, Zhou, & Han, 2012; McCormick, 1997) and provide an enhancement towards the processing of the stimulus, promoting the activation of the executive control network (Callejas, Lupiàñez, Funes, & Tudela, 2005; Callejas et al., 2004; Fan et al., 2005). Thus, in our study, it is likely that the orienting response given to congruent trials in that specific block, where they were less represented (therefore unexpected), resulted in higher RTs due to their processing costs, accounting for the differences found. This presumed orienting response was not observed in the visible condition because conflict effects on performance (derived from a deeper analysis) stood out more and concealed any orienting responses. We, nonetheless, agree that this previous explanation (i.e., orienting response) is only a tentative account on the unexpected effect observed in the masked condition, and future studies should further investigate this hypothesis. In the masked condition, as previously mentioned, our results also demonstrated significantly higher accuracy rates for congruent trials (although not significantly different from the incongruent ones), compared to the absence condition. Indeed, previous studies have shown that selective attention given to unattended stimuli varies according to several contextual factors, such as perceptual load (Lavie, 1995) or spatial proximity (Diede & Bugg, 2016). Literature has also shown that visual areas tend to respond more strongly to ignored stimulus when they share the same features as the target, even if irrelevant and subsequently not directly attended (Melcher, Papathomas, & Vidnyánszky, 2005; Saenz, Buracas, & Boynton, 2002). This low-level perceptual grouping effect seems to modulate the attention given to stimuli across the visual field, even in the early stages of visual processing, grouping simple features, such as orientation, color, motion, among others (Kapadia, Ito, Gilbert, & Westheimer, 1995; Mozer & Vecera, 2005; Raizada & Grossberg, 2001). In our study, attention given to a specific stimulus feature (target arrow orientation) may have led to an increased response of cortical visual areas to spatially different, and unconsciously perceived, similar stimuli (congruent flankers). This would account for the more cohesive distinctiveness between the mean accuracies of masked congruent trials and the absence condition, as well as similar accuracies between the masked and visible conditions when congruent flankers surrounded the target. Yet, this effect appeared to be subtle enough as to not have evoked differences to the opposing incongruent trials. In this regard, additional research is required to further explore this tentative explanation. Contrarily to our predictions, and although we did not directly compare suppressed flankers by CFS and CFF, our results seem to indicate that the use of CFF does not appear to differ from CFS, given the results shown by (Wu et al., 2015) using the latter technique, when it comes to eliciting high-level cognitive impact from suppressed information. In contrast, the use of the masked priming as a suppression technique and as a mean of presenting the distractors, seems to be more adequate to the exploration of executive control (e.g., Desender et al., 2013; Hasegawa & Takahashi, 2014; van Gaal, Lamme, & Ridderinkhof, 2010). Nevertheless, this apparent lack of conflict-related effects with CFF might be explained by the necessity of temporal-spatial direction of attention to the suppressed distractors, i.e., our visual focus must already be directed at the area where distractors are presented (den Bussche, Hughes, Humbeeck, & Reynvoet, 2010; Marzouki, Grainger, & Theeuwes, 2007). Similarly to Wu et al. (2015) study, however, the flankers presented during our study were in very close proximity to where the target arrow was displayed, i.e., likely within the area where the participants’ focus of attention was directed at (center of the screen), thus unlikely to be the cause of our results. Moreover, not only do masked stimuli tend to elicit weaker effects (Desender et al., 2013), but also top-down processes are relatively slow, in comparison to bottom-up processes (Mulckhuyse & Theeuwes, 2010). Hence, while competing with the target at the same time (SOA = 0 ms), it is plausible that flankers were not able to induce high-level effects. Consistent with this idea, some studies (e.g., Bahrami et al., 2010; Koivisto & Grassini, 2018) have shown prime induced conflict effects under interocular suppression (i.e., CFS; but see, e.g., Kang, Blake, & Woodman, 2011; Sakuraba, Sakai, Yamanaka, Yokosawa, & Hirayama, 2012). This points to the possible importance of primes (i.e., distractors that briefly precede the target) as opposed to flankers (which are presented simultaneously) in eliciting unconscious executive control effects (e.g., Almeida, Mahon, Nakayama, & Caramazza, 2008; Zabelina et al., 2013). Indeed, the manipulation of the SOA, more precisely, the increase of this value to above 0 ms, might be a prerequisite to enable high-level reach of the information presented subliminally. One other possible explanation comes from a study by Gayet, Van der Stigchel, and Paffen (2014). In this study, suppressed primes, under interocular suppression, only seemed to impact performance (i.e., reach executive control mechanisms and elicit conflict effects), when intermixed with visible congruent trials that gave the notion of high levels of predictability to the suppressed distractors. This suggests a possible need of first establishing a high value to suppressed distractors by simultaneously presenting visible flankers with high predictability (always congruent). Otherwise, the suppressed flankers may tend to remain impartial throughout the task. One final suggestion addressing the non-apparent impact in overall performance by the stimuli suppressed by means of CFF (a more “suppressive” technique, when compared to masked priming), regards the lack of emotional value associated with the masked distractors, as already suggested by Wu et al. (2015). Threat-related information not only captures our attention more easily (i.e., is more promptly detected relative to neutral stimuli), but has a more “privileged” neural processing, in order to enable faster and more efficient defensive responses (Gomes, Silva, Silva, & Soares, 2017; Li, Zinbarg, Boehm, & Paller, 2008; Öhman, 2005; Soares et al., 2017). It is thus a fair assumption that emotional stimuli, particularly threat-related information, may have a more powerful reach and impact on executive control (e.g., Jiang, Bailey, Chen, Cui, & Zhang, 2013; Li, et al., 2008; Murphy & Zajonc, 1993; Winkielman, Berridge, & Wilbarger, 2005). Therefore, an added emotional value to the stimuli might be a requirement in allowing conflict related effects to take place, which would then explain our current results with innocuous stimuli. In sum, the present study seems to support earlier theories highlighting the limitations of unconscious information in reaching high-level processes (Dehaene & Naccache, 2001). Although previous masked priming studies have come to prove otherwise, some authors (see Desender & Van den Bussche, 2012) have suggested alternative explanations regarding the impact elicited by unconscious primes on performance without involving executive control system (but see Huber-Huber & Ansorge, 2018). 4.1 Limitations and future directions Despite our verification procedure attending to possible incorrect calibrations or to spurious software related glitches by means of an end-of-task verbal inquiry, addressing the participants’ subjective awareness of any masked flankers, a more objective task for this purpose could have been implemented. For instance, in future studies, participants can be asked to perform a flanker discrimination task at the end of the experiment, by being presented only with the flanker stimuli under suppression, and asked to decide on its orientation. This forced-choice task should reinforce the subjective suppression assurance measures (Sandberg, Timmermans, Overgaard, & Cleeremans, 2010). Moreover, we did not introduce a block in which trials were mostly congruent as such option would have increased the repetitive nature and duration of the task. This would, however, allow for a direct comparison with the mostly incongruent block used in the current study and could aid in disentangling whether masked orienting responses were independent on the nature of the unexpected stimuli. Moreover, we suggest future studies to introduce target and flanker stimuli with emotional relevance (i.e., threatening) to assess whether unconscious executive control functions are mostly observed depending on the nature of the stimuli. Finally, since several studies have shown conflict effects to be more pronounced when distractors very briefly precede the target (e.g., Grice & Gwynne, 1985; Kopp, Rist, & Mattler, 1996; Taylor, 1977), another suggestion for further studies is the use of primes (i.e., SOA > 0 ms) under the same technique. Further, we suggest the addition of visible congruent trials in-between masked trials, possibly conceding an increased value to masked flankers, allowing the assessment of such requirement in granting distractors a more privileged processing. 5 Conclusion In the present study, results showed that stimuli under the suppression of the CFF technique, did not seem to evoke a clear conflict pattern, hence failing to reach and modulate executive control. Interestingly, however, under suppression, flankers still managed to produce effects on performance. One of the effects was translated in a reverse RT pattern to what was expected in the block where most flankers were incongruent. This was viewed as reflecting an orienting response to unexpected congruent stimuli, hence elevating the overall RTs for congruent trials. Additionally, accuracy values of congruent trials, due to what was interpreted as perceptual grouping, were distinctively different from the condition where no flankers were shown. Nonetheless, since these effects are presumably derived from a low-level processing, the present results are consistent with the notion that unconsciously perceived information (at least innocuous stimuli) have a limited access to the executive control system. Additional research work is needed to provide a more solid view into why and under which conditions is the executive control system modulated by unconscious information. Funding This article was supported by FEDER through the operation POCI-01-0145-FEDER-007746 funded by the Programa Operacional Competitividade e Internacionalização – COMPETE2020 and by National Funds through FCT - Fundação para a Ciência e a Tecnologia within CINTESIS, R&D Unit (reference UID/IC/4255/2013); and by FEDER and CENTRO 2020 funds through project SOCA – Smart Open Campus (ref. CENTRO-01-0145-FEDER-000010) and by research unit IEETA, funded by National Funds through the FCT - Foundation for Science and Technology, in the context of project UID/CEC/00127/2013. Declaration of interest None. References Almeida et al., 2008 J. Almeida B.Z. Mahon K. Nakayama A. Caramazza Unconscious processing dissociates along categorical lines Proceedings of the National Academy of Sciences 105 39 2008 15214 15218 10.1073/pnas.0805867105 Ansorge et al., 2011 U. Ansorge I. Fuchs S. Khalid W. Kunde No conflict control in the absence of awareness Psychological Research 75 5 2011 351 365 10.1007/s00426-010-0313-4 Ansorge et al., 2014 U. Ansorge W. Kunde M. Kiefer Unconscious vision and executive control: How unconscious processing and conscious action control interact Consciousness and Cognition 27 2014 268 287 10.1016/j.concog.2014.05.009 Atas et al., 2016 A. Atas K. Desender W. Gevers A. Cleeremans Dissociating perception from action during conscious and unconscious conflict adaptation Journal of Experimental Psychology: Learning, Memory, and Cognition 42 6 2016 866 881 10.1037/xlm0000206 Bahrami et al., 2010 B. Bahrami P. Vetter E. Spolaore S. Pagano B. Butterworth G. Rees Unconscious numerical priming despite interocular suppression Psychological Science 21 2 2010 224 233 10.1177/0956797609360664 Bonneh et al., 2001 Y.S. Bonneh A. Cooperman D. Sagi Motion-induced blindness in normal observers Nature 411 6839 2001 798 10.1038/35081073 Botvinick et al., 2001 M.M. Botvinick T.S. Braver D.M. Barch C.S. Carter J.D. Cohen Conflict monitoring and cognitive control Psychological Review 108 3 2001 624 10.1037/0033.295X.108.3.624 Boy et al., 2010 F. Boy M. Husain P. Sumner Unconscious inhibition separates two forms of cognitive control Proceedings of the National Academy of Sciences 107 24 2010 11134 11139 10.1073/pnas.1001925107 Braver, 2012 T.S. Braver The variable nature of cognitive control: A dual mechanisms framework Trends in Cognitive Sciences 16 2 2012 106 113 10.1016/j.tics.2011.12.010 Breitmeyer, 2015 B. Breitmeyer Psychophysical “blinding” methods reveal a functional hierarchy of unconscious visual processing Consciousness and Cognition 35 2015 234 250 10.1016/j.concog.2015.01.012 Breitmeyer and Öğmen, 2006 B. Breitmeyer H. Öğmen Visual masking: Time slices through conscious and unconscious vision 2006 Oxford University Press Bruchmann et al., 2010 M. Bruchmann K. Herper C. Konrad C. Pantev R.J. Huster Individualized EEG source reconstruction of Stroop interference with masked color words NeuroImage 49 2 2010 1800 1809 10.1016/j.neuroimage.2009.09.032 den Bussche et al., 2010 E.V. den Bussche G. Hughes N.V. Humbeeck B. Reynvoet The relation between consciousness and attention: An empirical study using the priming paradigm Consciousness and Cognition 19 1 2010 86 97 10.1016/j.concog.2009.12.019 Callejas et al., 2005 A. Callejas J. Lupiàñez M.J. Funes P. Tudela Modulations among the alerting, orienting and executive control networks Experimental Brain Research 167 1 2005 27 37 10.1007/s00221-005-2365-z Callejas et al., 2004 A. Callejas J. Lupiáñez P. Tudela The three attentional networks: On their independence and interactions Brain and Cognition 54 3 2004 225 227 10.1016/j.bandc.2004.02.012 Carter and van Veen, 2007 C.S. Carter V. van Veen Anterior cingulate cortex and conflict detection: An update of theory and data Cognitive, Affective & Behavioral Neuroscience 7 4 2007 367 379 10.3758/CABN.7.4.367 Cho et al., 2006 R.Y. Cho R.O. Konecky C.S. Carter Impairments in frontal cortical γ synchrony and cognitive control in schizophrenia Proceedings of the National Academy of Sciences 103 52 2006 19878 19883 10.1073/pnas.0609440103 Clayson and Larson, 2011 P.E. Clayson M.J. Larson Conflict adaptation and sequential trial effects: Support for the conflict monitoring theory Neuropsychologia 49 7 2011 1953 1961 10.1016/j.neuropsychologia.2011.03.023 Cohen et al., 2000 J.D. Cohen M. Botvinick C.S. Carter Anterior cingulate and prefrontal cortex: Who’s in control? Nature Neuroscience 3 5 2000 421 423 10.1038/74783 Corbetta and Shulman, 2002 M. Corbetta G.L. Shulman Control of goal-directed and stimulus-driven attention in the brain Nature Reviews Neuroscience 3 3 2002 215 229 10.1038/nrn755 Cushman and Levinson, 1983 W.B. Cushman J.Z. Levinson Phase shift in red and green counterphase flicker at high frequencies JOSA 73 11 1983 1557 1561 10.1364/JOSA.73.001557 Custers and Aarts, 2010 R. Custers H. Aarts The unconscious will: How the pursuit of goals operates outside of conscious awareness Science 329 5987 2010 47 50 10.1126/science.1188595 De Pisapia et al., 2012 N. De Pisapia M. Turatto P. Lin J. Jovicich A. Caramazza Unconscious priming instructions modulate activity in default and executive networks of the human brain Cerebral Cortex 22 3 2012 639 649 10.1093/cercor/bhr146 Dehaene and Naccache, 2001 S. Dehaene L. Naccache Towards a cognitive neuroscience of consciousness: Basic evidence and a workspace framework Cognition 79 1 2001 1 37 10.1016/S0010-0277(00)00123-2 Desender and Van den Bussche, 2012 K. Desender E. Van den Bussche Is Consciousness necessary for conflict adaptation? A state of the art Frontiers in Human Neuroscience 6 2012 10.3389/fnhum.2012.00003 Desender et al., 2013 K. Desender E. Van Lierde E. Van den Bussche Comparing conscious and unconscious conflict adaptation PLoS One 8 2 2013 10.1371/journal.pone.0055976 e55976 Diede and Bugg, 2016 N.T. Diede J.M. Bugg Spatial proximity as a determinant of context-specific attentional settings Attention, Perception, & Psychophysics 78 5 2016 1255 1266 10.3758/s13414-016-1086-7 Diede and Bugg, 2017 N.T. Diede J.M. Bugg Cognitive effort is modulated outside of the explicit awareness of conflict frequency: Evidence from pupillometry Journal of Experimental Psychology: Learning, Memory, and Cognition 43 5 2017 824 835 10.1037/xlm0000349 D’Ostilio and Garraux, 2012 K. D’Ostilio G. Garraux Dissociation between unconscious motor response facilitation and conflict in medial frontal areas: Unconscious motor facilitation and conflict European Journal of Neuroscience 35 2 2012 332 340 10.1111/j.1460-9568.2011.07941.x Duncan, 1980 J. Duncan The locus of interference in the perception of simultaneous stimuli Psychological Review 87 3 1980 272 10.1037/0033-295X.87.3.272 Egner and Hirsch, 2005 T. Egner J. Hirsch Cognitive control mechanisms resolve conflict through cortical amplification of task-relevant information Nature Neuroscience 8 12 2005 1784 1790 10.1038/nn1594 Eimer and Schlaghecken, 2003 M. Eimer F. Schlaghecken Response facilitation and inhibition in subliminal priming Biological Psychology 64 1–2 2003 7 26 10.1016/S0301-0511(03)00100-5 Eriksen and Eriksen, 1974 B.A. Eriksen C.W. Eriksen Effects of noise letters upon the identification of a target letter in a nonsearch task Attention, Perception, & Psychophysics 16 1 1974 143 149 10.3758/BF03203267 Fan, 2014 J. Fan An information theory account of cognitive control Frontiers in Human Neuroscience 8 2014 10.3389/fnhum.2014.00680 Fan et al., 2009 J. Fan X. Gu K.G. Guise X. Liu J. Fossella H. Wang M.I. Posner Testing the behavioral interaction and integration of attentional networks Brain and Cognition 70 2 2009 209 220 10.1016/j.bandc.2009.02.002 Fan et al., 2005 J. Fan B. Mccandliss J. Fossella J. Flombaum M. Posner The activation of attentional networks NeuroImage 26 2 2005 471 479 10.1016/j.neuroimage.2005.02.004 Fogelson et al., 2014 S.V. Fogelson P.J. Kohler K.J. Miller R. Granger P.U. Tse Unconscious neural processing differs with method used to render stimuli invisible Frontiers in Psychology 5 2014 601 10.3389/fpsyg.2014.00601 Gayet et al., 2014 S. Gayet S. Van der Stigchel C.L.E. Paffen Seeing is believing: Utilization of subliminal symbols requires a visible relevant context Attention, Perception, & Psychophysics 76 2 2014 489 507 10.3758/s13414-013-0580-4 Ghinescu et al., 2010 R. Ghinescu T.R. Schachtman M.A. Stadler M. Fabiani G. Gratton Strategic behavior without awareness? Effects of implicit learning in the Eriksen flanker paradigm Memory & Cognition 38 2 2010 197 205 10.3758/MC.38.2.197 Gomes et al., 2017 N. Gomes S. Silva C.F. Silva S.C. Soares Beware the serpent: The advantage of ecologically-relevant stimuli in accessing visual awareness Evolution and Human Behavior 38 2 2017 227 234 10.1016/j.evolhumbehav.2016.10.004 Gratton et al., 1992 G. Gratton M.G. Coles E. Donchin Optimizing the use of information: Strategic control of activation of responses Journal of Experimental Psychology: General 121 4 1992 480 10.1037//0096-3445.121.4.480 Grice and Gwynne, 1985 G.R. Grice J.W. Gwynne Temporal characteristics of noise conditions producing facilitation and interference Attention, Perception, & Psychophysics 37 6 1985 495 501 10.3758/BF03204912 Hasegawa and Takahashi, 2014 K. Hasegawa S. Takahashi The role of visual awareness for conflict adaptation in the masked priming task: Comparing block-wise adaptation with trial-by-trial adaptation Frontiers in Psychology 5 2014 1347 10.3389/fpsyg.2014.01347 Hoshiyama et al., 2006 M. Hoshiyama R. Kakigi Y. Takeshima K. Miki S. Watanabe Priority of face perception during subliminal stimulation using a new color-opponent flicker stimulation Neuroscience Letters 402 1–2 2006 57 61 10.1016/j.neulet.2006.03.054 Houtman et al., 2012 F. Houtman E.N. Castellar W. Notebaert Orienting to errors with and without immediate feedback Journal of Cognitive Psychology 24 3 2012 278 285 10.1080/20445911.2011.617301 Huber-Huber and Ansorge, 2018 C. Huber-Huber U. Ansorge Unconscious conflict adaptation without feature-repetitions and response time carry-over Journal of Experimental Psychology: Human Perception and Performance 44 2 2018 169 175 10.1037/xhp0000450 Jiang et al., 2013 J. Jiang K. Bailey A. Chen Q. Cui Q. Zhang Unconsciously triggered emotional conflict by emotional facial expressions PLoS One 8 2 2013 10.1371/journal.pone.0055907 e55907 Jiang et al., 2015 J. Jiang Q. Zhang S. van Gaal Conflict awareness dissociates theta-band neural dynamics of the medial frontal and lateral frontal cortex during trial-by-trial cognitive control Neuroimage 116 2015 102 111 10.1016/j.neuroimage.2015.04.062 Jiang et al., 2007 Y. Jiang K. Zhou S. He Human visual cortex responds to invisible chromatic flicker Nature Neuroscience 10 5 2007 657 662 10.1038/nn1879 Kahneman, 1973 D. Kahneman Attention and effort 1973 Prentice-Hall Englewood Cliffs NJ Kang et al., 2011 M.-S. Kang R. Blake G.F. Woodman Semantic analysis does not occur in the absence of awareness induced by interocular suppression Journal of Neuroscience 31 38 2011 13535 13545 10.1523/JNEUROSCI.1691-11.2011 Kapadia et al., 1995 M.K. Kapadia M. Ito C.D. Gilbert G. Westheimer Improvement in visual sensitivity by changes in local context: Parallel studies in human observers and in V1 of alert monkeys Neuron 15 4 1995 843 856 10.1016/0896-6273(95)90175-2 Kerby, 2014 D.S. Kerby The simple difference formula: An approach to teaching nonparametric correlation Comprehensive Psychology 3 1 2014 10.2466/11.IT.3.1 Kiefer and Martens, 2010 M. Kiefer U. Martens Attentional sensitization of unconscious cognition: Task sets modulate subsequent masked semantic priming Journal of Experimental Psychology: General 139 3 2010 464 489 10.1037/a0019561 Koechlin, 2003 E. Koechlin The architecture of cognitive control in the human prefrontal cortex Science 302 5648 2003 1181 1185 10.1126/science.1088545 Koivisto and Grassini, 2018 M. Koivisto S. Grassini Unconscious response priming during continuous flash suppression PLoS One 13 2 2018 10.1371/journal.pone.0192201 e0192201 Kopp et al., 1996 B. Kopp F. Rist U. Mattler N200 in the flanker task as a neurobehavioral tool for investigating executive control Psychophysiology 33 3 1996 282 294 10.1111/j.1469-8986.1996.tb00425.x Kuratomi and Yoshizaki, 2013 K. Kuratomi K. Yoshizaki Block-wise conflict adaptation of visual selectivity: Role of hemisphere-dependent and location-specific mechanisms: Block-wise conflict adaptation of visual selectivity Japanese Psychological Research 55 4 2013 315 328 10.1111/jpr.12015 Lamme, 2003 V.A. Lamme Why visual attention and awareness are different Trends in Cognitive Sciences 7 1 2003 12 18 10.1016/S1364-6613(02)00013-X Larson et al., 2009 M.J. Larson D.A.S. Kaufman W.M. Perlstein Neural time course of conflict adaptation effects on the Stroop task Neuropsychologia 47 3 2009 663 670 10.1016/j.neuropsychologia.2008.11.013 Lavie, 1995 N. Lavie Perceptual Load as a Necessary Condition for Selective Attention 21 3 1995 451 468 10.1037//0096-1523.21.3.451 Lee et al., 1988 B.B. Lee P.R. Martin A. Valberg The physiological basis of heterochromatic flicker photometry demonstrated in the ganglion cells of the macaque retina The Journal of Physiology 404 1 1988 323 347 10.1113/jphysiol.1988.sp017292 Li et al., 2008 W. Li R.E. Zinbarg S.G. Boehm K.A. Paller Neural and behavioral evidence for affective priming from unconsciously perceived emotional facial expressions and the influence of trait anxiety Journal of Cognitive Neuroscience 20 1 2008 95 107 10.1162/jocn.2008.20006 Logan and Gordon, 2001 G.D. Logan R.D. Gordon Executive control of visual attention in dual-task situations Psychological Review 108 2 2001 393 434 10.1037//0033-295X.108.2.393 Logan and Zbrodoff, 1979 G.D. Logan N.J. Zbrodoff When it helps to be misled: Facilitative effects of increasing the frequency of conflicting stimuli in a Stroop-like task Memory & Cognition 7 3 1979 166 174 10.3758/BF03197535 Logothetis et al., 1996 N.K. Logothetis D.A. Leopold D.L. Sheinberg What is rivalling during binocular rivalry? Nature 380 6575 1996 621 10.1038/380621a0 Lu et al., 2012 S. Lu Y. Cai M. Shen Y. Zhou S. Han Alerting and orienting of attention without visual awareness Consciousness and Cognition 21 2 2012 928 938 10.1016/j.concog.2012.03.012 Mack and Rock, 1998 A. Mack I. Rock Inattentional blindness 1998 MIT Press Cambridge MA Mackie et al., 2013 M.-A. Mackie N.T. Van Dam J. Fan Cognitive control and attentional functions Brain and Cognition 82 3 2013 301 312 10.1016/j.bandc.2013.05.004 Mansouri et al., 2009 F.A. Mansouri K. Tanaka M.J. Buckley Conflict-induced behavioural adjustment: A clue to the executive functions of the prefrontal cortex Nature Reviews Neuroscience 10 2 2009 141 152 10.1038/nrn2538 Marien et al., 2012 H. Marien R. Custers R.R. Hassin H. Aarts Unconscious goal activation and the hijacking of the executive function Journal of Personality and Social Psychology 103 3 2012 399 415 10.1037/a0028955 Marzouki et al., 2007 Y. Marzouki J. Grainger J. Theeuwes Exogenous spatial cueing modulates subliminal masked priming Acta Psychologica 126 1 2007 34 45 10.1016/j.actpsy.2006.11.002 McCormick, 1997 P.A. McCormick Orienting attention without awareness Journal of Experimental Psychology: Human Perception and Performance 23 1 1997 168 10.1037/0096-1523.23.1.168 Melcher et al., 2005 D. Melcher T.V. Papathomas Z. Vidnyánszky Implicit attentional selection of bound visual features Neuron 46 5 2005 723 729 10.1016/j.neuron.2005.04.023 Meyer and Schvaneveldt, 1971 D.E. Meyer R.W. Schvaneveldt Facilitation in recognizing pairs of words: Evidence of a dependence between retrieval operations Journal of Experimental Psychology 90 2 1971 227 234 10.1037/h0031564 Miller, 2000 E.K. Miller The prefrontal cortex and cognitive control Nature Reviews. Neuroscience 1 1 2000 59 10.1038/35036228 Moutoussis and Zeki, 2002 K. Moutoussis S. Zeki The relationship between cortical activation and perception investigated with invisible stimuli Proceedings of the National Academy of Sciences 99 14 2002 9527 9532 10.1073/pnas.142305699 Mozer and Vecera, 2005 M.C. Mozer S.P. Vecera Space-and object-based attention Neurobiology of Attention 130–134 2005 10.1016/B978-012375731-9/50027-6 Mudrik et al., 2011 L. Mudrik A. Breska D. Lamy L.Y. Deouell Integration without awareness: Expanding the limits of unconscious processing Psychological Science 22 6 2011 764 770 10.1177/0956797611408736 Mulckhuyse and Theeuwes, 2010 M. Mulckhuyse J. Theeuwes Unconscious attentional orienting to exogenous cues: A review of the literature Acta Psychologica 134 3 2010 299 309 10.1016/j.actpsy.2010.03.002 Murphy and Zajonc, 1993 S.T. Murphy R.B. Zajonc Affect, cognition, and awareness: Affective priming with optimal and suboptimal stimulus exposures Journal of Personality and Social Psychology 64 5 1993 723 Nieuwenhuis et al., 2006 S. Nieuwenhuis J.F. Stins D. Posthuma T.J. Polderman D.I. Boomsma E.J. de Geus Accounting for sequential trial effects in the flanker task: Conflict adaptation or associative priming? Memory & Cognition 34 6 2006 1260 1272 10.3758/BF03193270 Notebaert et al., 2009 W. Notebaert F. Houtman F.V. Opstal W. Gevers W. Fias T. Verguts Post-error slowing: An orienting account Cognition 111 2 2009 275 279 10.1016/j.cognition.2009.02.002 Öhman, 2005 A. Öhman The role of the amygdala in human fear: Automatic detection of threat Psychoneuroendocrinology 30 10 2005 953 958 10.1016/j.psyneuen.2005.03.019 Panadero et al., 2015 A. Panadero M.C. Castellanos P. Tudela Unconscious context-specific proportion congruency effect in a stroop-like task Consciousness and Cognition 31 2015 35 45 10.1016/j.concog.2014.09.016 Pashler, 1994 H. Pashler Dual-task interference in simple tasks: Data and theory Psychological Bulletin 116 2 1994 220 10.1037//0033-2909.116.2.220 Peremen and Lamy, 2014 Z. Peremen D. Lamy Comparing unconscious processing during continuous flash suppression and meta-contrast masking just under the limen of consciousness Frontiers in Psychology 5 2014 10.3389/fpsyg.2014.00969 Posner, 1980 M.I. Posner Orienting of attention Quarterly Journal of Experimental Psychology 32 1 1980 3 25 10.1080/00335558008248231 Posner, 1994 M.I. Posner Attention: The mechanisms of consciousness Proceedings of the National Academy of Sciences 91 16 1994 7398 7403 Posner, 2012 M.I. Posner Attentional networks and consciousness Frontiers in Psychology 3 64 2012 10.3389/fpsyg.2012.00064 Posner and Fan, 2008 M.I. Posner J. Fan Attention as an organ system J.R. Pomerantz Topics in Integrative Neuroscience 2008 Cambridge University Press Cambridge 31 61 10.1017/CBO9780511541681.005 Posner et al., 2004 M.I. Posner C.R. Snyder R. Solso Attention and cognitive control 2004 Psychology Press New York, NY, US Raizada and Grossberg, 2001 R.D.S. Raizada S. Grossberg Context-sensitive binding by the laminar circuits of V1 and V2: A unified model of perceptual grouping, attention, and orientation contrast Visual Cognition 8 3–5 2001 431 466 10.1080/13506280143000070 Raymond et al., 1992 J.E. Raymond K.L. Shapiro K.M. Arnell Temporary suppression of visual processing in an RSVP task: An attentional blink? Journal of Experimental Psychology: Human Perception and Performance 18 3 1992 849 10.1037/0096-1523.18.3.849 Ric and Muller, 2012 F. Ric D. Muller Unconscious addition: When we unconsciously initiate and follow arithmetic rules Journal of Experimental Psychology: General 141 2 2012 222 226 10.1037/a0024608 Ridderinkhof et al., 2004 K.R. Ridderinkhof M. Ullsperger E.A. Crone S. Nieuwenhuis The role of the medial frontal cortex in cognitive control Science 306 5695 2004 443 447 10.1126/science.1100301 Saenz et al., 2002 M. Saenz G.T. Buracas G.M. Boynton Global effects of feature-based attention in human visual cortex Nature Neuroscience 5 7 2002 631 632 10.1038/nn876 Sakuraba et al., 2012 S. Sakuraba S. Sakai M. Yamanaka K. Yokosawa K. Hirayama Does the human dorsal stream really process a category for tools? Journal of Neuroscience 32 11 2012 3949 3953 10.1523/JNEUROSCI.3973-11.2012 Sandberg et al., 2010 K. Sandberg B. Timmermans M. Overgaard A. Cleeremans Measuring consciousness: Is one measure better than the other? Consciousness and Cognition 19 4 2010 1069 1078 10.1016/j.concog.2009.12.013 Sänger et al., 2014 J. Sänger L. Bechtold D. Schoofs M. Blaszkewicz E. Wascher The influence of acute stress on attention mechanisms and its electrophysiological correlates Frontiers in Behavioral Neuroscience 8 353 2014 10.3389/fnbeh.2014.00353 Schouppe et al., 2014 N. Schouppe E. de Ferrerre F. Van Opstal S. Braem W. Notebaert Conscious and unconscious context-specific cognitive control Frontiers in Psychology 5 539 2014 10.3389/fpsyg.2014.00539 Silva and Spielberger, 2007 D.R. Silva C.D. Spielberger Manual do Inventário de Estado-Traço de Ansiedade (STAI) 2007 Mind Garden Inc. Silverstein et al., 2015 B.H. Silverstein M. Snodgrass H. Shevrin R. Kushwaha P3b, consciousness, and complex unconscious processing Cortex 73 2015 216 227 10.1016/j.cortex.2015.09.004 Simons and Levin, 1997 D.J. Simons D.T. Levin Change blindness Trends in Cognitive Sciences 1 7 1997 261 267 10.1016/S1364-6613(97)01080-2 Snodgrass and Shevrin, 2006 M. Snodgrass H. Shevrin Unconscious inhibition and facilitation at the objective detection threshold: Replicable and qualitatively different unconscious perceptual effects Cognition 101 1 2006 43 79 10.1016/j.cognition.2005.06.006 Soares et al., 2017 S.C. Soares D. Kessel M. Hernández-Lorca M.J. García-Rubio P. Rodrigues N. Gomes L. Carretié Exogenous attention to fear: Differential behavioral and neural responses to snakes and spiders Neuropsychologia 99 2017 139 147 10.1016/j.neuropsychologia.2017.03.007 Sokolov, 1963 E. Sokolov Higher nervous functions: The orienting reflex Annual Review of Physiology 25 1 1963 545 580 10.1146/annurev.ph.25.030163.002553 Sokolov, 1990 E. Sokolov The orienting response, and future directions of its development Integrative Physiological and Behavioral Science 25 3 1990 142 150 10.1007/BF02974268 Spielberger et al., 1970 C.D. Spielberger R.L. Gorsuch R.E. Lushene Manual for the state-trait anxiety inventory 1970 Consulting Psychologists Press CA Stroop, 1935 J.R. Stroop Studies of interference in serial verbal reactions Journal of Experimental Psychology 18 6 1935 643 10.1037/h0054651 Taylor, 1977 D.A. Taylor Time course of context effects Journal of Experimental Psychology: General 106 4 1977 404 10.1037/0096-3445.106.4.404 Tomczak and Tomczak, 2014 M. Tomczak E. Tomczak The need to report effect size estimates revisited. An overview of some recommended measures of effect size Trends in Sport Sciences 21 1 2014 Tsuchiya and Koch, 2005 N. Tsuchiya C. Koch Continuous flash suppression reduces negative afterimages Nature Neuroscience 8 8 2005 1096 1101 10.1038/nn1500 Tsuchiya et al., 2006 N. Tsuchiya C. Koch L.A. Gilroy R. Blake Depth of interocular suppression associated with continuous flash suppression, flash suppression, and binocular rivalry Journal of Vision 6 10 2006 6 10.1167/6.10.6 Tulving and Schacter, 1990 E. Tulving D.L. Schacter Priming and human memory systems Science 247 4940 1990 301 306 10.1126/science.2296719 Ullsperger et al., 2005 M. Ullsperger L.M. Bylsma M.M. Botvinick The conflict adaptation effect: It’s not just priming Cognitive, Affective, & Behavioral Neuroscience 5 4 2005 467 472 10.3758/CABN.5.4.467 Van den Bussche et al., 2009 E. Van den Bussche W. Van den Noortgate B. Reynvoet Mechanisms of masked priming: A meta-analysis Psychological Bulletin 135 3 2009 452 477 10.1037/a0015329 van Gaal et al., 2010 S. van Gaal V.A.F. Lamme K.R. Ridderinkhof Unconsciously triggered conflict adaptation PLoS One 5 7 2010 e11508 10.1371/journal.pone.0011508 Van Opstal et al., 2011 F. Van Opstal C.B. Calderon W. Gevers T. Verguts Setting the stage subliminally: Unconscious context effects Consciousness and Cognition 20 4 2011 1860 1864 10.1016/j.concog.2011.09.004 Van Veen et al., 2001 V. Van Veen J.D. Cohen M.M. Botvinick V.A. Stenger C.S. Carter Anterior cingulate cortex, conflict monitoring, and levels of processing Neuroimage 14 6 2001 1302 1308 10.1006/nimg.2001.0923 Wang et al., 2013 B. Wang L. Xiang J. Li Does conflict control occur without awareness? Evidence from an ERP study Brain Research 1490 2013 161 169 10.1016/j.brainres.2012.10.053 Wang et al., 2016 X. Wang X. Zhao G. Xue A. Chen Alertness function of thalamus in conflict adaptation NeuroImage 132 2016 274 282 10.1016/j.neuroimage.2016.02.048 Winkielman et al., 2005 P. Winkielman K.C. Berridge J.L. Wilbarger Unconscious affective reactions to masked happy versus angry faces influence consumption behavior and judgments of value Personality and Social Psychology Bulletin 31 1 2005 121 135 10.1177/0146167204271309 Wu et al., 2015 Q. Wu J.T.H. Lo Voi T.Y. Lee M.-A. Mackie Y. Wu J. Fan Interocular suppression prevents interference in a flanker task Frontiers in Psychology 6 2015 1110 10.3389/fpsyg.2015.01110 Wu et al., 2018 T. Wu A.J. Dufford L.J. Egan M.-A. Mackie C. Chen C. Yuan J. Fan Hick-hyman law is mediated by the cognitive control network in the brain Cerebral Cortex 28 7 2018 2267 2282 10.1093/cercor/bhx127 Yeung et al., 2004 N. Yeung M.M. Botvinick J.D. Cohen The neural basis of error detection: conflict monitoring and the error-related negativity Psychological Review 111 4 2004 931 959 10.1037/0033-295X.111.4.931 Zabelina et al., 2013 D.L. Zabelina E. Guzman-Martinez L. Ortega M. Grabowecky S. Suzuki M. Beeman Suppressed semantic information accelerates analytic problem solving Psychonomic Bulletin & Review 20 3 2013 581 585 10.3758/s13423-012-0364-1 Zernicki, 1987 B. Zernicki Pavlovian orienting reflex Acta Neurobiologiae Experimentalis (Warsz) 47 1987 239 247 Zou et al., 2016 J. Zou S. He P. Zhang Binocular rivalry from invisible patterns Proceedings of the National Academy of Sciences 113 30 2016 8408 8413 10.1073/pnas.1604816113 Zysset et al., 2001 S. Zysset K. Müller G. Lohmann D.Y. von Cramon Color-word matching stroop task: Separating interference and response conflict NeuroImage 13 1 2001 29 36 10.1006/nimg.2000.0665 "
    },
    {
        "doc_title": "Multimodal interaction for accessible smart homes",
        "doc_scopus_id": "85061404918",
        "doc_doi": "10.1145/3218585.3218595",
        "doc_eid": "2-s2.0-85061404918",
        "doc_date": "2018-06-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Accessibility",
            "Context-Aware",
            "Design for all",
            "Multi-Modal Interactions",
            "Multi-platform",
            "Personas",
            "Smart homes",
            "Speech interface"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery.Nowadays, houses are being equipped with new smart products and smart sensors, from multiple manufacturers, each offering their own options for interaction with providing varying degrees of usability and user experience. This diverse nature of smart homes and buildings, in general, poses new challenges to interaction design and accessibility. To tackle them, human-building interaction needs to move from articulating different interactive artifacts towards an holistic view of the house as an interactive ecosystem. In a joint effort with Bosch Termotecnologia, S.A., and profiting from recent contributions including an architecture and framework supporting multimodal Interaction, the authors aim to explore novel ways of approaching interaction design with a smart house and proposing smart home applications for all. This paper presents the status of this ongoing work, in the scope of project Smart Green Homes, proposing how multimodal interaction can be supported in the scenario of a smart home and showing first results of tackling human-building interaction through a home assistant serving a family in their daily interactions with the house.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "EmotionalIy-aware multimodal interfaces: Preliminary work on a generic affective modality",
        "doc_scopus_id": "85061374910",
        "doc_doi": "10.1145/3218585.3218589",
        "doc_eid": "2-s2.0-85061374910",
        "doc_date": "2018-06-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Affective Computing",
            "Affective interaction",
            "Multi-Modal Interactions",
            "Multi-modal interfaces",
            "Multi-platform",
            "Multimodal interactive systems",
            "Natural interactions",
            "Straight-forward method"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery.In interactive systems, knowing the user's emotional state is not only important to understand and improve overall user experience, but also of the utmost relevance in scenarios where such information might foster our ability to help users manage and express their emotions (e.g., anxiety), with a strong impact on their daily life and on how they interact with others. Nevertheless, although there is a clear potential for emotionally-aware applications, several challenges preclude their wider availability, sometimes resulting from the low translational nature of the research in affective computing methods, and from a lack of straightforward methods for easy integration of emotion in applications. In light of these challenges, we propose a conceptual vision for the consideration of emotion in the scope of multimodal interactive systems, and how it can articulate with research in affective computing. Aligned with this vision, a first instantiation of an affective generic modality is presented, and a proof-of-concept application, enabling multimodal interaction with Spotify, illustrates how the modality can provide emotional context in interactive scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Conversational assistant for an accessible smart home: Proof-of-concept for Portuguese",
        "doc_scopus_id": "85061370847",
        "doc_doi": "10.1145/3218585.3218594",
        "doc_eid": "2-s2.0-85061370847",
        "doc_date": "2018-06-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Accessibility",
            "Conversational Assistants",
            "Design for all",
            "Smart homes",
            "Spoken interaction"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery.There is a continued increase in the integration of intelligent devices in our homes. Making these new complex ecosystems accessible poses great challenges, stemming from the very nature of the Home (several spaces separated by walls), the diversity of inhabitants (e.g., children, adults, older adults, persons with temporary or permanent impairments), the tasks they perform (e.g., cooking) or their native languages. In this context, an assistant capable of spoken and written conversation with the inhabitants can be a valuable help in making a household more accessible for several groups of persons, if not for all. This paper presents information regarding the development and first results of such an assistant targeting an Accessible Smart Home for a Portuguese speaking family with some accessibility needs.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Geriatric helper: An mhealth application to support comprehensive geriatric assessment",
        "doc_scopus_id": "85045956017",
        "doc_doi": "10.3390/s18041285",
        "doc_eid": "2-s2.0-85045956017",
        "doc_date": "2018-04-22",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Clinical settings",
            "Domain experts",
            "Geriatric assessments",
            "Geriatric cares",
            "Health care professionals",
            "Health professionals",
            "Mobile applications",
            "Positive impacts",
            "Aged",
            "Delivery of Health Care",
            "Geriatric Assessment",
            "Humans",
            "Mobile Applications",
            "Telemedicine"
        ],
        "doc_abstract": "© 2018 by the authors. Licensee MDPI, Basel, Switzerland.The Comprehensive Geriatric Assessment (CGA) is a multidisciplinary diagnosis approach that considers several dimensions of fragility in older adults to develop an individualized plan to improve their overall health. Despite the evidence of its positive impact, CGA is still applied by a reduced number of professionals in geriatric care in many countries, mostly using a paper-based approach. In this context, we collaborate with clinicians to bring CGA to the attention of more healthcare professionals and to enable its easier application in clinical settings by proposing a mobile application, Geriatric Helper, to act as a pocket guide that is easy to update remotely with up-to-date information, and that acts as a tool for conducting CGA. This approach reduces the time spent on retrieving the scales documentation, the overhead of calculating the results, and works as a source of information for non-specialists. Geriatric Helper is a tool for the health professionals developed considering an iterative, User-Centred Design approach, with extensive contributions from a broad set of users including domain experts, resulting in a highly usable and accepted system. Geriatric Helper is currently being tested in Portuguese healthcare units allowing for any clinician to apply the otherwise experts-limited geriatric assessment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GeriatricHelper: Iterative development of a mobile application to support geriatric assessment",
        "doc_scopus_id": "85053125375",
        "doc_doi": "10.1007/978-3-319-98551-0_17",
        "doc_eid": "2-s2.0-85053125375",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Android",
            "Clinical assessments",
            "Functional Prototypes",
            "Geriatric assessments",
            "Iterative development",
            "Patient follow-ups",
            "Pocket guide",
            "User-centered design approaches"
        ],
        "doc_abstract": "© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018.Clinical assessment scales for specific medical subareas include domain knowledge that may not be of general awareness among practitioners, hindering the adoption of best practices. In this context, we propose a pocket guide for comprehensive geriatric assessment as a mobile application. The GeriatricHelper is an Android mHealth application developed under an iterative, User-Centered Design approach. Feedback from a broad set of users including domain experts has been obtained throughout and a functional prototype is currently being tested in a Portuguese hospital, allowing for any clinician to apply the otherwise experts-limited geriatric assessment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and development of Medication Assistant: older adults centred design to go beyond simple medication reminders",
        "doc_scopus_id": "84978174530",
        "doc_doi": "10.1007/s10209-016-0487-7",
        "doc_eid": "2-s2.0-84978174530",
        "doc_date": "2017-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Design and Development",
            "Easy-to-use products",
            "Interaction design",
            "Iterative development",
            "Medication management",
            "Medication non-adherence",
            "Mobile applications",
            "Older adults"
        ],
        "doc_abstract": "© 2016, Springer-Verlag Berlin Heidelberg.Older adults have much to gain from bringing technology into their daily lives. The extent to which this is possible strongly depends on careful design and accessible, easy-to-use products, developed using an older adults centred methodology. This paper follows this design approach and puts it to the test in developing “Medication Assistant”, an application aimed to contribute to lower the high levels of non-adherence to medication in the ageing population. This application is developed following an iterative method centred on the older adults and interaction design. The method repeats short development cycles encompassing the definition of scenarios and goals, requirements engineering, design, prototyping and evaluation by the target users. The evaluation of the increasingly refined prototypes is of paramount importance in this methodology, gathering information about the strengths and weaknesses of the application. These, along with user suggestions, constitute an important starting point to support further improvements in the subsequent development cycle. The first three development cycles for “Medication Assistant” are presented, highlighting the main aspects of each stage, and how the evaluation performed, at the end of each cycle, provided feedback to further refine the application with new and improved features. At its current stage, “Medication Assistant” obtained very positive evaluation outcomes and already provides a set of useful features concerning medication management. These features go beyond the typical medication reminders and aim to provide a first contribution towards a more holistic approach to medication non-adherence.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Beware the serpent: the advantage of ecologically-relevant stimuli in accessing visual awareness",
        "doc_scopus_id": "85006918209",
        "doc_doi": "10.1016/j.evolhumbehav.2016.10.004",
        "doc_eid": "2-s2.0-85006918209",
        "doc_date": "2017-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Ecology, Evolution, Behavior and Systematics",
                "area_abbreviation": "AGRI",
                "area_code": "1105"
            },
            {
                "area_name": "Experimental and Cognitive Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3205"
            },
            {
                "area_name": "Arts and Humanities (miscellaneous)",
                "area_abbreviation": "ARTS",
                "area_code": "1201"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2016 Elsevier Inc.Snakes and spiders constitute fear-relevant stimuli for humans, as many species have deleterious and even fatal effects. However, snakes provoked an older and thus stronger evolutionary pressure than spiders, shaping the vision of earliest primates toward preferential visual processing, mainly in the most complex perceptual conditions. To the best of our knowledge, no study has yet directly assessed the role of ecologically-relevant stimuli in preferentially accessing visual awareness. Using continuous flash suppression (CFS), the present study assessed the role of evolutionary pressure in gaining a preferential access to visual awareness. For this purpose, we measured the time needed for three types of stimuli - snakes, spiders (matched with snakes for rated fear levels, but for which an influence on humans but not other primates is well grounded) and birds - to break the suppression and enter visual awareness in two different suppression intensity conditions. The results showed that in the less demanding awareness access condition (stimuli presented to the participants' dominant eye) both evolutionarily relevant stimuli (snakes and spiders) showed a faster entry into visual awareness than birds, whereas in the most demanding awareness access condition (stimuli presented to the participants' non-dominant eye) only snakes showed this privileged access. Our data suggest that the privileged unconscious processing of snakes in the most complex perceptual conditions extends to visual awareness, corroborating the proposed influence of snakes in primate visual evolution.",
        "available": true,
        "clean_text": "serial JL 271894 291210 291726 291730 291805 291833 291840 31 Evolution and Human Behavior EVOLUTIONHUMANBEHAVIOR 2016-10-22 2016-10-22 2017-02-10 2017-02-10 2017-02-10T15:12:08 S1090-5138(16)30287-2 S1090513816302872 10.1016/j.evolhumbehav.2016.10.004 S300 S300.1 FULL-TEXT 2017-02-10T15:30:33.246803-05:00 0 0 20170301 20170331 2017 2016-10-22T15:07:38.04922Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure table body affil articletitle auth authfirstini authfull authkeywords authlast primabst pubtype ref 1090-5138 10905138 true 38 38 2 2 Volume 38, Issue 2 10 227 234 227 234 201703 March 2017 2017-03-01 2017-03-31 2017 Original Articles article fla © 2016 Elsevier Inc. All rights reserved. BEWARESERPENTADVANTAGEECOLOGICALLYRELEVANTSTIMULIINACCESSINGVISUALAWARENESS GOMES N 1 Introduction 2 Methods 2.1 Participants 2.2 Stimuli 2.3 CFS masks 2.4 Display 2.5 Procedure 2.6 Statistical analysis 3 Results 4 Discussion References AGRAS 1969 151 156 S ALMEIDA 2010 772 778 J ALMEIDA 2008 15214 15218 J ALMEIDA 2014 319 335 J ALMEIDA 2013 94 101 J ALMEIDA 2015 1 21 I AREND 2008 I BAHRAMI 2008 12.1 1210 B BAHRAMI 2007 509 513 B CARLSSON 2004 340 353 K CASAGRANDE 1994 V CASAGRANDE 2004 494 506 V VISUALNEUROSCIENCES PARALLELVISUALPATHWAYSACOMPARATIVEPERSPECTIVE EMERY 2000 156 191 N COGNITIVENEUROSCIENCEEMOTION ROLEAMYGDALAINPRIMATESOCIALCOGNITION FEINSTEIN 2011 34 38 J GERDES 2009 66 73 A GOODALE 1992 20 25 M GOODALE 2004 M SIGHTUNSEENEXPLORATIONCONSCIOUSUNCONSCIOUSVISION HANDA 2004 377 383 T HONGSHEN 2014 1049 1053 H ISBELL 2006 1 35 L ISBELL 2009 L FRUITTREESERPENT JENSEN 2011 529 546 M JIANG 2006 2023 2029 Y JIANG 2006 17048 17052 Y JIANG 2007 349 355 Y KALIN 2004 5506 5515 N KANAI 2006 2332 2336 R KASTURIRATNE 2008 e218 A KAUNITZ 2011 10 L KAUNITZ 2011 L KLUVER 1939 979 1000 H KNUDSEN 2007 57 78 E LANG 1997 P NIMHCENTERFORSTUDYEMOTIONATTENTION INTERNATIONALAFFECTIVEPICTURESYSTEMIAPSTECHNICALMANUALAFFECTIVERATINGS LANG 2005 P TECHNICALREPORTA6CENTERFORRESEARCHINPSYCHOPHYSIOLOGYUNIVERSITYFLORIDA INTERNATIONALAFFECTIVEPICTURESYSTEMIAPSINSTRUCTIONSMANUALAFFECTIVERATINGS LE 2016 20595 Q LE 2013 19000 19005 Q LEDOUX 1996 J EMOTIONALBRAINMYSTERIOUSUNDERPINNINGSEMOTIONALLIFE LEVENTHAL 1985 216 226 A LIDDELL 2005 235 243 B LIN 2009 195 211 Z LIVINGSTONE 1988 740 749 M LOBUE 2008 284 V LOBUE 2010 375 379 V MACHADO 2009 147 163 C MAIOR 2011 257 260 R MAYER 2006 510 519 B MERIGAN 1993 369 402 W MILES 1930 412 430 W MILNER 1993 317 337 A MILNER 2008 774 785 A MINEKA 1980 S MORRIS 1999 1680 1685 J NEW 2015 165 173 J NYFFELER 1999 317 324 M OHMAN 2005 953 958 A OHMAN 2001 483 522 A OHMAN 1994 231 240 A OHMAN 2007 180 185 A OHMAN 2001 466 478 A PASLEY 2004 163 172 B PEIRA 2010 470 475 N PELI 1990 2032 2040 E PENKUNAS 2013 463 475 M PENKUNAS 2013 522 536 M PESSOA 2010 773 783 L PHELPS 2005 E PORAC 1976 880 897 C PRATHER 2001 653 658 M ROTHKIRCH 2012 M SCHILLER 1978 788 797 P SCHILLER 1979 1124 1133 P SCHMACK 2015 1 8 K SHIBASAKI 2009 131 135 M SHIN 2009 1507 1513 K SOARES 2012 187 197 S SOARES 2013 11 16 S SOARES 2009 1032 1042 S SOARES 2014 e114724 S STEEN 2004 819 842 l C STEIN 2011 307 311 T STEIN 2014 566 574 T STERZER 2011 1615 1624 P STERZER 2014 1 12 P TAMIETTO 2010 697 709 M TROIANI 2013 241 V TROIANI 2012 133 140 V TSUCHIYA 2005 1096 1101 N VAKALOPOULOS 2005 1183 1190 C VALLEINCLAN 2008 55 64 F VANSTRIEN 2014 150 157 J VANSTRIEN 2014 1 9 J VUILLEUMIER 2003 624 631 P VUILLEUMIER 2004 1271 1278 P WATANABE 2011 829 831 M WIENS 2006 675 680 S WILLENBOCKEL 2012 1 12 V WILLIAMS 2004 2898 2904 M XU 2011 2048 2056 S YANG 2014 1 17 E YANG 2010 24 E YANG 2007 882 886 E YANG 2011 Z YORZINSKI 2014 534 548 J YUVALGREENBERG 2013 9635 9643 S ZHAN 2015 1 13 M GOMESX2017X227 GOMESX2017X227X234 GOMESX2017X227XN GOMESX2017X227X234XN 2018-02-10T00:00:00.000Z UnderEmbargo © 2016 Elsevier Inc. All rights reserved. item S1090-5138(16)30287-2 S1090513816302872 10.1016/j.evolhumbehav.2016.10.004 271894 2017-02-10T11:33:24.434433-05:00 2017-03-01 2017-03-31 true 623461 MAIN 8 61407 849 656 IMAGE-WEB-PDF 1 gr1 9335 52 219 gr2 17227 164 213 gr3 12625 57 219 gr4 12690 125 219 gr1 22038 132 559 gr2 25849 273 354 gr3 51593 197 758 gr4 22579 221 387 gr1 73049 351 1485 gr2 98950 724 940 gr3 477647 872 3356 gr4 74938 587 1028 ENS 6082 S1090-5138(16)30287-2 10.1016/j.evolhumbehav.2016.10.004 Elsevier Inc. Fig. 1 Four examples of each stimuli category (snakes, spiders, birds) used in the experiment. Fig. 1 Fig. 2 Schematic example of the stimuli presentation. Participants were presented with an 8°×8° frame with a 0.5° border presenting a white noise pattern. The stimuli were presented in one of the frame quadrants, centered at a horizontal and vertical distance of 1.9 degrees relative to a white fixation cross located at the center of the frame. Fig. 2 Fig. 3 Schematic example of a trial. The stimuli were gradually introduced to the right eye, while CFS masks (Mondrian-like pattern blinking at 10 Hz) were presented to the left eye. Participants identified, as quickly as possible, the quadrant in which the stimulus became visible. Fig. 3 Fig. 4 Mean response times (RTs) to access visual awareness in milliseconds (ms) for the three animal stimuli in the two suppression conditions. ** indicates p<0.01; *** indicates p<0.001. Fig. 4 Table 1 Means and standard deviations (in parenthesis) of: (i) spatial frequency bands (ii) luminosity, and (iii) subjective ratings of the stimuli. Table 1 Spider Bird Snake Spatial frequency bands 768–384 (cpp) 9.36*101 (3.41*101) 9.72*101 (5.44*101) 9.64*101 (4.26*101) 384–192 (cpp) 9.03*102 (1.43*102) 7.46*102 (3.12*102) 8.12*102 (2.19*102) 192–96 (cpp) 6.65*103 (1.34*103) 5.15*103 (1.83*103) 5.22*103 (1.36*103) 96–48 (cpp) 4.33*104 (1.17*104) 3.50*104 (0.98*104) 3.42*104 (0.95*104) 48–24 (cpp) 2.73*105 (0.58*105) 2.24*105 (0.54*105) 2.27*105 (0.75*105) 24–12 (cpp) 1.32*106 (0.28*106) 1.40*106 (0.36*106) 1.46*106 (0.59*106) 12–6 (cpp) 6.29*106 (1.55*106) 7.28*106 (2.60*106) 8.38*106 (3.33*106) 6–3 (cpp) 2.52*107 (0.95*107) 4.66*107 (1.05*107) 3.25*107 (3.33*107) Residuals (cpp) 3.70*109 (0.09*109) 3.67*109 (0.08*109) 3.58*109 (2.37*109) Luminosity (0=black to 255=white) 235 (4) 234 (4) 231 (9) Subjective ratings of stimuli Valence (1=negative to 9=positive) 2.87 (1.40) 6.57 (1.45) 3.48 (1.53) Arousal (1=calming to 9=arousing) 5.34 (2.24) 2.74 (1.62) 5.21 (2.12) Note: Luminosity corresponds to the average luminosity of each picture. Spatial frequency was measured as the energy in eight frequency bands, expressed in cycles per picture (cpp), plus residuals (size of images was 1024×768 pixels in all cases). Original Article Beware the serpent: the advantage of ecologically-relevant stimuli in accessing visual awareness Nuno Gomes a b Samuel Silva c Carlos F. Silva d Sandra C. Soares b d e ⁎ a Portuguese Catholic University, Institute of Health Siences, Lisbon, Portugal Portuguese Catholic University Institute of Health Siences Lisbon Portugal b William James Center for Research, ISPA‐Instituto Universitário, Lisbon, Portugal William James Center for Research ISPA‐Instituto Universitário Lisbon Portugal c Department of Electronics, Telecommunication and Informatics (DETI)/Institute of Electronics and Informatics Engineering (IEETA), University of Aveiro, Portugal Department of Electronics, Telecommunication and Informatics (DETI)/Institute of Electronics and Informatics Engineering (IEETA) University of Aveiro Portugal d CINTESIS.UA, Department of Education and Psychology, University of Aveiro, Aveiro, Portugal CINTESIS.UA, Department of Education and Psychology University of Aveiro Aveiro Portugal e Department of Clinical Neuroscience, Division for Psychology, Karolinska Institutet, Sweden Department of Clinical Neuroscience, Division for Psychology Karolinska Institutet Sweden ⁎ Corresponding author. University of Aveiro, Department of Education and Psychology, Campus Universitário de Santiago, 3810-193, Aveiro, Portugal. University of Aveiro Department of Education and Psychology, Campus Universitário de Santiago Aveiro 3810-193 Portugal Snakes and spiders constitute fear-relevant stimuli for humans, as many species have deleterious and even fatal effects. However, snakes provoked an older and thus stronger evolutionary pressure than spiders, shaping the vision of earliest primates toward preferential visual processing, mainly in the most complex perceptual conditions. To the best of our knowledge, no study has yet directly assessed the role of ecologically-relevant stimuli in preferentially accessing visual awareness. Using continuous flash suppression (CFS), the present study assessed the role of evolutionary pressure in gaining a preferential access to visual awareness. For this purpose, we measured the time needed for three types of stimuli - snakes, spiders (matched with snakes for rated fear levels, but for which an influence on humans but not other primates is well grounded) and birds - to break the suppression and enter visual awareness in two different suppression intensity conditions. The results showed that in the less demanding awareness access condition (stimuli presented to the participants' dominant eye) both evolutionarily relevant stimuli (snakes and spiders) showed a faster entry into visual awareness than birds, whereas in the most demanding awareness access condition (stimuli presented to the participants' non-dominant eye) only snakes showed this privileged access. Our data suggest that the privileged unconscious processing of snakes in the most complex perceptual conditions extends to visual awareness, corroborating the proposed influence of snakes in primate visual evolution. Keywords Evolution Fear Snake detection theory Continuous flashing suppression 1 Introduction Evolution has equipped humans with a readiness to associate fear with situations that threatened the survival of their ancestors, with potentially deadly predators being a prime example. According to the Snake Detection Theory (SDT; Isbell, 2009) snakes may represent an archetypal fear stimulus. The SDT posits that primates (including humans) have been shaped, by evolutionary arms races, to fear and avoid snakes over evolutionary time (starting about 90–80 million years ago). Isbell (2006, 2009, for in-depth reviews) argues that the selection pressures posed by snakes, as well as the common fear of snakes in humans (Agras, Sylvester, & Oliveau, 1969) and in other primates (Mineka, Keir, & Price, 1980), have favored the origin of primates via changes in the visual system that enabled them to detect and avoid dangerous snakes. Accordingly, several recent studies have provided neurobehavioral evidence for a preferential snake processing in primates. Le et al. (2013), for instance, have shown that neurons in the medial and dorsolateral pulvinar of Japanese monkeys (Macaca fuscata) exhibit faster and stronger responses to snake images (compared with images of faces, hands of monkeys, or simple geometric shapes). In a further study with macaques, Le et al. (2016) found that snakes, again compared with images of faces and hands of monkeys, elicited earlier gamma oscillations (involved in feedforward visual information processing), in macaque pulvinar neurons, confirming that primates can detect snakes very rapidly. Preferential processing of snakes, compared to other stimuli, such as flowers, mushrooms, and other animal stimuli, has also been shown in several visual search tasks in rhesus monkeys (Shibasaki & Kawai, 2009), human children (LoBue & DeLoache, 2008; LoBue, Rakison, & DeLoache, 2010; Penkunas & Coss, 2013a, 2013b; Yorzinski, Penkunas, Platt, & Coss, 2014) and human adults (Öhman, Flykt, & Esteves, 2001; Soares & Esteves, 2013; Soares, Lindström, Esteves & Öhman, 2014; Soares, Esteves, Lundqvist & Öhman, 2009; Soares, 2012). This neurobehavioral evidence with humans and monkeys has provided support for the notion that the undeniable need for an effective predatory defense system tailored a fear module – an independent behavioral, psychophysiological and neural system – that is relatively encapsulated from more advanced human cognition in order to foster a successful development of the defense systems (see Öhman & Mineka, 2001). Although there is evidence that the fear module is selectively sensitive and automatically activated by evolutionary-relevant fear stimuli, the results from most of these studies preclude a direct test of the SDT, since no equivalent animal fear stimuli with distinct evolutionary baggage have been considered for comparison. More recently, however, Soares and her colleagues (Soares, 2012; Soares, Esteves, Lundqvist and Öhman, 2009; Soares & Esteves, 2013; Soares, Lindström, Esteves and Öhman, 2014) proposed spiders as the ideal candidate for humans, based on the premise, derived from the SDT, that selection has favored perceptual abilities to detect snakes more strongly than spiders (Isbell, 2009). Spiders attack other spiders and insects (Nyffeler, 1999) and, unlike poisonous snakes, spiders' poison did not evolve to be effective against mammals (Gerdes, Uhl, & Alpers, 2009). Moreover, unlike snakes, that continue to pose a threat to human life even today (Kasturiratne et al., 2008), only a small amount of spiders have a direct contact with humans and only a few are considered as a cause of morbidity or mortality (e.g., Steen, Carbonaro, & Schwartz, 2004). Hence, the perceptual abilities to detect camouflaged snakes have been more consistently selected for among serpents than among arachnids, making the genes promoting defense against snakes more prominent among the former than the latter (Isbell, 2009). Therefore, spiders are the ideal comparison stimuli to test the SDT, because they are also fear-relevant for humans, compared to snakes, but have a distinct evolutionary baggage. Moreover, snake and spider stimuli are matched for fear levels in humans (Lang, Bradley, & Cuthbert, 2005) and are both highly frequent objects of phobias (e.g., Agras et al., 1969). Following this premise, a growing body of behavioral (e.g., faster detection in visual search settings) and electrophysiological data (maximal amplitudes in specific early attention-related brain potentials; P1 and EPN) has now provided more direct evidence in favor of snakes' preferential processing, compared to spiders and innocuous animal stimuli (other reptiles, insects, birds, and slugs) (Hongshen, Kenta, & Nobuyuki, 2014; Soares, Kessel, Hernández-Lorca, García-Rubio, Rodrigues, Gomes, Carretié, submitted; Van Strien, Eijlers, Franken, & Huijding, 2014; Van Strien, Franken, & Huijding, 2014). More importantly, and conforming to the predictions of the SDT (Isbell, 2009), snake preferential processing has been observed particularly under conditions that may have been critical for survival, such as those involved in taxing visual conditions, such as peripheral visual field (Soares, Lindström, Esteves and Öhman, 2014), brief exposure durations (Soares & Esteves, 2013; Soares, Lindström, Esteves and Öhman, 2014), and a more cluttered environment (Soares, 2012; Soares & Esteves, 2013; Soares, Esteves, Lundqvist and Öhman, 2009; Soares, Lindström, Esteves and Öhman, 2014). As proposed by Öhman and Mineka (2001), the rapid and efficient processing of evolutionary-relevant stimuli by the fear module may occur without the need for conscious processing before a response is elicited, most likely due to a dedicated neural circuitry, centered in the amygdala, that bypasses the visual cortex (e.g., Phelps & LeDoux, 2005; but see Pessoa & Adolphs, 2010). Although some studies have shown that such stimuli are processed preferentially outside of awareness, researchers were targeting the neurobehavioral responses of phobic participants, with no interest in showing dissociations between snake and spider stimuli (Carlsson et al., 2004; Öhman & Soares, 1994). Moreover, the authors have mainly used the backward masking (BM) paradigm to render the stimuli under unconscious awareness for a limited time frame (<40 ms) (see Wiens, 2006), and without examining whether the fear stimuli hold an advantage in entering into visual awareness. Recently, interest in how emotional (fear) stimuli are processed under unawareness has grown, partly due to the emergence of interocular suppression techniques, such as the continuous flash suppression (CFS; Tsuchiya & Koch, 2005). This technique allows stronger and more time enduring states of unawareness (around ten times longer than BM) due to the suppression of static images by dynamic noise. Several studies have demonstrated that threatening stimuli, such as fearful faces (Stein, Seymour, Hebart, & Sterzer, 2014; Sterzer, Hilgenfeldt, Freudenberg, Bermpohl, & Adli, 2011; Tsuchiya, Moradi, Felsen, & Yamazaki, 2009; Yang, Zald, & Blake, 2007), faces with a direct gaze (Stein, Senju, Peelen, & Sterzer, 2011), angry body postures (Zhan, Hortensius, & De Gelder, 2015), and spiders (Schmack, Burk, Haynes, & Sterzer, 2015), emerge faster into awareness (breaking-CFS; Jiang, Costello, & He, 2007) than neutral stimuli. In this context, it is worth noting that these previous studies with CFS showing that threat-related stimuli gain a preferential access to visual awareness, have mostly considered social stimuli, i.e., differences in facial expression and bodily posture. However, as we have discussed above, ecological stimuli are also important. To the best of our knowledge no study has yet directly investigated the role of ecologically relevant fear stimuli in accessing awareness, comparing stimuli with and without such relevance. Although Schmack et al. (2015) have used spiders, the authors were only interested in studying the phobic characteristics of the stimulus, thus not attending to their evolutionary relevance. Accordingly, studies using other methodologies aiming at testing the access to visual awareness, such as change blindness and intentional blindness (for a review see Jensen, Yao, Street, & Simons, 2011), have evidenced that spiders are detected, located, and identified by a higher percentage of observers, both by participants with a specific phobia to the stimulus (Peira, Golkar, Larsson, & Wiens, 2010), and by participants with no such phobia (Mayer, Muris, Vogel, Nojoredjo, & Merckelbach, 2006; New & German, 2015;). However, and as in the study by Schmack et al. (2015), none of these studies were interested in studying the role of the evolutionary relevance of the stimulus in entering visual awareness. In the present study, we used CFS to investigate whether snakes overcame suppression and entered into awareness faster than spiders (compared to birds, an innocuous animal stimulus) in humans. Based on previous results showing preferential processing of evolutionarily relevant stimuli by the fear module, the first prediction of this study was that both snakes and spiders (when compared with birds) would have an advantage in entering into visual awareness (reflected in faster reaction times, RTs). Furthermore, and since no study has yet directly investigated the role of ecological stimuli in gaining preferential access to visual awareness, as mentioned earlier, we directly compared two stimuli with distinctly different evolutionary relevance for primates - snakes and spiders. Inspired by the SDT, (Isbell, 2009) and based on previous findings showing a facilitated processing of snakes (compared to spiders and neutral stimulus) under the most perceptually demanding conditions (e.g., Soares, Lindström, Esteves and Öhman, 2014) we considered, as our second prediction, that snakes would have an advantage in entering into awareness (reflected in faster RTs), compared to spiders (and innocuous animals, birds) in the most complex perceptual condition. In order to create two distinct perceptual complexity conditions during CFS, we divided participants based on their ocular dominance. The concept of ocular dominance (see Porac & Coren, 1976) refers to an evident monocular processing preference when the images viewed by the two eyes cannot be merged, such as in a dichotic stimulation condition (Valle-Inclán, Blanco, Soto, & Leirós, 2008). Data from studies that use binocular rivalry paradigms (also an interocular suppression technique) have shown that a stimulus presented to the dominant eye (assessed with sight dominance tests, such as Miles' test; see Miles, 1930) was visible for longer periods and was detected with higher accuracy than a stimulus presented to the non-dominant eye (e.g., Handa et al., 2004; Valle-Inclán et al., 2008). These data suggest a preference for processing stimuli when these are presented to the dominant eye over stimuli presented to the non-dominant eye. Therefore, during CFS, presenting the stimulus to the dominant eye or to the non-dominant eye of the participant may represent different conditions of suppression, with the latter being a more demanding stimulus detection condition. Thus, we predict that snakes will have an advantage in entering into visual awareness (reflected in faster RTs) in the most demanding suppression condition (i.e., when stimuli are presented to the non-dominant eye) compared to spiders, for which evolutionary pressures were weaker (and innocuous animals, birds) (see Soares, Lindström, Esteves and Öhman, 2014). However, in the less demanding suppression condition (i.e., when stimuli are presented to the dominant eye), although we expect both snakes and spiders to have an advantage in entering into awareness (when compared with birds), no differences are expected between the two, as they are both fear-relevant stimuli for humans (e.g., Agras et al., 1969). 2 Methods 2.1 Participants Sixty-one university students (forty-six women), aged between 17 and 42 (M=21.64, SD=4.16), participated voluntarily in the experiment after informed consent. Participants were screened for ocular dominance, revealing 32 participants with right dominance (23 women), aged 18 to 35 (M=22.03; SD=3.90), and 29 participants with left dominance (22 women), aged 17 to 42 (M=21.21, SD=4.45). All participants reported normal, or corrected to normal eyesight, no psychiatric medication intake, and no registered or observed symptoms of mental illness. The study was approved by the local ethics committee. Moreover, the standards of the American Psychological Association and the guidelines of the Declaration of Helsinki were followed. 2.2 Stimuli Stimuli encompassed 10 images of snakes, 10 images of spiders and 10 images of birds (see Fig. 1 for an illustration). For snakes and spiders, grayscale images were chosen from those used in Soares, Lindström, Esteves and Öhman (2014), Experiment 4). The bird images were selected from the Internet and converted to grayscale. To rule out the role of the low level features of the stimuli on the results, spatial frequency energy across stimuli was computed (Soares, Lindström, Esteves and Öhman, 2014) along with the mean stimulus intensity (luminance) (Peli, 1990). Separate one-way ANOVAs for spatial frequency bands and luminance revealed no statistically significant differences (p>0.05 in all cases) (Table 1 ). Sixty-five volunteer university students (44 women), aged 18 to 48 (M=21.20; SD=4.09) took part in a pilot study to validate the stimuli to be used in the experimental task. These participants were asked to rate the 30 images, using the valence and arousal scales of the Self-Assessment Manikin (SAM) (Lang, Bradley, & Cuthbert, 1997). The results of the one way ANOVAs showed statistically significant effects in both valence [F(2, 128)=129.48, p<.0001, η2p=.67] and arousal [F(2, 128)=67.89, p<.0001, η2p=.52]. Bonferroni corrected post-hoc comparisons showed that both spiders and snakes were rated as more unpleasant and arousing than birds and that spiders were rated as more unpleasant and arousing than snakes (p<.01) (Table 1). 2.3 CFS masks For building CFS, several Mondrian patterns were generated, composed of randomly arranged greyscale circles with diameters between 0.39° and 1.4°, and animated at 10 Hz. In order to enable CFS using “red-blue anaglyph glasses” (e.g., Almeida, Mahon, & Caramazza, 2010; Almeida, Mahon, Nakayama, & Caramazza, 2008; Almeida, Pajtas, Mahon, Nakayama, & Caramazza, 2013; Almeida et al., 2014; Kaunitz et al., 2011; Troiani, Price, & Schultz, 2012) the stimuli were presented using the blue RGB channel and the CFS masks using the red RGB channel. Therefore, even though the stimulus and masks overlapped, each eye of the participant was only able to see the part of the experiment shown in the same color as the correspondent lens of the “red-blue anaglyph glasses”. In this case, the stimuli were always presented to the right eye while the mask was always shown to the left eye of the participant. 2.4 Display Each participant was presented with an 8°×8° frame with a 0.5° border presenting a white noise pattern inside of which the mask and the stimulus overlapped. The stimuli were presented in one of the frame quadrants, centered at a horizontal and vertical distance of 1.9 degrees relative to a white fixation cross located at the center of the frame. All stimuli were presented randomly, appearing twice in each quadrant (see Fig. 2 for an illustration). In some studies with facial stimuli (e.g., Stein et al., 2014), the authors used the mean luminance values of the images as the frame background and their procedure was meant to make the face image blend with the frame to avoid a detection by quadrant contrast. Since we included images of a different nature, we identified two main challenges: 1) while the face images in the studies with facial stimuli (e.g. Stein et al., 2014) were trimmed to occupy the whole area, our images necessarily have a distinct background; and 2) the face images have regions that stand out, for their different luminance, in the eyes and mouth, while our images are more uniform. Considering these differences, we could not set the background to the mean luminance since it would make detecting the images very difficult, as confirmed by preliminary experiments. Therefore, we followed a slightly different procedure: we empirically defined a fixed background value, set at 115, considering that it should be close to the mean luminance of the images, but slightly lower to allow better detection. This procedure was applied consistently to all stimuli. Stimuli presentation and data collection were performed on computer with a Dell Professional P2212H monitor 21.5-inch LED VGA (1920x1080) using a custom software developed for this experiment. 2.5 Procedure After evaluating ocular dominance using the Miles's test (Miles, 1930), the participant's position was adjusted to ensure that the head was 50 cm away from the screen center. Each trial (see Fig. 3 for an illustration) started with a 1 s presentation of a blank frame with the white fixation cross only; next, the stimulus was introduced, in one of the four quadrants, by ramping up its contrast over 1.1 s; during this ramping, CFS masks were shown at 10 Hz. After 1.1 s, CFS mask contrast was ramped down over a period of 4 s. Trials ended with the participant's response or after 7 s. To achieve an objective measure of perceptual awareness, we considered a forced choice paradigm (e.g., Stein et al., 2014; Yang et al., 2007). Participants were instructed to identify, as quickly and accurately as possible, in which quadrant a stimulus or any part of a stimulus became visible. This was accomplished by pressing one of four keys on a QWERTY keyboard corresponding to the four quadrants (“keys ‘F′ and ‘V′, with their left hand, for the 2nd and 3rd quadrants, and keys ‘J’ and ‘N′, with their right hand, for the 1st and 4th), and the response times were recorded. The experiment started with a training session consisting of 30 trials randomly selected from the full trial set for the experiment, followed by the main experiment, consisting of 240 trials (30 stimuli × 4 quadrants × 2 repetitions), with three mandatory breaks (one every 60 trials). The average total duration of the experimental procedure was 45min. 2.6 Statistical analysis Trials with no response or incorrect responses (<10%) were excluded from the data analyses. The mean reaction times were compared in a mixed 3×2 ANOVA factorial design with the animal category (spider, snake, bird) as a within-participants factor and the ocular dominance (right dominance corresponding the less demanding suppression condition and left dominance corresponding to the most demanding suppression condition) as a between-participants factor. For the repeated measures effect we used the Greenhouse–Geisser correction to correct the degrees of freedom. We performed post-hoc comparisons, using the Bonferroni correction procedure, to determine the significance of pairwise contrasts. As a measure of effect size of ANOVAs, we reported the partial eta square (η2p). 3 Results Conforming to our first prediction, the results showed a significant main effect of animal stimuli [F(2, 118)=24.43, p <0.001, η2p =0.29], with snakes and spiders (M =3927.50 ms; SD =982.70, and M =3918.58 ms; SD =1022.98, respectively) showing faster access to visual awareness than birds (M =4099.46 ms; SD =1006.65), as confirmed by Bonferroni post-hoc comparisons (p <0.001). No statistically significant differences were found between snakes and spiders (p =1.000). The results also showed a significant interaction between the animal stimuli and ocular dominance [F(2, 118)=4.48, p <0.05, η2p =0.07]. Again conforming to our prediction, the results showed that when participants were presented with the stimuli in their dominant eye, snakes and spiders (M =3750.14 ms; SD =983.22, and M =3706.01 ms; SD=994.76, respectively) accessed visual awareness faster than birds (M =3968.11 ms SD =1027.69), as confirmed by Bonferroni post-hoc comparisons (p <0.001). As predicted, no significant differences were found between snakes and spiders in this condition (p =0.816). Importantly, however, in the non-dominant eye group, snakes (M =4123.21 ms; SD=960.94) showed faster access to visual awareness than birds (M =4244.40 ms SD =980.14), as shown by Bonferroni post-hoc comparisons (p <0.01). No statistically significant differences were found between snakes and spiders (p =1.000) or between spiders and birds (p =0.181) for this condition (see Fig. 4 ). No significant main effect of ocular dominance was revealed [F(1, 59)=2.09, p =0.154, η2p =0.03]. 4 Discussion In the present study we assessed the average reaction times for snakes and spiders (stimuli with different histories as dangerous stimuli to primates) in entering into awareness (compared to an innocuous animal stimulus, birds), across two different suppression conditions. Confirming our first prediction, the results showed an advantage of emotional stimuli in general (snakes and spiders vs birds), corroborating the evidence that emotional stimuli (e.g., fearful faces) gain preferential access to visual awareness during CFS (Stein et al., 2011, 2014; Sterzer et al., 2011; Tsuchiya et al., 2009; Yang et al., 2007; Zhan et al., 2015). Our results also extend recent data obtained by Schmack et al. (2015), showing that the advantage of emotional stimuli in entering into awareness is also observed for ecological stimuli, such as spiders. However, in contrast to the aim of the current study, Schmack et al. (2015) were only interested in studying individual differences in spider fear, as previously mentioned, thus neglecting the role of other evolutionary-relevant selective pressures in this advantage. Our results also confirmed the second prediction, while showing differences between snakes and spiders (compared to birds) in gained preferential access to awareness depending on the level of suppression. More specifically, when the stimulus was presented to the participant's dominant eye (the less demanding suppression condition), the two evolutionarily fear-relevant stimuli (snakes and spiders) showed faster access to awareness than birds. However, when the stimulus was presented to the participant's non-dominant eye (the more demanding suppression condition), only snakes gained preferential access to visual awareness (compared to birds). These data constitute the first direct evidence for the role of ecological stimuli in gaining preferential access to visual awareness, consistent with the proposed privileged role of snakes in primate visual evolution (Isbell, 2009). Following the SDT (Isbell, 2009), the pressure exerted by snakes resulted in adjustments to the mammalian visual system, thus resulting in the evolution of primates with a specialized vision that would enable a better detection of this stimulus (as an anti-predator measure), even in highly taxing perceptual conditions, in which the detection of other classes of stimuli is difficult. Confirming this notion, the results of the present study showed that the snake advantage in entering into awareness was particularly evident under the more demanding awareness access condition (i.e., when the stimulus was present to the participant's non-dominant eye). Although the results did not show a significant main effect of ocular dominance, we did obtain a significant interaction between the animal stimuli and ocular dominance showing a different pattern of response depending on the type of stimulus. These results are in agreement with the results obtained by Soares, Esteves, Lundqvist and Öhman (2009), Soares (2012), Soares & Esteves (2013), Soares, Lindström, Esteves and Öhman (2014)), that showed consistent snake preferential processing in taxing visual conditions (peripheral visual field, brief exposure durations, and a more cluttered environment). These behavioral studies have also been complemented with event-related potential (ERP) data, which suggest that snakes are better attention grabbers than spiders, being more efficient in attracting early visual attention, as reflected in larger early posterior negativity (EPN) amplitudes (Hongshen et al., 2014; Van Strien, Franken and Huijding, 2014; Van Strien et al., 2014). More recent ERP data also showed that snakes are better (compared with spiders and birds) at capturing early exogenous attention (evidenced by significantly larger amplitudes in P1) under high perceptual load conditions, indicating more dependence on bottom-up processes (Soares et al., submitted). The more efficient capture of attention by snakes and its reliance on bottom-up processing, consistently shown in previous studies (e.g., Van Strien et al., 2014), may help explain the results of the present study. Some behavioral studies have shown that certain classes of stimuli (e.g., high arousal images or emotional facial expressions), suppressed by CFS, attract visual attention for their physical location (Jiang, Costello, Fang, Huang, & He, 2006; Rothkirch, Stein, Sekutowicz, & Sterzer, 2012; Xu, Zhang, & Geng, 2011; Yang et al., 2011). Moreover, voluntary allocation of attention to the localization of a suppressed stimulus seems to enhance the degree to which it is processed unconsciously, as confirmed by stronger visual aftereffects (Kanai, Tsuchiya, & Verstraten, 2006; Shin, Stolte, & Chong, 2009; Yang, Hong, & Blake, 2010), while the deliberate removal of attention leads to the disappearance of these visual aftereffects (Bahrami, Carmel, Walsh, Rees, & Lavie, 2008; Kaunitz, Fracasso, & Melcher, 2011; Shin et al., 2009). These data suggest that attention can modulate the degree of unconscious visual processing during CFS (for a review, see Yang, Brascamp, Kang, & Blake, 2014). In fact, fMRI studies confirm this effect in the processing of suppressed stimuli, showing that the allocation of attention modulates BOLD responses in V1 to the suppressed stimuli (Bahrami, Lavie, & Rees, 2007; Watanabe et al., 2011; Yuval-Greenberg & Heeger, 2013). We suggest that, in our more demanding awareness access condition, snakes captured attention to a larger degree. Therefore, its processing occurred with a stronger intensity (compared with the processing of spiders and birds), thus reducing the time needed to access visual awareness. Worthy of note is the fact that spiders were rated as more negatively valenced and arousing than snakes and low-level perceptual features (luminance and spatial frequency) were controlled for. Therefore, the effects observed in entering into awareness cannot be attributed to the conscious ratings nor to the low level features of the stimuli, hence arguing in favor of an advantage of snakes in grabbing attention as the result of an evolutionary adaptation. The effectiveness of snakes in grabbing early attention (Hongshen et al., 2014; Soares et al., submitted; Van Strien et al., 2014a, 2014b), regardless of available resources (Soares, Esteves, Lundqvist and Öhman, 2009; Soares, 2012; Soares & Esteves, 2013; Soares, Lindström, Esteves and Öhman, 2014), and the faster access to awareness shown in the current study, are probably associated with the existence of a subcortical pathway to the amygdala (LeDoux, 1996; Öhman, Carlsson, Lundqvist, & Ingvar, 2007; although this has been disputed, see Pessoa & Adolphs, 2010). This pathway is thought to include the superior colliculus and the pulvinar (Liddell et al., 2005; Morris, Öhman, & Dolan, 1999; Öhman et al., 2007). Its role is to perform a “quick and dirty” analysis (linked to magnocellular pathways; Vuilleumier, Armony, Driver, & Dolan, 2003) with the aim of identifying the threat (by orienting attention through the superior colliculus and the pulvinar; Arend et al., 2008; Knudsen, 2007), and quickly activating the defense mechanisms (via the superior colliculus and the amygdala; see Isbell, 2006; Tamietto & de Gelder, 2010) to avoid it (LeDoux, 1996; Öhman, 2005; Öhman et al., 2007). The action of this pathway also appears to interfere with perception. The efferent cortical connections (Emery & Amaral, 2000) between the amygdala and the ventral visual stream (responsible for visual awareness; (Goodale & Milner, 1992, 2004; Milner & Goodale, 1993, 2008) seem to enhance the visual processing for emotional stimuli in the ventral visual stream (Vuilleumier, Richardson, Armony, Driver, & Dolan, 2004). This highlights the role of the amygdala (and probably the subcortical pathway to such stimuli, via the superior colliculus and pulvinar) in perceptual performance (for a review, see Öhman et al., 2007). Indeed, the superior colliculus and pulvinar have been associated with the processing of snakes in primates. Capuchin monkeys with bilateral lesions of the superior colliculus (compared to healthy monkeys) seem to lose the ability to process snakes as a threatening stimulus (Maior et al., 2011). Moreover, snakes appear to modulate pulvinar activity in monkeys (Le et al., 2013, 2016) and, in addition, monkeys with lesions in the amygdala (compared to healthy monkeys) show a reduced aversion to snakes (Kalin, Shelton, Davidson, & Anonymous, 2004; Kluver & Bucy, 1939; Machado, Kazama, & Bachevalier, 2009; Prather et al., 2001). Also, in humans, fMRI data show the activation of the superior colliculus, the pulvinar and the amygdala for true snake stimuli (vs. stimuli with snake shapes, such as cables) (Almeida, Soares, & Castelo-Branco, 2015). Finally, a study with a patient with a bilateral lesion of the amygdala evidenced that, after the lesion, the aversion to snakes was extinguished (Feinstein, Adolphs, Damasio, & Tranel, 2011). Together, these data argue in favor of the important role of these subcortical structures in processing snakes. Several studies have shown that, during CFS, the superior colliculus, the pulvinar, and the amygdala are implicated in the processing of suppressed relevant stimuli (for a review, see Sterzer, Stein, Ludwig, Rothkirch, & Hesselmann, 2014). Interocular suppression techniques are known to reduce the processing throughout the geniculostriate pathway and to strongly suppress the activity in visual striate cortex (Lin & He, 2009). Thus, the action of a subcortical pathway and the amygdala are proposed as the explanation for the differentiated processing of these relevant stimuli during suppression (for a review, see Lin & He, 2009; but see Stein et al., 2014; Willenbockel, Lepore, Nguyen, Bouthillier, & Gosselin, 2012). The action of these structures most likely involve the allocation of attentional resources of the suppressed stimulus (see above), thus modulating the activity of V1 (e.g., Bahrami et al., 2007; Watanabe et al., 2011), as well as other parts in the ventral visual stream; Vuilleumier et al., 2004), and therefore gaining a preferential access to awareness. Indeed, neuroimaging studies (using interocular suppression techniques) have shown the activation of the amygdala for relevant stimuli (i.e., emotional faces) suppressed from awareness (Jiang & He, 2006; Pasley, Mayes, & Schultz, 2004; Troiani et al., 2012; Williams, Morris, McGlone, Abbott, & Mattingley, 2004; but see Schmack et al., 2015; Tsuchiya et al., 2009). Neuroimaging data have also presented indirect evidence for the role of superior colliculus (Pasley et al., 2004) and the pulvinar (Troiani & Schultz, 2013; Troiani et al., 2012) in driving this information to the amygdala. However, a recent fMRI study found that the advantage in entering into visual awareness during CFS by spiders (compared to flowers), could be predicted by the activation of the orbitofrontal and the occipitotemporal cortex but not by the activation of the amygdala (Schmack et al., 2015), arguing against the role of this structure in the access to awareness during CFS. To our understanding, this is in agreement with our interpretation. Although this seems counter-intuitive, previous data have shown that spider processing, unlike snakes (which probably relies on magnocellular pathways, related with faster visual information with low acuity; see Leventhal, Rodieck, & Dreher, 1985; Schiller & Malpeli, 1978), appears to rely on high visual acuity information (see Soares, Lindström, Esteves and Öhman, 2014), which is linked to parvocellular pathways (Livingstone & Hubel, 1988). These pathways (unlike magnocellular pathways, which are associated with subcortical visual structures; see Schiller, Malpeli, & Schein, 1979) are known to primarily project to cortical areas (Merigan & Maunsell, 1993). Therefore, it is likely that the subcortical pathways are more involved in the processing of snakes than spiders, probably because snakes represented a more rooted evolutionary stimulus for primates. However, additional neuroimaging data using stimuli that represented different evolutionary pressures (as it is the case with snakes and spiders) are needed in order disentangle this question. In addition to the previously argued, it is also important to mention the possible role of the koniocellular pathway in the privileged processing of snakes. According to Isbell (2006, 2009), the involvement of koniocellular pathways could be associated with the advantage of primates to process and detect snakes. Unlike the magnocellular pathways, which remain preserved throughout the different primate species, the koniocellular pathways seem to increase consistently throughout evolution depending on the coexistence with poisonous snakes. Actually, these visual pathways appear to strongly contribute to the fear system. These pathways constitute the largest retinal input for the superior colliculus and they are the only visual pathway known to connect this structure with the lateral geniculate nucleus (Casagrande, 1994), thus being strongly related to the superior colliculus–pulvinar connection, which has been deeply involved in the privileged processing snakes (see above). The koniocellular pathways, due to their connection with the superior colliculus, have also been implicated in attentional processes, arousal and eye movements (e.g., Casagrande & Xu, 2004). In addition, they appear to be involved in unconscious visual stimuli processing (e.g., in blindsight phenomena; see Vakalopoulos, 2005). The action of this type of visual pathways, during the CFS, could then provide an explanation for the results of the present study. A possible issue for future research would be to evaluate the role of this kind of pathways in the advantage of fear stimuli in the access to awareness. Taken together, our findings show evidence that the evolutionary value of ecological stimuli (snakes and spiders) is associated with an advantage in entering into visual awareness in humans during CFS. While showing that an advantage of snakes (unlike spiders) remains throughout suppression conditions with different intensities, the results corroborate the assumptions derived from the SDT (Isbell, 2009), supporting the importance of snakes in the evolution of primate vision and showing the relevance of snakes in understanding threat detection mechanisms. Future studies should aim at replicating the findings of the present study using a within-subjects design, since in the present study, and in order to prevent fatigue, participants were only exposed to one condition of ocular dominance. References Agras et al., 1969 S. Agras D. Sylvester D. Oliveau The epidemiology of common fears and phobia Comprehensive Psychiatry 10 2 1969 151 156 10.1016/0010-440X(69)90022-4 Almeida et al., 2010 J. Almeida B.Z. Mahon A. Caramazza The role of the dorsal visual processing stream in tool identification Psychological Science: A Journal of the American Psychological Society/APS 21 6 2010 772 778 10.1177/0956797610371343 Almeida et al., 2008 J. Almeida B.Z. Mahon K. Nakayama A. Caramazza Unconscious processing dissociates along categorical lines Proceedings of the National Academy of Sciences of the United States of America 105 39 2008 15214 15218 10.1073/pnas.0805867105 Almeida et al., 2014 J. Almeida B.Z. Mahon V. Zapater-Raberov A. Dziuba T. Cabaço J.F. Marques A. Caramazza Grasping with the eyes: The role of elongation in visual recognition of manipulable objects Cognitive, Affective & Behavioral Neuroscience 14 1 2014 319 335 10.3758/s13415-013-0208-0 Almeida et al., 2013 J. Almeida P.E. Pajtas B.Z. Mahon K. Nakayama A. Caramazza Affect of the unconscious: visually suppressed angry faces modulate our decisions Cognitive, Affective & Behavioral Neuroscience 13 1 2013 94 101 10.3758/s13415-012-0133-7 Almeida et al., 2015 I. Almeida S.C. Soares M. Castelo-Branco The distinct role of the amygdala, superior colliculus and pulvinar in processing of central and peripheral snakes PloS One 10 6 2015 1 21 10.1371/journal.pone.0129949 Arend et al., 2008 I. Arend L. Machado R. Ward M. McGrath T. Ro R.D. Rafal The role of the human pulvinar in visual attention and action: Evidence from temporal-order judgment, saccade decision, and antisaccade tasks Progress in Brain Research 2008 10.1016/S0079-6123(08)00669-9 Bahrami et al., 2008 B. Bahrami D. Carmel V. Walsh G. Rees N. Lavie Unconscious orientation processing depends on perceptual load Journal of Vision 8 3 2008 12.1 1210 10.1167/8.3.12 Bahrami et al., 2007 B. Bahrami N. Lavie G. Rees Attentional load modulates responses of human primary visual cortex to invisible stimuli Current Biology 17 6 2007 509 513 10.1016/j.cub.2007.01.070 Carlsson et al., 2004 K. Carlsson K.M. Petersson D. Lundqvist A. Karlsson M. Ingvar A. Öhman Fear and the amygdala: Manipulation of awareness generates differential cerebral responses to phobic and fear-relevant (but nonfeared) stimuli Emotion (Washington, D.C.) 4 4 2004 340 353 10.1037/1528–3542.4.4.340 Casagrande, 1994 V.A. Casagrande A third parallel visual pathway to primate area V1 Trends in Neurosciences 1994 10.1016/0166-2236(94)90065-5 Casagrande and Xu, 2004 V.A. Casagrande X. Xu Parallel visual pathways: A comparative perspective L. Chalupa J.S. Werner The visual neurosciences 2004 MIT Press Cambridge, Massachusetts 494 506 Emery and Amaral, 2000 N.J. Emery D.G. Amaral The role of the amygdala in primate social cognition R.D. Lane L. NAdel Cognitive neuroscience of emotion 2000 Oxford University Press New York 156 191 Feinstein et al., 2011 J.S. Feinstein R. Adolphs A. Damasio D. Tranel The human amygdala and the induction and experience of fear Current Biology 21 1 2011 34 38 10.1016/j.cub.2010.11.042 Gerdes et al., 2009 A.B.M. Gerdes G. Uhl G.W. Alpers Spiders are special: Fear and disgust evoked by pictures of arthropods Evolution and Human Behavior 30 1 2009 66 73 10.1016/j.evolhumbehav.2008.08.005 Goodale and Milner, 1992 M.A. Goodale A.D. Milner Separate visual pathways for perception and action Trends in Neurosciences 15 1 1992 20 25 10.1016/0166-2236(92)90344-8 Goodale and Milner, 2004 M.A. Goodale A.D. Milner Sight unseen: an exploration of conscious and unconscious vision 2004 Oxford University Press Oxford Handa et al., 2004 T. Handa K. Mukuno H. Uozato T. Niida N. Shoji K. Shimizu Effects of dominant and nondominant eyes in binocular rivalry Optometry and Vision Science. 81 5 2004 377 383 10.1097/01.opx.0000135085.54136.65 Hongshen et al., 2014 H. Hongshen K. Kenta K. Nobuyuki Spiders do not evoke greater early posterior negativity in the event-related potential as snakes Neuroreport 25 13 2014 1049 1053 10.1097/WNR.0000000000000227 Isbell, 2006 L.A. Isbell Snakes as agents of evolutionary change in primate brains Journal of Human Evolution 51 2006 1 35 10.1016/j.jhevol.2005.12.012 Isbell, 2009 L.A. Isbell The fruit, the tree, and the serpent 2009 Harvard University Press Jensen et al., 2011 M.S. Jensen R. Yao W.N. Street D.J. Simons Change blindness and inattentional blindness Wiley Interdisciplinary Reviews: Cognitive Science 2 5 2011 529 546 10.1002/wcs.130 Jiang and He, 2006 Y. Jiang S. He Cortical responses to invisible faces: Dissociating subsystems for facial-information processing Current Biology 16 20 2006 2023 2029 10.1016/j.cub.2006.08.084 Jiang et al., 2006 Y. Jiang P. Costello F. Fang M. Huang S. He A gender- and sexual orientation-dependent spatial attentional effect of invisible images Proceedings of the National Academy of Sciences of the United States of America 103 45 2006 17048 17052 10.1073/pnas.0605678103 Jiang et al., 2007 Y. Jiang P. Costello S. He Processing of invisible stimuli Psychological Science : A Journal of the American Psychological Society / APS 18 4 2007 349 355 10.1111/j.1467-9280.2007.01902.x Kalin et al., 2004 N.H. Kalin S.E. Shelton R.J. Davidson Anonymous The role of the central nucleus of the amygdala in mediating fear and anxiety in the primate Journal of Neuroscience 24 24 2004 5506 5515 10.1523/JNEUROSCI.0292-04.2004 Kanai et al., 2006 R. Kanai N. Tsuchiya F.A.J. Verstraten The scope and limits of top-down attention in unconscious visual processing Current Biology 16 23 2006 2332 2336 10.1016/j.cub.2006.10.001 Kasturiratne et al., 2008 A. Kasturiratne A.R. Wickremasinghe N. de Silva N.K. Gunawardena A. Pathmeswaran R. Premaratna H.J. de Silva The global burden of snakebite: a literature analysis and modelling based on regional estimates of envenoming and deaths PLoS Medicine 5 11 2008 e218 Kaunitz et al., 2011a L. Kaunitz A. Fracasso D. Melcher Unseen complex motion is modulated by attention and generates a visible aftereffect Journal of Vision 11 13 2011 10 10.1167/11.13.10 Kaunitz et al., 2011b L.N. Kaunitz J.E. Kamienkowski E. Olivetti B. Murphy P. Avesani D.P. Melcher Intercepting the first pass: Rapid categorization is suppressed for unseen stimuli Frontiers in Psychology 2 AUG 2011 10.3389/fpsyg.2011.00198 Kluver and Bucy, 1939 H. Kluver P. Bucy Preliminary analysis of functions of the temporal lobes in monkeys Archives of Neurology & Psychiatry 42 6 1939 979 1000 10.3171/2013.4.JNS121829 Knudsen, 2007 E.I. Knudsen Fundamental components of attention Annual Review of Neuroscience 30 2007 57 78 10.1146/annurev.neuro.30.051606.094256 Lang et al., 1997 P.J. Lang M.M. Bradley B.N. Cuthbert International affective picture system (IAPS): Technical manual and affective ratings NIMH: Center for the Study of emotion and attention 1997 Lang et al., 2005 P.J. Lang M.M. Bradley B.N. Cuthbert International affective picture system (IAPS): Instructions manual and affective ratings Technical report A-6, the center for research in psychophysiology, University of Florida 2005 Le et al., 2016 Q. Le L.A. Isbell J. Matsumoto V.Q. Le H. Nishimaru E. Hori H. Nishijo Snakes elicit earlier, and monkey faces, later, gamma oscillations in macaque pulvinar neurons Scientific Reports 6 February 2016 20595 10.1038/srep20595 Le et al., 2013 Q. Le L.A. Isbell J. Matsumoto M. Nguyen E. Hori R.S. Maior H. Nishijo Pulvinar neurons reveal neurobiological evidence of past selection for rapid detection of snakes Proceedings of the National Academy of Sciences of the United States of America 110 47 2013 19000 19005 10.1073/pnas.1312648110 LeDoux, 1996 J. LeDoux The emotional brain: the mysterious underpinnings of emotional life 1996 Shuster Sa New York Leventhal et al., 1985 A.G. Leventhal R.W. Rodieck B. Dreher Central projections of cat retinal ganglion cells The Journal of Comparative Neurology 237 1985 1985 216 226 10.1002/cne.902370206 Liddell et al., 2005 B.J. Liddell K.J. Brown A.H. Kemp M.J. Barton P. Das A. Peduto L.M. Williams A direct brainstem-amygdala-cortical “alarm” system for subliminal signals of fear NeuroImage 24 2005 2005 235 243 10.1016/j.neuroimage.2004.08.016 Lin and He, 2009 Z. Lin S. He Seeing the invisible: The scope and limits of unconscious processing in binocular rivalry Progress in Neurobiology 87 4 2009 195 211 10.1016/j.pneurobio.2008.09.002 Livingstone and Hubel, 1988 M. Livingstone D. Hubel Segregation of depth: Form, anatomy, color, physiology, and movement, and perception Science. New Series 240 4853 1988 740 749 10.1126/science.3283936 LoBue and DeLoache, 2008 V. LoBue J.S. DeLoache Detecting the snake in the grass Psychological Science 19 3 2008 284 10.1111/j.1467-9280.2008.02081.x LoBue et al., 2010 V. LoBue D.H. Rakison J.S. DeLoache Threat perception across the life span: evidence for multiple converging pathways Current Directions in Psychological Science 19 6 2010 375 379 10.1177/0963721410388801 Machado et al., 2009 C.J. Machado A.M. Kazama J. Bachevalier Impact of amygdala, orbital frontal, or hippocampal lesions on threat avoidance and emotional reactivity in nonhuman primates Emotion 9 2 2009 147 163 10.1037/a0014539 Maior et al., 2011 R.S. Maior E. Hori M. Barros D.S. Teixeira M.C.H. Tavares T. Ono C. Tomaz Superior colliculus lesions impair threat responsiveness in infant capuchin monkeys Neuroscience Letters 504 3 2011 257 260 10.1016/j.neulet.2011.09.042 Mayer et al., 2006 B. Mayer P. Muris L. Vogel I. Nojoredjo H. Merckelbach Fear-relevant change detection in spider-fearful and non-fearful participants Journal of Anxiety Disorders 20 4 2006 510 519 10.1016/j.janxdis.2005.05.001 Merigan and Maunsell, 1993 W.H. Merigan J.H.R. Maunsell How parallel are the primate visual pathways? Annual Review of Neuroscience 16 1993 369 402 10.1146/annurev.ne.16.030193.002101 Miles, 1930 W.R. Miles Ocular dominance in human adults Journal of General Psychology 3 1930 412 430 Milner and Goodale, 1993 A.D. Milner M.A. Goodale Visual pathways to perception and action Progress in Brain Research 95 1993 317 337 Milner and Goodale, 2008 A.D. Milner M.A. Goodale Two visual systems re-viewed Neuropsychologia 46 3 2008 774 785 10.1016/j.neuropsychologia.2007.10.005 Mineka et al., 1980 S. Mineka R. Keir V. Price Fear of snakes in wild- and laboratory-reared rhesus monkeys (Macaca mulatta) Animal Learning & Behavior 1980 10.3758/BF03197783 Morris et al., 1999 J.S. Morris A. Öhman R.J. Dolan A subcortical pathway to the right amygdala mediating “unseen” fear Proceedings of the National Academy of Sciences of the United States of America 96 February 1999 1680 1685 10.1073/pnas.96.4.1680 New and German, 2015 J.J. New T.C. German Spiders at the cocktail party: An ancestral threat that surmounts inattentional blindness Evolution and Human Behavior 36 3 2015 165 173 10.1016/j.evolhumbehav.2014.08.004 Nyffeler, 1999 M. Nyffeler Prey selection of spiders in the field The Journal of Arachnology 27 1999 317 324 10.2307/3706003 (Table 4) Öhman, 2005 A. Öhman The role of the amygdala in human fear: Automatic detection of threat Psychoneuroendocrinology 30 2005 953 958 10.1016/j.psyneuen.2005.03.019 Öhman and Mineka, 2001 A. Öhman S. Mineka Fears, phobias, and preparedness: Toward an evolved module of fear and fear learning Psychological Review 108 3 2001 483 522 10.1037/0033-295X.108.3.483 Öhman and Soares, 1994 A. Öhman J.J.F. Soares “unconscious anxiety”: Phobic responses to masked stimuli Journal of Abnormal Psychology 103 2 1994 231 240 10.1037/0021-843X.103.2.231 Öhman et al., 2007 A. Öhman K. Carlsson D. Lundqvist M. Ingvar On the unconscious subcortical origin of human fear Physiology and Behavior 92 2007 180 185 10.1016/j.physbeh.2007.05.057 Öhman et al., 2001 A. Öhman A. Flykt F. Esteves Emotion drives attention: Detecting the snake in the grass Journal of Experimental Psychology. General 130 2001 466 478 10.1037/0096-3445.130.3.466 Pasley et al., 2004 B.N. Pasley L.C. Mayes R.T. Schultz Subcortical discrimination of unperceived objects during binocular rivalry Neuron 42 1 2004 163 172 10.1016/S0896-6273(04)00155-2 Peira et al., 2010 N. Peira A. Golkar M. Larsson S. Wiens What you fear will appear: Detection of schematic spiders in spider fear Experimental Psychology 57 6 2010 470 475 10.1027/1618-3169/a000058 Peli, 1990 E. Peli Contrast in complex images Journal of the Optical Society of America. A, Optics and Image Science 7 10 1990 2032 2040 (Retrieved from Penkunas and Coss, 2013a M.J. Penkunas R.G. Coss A comparison of rural and urban Indian children's visual detection of threatening and nonthreatening animals Developmental Science 16 2013 463 475 10.1111/desc.12043 Penkunas and Coss, 2013b M.J. Penkunas R.G. Coss Rapid detection of visually provocative animals by preschool children and adults Journal of Experimental Child Psychology 114 4 2013 522 536 10.1016/j.jecp.2012.10.001 Pessoa and Adolphs, 2010 L. Pessoa R. Adolphs Emotion processing and the amygdala: From a “low road” to “many roads” of evaluating biological significance Nature Reviews Neuroscience 11 11 2010 773 783 10.1038/nrn2920 Phelps and LeDoux, 2005 E.A. Phelps J.E. LeDoux Contributions of the amygdala to emotion processing: From animal models to human behavior Neuron 2005 10.1016/j.neuron.2005.09.025 Porac and Coren, 1976 C. Porac S. Coren The dominant eye Psychological Bulletin 83 5 1976 880 897 10.1097/00006324-192910000-00018 Prather et al., 2001 M.D. Prather P. Lavenex M.L. Mauldin-Jourdain W.A. Mason J.P. Capitanio S.P. Mendoza D.G. Amaral Increased social fear and decreased fear of objects in monkeys with neonatal amygdala lesions Neuroscience 106 4 2001 653 658 10.1016/S0306-4522(01)00445-6 Rothkirch et al., 2012 M. Rothkirch T. Stein M. Sekutowicz P. Sterzer A direct oculomotor correlate of unconscious visual processing Current Biology 22 13 2012 10.1016/j.cub.2012.04.046 Schiller and Malpeli, 1978 P.H. Schiller J.G. Malpeli Functional specificity of lateral geniculate nucleus laminae of the rhesus monkey Journal of Neurophysiology 41 3 1978 788 797 Schiller et al., 1979 P.H. Schiller J.G. Malpeli S.J. Schein Composition of geniculostriate input ot superior colliculus of the rhesus monkey Journal of Neurophysiology 42 4 1979 1124 1133 (Retrieved from Schmack et al., 2015 K. Schmack J. Burk J.-D. Haynes P. Sterzer Predicting subjective affective salience from cortical responses to invisible object stimuli Cerebral Cortex 2015 1 8 10.1093/cercor/bhv174 Shibasaki and Kawai, 2009 M. Shibasaki N. Kawai Rapid detection of snakes by Japanese monkeys (Macaca fuscata): an evolutionarily predisposed visual system Journal of Comparative Psychology 123 2009 131 135 10.1037/a0015095 Shin et al., 2009 K. Shin M. Stolte S.C. Chong The effect of spatial attention on invisible stimuli Attention, Perception, & Psychophysics 71 7 2009 1507 1513 10.3758/App.71.7.1507 Soares, 2012 S.C. Soares The lurking snake in the grass: Interference of snake stimuli in visually taxing conditions Evolutionary Psychology 10 2 2012 187 197 Soares and Esteves, 2013 S.C. Soares F. Esteves A glimpse of fear: Fast detection of threatening targets in visual search with brief stimulus durations PsyCh Journal 2 1 2013 11 16 10.1002/pchj.18 Soares et al., 2009 S.C. Soares F. Esteves D. Lundqvist A. Öhman Some animal specific fears are more specific than others: Evidence from attention and emotion measures Behaviour Research and Therapy 47 12 2009 1032 1042 10.1016/j.brat.2009.07.022 Soares et al., n.d Soares, S. C., Kessel, D., Hernández-Lorca, M., García-Rubio, M. J., Rodrigues, P., Gomes, N., & Carretié, L. Deconstructing exogenous attention to fear: behavioral and electrophysiological correlates. (submitted). Soares et al., 2014 S.C. Soares B. Lindström F. Esteves A. Öhman The hidden snake in the grass: Superior detection of snakes in challenging attentional conditions PloS One 9 12 2014 e114724 10.1371/journal.pone.0114724 Steen et al., 2004 C.J. Steen P.A. Carbonaro R.A. Schwartz Arthropods in dermatology Journal of the American Academy of Dermatology 50 6 2004 819 842 l 10.1016/j.jaad.2003.12.019 Stein et al., 2011 T. Stein A. Senju M.V. Peelen P. Sterzer Eye contact facilitates awareness of faces during interocular suppression Cognition 119 2 2011 307 311 10.1016/j.cognition.2011.01.008 Stein et al., 2014 T. Stein K. Seymour M.N. Hebart P. Sterzer Rapid fear detection relies on high spatial frequencies Psychological Science 25 2014 566 574 10.1177/0956797613512509 Sterzer et al., 2011 P. Sterzer T. Hilgenfeldt P. Freudenberg F. Bermpohl M. Adli Access of emotional information to visual awareness in patients with major depressive disorder Psychological Medicine 41 8 2011 1615 1624 10.1017/S0033291710002540 Sterzer et al., 2014 P. Sterzer T. Stein K. Ludwig M. Rothkirch G. Hesselmann Neural processing of visual information under interocular suppression: A critical review Frontiers in Psychology 5 MAY 2014 1 12 10.3389/fpsyg.2014.00453 Tamietto and de Gelder, 2010 M. Tamietto B. de Gelder Neural bases of the non-conscious perception of emotional signals Nature Reviews. Neuroscience 11 10 2010 697 709 10.1038/nrn2889 Troiani and Schultz, 2013 V. Troiani R.T. Schultz Amygdala, pulvinar, and inferior parietal cortex contribute to early processing of faces without awareness Frontiers in Human Neuroscience 7 June 2013 241 10.3389/fnhum.2013.00241 Troiani et al., 2012 V. Troiani E.T. Price R.T. Schultz Unseen fearful faces promote amygdala guidance of attention Social Cognitive and Affective Neuroscience 9 2 2012 133 140 10.1093/scan/nss116 Tsuchiya and Koch, 2005 N. Tsuchiya C. Koch Continuous flash suppression reduces negative afterimages Nature Neuroscience 8 8 2005 1096 1101 (Retrieved from Tsuchiya et al., 2009 Tsuchiya, N., Moradi, F., Felsen, C., & Yamazaki, M. (2009). Intact rapid detection of fearful faces, 12(10), 1224–1225. doi:10.1038/nn.2380.Intact Vakalopoulos, 2005 C. Vakalopoulos A theory of blindsight - The anatomy of the unconscious: a proposal for the koniocellular projections and intralaminar thalamus Medical Hypotheses 65 6 2005 1183 1190 10.1016/j.mehy.2005.05.039 Valle-Inclán et al., 2008 F. Valle-Inclán M.J. Blanco D. Soto L. Leirós A new method to assess eye dominance Psicologica 29 1 2008 55 64 Van Strien et al., 2014a J.W. Van Strien R. Eijlers I.H.A. Franken J. Huijding Snake pictures draw more early attention than spider pictures in non-phobic women: Evidence from event-related brain potentials Biological Psychology 96 2014 150 157 10.1016/j.biopsycho.2013.12.014 Van Strien et al., 2014b J.W. Van Strien I.H.A. Franken J. Huijding Testing the snake-detection hypothesis: Larger early posterior negativity in humans to pictures of snakes than to pictures of other reptiles, spiders and slugs Frontiers in Human Neuroscience 8 September 2014 1 9 10.3389/fnhum.2014.00691 Vuilleumier et al., 2003 P. Vuilleumier J.L. Armony J. Driver R.J. Dolan Distinct spatial frequency sensitivities for processing faces and emotional expressions Nature Neuroscience 6 6 2003 624 631 10.1038/nn1057 Vuilleumier et al., 2004 P. Vuilleumier M.P. Richardson J.L. Armony J. Driver R.J. Dolan Distant influences of amygdala lesion on visual cortical activation during emotional face processing Nature Neuroscience 7 11 2004 1271 1278 10.1038/nn1341 Watanabe et al., 2011 M. Watanabe K. Cheng Y. Murayama K. Ueno T. Asamizuya K. Tanaka N. Logothetis Attention but not awareness modulates the BOLD signal in the human V1 during binocular suppression Science 334 6057 2011 829 831 10.1126/science.1203161 Wiens, 2006 S. Wiens Current concerns in visual masking Emotion (Washington, D.C.) 6 4 2006 675 680 10.1037/1528–3542.6.4.675 Willenbockel et al., 2012 V. Willenbockel F. Lepore D.K. Nguyen A. Bouthillier F. Gosselin Spatial frequency tuning during the conscious and non-conscious perception of emotional facial expressions - An intracranial ERP study Frontiers in Psychology 3 JUL 2012 1 12 10.3389/fpsyg.2012.00237 Williams et al., 2004 M.A. Williams A.P. Morris F. McGlone D.F. Abbott J.B. Mattingley Amygdala responses to fearful and happy facial expressions under conditions of binocular suppression Journal of Neuroscience 24 12 2004 2898 2904 10.1523/JNEUROSCI.4977-03.2004 Xu et al., 2011 S. Xu S. Zhang H. Geng Gaze-induced joint attention persists under high perceptual load and does not depend on awareness Vision Research 51 18 2011 2048 2056 10.1016/j.visres.2011.07.023 Yang et al., 2014 E. Yang J. Brascamp M.S. Kang R. Blake On the use of continuous flash suppression for the study of visual processing outside of awareness Frontiers in Psychology 5 JUL 2014 1 17 10.3389/fpsyg.2014.00724 Yang et al., 2010 E. Yang S.-W. Hong R. Blake Adaptation aftereffects to facial expressions suppressed from visual awareness Journal of Vision 10 2010 24 10.1167/10.12.24 Yang et al., 2007 E. Yang D.H. Zald R. Blake Fearful expressions gain preferential access to awareness during continuous flash suppression Emotion (Washington, D.C.) 7 2007 882 886 10.1037/1528–3542.7.4.882 Yang et al., 2011 Z. Yang J. Zhao Y. Jiang C. Li J. Wang X. Weng G. Northoff Altered negative unconscious processing in major depressive disorder: An exploratory neuropsychological study PloS One 6 7 2011 10.1371/journal.pone.0021881 Yorzinski et al., 2014 J.L. Yorzinski M.J. Penkunas M.L. Platt R.G. Coss Dangerous animals capture and maintain attention in humans Evolutionary Psychology : An International Journal of Evolutionary Approaches to Psychology and Behavior 12 3 2014 534 548 (Retrieved from Yuval-Greenberg and Heeger, 2013 S. Yuval-Greenberg D.J. Heeger Continuous flash suppression modulates cortical activity in early visual cortex Journal of Neuroscience 33 23 2013 9635 9643 10.1523/JNEUROSCI.4612-12.2013 Zhan et al., 2015 M. Zhan R. Hortensius B. De Gelder The body as a tool for anger awareness-differential effects of angry facial and bodily expressions on suppression from awareness PloS One 10 10 2015 1 13 10.1371/journal.pone.0139768 "
    },
    {
        "doc_title": "Preface",
        "doc_scopus_id": "85101170948",
        "doc_doi": "10.2307/j.ctv6gqqkx.3",
        "doc_eid": "2-s2.0-85101170948",
        "doc_date": "2017-01-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SSI Modalities II: Articulation and Its Consequences",
        "doc_scopus_id": "85101170935",
        "doc_doi": "10.1007/978-3-319-40174-4_3",
        "doc_eid": "2-s2.0-85101170935",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Muscular activities",
            "Silent speech interfaces",
            "Speech production",
            "Speech sounds",
            "Visual aspects",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2017, The Author(s).Brain and muscular activity originates the change in shape or position of articulators such as the tongue or lips and, as a consequence, the vocal tract assumes different configurations. Most of these changes, namely of articulators and tract, are internal and are not easy to measure but, in some cases, like the lips or the tongue tip, such changes are visible or have visible effects. Even without the production of speech sound, these different configurations of articulators provide valuable information that can be used in the context of silent speech interfaces (SSIs). In this chapter, the reader finds an overview of the technologies used to assess articulatory and visual aspects of speech production and how researchers have exploited their capabilities for the development of silent speech interfaces.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SSI Modalities I: Behind the Scenes—From the Brain to the Muscles",
        "doc_scopus_id": "85101161211",
        "doc_doi": "10.1007/978-3-319-40174-4_2",
        "doc_eid": "2-s2.0-85101161211",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Direct use",
            "Muscular activities",
            "Myoelectric signals",
            "Sensorimotor cortex",
            "Silent speech",
            "Silent speech interfaces",
            "Speech production"
        ],
        "doc_abstract": "© 2017, The Author(s).Silent speech approaches can profit not only from an understanding of the brain and motor stages associated with speech production, but also from a direct use of the information coming from these stages. Therefore, in this chapter, readers can find a short overview of recent work regarding the study of brain and muscular activity related to speech production, and the use of such knowledge to serve silent speech interfaces (SSIs). In this context, the chapter takes in consideration the importance of understanding the sensorimotor cortex’s role in speech production and the application of such knowledge in the context of brain–computer interfaces (BCI). Regarding muscular activity, the concept of myoelectric signal is introduced and the literature surveyed on the technologies used to measure it. For each of the mentioned speech production stages, recent accomplishments in the domain of silent speech interfaces are also covered.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Introduction",
        "doc_scopus_id": "85101116561",
        "doc_doi": "10.1007/978-3-319-40174-4_1",
        "doc_eid": "2-s2.0-85101116561",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Acoustic signals",
            "Background noise",
            "Complex Processes",
            "Computer interaction",
            "Different stages",
            "Muscular activities",
            "Silent speech interfaces",
            "Speech production"
        ],
        "doc_abstract": "© 2017, The Author(s).The concept of silent speech, when applied to human–computer interaction (HCI), describes a system that allows for speech communication between humans and machines in the absence of an audible acoustic signal. This type of system can be used as an input HCI modality in high-background-noise environments such as in living rooms, or in aiding speech-impaired individuals. The audible acoustic signal is, in fact, just the end result of the complex process of speech production, which starts at the brain, triggers relevant muscular activity, and results in movements of the articulators. It is this information that silent speech interfaces (SSIs) strive to harness and, in this context, understanding the different stages of speech production is of major importance. In this chapter, the reader finds a brief introduction into the historical context for the rising interest in silent speech, followed by an overview on the different stages involved in speech production. Along the way, we establish a correspondence between the natural speech production process and the technology, which will be further discussed in the following chapters, leading to the existing silent speech interface (SSI) systems. Additionally, we identify overall challenges in the development of SSI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Conclusions",
        "doc_scopus_id": "85101108170",
        "doc_doi": "10.1007/978-3-319-40174-4_6",
        "doc_eid": "2-s2.0-85101108170",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Daily lives",
            "Enabling technologies",
            "Silent speech interfaces"
        ],
        "doc_abstract": "© 2017, The Author(s).In this book, we covered silent speech interfaces (SSIs), from the core principles and motivations that have fostered research on the topic, up to the enabling technologies that serve its development and the methods applied to analyze and process the data they provide. We have realized that research in silent speech interfaces is just at its infancy and evolving at a fast pace, boosted by novel technologies and views from a growing multidisciplinary community. In this chapter, the reader may find a reflection about upcoming steps and the future of SSI systems. After summarizing the topics addressed in this book, we discuss our views regarding the future of research in this area, a future brimming with challenges, but also with unquestionable potential for SSI to become a technological asset on our daily lives.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Application Examples",
        "doc_scopus_id": "85101086423",
        "doc_doi": "10.1007/978-3-319-40174-4_5",
        "doc_eid": "2-s2.0-85101086423",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Multi-modal",
            "Silent speech interfaces",
            "Surface electromyography",
            "Technical aspects",
            "Visual speech recognition"
        ],
        "doc_abstract": "© 2017, The Author(s).In the previous chapters, we covered the main concepts and technologies behind silent speech interfaces (SSIs). While that material provides the reader with the necessary knowledge to understand the different concepts and proposed solutions, the technical aspects behind designing and developing interactive SSI systems are still a challenge given that they involve articulating different technologies and methods. In this chapter, we provide some examples to allow the reader to go from theory to practice. We start with a tutorial of a basic visual speech recognition system, using accessible hardware and resources. Then, we describe a more complex practical example that shows how to leverage the multimodal SSI concept introduced in Chap. 4. With this illustration, the reader has the opportunity to assess, hands-on, the capabilities of surface electromyography sensors. The last part of the chapter describes the creation of an SSI system that handles live data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Combining Modalities: Multimodal SSI",
        "doc_scopus_id": "85101060727",
        "doc_doi": "10.1007/978-3-319-40174-4_4",
        "doc_eid": "2-s2.0-85101060727",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Baseline methods",
            "Different stages",
            "Information concerning",
            "Muscular activities",
            "Silent speech interfaces",
            "Speech production",
            "Ultrasonic doppler",
            "Visual speech features"
        ],
        "doc_abstract": "© 2017, The Author(s).In previous chapters, we have seen how various silent speech interface (SSI) modalities gather information concerning the different stages of speech production, covering brain and muscular activity, articulation, acoustics, and visual speech features. In this chapter, the reader is introduced to the combination of different modalities, not only to drive silent speech interfaces, but also to further enhance the understanding regarding emerging and promising modalities, e.g., ultrasonic Doppler. This approach poses several challenges dealing with the acquisition, synchronization, processing and analysis of the multimodal data. These challenges lead the authors to propose a framework to support research on multimodal silent speech interfaces (SSIs) and to provide concrete examples of its practical application, considering several of the SSI modalities covered in previous chapters. For each example, we propose baseline methods for comparison with the collected data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An anthropomorphic perspective for audiovisual speech synthesis",
        "doc_scopus_id": "85051795847",
        "doc_doi": "10.5220/0006150201630172",
        "doc_eid": "2-s2.0-85051795847",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Audio-visual speech",
            "Complex services",
            "Conceptual frameworks",
            "Data-driven methods",
            "Lip movements",
            "Multi-Modal Interactions",
            "Research and development",
            "Speech production"
        ],
        "doc_abstract": "Copyright © 2017 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.In speech communication, both the auditory and visual streams play an important role, ensuring both a certain level of redundancy (e.g., lip movement) and transmission of complementary information (e.g., to emphasize a word). The common current approach to audiovisual speech synthesis, generally based on data-driven methods, yields good results, but relies on models controlled by parameters that do not relate with how humans do it, being hard to interpret and adding little to our understanding of the human speech production apparatus. Modelling the actual system, adopting an anthropomorphic perspective would provide a myriad of novel research paths. This article proposes a conceptual framework to support research and development of an articulatory-based audiovisual speech synthesis system. The core idea is that the speech production system is modelled to produce articulatory parameters with anthropomorphic meaning (e.g., lip opening) driving the synthesis of both the auditory and visual streams. A first instantiation of the framework for European Portuguese illustrates its viability and constitutes an important tool for research in speech production and the deployment of audiovisual speech synthesis in multimodal interaction scenarios, of the utmost relevance for the current and future complex services and applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Critical articulators identification from RT-MRI of the vocal tract",
        "doc_scopus_id": "85039167406",
        "doc_doi": "10.21437/Interspeech.2017-742",
        "doc_eid": "2-s2.0-85039167406",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Critical articulators",
            "Data-driven approach",
            "Dynamic aspects",
            "Quantitative assessments",
            "Real time",
            "Speech production",
            "Vocal-tracts"
        ],
        "doc_abstract": "Copyright © 2017 ISCA.Several technologies, such as electromagnetic midsagittal articulography (EMA) or real-time magnetic resonance (RTMRI), enable studying the static and dynamic aspects of speech production. The resulting knowledge can, in turn, inform the improvement of speech production models, e.g., for articulatory speech synthesis, by enabling the identification of which articulators and gestures are involved in producing specific sounds. The amount of data available from these technologies, and the need for a systematic quantitative assessment, advise tackling these matters through data-driven approaches, preferably unsupervised, since annotated data is scarce. In this context, a method for statistical identification of critical articulators has been proposed, in the literature, and successfully applied to EMA data. However, the many differences regarding the data available from other technologies, such as RT-MRI, and language-specific aspects create a challenging setting for its direct and wider applicability. In this article, we address the steps needed to extend the applicability of the proposed statistical analyses, initially applied to EMA, to an existing RT-MRI corpus and test it for a different language, European Portuguese. The obtained results, for three speakers, and considering 33 phonemes, provide phonologically meaningful critical articulator outcomes and show evidence of the applicability of the method to RT-MRI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Silent speech interaction for ambient assisted living scenarios",
        "doc_scopus_id": "85025166995",
        "doc_doi": "10.1007/978-3-319-58530-7_29",
        "doc_eid": "2-s2.0-85025166995",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Ambient assisted living (AAL)",
            "Automatic recognition",
            "Elderly",
            "Silent speech interfaces",
            "Speech recognition performance",
            "System accuracy",
            "Visual speech recognition"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.In many Ambient Assisted Living (AAL) contexts, the speech signal cannot be used or speech recognition performance is highly affected due to ambient noise from televisions or music players. Trying to address these difficulties resulted in the exploration of Silent Speech interfaces (SSI), making use of other means to obtain information regarding what the user is uttering, even when no acoustic speech signal is produced. The automatic recognition of what has been said, based only on images of the face, is the purpose of Visual Speech Recognition (VSR) systems, a type of SSI. However, despite the potential of VSR for enabling the interaction of older adults with new AAL applications, and current advances in SSI technologies, no real VSR application can be found in the literature. Based on recent work in SSI, for European Portuguese, a first working application of VSR targeting older adults is presented along with and results from an initial evaluation. The system performed well, enabling real-time control of a media player with an accuracy of 81.3% and performing classification in around 1.3 s. At this stage, the results vary from speaker to speaker and the system performs better if the words are correctly articulated. The effect of distance of the speaker to the video apparatus (a Kinect One) proved not to be an issue in terms of the system accuracy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "“Tell your day”: Developing multimodal interaction applications for children with ASD",
        "doc_scopus_id": "85025140611",
        "doc_doi": "10.1007/978-3-319-58706-6_43",
        "doc_eid": "2-s2.0-85025140611",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autism spectrum disorders",
            "Children",
            "Design and Development",
            "Information exchanges",
            "Multi-Modal Interactions",
            "Multi-modality"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.The development of applications for children, and particularly for those diagnosed with autism spectrum disorders (ASD), is a challenging task. In this context, careful consideration of the characteristics of these users, along with those of different stakeholders, such as parents and teachers, is essential. Also, it is important to provide different ways of using applications through multimodal interaction, in order to adapt, as much as possible, to the users’ needs, capabilities and preferences. Providing multimodality does not mean that users will interact multimodally, but provides freedom of choice to the user. Additionally, enabling multiple forms of interaction might also help understanding what actually works better, for an audience that is not always able to express an opinion regarding what might work. In this article, we take on previous work regarding the definition of a Persona for a child diagnosed with ASD and, considering the goals above, propose and evaluate a first prototype of an application targeting the audience represented by this Persona. This application, aims to serve as a place for communication and information exchange among the child, her family, and teachers and supports multimodal interaction.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An exploratory study on the predictive capacity of heuristic evaluation in visualization applications",
        "doc_scopus_id": "85021624109",
        "doc_doi": "10.1007/978-3-319-58071-5_28",
        "doc_eid": "2-s2.0-85021624109",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Exploratory studies",
            "Heuristic evaluation",
            "Information visualization",
            "Potential problems",
            "Predictive capacity",
            "Usability evaluation",
            "User study",
            "Visualization application"
        ],
        "doc_abstract": "© 2017, Springer International Publishing AG.Heuristic evaluation is generally considered as an adequate method to perform formative usability evaluation as it helps identify potential problems from early stages of development and may provide useful results even with a relatively low investment. In particular, the method has been adapted and used to evaluate visualization applications. This paper presents an exploratory study aimed at better understanding the capacity of heuristic evaluation to predict the issues experienced by users when using a visualization application and how to assess it. The main usability potential problems pointed out in a visualization application by 20 evaluators using heuristic evaluation are compared with the problems reported for the same application by 44 users.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantitative systematic analysis of vocal tract data",
        "doc_scopus_id": "84949548344",
        "doc_doi": "10.1016/j.csl.2015.05.004",
        "doc_eid": "2-s2.0-84949548344",
        "doc_date": "2016-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Objective measure",
            "Quantitative comparison",
            "Speech production",
            "Systematic analysis",
            "Traditional approaches",
            "Visual representations",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2015 Elsevier Ltd. All rights reserved.Articulatory data can nowadays be obtained using a wide range of techniques, with a notable emphasis on imaging modalities such as ultrasound and real-time magnetic resonance, resulting in large amounts of image data. One of the major challenges posed by these large datasets concerns how they can be efficiently analysed to extract relevant information to support speech production studies. Traditional approaches, including the superposition of vocal tract profiles, provide only a qualitative characterisation of notable properties and differences. While providing valuable information, these methods are rather inefficient and inherently subjective. Therefore, analysis must evolve towards a more automated, replicable and quantitative approach. To address these issues we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative normalised data regarding differences covering meaningful regions under the influence of various articulators. An important part of the framework is the visual representation of the data, proposed to support analysis, and depicting the differences found and corresponding direction of change. The normalised nature of the computed data allows comparison among different sounds and speakers in a common representation. Representative application examples, concerning the articulatory characterisation of European Portuguese vowels, are presented to illustrate the capabilities of the proposed framework, both for static configurations and the assessment of dynamic aspects during speech production.",
        "available": true,
        "clean_text": "serial JL 272453 291210 291718 291723 291743 291782 291874 31 Computer Speech & Language COMPUTERSPEECHLANGUAGE 2015-06-03 2015-06-03 2015-12-09 2015-12-09 2015-12-09T16:48:50 S0885-2308(15)00054-6 S0885230815000546 10.1016/j.csl.2015.05.004 S300 S300.1 FULL-TEXT 2021-03-08T20:15:19.730842Z 0 0 20160301 20160331 2016 2015-06-03T05:31:18.457577Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor grantsponsorid highlightsabst primabst ref 0885-2308 08852308 true 36 36 C Volume 36 9 307 329 307 329 201603 March 2016 2016-03-01 2016-03-31 2016 Speech Production in Speech Technologies Edited by Karen Livescu, Frank Rudzicz, Eric Fosler-Lussier, Mark Hasegawa-Johnson and Jeff Bilmes Automated analysis of articulatory data article fla Copyright © 2015 Elsevier Ltd. All rights reserved. QUANTITATIVESYSTEMATICANALYSISVOCALTRACTDATA SILVA S 1 Introduction 1.1 Challenges 1.2 Related work 1.3 Contributions and overview 2 Vocal tract configuration comparison 2.1 Requirements and definitions 2.2 Segments and features of interest 2.3 Comparison methods 2.4 Taking advantage of repeated utterances 3 Visual representations 3.1 Parameter location 3.2 Value representation 3.3 Difference level 3.4 Overall interpretation 3.5 Directional information 3.6 Variability 4 Support for analysis of dynamic aspects 4.1 Aligning multiple utterance realisations 4.2 Dynamic analysis of the vocal tract 4.3 Dynamic data representation 5 Application examples 5.1 Application scenario 5.1.1 Data acquisition and feature extraction 5.2 Static analysis 5.2.1 Intra-speaker analysis 5.2.2 Inter-speaker comparison 5.2.3 Overall characterisation of articulatory differences 5.3 Dynamic analysis 5.3.1 Intra-sound dynamics 5.3.2 Inter-sound production differences 6 Discussion 7 Conclusions Acknowledgements References BADIN 2014 17 20 P PROCISSP COMPARISONARTICULATORYSTRATEGIESFORBILINGUALSPEAKERPRELIMINARYDATAMODELS BENITEZ 2014 701 705 A PROCINTERSPEECH REALTIMEMRIARTICULATORYSETTINGINSECONDLANGUAGESPEECH BERNDT 1994 229 248 D PROCAAAIWORKSHOPKNOWLEDGEDISCOVERYINDATABASES USINGDYNAMICTIMEWARPINGFINDPATTERNSINTIMESERIES BIRKHOLZ 2011 1422 1433 P BOERSMA 2014 P PRAATDOINGPHONETICSBYCOMPUTERCOMPUTERPROGRAMVERSION5342 BRESCH 2010 1584 1587 E PROCINTERSPEECH STATISTICALMULTISTREAMMODELINGREALTIMEMRIARTICULATORYSPEECHDATA BRESCH 2009 323 338 E CLELAND 2011 163 170 J PROCINTSEMINARSPEECHPRODUCTIONISSP COMPARINGARTICULATORYIMAGESMRIULTRASOUNDTONGUEIMAGEDATABASE DAVIDSON 2006 407 415 L DELVAUX 2002 348 352 V PROCJOURNEESDETUDESURLAPAROLE PROPRIETESACOUSTIQUESEARTICULATOIRESDESVOYELLESNASALESDUFRANCAIS ENGWALL 2003 43 48 O PROC6THINTSEMINARSPEECHPRODUCTIONISSP AREVISITAPPLICATIONMRIANALYSISSPEECHPRODUCTIONTESTINGASSUMPTIONS GICK 2004 220 233 B GREGIO 2006 F CONFIGURACAOTRATOVOCALSUPRAGLOTICONAPRODUCAODASVOGAISPORTUGUESBRASILEIRODADOSDEIMAGENSDERESSONANCIAMAGNETICASUPRAGLOTTICVOCALTRACTSHAPINGINPRODUCTIONBRAZILIANPORTUGUESEVOWELSDATAMAGNETICRESONANCEIMAGING HAGEDORN 2011 409 412 C PROCINTERSPEECH AUTOMATICANALYSISSINGLETONGEMINATECONSONANTARTICULATIONUSINGREALTIMEMAGNETICRESONANCEIMAGING HOWING 1999 59 67 F KIM 2014 EL115 EL121 J LAMMERT 2010 1572 1575 A PROCINTERSPEECH DATADRIVENANALYSISREALTIMEVOCALTRACTMRIUSINGCORRELATEDIMAGEREGIONS LAPRIE 2014 245 248 Y PROC10THINTSEMINARSPEECHPRODUCTIONISSP STUDYINGMRIACQUISITIONPROTOCOLSSUSTAINEDSOUNDSAMULTIMODALACQUISITIONSYSTEM LOVATTO 2007 549 552 L PROC16THINTCONGRESSPHONETICSCIENCESICPHS AFIBERSCOPICANALYSISNASALVOWELSINBRAZILIANPORTUGUESE MARTINS 2008 925 952 P MARTINS 2010 33 36 P PROCFALA ARTICULATORYCHARACTERISTICSEUROPEANPORTUGUESELATERALSA2D3DMRISTUDY MARTINS 2011 P PROCIADISINTLCONFERENCESCOMPUTERGRAPHICSVISUALIZATIONCOMPUTERVISIONIMAGEPROCESSING TONGUESEGMENTATIONSMRIIMAGESUSINGITKSNAPPRELIMINARYEVALUATION MARTINS 2012 231 240 P PROCIBERSPEECH2012VIIJORNADASENTECNOLOGIADELHABLAIIIIBERIANSLTECHWORKSHOP VELARMOVEMENTINEUROPEANPORTUGUESENASALVOWELS MILLER 2014 N NAM 2015 H TADATASKDYNAMICAPPLICATION NIEBERGALL 2013 477 485 A OLIVEIRA 2012 2690 2693 C PROCINTERSPEECH MRISTUDYORALARTICULATIONEUROPEANPORTUGUESENASALVOWELS OLIVEIRA 2009 480 483 C PROCINTERSPEECH SPEECHRATEEFFECTSEUROPEANPORTUGUESENASALVOWELS PRATT 2007 W DIGITALIMAGEPROCESSING PROCTOR 2010 1576 1579 M PROCINTERSPEECH RAPIDSEMIAUTOMATICSEGMENTATIONREALTIMEMAGNETICRESONANCEIMAGESFORPARAMETRICVOCALTRACTANALYSIS PROCTOR 2012 113 116 M PROCSPEECHSCITECH ARTICULATIONMANDARINSIBILANTSAMULTIPLANEREALTIMEMRISTUDY PROCTOR 2011 281 284 M PROCINTERSPEECH DIRECTESTIMATIONARTICULATORYKINEMATICSREALTIMEMAGNETICRESONANCEIMAGESEQUENCES RAMANARAYANAN 2010 1994 1997 V PROCINTERSPEECH INVESTIGATINGARTICULATORYSETTINGPAUSESREADYPOSITIONRESTUSINGREALTIMEMRI RAMANARAYANAN 2013 510 519 V SALTZMAN 1989 333 382 E SCOTT 2014 A SHADLE 2008 C PROCACOUSTICS08PARISJOINTMEETINGASAEAASOCIETEFRANCAISEDACOUSTIQUE MRISTUDYEFFECTVOWELCONTEXTENGLISHFRICATIVES SHOSTED 2012 2182 2185 R PROCINTERSPEECH USINGMAGNETICRESONANCEIMAGEPHARYNXDURINGARABICSPEECHSTATICDYNAMICASPECTS SILVA 2014 901703 S PROCSPIEVISUALIZATIONDATAANALYSIS AFRAMEWORKFORANALYSISUPPERAIRWAYREALTIMEMRISEQUENCES SILVA 2014 399 402 S PROCINTSEMINARSPEECHPRODUCTIONISSP RTMRIBASEDDYNAMICANALYSISVOCALTRACTCONFIGURATIONSPRELIMINARYWORKREGARDINGINTRAINTERSOUNDVARIABILITY SILVA 2014 403 406 S PROCINTSEMINARSPEECHPRODUCTIONISSP SYSTEMATICQUANTITATIVEANALYSISVOCALTRACTDATAINTRAINTERSPEAKERANALYSIS SILVA 2015 25 46 S SILVA 2013 459 466 S PROCICIAR2013LNCSVOL7950 SEGMENTATIONANALYSISVOCALTRACTMIDSAGITTALREALTIMEMRI SILVA 2013 1307 1311 S PROCINTERSPEECH TOWARDSASYSTEMATICQUANTITATIVEANALYSISVOCALTRACTDATA SMITH 2014 413 416 C PROCISSP COMPLEXTONGUESHAPINGINLATERALLIQUIDPRODUCTIONWITHOUTCONSTRICTIONBASEDGOALS TEIXEIRA 2012 306 317 A PROCPROPOR2012LNCSVOL7243 REALTIMEMRIFORPORTUGUESEDATABASEMETHODSAPPLICATIONS TEIXEIRA 2011 243 250 A PROC9THINTSEMINARSPEECHPRODUCTIONISSP MRISTUDYCONSONANTALCOARTICULATIONRESISTANCEINPORTUGUESE TEIXEIRA 2002 31 34 A PROCIEEEWORKSHOPSPEECHSYNTHESIS SAPWINDOWSTOWARDSAVERSATILEMODULARARTICULATORYSYNTHESIZER TEIXEIRA 1999 2557 2560 A PROCINTCONGRESSPHONETICSCIENCESICPHS INFLUENCEDYNAMICSINPERCEIVEDNATURALNESSPORTUGUESENASALVOWELS TIEDE 2000 25 28 M PROC5THSEMINARSPEECHPRODUCTIONISSP CONTRASTSINSPEECHARTICULATIONOBSERVEDINSITTINGSUPINECONDITIONS WANG 2011 Y SMOOTHINGSPLINESMETHODSAPPLICATIONS ZHARKOVA 2009 248 256 N ZHARKOVA 2011 118 140 N ZHARKOVA 2011 374 388 N ZHU 2013 1292 1296 Y PROCINTERSPEECH FASTER3DVOCALTRACTREALTIMEMRIUSINGCONSTRAINEDRECONSTRUCTION SILVAX2016X307 SILVAX2016X307X329 SILVAX2016X307XS SILVAX2016X307X329XS CHU_DOE publishAcceptedManuscriptIndexable 2016-12-08T00:00:00Z UnderEmbargo 2016-12-08T00:00:00Z item S0885-2308(15)00054-6 S0885230815000546 10.1016/j.csl.2015.05.004 272453 2020-10-25T02:43:49.897481Z 2016-03-01 2016-03-31 true 5938618 MAIN 23 56189 849 656 IMAGE-WEB-PDF 1 fx1 3233 56 219 fx2 2089 24 219 fx3 2981 69 203 fx4 1794 56 37 gr1 16527 164 129 gr10 20330 153 219 gr11 17693 75 219 gr12 13336 73 219 gr13 20999 164 192 gr14 23960 164 199 gr2 15498 123 219 gr3 21128 164 218 gr4 15474 105 219 gr5 19556 164 167 gr6 14428 145 219 gr7 26260 164 160 gr8 32878 143 219 gr9 15421 76 219 fx1 1134 19 76 fx2 2539 19 170 fx3 829 15 45 fx4 393 12 8 gr1 89059 620 489 gr10 141554 499 713 gr11 95587 243 713 gr12 80268 238 713 gr13 119153 476 558 gr14 122181 456 553 gr2 106795 411 732 gr3 83525 396 527 gr4 46248 234 489 gr5 102010 583 595 gr6 39535 261 395 gr7 226518 730 713 gr8 78945 365 558 gr9 78406 249 713 si1 152 14 16 si10 1249 40 279 si11 1773 43 306 si12 1086 38 211 si2 176 15 18 si3 924 16 280 si4 644 15 245 si5 1135 48 321 si6 701 20 251 si7 1741 30 431 si8 1143 16 343 si9 773 49 135 Samuel_Silva_quantitative_comparison_2015 5158083 Samuel_Silva_quantitative_comparison_2015_P01 524800 01 Samuel_Silva_quantitative_comparison_2015_P02 612705 02 Samuel_Silva_quantitative_comparison_2015_P03 616551 03 Samuel_Silva_quantitative_comparison_2015_P04 707321 04 Samuel_Silva_quantitative_comparison_2015_P05 733536 05 Samuel_Silva_quantitative_comparison_2015_P06 651903 06 Samuel_Silva_quantitative_comparison_2015_P07 307439 07 Samuel_Silva_quantitative_comparison_2015_P08 643667 08 Samuel_Silva_quantitative_comparison_2015_P09 541157 09 Samuel_Silva_quantitative_comparison_2015_P10 449891 10 Samuel_Silva_quantitative_comparison_2015_P11 579634 11 Samuel_Silva_quantitative_comparison_2015_P12 577739 12 Samuel_Silva_quantitative_comparison_2015_P13 610263 13 Samuel_Silva_quantitative_comparison_2015_P14 257027 14 Samuel_Silva_quantitative_comparison_2015_P15 585514 15 Samuel_Silva_quantitative_comparison_2015_P16 521898 16 Samuel_Silva_quantitative_comparison_2015_P17 591985 17 Samuel_Silva_quantitative_comparison_2015_P18 343480 18 Samuel_Silva_quantitative_comparison_2015_P19 641547 19 Samuel_Silva_quantitative_comparison_2015_P20 647612 20 Samuel_Silva_quantitative_comparison_2015_P21 511358 21 Samuel_Silva_quantitative_comparison_2015_P22 625422 22 Samuel_Silva_quantitative_comparison_2015_P23 500351 23 Samuel_Silva_quantitative_comparison_2015_P24 698632 24 Samuel_Silva_quantitative_comparison_2015_P25 613651 25 Samuel_Silva_quantitative_comparison_2015_P26 507531 26 Samuel_Silva_quantitative_comparison_2015_P27 554649 27 Samuel_Silva_quantitative_comparison_2015_P28 549451 28 Samuel_Silva_quantitative_comparison_2015_P29 595354 29 Samuel_Silva_quantitative_comparison_2015_P30 519552 30 Samuel_Silva_quantitative_comparison_2015_P31 527787 31 Samuel_Silva_quantitative_comparison_2015_P32 724763 32 Samuel_Silva_quantitative_comparison_2015_P33 691255 33 Samuel_Silva_quantitative_comparison_2015_P34 716152 34 Samuel_Silva_quantitative_comparison_2015_P35 510220 35 Samuel_Silva_quantitative_comparison_2015_P36 533221 36 Samuel_Silva_quantitative_comparison_2015_P37 541984 37 Samuel_Silva_quantitative_comparison_2015_P38 521645 38 Samuel_Silva_quantitative_comparison_2015_P39 557884 39 Samuel_Silva_quantitative_comparison_2015_P40 566140 40 Samuel_Silva_quantitative_comparison_2015_P41 184026 41 am 5211457 YCSLA 725 S0885-2308(15)00054-6 10.1016/j.csl.2015.05.004 Elsevier Ltd Fig. 1 Pipeline depicting the main steps involved in performing speech production analysis based on image data. At the centre, in more detail, the analysis step, the focus of this work. Fig. 2 Examples of contour superpositions used to assess the differences between the configurations assumed by the vocal tract to produce various sounds. Fig. 3 Illustration of the different aspects involved in computing the difference between two vocal tract profiles. Please refer to the text for additional details on how the different features are computed. Fig. 4 Comparison between two sounds taking into account existing repetitions. The contours are cross-compared between the sounds and the average difference, D ¯ , and standard deviation, σ, computed. Fig. 5 Different steps involved in building the abstract representation for comparison data: (a) different orientations are considered for the chosen features; (b) each feature is assigned an orientation over the unitary circle; (c) the value computed for each feature is represented by a dot along the corresponding orientation; (d) circular coronas are added to easy interpretation; (e) the different dots are connected by lines, forming a polygon; (f) for dots in the yellow and red regions, directional information is added; and (g) abstract representation and corresponding vocal tract contours side-by-side. Fig. 6 Aligning multiple realisations (or utterances) by normalising duration. The duration of one realisation is used as reference and the corresponding image frames interpolated to obtain a fixed set of evenly distributed frames. The second realisation is compressed/stretched to the duration of the first and a similar interpolation procedure is applied enabling common time points for comparison. Fig. 7 Extension of the comparison framework to cover vocal tract differences over time. Multiple realisations of [ e ˜ ] and [ɛ] are aligned by normalising duration. At 10 points in time the vocal tract configurations are cross-compared between vowels resulting in comparison data along the vowels’ duration. At the bottom, the comparison values obtained for each feature (e.g., velum (VEL), tongue tip (TT)) are presented, over time, using a line plot. Fig. 8 Sample images extracted from RT-MRI database acquired to study European Portuguese oral and nasal vowels and depicting the vocal tract contours. Fig. 9 Vowel production variability per speaker obtained by cross-comparing all occurrences available. From left to right: [a] (speaker CM), [ũ] (speaker CO) and [ɛ] (speaker SV). Fig. 10 Comparison between the vocal tract configurations for two vowels for each speaker. The top row shows the considered vocal tract contours and the bottom row shows the corresponding difference diagram. From left to right: [ĩ] vs [i] (speaker CM), [ũ] vs [u] (speaker CO) and [ɛ] vs [e] (speaker SV). Fig. 11 Inter-speaker comparison of differences between vowels. From left to right: [a] vs [i], [a] vs [u] and [õ] vs [O]. Fig. 12 Differences among vowel pairs computed using data from all speakers. From left to right: [a][i][u], and [ũ][u]. Fig. 13 Analysis of variation over time among utterances, along the production of [ ], for speaker CM (top) and [ũ] for all speakers (bottom). Fig. 14 Comparison between the productions of [ e ˜ ] and [ɛ] (top) and [ũ] and [ĩ] (bottom). Analysis performed considering data from all speakers. Table 1 Features considered to compare vocal tract profiles, abbreviated name and vocal tract segments used to compute them. Feature Notation Feature context (F′) Tongue back F TB Tongue back Tongue dorsum F TD Tongue dorsum Velum position F VEL Velum Tongue tip position F TT Tongue tip, alveolar ridge Lip protrusion F LP Upper lip, lower lip, alveolar ridge Lip aperture F LA Upper lip, lower lip, alveolar ridge Pharynx F Ph Pharynx ☆ This paper has been recommended for acceptance by Shrikanth Narayanan. Quantitative systematic analysis of vocal tract data Samuel Silva a b ⁎ António Teixeira a b a Dep. Electronics, Telecommunications and Informatics (DETI), University of Aveiro, Aveiro, Portugal Dep. Electronics, Telecommunications and Informatics (DETI), University of Aveiro Aveiro Portugal b Inst. Electronics Eng. and Telematics of Aveiro (IEETA), University of Aveiro, Aveiro, Portugal Inst. Electronics Eng. and Telematics of Aveiro (IEETA), University of Aveiro Aveiro Portugal ⁎ Corresponding author at: IEETA, Campus Univ. de Santiago, 3810-193 Aveiro, Portugal. Tel.: +351 234370500. Articulatory data can nowadays be obtained using a wide range of techniques, with a notable emphasis on imaging modalities such as ultrasound and real-time magnetic resonance, resulting in large amounts of image data. One of the major challenges posed by these large datasets concerns how they can be efficiently analysed to extract relevant information to support speech production studies. Traditional approaches, including the superposition of vocal tract profiles, provide only a qualitative characterisation of notable properties and differences. While providing valuable information, these methods are rather inefficient and inherently subjective. Therefore, analysis must evolve towards a more automated, replicable and quantitative approach. To address these issues we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative normalised data regarding differences covering meaningful regions under the influence of various articulators. An important part of the framework is the visual representation of the data, proposed to support analysis, and depicting the differences found and corresponding direction of change. The normalised nature of the computed data allows comparison among different sounds and speakers in a common representation. Representative application examples, concerning the articulatory characterisation of European Portuguese vowels, are presented to illustrate the capabilities of the proposed framework, both for static configurations and the assessment of dynamic aspects during speech production. Keywords Vocal tract analysis Quantitative comparison RT-MRI 1 Introduction Speech production studies are currently served by a wide range of technologies that allow research on the dynamic aspects of speech. Methods such as ultrasound (US) and real-time magnetic resonance imaging (RT-MRI) (Scott et al., 2014) provide data regarding the position and coordination of the different articulators over time (Hagedorn et al., 2011). Furthermore, they offer the possibility to improve on the studies based on information regarding a static sustained production by reducing the hyperarticulation effect (Engwall, 2003). After image acquisition, the different regions of interest must be segmented (e.g., Bresch and Narayanan, 2009; Silva and Teixeira, 2015), or points of interest identified, often resulting in contours delimiting the vocal tract or specific structures such as the tongue or velum. Analysis of different vocal tract contours is typically performed visually by characterising the position of the different articulators or by describing articulator differences between different sounds (e.g., Delvaux et al., 2002; Shadle et al., 2008). This is often done by superimposing contours and performing qualitative analysis of the main differences (Martins et al., 2008; Cleland et al., 2011; Badin et al., 2014). Adding to the subjective nature of such analysis, when the database is large, e.g., as happens when RT-MRI is used (Niebergall et al., 2013), it becomes an almost infeasible task to explore all available data. 1.1 Challenges Beyond the sheer amount of data made available by current technologies, the field of speech production faces several challenges that should be addressed to allow further advances, harnessing the full potential of the data available. A framework should be proposed that tackles the large amounts of data addressing, among others, the following aspects: • Objectivity — The subjective nature of the methods used to describe articulatory differences, for example, results in variability among researchers that precludes true comparison among works in the literature describing the same phenomena. • Intra-speaker assessment — The analysis of articulatory features for different sounds produced by one speaker lacks methods to profit from multiple repetitions and common grounds for comparison among sounds. • Inter-speaker assessment — The comparison among speakers lacks common grounds for comparison, e.g., a common normalised measure of difference, without losing sight of the contributions provided by each articulator. • Variability — Not only average behaviour is relevant for the researchers, there is also a strong need to have information on variability (across repetitions, across speakers, etc.). • Inter-language comparison — Data from multiple speakers of one language could be jointly used to provide overall quantitative characterisation of its main features. This would allow new ways of comparing sounds in multiple languages, advancing on the current status of inter-language comparisons mostly based on qualitative assessment of data from different speakers. • Multimodality — Data provided by different modalities and concerning similar phenomena or providing complementary data might benefit from joint analysis. For example, several technologies that support speech production studies (e.g., EMA, Kim et al., 2014 and ultrasound, Laprie et al., 2014) are used in combination with MRI (Scott et al., 2014). Regardless of how the different data is analysed, if their individual contributions to the understanding of specific phenomena could be gathered in joint representations it might motivate a generalisation of multimodal studies and an easier interpretation of the data. One important route to attain a systematic analysis addressing these issues is to move towards quantitative methods that allow it to be performed automatically, in an expedite and replicable way, resulting in data providing a summary of the most important features which researchers can analyse. In the work presented here we consider these challenges in the scope of real-time MRI data. 1.2 Related work Considering that we are mostly addressing vocal tract data analysis from vocal tract profiles extracted from image data (e.g., RT-MRI), in this overview of related work we focus on methods applied to image data of the vocal tract, or to full (or partial) vocal tract profiles extracted from them. The rationale is to overview notable recent literature that describes some level of quantification regarding the analysis and comparison of the data extracted from these images. Therefore, works supported on subjective analysis based, for example, on visual assessment of articulatory configuration differences, are not covered. Regarding pixel-based methods, i.e., without an explicit extraction of vocal tract profiles, notable works include the detection of constriction regions along the tract (Lammert et al., 2010; Hagedorn et al., 2011) and estimation of articulator trajectories (Proctor et al., 2011; Shosted et al., 2012) contributing to improved analysis of articulatory configurations, articulatory gestures and their coordination. Other authors have performed quantitative comparison of specific regions/articulators, the most common being the tongue. Davidson (2006) compare tongue contours using smoothing splines ANOVA (Wang, 2011). The use of Bayesian confidence intervals allows a distinction of which parts of the tongue contour are statistically different, but fails to quantify the extent of the difference between the curves (Zharkova and Hewlett, 2009). In Teixeira et al. (2011) tongue contours are compared using the Pratt index (Pratt, 2007). In the work by Zharkova et al. (2014) tongue contours are compared by computing the Euclidean distances between each point in one contour to the nearest point in the other and vice-versa and averaging the sum of all computed distances. When considering the whole vocal tract, Proctor et al. (2010) and Shosted et al. (2012), for example, have extracted distance functions, which might not be a clear way to detect which articulator has the most influence. On a different level, Miller et al. (2014) gather vocal tract configuration data (also including the cervical spine and airway), from multiple speakers, by building an active shape model (ASM). Through the identification of independent modes of variation, and selecting those that account for the most part of the explained variance, the authors study coordinated changes affecting the cervical spine, airway and vocal structures. One interesting idea to note concerns gathering the data for all speakers to reach a quantitative measure of mean behaviour and variability, which allows the characterisation of the articulatory space defined by the vocal tract configurations used for training. Comparison among vocal tract profiles might eventually be performed based on the weights of each of the modes of variation, but with the inherent difficulties in isolating the contribution of individual articulators. Furthermore, the resulting ASM model depends on the images used to train it and, therefore, a comparison cannot be performed with weights obtained for models resulting from different training sets. Ramanarayanan et al. (2010, 2013) have proposed a set of vocal tract area descriptors, analogous to the distance functions, by dividing the vocal tract in clearly defined regions considering lip and velum aperture and the constriction locations at tongue tip, dorsum and root. The authors compute the area contained in the vocal tract regions delimited by the above locations and use these area descriptors to compare among vocal tract postures (Benítez et al., 2014). While it does not exactly provide information on specific articulators it allows a slightly better grasp of overall localisation than with distance functions. On the subject of inter-speaker or inter-language comparison and regarding specific articulators, Zharkova et al. (2011) normalised measures performed over tongue contours (to compare between pre-adolescents and adults) by computing the tongue's length for specific tongue postures. To allow direct comparison among different speakers, accounting for the morphological differences, vocal tract normalisation has also been proposed by several authors. A common approach is the normalisation of vocal tract length and the presentation of distance functions (e.g., Martins et al., 2008; Proctor et al., 2010). As alluded before, the use of distance functions, even though providing a good overall idea of the differences among speakers does not explicitly address particular articulators. A normalisation of area descriptors of the vocal tract has also been performed, e.g., by Benítez et al. (2014), with advantages over distance functions, due to an association between each descriptor and a region of the vocal tract. Badin et al. (2014) use articulatory modelling to compare between classes of sounds produced by one speaker for two languages. By using data from vowels, analogous consonants and the full corpus, obtained using MRI, articulatory models are created. While this provides a quantitative method to gather the different data and characterise the articulatory space for each class of sounds, involving a selection of the variation modes accounting for a large percentage of the explained variance, the comparison is still performed by varying model parameters, in specific ranges, and visually comparing the resulting model configuration. The authors propose to evaluate how the articulatory spaces from both languages overlap by using the models built for one language to reconstruct the articulations collected for the other. Nevertheless, it is still not clear how this procedure can be used to draw any conclusion since the models are built for more than one sound and are, therefore, naturally capable of encompassing some variability. Overall, while several methods have been proposed to support quantitative analysis and comparison of vocal tract data, there are no clear approaches to systematically handling the large amounts of data available from imaging technologies, such as RT-MRI, to take advantage of multiple realisations of each sound and explicitly supporting the comparison within and among speakers and languages. 1.3 Contributions and overview The pipeline in Fig. 1 shows the main steps associated with image-based speech production studies, taking into consideration what might be at stake to address the challenges identified above, comprising: data acquisition (e.g., using RT-MRI); feature extraction such as vocal tract segmentation (e.g., Bresch and Narayanan, 2009; Silva and Teixeira, 2015); comparison of features between vocal tract profiles; and an additional step regarding computational analysis. This last step can build on the data computed for vocal tract configuration differences, apply more advanced analysis methods, such as cluster analysis, and will depend on the nature of the data made available by the preceding step. Considering the literature surveyed above, and to the best of our knowledge, no method has been proposed to support a more complete vocal tract profile comparison, providing meaningful data regarding the regions under the influence of the different articulators and addressing the different challenges posed to current speech production studies. To address these aspects, dealing with the analysis of such data, as depicted at the centre of the pipeline presented in Fig. 1, and following on work presented in (Silva et al., 2013b; Silva and Teixeira, 2014b,c), we propose a framework which, among others, provides the following notable features: • Quantitative comparison of vocal tract profiles considering the regions under the influence of different articulators. • Normalised differences allowing intra- and inter-speaker comparisons. • Analysis of dynamic aspects of speech production by gathering data from multiple speakers and considering multiple realisations for each sound or utterance. • Abstract representation to support analysis and comparison of the computed difference data. To illustrate, on a real scenario, the potential and different applications made available by the proposed methods, examples are presented dealing with data from an RT-MRI dataset collected to study the nasal vowels in European Portuguese (Teixeira et al., 2012). The remainder of this paper is organised as follows: Section 2 provides a description of the proposed methods for vocal tract comparison; Section 3 presents the abstract representation built to depict the comparison data; Section 4 extends the vocal tract comparison method to the analysis of dynamic aspects of speech production; Section 5 describes the application of the proposed methods to data gathered in an RT-MRI study involving European Portuguese (EP) oral and nasal vowels, providing a brief illustration of their application in a real scenario; Section 6 briefly discusses the work carried out; finally, Section 7 presents conclusions and ideas for further work. 2 Vocal tract configuration comparison Vocal tract comparison is traditionally performed to assess articulatory differences among sounds and speakers. While the analysis for each speaker is usually performed by superimposing vocal tract profiles, the comparison among speakers is performed by comparing these superpositions side-by-side: since the anatomy for each speaker is different it is not possible to extract meaningful data by superposing vocal tract contours from different speakers. Fig. 2 presents some examples of different pairs of superimposed contours for different speakers and sounds. Notice, for example, the subjectivity inherent to the judgement of what is a meaningful (or comparably large) difference and the difficulty to apply the same criteria when the analysis is performed by different researchers (e.g., in different studies). The framework to support quantitative analysis of differences among vocal tract profiles (centre blocks in Fig. 1) is grounded on three main blocks: • Static analysis, dealing with the comparison between vocal tract profiles deemed representative of the configurations assumed to produce different sounds (e.g., representative image frames of the vocal tract configuration for [a], [ĩ] or [u]). • Dynamic analysis, dealing with the comparison between the configurations assumed by the vocal tract along utterances and conveying the differences along time. • Visual representation, concerning the depiction of the quantitative data computed in the analysis blocks using abstract representations to support interpretation of the data and dissemination. Vocal tract comparison occupies a central role in the proposed analysis framework (Fig. 1) as it provides the grounds for all the provided analysis features. In what follows we set the requirements for a vocal tract comparison method, describe the adopted terminology, discuss a first set of considered regions of interest and define the normalised measures proposed to compare them between vocal tract profiles. 2.1 Requirements and definitions To tackle the challenges faced by speech production studies, discussed in the introduction, the vocal tract comparison method must go beyond current practice towards an objective measure of difference that can be systematically applied to the increasingly large real-time datasets. As a first set of requirements, the differences between vocal tract profiles should be: • Obtained using objective, comparable measures. • Computed for the different anatomical regions of interest and/or vocal tract features to provide a meaningful regional measure of difference. • Normalised to allow comparison among different speakers and languages. • Informative on how the difference occurred (e.g., in what direction did the tongue back move). • Represented visually to help users understand the resulting data and support results dissemination. Before advancing to a detailed definition of the comparison method, and in order to set a terminology to be used throughout the remainder of this article, let us consider each vocal tract contour, C i , defined as a set of points in 2D: (1) C i = ( x i 1 , y i 1 ) , ( x i 2 , y i 2 ) , … , ( x iM , y iM ) , where M is the number of points (x, y) in the contour and can vary among contours. For each contour, a fixed number of segments of interest, N S , can be considered (2) C i = { S i 1 , S i 2 , … , S iN S } , N S ≤ M , where each segment is defined as a subset of contour points of size 1< l s < M, as given by (3) S s = [ ( x w s , y w s ) ( x ( w s + l s ) , y ( w s + l s ) ] , w s = ∑ i = 1 s − 1 l i . Each segment of interest identifies a relevant part of the vocal tract such as the tongue, the hard palate or the degenerate case of a single point (landmark). After identification of the segments of interest, a set of N F features to consider for comparison between vocal tract profiles can be chosen. A feature can be associated to a particular segment or can rely on multiple segments for its computation. For example, lip aperture needs the segments corresponding to both lips. The feature context (F′), i.e., the relevant data that is needed to compute and compare each feature between vocal tract profiles, can then be defined based on the segments of interest. As remarked, each feature might depend on a variable number of segments: (4) F f ′ = { S ia , S ib , … } , a , b ∈ [ 1 … N S ] The difference between two contours, C i and C j , can be expressed as (5) D ( C i , C j ) = F 1 ( F i 1 ′ , F j 1 ′ ) , F 2 ( F i 2 ′ , F j 2 ′ ) , … , F N F ( F iN F ′ , F jN F ′ ) , with 0≤ Ff (A, B)≤1 denoting the normalised difference computed for feature f and defined individually according to the feature characteristics. 2.2 Segments and features of interest At this stage, our purpose was to provide a first set of features that could cover the whole vocal tract. To select the segments and features considered for comparing vocal tract profiles we considered the speech production studies literature and the relevance given to the different articulators and landmarks. Considering the vocal tract, the literature (e.g., Bresch and Narayanan, 2009) identifies multiple anatomical regions with relevance for speech production, namely: upper lip, lower lip, tongue, tongue tip, velum, hard palate, pharyngeal wall, epiglottis and glottis. Regarding relevant features extracted from the vocal tract, notable examples in the literature (Saltzman and Munhall, 1989; Gick et al., 2004; Bresch et al., 2010; Niebergall et al., 2013) include: lip protrusion; lip aperture; tongue tip constriction degree, defined by the distance from the tongue tip to the alveolar ridge; tongue dorsum constriction location and tongue dorsum constriction degree, based on the point of minimum distance between the tongue dorsum and the hard palate; tongue root constriction degree, obtained from the distance between the tongue root and the pharyngeal wall; and velar aperture. The literature regarding the characterisation of articulatory differences (typically from contours) also provides useful information regarding which aspects are the most important in vocal tract configuration characterisation: tongue height and backness, lip aperture and protrusion and velum height. Considering the different features described in the literature we aimed to choose a set that would cover the whole vocal tract and provide data regarding the aspects described and used the most, by researchers, to differentiate among vocal tract configurations. In this first instantiation of the framework, the considered features for comparison are: tongue back differences, tongue dorsum differences, velum differences, tongue tip position variation, lip protrusion variation, lip aperture variation and pharynx configuration differences. Table 1 presents a summary of the considered features, the adopted notation and segments considered as part of their feature context. The alveolar ridge is included in the context of various features because it is used for normalisation purposes, as explained ahead. The difference between two vocal tract profiles can, therefore, be expressed as a features vector: (6) D ( C i , C j ) = F TB , F VEL , F TD , F TT , F LP , F LA , F Ph It is important to make it clear that in the context of our work we are not currently dealing with intrinsic features, i.e., features that are defined only considering one vocal tract profile. All features should be understood in the context of a comparison between two vocal tract profiles. For example, tongue backness, F TB , is not an absolute measure of the tongue back position towards the pharyngeal wall, but the comparison between the position of the tongue backs in two vocal tract configurations. Please also note that the consideration of this set of features should not be understood as a restriction of the framework to it. By definition, this set is extendible to other features, as long as they abide to the requirements described in Section 2.1. The details regarding how each of the selected features is computed are provided in the following section. 2.3 Comparison methods When performing the comparison for the multiple features defined above, three different types of comparisons need to be performed: (1) between segments of the vocal tract (e.g., tongue dorsum); (2) between landmarks (e.g., tongue tip); or (3) between distances (e.g., lip aperture). Therefore, for each particular situation, a different comparison approach is needed. It should be noted that comparison measures are always computed between vocal tracts of the same speaker, considering they are already aligned, e.g., as a result of the acquisition procedure, or post acquisition alignment. Fig. 3 illustrates the different vocal tract regions that need to be compared and relevant aspects of their computation. The pharynx (F Ph ), tongue back (F TB ), tongue dorsum (F TD ) and velum (F VEL ) are part of the features that involve the comparison between contour segments. These are compared by computing the Pratt index for each pair of corresponding contours and given by Pratt (2007): P = 1 N ∑ i = 1 N 1 1 + α d i 2 , where N is the number of corresponding points between the compared contour segments (e.g., tongue back), d i is the Euclidean distance between two corresponding points, and α is a constant set to 1/9, based on Pratt's work and similar works in the literature (Pratt, 2007). At this stage, the same constant value has been used for all regions, but it might be tuned for each region if different sensibilities to differences are desired. To obtain corresponding points between segments, the segment with the smallest number of points is selected and, for each point, the closest point in the other segment is considered the corresponding point. The Pratt's figure of merit provides values in the range ]0, 1] where 1 is attained when there are no differences between the contour segments. The tongue tip position variation, F TT , is the only feature that deals with the comparison of two landmarks. Comparison is performed by computing the distance between the tongue tips (d TT ), in both contours, and normalised by the longest distance from each tongue tip to the alveolar ridge (AR) between the two contours. Fig. 3 depicts the values considered and used as follows: F TT = 1.0 − d TT max ( dA ( TT – AR ) , dB ( TT – AR ) ) . The remaining two features belong to the third type and deal with the comparison between two distances that are first computed from each of the vocal tracts. Lip protrusion, F LP , is obtained (refer to Fig. 3) by computing the horizontal displacement of the mid-point between the upper and lower lips, MPL. The mid-point is computed considering the line that connects the lowest point of the upper lip with the highest point of the lower lip. To perform normalisation, the horizontal distance between the mid-points between the lips and the alveolar ridge (AR) is obtained (dA (MPL–AR) and dB (MPL–AR)) and used as follows (Fig. 3): F LP = 1.0 − dB ( MPL – AR ) − dA ( MPL – AR ) max ( dA ( MPL – AR ) , dB ( MPL – AR ) ) . Lip aperture, F LA , is computed based on the lip aperture values for both vocal tract profiles (LA A , LA B ) and is normalised by considering the longest of the two: F LA = 1.0 − LA B − LA A max ( LA A , LA B ) . The features described above still do not provide any information regarding the direction in which the difference occurred (e.g., did the tongue tip go up or down). For each of the features the direction can be computed as the vector characterising the movement of the centre of gravity of the feature context (excluding the alveolar ridge, where present) or the difference between notable points (e.g., the midpoint between the lips in F LP . The obtained directional information is processed to provide only the component of interest. For example, for the tongue back there is no particular interest in the vertical component of displacement and the opposite happens for the tongue dorsum for which the horizontal displacement, if exists, has no major relevance. 2.4 Taking advantage of repeated utterances It is common that the corpora acquired during speech production studies include repetitions of each sound considered. Therefore, analysis should consider these repetitions when comparing between sounds by cross comparing all occurrences of each sound. Fig. 4 illustrates the comparison between two sounds. The relevant repetitions of each sound are considered and cross compared using the methods previously described. From each of these comparisons results a set of difference values covering each of the considered regions. The average values are obtained along with the standard deviation and used as representative of the comparison between the two sounds. 3 Visual representations The quantitative comparison method presented above, resulting in a set of numerical values, demands a visualisation method that can help users to interpret/compare and disseminate them in an easy way as opposed, for example, to value tables. The adoption of a visual representation for the data, to improve how it can be read, should not be understood as the ’end of the line’ for the gathered numerical data. It will serve to provide users with an insight over the numbers, but additional methods (e.g., cluster analysis) can be applied to further explore them as depicted in the pipeline of Fig. 1. One possible representation would be to present the values in a line plot, with the parameters shown in a fixed order, in the X-axis. Nevertheless, we considered that a different representation might bring easier interpretation if, somehow, one could build some analogy with the vocal tract and the position of the different articulators. Fig. 5 illustrates the construction of the proposed representation. The computed comparison values are represented over the unitary circle in a radar-like representation and its design is grounded in the ideas described in what follows. 3.1 Parameter location First of all, we consider the traditionally adopted orientation for speech production images with the lips presented to the left (e.g., Höwing et al., 1999; Ramanarayanan et al., 2013). The location of each computed parameter in the representation is chosen as if the vocal tract was inscribed in the unitary circle and selecting, at each time, the parameter concerning the closest region or articulator. Furthermore, the orientation associated with each value is, where applicable/possible, related with its movement direction (Niebergall et al., 2013). Accounting for these criteria, tongue back differences, for example, should be on the opposite side regarding lip protrusion. Starting at zero degrees, for the tongue back (F TB ), it follows the velum (F VEL ), tongue dorsum (F TD ), tongue tip (F TT ), lip protrusion (F LP ), lip aperture (F LA ) and pharynx (F Ph ). In the case of lip aperture, the direction used in the representation is not directly related with the direction of the expected difference. Notice, however, that considering the two parameters associated with the lips, F LP and F LA , the directions adopted for the pair resemble an open mouth, with F LA as the jaw. Each feature name is presented on the exterior of the circle close to its corresponding orientation. 3.2 Value representation The values obtained for each of the considered features are represented over the corresponding direction in the unitary circle using a dot. Since it is the unitary circle, with the origin at its centre, the closest the dot is to the centre, the lowest the value it represents. As the represented value gets closer to 1.0, the dot gets closer to the circumference of the unitary circle. 3.3 Difference level To help on the interpretation of the presented values, three circular coronas are proposed. The first, between 0.75 and 1.0, in green, is suggested to mean no relevant difference. The interval lower bound is chosen based on the value typically interpreted as meaning a good match between contours compared using the Pratt index. The second circular corona, between 0.50 and 0.75 is proposed to be interpreted as a mild difference and the last, from 0.0 to 0.50, as a strong difference. These are proposed interpretations and different criteria can be adopted, but their intent is to help users to have common interpretation criteria and to move away from direct numerical comparison which would lead, yet again, to a rather subjective assessment. Furthermore, the intent is also not to have too much difference levels. 3.4 Overall interpretation As a first approach, trying to provide easier analysis of the proposed representation, namely a faster perception of each point location, and a simple way of “grouping” the points belonging to the same comparison, when multiple comparison data is shown, the points are connected using line segments, forming a polygon (hereafter know as the difference polygon). This should not be interpreted as implying anatomical connection or proximity between articulators, but as a way to ease visualisation. The difference polygon also allows reducing the size of the value dots, which leaves more space in the representation for adding other features, in the future. 3.5 Directional information One important aspect regarding the differences found when comparing vocal tract profiles is to characterise how that difference occurred from one configuration to the other, e.g., by a rise in the tongue dorsum or lowering of the velum. To provide such information, a vector is used, with origin in the parameter value and the direction of displacement. For the sake of simplicity, displacement direction is only represented for values corresponding to mild or strong differences (i.e., <0.75, according to the proposed interpretation thresholds). If the difference for a particular feature is deemed irrelevant (green circular corona), then representing a direction would have no meaning. Furthermore, it helps to highlight features depicting notable differences. 3.6 Variability Since differences are often computed for multiple realisations of two sounds, and the average difference represented, it is also important to depict the variability associated with each value. To this end, several alternatives were tested such as bars and/or lines associated to each represented value, depicting data as in a boxplot, or a representation of dispersion outside the unitary circle. These tended to clutter the representation and became problematic if more that one difference polygon was represented. Therefore, to provide variability data, the standard deviation for each parameter is represented over its direction and all parameter points used to draw an orange polygon (hereafter known as the deviation polygon) at the centre of the representation (Fig. 5g). 4 Support for analysis of dynamic aspects The previous sections proposed a method to compare between vocal tract contours and an abstract representation of the gathered data to support analysis, but leaving behind the assessment of dynamic aspects of speech production. Considering the proposed quantitative comparison methods, they can be extended to assess these dynamic aspects (Silva et al., 2013b; Silva and Teixeira, 2014b). We propose a method for dynamic analysis of articulatory data jointly exploring multiple realisations of each sound and covering multiple articulators. Instead of analysing each utterance and then inferring notable features, all realisations for a particular utterance can be considered simultaneously, possibly from more than one speaker, aiming towards a characterisation of average dynamic aspects and their corresponding variation. 4.1 Aligning multiple utterance realisations Multiple realisations of the same utterance (or different utterances), identified by previously performed annotation (e.g., using Praat, Boersma and Weenink, 2014), commonly present different durations. Before performing any kind of analysis, taking advantage of multiple realisations to characterise the mean dynamic behaviour and dynamic intra- and inter-sound variability, they have to be properly matched. The method used to perform the match can account for aspects such as the duration and complexity of the utterances and adapt accordingly. For example, for small utterance durations (or reduced number of phones), time axis normalisation can be adopted, with each realisation's duration assumed to vary between 0 and 1. Other approaches, when the utterances are longer or more complex, might include Dynamic Time Warping (DTW) (Berndt and Clifford, 1994). Based on the utterance realisations matching, the vocal tract contours extracted along the utterance are interpolated in order to allow comparison of corresponding contours between the realisations. Fig. 6 shows an example of matching two realisations of [ĩ] using time axis normalisation. One of the realisations is considered as reference and the existing vocal tract contours (associated to the corresponding image frames) are interpolated to obtain 10 vocal tract configurations equally spaced along the realisation. The second realisation is assumed to last the same amount of time as the first and the existing vocal tract contours interpolated to obtain 10 configurations. 4.2 Dynamic analysis of the vocal tract After matching the different realisations for each utterance and obtaining the corresponding contours, the vocal tract comparison method presented earlier can be used to compare each contour pair along the production. Fig. 7 depicts an illustrative example for the comparison between two sounds and considering 10 comparisons along the utterances’ duration. Since several realisations are considered for each utterance there are multiple contours to consider at each time instant. For each of these comparisons the vocal tract contours of the two utterances are cross-compared (as detailed in section 2.4) and the standard deviation computed. 4.3 Dynamic data representation Depicting dynamic data by representing the difference polygon for each vocal tract profile pair along the utterances (top rows in Fig. 7) is space consuming and can be difficult to grasp the full extent of the represented data. Therefore, we propose that the dynamic data is presented differently. The representation of the data gathered for the dynamic analysis has four different parts as depicted, at the bottom, in Fig. 7. On the left, a difference diagram for the comparison among the first frames and, on the right, a difference diagram for the comparison among the last frames of the considered utterances. The dynamic aspects are depicted in the middle. A line plot, with a line for each of the considered difference parameters, is provided. The line plot shows three horizontal colour bands as the difference diagrams, to hint on possible interpretations of the obtained values: as before, while green is proposed to stand for negligible differences, yellow and red stand for mild and strong differences. Since utterance matching has been performed, the time axis shows percentages instead of any particular time range. Below the central line plot, another line plot shows the standard deviation for each of the difference parameters. If needed, more compact representations can be easily obtained, based on this first proposal, by not showing the difference diagrams for the first and last frame, only showing the standard deviation plot if it goes above a certain threshold, or just plotting the lines for features with notable variation. 5 Application examples In this section we show some examples of what can be accomplished using the framework, in its current state of development, by addressing different tasks relevant for articulatory analysis performed over an existing RT-MRI corpus for European Portuguese (EP). The main purpose is not to perform a detailed characterisation of the articulatory characteristics of EP, or to fully explore the existing corpus, but to highlight those innovative aspects provided by the proposed quantitative analysis framework. 5.1 Application scenario As subject we selected EP nasal vowels due to the availability of the data, its reported dynamic nature (Teixeira et al., 1999) and our group continued interest. The articulation of EP nasal vowels has been studied by the authors and colleagues mainly focusing on velar dynamics as provided by electromagnetic midsagittal articulography (EMMA) (Oliveira et al., 2009). To extend these studies, with a characterisation of the oral configuration of EP nasal vowels, static (Martins et al., 2008) and, more recently, real-time magnetic resonance imaging (RT-MRI) data of the vocal tract was acquired (Oliveira et al., 2012; Martins et al., 2012).This imaging modality provides adequate data regarding the position and coordination of the different articulators over time (Hagedorn et al., 2011) and might provide a good choice to tackle the hyperarticulation effect observed in sustained productions (Engwall, 2003). Additionally, it might also help reduce the gravity effect on articulators for acquisitions in supine position (Tiede and Vatikiotis-Bateson, 2000). 5.1.1 Data acquisition and feature extraction Image sequences were acquired containing: (a) the five European Portuguese (EP) nasal vowels uttered in three word positions: initial, internal and final (e.g., the nonsense words “ampa, pampa, pan” ( ); and (b) the eight EP oral vowels (e.g., “papa” or “pupa”). Acquisition was performed at the midsagittal plane of the vocal tract. The images were acquired on an unmodified 3.0 T MR scanner (Magneton Tim Trio, Siemens, Erlanger, Germany) equipped with high performance gradients (Gmax=45mT/m, rise time=0.2s, slew rate=200T/m/s, and FOV=50cm). Custom 12-channel head and 4-channel neck phased-array coils were used for data acquisition. Parallel imaging (GRAPPA 2) and magnetic field gradients operating at FAST mode were used to speed up the acquisition. After localisation images, a T1 W 2D-midsagittal MRI slice of the vocal tract was obtained, using an Ultra-Fast RF-spoiled Gradient Echo (GE) pulse sequence (Single-Shot TurboFLASH), with a slice thickness of 8mm and the following parameters: TR/TE/FA=72ms/1.02ms/5°, Bandwidth=1395Hz/pixel, FOV(mm2)=210×210, reconstruction matrix of (128×128) elements with 50% phase resolution, in-plane resolution (mm2)=3.3×1.6, yielding a frame rate of 14images/s. Typically, each recorded sequence contained 75 images (taking around 5 seconds to acquire) although some longer sequences (300 images) were also acquired, mostly with the speakers producing sequences of isolated vowels (e.g., “am, em, im, om, um, am, …” , …). Audio was recorded simultaneously with the RT images inside the MR scanner, at a sampling rate of 16,000Hz, using a fiberoptic microphone, and manually annotated using the software tool Praat (Boersma and Weenink, 2014), in order to identify the time intervals corresponding to different sounds. The time intervals allow the determination (because both data are aligned) of the corresponding image frames. Data was acquired for three female speakers (CM, CO and SV), aged between 21 and 33, phonetically trained, with no history of hearing or speech disorders. For additional details regarding the acquisition and corpus the reader is forwarded to Teixeira et al. (2012). At an early stage of our research, vocal tract segmentation for this dataset was performed using a region-growing based method (Silva et al., 2013a) with the different segments of interest identified with the help of manually positioned landmarks, for each speaker (Silva et al., 2013b). Currently, the segmentation is handled by a model-based method (Silva and Teixeira, 2015) which results in a contour with the different segments of interest already tagged. Fig. 8 shows some examples of image frames taken from the RT-MRI dataset. The examples that follow were computed gathering the data for each vowel irrespective of its context and considering the isolated vowels, which were not considered previously (Silva and Teixeira, 2014b,c). 5.2 Static analysis For the static analysis, since we are working with a real-time dataset, a representative contour of the configuration assumed by the vocal tract for each sound utterance is chosen. For example, based on previous experience with the considered dataset (Oliveira et al., 2012) we chose the middle frame for oral vowels and the final frame for nasal vowels. The different criterion was adopted since nasal vowels generally presented the lowest velum position at the end of the annotated segment. Nevertheless, any other criterion for choosing the vocal tract contours is possible since it does not influence how the differences are computed, but only how the resulting data is interpreted. 5.2.1 Intra-speaker analysis As already noted, multiple realisations of each vowel, produced by the same speaker, can be cross compared. This allows the assessment of the variability associated with the production of each vowel, for each speaker. Fig. 9 presents some representative examples of the differences obtained for several vowels and speakers. In general, for the studied vowels, no major intra-vowel differences were found. Quantitative analysis can also be performed to compare among vocal tract configurations for different sounds and for the same speaker. In EP, such comparisons are useful, for example, to study oral vs nasal vowels. Fig. 10 depicts comparisons among different vowel pairs. The corresponding contour superpositions are also presented to allow observing how the computed data and abstract representation relate with the traditional visual assessment. To note, the clearly depicted differences: (1) at the velum between [ĩ] and [i] and [ũ] and [u]; (2) in lip aperture, for all comparisons; and (3) at the tongue tip and tongue back, between [ɛ] and [e]. Regarding the directional information, note that it should be read as an answer to the question “In which direction did the change occur from the first to the second vocal tract configuration?”. 5.2.2 Inter-speaker comparison One important feature provided by the proposed framework is the comparison among speakers. Since the provided comparison values are normalised, it is possible to visualise the differences for various speakers on a common representation. The difference polygons for each speaker and for each vowel pair were superimposed to assess inter-speaker differences (see Fig. 11 ). In general, the difference polygons for all speakers, for the same vowel pair, were similar. Notable inter-speaker differences were sometimes observed for lip aperture: notice, for example, for the comparison between [a] and [i], that speaker SV presents a mild difference for lip aperture. 5.2.3 Overall characterisation of articulatory differences The data gathered for all speakers can now be used to assess the overall articulatory differences between vowels. Observing Fig. 12 , a notable aspect, evidenced by the standard deviation, is that, overall, the strongest variability among the different productions is mostly observed at the tongue tip and lip aperture (protruding vertices of the orange polygon at the centre). 5.3 Dynamic analysis Considering the available data, three different kinds of dynamic analysis can be performed, covering the different articulators: • Intra-sound dynamics, i.e., how does the vocal tract vary from the beginning to the end of the production and how consistent it is across productions. For example, cross compare all utterances of [i]. • Inter-sound comparison, i.e., compare the production of two different sounds along time to check differences/similarities. For example, vowel [o] and its nasal congener [õ]. • Comparison, over time, with a reference, i.e., analyse production of a sound using a particular (static) vocal tract configuration, as reference, to check how similar (and when) is the vocal tract configuration along the production to other relevant configurations (e.g., [ e ˜ ] compared to representative (static) configuration of [ɛ]). 5.3.1 Intra-sound dynamics All utterances of a particular vowel are matched and cross compared. The resulting comparison data for each considered frame is averaged and represented in the line plot to depict evolution over time. Fig. 13 shows, as representative examples, variation data for vowels [ ] and [ũ]. For [ ] only data from speaker CM was considered, but since all utterances can be matched and the vocal tract comparison method provides normalised data, the presented dynamic analysis for vowel [ũ] was performed taking into account the data for all three speakers in the RT-MRI dataset. Visualisations made possible by the proposed method show no major discrepancy of vocal tract configuration across utterances, for both vowels, i.e., all utterances exhibit a similar vocal tract variation pattern: a slight difference is observed for the lip aperture (LA), in vowel [ ], at the beginning and end of the production, but the remaining parameters keep above 0.75 with low standard deviations. 5.3.2 Inter-sound production differences Fig. 14 shows two examples regarding the assessment of dynamic differences between sounds. In the first example (top row), the production of [ e ˜ ] was compared with the production of [ɛ]: both start very similar with only a slight difference at tongue tip and gradually differ at lip aperture and velum. This gradual variation at the velum is in agreement with previous studies (Gregio, 2006; Lovatto et al., 2007; Martins et al., 2012; Oliveira et al., 2012) describing different phases in the production of nasal vowels. The second example (Fig. 14, bottom row) shows the comparison between vowels [ũ] and [ĩ]. As expected there are strong differences at the tongue tip (TT), tongue back (TB) and tongue dorsum (TD). Nevertheless, note how no substantial difference exists at the velum (VEL) between the two sounds. Since both are nasal vowels, the velum is expected to gradually open along the production (Martins et al., 2012). The dynamic analysis shows that both sounds exhibit a similar velum aperture pattern. 6 Discussion Taking into consideration the challenges identified in the introduction, we consider that the proposed framework provides important contributions: addresses subjectivity by proposing quantitative methods for comparing vocal tract profiles; allows comparing data among speakers due to the normalised nature of the computed differences; makes it possible to gather the data from different realisations of each sound in order to compute variability; and supports gathering data from all available speakers to depict overall characterisation of articulatory differences, e.g., for a particular language. The application examples presented show how the proposed methods can be useful to tackle the analysis of large datasets and provide data enabling the characterisation of the articulatory differences between different sounds both considering static and dynamic analysis. The only challenge that we do not explicitly address, from those pointed in the introduction, is multimodality. It is our understanding that the defined framework is general enough to allow its extension to encompass data from other modalities. Nevertheless, it is important to note that this extension poses interesting new challenges such as how to deal with the different number of features covered by each modality (e.g., the tongue in US and the whole vocal tract in MRI) or how to consider the contributions of each modality to the presented average data. In a systematic, unsupervised approach to the analysis of articulatory data (e.g., considering our pipeline in Fig. 1), it is possible that, due to image noise or artefacts, the quality of the unsupervised vocal tract segmentation (Silva and Teixeira, 2015) might sometimes be compromised (Ramanarayanan et al., 2013). The inclusion of such segmentations in the analysis (albeit in small number) might eventually affect the results and the pipeline would profit from methods to filter the data or signal these problematic cases for further analysis. One possible approach, a relevant additional application made possible by the proposed framework, might be the detection of outliers based on intra-sound comparisons. The features proposed for this first instantiation of the framework, and the methods used to compute them, are a first proposal and can be subject to changes and improvements. For example, comparison of regions using the Pratt index is performed by determining the corresponding point by a proximity criterion. This might be improved by searching for the corresponding points along the expected principal direction of movement, e.g., radial, for tongue dorsum. Regarding the velum, if the available vocal tract contours allow analysis of velar aperture, adding to the velum position (e.g., Silva and Teixeira, 2015), this data can be considered for the computation of F VEL . Nevertheless, despite the possible improvements and changes to the methods used to compute each feature, it is important to note that the proposed framework allows systematic and objective application of an uniform comparison criteria throughout the compared data. Furthermore, the proposed comparison features are not to be understood as a closed set. For this first instantiation of the framework, we chose a set of features that, overall, cover the different articulators, but new features (e.g., Saltzman and Munhall, 1989; Smith, 2014) can be added to better serve the study of specific sounds. For example, considering features that deal with constriction degree and location would allow the study of consonantal constriction. One of the aspects we consider relevant for the framework is the possibility to depict the comparison outcomes in such a way that it is easily interpreted by researchers. The abstract visual representations are a first proposal to help gather insight over the computed comparison values and, even though simple in nature, serve their main purpose well, as demonstrated in the application examples. Nevertheless, these representations present some limitations. When multiple difference polygons are represented and, hence, multiple polygons depicting the corresponding standard deviations, there is overlap among the polygons that might hinder the interpretation: matching between the difference polygons and the corresponding standard deviations is not possible and the larger polygon might cover the remaining polygons, if drawn last. Nonetheless, this already provides an idea of the worst case scenario. We have been gathering feedback from different researchers concerning the proposed representations, a first informal evaluation was presented in Silva and Teixeira (2014a) and we intend to further explore user inputs. It is important to note that using the proposed framework results in a set of quantitative difference data that can be further explored for computational analysis. The proposal of the visual representations should not be understood as the final stage for the computed comparison data. Although not explored in this article, methods such as clustering can be tested to study the proximity between sounds, speakers or languages and the importance of particular features in differencing between sounds. 7 Conclusions This article proposes a quantitative framework for vocal tract profile comparison. To the best of our knowledge, this is the first time a quantitative analysis framework is presented encompassing the assessment of per sound intra-speaker variability, inter-speaker differences and overall inter-sound differences using data from all speakers in a single representation. The presented application examples show the potential of the proposed framework to take advantage of the increasing amounts of articulatory data available and move towards quantitative inter-speaker and inter-language comparison, both for static and dynamic analysis scenarios. The application of the proposed methods to other classes of sounds (e.g., laterals, Martins et al., 2010) and the study (and inclusion in the proposed visualisations) of landmark trajectories and their variability, considering all available speakers, should provide further insight into the usefulness of the proposed methods. Adding to the possible improvements already mentioned in the discussion, there are a few other lines of work that deserve further attention. Grounded on the principles and methods encompassed by the proposed framework, promoting systematic analysis of the available vocal tract data, the presented work can evolve to provide support to the use of high-level models of articulator organisation and control (e.g., TAsk Dynamics Application (TADA), Nam et al., 2015). Regarding the identification of relevant gestures, the analysis based on multiple realisations of each sound can already be helpful, and the instantiation of the framework considering other features (e.g., tongue body and tip constriction location and degree) can bring the outcomes closer to the variables required by the models. The aspect that needs to be further developed is the computation of temporal aspects that, despite being present in the dynamic analysis, is not explicitly addressed. In this regard, the long-term goal is the generation of gestural scores, from the data, that can serve as input, for example, to articulatory synthesisers (e.g., Teixeira et al., 2002; Birkholz et al., 2011) The use of multi-planar (e.g., Proctor et al., 2012) or 3D imaging (e.g., Martins et al., 2011; Zhu et al., 2013) of the vocal tract assumes importance for studying sounds exhibiting important characteristics not observable in the sagittal plane (Zhu et al., 2013). While the work presented here does not explicitly address the application of the framework to these kind of MRI data, there is no impediment to it, as long as the new features comply with the requirements set in section 2 and analysis follows the systematic procedure inherent to the framework. Instead of one midsagittal vocal tract contour, several contours can be considered, in different planes of interest. For instance, regarding lateral sounds (e.g., /l/), a comparison feature might be used accounting for the asymmetric nature of the lateral channels forming on the tongue sides (Martins et al., 2010), based on their section area, in the coronal plane, and their length. One route we are considering for further development of the proposed framework, addressing the multimodality challenge discussed earlier, is its use with data from different/multiple imaging modalities. We are currently considering the analysis of RT-MRI and ultrasound data simultaneously. The presented methods (and envisaged steps regarding computational analysis) are computationally demanding and generate large amounts of data. Therefore, their deployment in a cloud environment would provide a more suitable scenario for further developments and an important first step towards their validation and use by third parties such as phoneticians. The use of the proposed framework and its developments by other research groups, over their data, would pave the way for comparisons between dialects and languages. We are currently starting this migration process in the scope of projects Cloud Thinking 1 1 and IRIS. 2 2 www.microsoft.com/pt-pt/mldc/iris/default.aspx. Acknowledgements The authors thank the anonymous reviewers for their helpful comments and suggestions. Research partially funded by FEDER through the Program COMPETE and by National Funds (FCT) in the context of HERON II (PTDC/EEA-PLP/098298/2008), Project Marie Curie IAPP “IRIS” (FP7-PEOPLE-2013-IAPP, ref. 610986) and project Cloud Thinking (QREN Mais Centro, ref. CENTRO-07-ST24-FEDER-002031). References Badin et al., 2014 P. Badin T.R. Sawallis S. Crépel L. Lamalle Comparison of articulatory strategies for bilingual speaker: preliminary data and models Proc. ISSP Germany 2014 17 20 Benítez et al., 2014 A. Benítez V. Ramanarayanan L. Goldstein S. Narayanan Real-time MRI of articulatory setting in second language speech Proc. Interspeech Singapore 2014 701 705 Berndt and Clifford, 1994 D. Berndt J. Clifford Using dynamic time warping to find patterns in time series Proc. AAAI Workshop on Knowledge Discovery in Databases 1994 229 248 Birkholz et al., 2011 P. Birkholz B. Kroger C. Neuschaefer-Rube Model-based reproduction of articulatory trajectories for consonant-vowel sequences IEEE Trans. Audio Speech Lang. Process. 19 2011 1422 1433 10.1109/TASL.2010.2091632 Boersma and Weenink, 2014 P. Boersma D. Weenink Praat: Doing Phonetics by Computer [Computer Program]. Version 5.3.42 2014 Bresch et al., 2010 E. Bresch A. Katsamanis L. Goldstein S. Narayanan Statistical multi-stream modeling of real-time MRI articulatory speech data Proc. Interspeech Makuhari, Japan 2010 1584 1587 Bresch and Narayanan, 2009 E. Bresch S. Narayanan Region segmentation in the frequency domain applied to upper airway real-time magnetic resonance images IEEE Trans. Med. Imaging 28 2009 323 338 10.1109/TMI.2008.928920 Cleland et al., 2011 J. Cleland A.A. Wrench J.M. Scobbie S. Semple Comparing articulatory images: an MRI ultrasound tongue image database Proc. Int. Seminar on Speech Production (ISSP) Montreal, Canada 2011 163 170 Davidson, 2006 L. Davidson Comparing tongue shapes from ultrasound imaging using smoothing spline analysis of variance J. Acoust. Soc. Am. 120 2006 407 415 Delvaux et al., 2002 V. Delvaux T. Metens A. Soquet Propriétés acoustiques e articulatoires des voyelles nasales du Français Proc. Journées d’Étude sur la Parole Nancy 2002 348 352 Engwall, 2003 O. Engwall A revisit to the application of MRI to the analysis of speech production – testing our assumptions Proc. 6th Int. Seminar on Speech Production (ISSP) Sydney, Australia 2003 43 48 Gick et al., 2004 B. Gick I. Wilson K. Koch C. Cook Language-specific articulatory settings: evidence from inter-utterance rest position Phonetica 61 2004 220 233 Gregio, 2006 F.N. Gregio Configuração do trato vocal supraglótico na produção das vogais do Português Brasileiro: dados de imagens de ressonância magnética [Supraglottic vocal tract shaping in the production of Brazilian Portuguese vowels: data from magnetic resonance imaging] 2006 PUC/SP (Master thesis) Hagedorn et al., 2011 C. Hagedorn M.I. Proctor L. Goldstein Automatic analysis of singleton and geminate consonant articulation using real-time magnetic resonance imaging Proc. Interspeech Florence, Italy 2011 409 412 Höwing et al., 1999 F. Höwing S. Dooley D. Wermser Tracking of non-rigid articulatory organs in X-ray image sequences Comput. Med. Imaging Graph. 23 1999 59 67 Kim et al., 2014 J. Kim A. Lammert P. Kumar Ghosh S. Narayanan Co-registration of speech production datasets from electromagnetic articulography and real-time magnetic resonance imaging J. Acoust. Soc. Am. 135 2014 EL115 EL121 10.1121/1.4862880 Lammert et al., 2010 A. Lammert M. Proctor S. Narayanan Data-driven analysis of realtime vocal tract MRI using correlated image regions Proc. Interspeech Makuhari, Japan 2010 1572 1575 Laprie et al., 2014 Y. Laprie M. Aron M.O. Berger B. Wrobel-Dautcourt Studying MRI acquisition protocols of sustained sounds with a multimodal acquisition system Proc. 10th Int. Seminar on Speech Production (ISSP) Cologne, Germany 2014 245 248 Lovatto et al., 2007 L. Lovatto A. Amelot L. Crevier-Buchman P. Basset J. Vaissière A fiberscopic analysis of nasal vowels in Brazilian Portuguese Proc. 16th Int. Congress of Phonetic Sciences (ICPhS) Saarbrücken, Germany 2007 549 552 Martins et al., 2008 P. Martins I. Carbone A. Pinto A. Silva A. Teixeira European Portuguese MRI based speech production studies Speech Commun. 50 2008 925 952 Martins et al., 2010 P. Martins C. Oliveira A. Silva A. Teixeira Articulatory characteristics of European Portuguese laterals: a 2D & 3D MRI study Proc. FALA Vigo, Spain 2010 33 36 Martins et al., 2011 P. Martins C. Oliveira S. Silva A. Silva A. Teixeira Tongue segmentations from MRI images using ITK-Snap: preliminary evaluation Proc. IADIS Intl Conferences on Computer Graphics, Visualization, Computer Vision and Image Processing 2011 Martins et al., 2012 P. Martins C. Oliveira S. Silva A. Teixeira Velar movement in European Portuguese nasal vowels Proc. IberSpeech 2012 – VII Jornadas en Tecnología del Habla and III Iberian SLTech Workshop Madrid, Spain 2012 231 240 Miller et al., 2014 N.A. Miller J.S. Gregory R.M. Aspden P.J. Stollery F.J. Gilbert Using active shape modeling based on MRI to study morphologic and pitch-related functional changes affecting vocal structures and the airway J. Voice 2014 10.1016/j.jvoice.2013.12.002 (in press) Nam et al., 2015 H. Nam C. Browman L. Goldstein M. Proctor P. Rubin E. Saltzman TADA: TAsk Dynamic Application 2015 Niebergall et al., 2013 A. Niebergall S. Zhang E. Kunay G. Keydana M. Job M. Uecker J. Frahm Real-time MRI of speaking at a resolution of 33ms: undersampled radial FLASH with nonlinear inverse reconstruction Magn. Reson. Med. 69 2013 477 485 Oliveira et al., 2012 C. Oliveira P. Martins S. Silva A. Teixeira An MRI study of the oral articulation of European Portuguese nasal vowels Proc. Interspeech Portland, OR, USA 2012 2690 2693 Oliveira et al., 2009 C. Oliveira P. Martins A. Teixeira Speech rate effects on European Portuguese nasal vowels Proc. Interspeech Brighton, UK 2009 480 483 Pratt, 2007 W.K. Pratt Digital Image Processing 2007 Wiley-Interscience Proctor et al., 2010 M. Proctor D. Bone A. Katsamanis S. Narayanan Rapid semi-automatic segmentation of real-time magnetic resonance images for parametric vocal tract analysis Proc. Interspeech Makuhari, Japan 2010 1576 1579 Proctor et al., 2012 M. Proctor L.H. Lu Y. Zhu L. Goldstein S. Narayanan Articulation of Mandarin sibilants: a multi-plane realtime MRI study Proc. Speech Sci. Tech. Sydney, Australia 2012 113 116 Proctor et al., 2011 M.I. Proctor A.C. Lammert A. Katsamanis L.M. Goldstein C. Hagedorn S.S. Narayanan Direct estimation of articulatory kinematics from real-time magnetic resonance image sequences Proc. Interspeech Florence, Italy 2011 281 284 Ramanarayanan et al., 2010 V. Ramanarayanan D. Byrd L. Goldstein S.S. Narayanan Investigating articulatory setting – pauses, ready position, and rest – using real-time MRI Proc. Interspeech Makuhari, Japan 2010 1994 1997 Ramanarayanan et al., 2013 V. Ramanarayanan L. Goldstein D. Byrd S. Narayanan An investigation of articulatory setting using real-time magnetic resonance J. Acoust. Soc. Am. 134 2013 510 519 Saltzman and Munhall, 1989 E. Saltzman K. Munhall A dynamical approach to gestural patterning in speech production Ecol. Psychol. 1 1989 333 382 Scott et al., 2014 A.D. Scott M. Wylezinska M.J. Birch M.E. Miquel Speech MRI: morphology and function Phys. Med. 2014 10.1016/j.ejmp.2014.05.001 (in press) Shadle et al., 2008 C. Shadle M.I. Proctor K. Iskarous An MRI study of the effect of vowel context on English fricatives Proc. Acoustics ’08 Paris: Joint meeting of the ASA, EAA & Société Française d’Acoustique 2008 Shosted et al., 2012 R. Shosted B.P. Sutton A. Benmamoun Using magnetic resonance to image the pharynx during Arabic speech: static and dynamic aspects Proc. Interspeech Portland, OR, USA 2012 2182 2185 Silva and Teixeira, 2014a S. Silva A. Teixeira A framework for analysis of the upper airway from real-time MRI sequences Proc. SPIE Visualization and Data Analysis SF, CA, USA 2014 901703 10.1117/12.2042081 Silva and Teixeira, 2014b S. Silva A. Teixeira RT-MRI based dynamic analysis of vocal tract configurations: preliminary work regarding intra- and inter-sound variability Proc. Int. Seminar on Speech Production (ISSP) Cologne, Germany 2014 399 402 Silva and Teixeira, 2014c S. Silva A. Teixeira Systematic and quantitative analysis of vocal tract data: Intra- and inter-speaker analysis Proc. Int. Seminar on Speech Production (ISSP) Cologne, Germany 2014 403 406 Silva and Teixeira, 2015 S. Silva A. Teixeira Unsupervised segmentation of the vocal tract from real-time MRI sequences Comput. Speech Lang. 33 2015 25 46 10.1016/j.csl.2014.12.003 Silva et al., 2013a S. Silva A. Teixeira C. Oliveira P. Martins Segmentation and analysis of vocal tract from midsagittal real-time MRI Proc. ICIAR 2013. LNCS vol. 7950 Póvoa de Varzim, Portugal 2013 459 466 Silva et al., 2013b S. Silva A. Teixeira C. Oliveira P. Martins Towards a systematic and quantitative analysis of vocal tract data Proc. Interspeech Lyon, France 2013 1307 1311 Smith, 2014 C. Smith Complex tongue shaping in lateral liquid production without constriction-based goals Proc. ISSP Cologne, Germany 2014 413 416 Teixeira et al., 2012 A. Teixeira P. Martins C. Oliveira C. Ferreira A. Silva R. Shosted Real-time MRI for Portuguese: database, methods and applications Proc PROPOR 2012. LNCS vol. 7243 Coimbra, Portugal 2012 306 317 Teixeira et al., 2011 A. Teixeira P. Martins A. Silva C. Oliveira An MRI study of consonantal coarticulation resistance in Portuguese Proc. 9th Int. Seminar on Speech Production (ISSP) Montreal, Canada 2011 243 250 Teixeira et al., 2002 A. Teixeira L. Silva R. Martinez F. Vaz Sapwindows – towards a versatile modular articulatory synthesizer Proc. IEEE Workshop on Speech Synthesis 2002 31 34 10.1109/WSS.2002.1224366 Teixeira et al., 1999 A. Teixeira F. Vaz J.C. Prncípe Influence of dynamics in the perceived naturalness of Portuguese nasal vowels Proc. Int. Congress on Phonetic Sciences (ICPhS) San Francisco, CA, USA 1999 2557 2560 Tiede and Vatikiotis-Bateson, 2000 M.K. Tiede E. Vatikiotis-Bateson Contrasts in speech articulation observed in sitting and supine conditions Proc. 5th Seminar on Speech Production (ISSP) Chiemgau, Germany 2000 25 28 Wang, 2011 Y. Wang Smoothing Splines: Methods and Applications 2011 CRC Press Zharkova and Hewlett, 2009 N. Zharkova N. Hewlett Measuring lingual coarticulation from midsagittal tongue contours: description and example calculations using English /t/and /a J. Phonet. 37 2009 248 256 10.1016/j.wocn.2008.10.005 Zharkova et al., 2011 N. Zharkova N. Hewlett W.J. Hardcastle Coarticulation as an indicator of speech motor control development in children: An ultrasound study Motor Control 15 2011 118 140 Zharkova et al., 2014 N. Zharkova N. Hewlett W.J. Hardcastle R.J. Lickley Spatial and temporal lingual coarticulation and motor control in preadolescents J. Speech Lang. Hearing Res. 57 2011 374 388 Zhu et al., 2013 Y. Zhu A. Toutios S. Narayanan K. Nayak Faster 3D vocal tract real-time MRI using constrained reconstruction Proc. Interspeech Lyon, France 2013 1292 1296 "
    },
    {
        "doc_title": "Applications of the multimodal interaction architecture in ambient assisted living",
        "doc_scopus_id": "85009674996",
        "doc_doi": "10.1007/978-3-319-42816-1_12",
        "doc_eid": "2-s2.0-85009674996",
        "doc_date": "2016-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Ambient assisted living (AAL)",
            "Heterogeneous environments",
            "Multi-Modal Interactions",
            "Multimodal application",
            "Research outcome",
            "User groups",
            "User interaction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2017.Developing applications for ambient assisted living (AAL) scenarios requires dealing with diverse user groups, heterogeneous environments, and a large plethora of devices. These requirements pose several challenges on how to design and develop user interaction with the proposed applications and services. In this context, the versatility provided by multimodal interaction (MMI) is paramount and the adopted architecture should be instrumental in harnessing its full potential. This chapter offers an insight on how AAL challenges can be tackled by multimodal-based solutions. It presents the authors’ views and research outcomes in multimodal application development for AAL grounded on an architecture for MMI aligned with the W3C recommendations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-device applications using the multimodal architecture",
        "doc_scopus_id": "85009673838",
        "doc_doi": "10.1007/978-3-319-42816-1_17",
        "doc_eid": "2-s2.0-85009673838",
        "doc_date": "2016-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Architecture-based",
            "Concrete applications",
            "Design and Development",
            "Multi-devices",
            "Multi-Modal Interactions",
            "Multimodal architectures",
            "Research outcome",
            "Screen sizes"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2017.Nowadays, users have access to a multitude of devices at their homes, workplaces or that they can carry around. Each of these devices, given its features (e.g., interaction modalities, screen size), might be more suitable for particular users, tasks, and contexts. While having one application installed in several devices might be common, they mostly work isolated, not exploring the possibilities of several devices working together to provide a more versatile and richer interaction scenario. Adopting a multimodal interaction (MMI) architecture based on the W3C recommendations, beyond the advantages to the design and development of MMI, provides, we argue, an elegant approach to tackle multi-device interaction scenarios. In this regard, this chapter conveys our views and research outcomes addressing this subject, presenting concrete application examples.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive, multi-device visualization supported by a multimodal interaction framework: Proof of concept",
        "doc_scopus_id": "84978914860",
        "doc_doi": "10.1007/978-3-319-39943-0_27",
        "doc_eid": "2-s2.0-84978914860",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Device characteristics",
            "First impressions",
            "Future improvements",
            "Interactive visualizations",
            "Multi-devices",
            "Multi-Modal Interactions",
            "Multiple representation",
            "User satisfaction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.Nowadays, users can interact with a system using a wide variety of modalities, such as touch and speech. Nevertheless, multimodal interaction has yet to be explored for interactive visualization scenarios. Furthermore, users have access to a wide variety of devices (e.g., smartphones, tablets) that could be harnessed to provide a more versatile visualization experience, whether by providing complementary views or by enabling multiple users to jointly explore the visualization using their devices. In our effort to gather multimodal interaction and multi-device support for visualization, this paper describes our first approach to an interactive multi-device system, based on the multimodal interaction architecture proposed by the W3C, enabling interactive visualization using different devices and representations. It allows users to run the application in different types of devices, e.g., tablets or smartphones, and the visualizations can be adapted to multiple screen sizes, by selecting different representations, with different levels of detail, depending on the device characteristics. Groups of users can rely on their personal devices to synchronously visualize and interact with the same data, maintaining the ability to use a custom representation according to their personal needs. A preliminary evaluation was performed, mostly to collect users’ first impressions and guide future developments. Although the results show a moderate user satisfaction, somehow expected at this early stage of development, user feedback allowed the identification of important routes for future improvement, particularly regarding a more versatile navigation along the data and the definition of composite visualizations (e.g., by gathering multiple representations on the same screen).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the creation of a persona to support the development of technologies for children with autism spectrum disorder",
        "doc_scopus_id": "84978904050",
        "doc_doi": "10.1007/978-3-319-40238-3_21",
        "doc_eid": "2-s2.0-84978904050",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autism spectrum disorders",
            "Children",
            "Children with autisms",
            "Design and Development",
            "Persona"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.When developing technologies for persons with autism spectrum disorder (ASD) there are multiple aspects posing challenges to the community. First of all, there are several viewpoints at stake, from the targeted person to family and caretakers, needing careful consideration and yielding conflicting interests and motivations that need to be considered. Second, design and development teams often include people with a very diverse background, from psychologists to software engineers, who need to be able to fully communicate their knowledge and ideas regarding the users, and understand the different team viewpoints towards the best possible outcome. In this context, we argue that Personas (and in particular, families of Personas) can be a powerful tool to tackle these challenges. As a first stage of our work, we present the methods considered for the creation of a Persona for a 10 years old kid with ASD along with its full description. At this stage, the Persona has been evaluated by a panel of experts and was considered in the design of a first application prototype for children with ASD.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Developing technologies for the elderly: To whom are we really developing?",
        "doc_scopus_id": "84953271448",
        "doc_doi": "10.1109/EMBC.2015.7320256",
        "doc_eid": "2-s2.0-84953271448",
        "doc_date": "2015-11-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Aged",
            "Disabled Persons",
            "Humans",
            "Technology"
        ],
        "doc_abstract": "© 2015 IEEE.The increasing number of older adults worldwide boosted the proposal of applications specifically targeting their needs. While the elderly serve as the target audience, this article argues that in many cases they are being treated as any other age group, disregarding their unique characteristics and expectations, which might eventually result in applications that, despite being used by the elderly, are adapted applications to cope with their disabilities, instead of approaches that leverage their abilities and views. As a first step of this discussion, this article aims to show how, in different stages of design and development, we are jump-starting the whole process putting our own assumptions and views where those of the elderly should be.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "CraMs: Craniometric analysis application using 3d skull models",
        "doc_scopus_id": "84961700580",
        "doc_doi": "10.1109/MCG.2015.136",
        "doc_eid": "2-s2.0-84961700580",
        "doc_date": "2015-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "3D models",
            "Bone damages",
            "Initial guess",
            "Interobserver variability",
            "Point detection",
            "Points of interest",
            "Skull models",
            "Anatomic Landmarks",
            "Cephalometry",
            "Computer Graphics",
            "Humans",
            "Image Interpretation, Computer-Assisted",
            "Imaging, Three-Dimensional",
            "Models, Anatomic",
            "Reproducibility of Results",
            "Sensitivity and Specificity",
            "Skull",
            "Software"
        ],
        "doc_abstract": "© 1981-2012 IEEE.Craniometric analysis plays an important role in anthropology studies and forensics. This paper presents CraMs, an application using a new craniometric approach based on 3D models of the skull. The main objective is to obtain, through a process supervised by anthropologists, the main points of interest used to compute craniometric measurements. The application aids this process by analyzing the skull geometry and automatically providing points of interest. The application also allows for semiautomatic point detection, where the user provides an initial guess that might be refined based on the curvature of the skull, as well as the manual selection of any other points of interest. Moreover, results comparing measurements obtained with CraMs and traditional craniometry methods on eight skulls suggest that the application provides comparable craniometric measurements and lower inter-observer variability. This approach offers advantages such as an easier access to skulls with no risk of bone damage and the possibility of defining new measurements based on morphology or other skull characteristics, which are not possible using traditional methods.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "\"Read that article\": Exploring synergies between gaze and speech interaction",
        "doc_scopus_id": "84962648230",
        "doc_doi": "10.1145/2700648.2811369",
        "doc_eid": "2-s2.0-84962648230",
        "doc_date": "2015-10-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Active-assisted",
            "Gaze",
            "HCI system",
            "Human computer interaction (HCI)",
            "Input modalities",
            "Multi-modal",
            "Speech interaction",
            "User intention"
        ],
        "doc_abstract": "© 2015 ACM.Gaze information has the potential to benefit Human-Computer Interaction (HCI) tasks, particularly when combined with speech. Gaze can improve our understanding of the user intention, as a secondary input modality, or it can be used as the main input modality by users with some level of permanent or temporary impairments. In this paper we describe a multimodal HCI system prototype which supports speech, gaze and the combination of both. The system has been developed for Active Assisted Living scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detecting nasal vowels in speech interfaces based on surface electromyography",
        "doc_scopus_id": "84936817954",
        "doc_doi": "10.1371/journal.pone.0127040",
        "doc_eid": "2-s2.0-84936817954",
        "doc_date": "2015-06-12",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Electromyography",
            "Female",
            "Humans",
            "Magnetic Resonance Imaging",
            "Phonetics",
            "Reproducibility of Results",
            "Speech Acoustics",
            "User-Computer Interface",
            "Young Adult"
        ],
        "doc_abstract": "© 2015 Freitas et al.Nasality is a very important characteristic of several languages, European Portuguese being one of them. This paper addresses the challenge of nasality detection in surface electromyography (EMG) based speech interfaces. We explore the existence of useful information about the velum movement and also assess if muscles deeper down in the face and neck region can be measured using surface electrodes, and the best electrode location to do so. The procedure we adopted uses Real-Time Magnetic Resonance Imaging (RT-MRI), collected from a set of speakers, providing a method to interpret EMG data. By ensuring compatible data recording conditions, and proper time alignment between the EMG and the RT-MRI data, we are able to accurately estimate the time when the velum moves and the type of movement when a nasal vowel occurs. The combination of these two sources revealed interesting and distinct characteristics in the EMG signal when a nasal vowel is uttered, which motivated a classification experiment. Overall results of this experiment provide evidence that it is possible to detect velum movement using sensors positioned below the ear, between mastoid process and the mandible, in the upper neck region. In a frame-based classification scenario, error rates as low as 32.5% for all speakers and 23.4% for the best speaker have been achieved, for nasal vowel detection. This outcome stands as an encouraging result, fostering the grounds for deeper exploration of the proposed approach as a promising route to the development of an EMG-based speech interface for languages with strong nasal characteristics.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Velar movement assessment for speech interfaces: An exploratory study using surface electromyography",
        "doc_scopus_id": "84955320352",
        "doc_doi": "10.1007/978-3-319-26129-4_16",
        "doc_eid": "2-s2.0-84955320352",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Exploratory studies",
            "Movement detection",
            "Nasal vowels",
            "Neck region",
            "Noninvasive methods",
            "Silent speech interfaces",
            "Speech interface",
            "Surface electromyography"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.In the literature several silent speech interfaces based on Surface Electromyography (EMG) can be found. However, it is yet unclear if we are able to sense muscles activity related to nasal port opening/closing. Detecting the nasality phenomena, would increase the performance of languages with strong nasal characteristics such as European Portuguese. In this paper we explore the use of surface EMG electrodes, a non-invasive method, positioned in the face and neck regions to explore the existence of useful information about the velum movement. For an accurate interpretation and validation of the proposed method, we use velum movement information extracted from Real-Time Magnetic Resonance Imaging (RT-MRI) data. Overall, results of this study show that differences can be found in the EMG signals for the case of nasal vowels, by sensors positioned below the ear between the mastoid process and the mandible in the upper neck region.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of complex distributed multimodal applications: Evaluating a TeleRehabilitation system when it really matters",
        "doc_scopus_id": "84949789710",
        "doc_doi": "10.1007/978-3-319-20913-5_14",
        "doc_eid": "2-s2.0-84949789710",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living (AAL)",
            "Dynamic environments",
            "Evaluation",
            "Evaluation methodologies",
            "Hardware and software",
            "Multi-modality",
            "Multimodal application",
            "Telerehabilitation"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.The evaluation of applications or systems within dynamic environments is complex. The existence of multiple hardware and software items which share the same space can provoke concurrency issues and result in erratic interactions. A sudden change within the environment can result is dramatic changes both to the user and application itself which can pass unnoticed in traditional evaluation methodologies. To verify if a component is compatible with a given environment is of paramount importance for areas like pervasive computing, ambient intelligence or ambient assisted living (AAL). In this paper, a semi-automatic platform for evaluation is presented and integrated with a TeleRehabilitation system in an AAL scenario to enhance evaluation. Preliminary results show the advantages of the platform in comparison with typical observation solutions mainly in terms of achieved data and overall ease of use.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and development of multimodal applications: A vision on key issues and methods",
        "doc_scopus_id": "84947276451",
        "doc_doi": "10.1007/978-3-319-20678-3_11",
        "doc_eid": "2-s2.0-84947276451",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Application scenario",
            "Constant improvement",
            "Design and Development",
            "Evaluation",
            "Evaluation methodologies",
            "Multi-Modal Interactions",
            "Multimodal application",
            "Multimodal user interface"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Multimodal user interfaces provide users with different ways of interacting with applications. This has advantages both in providing interaction solutions with additional robustness in environments where a single modality might result in ambiguous input or output (e.g., speech in noisy environments), and for users with some kind of limitation (e.g., hearing difficulties resulting from ageing) by yielding alternative andmore natural ways of interacting. The design and development of applications supporting multimodal interaction involves numerous challenges, particularly if the goals include the development of multimodal applications for a wide variety of scenarios, designing complex interaction and, at the same time, proposing and evolving interaction modalities. These require the choice of an architecture, development and evaluation methodologies and the adoption of principles that foster constant improvements at the interaction modalities level without disrupting existing applications. Based on previous and ongoing work, by our team, we present our approach to the design, development and evaluation of multimodal applications covering several devices and application scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Giving Voices to Multimodal Applications",
        "doc_scopus_id": "84944244352",
        "doc_doi": "10.1007/978-3-319-20916-6_26",
        "doc_eid": "2-s2.0-84944244352",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Age effects",
            "Intrinsic property",
            "Multi-Modal Interactions",
            "Multimodal application",
            "Speech interaction",
            "Speech output",
            "Synthetic voices",
            "User satisfaction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.The use of speech interaction is important and useful in a wide range of applications. It is a natural way of interaction and it is easy to use by people in general. The development of speech enabled applications is a big challenge that increases if several languages are required, a common scenario, for example, in Europe. Tackling this challenge requires the proposal of methods and tools that foster easier deployment of speech features, harnessing developers with versatile means to include speech interaction in their applications. Besides, only a reduced variety of voices are available (sometimes only one per language) which raises problems regarding the fulfillment of user preferences and hinders a deeper exploration regarding voices’ adequacy to specific applications and users. In this article, we present some of our contributions to these different issues: (a) our generic modality that encapsulates the technical details of using speech synthesis; (b) the process followed to create four new voices, including two young adult and two elderly voices; and (c) some initial results exploring user preferences regarding the created voices. The preliminary studies carried out targeted groups including both young and older-adults and addressed: (a) evaluation of the intrinsic properties of each voice; (b) observation of users while using speech enabled interfaces and elicitation of qualitative impressions regarding the chosen voice and the impact of speech interaction on user satisfaction; and (c) ranking of voices according to preference. The collected results, albeit preliminary, yield some evidence of the positive impact speech interaction has on users, at different levels. Additionally, results show interesting differences among the voice preferences expressed by both age groups and genders.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Unsupervised segmentation of the vocal tract from real-time MRI sequences",
        "doc_scopus_id": "84921672332",
        "doc_doi": "10.1016/j.csl.2014.12.003",
        "doc_eid": "2-s2.0-84921672332",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Active appearance models",
            "Inter-frame differences",
            "Model convergence",
            "Real-Time MRI",
            "Regions of interest",
            "Speech production",
            "Unsupervised segmentation",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2014 Elsevier Ltd. All rights reserved.Advances on real-time magnetic resonance imaging (RT-MRI) make it suitable to study the dynamic aspects of the upper airway. One of the main challenges concerns how to deal with the large amount of data resulting from these studies, particularly to extract relevant features for analysis such as the vocal tract profiles. A method is proposed, based on a modified active appearance model (AAM) approach, for unsupervised segmentation of the vocal tract from midsagittal RT-MRI sequences. The described approach was designed considering the low inter-frame difference. As a result, when compared to a traditional AAM approach, segmentation is performed faster and model convergence is improved, attaining good results using small training sets. The main goal is to extract the vocal tract profiles automatically, over time, providing identification of different regions of interest, to allow the study of the dynamic features of the vocal tract, for example, during speech production. The proposed method has been evaluated against vocal tract delineations manually performed by four observers, yielding good agreement.",
        "available": true,
        "clean_text": "serial JL 272453 291210 291718 291723 291743 291782 291874 31 Computer Speech & Language COMPUTERSPEECHLANGUAGE 2014-12-25 2014-12-25 2015-03-29T00:56:13 S0885-2308(14)00128-4 S0885230814001284 10.1016/j.csl.2014.12.003 S300 S300.1 FULL-TEXT 2015-05-15T02:21:12.156754-04:00 0 0 20150901 20150930 2015 2014-12-25T02:28:36.199723Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor grantsponsorid highlightsabst orcid primabst ref 0885-2308 08852308 true 33 33 1 1 Volume 33, Issue 1 2 25 46 25 46 201509 September 2015 2015-09-01 2015-09-30 2015 Research Articles article fla Copyright © 2014 Elsevier Ltd. All rights reserved. UNSUPERVISEDSEGMENTATIONVOCALTRACTREALTIMEMRISEQUENCES SILVA S 1 Introduction 1.1 Related work 1.2 Contributions and overview 2 Active shape and appearance models 2.1 Shape modelling 2.2 Appearance modelling 3 Methods 3.1 Image data acquisition 3.2 Model building 3.3 Vocal tract segmentation 4 Evaluation 4.1 Precision 4.2 Accuracy 4.3 Computational performance 4.4 Discussion 5 Conclusions 5.1 Future work Acknowledgements References AVILAGARCIA 2004 288 291 M PROCINTCONFSIGNALPROCESSINGICSP EXTRACTINGTONGUESHAPEDYNAMICSMAGNETICRESONANCEIMAGESEQUENCES BABALOLA 2009 1435 1447 K BIRKHOLZ 2011 1422 1433 P BOUIX 2007 1207 1224 S BRESCH 2009 323 338 E CARIGNAN 2011 408 411 C PROC17THINTERNATIONALCONGRESSPHONETICSCIENCESICPHS ORALARTICULATIONNASALVOWELSINFRENCH CHALANA 1997 642 652 V CHANG 2009 122 135 H CHEN 2013 567 570 Y COOTES 1998 484 498 T PROCEUROPEANCONFERENCECOMPUTERVISION ACTIVEAPPEARANCEMODELS COOTES 2001 681 685 T COOTES 1994 327 336 T PROCBRITISHMACHINEVISIONCONFERENCE ACTIVESHAPEMODELSEVALUATIONAMULTIRESOLUTIONMETHODFORIMPROVINGIMAGESEARCH COOTES 1995 38 59 T DEMOLIN 2002 547 556 D ENGWALL 2003 43 48 O PROC6THINTSEMINARSPEECHPRODUCTIONISSP AREVISITAPPLICATIONMRIANALYSISSPEECHPRODUCTIONTESTINGASSUMPTIONS ENGWALL 2006 3 10 O PROC7THINTSEMINARSPEECHPRODUCTIONISSP INTERSPEAKERVARIATIONINARTICULATIONFRENCHNASALVOWELS ERYILDIRIM 2011 61 65 A PROCEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO AGUIDEDAPPROACHFORAUTOMATICSEGMENTATIONMODELINGVOCALTRACTINMRIIMAGES FREITAS 2014 13 20 J PROCBIOSIGNALS2014 VELUMMOVEMENTDETECTIONBASEDSURFACEELECTROMYOGRAPHYFORSPEECHINTERFACE GAO 2010 145 158 X HAGEDORN 2011 409 412 C PROCINTERSPEECH AUTOMATICANALYSISSINGLETONGEMINATECONSONANTARTICULATIONUSINGREALTIMEMAGNETICRESONANCEIMAGING HOWING 1999 59 67 F IONITA 2011 453 460 M PROCCOMPUTERVISIONWORKSHOPSICCVWORKSHOPS REALTIMEFEATUREPOINTTRACKINGAUTOMATICMODELSELECTION JAYENDER 2013 281 292 J KATSAMANIS 2011 2841 2844 A PROCINTERSPEECH VALIDATINGRTMRIBASEDARTICULATORYREPRESENTATIONSVIAARTICULATORYRECOGNITION KOUWENHOVEN 2009 2863 2873 E KUMAR 2013 215 219 K LAMMERT 2010 1572 1575 A PROCINTERSPEECH DATADRIVENANALYSISREALTIMEVOCALTRACTMRIUSINGCORRELATEDIMAGEREGIONS MAEDA 1979 S PROC10EMEJOURNEESDETUDESURLAPAROLE UNMODELEARTICULATOIREDELALANGUEAVECDESCOMPOSANTESLINEAIRES MARTINS 2008 925 952 P MARTINS 2012 231 240 P PROCIBERSPEECH2012VIIJORNADASENTECNOLOGIADELHABLAIIIIBERIANSLTECHWORKSHOP VELARMOVEMENTINEUROPEANPORTUGUESENASALVOWELS MCGUINNESS 2010 434 444 K NARAYANAN 2011 837 840 S PROCINTERSPEECH AMULTIMODALREALTIMEMRIARTICULATORYCORPUSFORSPEECHRESEARCH OLIVEIRA 2012 2690 2693 C PROCINTERSPEECH MRISTUDYORALARTICULATIONEUROPEANPORTUGUESENASALVOWELS OLIVEIRA 2009 480 483 C PROCINTERSPEECH SPEECHRATEEFFECTSEUROPEANPORTUGUESENASALVOWELS OLIVEIRA 2007 405 408 C PROCINTERNATIONALCONGRESSPHONETICSCIENCESICPHS GESTURESTIMINGINEUROPEANPORTUGUESENASALS PENG 2010 662 665 T PROCINTCONFACOUSTICSSPEECHSIGNALPROCESSINGICASSP ASHAPEBASEDFRAMEWORKSEGMENTATIONTONGUECONTOURSMRIDATA POPOVIC 2007 169 181 A PROCTOR 2010 1576 1579 M PROCINTERSPEECH RAPIDSEMIAUTOMATICSEGMENTATIONREALTIMEMAGNETICRESONANCEIMAGESFORPARAMETRICVOCALTRACTANALYSIS RAEESY 2013 1328 1331 Z PROC10THINTERNATIONALSYMPOSIUMBIOMEDICALIMAGINGISBI AUTOMATICSEGMENTATIONVOCALTRACTMRIMAGES SALTZMAN 1989 333 382 E SEISE 2007 666 677 M SESHADRI 2012 1255 1269 K SHOSTED 2012 455 465 R SHOSTED 2012 2182 2185 R PROCINTERSPEECH USINGMAGNETICRESONANCEIMAGEPHARYNXDURINGARABICSPEECHSTATICDYNAMICASPECTS SILVA 2012 214 221 S PROCICIAR2012LNCSVOL7325 SEGMENTATIONANALYSISORALNASALCAVITIESMRTIMESEQUENCES SILVA 2013 459 466 S PROCICIAR2013LNCSVOL7950POVOADEVARZIM SEGMENTATIONANALYSISVOCALTRACTMIDSAGITTALREALTIMEMRI SILVA 2013 1307 1311 S PROCINTERSPEECH TOWARDSASYSTEMATICQUANTITATIVEANALYSISVOCALTRACTDATA STONE 2001 1026 1040 M SUNG 2009 359 367 J TEIXEIRA 2005 1435 1448 A TEIXEIRA 2012 306 317 A PROCPROPOR2012LNCSVOL7243 REALTIMEMRIFORPORTUGUESEDATABASEMETHODSAPPLICATIONS TEIXEIRA 2012 318 328 A COMPUTATIONALPROCESSINGPORTUGUESELANGUAGEVOL7243LECTURENOTESINCOMPUTERSCIENCE PRODUCTIONMODELINGEUROPEANPORTUGUESEPALATALLATERAL TIEDE 2000 25 28 M PROC5THSEMINARSPEECHPRODUCTIONISSP CONTRASTSINSPEECHARTICULATIONOBSERVEDINSITTINGSUPINECONDITIONS UDUPA 2006 75 87 J VANGINNEKEN 2002 924 933 B VANBELLE 2009 82 100 S VASCONCELOS 2011 732 742 M WILLIAMS 1976 619 627 G WRENCH 2011 2161 2164 A PROCINTERNATIONALCONGRESSPHONETICSCIENCESICPHS ULTRASOUNDPROTOCOLCOMPARETONGUECONTOURSUPRIGHTVSSUPINE ZHANG 2008 260 280 H ZHANG 2012 265 277 S ZIJDENBOS 1994 716 724 A SILVAX2015X25 SILVAX2015X25X46 SILVAX2015X25XS SILVAX2015X25X46XS item S0885-2308(14)00128-4 S0885230814001284 10.1016/j.csl.2014.12.003 272453 2015-03-29T01:53:17.160521-04:00 2015-09-01 2015-09-30 true 5079405 MAIN 22 57749 849 656 IMAGE-WEB-PDF 1 si9 433 16 98 si8 669 48 101 si7 144 14 15 si6 361 14 83 si5 919 48 216 si4 524 48 84 si3 120 10 11 si2 693 19 209 si12 1121 49 172 si11 718 37 119 si10 131 13 11 si1 319 15 64 gr9 112932 420 721 gr8 18538 109 414 gr7 34626 442 301 gr6 108242 295 715 gr5 68713 325 667 gr4 24148 272 272 gr3 22082 167 341 gr2 42713 218 715 gr15 18906 197 275 gr14 68226 367 592 gr13 58329 221 689 gr12 22964 223 316 gr11 83480 615 715 gr10 20218 163 338 gr1 30174 256 349 fx3 32916 168 658 fx2 107 7 7 fx1 123 11 7 fx2 395 68 67 fx1 491 102 68 gr9 15953 128 219 gr8 2822 58 219 gr7 4563 163 111 gr6 7382 90 219 gr5 12768 107 219 gr4 12615 164 164 gr3 10681 108 219 gr2 5482 67 219 gr15 3542 157 219 gr14 12699 136 219 gr13 8727 70 219 gr12 14908 155 219 gr11 7032 164 190 gr10 10148 106 219 gr1 13867 160 219 fx3 2671 56 219 YCSLA 697 S0885-2308(14)00128-4 10.1016/j.csl.2014.12.003 Elsevier Ltd Fig. 1 Example of an MRI image depicting relevant anatomical structures. Fig. 2 Processing pipeline depicting the main stages of articulatory studies using RT-MRI. At the centre, in more detail, the segmentation stage, the subject matter of this paper. Fig. 3 Desired segmentations for an open (left) and closed (right) velum configuration. Fig. 4 Example image showing the landmarks defined over the vocal tract. Fig. 5 Vocal tract images on the training set depicting the defined landmarks for nasal (top) and oral (bottom) vowels. Fig. 6 First four variation modes for the oral (top row) and nasal (bottom row) AAM models. Variation shown for the variance interval − 3 σ , 3 σ . Fig. 7 Detail of the segmentation method showing the steps considered to choose the proper model (oral or nasal) to segment each image frame. Fig. 8 Velar port region analysis: a small region between the segmented velum tip and pharynx is used to detect if the velum is open by computing the average intensity and comparing it with a specified threshold. Fig. 9 Segmentation examples. Each row presents vocal tract segmentations for one speaker. These cover not only oral and nasal vowels but also configurations presenting closed lips. Different regions of interest in the vocal tract are represented in alternating colours and line styles. Fig. 10 Two examples of the regions covered by moving the mean model on the [−15, 15] interval, on both axes. The covered regions are presented brighter than the remaining image. The mean model is presented in the initial position for reference. Fig. 11 Influence of initial model location on segmentation: (a) and (b) depict examples of the DSC values distribution obtained for different model initialisation positions (brighter regions correspond to good initialisations); (c) shows box plots for the overall results on how the method performed using all good initialisations to segment four sequences (30 frames per sequence), for all speakers, when compared to a reference segmentation. Fig. 12 Screenshot of the delineation tool while one observer is setting the vocal tract contour. The contour of the lower lip and tongue have already been marked. Fig. 13 Examples of segmentations performed by the observers involved in the evaluation study and the corresponding segmentation provided by the proposed method. Fig. 14 Regional difference between the observers and the segmentation method: (a) box plot depicting distance (in pixels) for each region considered and a coloured mask of difference incidence for speaker CM and (b) notable examples of observed differences between the observers and the proposed method. Fig. 15 Box plots for the average agreement provided by the Williams index, over all images, computed for the segmentations performed by each observer and for the proposed segmentation method. Notches, on the box plots, represent the 95% confidence interval. Table 1 Comparison of proposed segmentation method with observer segmentations: mean (μ) and standard deviation (σ) values for DSC, sensitivity (p), specificity (q) and specificity corrected to the bounding box enclosing both segmentations (q′). DSC (%) p (%) q (%) q′ (%) Observer μ σ μ σ μ σ μ σ OBS1 83 5 86 7 99 0 94 3 OBS2 80 5 89 5 99 0 92 3 OBS3 84 3 85 6 99 0 95 2 OBS4 82 5 80 8 99 0 95 3 Overall 82 5 85 7 99 0 94 3 Table 2 Summary of the main features characterising the AAM alternatives tested. Feature Proposed Tested alternatives Alternate oral/nasal Single oral nasal Oral Nasal Num. models 2 2 1 1 1 Training images Oral/nasal Oral/nasal Oral+nasal Oral Nasal Initialisation Yes No Init. model per frame Prev. frame Mean model Search strategy One stage Four stages Full resolution Multi-resolution Table 3 On the left, overall comparison results of segmentations using traditional AAM based segmentations with those performed manually by observers. The comparison values for the proposed method are shown for reference, on the top row. On the right, presented in decreasing order of mean DSC value, box plots depicting the distribution of the DSC values obtained. Table 4 Segmentation times in seconds (per frame) for the manual segmentations, the different AAM alternatives tested and for the proposed method. Segmentation method μ(s) σ(s) Observers 54 18 AAM alternatives 7 1 Proposed method 3 1 ☆ This paper has been recommended for acceptance by P. Jackson. Unsupervised segmentation of the vocal tract from real-time MRI sequences Samuel Silva a ⁎ António Teixeira b a a Inst. Electronics Eng. and Telematics of Aveiro (IEETA), University of Aveiro, Aveiro, Portugal Inst. Electronics Eng. and Telematics of Aveiro (IEETA), University of Aveiro Aveiro Portugal b Dept. Electronics, Telecommunications and Informatics (DETI), University of Aveiro, Aveiro, Portugal Dept. Electronics, Telecommunications and Informatics (DETI), University of Aveiro Aveiro Portugal ⁎ Corresponding author at: IEETA, Campus Univ. de Santiago, 3810-193 Aveiro, Portugal. Tel.: +351 234370500. Advances on real-time magnetic resonance imaging (RT-MRI) make it suitable to study the dynamic aspects of the upper airway. One of the main challenges concerns how to deal with the large amount of data resulting from these studies, particularly to extract relevant features for analysis such as the vocal tract profiles. A method is proposed, based on a modified active appearance model (AAM) approach, for unsupervised segmentation of the vocal tract from midsagittal RT-MRI sequences. The described approach was designed considering the low inter-frame difference. As a result, when compared to a traditional AAM approach, segmentation is performed faster and model convergence is improved, attaining good results using small training sets. The main goal is to extract the vocal tract profiles automatically, over time, providing identification of different regions of interest, to allow the study of the dynamic features of the vocal tract, for example, during speech production. The proposed method has been evaluated against vocal tract delineations manually performed by four observers, yielding good agreement. Keywords Vocal tract Segmentation Real-time MRI 1 Introduction During speech production, the vocal tract configuration continuously changes over time. This dynamic nature has long been recognised as of paramount importance to study speech production (Saltzman and Munhall, 1989) and should cover the characterisation of the position and movement of the different articulators involved, such as the tongue, lips and velum (Fig. 1 ). Several techniques have been proposed and used to gather data to enable the study of these dynamic aspects such as electromagnetic midsagittal articulography (EMMA) (e.g., Oliveira and Teixeira, 2007) or ultrasound (e.g., Wrench et al., 2011). Even though they provide high frame rates, they present some limitations by focusing in a restricted set of regions of the vocal tract. In recent years, real-time magnetic resonance imaging (RT-MRI) has been used for speech studies (e.g., Narayanan et al., 2011; Teixeira et al., 2012), allowing enough frame rate to provide useful data regarding the position and coordination of the different articulators, over time, while potentially avoiding the hyperarticulation effect observed in sustained productions (i.e., the speaker sustains vowel production while a single static image is acquired) (Engwall, 2003). Although typically acquired at the midsagittal plane, any other plane of interest may be considered (Silva et al., 2012). A wide range of applications can profit from these dynamic studies (Höwing et al., 1999), such as the assessment of swallowing disorders (Kumar et al., 2013) or the characterisation of the articulatory properties of speech (Oliveira et al., 2012; Shosted et al., 2012; Silva et al., 2013). A notable example of the latter is the study of nasal and oral vowels (for example, considering European Portuguese, the second sound in “canto” ([I] sing) and “cato” (cactus)). These have traditionally been considered to differ essentially on the lowering of the velum, for nasal vowels, without any additional articulatory adjustment, but a few studies have recently shown evidence of modifications occurring also at the tongue and lips (Engwall et al., 2006; Carignan, 2011; Shosted et al., 2012a). The articulation of European Portuguese (EP) nasal vowels has been addressed by the authors and colleagues (Martins et al., 2008; Oliveira et al., 2009), focusing on velum dynamics or using limited tongue information obtained from EMMA studies. Real-time MRI has been acquired (Teixeira et al., 2012) to extend these studies with a characterisation of the oral configuration of EP nasal vowels, important, for example, for articulatory synthesis (Teixeira et al., 2005; Birkholz et al., 2011). The often used visual comparison among different image frames is of limited use due to the subjective nature of the comparison and the difficulty to cross compare among multiple images. Therefore, relevant data regarding the vocal tract and different articulators must be extracted for analysis. Real-time MRI studies rapidly result in several thousands of images and one of the main challenges concerns how to deal with such a large amount of data in order to extract relevant features and provide researchers with the materials and visualisations that allow systematic analysis (e.g., Silva et al., 2013). In this scenario, manual segmentation of each image is not only unthinkable, given the large number of images, but also inadvisable in a scenario where a large number of observers might be available to perform the annotation, since the noisy nature of the images makes it difficult to maintain consistency intra- and inter-observer. Therefore, a systematic (semi-)automatic method to extract the relevant data should be used. 1.1 Related work Analysis of dynamic MRI vocal tract data has typically been performed looking into pattern variations at pixel level, with no segmentation of the anatomical structures of interest (Demolin et al., 2002), or focusing on specific regions, e.g., tongue dorsum (Stone et al., 2001). In recent years, a few authors have presented methods to segment the vocal tract (or specific articulators) from MRI. Avila-Garcia et al. (2004) perform automatic tongue segmentation on dynamic MRI studies by combining active shape models (ASM), trained from 39 manually annotated images, with the dynamic Hough transform. To constrain the search in Hough space, the full image sequences are considered in order to find a global optimum. Although the proposed method seems promising, the authors report a high computational complexity of this approach to the point that only a single variation mode for the ASM could be used. This fact, and the lack of quantitative data regarding its performance and the effects of considering the Hough transform, preclude any consideration regarding its merits. Bresch and Narayanan (2009) proposed a method that uses data on the spatial frequency domain, extracted from k-space, to perform segmentation of the vocal tract from RT-MRI. Each image in the sequence is processed independently and explicit identification of the different articulators is provided based on an anatomically informed geometric model. Peng et al. (2010) use a shape-based method to segment the tongue contour in MRI images of sustained productions of different sounds. Shape priors are obtained from a set of 39 images from one speaker and then used to define a curve representation which is the base for segmenting 64 images from three other speakers. Prior to segmentation, the images are roughly aligned and scaled to match the pose of the speaker considered for the training set. Enhancements to this method are proposed by Eryildirim et al. (2011) to allow a correspondence between the contours and include an automated method to detect the tongue extremities (lingual frenulum and epiglottis). Proctor et al. (2010) describe a vocal tract segmentation method applied to RT-MRI. Based on the works of Maeda (1979) a grid is superimposed on each image frame. To perform segmentation, the pixel intensity profiles along the line grids are analysed looking for local minima and the vocal tract defined by an optimal path between the relevant minima. A post-processing is applied to the extracted vocal tract profile to smooth the contour segment corresponding to the tongue. Vasconcelos et al. (2011) have presented an approach to vocal tract segmentation using active shape/appearance models applied to MRI images of the vocal tract for several relevant sustained productions of EP sounds. Nevertheless, the application scope is limited as segmentation is only applied to images from a single speaker and validation of the proposed method is performed using four images (with 21 images in the training set). Katsamanis et al. (2011) also propose an active shape model trained from 460 utterances of a single speaker (circa 30,000 vocal tract images). Note, however, that the purpose of the presented work was not segmentation, which was performed using the method by Bresch and Narayanan (2009). The main goal was to obtain a model that could provide data on the different/relevant shape properties of the vocal tract during speech production to distinguish between different types of articulation. Raeesy et al. (2013) perform vocal tract segmentation from RT-MRI images by using oriented active shape models on each image frame separately. They also propose an automatic method to set the landmarks used for training. Since these are computed from manually segmented contours, it is not yet clear how much is gained from such method and it introduces further variability in landmark position, regarding anatomical landmarks. This results in a lack of landmarks in important points, such as the upper lip, leading to poor segmentation results as depicted in Fig. 4 of Raeesy et al. (2013). Furthermore, careful positioning of landmarks might be very useful for easier contour partitioning (e.g., identifying different regions of interest, such as the tongue or the velum). The authors (Silva et al., 2013) presented a first segmentation method to tackle vocal tract segmentation from RT-MRI, based on region growing propagation over time and considering different regions of the vocal tract, with different intensity properties. Even though this method generally provides very good segmentations, it poses some difficulties when a strong contact between the tongue dorsum and the hard palate is observed. In these situations the segmentation can be edited manually, to separate both structures, but it requires a considerable amount of work and often affects tongue shape. Furthermore, even considering that, at most times, user intervention was not needed, it was still important to supervise the segmentation outputs in case it was necessary to tweak seed point position or adjust threshold intervals. Regarding image acquisition, no standard protocol exists for RT-MRI of the vocal tract. Even though, for static images, the final results might be comparable in terms of image resolution and quality, for RT-MRI acquisitions there is a lot of variability among research groups regarding the used acquisition protocols, resulting in different frame rates with significant impact on image resolution and noise levels. It is also important to highlight that none of the works presented in the literature performs thorough evaluation of the proposed methods. Evaluation is often qualitative (Bresch and Narayanan, 2009), performed over a small image set (Vasconcelos et al., 2011; Raeesy et al., 2013) or considering annotations by a single observer (Proctor et al., 2010). 1.2 Contributions and overview A novel segmentation approach is proposed, based on active appearance models (AAMs), to address the task of segmenting a large database of RT-MRI images acquired to study EP vowels (for which k-space data is not available and avoiding the frequency domain analysis complexity). Its main purpose is to tackle the issues detected in our previous work (Silva et al., 2013) and to build a more versatile framework, providing suitable data for parametric vocal tract analysis (Silva et al., 2013) It is important to clarify that these RT-MRI databases, acquired for articulatory studies, involve a fair amount of resources and do not typically include data for many speakers. Instead, they include extensive data for each speaker (in our case, around 4000 images/speaker). Therefore, our aim is not to provide a segmentation method that can be used, off-the-shelf, with new speakers. We aim, instead, to propose a segmentation method that, following clearly defined criteria for selecting a small number of manually annotated training images, allows unsupervised segmentation of the full database and later extension to other speakers, following the same criteria. Since we need to rely on the segmentations provided by the proposed method to perform automatic articulatory analysis, proper evaluation of the method is of the utmost importance and must be carried out to assess its precision and accuracy. Regarding performance, it was not a major goal to provide a very fast method since it can be left running offline, generating the data for later analysis. The work presented in this paper, considering the surveyed literature, provides several notable contributions: (a) Consideration of the sequential nature of the data, exploring the proximity between adjacent frames to minimise convergence problems, which, to the best of our knowledge, has never been successfully considered for the segmentation of the vocal tract in this kind of image. (b) Explicit consideration of two configurations of the vocal tract with an open and closed velum. This can have a major impact on automated parametric analysis of velum height if, as happens for some speakers, velum aperture does not necessarily result in a considerable height variation and automated analysis must rely on other factors to assess velar differences. Furthermore, it is a requirement for velar aperture assessment and very important if the extracted contours are to be inspected by observers, for exploratory analysis of the data. (c) Clear separation between the contour segments corresponding to different articulators (e.g., tongue and hard palate) is guaranteed at all times, even when they are in contact (coincide). (d) Single, high level, user defined initialisation of the method for each speaker and unsupervised operation thereafter. (e) Automatic identification of different regions of interest in the vocal tract based on the criterial landmark placement during training. The proposed method has also been evaluated regarding the influence of different initialisations and by comparing its outputs against segmentations performed manually by four observers on 50 images, yielding very good results. The evaluation procedure and the number of observers and images considered for the evaluation setting are also a notable difference regarding related works where evaluation, when performed, has been mostly done qualitatively or considering a very limited number of images and/or observers. The remainder of this paper is organised as follows: Section 2 presents a brief description of the basic aspects involved in using active shape/appearance models for segmentation; Section 3 describes how the AAM models were created and applied to segment the vocal tract from a large set of RT-MRI image sequences; Section 4 presents the evaluation protocol used to assess the quality of the resulting segmentations and discusses evaluation data regarding precision, accuracy and performance; finally, Section 5 presents some conclusions and ideas for further work. 2 Active shape and appearance models A brief description of the main aspects of active shape/appearance models is presented in order to provide a context to the presented work. For additional details concerning these methods, the reader is forwarded to Cootes et al. (1995, 2001), Van Ginneken et al. (2002). 2.1 Shape modelling To build active shape models (ASM) (Cootes et al., 1995; Van Ginneken et al., 2002) shapes need to be described by a set of L points (landmarks) which are initially defined manually over a set of N training images. For shape i, with i =1, …, N, the landmark vector is given by: (1) x i = ( x i 1 , y i 1 ) , … , ( x iL , y iL ) T After alignment of the different shapes, the mean shape x ¯ is given by: (2) x ¯ = 1 N ∑ i = 0 N x i The covariance can then be computed by (3) S = 1 N − 1 ∑ i = 1 N ( x i − x ¯ ) ( x i − x ¯ ) T . The eigenvectors, ϕ i , also known as variation modes, corresponding to the t largest eigenvalues, λ i , of the covariance matrix are considered, Φ =(ϕ 1|ϕ 2|…|ϕ t ), and used to express the shape of a new object (i.e., its landmarks vector) as a point distribution model (PDM): (4) x ≈ x ¯ + Φ b , where b =(b 1, b 2, …, b t ) T is a vector of weights associated to each variation mode. The number of variation modes considered, t, is chosen as the minimum that allows explaining a certain proportion, f v , of the total variance in the training shapes and typically ranges from 90% to 99.5%: (5) ∑ i = 1 t λ i ≥ f v ∑ i = 1 2 L λ i To attain the ASM, a grey-level appearance model must also be built. This is performed by considering the neighbourhood of each landmark: the normal direction at each landmark is determined (based on neighbour landmarks) and a grey-level profile is obtained sampling k pixels to either side. Similarly to what was performed with landmark coordinates, the mean grey-level profile and covariance can be computed leading to an expression analogous to Eq. (4). The grey-level appearance model used in conjunction with the PDM described in Eq. (2) can then be used to segment objects in new images by positioning the mean model over it and then searching for the best landmark positions that satisfy both the geometric characteristics defined by the PDM and the expected grey-level profiles. This search operation can be optimised if a multi-resolution scheme is used as proposed by Cootes et al. (1994). Instead of searching over the image at the original resolution, search can be performed at different resolution levels, starting at a coarser resolution and finishing at the original resolution. This improves speed and, since the grey-level profiles are also adjusted according to resolution, it also avoids that, during the first search iterations, the model is caught by fine features near its target structure. 2.2 Appearance modelling The ASM method can be further enhanced by building an active appearance model (AAM) (Cootes et al., 1998; Gao et al., 2010) which uses all the available data instead of just the landmark neighbourhoods. A PDM also needs to be built, but appearance is now modelled by distorting each of the training images so that the landmarks defined in each match the mean shape. Next, the region covered by the mean shape is sampled and the grey values used to build a texture model. Similar to what was performed for the PDM, the appearance model can then be expressed by: (6) g ≈ g ¯ + Φ g b g , where g ¯ is the mean texture, Φ g is the matrix of eigenvectors (texture variation modes derived from the training set), and b g is the weight vector. 3 Methods The research work carried out by the authors, concerning the articulatory characterisation of EP, includes all stages from RT-MRI image acquisition (Teixeira et al., 2012) to the extracted data analysis (Oliveira et al., 2012; Silva et al., 2013). To provide context, Fig. 2 depicts the main stages of the work highlighting, at the centre, the focus of this paper, image segmentation. In what follows details are provided regarding image acquisition and the different aspects concerning the development and application of the proposed segmentation method are described. 3.1 Image data acquisition The articulation of European Portuguese (EP) nasal vowels has been studied by our group using modalities such as EMMA (Oliveira et al., 2009) and static MRI Martins et al. (2008). More recently, RT-MRI has been acquired to extend these studies with further characterisation of the oral configuration of EP nasal vowels (Oliveira et al., 2012; Martins et al., 2012). It is important to note, as previously mentioned, that this imaging modality provides adequate data regarding the position and coordination of the different articulators over time (Hagedorn et al., 2011) and might provide a good choice to tackle the hyperarticulation effect observed in sustained productions (Engwall, 2003). Additionally, it might also help reduce the gravity effect on articulators for acquisitions in supine position (Tiede and Vatikiotis-Bateson, 2000). Therefore, beyond providing the data regarding the dynamic aspects of speech, RT-MRI can also support static studies, tackling some of the associated issues. Image sequences were acquired containing: (a) the five European Portuguese (EP) nasal vowels uttered in three word positions: initial, internal and final (e.g., the nonsense words “ampa, pampa, pan” [ p , p p , p ]); and (b) the eight EP oral vowels (e.g., “papa” or “pupa”). Acquisition was performed at the midsagittal plane of the vocal tract. The images were acquired on an unmodified 3.0T MR scanner (Magneton Tim Trio, Siemens, Erlanger, Germany) equipped with high performance gradients (Gmax = 45mT/m, rise time=0.2s, slew Rate=200T/m/s, and FOV=50cm). Custom 12-channel head and 4-channel neck phased-array coils were used for data acquisition. Parallel imaging (GRAPPA 2) and magnetic field gradients operating at FAST mode were used to speed up the acquisition. After localisation images, a T1 W 2D-midsagittal MRI slice of the vocal tract was obtained, using an Ultra-Fast RF-spoiled Gradient Echo (GE) pulse sequence (Single-Shot TurboFLASH), with a slice thickness of 8mm and the following parameters: TR/TE/FA=72ms/1.02ms/5°, Bandwidth=1395Hz/pixel, FOV (mm2)=210×210, reconstruction matrix of (128×128) elements with 50% phase resolution, in-plane resolution (mm2)=3.3×1.6, yielding a frame rate of 14images/s. Typically, each recorded sequence contained 75 images (taking around 5s to acquire) although some longer sequences (300 images) were also acquired, mostly with the speakers producing sequences of isolated vowels (e.g., [ ], [˜e], [ĩ], [õ], [ũ], [ ], etc.). Audio was recorded simultaneously with the RT-MRI images inside the MR scanner, at a sampling rate of 16,000Hz, using a fibreoptic microphone and manually annotated, using the software tool Praat in order to identify the time intervals corresponding to different sounds. The time intervals allow the determination (because both data are aligned) of the corresponding image frames. Data was acquired for three female speakers (CM, CO and SV), aged between 21 and 33, phonetically trained, with no history of hearing or speech disorders. It is important to note that RT-MRI acquisitions require a large amount of resources. A greater diversity of speakers, at this moment, could have been accomplished by reducing the number of sequences acquired per speaker, but this, beyond increasing the overhead regarding speaker preparation, would have a negative impact on the range of studies that can be performed using the data. The obtained frame rate of 14 frames/s, for this dataset, although not as high as reported by other authors (e.g., Lammert et al., 2010; Raeesy et al., 2013), has already been useful to support articulatory studies (e.g., Oliveira et al., 2012; Martins et al., 2012) and research on velar movement detection using surface electromyography (Freitas et al., 2014). Therefore, it constitutes an adequate dataset to demonstrate the proposed segmentation method. Further details concerning the image acquisition protocol and corpus can be found in Teixeira et al. (2012). 3.2 Model building There are two different kinds of vocal tract configurations that result in slightly different segmentations: those with an open velum and those with a closed velum (refer to Fig. 3 for an example of each case). From an analysis point of view by, for instance, a linguist, when looking just to the extracted vocal tract profile, it is easier to distinguish between an open and closed velum if the segmentation for the closed velum does not include the nasal cavity. Furthermore, velar aperture assessment requires that the velar passage is segmented. Quantitative assessment of velar differences might also profit from this approach (Silva et al., 2013): for some speakers velum height does not vary much between the open and closed velum configurations and the assessment of the bottom side of the velum is not enough to detect the difference. Building a single model to accommodate both oral and nasal options resulted in a less stable option as it often failed to converge to a proper solution. This happened because such a model encompasses much more variability in the velar region which, considering the noise in the images, was often a problem. An approach based on bifurcating contours (Seise et al., 2007) might be suitable, but added complexity which we considered avoidable. We opted for building a different model for each situation (hereafter referred to as the nasal model, which contains a nasal cavity, and the oral model) and define rules to decide, at each image frame, which model to use. The choice of which images to use for training the model was based on three main aspects: (a) there is significant variability among speakers concerning some characteristics of the vocal tract (e.g., size, angle of the pharynx) and therefore it would be important to include images from all three speakers; (b) training images should cover the most notably different vocal tract configurations present, relying on the model to adapt to intermediate configurations; and (c) as manual segmentation of the vocal tract is a tiresome, time consuming task, the number of images included should not be unnecessarily large. The goal is to build an AAM model with a small set of annotated images that allows proper segmentation of the entire database. Both oral and nasal models have been built using the same procedure, with the sole differences residing on the images used for training and the inclusion of the nasal cavity in the nasal model. Twenty-six landmark points have been defined manually along the vocal tract and covering the main anatomical structures of interest. Fig. 4 shows the landmarks depicted over a vocal tract image: 3 for each lip, 2 for the lingual frenulum, 1 at the tongue tip, 6 evenly distributed over tongue dorsum and back and 1 at tongue root, 4 along the pharynx, 3 on the velum and 3 at the hard palate and alveolar ridge. Interpolation was performed, adding 18 points between landmarks. The method used to manually define the landmarks also allowed setting secondary points, between landmarks, in order to guide how interpolation was performed between them. For the nasal model a few secondary points were used to include the nasal cavity. The training set contained a total of 51 images, 30 images for the nasal model, covering the three speakers and all EP nasal vowels and 21 images for the oral model, covering all speakers and oral vowels. These were selected from all occurrences available in the database, identified according to the annotations of the audio signal and, for each occurrence, selecting the most representative frame of the interval (e.g., velum clearly open). Images presenting acquisition artefacts that might hinder proper creation of the statistical model, e.g., unclear separation between tongue and velum, were not considered. Vocal tract configurations with the lips completely closed were also not included as preliminary experiments seemed to show better results when these were not used. Fig. 5 shows some of the images used and defined landmarks and Fig. 6 presents the first four variation modes for each model (oral and nasal). Although not the subject matter of this paper it is important to note that these variation modes are also an important result as they describe the most important articulatory characteristics of the vocal tract and might be an important tool for articulatory recognition (Vasconcelos et al., 2011; Katsamanis et al., 2011). 3.3 Vocal tract segmentation The AAM models created were used to perform segmentation of the vocal tract along the image sequences. An initialisation stage, requiring user supervision, was performed once for each speaker and for the nasal and oral models separately. Initialisation consisted in two tasks: the definition of the initial mean model position, over one image frame, and verification if a proper segmentation was obtained with such positioning. Mean model positioning is performed interactively: the user moves the mean model over the image and clicks on the desired position. Then, the segmentation method takes over and the model is automatically adjusted to the image using a four level multi-resolution search approach (Cootes et al., 1994). To assess if a proper segmentation resulted the user is required to validate it by simply checking if the model converged to the speaker's vocal tract size and if it reached the lips and nasal cavity. If any major adjustment problem is detected the mean model can be repositioned and the process repeated. This is the only step of the presented method requiring user intervention and the complete initialisation step takes around 10–15s. When compared with the initialisation of the methods presented by Bresch and Narayanan (2009) and Proctor et al. (2010), it is simpler, faster and performed at a higher level, since a single click is needed and no manual annotation of the vocal tract is required. The initialisation is performed over images of the vocal tract for [ ] and [a], since these configurations are closer to those depicted by the mean models. The two segmentations obtained in this stage were thereafter used as the initial models for each speaker (hereafter known as the speaker-specific initial models), instead of the mean models, to perform the segmentation of the first oral/nasal image frame in each sequence. Since this initial model is already close to several speaker dependant features (e.g., dimension, hard palate and pharynx configurations) search for the optimum solution could be performed at the highest image resolution and the model generally converged better/faster than when the mean model was used. For the remaining images beyond the first, in each sequence, using only the highest resolution for search, segmentation was performed using the final segmentation of the previous frame (Cootes et al., 1994; Ionita et al., 2011) as the starting condition. This approach was chosen since neighbour image frames typically presented vocal tract configurations which did not strongly differ and allowed for faster segmentation as it required a smaller number of iterations to converge. As noted by Ionita et al. (2011), when small displacements occur, it is inefficient to discard the current solution, at high resolution, and start the search back at the lowest resolution as this increases the chance of error (e.g., in the presence of image noise or contact between articulators). On the other hand, for large displacements, this approach might fail, but no severe case has been observed for our data. At most, the contour might not adjust immediately and take a couple of frames to do it. It might also be argued that using one segmentation as the initialisation of the next frame might pose problems if the segmentation starts to diverge. We observed that this does not occur often, but it might start to happen for configurations with strong contact between articulators (e.g., between tongue and hard palate), particularly if the contact lasts for a few frames and the lips are blurry. In this situation, the model typically returns to normal once the vocal tract configuration changes, a few frames ahead (e.g., the tongue or the lips move). For each segmented image frame, the proper model (oral or nasal) needed to be selected. One possibility would be to perform an a priori analysis of the image region concerned with the velar port (e.g., Silva et al., 2013), to extract velum aperture data, choosing the model accordingly. Taking into account that the change between models was not expected to be very frequent, i.e., the same model would be used in 5–6 frames before the other model was chosen, we apply one of the models and then perform analysis of the image in the vicinity of the landmarks located at the velum tip and at the top of the pharynx. Fig. 7 shows a diagram depicting the main steps considered to choose the proper model. The region between the two landmarks is analysed in order to compute the mean image intensity value (see Fig. 8 ). When the mean intensity between the landmarks exceeds half the maximum intensity of the image, the velum is considered closed and the oral model should be applied. If this is not the current model, then the oral model is applied to the same image frame. The error associated with this segmentation is compared with that obtained with the nasal model to check if it is smaller. If, even though the region between velum tip and top of the pharynx is above the intensity threshold, the error got larger, then the nasal model is kept. A similar procedure is followed for the case when the region between velum and pharynx is below the intensity threshold leading to the choice of the nasal model. Fig. 9 shows several examples of the obtained segmentations, for all speakers, and covering a wide variety of vocal tract configurations. These show smooth contours presenting very good adjustment to the different articulators. One important aspect to note is that, due to the positions chosen for the different landmarks, when creating the models, it is easy to automatically identify different regions of interest from the segmented contours (as depicted, in different colours and line styles, in Fig. 9) namely, the lips, tongue body and tip, velum, hard palate and pharynx, important to support analysis (e.g., Silva et al., 2013). Considering other ASM/AAM based vocal tract segmentation methods (Vasconcelos et al., 2011; Raeesy et al., 2013) our approach adds to them by: (1) requiring a single manual initialisation step per speaker (as opposed to one initialisation per image); (2) using two models to explicitly tackle oral and nasal configurations; (3) addressing data from multiple speakers (only one speaker considered in Vasconcelos et al., 2011); (4) using criterial positioning of landmarks allowing subsequent identification of different regions of interest in the vocal tract; and (5) starting the segmentation of each frame with the segmentation of the previous frame. 4 Evaluation The evaluation of segmentation methods should typically consider three main aspects (Udupa et al., 2006): precision, accuracy and performance. To assess how the proposed segmentation method performs on these three aspects, a set of evaluation tasks was devised and carried out as described in what follows. 4.1 Precision Precision, or reliability, deals with the similarity among segmentations obtained for the same data at different times. In our case, one important aspect that might affect segmentation is the mean model initial position, defined by the user, regarding how it affects repeatability (Chang et al., 2009; McGuinness and O’Connor, 2010; Seshadri and Savvides, 2012). The proposed method includes an initialisation stage in which the initial position for the mean model is set and the segmentation is performed for a single image frame to attain the speaker-specific initial model. Therefore, since the initialisation step is supervised by an observer, the initial mean model localisation will always result in a proper initialisation (otherwise, the localisation is changed until it does). Nevertheless, two aspects deserve attention: (1) how the position chosen for the mean model influences the obtained speaker-specific initial model and (2) how different speaker-specific initial models influence image sequence segmentation. Effect of position in speaker-specific model initialisation – considering the image used for initialisation, for each speaker, a proper vocal tract segmentation (as judged by an observer) was used as the reference initialisation. Considering a position where the mean model was roughly aligned with the vocal tract, translations from that position, on both axes, were applied in the (empirically set) range −15 to 15 pixels in 5 pixels steps. The initial speaker-specific model was then obtained for each of these locations. The reason by which the position used as a starting point for the displacements is not of paramount importance is that our main interest is not to find the optimal initial position (or all positions yielding good segmentations) for the model, but to characterise the effects of changing that position. The maximum mean model displacement values were chosen in order to encompass the region where a user could typically position the mean model. Fig. 10 presents two examples of the region covered by moving the mean model on the chosen intervals. For the sake of simplicity, the initial mean model positions for the oral and nasal models were kept the same. The new speaker-specific initial models thus obtained were compared with the reference initialisation using the Dice similarity coefficient (DSC) (Popovic et al., 2007): (7) DSC = 2 A ∩ B A + B The DSC measures the amount of overlap between two segmentations (A and B) normalised by the summed areas of both: therefore, it is 1 when the regions contained inside both contours coincide and 0 (zero) when they are completely different. The literature suggests that a value of DSC >0.7 represents a good overlap (Zijdenbos et al., 1994). Fig. 11 a and b presents two examples of DSC values distribution over the tested regions, obtained for two of the speakers. The brighter regions depict locations where the obtained speaker-specific initial models were closer to the reference model, while darker regions depict increasing differences between both. The plot in Fig. 11b, for example, shows how, for that speaker, initialisation needed more care, noticeable by a smaller region leading to good initialisation than for the speaker in Fig. 11a. Nevertheless, the gathered data shows evidence that the region where the user can place the mean model (and will result in a good initialisation) is not very restricted. Effect of different speaker-specific models on image sequence segmentation – in the evaluation step described above several mean model initial positions were tested resulting in different model initialisations (per speaker). Then, we analysed which positions provided good initialisations by comparing with a reference initialisation using the DSC. We now wanted to assess how these different initialisations resulted in differences during the segmentation of image sequences. First, the initialisations used as reference were considered for the segmentation of four image sequences per speaker (30 frames per sequence) providing reference segmentations. Then, all initialisations evaluated earlier which obtained a DSC above 0.8 were considered (Zijdenbos et al., 1994). The rationale behind the choice of only these initialisations for testing was that, in our method, the model initialisation is supervised by the user and, therefore, a good initialisation will always be used for segmentation. Next, the chosen initialisations were used to perform the segmentation of the four image sequences per speaker and the resulting segmentations compared with the reference segmentations. Fig. 11c shows the overall box plots drawn considering all sequences and speakers depicting how the segmentation generally evolves along the sequences. One notable aspect observed is that several initial locations yield a segmentation that differs more from the reference on the first few frames, depicted by the wider box plots (although generally keeping high DSC values), but then comes closer to the reference on the remaining frames. This shows that the different initialisations have a small influence on the final segmentation results. 4.2 Accuracy Accuracy, or validity, concerns how the proposed segmentations compare with true segmentation or, in most cases, a surrogate of true segmentation (Udupa et al., 2006). Several aspects of accuracy are assessed for the proposed method: (a) the overall comparison with manual segmentations performed by observers; (b) regional assessment of which parts of the vocal tract were the main source of existing differences; (c) agreement between the proposed method and the group of observers; and (d) the influence of design choices on accuracy. Experimental setting – To assess the accuracy of the proposed segmentation method, image frames of every oral/nasal vowel available on the database were randomly chosen from all speakers by selecting the last frame of the vowel occurrence based on the audio annotation. Furthermore, additional images where considered covering intermediate vocal tract configurations, namely presenting closed lips and the first frame for the nasal congeners of the EP cardinal vowels ([ , ĩ, ũ]), as some difference is expected on the vocal tract configuration over their production for EP (Oliveira et al., 2012). In total, 50 images were considered, none of which was part of the training sets for the AAMs. A supervised evaluation method was adopted (Zhang et al., 2008). A set of four observers, two radiographers (OBS1 and OBS3), and two phoneticists (OSB2 and OBS4) were asked to segment the vocal tract using a completely manual contour delineation tool (Fig. 12 ). The first radiographer (OBS1) and the first phoneticist (OBS2) had previous experience with RT-MRI images and speech studies, while the second element of both groups (OBS3 and OBS4) had general experience in their fields of work. None of these observers was involved in the vocal tract annotations of the training image set. The session started with a brief explanation regarding what was required, how the tool worked and each observer was allowed to experiment with contour delineation (by clicking points along the desired contour) over a training image which was not considered for evaluation. When the observers felt comfortable working with the delineation tool, they could proceed with the 50 segmentations. Each observer segmented the images in a different order to minimise the effects of fatigue or previous segmentations on the overall results. All observers performed the segmentations using the same desktop computer and similar ambient light conditions. Fig. 13 presents some of the segmented images, representative of the overall results, showing the manual and automatic segmentations. The vocal tract contours, obtained using the proposed segmentation method, were compared with the manual segmentations using the DSC, sensitivity (p) and specificity (q) (Jayender et al., 2013). Overall comparison (method vs. observers) – Table 1 presents the mean and standard deviation values for DSC, p and q obtained by comparing between the proposed method and the manual segmentations performed by each observer. It can be noted that the values for each observer, along with the overall values, express good agreement between the segmentations provided by the proposed method and those performed by observers. The specificity value is always 99% which was expected and explained by the fact that the size of the vocal tract is relatively small compared to the whole image (Zhang et al., 2012). To minimise this effect on specificity, the bounding box enclosing the corresponding segmentations (observer and segmentation method) was computed and a new specificity value computed (q′). These new specificity values are lower, but do not fall below 92%. Regional difference assessment – Even though the differences between the manual segmentations and those obtained using the proposed method were small, we also assessed if any particular region of the vocal tract had a more frequent discrepancy between the manual segmentations and those provided by the proposed segmentation method. To perform regional difference assessment, the contours provided by the segmentation method were divided into different regions exhibiting anatomical/articulatory relevance and the Euclidean distance computed from each point in the region towards the observer contours. Fig. 14 a shows box plots of the distances computed for each of the regions considered. A mask of the difference between the regions contained inside each manual segmentation and its corresponding automatic segmentation was computed and all masks added, by speaker. This resulted in an image in which the regions where differences are more frequent appear with a higher pixel value. These masks should not be looked at as an absolute measure of error incidence, but as a complementary view to that provided by the box plots. Fig. 14b shows an example of the overall difference mask for speaker CM. Excluding the difference denoted at the lips, which was expected as there is no clear feature delimiting where to start/stop the segmentation, it is possible to note that there is a stronger difference for the lingual frenulum and velar regions and that the lingual frenulum is where differences occur more frequently. In the case of the lingual frenulum, difference originated mostly from the automatic segmentation not adjusting properly (or as tightly as the observers) or different segmentation criteria used by the observers. Regarding the velar region, segmentation was more difficult due to the frequent presence of motion artefacts or an unclear passage between the oral and nasal cavities resulting in some observers assuming a closed velum. Fig. 14c shows examples of different situations which originated discrepancy between segmentations. On the top left, the observer considered a closed velum, while the segmentation method (and remaining observers) considered it open; on the top right, the observer, due to artefacts, did not fully segment the pharynx; on the bottom left image, the segmentation method did not segment the region between the lower lip and the tongue as tightly as the observer; and, on the bottom right, the observer considered that the epiglottis was clearly separated from the tongue while the segmentation method included it with the tongue. Agreement with observers group – Similarity measures such as the DSC allow to compare pairs of segmentations. Since we have a set of surrogate truths (provided by qualified observers) it is also interesting to assess the agreement of the proposed segmentation method with the group of observers. To determine a consensus among all observers is not trivial and several methods have been discussed in the literature (Vanbelle and Albert, 2009), which might even lead to different conclusions about the same data. To address these issues some measures have been proposed (Williams, 1976; Chalana and Kim, 1997; Kouwenhoven et al., 2009) such as the Williams agreement index. This index, widely used in the literature (Bouix et al., 2007; Babalola et al., 2009), can be expressed as: (8) WI i = ( n − 2 ) ∑ j ≠ i n D ij 2 ∑ j ≠ i n ∑ k ≠ i j − 1 D jk where D ij is a similarity/discrepancy measure between segmentations i and j (e.g., DSC) and n is the number of segmentations. The value provided by the Williams index expresses how a particular segmentation i compares with the group of the remaining segmentations and can be interpreted as follows: if greater than 1, the remaining segmentations are more similar to i than to each other; if close to 1, segmentations are as similar to i as they are to each other; and if below 1, remaining segmentations are more similar to each other than each is to i. The Williams index was computed for the proposed method versus the observers group and for each observer considering the remaining observers. Fig. 15 shows box plots depicting agreement as provided by the Williams index and presenting notches marking the 95% confidence interval. It can be noted that, for the segmentation method proposed, the Williams index attains a median slightly above 1. This provides some evidence that the segmentation method produces segmentations that are closer to those performed by the observers than the observers are among themselves. Assessment of alternative segmentation approaches – The evaluation data gathered allows to conclude that the proposed segmentation method provides very good accuracy results. Nevertheless, the different design choices made, such as initialising the segmentation of each frame with the segmentation of the previous frame, or using two models with different nasal cavity configurations, were performed considering the application scenario and on the base of preliminary experiences. In order to confirm that, as expected, these design choices actually improved the overall results of the segmentation, when compared with a more traditional AAM approach, additional assessment was performed. All the 50 image frames, manually segmented by the four observers, were also segmented using the traditional AAM approach, segmenting each image frame individually and considering four different alternatives to the proposed method which mainly differ in the training data used to create the models (main features summarised in Table 2 ): Alternate oral/nasal – similarly to the proposed method, this alternative also uses two models, each trained for one of the considered configurations (oral or nasal). The model to use, at each frame, is also chosen as described in Fig. 7. The major differences towards the proposed method are that no speaker specific initialisation is performed and, for each frame, the segmentation starts from the mean model and uses a four-stage multi-resolution search strategy (Cootes et al., 1994). Single oral+nasal – this alternative uses a single model trained using all the annotated images (oral and nasal). It does not include speaker specific initialisation and search is also performed using a multi-resolution strategy. Oral – this alternative uses a single model trained using the images annotated with no nasal cavity (oral model). No speaker specific initialisation is performed and search is also performed using a multi-resolution strategy. Nasal – this alternative is similar to the previous, but the model is trained using only the annotated images including the nasal cavity. The initial mean model position for all alternatives was set to the one used previously for the initialisation of the proposed method. The resulting segmentations were compared with the observer segmentations and Table 3 shows the overall comparison results for each of the alternatives. The comparison values show that the proposed segmentation method provides the best results. Notice also that the single model trained considering both oral and nasal configurations yielded the poorest results. This is explained by the greater instability created by using a single model to deal with the different configurations at the velum and nasal cavity. Furthermore, the box plots presented show that the proposed method consistently obtains good results while the tested alternatives present more dispersion. 4.3 Computational performance Our main goals, at this stage, did not consider any specific computational performance requirement other than allowing unsupervised segmentation in reasonable time (e.g., image data for one speaker processed overnight). To provide a reference against which to assess the computational performance of the proposed method we also measured segmentation times per image frame for the manual segmentations performed by the observers and for the other (more traditional) AAM alternatives tested in the previous section. The segmentations were performed on a quad-core i7-3520M CPU, 8GB RAM, running Matlab 2011. Table 4 presents the different segmentation times measured. As expected, the proposed method performs a lot faster (3s/frame) than the manual segmentations (54s/frame) and, despite the added features, such as the automatic selection of the proper model to use (oral or nasal), it still performs better than the tested AAM alternatives (7s/frame), mostly due to fact that a single search stage, at full resolution, is performed. 4.4 Discussion In summary, the evaluation of different aspects of the proposed segmentation method provides evidence that: 1. The segmentations resulting from the proposed method show good accuracy when compared with segmentations performed manually by observers. Agreement, as measured by the Williams index, is stronger between the proposed method and the observers than among observers. 2. The choice of using two different models (oral and nasal), and initialisation of each frame segmentation with the segmentation of the previous frame, improved accuracy when compared with alternatives using individual image frame segmentation. This also reduced the number of search stages from four to a single stage performed at the highest resolution. 3. Positioning of the mean model, to build the speaker-specific initialisation, works well on a reasonably sized region. Nevertheless, as this varies from speaker to speaker, user supervision at this stage is still needed, to check if initialisation is done properly. 4. For the different speaker-specific initialisations possible, the resulting segmentations do not present much variability among them, with an exception to the first few frames of the image sequence where differences are more prominent. This is evidence that the only user dependant task, on our method, is prone to have little influence on the resulting segmentations. 5. Although not an important goal at this stage, the proposed method, beyond including automatic choice of the proper model (oral or nasal) to use, and being more accurate than the tested alternatives, also runs faster. Considering the works discussed in the related work, no quantitative comparison with those methods is performed due to a lack of common image databases that could be used for that purpose. Note that all proposed methods address the problem dealing with databases containing images obtained using different acquisition protocols and frame rates, yielding very different image resolution and quality, which might influence the resulting segmentations. Nevertheless, based on what can be observed in the literature, for full vocal tract segmentations, when compared to Vasconcelos et al. (2011) and Raeesy et al. (2013), dealing with static images (expected to have better quality), our method provides better segmentations resulting in smoother contours, correctly covering all articulators. Compared to Bresch and Narayanan (2009), our approach provides similar results with the main difference of being applied in the image rather than the frequency domain and taking less time (Bresch and Narayanan (2009) report 20min/per frame). Furthermore, the works presented in the literature limit evaluation of the proposed methods to a qualitative evaluation (e.g., Bresch and Narayanan, 2009) or to the assessment of segmentation results in a very small number of images (Vasconcelos et al., 2011; Raeesy et al., 2013) with quantitative evaluations applied to no more than six images or using a single human observer as reference (Proctor et al., 2010). 5 Conclusions In this paper we propose a model based method to perform the segmentation of the vocal tract from midsagittal RT-MRI image sequences. While segmentation of this kind of data has been previously addressed, that was performed without attending to the sequential nature of the data, i.e., by processing each image without accounting for its neighbours. The proposed method allowed tackling the segmentation of a large RT-MRI database by building a model-based approach using a small set of annotated images. Although not specifically included in the training set, the vocal tract configurations along the sequences, such as those exhibiting tightly closed lips, were no particular challenge to the proposed method. This was possible by exploring the small inter-frame variability, in each image sequence, using the segmentation of one image as starting point for the next. Evaluation results, comparing the segmentations provided by the proposed method with those performed by four observers, show that the presented approach performs well, providing good levels of precision, accuracy and performance. Our approach, using two different models (nasal and oral) to address the different velum configurations (open and closed), allowed accurate segmentations that provide important extra data regarding velum aperture assessment, whether it is performed visually, by a phoneticist, or using computational tools. This is also a notable difference to the works of Proctor et al. (2010), Vasconcelos et al. (2011) and Raeesy et al. (2013). One of the main positive implications resulting from the proposed method is that it allows innovative approaches regarding automatic quantitative analysis of vocal tract data, as presented by the authors in Silva et al. (2013), considering the whole database available instead of a few chosen occurrences. The image processing is performed sequentially, hindering parallel processing of the images inside a sequence which, as advocated by Bresch and Narayanan (2009), might improve performance on a cloud computing scenario. Nevertheless, we do not consider it a problem since parallel processing can still be performed for multiple sequences at once. The dominant factor in our database is not the size of each sequence, but their number. Besides, although, at this moment, we do not aim for a scenario where on-the-fly segmentation must be performed, the performance of the current implementation (in Matlab) is fast enough to provide, upon request, the segmentation of a particular image sequence (75 images) in less than four minutes. Considering the different characteristics of the images gathered in RT-MRI studies of the upper airway, due to the different acquisition protocols used, the specific AAM models created for our database are most probably not directly applicable to other upper airway RT-MRI databases. Nevertheless, we consider that the proposed methodology is general enough to be used to deal with any upper airway RT-MRI database. The only requirement is that the models are retrained using a new training set, chosen according to the same criteria used here, i.e., selecting the frames for the most distinctive vocal tract configurations in the database. This, of course, is not limited to oral and nasal vowels as is the case of our database. For example, if an RT-MRI study of the upper airway is to include the articulation of lateral sounds (Teixeira et al., 2012) (e.g., /l/ as in “sal” (salt)), since the configuration of such sounds presents very distinctive characteristics on the range of movements of the tongue tip, when compared to vowels, frames showing that configuration should also be included in the training set. 5.1 Future work The difficulty of imaging the hard palate using MRI is well known. This results in considerable variability in identifying its contour. For the proposed method not much variability has been observed in this region. Nevertheless, as the hard palate is a rigid structure, it is expected that it generally stays the same for every image. One simple approach that can be used to minimise the variability is to choose the hard palate segment of one of the contours (e.g., the one in the speaker specific initialisation), for each speaker, and replace the hard palate in every segmentation with that segment (Proctor et al., 2010). The addition of more speakers to the database was not tested, given the scarcity of speakers. Nevertheless, based on the obtained results, we consider that the proposed methodology can easily encompass additional speakers through the inclusion of a small set (no more than five per model) of annotated images of that speaker in the training set, chosen based on the same criteria. Enlarging the training set with data from additional speakers also iteratively improves the model in such a way that, in the future, it might be able to deal properly with new speakers without needing to be retrained. Although, at this moment, the training set is small and retraining the models can still be done in reasonable time, in the future, and to avoid completely retraining the models whenever a new speaker is added, incremental approaches for model evolution and learning might be considered (Sung and Kim, 2009; Chen et al., 2013). Regarding the possibility that using one segmentation as starting point for the next might cause problems, if the segmentation starts to diverge, we did not observe any severe situation for our data. Nevertheless, in case this problem becomes an issue, for other datasets, there are several solutions that might be adopted. For example, since the speaker-specific initialisations provide a reference of vocal tract dimension and location of different regions of interest, it is possible to test for divergence and then act accordingly (e.g., reverting to the speaker-specific initialisations). Contour self intersections were not addressed. As far as we could observe, self intersections do not happen often, and when they occur it is usually at the lips, when they are closed, and at the hard palate, when there is strong tongue contact. These do not pose critical problems for analysis and will be addressed in the future, for example, by post-processing the extracted contours. Considering the work being carried out regarding speech production, a validated segmentation framework, as the one presented, providing complete segmentations of the vocal tract, is an important tool for the systematic study of speech production, not only on normal speakers, but also on those presenting pathologies such as a cleft palate, allowing our research work to follow that path. Acknowledgements The authors thank the observers involved in the evaluation study and the anonymous reviewers for their helpful comments and suggestions. Research partially funded by FEDER through the Program COMPETE and by National Funds (FCT) in the context of HERON II (PTDC/EEA-PLP/098298/2008), IEETA Research Unit funding FCOMP-01-0124-FEDER-022682 (FCT-PEst-C/EEI/UI0127/2011) and project Cloud Thinking (funded by the QREN Mais Centro program, ref. CENTRO-07-ST24-FEDER-002031). References Avila-Garcia et al., 2004 M.S. Avila-Garcia J.N. Carter R.I. Damper Extracting tongue shape dynamics from magnetic resonance image sequences Proc. Int. Conf. on Signal Processing (ICSP) Beijing, China 2004 288 291 Babalola et al., 2009 K.O. Babalola B. Patenaude P. Aljabar J. Schnabel D. Kennedy W. Crum S. Smith T. Cootes M. Jenkinson D. Rueckert An evaluation of four automatic methods of segmenting the subcortical structures in the brain NeuroImage 47 2009 1435 1447 10.1016/j.neuroimage.2009.05.029 Birkholz et al., 2011 P. Birkholz B. Kroger C. Neuschaefer-Rube Model-based reproduction of articulatory trajectories for consonant-vowel sequences IEEE Trans. Audio Speech Lang. Process. 19 2011 1422 1433 10.1109/TASL.2010.2091632 Bouix et al., 2007 S. Bouix M. Martin-Fernandez L. Ungar M. Nakamura M.S. Koo R.W. McCarley M.E. Shenton On evaluating brain tissue classifiers without a ground truth NeuroImage 36 2007 1207 1224 10.1016/j.neuroimage.2007.04.031 Bresch and Narayanan, 2009 E. Bresch S. Narayanan Region segmentation in the frequency domain applied to upper airway real-time magnetic resonance images IEEE Trans. Med. Imaging 28 2009 323 338 10.1109/TMI.2008.928920 Carignan, 2011 C. Carignan Oral articulation of nasal vowels in French Proc. 17th International Congress of Phonetic Sciences (ICPhS) Hong Kong, China 2011 408 411 Chalana and Kim, 1997 V. Chalana Y. Kim A methodology for evaluation of boundary detection algorithms on medical images IEEE Trans. Med. Imaging 16 1997 642 652 10.1109/42.640755 Chang et al., 2009 H.H. Chang A.H. Zhuang D.J. Valentino W.C. Chu Performance measure characterization for evaluating neuroimage segmentation algorithms NeuroImage 47 2009 122 135 10.1016/j.neuroimage.2009.03.068 Chen et al., 2013 Y. Chen F. Yu C. Ai Sequential active appearance model based on online instance learning IEEE Signal Process. Lett. 20 2013 567 570 10.1109/LSP.2013.2257753 Cootes et al., 1998 T. Cootes G. Edwards C. Taylor Active appearance models Proc. European Conference on Computer Vision Freiburg, Germany 1998 484 498 Cootes et al., 2001 T. Cootes G. Edwards C. Taylor Active appearance models IEEE Trans. Pattern Anal. Mach. Intell. 23 2001 681 685 10.1109/34.927467 Cootes et al., 1994 T. Cootes C. Taylor A. Lanitis Active shape models: evaluation of a multi-resolution method for improving image search Proc. British Machine Vision Conference York, UK 1994 327 336 Cootes et al., 1995 T.F. Cootes C.J. Taylor D.H. Cooper J. Graham Active shape models – their training and application Comput. Vision Image Underst. 61 1995 38 59 Demolin et al., 2002 D. Demolin S. Hassid T. Metens A. Soquet Real-time MRI and articulatory coordination in speech Comptes Rendus Biol. 325 2002 547 556 Engwall, 2003 O. Engwall A revisit to the application of MRI to the analysis of speech production – testing our assumptions Proc. 6th Int. Seminar on Speech Production (ISSP) Sydney, Australia 2003 43 48 Engwall et al., 2006 O. Engwall V. Delvaux T. Metens Interspeaker variation in the articulation of French nasal vowels Proc. 7th Int. Seminar on Speech Production (ISSP) Ubatuba, Brazil 2006 3 10 Eryildirim et al., 2011 A. Eryildirim M.O. Berger A guided approach for automatic segmentation and modeling of the vocal tract in MRI images Proc. European Signal Processing Conference (EUSIPCO) Barcelona, Spain 2011 61 65 Freitas et al., 2014 J. Freitas M.S. Dias S. Silva A. Teixeira Velum movement detection based on surface electromyography for speech interface Proc. BIOSIGNALS 2014 Angers, France 2014 13 20 Gao et al., 2010 X. Gao Y. Su X. Li D. Tao A review of active appearance models IEEE Trans. Syst. Man Cybern. C: Appl. Rev. 40 2010 145 158 10.1109/TSMCC.2009.2035631 Hagedorn et al., 2011 C. Hagedorn M.I. Proctor L. Goldstein Automatic analysis of singleton and geminate consonant articulation using Real-Time Magnetic Resonance Imaging Proc. Interspeech Florence, Italy 2011 409 412 Höwing et al., 1999 F. Höwing L.S. Dooley D. Wermser Tracking of non-rigid articulatory organs in X-ray image sequences Comput. Med. Imaging Gr. 23 1999 59 67 10.1016/S0895-6111(98)00067-6 Ionita et al., 2011 M. Ionita P. Tresadern T. Cootes Real time feature point tracking with automatic model selection Proc. Computer Vision Workshops (ICCV Workshops) Barcelona, Spain 2011 453 460 10.1109/ICCVW.2011.6130276 Jayender et al., 2013 J. Jayender E. Gombos S. Chikarmane D. Dabydeen F.A. Jolesz K.G. Vosburgh Statistical learning algorithm for in situ and invasive breast carcinoma segmentation Comput. Med. Imaging Gr. 37 2013 281 292 10.1016/j.compmedimag.2013.04.003 Katsamanis et al., 2011 A. Katsamanis E. Bresch Ramanarayanan V. Nara Validating RT-MRI based articulatory representations via articulatory recognition Proc. Interspeech Florence, Italy 2011 2841 2844 Kouwenhoven et al., 2009 E. Kouwenhoven M. Giezen H. Struikmans Measuring the similarity of target volume delineations independent of the number of observers Phys. Med. Biol. 54 2009 2863 2873 Kumar et al., 2013 K.V. Kumar V. Shankar R. Santosham Assessment of swallowing and its disorders – a dynamic MRI study Eur. J. Radiol. 82 2013 215 219 10.1016/j.ejrad.2012.09.010 Lammert et al., 2010 A. Lammert M. Proctor S. Narayanan Data-driven analysis of realtime vocal tract MRI using correlated image regions Proc. Interspeech Makuhari, Japan 2010 1572 1575 Maeda, 1979 S. Maeda Un modèle articulatoire de la langue avec des composantes lineaires Proc. 10ème Journées d’Etude sur la Parole 1979 Martins et al., 2008 P. Martins I. Carbone A. Pinto A. Silva A. Teixeira European Portuguese MRI based speech production studies Speech Comm. 50 2008 925 952 Martins et al., 2012 P. Martins C. Oliveira S. Silva A. Teixeira Velar movement in European Portuguese nasal vowels Proc. IberSpeech 2012 – VII Jornadas en Tecnología del Habla and III Iberian SLTech Workshop Madrid, Spain 2012 231 240 McGuinness and O’Connor, 2010 K. McGuinness N.E. O’Connor A comparative evaluation of interactive segmentation algorithms Pattern Recognit. 43 2010 434 444 10.1016/j.patcog.2009.03.008 Narayanan et al., 2011 S. Narayanan E. Bresch P. Ghoosh L. Goldstein A. Katsamanis Y. Kim A.C. Lammert M. Proctor V. Ramanarayanan Y. Zhu A multimodal real-time MRI articulatory corpus for speech research Proc. Interspeech Florence, Italy 2011 837 840 Oliveira et al., 2012 C. Oliveira P. Martins S. Silva A. Teixeira An MRI study of the oral articulation of European Portuguese nasal vowels Proc. Interspeech Portland, Oregon, USA 2012 2690 2693 Oliveira et al., 2009 C. Oliveira P. Martins A. Teixeira Speech rate effects on European Portuguese nasal vowels Proc. Interspeech Brighton, UK 2009 480 483 Oliveira and Teixeira, 2007 C. Oliveira A. Teixeira On gestures timing in European Portuguese nasals Proc. International Congress of Phonetic Sciences (ICPhS) Saarbrücken, Germany 2007 405 408 Peng et al., 2010 T. Peng E. Kerrien M.O. Berger A shape-based framework to segmentation of tongue contours from MRI data Proc. Int. Conf. Acoustics Speech and Signal Processing (ICASSP) Dallas, Texas, USA 2010 662 665 10.1109/ICASSP.2010.5495123 Popovic et al., 2007 A. Popovic M. Fuente M. Engelhardt K. Radermacher Statistical validation metric for accuracy assessment in medical image segmentation Int. J. Comput. Assist. Radiol. Surg. 2 2007 169 181 10.1007/s11548-007-0125-1 Proctor et al., 2010 M. Proctor D. Bone A. Katsamanis S. Narayanan Rapid semi-automatic segmentation of real-time Magnetic Resonance images for parametric vocal tract analysis Proc. Interspeech Makuhari, Japan 2010 1576 1579 Raeesy et al., 2013 Z. Raeesy S. Rueda J.K. Udupa J. Coleman Automatic segmentation of vocal tract MR images Proc. 10th International Symposium on Biomedical Imaging (ISBI) San Francisco, CA, USA 2013 1328 1331 10.1109/ISBI.2013.6556777 Saltzman and Munhall, 1989 E. Saltzman K. Munhall A dynamical approach to gestural paterning in speech production Ecol. Psychol. 1 1989 333 382 Seise et al., 2007 M. Seise S. McKenna I. Ricketts C. Wigderowitz Learning active shape models for bifurcating contours IEEE Trans. Med. Imaging 26 2007 666 677 10.1109/TMI.2007.895479 Seshadri and Savvides, 2012 K. Seshadri M. Savvides An analysis of the sensitivity of active shape models to initialization when applied to automatic facial landmarking IEEE Trans. Inf. Forensics Secur. 7 2012 1255 1269 10.1109/TIFS.2012.2195175 Shosted et al., 2012a R. Shosted C. Carignan P. Rong Managing the distinctiveness of phonemic nasal vowels: articulatory evidence from Hindi JASA 131 2012 455 465 Shosted et al., 2012 R. Shosted B.P. Sutton A. Benmamoun Using magnetic resonance to image the pharynx during Arabic speech: static and dynamic aspects Proc. Interspeech Portland, Oregon, USA 2012 2182 2185 Silva et al., 2012 S. Silva P. Martins C. Oliveira A. Silva A. Teixeira Segmentation and analysis of the oral and nasal cavities from MR time sequences Proc. ICIAR 2012, LNCS, vol. 7325 Aveiro, Portugal 2012 214 221 Silva et al., 2013 S. Silva A. Teixeira C. Oliveira Martins Segmentation and analysis of vocal tract from midsagittal real-time MRI Proc. ICIAR 2013, LNCS, vol. 7950, Póvoa de Varzim Portugal 2013 459 466 Silva et al., 2013 S. Silva A. Teixeira C. Oliveira Paula Towards a systematic and quantitative analysis of vocal tract data Proc. Interspeech Lyon, France 2013 1307 1311 Stone et al., 2001 M. Stone E.P. Davis A.S. Douglas M.N. Aiver R. Gullapalli W.S. Levine A.J. Lundberg Modeling tongue surface contours from cine-MRI images J. Speech Lang. Hear. Res. 44 2001 1026 1040 Sung and Kim, 2009 J. Sung D. Kim Adaptive active appearance model with incremental learning Pattern Recognit. Lett. 30 2009 359 367 10.1016/j.patrec.2008.11.006 Teixeira et al., 2005 A. Teixeira R. Martinez L. Silva L. Jesus J.C. Principe F. Vaz Simulation of human speech production applied to the study and synthesis of European Portuguese EURASIP J. Adv. Signal Process. 9 2005 1435 1448 Teixeira et al., 2012 A. Teixeira P. Martins C. Oliveira C. Ferreira A. Silva R. Shosted Real-time MRI for Portuguese: database, methods and applications Proc PROPOR 2012, LNCS, vol. 7243 Coimbra, Portugal 2012 306 317 Teixeira et al., 2012 A. Teixeira P. Martins C. Oliveira A. Silva Production and modeling of the European Portuguese palatal lateral H. Caseli A. Villavicencio A. Teixeira F. Perdigão Computational Processing of the Portuguese Language, vol. 7243 of Lecture Notes in Computer Science 2012 Springer Berlin Heidelberg 318 328 10.1007/978-3-642-28885-2_36 Tiede and Vatikiotis-Bateson, 2000 M.K. Tiede E. Vatikiotis-Bateson Contrasts in speech articulation observed in sitting and supine conditions Proc. 5th Seminar on Speech Production (ISSP) Chiemgau, Germany 2000 25 28 Udupa et al., 2006 J.K. Udupa V.R. LeBlanc Y. Zhuge C. Imielinska H. Schmidt L.M. Currie B.E. Hirsch J. Woodburn A framework for evaluating image segmentation algorithms Comput. Med. Imaging Gr. 30 2006 75 87 10.1016/j.compmedimag.2005.12.001 Van Ginneken et al., 2002 B. Van Ginneken A. Frangi J. Staal B. ter Haar Romeny M. Viergever Active shape model segmentation with optimal features IEEE Trans. Med. Imaging 21 2002 924 933 10.1109/TMI.2002.803121 Vanbelle and Albert, 2009 S. Vanbelle A. Albert Agreement between an isolated rater and a group of raters Stat. Neerl. 63 2009 82 100 10.1111/j.1467-9574.2008.00412.x Vasconcelos et al., 2011 M.J.M. Vasconcelos S.M.R. Ventura D.R.S. Freitas J.M.R. Tavares Towards the automatic study of the vocal tract from magnetic resonance images J. Voice 25 2011 732 742 10.1016/j.jvoice.2010.05.002 Williams, 1976 G. Williams Comparing the joint agreement of several raters with another rater Biometrics 32 1976 619 627 Wrench et al., 2011 A. Wrench J. Cleland J. Scobbie An ultrasound protocol to compare tongue contours: upright vs supine Proc. International Congress of Phonetic Sciences (ICPhS) Hong Kong, China 2011 2161 2164 Zhang et al., 2008 H. Zhang J.E. Fritts S.A. Goldman Image segmentation evaluation: a survey of unsupervised methods Comput. Vis. Image Underst. 110 2008 260 280 10.1016/j.cviu.2007.08.003 Zhang et al., 2012 S. Zhang Y. Zhan M. Dewan J. Huang D.N. Metaxas X.S. Zhou Towards robust and effective shape modeling: sparse shape composition Med. Image Anal. 16 2012 265 277 10.1016/j.media.2011.08.004 Zijdenbos et al., 1994 A. Zijdenbos B. Dawant R. Margolin A. Palmer Morphometric analysis of white matter lesions in MR images: method and validation IEEE Trans. Med. Imaging 13 1994 716 724 10.1109/42.363096 "
    },
    {
        "doc_title": "Computer-assisted myocardial perfusion assessment",
        "doc_scopus_id": "84912093309",
        "doc_doi": "10.1109/IV.2014.71",
        "doc_eid": "2-s2.0-84912093309",
        "doc_date": "2014-09-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Automatically match",
            "Computer assisted",
            "Computer-assisted analysis",
            "Interactive tool",
            "Left ventricles",
            "Myocardial perfusion"
        ],
        "doc_abstract": "© 2014 IEEE.Left ventricle functional analysis can be performed using CTA images. In recent years it has been proposed that CTA imaging might also be used for myocardial perfusion assessment, by visually looking for hypo attenuated regions that may correspond to hypo perfusion. Assigning a detected lesion to a specific myocardial segment requires significant effort while the clinician matches the Ahab standard myocardial segments to the anatomy of each patient. This article presents an interactive tool which, based on the existence of a previous left ventricle segmentation, automatically matches the lesions detected by the clinician to a myocardial segment and provides feedback over the image regarding the myocardial segments with detected lesions and their severity.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A framework for analysis of the upper airway from real-time MRI sequences",
        "doc_scopus_id": "84894565763",
        "doc_doi": "10.1117/12.2042081",
        "doc_eid": "2-s2.0-84894565763",
        "doc_date": "2014-03-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "In recent years, real-time Magnetic Resonance Imaging (RT-MRI) has been used to acquire vocal tract data to support articulatory studies. The large amount of images resulting from these acquisitions needs to be processed and the resulting data analysed to extract articulatory features. This analysis is often performed by linguists and phoneticists and requires not only tools providing a high level exploration of the data, to gather insight over the different aspects of speech, but also a set of features to compare different vocal tract configurations in static and dynamic scenarios. In order to make the data available in a faster and systematic fashion, without the continuous direct involvement of image processing specialists, a framework is being developed to bridge the gap between the more technical aspects of raw data and the higher level analysis required by speech researchers. In its current state it already includes segmentation of the vocal tract, allows users to explore the different aspects of the acquired data using coordinated views, and provides support for vocal tract configuration comparison. Beyond the traditional method of visual comparison of vocal tract profiles, a quantitative method is proposed, considering relevant anatomical features, supported by an abstract representation of the data both for static and dynamic analysis. © 2014 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Assessing the applicability of surface EMG to tongue gesture detection",
        "doc_scopus_id": "84921403499",
        "doc_doi": "10.1007/978-3-319-13623-3_20",
        "doc_eid": "2-s2.0-84921403499",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "False positive rates",
            "Gesture detections",
            "Sensor positioning",
            "Silent speech interfaces",
            "Surface electromyography",
            "Synchronous acquisition",
            "Tongue gestures",
            "Ultrasound imaging"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.The most promising approaches for surface Electromyography (EMG) based speech interfaces commonly focus on the tongue muscles. Despite the interesting results in small vocabularies tasks, it is yet unclear which articulation gestures these sensors are actually detecting. To address these complex aspects, in this study we propose a novel method, based on synchronous acquisition of surface EMG and Ultrasound Imaging (US) of the tongue, to assess the applicability of EMG to tongue gesture detection. In this context, the US image sequences allow us to gather data concerning tongue movement over time, providing the grounds for the EMG analysis. Using this multimodal setup, we have recorded a corpus that covers several tongue transitions (e.g. back to front) in different contexts. Considering the annotated tongue movement data, the results from the EMG analysis show that tongue transitions can be detected using the EMG sensors, with some variability in terms of sensor positioning, across speakers, and the possibility of high false-positive rates.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "RT-MRI based dynamic analysis of vocal tract configurations: Preliminary work regarding intra- and inter-sound variability",
        "doc_scopus_id": "84921385715",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84921385715",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Articulatory data",
            "Different class",
            "Dynamic aspects",
            "Large amounts",
            "Nasal vowels",
            "Repeated utterance",
            "Speech production"
        ],
        "doc_abstract": "© 2014 10th International Seminar on Speech Production, ISSP 2014. All rights reserved.The study of the dynamic aspects of speech production is relevant for different classes of sounds. To improve knowledge regarding this subject it is important to consider data from various repeated utterances, from different speakers, leading to the challenging scenario of processing and analysing a large amount of data in a common framework. We propose a method for dynamic analysis of articulatory data jointly exploring multiple utterances of each sound using a quantitative analysis framework and covering multiple articulators. Instead of analysing each utterance and then inferring notable features, all utterances for a particular sound are considered simultaneously, possibly from more than one speaker, aiming towards a characterization of average dynamic aspects and their corresponding variation. Application examples are provided involving the analysis of vowel dynamics and inter-vowel comparison of European Portuguese nasal vowels data gathered using real-time Magnetic Resonance (RT-MRI).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantitative analysis of /l/ production from RT-MRI: First results",
        "doc_scopus_id": "84921383919",
        "doc_doi": "10.1007/978-3-319-13623-3_4",
        "doc_eid": "2-s2.0-84921383919",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Co-articulation",
            "Dynamic property",
            "Large amounts of data",
            "Laterals",
            "Position effect",
            "Quantitative frameworks",
            "Systematic analysis",
            "Temporal aspects"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.Lateral consonants are complex and variable sounds. Static MRI provides relevant information regarding /l/ geometry, but does not address dynamic properties. Real-time MRI is a well suited technique for dealing with temporal aspects. However, large amounts of data have to be processed to harness its full potential. The main goal of this paper is to extend a recently proposed quantitative framework to the analysis of real-time MRI data for European Portuguese /l/. Several vocal tract configurations of the alveolar consonant, acquired in different syllable positions and vocalic contexts, were compared. The quantitative framework revealed itself capable of dealing with the data for the /l/, allowing a systematic analysis of the multiple realisations. The results regarding syllable position effects and coarticulation of /l/ with adjacent vowels are in line with previous findings.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Systematic and quantitative analysis of vocal tract data: Intra- and inter-speaker analysis",
        "doc_scopus_id": "84921374092",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84921374092",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Articulatory data",
            "Articulatory differences",
            "Current technology",
            "Quantitative assessments",
            "Quantitative frameworks",
            "Vocal-tract data",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2014 10th International Seminar on Speech Production, ISSP 2014. All rights reserved.Articulatory analysis has traditionally been performed visually. Besides the inherent subjectivity, such approach is not feasible when dealing with the large amount of articulatory data made available by current technologies, such as real-time Magnetic Resonance Imaging. Therefore, analysis of the available data should move towards systematic quantitative assessment. As a first approach to this issue, we present the main aspects of a quantitative framework for vocal tract profile comparison which provides normalized comparison data concerning different articulators. Application examples are shown illustrating how it can be used to perform analysis of the articulatory differences between sounds, comparison among speakers and overall articulatory characterization obtained from multiple speakers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic annotation of an ultrasound corpus for studying tongue movement",
        "doc_scopus_id": "84908661679",
        "doc_doi": "10.1007/978-3-319-11758-4_51",
        "doc_eid": "2-s2.0-84908661679",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic annotation",
            "Environmental noise",
            "Manual annotation",
            "Movement detection",
            "Silent speech interfaces",
            "Surface electromyography",
            "Systematic analysis",
            "Ultrasound imaging"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.Silent speech interfaces can work as an alternative way of interaction in situations where the acoustic speech signal is absent (e.g., speech impairments) or is not suited for the current context (e.g., environmental noise). The goal is to use external data to infer/improve speech recognition. Surface electromyography (sEMG) is one of the modalities used to gather such data, but its applicability still needs to be further explored involving methods to provide reference data about the phenomena under study. A notable example concerns exploring sEMG to detect tongue movements. To that purpose, along with the acquisition of the sEMG, a modality that allows observing the tongue, such as ultrasound imaging, must also be synchronously acquired. In these experiments, manual annotation of the tongue movement in the ultrasound sequences, to allow the systematic analysis of the sEMG signals, is mostly infeasible. This is mainly due to the size of the data involved and the need to maintain uniform annotation criteria. Therefore, to address this task, we present an automatic method for tongue movement detection and annotation in ultrasound sequences. Preliminary evaluation comparing the obtained results with 72 manual annotations shows good agreement.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "AgeCI: HCI and age diversity",
        "doc_scopus_id": "84903463873",
        "doc_doi": "10.1007/978-3-319-07446-7_18",
        "doc_eid": "2-s2.0-84903463873",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "age diversity",
            "Current practices",
            "Driving factors",
            "Interaction design",
            "overview"
        ],
        "doc_abstract": "We present an overview of recent works in which age is an important driving factor for Human-Computer Interaction design and development. These serve as starting grounds to discuss current practices and highlight challenges that might serve as beacons for future research in the field. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and development of speech interaction: A methodology",
        "doc_scopus_id": "84903128327",
        "doc_doi": "10.1007/978-3-319-07230-2_36",
        "doc_eid": "2-s2.0-84903128327",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Computer interaction",
            "decoupled modalities",
            "Design and Development",
            "Multimodal architectures",
            "Multimodal frameworks",
            "Speech interaction",
            "Speech modality",
            "Speech recognizer"
        ],
        "doc_abstract": "Using speech in computer interaction is advantageous in many situation and more natural for the user. However, development of speech enabled applications presents, in general, a big challenge when designing the application, regarding the implementation of speech modalities and what the speech recognizer will understand. In this paper we present the context of our work, describe the major challenges involved in using speech modalities, summarize our approach to speech interaction design and share experiences regarding our applications, their architecture and gathered insights. In our approach we use a multimodal framework, responsible for the communication between modalities, and a generic speech modality allowing developers to quickly implement new speech enabled applications. As part of our methodology, in order to inform development, we consider two different applications, one targeting smartphones and the other tablets or home computers. These adopt a multimodal architecture and provide different scenarios for testing the proposed speech modality. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Velum movement detection based on surface electromyography for speech interface",
        "doc_scopus_id": "84902334838",
        "doc_doi": "10.5220/0004741100130020",
        "doc_eid": "2-s2.0-84902334838",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Acoustic signals",
            "Movement detection",
            "Nasal vowels",
            "Noisy environment",
            "Noninvasive methods",
            "Silent speech interfaces",
            "Speech interface",
            "Surface electromyography"
        ],
        "doc_abstract": "Conventional speech communication systems do not perform well in the absence of an intelligible acoustic signal. Silent Speech Interfaces enable speech communication to take place with speech-handicapped users and in noisy environments. However, since no acoustic signal is available, information on nasality may be absent, which is an important and relevant characteristic of several languages, particularly European Portuguese. In this paper we propose a non-invasive method - surface Electromyography (EMG) electrodes - positioned in the face and neck regions to explore the existence of useful information about the velum movement. The applied procedure takes advantage of Real-Time Magnetic Resonance Imaging (RT-MRI) data, collected from the same speakers, to interpret and validate EMG data. By ensuring compatible scenario conditions and proper alignment between the EMG and RT-MRI data, we are able to estimate when the velum moves and the probable type of movement under a nasality occurrence. Overall results of this experiment revealed interesting and distinct characteristics in the EMG signal when a nasal vowel is uttered and that it is possible to detect velum movement, particularly by sensors positioned below the ear between the mastoid process and the mandible in the upper neck region. Copyright © 2014 SCITEPRESS - Science and Technology Publications. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Elderly centered design for interaction - The case of the S4S Medication Assistant",
        "doc_scopus_id": "84897816217",
        "doc_doi": "10.1016/j.procs.2014.02.044",
        "doc_eid": "2-s2.0-84897816217",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Development method",
            "Elderly evaluation",
            "Interaction",
            "Medication",
            "Mobile applications",
            "User-centred"
        ],
        "doc_abstract": "Several aspects of older adults' life can benefit from recent technological developments, but success in harnessing this potential depends on careful design and accessible, easy to use products. Design and development must be centered on the elderly and adequately consider interaction. In this paper we follow this design approach and put it to the test in developing a concrete application, aimed to contribute to lower the high levels of non-adherence to medication in the elderly population. The \"Medication Assistant\" application was developed following an iterative method centered, from the start, on the elderly and interaction design. The method repeats short-time development cycles integrating definition of scenarios and goals, requirements engineering, design, prototyping and evaluation. Evaluation, by end-users, of the increasingly refined prototypes, is a key characteristic of the method. The evaluation results provide information related to strengths and weaknesses of the application and yield suggestions regarding changes and improvements, valuable support further development. Results regarding evaluation of the second prototype of \"Medication Assistant\" are presented. © 2013 The Authors. Published by Elsevier B.V.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2014-02-25 2014-02-25 2014-10-23T08:54:46 S1877-0509(14)00046-5 S1877050914000465 10.1016/j.procs.2014.02.044 S300 S300.2 HEAD-AND-TAIL 2021-10-14T13:20:00.670388Z 0 0 20140101 20141231 2014 2014-02-25T00:00:00Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 false 27 27 C Volume 27 45 398 408 398 408 2014 2014 2014-01-01 2014-12-31 2014 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, DSAI 2013 Dr. Manuel Pérez Cota Dr. João Barroso Dr. Simone Bacellar Leal Ferreira Dr. Benjamim Fonseca Dr. Tassos Mikropoulos Dr. Hugo Paredes article fla Copyright © 2013 The Authors. Published by Elsevier B.V. ELDERLYCENTEREDDESIGNFORINTERACTIONCASES4SMEDICATIONASSISTANT FERREIRA F FERREIRAX2014X398 FERREIRAX2014X398X408 FERREIRAX2014X398XF FERREIRAX2014X398X408XF Full 2014-02-25T10:44:25Z OA-Window ElsevierWaived 0 item S1877-0509(14)00046-5 S1877050914000465 10.1016/j.procs.2014.02.044 280203 2014-10-23T04:53:38.277668-04:00 2014-01-01 2014-12-31 true 770861 MAIN 11 52416 849 656 IMAGE-WEB-PDF 1 P r o c e d i a C o m p u t e r S c i e n c e 2 7 ( 2 0 1 4 ) 3 9 8 4 0 8 1877-0509 ' 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). doi: 10.1016/j.procs.2014.02.044 ScienceDirect Available online at www.sciencedirect.com 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, DSAI 2013 Elderly centered design for Interaction â€“ the case of the S4S Medication Assistant FlÃ¡vio Ferreira a , Nuno Almeida a,b , Ana Filipa Rosa a , AndrÃ© Oliveira a , JosÃ© Casimiro c , Samuel Silva a,b , AntÃ³nio Teixeira a,b, * a Institute of Electronics and Telematics Engineering of Aveiro (IEETA), 3810-193 Aveiro, Portugal b Dep. of Electronics Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal c Polytecnic Institute of Tomar, 2300-531 Tomar, Portugal Abstract Several aspects of older adultsâ€™ life can benefit from recent technological developments, but success in harnessing this potential depends on careful design and accessible, easy to use products. Design and development must be centered on the elderly and adequately consider interaction. In this paper we follow this design approach and put it to the test in developing a concrete application, aimed to contribute to lower the high levels of non-adherence to medication in the elderly population. The â€œMedication Assistantâ€� application was developed following an iterative method centered, from the start, on the elderly and interaction design. The method repeats short-time development cycles integrating definition of scenarios and goals, requirements engineering, design, prototyping and evaluation. Evaluation, by end-users, of the increasingly refined prototypes, is a key characteristic of the method. The evaluation results provide information related to strengths and weaknesses of the application and yield suggestions regarding changes and improvements, valuable support further development. Results regarding evaluation of the second prototype of â€œMedication Assistantâ€� are presented. Â© 2013 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013) * Corresponding author. Tel.: +351 234370500; fax: +351 234370545. E-mail address: ajst@ua.pt. ' 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and peer-review under responsibility of the Scientific Programme Com ittee of the 5th International Conference on Soft are Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). 399 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 \"Keywords: development method; interaction; user centred; elderly evaluation; mobile applications; medication\" 1. Introduction The impact of using advanced technologies has been very positive for the general population. Due to the continuous increase in the elderly population worldwide [1], development starts to contemplate this group, taking into account the significant contribution that new technologies may provide to improve the quality of life of these population. This meets the guidelines of the EU countries for the elderly population, which highlight active aging and independent living, by the development of services based on the specific needs of the population, helping them to live in community [2]. Developers know the importance of involving the target users in the design process [3] and this is even more important when the target users are elderly [4]. Throughout the design process of products suitable for and usable by elderly it must be taken into account their unique needs, limitations and capabilities [5]. To accomplish this, it is essential to know and use effective approaches for interaction and gather data on how they are used. This data will give information about the features and attributes that elderly prefer, and consequently allow an understanding of which factors can improve the usability of the product [6]. The design process in this perspective arises in the literature as User Centered Design (UCD), but the traditional user centered design provides little guidance on how to involve the elderly [3]. UCD is a collection of methods that aim to involve the users in an appropriate way during development. The basic principle of this approach is placing users in the focus of the design process through the use of various techniques, first to collect information from them and then to initiate the design of prototypes which they will be asked to test [7]. â€œNeeds assessment and requirements analysis are the most important activities for initiating system improvement because, done well, they are the foundation upon which all other activities build\" [5]. With the continuous progress and sophistication of the prototypes, the user should be asked to perform tasks with minimum guidance by the testers. The results of this tests, according to the iterative process inherited from UCD, are analyzed and will guide the design of subsequent prototypes [7]. Product development based in UCD should adhere to the following principles [8, 9]: knowledge gathering concerning usersâ€™ needs, capabilities, attitudes and characteristics; active involvement of users; prototypes redesign, as often as necessary; iterations of design solutions (repetition of a cyclic process of design, evaluation and redesign as often as necessary); multidisciplinary design teams. According to the international standard ISO 13407 this process should be developed through four stages in the following order: specify the context of use, define the requirements, design and evaluation [8]. One of the important contributions to guide the process of product development according to the principles of UCD, particularly at an early stage of requirement definition, is the creation of Personas [10]. Personas are fictional persons, with name, occupations, age, gender, socioeconomic status, hobbies, stories and goals and are used to personify the principal characteristics and functions for the product design [11, 12]. There are various data-driven Personas for elderly based on European statistics, which should be followed by teams that intend to develop products to the elderly [13]. Personas are an important and valuable tool, mainly if the objective of the team is to test and evaluate the usability and effectiveness of a product. The Personas â€œallow us to see the scope and nature of the design problem. They make it clear exactly what the user's goals are, so we can see what the product must do\" [13]. Despite the general applicability of the UCD development method, it must be tuned and adapted when developing applications for older adults. With the increased use of new interaction modalities (e.g. touch), multimodal interaction requirements, design, development and evaluation must also be part of the development process. 400 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 Considering the elderly population, one important issue affecting their daily life and health, concerns medication non-adherence [14]. This is often due to the increasing number of medicines to take, to age related changes (such as memory loss, or different levels of physical and cognitive impairment), to lack of information on its usefulness and transient side-effects and to demotivation. The impact of medication non-adherence on the quality of life and associated costs dealing, for example, with reduced or unwanted therapeutic effects (e.g. antibiotic resistance and longer treatment timespans) [15], configure this as a very important problem to tackle in which mobile devices and adequate interaction design can play an important role. This, we argue, is a context were the UCD development method can prove its mettle. In this paper, we present an UCD development method centered on the elderly and interaction, aimed at increased usability and accessibility [9], and its application to the development of a concrete system, the â€œMedication Assistantâ€�, that allows voice and touch interaction to facilitate the access by elderly. The main features of this system are the alerts of medication to intake, the visualization of information of each medication and the advice service that allows users to obtain information about what to do in case of forgetting to take a medication. The prototype of this application has been already evaluated in two tests, with two different groups of users, in order to detect problems and collect opinions and suggestions to further meet the needs and specificities of the users. This paper starts by providing an overview of the proposed development method. After, a description of the primary Persona, context scenarios and application requirements for the â€œMedication Assistantâ€� are presented. It is also discussed how the application meets the requirements, followed by the evaluation method, main results and the conclusions. 2. Elderly Centered Development Method The proposed development method is aligned with the methodology described in [16]. After obtaining the requirements (phase 1), a prototype is proposed (phase 2) and evaluated (phase 3), in order to refine the requirements. This iterative methodology continues with additional prototypes and evaluations towards an increasingly refined application. In order to get system Personas, the context scenarios and the system requirements for phase 1 we adopted a five stages method aligned with Cooper and collaborators [17]. The first stage aims at identifying the behavioral variables, such as age and demographic localization, activities, attitudes, aptitudes, motivations and skills and significant behavior patterns, by analyzing the interview results. Following these guides, it is possible to synthetize the characteristics and relevant goals for the Persona. After, the description of the Persona is expanded in order to have a small story about the Persona and its daily life. Finally, Persona types are designated. At the beginning only the primary Persona should be created. On the second stage, to get the requirements and context scenarios a problem and vision statement must me produced. The third stage consists in brainstorming with people from different domains and end-users. The brainstorming should last a couple of hours and comprise a few questions in order to stimulate the ideas flow. After the brainstorming the main ideas should be filtered. On the fourth stage Persona expectations are set by the same people who brainstormed earlier. This is very important in order to understand the main end-user and its expectations. At the end of this stage, context scenarios are defined, typically as a group of short stories. They should be simple and represent use of the application by end-users. Finally, the requirements analysis should be made. To create the requirements it is important to be aware of the information gathered in the previous stages. It is important to note that the Persona and the scenarios should always be present during the development process (phase 2) and not only in requirements elicitation. 401 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 Towards the development of an application designed taking into account the accessibility and usability features of end users, the application is evaluated along all the development process (several iterations of phase 3). The evaluation method adopted is based on a methodology recently proposed [16]. This methodology is adapted to the characteristics of the application and its end users. The evaluation consists of three phases: conceptual validation, prototype test and pilot test. These three phases are connected, in a cyclic process, as the results of the several evaluations will influence the development process of the application. The first phase of evaluation aims at collecting information to verify the viability of the application interface and functions. The second phase comprises a test performed by the users through interaction with the application prototype, with the presence of an evaluator to register opinions and suggestions of the user. The pilot test is similar to the prototype test, but it adds the assessment of the impact the application has on the lives of its end users. 3. Application to a Medication Assistant for the Elderly In this section, after the generic presentation of the method in the previous section, we show how it can be applied in a real scenario using, as an example, the development of a new multimodal application for Smartphones, named â€œMedication Assistantâ€�. The main purpose of this application is to make elders become healthier, addressing the problem of medication non-adherence due to age related factors such as the inherent medication increase, cognitive losses and demotivation. It helps them with medication management, providing multimodality and context awareness, fully advising and supporting with the medication. 3.1. Personas, Context Scenarios and Requirements As already explained, Personas and Scenarios have a key role in supporting requirement analysis. Information on the Persona and scenarios developed for â€œMedication Assistantâ€� are presented below. 3.1.1. Persona Our Persona should live in Portugal, does not need to have the aptitude to use electronic devices, must be an elder person with health problems (although it is not necessary to have regular doctor appointments), and should have the desire to control its own medication. Furthermore, the Persona does not need to have a family, but commonly speaks with friends and family through a computer or mobile device. In the first phase we are developing our application in order to respond to the requirements of the Primary Persona. The Primary Persona is the Persona at which the application should address all the requirements. We identified the following expectations: the Persona wants to be able to use the same application with arthritis; the application should help prevent gaps in medication, report medication, report side effects, and provide alerts. Furthermore, the Persona wants to use the application even with little aptitude for working with electronic devices. Thereafter, we created the context scenarios, after which we identified the requirements. Primary Persona, Mrs. EmÃ­lia, lives in Coimbra with her husband Filipe Rodrigues. Sheâ€™s a housewife and does not have experience with electronic devices. She is right handed. She is diabetic and has arthritis in her right superior member. Her health condition requires a regular and daily medication. Mrs. EmÃ­lia has the habit to call her daughter during dinner preparation. However, she has some difficulty in doing the two activities simultaneously due to her limitation in the right superior member. She has weekly appointments in her local health center for surveillance. Mrs. EmÃ­lia would like to buy equipment that facilitates the contact with her daughter and allows her to control her medication, which she often forgets to take. 402 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 3.1.2. An example of a context scenario Our context scenarios intend to describe several scenarios of application usage. These scenarios outline different ways to interact with the application, as well as its features. Each scenario is represented by a short story. Two scenario examples are presented below. Mrs. EmÃ­lia waked up to prepare breakfast. The application showed an alert (with the medication names and dosage) warning that Mrs. EmÃ­lia needed to take her medication in the fasted state. Mrs. EmÃ­lia, warned that she had to take the medication, took it immediately and then prepared breakfast. After a while, the application asked Mrs. EmÃ­lia if she had already taken the medication. The application interacted through speech, since Mrs. EmÃ­lia hands were busy. She answered â€œYes, I took my medicationâ€�. A few hours later Mrs. EmÃ­lia starts to feel unwell. She felt concern and, through speech, she asked the application if that condition was normal. In order to answer that, the application explained that she took a medication that could induce a feeling of being unwell, also giving information about what is the reason to take the medication and what are the side effects of it. Thereafter, Mrs. EmÃ­lia felt relieved and returned to her tasks. Table 1 - Context Scenario Mrs. EmÃ­lia Action Action on smartphone application Output from smartphone application to user Wakes up and prepares breakfast. Prepares an alert to show The application triggers an alert: â€œYou need to take medication in the fasted stateâ€� Takes the smartphone and read the alert. The user opens the alert [IM=Touch]. Opens the alert. Shows the list of medications to take [IM=Text and Images] Takes the medication and lock de mobile. Closes application and lock. Locks Screen Eats the breakfast. Needs to know if the medication was taken. Prepares a speech message. Shows a message: â€œDid you take the medication in fasted state?â€� [IM=Text and Speech] Answers â€œYes, I took my medicationâ€� [IM=Speech] Recognizes the answer. Notes the take of the medication. Locks. Locks Screen. A few hours later, starts to feel unwell. The user unlocks the mobile and asks â€œIâ€™m felling unwell, is it normal?â€� [IM=Speech] Recognizes the sentence. Find if any medication taken has side effects and prepares the response. Shows a message: â€œYes, the MEDX could induce a feeling of being unwell. It can induce nauseas too. But you should take it for arthritis.â€� [IM=Text and Speech] Feels relieved. 3.2. Main requirements The requirements were divided in two main groups: the functional requirements and the user requirements. The main functional requirements are: (1) the application should provide medication insertion and management by third parties, so that seniors do not need to perform this task since it can be complicated; (2) the application should provide medication alerts to remember users about medication schedules; (3) the application should provide medication advice to help elders in daily medication questions. It must be able to respond to commands given in Portuguese expressing questions such as â€œWhat should I do if I forget to take 403 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 my medication?â€�, â€œPlan the day because Iâ€™m leavingâ€� or â€œIâ€™m with headaches. Is it a side effect?â€�; (4) the application should provide configurable interaction; (5) the application should provide multimodal interaction; (6) the application should allow registry of medication taken. The main user requirements are: (1) the application should inform users in an everyday language since technical language could be misunderstood; (2) the application should provide touch and speech interaction for everything in order to facilitate user interaction since some users may have physical or cognitive limitations; (3) the application should be reliable and credible because the user must trust its advices and alerts; (4) as elders usually have low proficiency with technology and vision problems it is crucial that the application avoids overloading content and small icons; (5) the application should have extra care with language and dimension; (6) the application should adapt to the user and the context; (7) the application should provide personalization; (8) the application should avoid information overload; (9) the application should be able to provide more information when the user wants it; (10) the application should provide help to the elder when they present difficulties interacting with the application; (11) the application should provide a group of â€œhow to use guidesâ€�. 3.3. Prototype 2 â€“ Meeting the requirements After completing one development cycle, which resulted in a first prototype, the data gathered during its evaluation was used to feed the following development cycle. Furthermore, beyond the consideration of user feedback, existing features have been expanded and new features added, resulting in a second application prototype. In order to enable the insertion of medication and its management by third parties, we created an external service. The smartphone uses the service to get all the information related to the medication. When the application is open for the first time it will ask for a login that will be used to get the elderly medication plan. In the second prototype the insertion was made by a formal or informal caregiver, i.e., the elderly didnâ€™t need to perform this task. This has the advantage of preventing the elderly from getting bored and demotivated to use the application, since this task can be tiresome and time consuming. However, this feature needs to be improved in order to simplify the insertion process. The application provides medication alerts using both Windows Phone push notification and local notification. If the application is connected to the Internet, the user can receive push notifications. However, when the application opens, it creates local notifications for the next four alerts and informs the push notification service that it only needs to work for the fifth alert. This process is executed whenever the application opens. Therefore, the push notification service is only required when the elderly do not open the application for the next four straight alerts. This way it is very likely that the application will inform the user in the need of taking the medication in a timely fashion, even without constant access to the Internet, creating a high level of credibility and reliability, very important for older adults. Furthermore, the application allows the elderly to see the next alerts list. 404 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 When the elderly forget to take medication, or want to know what its side effects are, they can refer to the application. All the medication should have information about what to do if the user forgets to take it, what are the side effects, and why the elderly needs to take it. In this way, the application can give that information to the elderly when needed. In the prototype, it is possible to know the reason of taking that medication, what are the side effects, the expiration date of the medication, and what to do if a medication intake was forgotten. We will add additional information to the medication in order to allow the elderly to get more advice about it. This information is based on the medicines leaflets, increasing the credibility and reliability of the information given. The application provides a set of options so the elderly can change the application settings. The user can allow noise control features, change the font size, change between dark and light mode and activate the help mode. Regarding the noise control feature, for example, if it is activated the application will automatically detect the user context noise and adapt its volume. The user can also change interaction features such as how speech commands are input (touch-to-speak or use of speech by voice activation) and activate the auto-zoom. The touch-to-speak feature will allow elderly to touch a button and speak to the application. The application can also be configured to automatically detect speech inputs without any type of touch interaction. However, this alternative is not so precise and has not yet been carefully evaluated. The auto-zoom feature adapts the size of text and images to the user distance to the smartphone. If this feature is activated, the size of the items on the screen will automatically increase when the user moves away or closer to the smartphone depending on the user context: if the user has nearsightedness, when approaching the screen the size of the images/text will increase; if the elderly has astigmatism, when moving farther from the screen the size of the images/text will increase too. Furthermore, when the elderly needs to know more information about a specific medication, the application will adapt to the needs. Thereby, the application provides personalization, configurable interaction, extra information and user/context adaptation. The elderly can use touch or speech as input in order to interact with the application. The speech can be used to get advice (e.g. â€œI forgot to take the lunch medicationâ€�) or to get extra information (e.g. â€œWhat are the side effects of this medication?â€�). The advantage of speech is that it normally provides a faster and more intuitive way to get response from the system than touch. As output we use text, speech and images. Thus, in the second prototype we already used multiple modalities both for input and output, making it really multimodal [18]. As the elders usually have low proficiency with technology and vision problems, we created a User Interface based in big text and big items/images. The UI follows the Metro Style guidelines, avoiding extra bars, icons, buttons, etc. [19]. The application has a simple and clear layout that gives the user the opportunity to get more information when required (through speech) to avoid information overload in the views. Furthermore, the auto- zoom feature will be aware of the user difficulties and adapts the UI to it. On the other hand, the application tries to give information in a common and informal language avoiding the technical one. However, this is a requirement that needs more improvement since it is hard to replace the medication names and side effects by informal and common names. Lastly, the application provides auto adaptable help. When the elderly are inexperienced, the application offers many suggestions, but when they learn to interact with the application it will stop providing them. However, if an expert user starts to show some difficulty (e.g. increasing time to perform tasks) using one of the features, the application adapts and starts offering suggestions again. In addition, the application provides a Fig. 1 - Medication Assistant example views: (a) application starting; (b) main menu; (c) advice menu; (d) next alerts; (e) about the medication. 405 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 group of â€œhow to use guidesâ€� composed by some example phrases about how to interact with it through speech. In the future, we will add video guides in order to help inexperienced users. 3.4. Evaluation 3.4.1. Methodology As mentioned, the evaluation method adopted for â€œMedication Assistantâ€� is based on a methodology proposed recently in [16]. A first evaluation of the prototype was conducted with a heterogeneous group of engineers and health professionals, with the intent of gathering an extended number of opinions and suggestions to validate the general design options and guide the following development. In the first prototype test the user was accompanied by an evaluator, member of the project, to explain the test and respond to user doubts. During this test the think aloud method was used and the audio was recorded. The group was composed by three women and one man with age ranges between 25 and 60 years. The results of this first evaluation of prototype was presented in [20] and used to support the development of a second prototype, for which evaluation results are presented in this paper. The second evaluation was made with a group of end users and followed a previously defined structured plan of evaluation with a set of tasks. The evaluation consisted of two phases: the interaction assessment phase and the usability evaluation phase. In both phases the user is accompanied by an evaluator to explain the evaluation method. The interaction phase is composed by a set of tasks requiring the user to interact with the application: for instance â€œSee the list of medications to be taken at breakfastâ€� and â€œInform the application that you missed the last medicationâ€�. In this phase the think aloud method is used and the evaluator takes notes regarding users behavior and their main difficulties, comments, doubts, suggestions and problems during the completion of the tasks. In the second phase, pertaining usability, the evaluator applies a questionnaire about the user interaction experience with the application. In this stage the user answers some questions about the application, such as â€œIn your opinion what are the strengths and weaknesses of the application?â€� and â€œWhat would you change in the application?â€�. The user should give an opinion about some aspects of the application such as the layout, font size and color, the features and the interaction. The second evaluation of the prototype was with a group of three women and one man, with ages between 57 and 76 years. Accordingly to [21], the sample size is appropriate to a qualitative evaluation, considered adequate for a second prototype evaluation. 3.4.2. Results In this section we present the results of the evaluation. Regarding usability evaluation, through the analysis of the opinion questionnaires, the results that stand out are the strengths and weaknesses of the application and the suggestions of changes to be made. Fig. 2 shows the strengths and weaknesses of the â€œMedication Assistantâ€�. In the tag clouds the words with larger font size are the most referred by the users and the smaller ones are the least identified by the sample. Fig. 2 â€“ (left box) strengths of the application; (right box) weaknesses of the application 406 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 The changes suggested by the users are presented in the following table (In each type of alteration are presented the object and the action to be taken, accordingly to the user suggestions. The last column presents the priority of the alteration, according to the development team opinion. Regarding interaction evaluation, based on the data collected from observation and notes of the evaluators several features were identified that users considered more difficult or easier to use. The overall results presented in the two graphics depicted in Fig. 3, showing the number of users that considered each feature difficult or easy to use. ) and organized by types. In each type of alteration are presented the object and the action to be taken, accordingly to the user suggestions. The last column presents the priority of the alteration, according to the development team opinion. Regarding interaction evaluation, based on the data collected from observation and notes of the evaluators several features were identified that users considered more difficult or easier to use. The overall results presented in the two graphics depicted in Fig. 3, showing the number of users that considered each feature difficult or easy to use. Table 2 - Application improvements suggested by users on the evaluation questionnaire Type Object Action Comment New features Tutorial Add Priority Register Add Priority Interaction Speech input Improve Priority Design/Layout Panoramic view Improve Nonpriority Real menu Improve Nonpriority 4 3 4 2 1 Difficulties 3 4 2 4 Easy to use 407 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 4. Conclusions Since the beginning of the â€œMedication Assistantâ€� application development, one of the main focuses was the inclusion of end-users. User intervention is crucial since the initial phase, mostly with their participation in brainstorming sessions to generate ideas and support the definition of Persona and scenarios. During the continuous development process of the application users have an active role, by participating in the process of evaluation, thus enabling the application to be shaped accordingly to the usersâ€™ needs and capabilities, suffering a continuous process of redesign. In this paper all this process is explained and the results of the second prototype are presented. The data analysis was qualitative, taking into account the size of the evaluation sample. In the data analysis priority was given to the questionnaires, which provide information related to strengths and weaknesses of the application and give suggestions of changes proposed by users. By analyzing the strengths of the application we can say that the application is already useful, even being on a prototype development phase. Features like forgetfulness support, medication images and the expiration date were proposed in the brainstorming, and as we can see they seem to be of interest for the end-users. The Help menu feature is a result of the first prototype evaluation. It shows that our method can lead to the development of useful applications for end-users. The weaknesses can provide important information too. First, they provide basis for the next iteration and prototype. Second, they show that interaction is really important as we defend in our proposed method. Since we are in the second prototype, the interaction features are only in the initial phase of development, unlike other features of the application, being the reported weaknesses expected. With the intent tackle the needs identified in the evaluation of the second prototype, the future work should focus, mainly, in the development of a new prototype, considering particularly touch and speech interaction. Acknowledgements This work is part of the Smart Phones for Seniors (S4S) project, a QREN project (QREN 21541), co-funded by COMPETE and FEDER. References [1] DESA. (2013). Available: [2] H. Matlabi, S. Parker, and K. McKee, \"The contribution of home-based technology to older people's quality of life in extra care housing,\" BMC Geriatrics, vol. 11, p. 68, 2011. [3] A. Newell, J. Arnott, A. Carmichael, and M. Morgan, \"Methodologies for Involving Older Adults in the Design Process,\" in Universal Acess in Human Computer Interaction. Coping with Diversity. vol. 4554, C. Stephanidis, Ed., ed: Springer Berlin Heidelberg, 2007, pp. 982-989. [4] R. Eisma, A. Dickinson, J. Goodman, O. Mival, A. Syme, and L. Tiwari, \"Mutual inspiration in the development of new technology for older people,\" in In Proceedings of Include 2003, 2003, pp. 7--252. [5] E. Mynatt and W. Rogers, \"Developing technology to support the functional independence of older adults,\" Ageing International, vol. 27, pp. 24-41, 2001/12/01 2001. [6] R. Eisma, A. Dickinson, J. Goodman, A. Syme, L. Tiwari, and F. Newell, \"Early user involvement in the development of information technology-related products for older people,\" Univers. Access Inf. Soc., vol. 3, pp. 131-140, 2004. [7] S. Chamberlain, H. Sharp, and N. Maiden, \"Towards a framework for integrating agile development and user-centred design,\" presented at the Proceedings of the 7th international conference on Extreme Programming and Agile Processes in Software Engineering, Oulu, Finland, 2006. [8] J. Gulliksen, A. Lantz, and I. Boivie, User Centered Design in Practice - Problems and Possibilities, 1999. [9] R. D. Buurman, \"User-centred design of smart products,\" Ergonomics, vol. 40, pp. 1159-1169, 1997/10/01 1997. [10] J. Pruitt and J. Grudin, \"Personas: practice and theory,\" presented at the Proceedings of the 2003 conference on Designing for user experiences, San Francisco, California, 2003. [11] J. Grudin and J. Pruitt. (2002, Personas, Participatory Design and Product Development: An Infrastructure for Engagement. Fig. 3 â€“ Application features and corresponding number of users who considered them difficult (left) and easy (right). 408 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 [12] R. Sinha, \"Persona development for information-rich domains,\" presented at the CHI '03 Extended Abstracts on Human Factors in Computing Systems, Ft. Lauderdale, Florida, USA, 2003. [13] R. Casas, R. B. Maro, A. Robinet, A. R. Delgado, A. R. Yarza, J. Mcginn, R. Picking, and V. Grout, \"User Modelling in Ambient Intelligence for Elderly and Disabled People,\" presented at the Proceedings of the 11th international conference on Computers Helping People with Special Needs, linz, Austria, 2008. [14] C. M. Hughes, \"Medication non-adherence in the elderly: how big is the problem?,\" Drugs Aging, vol. 21, pp. 793-811, 2004. [15] M. BÃ¶hm, H. Schumacher, U. Laufs, P. Sleight, R. Schmieder, T. Unger, K. Teo, and S. Yusuf, \" Effects of nonpersistence with medication on outcomes in high-risk patients with cardiovascular disease,\" American Heart Journal, vol. 166, 2013. [16] A. I. Martins, A. QueirÃ³s, M. Cerqueira, N. P. da Rocha, and A. J. S. Teixeira, \"The International Classification of Functioning, Disability and Health as a Conceptual Model for the Evaluation of Environmental Factors.,\" Procedia CS, vol. 14, pp. 293-300, 2012. [17] A. Cooper, R. Reimann, and D. Cronin, About Face 3: The Essentials of Interaction Design: Wiley Pub., 2007. [18] A. J. S. Teixeira, F. Ferreira, N. Almeida, A. F. Rosa, J. Casimiro, S. Silva, A. QueirÃ³s, and A. Oliveira, \"Multimodality and Adaptation for an Enhanced Mobile Medication Assistant for the Elderly,\" in Third Mobile Accessibility Workshop (MOBACC), CHI 2013 Extended Abstracts, France, 2013. [19] Microsoft. (2013, Metro Design Language of Windows Phone 7 | Microsoft Design .toolbox. Available: [20] F. Ferreira, N. Almeida, J. C. Pereira, A. F. Rosa, A. Oliveira, and A. Teixeira, \"Multimodal and Adaptable Medication Assistant for the Elderly - A prototype for Interaction and Usability in Smartphones,\" presented at the CISTI'2013 - 8Âª ConferÃªncia IbÃ©rica de Sistemas e Tecnologias de InformaÃ§Ã£o, Lisbon, 2013. [21] J. Nielsen. (2012, How Many Test Users in a Usability Study? Jakob Nielsen's Alertbox. Available: odologies for Involving Older Adults in the Design Process,\" in Universal Acess in Human Computer Interaction. Coping with Diversity. vol. 4554, C. Stephanidis, Ed., ed: Springer Berlin Heidelberg, 2007, pp. 982-989. [4] R. Eisma, A. Dickinson, J. Goodman, O. Mival, A. Syme, and L. Tiwari, \"Mutual inspiration in the development of new technology for older people,\" in In Proceedings of Include 2003, 2003, pp. 7--252. [5] E. Mynatt and W. Rogers, \"Developing technology to support the functional independence of older adults,\" Ageing International, vol. 27, pp. 24-41, 2001/12/01 2001. [6] R. Eisma, A. Dickinson, J. Goodman, A. Syme, L. Tiwari, and F. Newell, \"Early user involvement in the development of information technology-related products for older people,\" Univers. Access Inf. Soc., vol. 3, pp. 131-140, 2004. [7] S. Chamberlain, H. Sharp, and N. Maiden, \"Towards a framework for integrating agile development and user-centred design,\" presented at the Proceedings of the 7th international conference on Extreme Programming and Agile Processes in Software Engineering, Oulu, Finland, 2006. [8] J. Gulliksen, A. Lantz, and I. Boivie, User Centered Design in Practice - Problems and Possibilities, 1999. [9] R. D. Buurman, \"User-centred design of smart products,\" Ergonomics, vol. 40, pp. 1159-1169, 1997/10/01 1997. [10] J. Pruitt and J. Grudin, \"Personas: practice and theory,\" presented at the Proceedings of the 2003 conference on Designing for user experiences, San Francisco, California, 2003. [11] J. Grudin and J. Pruitt. (2002, Personas, Participatory Design and Product Development: An Infrastructure for Engagement. Fig. 3 â€“ Application features and corresponding number of users who considered them difficult (left) and easy (right). 408 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 [12] R. Sinha, \"Persona development for information-rich domains,\" presented at the CHI '03 Extended Abstracts on Human Factors in Computing Systems, Ft. Lauderdale, Florida, USA, 2 PROCS 2894 S1877-0509(14)00046-5 10.1016/j.procs.2014.02.044 The Authors ☆ Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). Elderly Centered Design for Interaction – The Case of the S4S Medication Assistant Flávio Ferreira a Nuno Almeida a b Ana Filipa Rosa a André Oliveira a José Casimiro c Samuel Silva a b António Teixeira a b ⁎ a Institute of Electronics and Telematics Engineering of Aveiro (IEETA), 3810-193 Aveiro, Portugal b Dep. of Electronics Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal c Polytecnic Institute of Tomar, 2300-531 Tomar, Portugal ⁎ Corresponding author. Tel.: +351 234370500; fax: +351 234370545. Several aspects of older adults’ life can benefit from recent technological developments, but success in harnessing this potential depends on careful design and accessible, easy to use products. Design and development must be centered on the elderly and adequately consider interaction. In this paper we follow this design approach and put it to the test in developing a concrete application, aimed to contribute to lower the high levels of non-adherence to medication in the elderly population. The “Medication Assistant” application was developed following an iterative method centered, from the start, on the elderly and interaction design. The method repeats short-time development cycles integrating definition of scenarios and goals, requirements engineering, design, prototyping and evaluation. Evaluation, by end-users, of the increasingly refined prototypes, is a key characteristic of the method. The evaluation results provide information related to strengths and weaknesses of the application and yield suggestions regarding changes and improvements, valuable support further development. Results regarding evaluation of the second prototype of “Medication Assistant” are presented. Keywords development method interaction user centred elderly evaluation mobile applications medication References [1] DESA. (2013). Available: [2] H. Matlabi, S. Parker, and K. McKee, “The contribution of home-based technology to older people's quality of life in extra care housing,” BMC Geriatrics, vol. 11, p. 68, 2011. [3] A. Newell, J. Arnott, A. Carmichael, and M. Morgan, “Methodologies for Involving Older Adults in the Design Process,” in Universal Acess in Human Computer Interaction. Coping with Diversity. vol. 4554, C. Stephanidis, Ed., ed: Springer Berlin Heidelberg, 2007, pp. 982-989. [4] R. Eisma, A. Dickinson, J. Goodman, O. Mival, A. Syme, and L. Tiwari, “Mutual inspiration in the development of new technology for older people,” in In Proceedings of Include 2003, 2003, pp. 7--252. [5] E. Mynatt and W. Rogers, “Developing technology to support the functional independence of older adults,” Ageing International, vol. 27, pp. 24-41, 2001/12/01 2001. [6] R. Eisma, A. Dickinson, J. Goodman, A. Syme, L. Tiwari, and F. Newell, “Early user involvement in the development of information technology-related products for older people,” Univers. Access Inf. Soc., vol. 3, pp. 131-140, 2004. [7] S. Chamberlain, H. Sharp, and N. Maiden, “Towards a framework for integrating agile development and user-centred design,” presented at the Proceedings of the 7th international conference on Extreme Programming and Agile Processes in Software Engineering, Oulu, Finland, 2006. [8] J. Gulliksen, A. Lantz, and I. Boivie, User Centered Design in Practice - Problems and Possibilities, 1999. [9] R. D. Buurman, “User-centred design of smart products,” Ergonomics, vol. 40, pp. 1159-1169, 1997/10/01 1997. [10] J. Pruitt and J. Grudin, “Personas: practice and theory,” presented at the Proceedings of the 2003 conference on Designing for user experiences, San Francisco, California, 2003. [11] J. Grudin and J. Pruitt. (2002, Personas, Participatory Design and Product Development: An Infrastructure for Engagement. [12] R. Sinha, “Persona development for information-rich domains,” presented at the CHI’03 Extended Abstracts on Human Factors in Computing Systems, Ft. Lauderdale, Florida, USA, 2003. [13] R. Casas, R.B. Maro, A. Robinet, A.R. Delgado, A.R. Yarza, J. Mcginn, R. Picking, and V. Grout, “User Modelling in Ambient Intelligence for Elderly and Disabled People,” presented at the Proceedings of the 11th international conference on Computers Helping People with Special Needs, linz, Austria, 2008. [14] C. M. Hughes, “Medication non-adherence in the elderly: how big is the problem?,” Drugs Aging, vol. 21, pp. 793-811, 2004. [15] M. Böhm, H. Schumacher, U. Laufs, P. Sleight, R. Schmieder, T. Unger, K. Teo, and S. Yusuf, “Effects of nonpersistence with. medication on outcomes in high-risk patients with cardiovascular disease,” American Heart Journal, vol. 166, 2013. [16] A. I. Martins, A. Queirós, M. Cerqueira, N.P. da Rocha, and A. J. S. Teixeira, “The International Classification of Functioning, Disability and Health as a Conceptual Model for the Evaluation of Environmental Factors.,” Procedia CS, vol. 14, pp. 293-300, 2012. [17] A. Cooper, R. Reimann, and D. Cronin, About Face 3: The Essentials of Interaction Design: Wiley Pub., 2007. [18] A. J. S. Teixeira, F. Ferreira, N. Almeida, A.F. Rosa, J. Casimiro, S. Silva, A. Queirós, and A. Oliveira, “Multimodality and Adaptation for an Enhanced Mobile Medication Assistant for the Elderly,” in Third Mobile Accessibility Workshop (MOBACC), CHI 2013 Extended Abstracts, France, 2013. [19] Microsoft. (2013, Metro Design Language of Windows Phone 7 | Microsoft Design .toolbox. Available: [20] F. Ferreira, N. Almeida, J.C. Pereira, A.F. Rosa, A. Oliveira, and A. Teixeira, “Multimodal and Adaptable Medication Assistant for the Elderly - A prototype for Interaction and Usability in Smartphones,” presented at the CISTI’2013-8ª Conferência Ibérica de Sistemas e Tecnologias de Informação, Lisbon, 2013. [21] J. Nielsen. (2012, How Many Test Users in a Usability Study? Jakob Nielsen's Alertbox. Available: "
    },
    {
        "doc_title": "Extending the H-tree layout pedigree: An evaluation",
        "doc_scopus_id": "84893273871",
        "doc_doi": "10.1109/IV.2013.56",
        "doc_eid": "2-s2.0-84893273871",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Evaluation",
            "Family structure",
            "Genealogy",
            "H-tree layout",
            "Information visualization",
            "Space-filling",
            "User experience",
            "User performance"
        ],
        "doc_abstract": "Visualizing large family structures is becoming increasingly important, as more genealogical data becomes available. A space-filling h-tree layout pedigree has been recently proposed to make better use of the available space than traditional representations. In a previous paper we applauded the technique's usage of available space but remarked that it makes generation identification difficult and does not allow navigating to descendants of represented individuals. A set of extensions was proposed to help overcome these limitations and a preliminary evaluation suggested that those extensions enhance the original technique. This paper presents a more thorough evaluation carried out to assess if and how the proposed extensions improve the original h-tree layout pedigree technique. Results suggest that these extensions improve user performance on some tasks, effectively provide new functionality, and generally enhance user experience. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Segmentation and analysis of vocal tract from midsagittal real-time MRI",
        "doc_scopus_id": "84884492397",
        "doc_doi": "10.1007/978-3-642-39094-4_52",
        "doc_eid": "2-s2.0-84884492397",
        "doc_date": "2013-09-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Anatomical structures",
            "Data processing and analysis",
            "Dynamic aspects",
            "Large amounts",
            "Nasal vowels",
            "Real-Time MRI",
            "Speech production",
            "Vocal-tracts"
        ],
        "doc_abstract": "The articulatory description of European Portuguese (EP) requires the analysis of different anatomical structures (e.g. tongue dorsum and velum), and the study of dynamic aspects of speech production. The use of real-time magnetic resonance imaging (RT-MRI), with frame rates above 10 frames/s, provides adequate support for these studies and results in a large amount of images that need to be processed to extract relevant data to be analysed by linguists. To tackle the required data processing and analysis this article presents methods to perform segmentation of the vocal tract from midsagittal real-time MR image sequences and provide researchers with visualizations of the relevant extracted data. Examples are provided illustrating the analysis of dynamic aspects of EP nasal vowels. © 2013 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a systematic and quantitative analysis of vocal tract data",
        "doc_scopus_id": "84906221888",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84906221888",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Large amounts of data",
            "Quantitative approach",
            "Quantitative comparison",
            "Real-Time MRI",
            "Traditional approaches",
            "Visual representations",
            "Vocal-tracts"
        ],
        "doc_abstract": "Articulatory data can nowadays be obtained using a wide range of techniques, such as real-time magnetic resonance (RT-MRI), enabling acquisitions of large amounts of data. A major challenge arises: Analysing these new large data sets to extract meaningful information regarding speech production in an expedite and replicable way. Traditional approaches such as superimposing vocal tract profiles and qualitatively characterizing relevant properties and differences, although providing valuable information, are rather inefficient and subjective. Therefore, analysis must evolve towards a more automated, quantitative approach. To tackle this issue we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative data regarding differences pertaining meaningful regions under the influence of various articulators. Visual representation of such data is a key part of the proposal and some concrete forms of visualization are proposed to depict the differences found and corresponding direction of change. Application examples concerning the articulatory characterization of EP vowels are presented with promising results, paving the way towards automated and objective analyses of articulatory data. Copyright © 2013 ISCA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Acoustic analysis of European Portuguese oral vowels produced by children",
        "doc_scopus_id": "84871478257",
        "doc_doi": "10.1007/978-3-642-35292-8_14",
        "doc_eid": "2-s2.0-84871478257",
        "doc_date": "2012-12-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Acoustic analysis",
            "Age effects",
            "Formant frequency",
            "Fundamental frequencies",
            "Male speakers"
        ],
        "doc_abstract": "This study investigates acoustic changes in the speech of European Portuguese children, as a function of age and gender. Fundamental frequency, formant frequencies and duration of vowels produced by a group of 30 children, ages 7 and 10 years, were measured. The results revealed that, for male speakers, F0, F1 and F2 decrease as age increases, although the age effect was not statistically significant for F0 and F1. A similar trend was observed for female speakers, but only in F2. Moreover, F0 and formant frequencies were found to be similar between male and female children. Between ages 7 and 10, vowel durations decreased significantly, and the values for females were higher than those for males. These results provide a base of information for establishing the normal pattern of development in European Portuguese children. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An MRI study of the oral articulation of European Portuguese nasal vowels",
        "doc_scopus_id": "84878599176",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84878599176",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "European Portuguese",
            "Imaging data",
            "Magnetic Resonance Imaging (MRI)",
            "Nasal vowels",
            "Production of",
            "Real-Time MRI",
            "Vocal-tracts",
            "Vowel production"
        ],
        "doc_abstract": "There is increasing evidence that, in addition to velopharyngeal coupling, lingual position may also change during production of phonemic nasal vowels. In order to investigate differences in oral articulation between European Portuguese (EP) nasal vowels and oral counterparts, imaging data (both static and real-time MRI) of several EP speakers (male and female) are used. Superimposition of outlines of the vocal tract profiles, semi-automatically extracted from MRI images, were used to compare the position of tongue and lips during nasal and oral vowel production. The results suggest that lingual and labial differences between nasal vowels and their oral counterparts are quite subtle in EP. Nasal vowels [ã], [õ] exhibited more articulatory adjustments with respect to oral congeners than [1̃] and [ũ].",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Segmentation and analysis of the oral and nasal cavities from MR time sequences",
        "doc_scopus_id": "84864125184",
        "doc_doi": "10.1007/978-3-642-31298-4_26",
        "doc_eid": "2-s2.0-84864125184",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Dynamic aspects",
            "Large images",
            "MR images",
            "Nasal cavity",
            "Nasal vowels",
            "Oral cavity",
            "Real-Time MRI",
            "Segmentation tool",
            "Speech production",
            "Temporal resolution",
            "Time sequences",
            "Time-consuming tasks",
            "Vocal-tracts"
        ],
        "doc_abstract": "The study of dynamic aspects of speech production in Portuguese is very important to characterize vowel nasalization. In this context, the analysis of velum movement remains a challenging task and only a few studies present articulatory descriptions of Portuguese nasal vowels. Advances in real-time MRI (magnetic resonance imaging) allow the acquisition of vocal tract images with reasonable spatial and temporal resolution to enable observation and quantification of articulatory movements. The resulting data consists of large image sequences and the structures of interest (e.g., oral cavity) have to be identified (segmented) throughout to enable analysis which can be a time consuming task. This article presents a segmentation tool for real-time MR image sequences of the oral and nasal cavities. The proposed tool has been implemented using MevisLab and provides features for the analysis of the resulting data. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring different parameters to assess left ventricle global and regional functional analysis from coronary CT angiography",
        "doc_scopus_id": "84879705431",
        "doc_doi": "10.1111/j.1467-8659.2012.02090.x",
        "doc_eid": "2-s2.0-84879705431",
        "doc_date": "2012-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Cardiac cycles",
            "Cardiac phasis",
            "Clinical practices",
            "Coronary artery disease",
            "Coronary ct angiographies",
            "Left ventricles",
            "Parameters characterizing",
            "Visual exploration"
        ],
        "doc_abstract": "Coronary CT angiography is widely used in clinical practice for the assessment of coronary artery disease. Several studies have shown that the same exam can also be used to assess left ventricle (LV) function. Even though coronary CT angiography provides data concerning multiple cardiac phases, along the cardiac cycle, LV function is usually evaluated using just the end-systolic and end-diastolic phases. This unused wealth of data, mostly due to its complexity and the lack of proper tools, has still to be explored to assess if further insight is possible regarding regional LV functional analysis. Furthermore, different parameters can be computed to characterize LV function and though some are well known by clinicians others still need to be tested concerning their value in clinical scenarios. Based on these premises, we present several parameters characterizing global and regional LV function, computed for several cardiac phases over one cardiac cycle. The data provided by the computed parameters is shown using a set of visualizations allowing synchronized visual exploration of the different data. The main purpose is to provide means for clinicians to explore the data and gather insight over their meaning and their correlation with each other and with diagnosis outcomes. © 2012 The Authors.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inter-observer variability assessment of a left ventricle segmentation tool applied to 4D MDCT images of the heart",
        "doc_scopus_id": "84862625754",
        "doc_doi": "10.1109/IEMBS.2011.6090923",
        "doc_eid": "2-s2.0-84862625754",
        "doc_date": "2011-12-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Cardiac angiography",
            "Cardiac phasis",
            "Computed Tomography",
            "Interobserver variability",
            "Left ventricles",
            "MDCT images",
            "Multiple detectors",
            "Myocardial perfusion",
            "Segmentation tool",
            "Semi-automatics"
        ],
        "doc_abstract": "Multiple detector row computed tomography (MDCT) cardiac angiography provides a large amount of data concerning multiple cardiac phases which are not often considered. Segmentation is a first step towards exploring how this additional data can be used to perform left ventricle functional analysis or myocardial perfusion assessment. We present preliminary results regarding the assessment of inter-observer variability for a semi-automatic (multi-phase) segmentation tool for the left-ventricle. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Tongue segmentation from MRI images using ITK-SNAP: Preliminary evaluation",
        "doc_scopus_id": "84864986322",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84864986322",
        "doc_date": "2011-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "3D Visualization",
            "European Portuguese",
            "Jaccard coefficients",
            "Magnetic resonance images",
            "Manual segmentation",
            "MRI Image",
            "Region competition",
            "Semi-automatic segmentation",
            "Semi-automatics",
            "Semiautomatic methods",
            "Similarity metrics",
            "Speech production",
            "Speech synthesizer",
            "Tongue segmentation",
            "Validation",
            "Volumetric data"
        ],
        "doc_abstract": "The purpose of this study was to evaluate and compare the efficiency, reliability and accuracy of manual and semiautomatic segmentation techniques to segment tongue images. This work is included in a vast framework (HERON II) that aims to improve an articulatory-based speech synthesizer (SAP-Windows), for European Portuguese (EP). Volumetric data from Magnetic resonance images (MRI) were used to extract tongue configurations from several speakers uttering different EP sounds, or the same sound produced in different contexts or syllabic positions, in a speech production study. Segmentations were performed manually and using a semi-automatic approach implemented in ITK-SNAP (Region Competition Snakes). Results from similarity metrics (Jaccard coefficient and voxelwise comparison) revealed that the semi-automatic (SA) method presents good agreement with the manual segmentation method (Jaccard=0.9002 and 10.495 voxel error). Furthermore, the semi-automatic method is more reproducible (Jaccard=0.9382 and 6.388 voxel error) than manual segmentation method (Jaccard= 0.9170 and 8.662 voxel error). The semi-automatic approach provides an efficient and reliable method to segment tongue images providing 3D visualizations that allows description and comparison of tongue configurations during the production of different sounds. This information is of great relevance in speech production field contributing to a better understanding of speech production mechanisms. © 2011 IADIS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inter-observer variability assessment of a left ventricle segmentation tool applied to 4D MDCT images of the heart.",
        "doc_scopus_id": "84055176594",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84055176594",
        "doc_date": "2011-12-01",
        "doc_type": "Article",
        "doc_areas": [],
        "doc_keywords": [],
        "doc_abstract": "The purpose of this study was to evaluate and compare the efficiency, reliability and accuracy of manual and semiautomatic segmentation techniques to segment tongue images. This work is included in a vast framework (HERON II) that aims to improve an articulatory-based speech synthesizer (SAP-Windows), for European Portuguese (EP). Volumetric data from Magnetic resonance images (MRI) were used to extract tongue configurations from several speakers uttering different EP sounds, or the same sound produced in different contexts or syllabic positions, in a speech production study. Segmentations were performed manually and using a semi-automatic approach implemented in ITK-SNAP (Region Competition Snakes). Results from similarity metrics (Jaccard coefficient and voxelwise comparison) revealed that the semi-automatic (SA) method presents good agreement with the manual segmentation method (Jaccard=0.9002 and 10.495 voxel error). Furthermore, the semi-automatic method is more reproducible (Jaccard=0.9382 and 6.388 voxel error) than manual segmentation method (Jaccard= 0.9170 and 8.662 voxel error). The semi-automatic approach provides an efficient and reliable method to segment tongue images providing 3D visualizations that allows description and comparison of tongue configurations during the production of different sounds. This information is of great relevance in speech production field contributing to a better understanding of speech production mechanisms. © 2011 IADIS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating user studies into computer graphics-related courses",
        "doc_scopus_id": "80052317465",
        "doc_doi": "10.1109/MCG.2011.78",
        "doc_eid": "2-s2.0-80052317465",
        "doc_date": "2011-09-07",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "computer graphics education",
            "graphics and multimedia",
            "Human-computer",
            "Information visualization",
            "Student collaboration",
            "User study"
        ],
        "doc_abstract": "The authors argue in favor of introducing user studies into computer graphics, human-computer interaction, and information visualization courses. They discuss two sets of user studies they developed and performed over several years, with student collaboration, and the different aspects of the studies they had to consider. They also discuss a user study they designed for an information visualization course. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Myocardial perfusion analysis from adenosine-induced stress MDCT",
        "doc_scopus_id": "79959984147",
        "doc_doi": "10.1007/978-3-642-21257-4_89",
        "doc_eid": "2-s2.0-79959984147",
        "doc_date": "2011-07-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic assessment",
            "Computed Tomography",
            "Coronary artery disease",
            "Image modality",
            "Myocardial perfusion",
            "Myocardial perfusion analysis",
            "Ventricular functions",
            "Automatic assessment",
            "Coronary artery disease",
            "Image modality",
            "Induced stress",
            "Multiple detectors",
            "Myocardial perfusion",
            "Myocardial perfusion analysis",
            "Ventricular functions"
        ],
        "doc_abstract": "Myocardial perfusion assessment is of paramount importance for the diagnosis of coronary artery disease. This can be performed using different image modalities such as single-photon emission computed tomography (SPECT) or magnetic resonance imaging (MRI). Recently, cardiac multiple-detector computed tomography (MDCT) has shown promising results with the benefit of gathering data regarding coronary anatomy, ventricular function and myocardial perfusion in a single study. Preliminary results for three different methods for automatic assessment of myocardial perfusion from adenosine-induced stress MDCT are presented. © 2011 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A preparatory study to choose similarity metrics for left-ventricle segmentations comparison",
        "doc_scopus_id": "79955785537",
        "doc_doi": "10.1117/12.878294",
        "doc_eid": "2-s2.0-79955785537",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Computational costs",
            "Ground truth",
            "Image processing and analysis",
            "Left ventricles",
            "preparatory studies",
            "Quantitative measures",
            "Segmentation methods",
            "Similarity metrics"
        ],
        "doc_abstract": "In medical image processing and analysis it is often required to perform segmentation for quantitative measures of extent, volume and shape. The validation of new segmentation methods and tools usually implies comparing their various outputs among themselves (or with a ground truth), using similarity metrics. Several such metrics are proposed in the literature but it is important to select those which are relevant for a particular task as opposed to using all metrics and therefore avoiding additional computational cost and redundancy. A methodology is proposed which enables the assessment of how different similarity and discrepancy metrics behave for a particular comparison and the selection of those which provide relevant data. © 2011 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using color in visualization: A survey",
        "doc_scopus_id": "79251621495",
        "doc_doi": "10.1016/j.cag.2010.11.015",
        "doc_eid": "2-s2.0-79251621495",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Color map",
            "Color scale",
            "Computer display",
            "Data and information",
            "Experimental research",
            "Guidelines",
            "Output devices",
            "Visual representations"
        ],
        "doc_abstract": "Color mapping is an important technique used in visualization to build visual representations of data and information. With output devices such as computer displays providing a large number of colors, developers sometimes tend to build their visualization to be visually appealing, while forgetting the main goal of clear depiction of the underlying data. Visualization researchers have profited from findings in adjoining areas such as human vision and psychophysics which, combined with their own experience, enabled them to establish guidelines that might help practitioners to select appropriate color scales and adjust the associated color maps, for particular applications. This survey presents an overview on the subject of color scales by focusing on important guidelines, experimental research work and tools proposed to help non-expert users. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2010-12-19 2010-12-19 2014-10-09T12:26:23 S0097-8493(10)00184-6 S0097849310001846 10.1016/j.cag.2010.11.015 S300 S300.2 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20110401 20110430 2011 2010-12-19T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb vol volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor primabst pubtype ref 0097-8493 00978493 false 35 35 2 2 Volume 35, Issue 2 17 320 333 320 333 201104 April 2011 2011-04-01 2011-04-30 2011 Virtual Reality in Brazil Luciano Pereira Soares a Judith Kelner b a Pontifical Catholic University of Rio de Janeiro, Tecgraf Rio de Janeiro, Brazil b Universidade Federal de Pernambuco (UFPE), Centro de Informática (CIn), Virtual Reality and Multimedia Group (GRVM), Cidade Universitária , Recife, PE, Brazil Visual Computing in Biology and Medicine Bernhard Preim c Charl P. Botha d c University of Magdeburg, Department of Computer Science, Magdeburg,Germany d Department of Mediamatics, Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands Semantic 3D media and content Bianca Falcidieno e Ivan Herman f e Cons. Nazionale delle Ricerche (CNR), Ist. di Matematica Applicata e Tecn. Inform., Genova, Italy f World Wide Web Consortium (W3C) and Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands Cultural Heritage Alan Chalmers g Mark Mudge h Luís Paulo Santos i g The Digital Laboratory, WMG, The University of Warwick, UK h Cultural Heritage Imaging, San Francisco, CA 94107, USA i Departamento de Informatica, University of Minho, Campus de Gualtar, Braga, Portugal article sco Copyright © 2010 Elsevier Ltd. All rights reserved. USINGCOLORINVISUALIZATIONASURVEY SILVA S 1 Introduction 2 Color models 2.1 Device-dependent color models 2.2 Device-independent color models 3 Desired properties for color scales 4 Univariate representations 4.1 Color model components 4.2 Redundant color scales 4.3 Double-ended color scales 5 Multivariate representations 5.1 Multivariate color scales 5.2 Color blending and weaving 6 Visualization characteristics and color scale selection 6.1 Data types 6.2 Spatial frequency 6.3 Task/goals 6.4 Visualization type 6.5 Audience, cultural connotations and context 7 Learning through experimentation 7.1 Which Blair project 7.2 Face-based luminance matching 7.3 Other examples 8 Auxiliary tools and methods 8.1 Dynamic exploration of color mappings 8.2 PRAVDAColor 8.3 ColorBrewer 8.4 Self-corrected perceptual colormaps 8.5 Enhancing visual exploration by appropriate color coding 8.6 VisCheck and Daltonize 8.7 Other examples 9 Conclusion Acknowledgment References BRODLIE 1992 K SCIENTIFICVISUALIZATIONTECHNIQUESAPPLICATIONS CARD 1999 S READINGSININFORMATIONVISUALIZATIONUSINGVISIONTHINK MACDONALD 1999 20 35 L BORLAND 2007 14 17 D SILVA 2007 943 948 S PROCEEDINGS11THINTERNATIONALCONFERENCEINFORMATIONVISUALIZATIONIV07 MORECOLORSCALESMEETSEYEAREVIEWUSECOLORINVISUALIZATION GONZALEZ 2002 R DIGITALIMAGEPROCESSING BRATKOVA 2009 42 55 M FOLEY 1990 J COMPUTERGRAPHICSPRINCIPLESPRACTICE LANDA 2005 436 443 E LEVKOWITZ 1992 72 80 H TRUMBO 1981 220 226 B ROGOWITZ 1996 268 273 B ROBERTSON 1986 24 32 P ROGOWITZ 1998 52 59 B WANG 2008 1739 1754 L WARE 1988 41 49 C ROBERTSON 1988 50 63 P HAGHSHENAS 2007 1270 1277 H CHUANG 2009 1275 1282 J SPENCE 2006 R INFORMATIONVISUALIZATIONDESIGNFORINTERACTION SALOMON 1990 269 278 G ARTHUMANCOMPUTERINTERFACEDESIGN NEWUSESFORCOLOR LIGHT 2004 385 391 A TEDFORD 1977 145 150 W CLEVELAND 1983 101 105 W JULESZ 1981 91 97 B DZMURA 1991 951 966 M BAUER 1996 1439 1446 B BAUER 1998 1083 1093 B ANDRIENKO 2006 N EXPLORATORYANALYSISSPATIALTEMPORALDATA BARTRAM 2002 66 79 L MEYER 1988 28 40 G BERLIN 1991 B BASICCOLORTERMSUNIVERSALITYEVOLUTION SAUNDERS 2002 302 355 B REGIER 2009 439 446 T HEALEY 1999 145 167 C GREGORY 1998 R EYEBRAINPSYCHOLOGYSEEING CHUANG 2009 203 211 J KOSARA 2003 20 25 R RHEINGANS 1990 145 146 P SILVAX2011X320 SILVAX2011X320X333 SILVAX2011X320XS SILVAX2011X320X333XS item S0097-8493(10)00184-6 S0097849310001846 10.1016/j.cag.2010.11.015 271576 2014-10-12T07:31:45.595335-04:00 2011-04-01 2011-04-30 true 2783623 MAIN 14 58808 849 656 IMAGE-WEB-PDF 1 si0019 114 13 8 si0018 114 13 8 si0017 114 13 8 si0016 114 13 8 si0015 114 13 8 si0014 114 13 8 si0013 114 13 8 si0012 114 13 8 si0011 114 13 8 si0010 114 13 8 si0009 114 13 8 si0008 114 13 8 si0007 114 13 8 si0006 114 13 8 si0005 114 13 8 si0004 114 13 8 si0003 114 13 8 si0002 114 13 8 si0001 367 12 102 gr9 889905 2274 3583 gr8 73328 833 833 gr7 582004 2010 2250 gr6 650541 1717 2500 gr5 724889 1678 3591 gr4 31161 432 400 gr3 24106 289 400 gr2 222538 635 1300 gr15 438188 1360 2500 gr14 142664 631 1300 gr13 127438 1258 1723 gr12 230509 1022 1710 gr11 1481375 2251 3583 gr10 1849286 2720 3583 gr1 145691 1133 1027 gr9 94600 513 809 gr8 17905 188 188 gr7 69471 454 508 gr6 76116 388 565 gr5 81297 379 811 gr4 14637 162 150 gr3 12340 108 150 gr2 40192 239 489 gr15 49326 307 565 gr14 35618 237 489 gr13 21202 284 389 gr12 36005 231 386 gr11 195665 508 809 gr10 151157 614 809 gr1 40809 426 386 gr9 14764 139 219 gr8 3355 164 164 gr7 13896 163 183 gr6 17825 150 219 gr5 11426 102 219 gr4 12461 164 152 gr3 15673 158 219 gr2 10558 107 219 gr15 11251 119 219 gr14 11017 106 219 gr13 2761 160 219 gr12 13862 131 219 gr11 16406 138 219 gr10 18797 164 216 gr1 9532 163 148 CAG 2079 S0097-8493(10)00184-6 10.1016/j.cag.2010.11.015 Elsevier Ltd Fig. 1 From left to right and top to bottom: greyscale, rainbow, heated-object, and linearized optimal color scales applied to a data set. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Color blending (left) and color weaving (right). Reproduced with permission from [23]. © 2003 IEEE. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 3 Quantitative scales available in ColorBrewer 2.0 [29] suitable for nominal data representation. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 4 Sequential scales available in ColorBrewer 2.0 [29] suitable for ordinal data representation. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 Two frequency modulated gratings represented using a saturation varying color scale (center) and a luminance varying color scale (right). Reproduced with permission from [30]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 Segmented color scales applied to low (top row) and high (bottom row) spatial frequency data. Reproduced with permission from [13]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 7 (a) The eight color scales used in the Which Blair Project [47]; (b) The eight overlapping LAB greyscales (top) and the results of applying them to the Blair photograph and; (c) An example of a faces matrix used in the experiment. Reproduced with permission from [47]. © 2001 IEEE. Fig. 8 Double face image. Reproduced with permission from [56]. © 2002 IEEE. Fig. 9 A screen capture showing PRAVDAColor integrated in the Data Explorer environment. Reproduced with permission from [33]. © 1995 IEEE. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 10 Three-dimensional color scale construction tool. Reproduced with permission from [33]. © 1995 IEEE. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 11 ColorBrewer's interface [29]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 12 ColorBrewer's interface details [29]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 Measured perception function for a rainbow color scale and for a normalized rainbow color scale. Reproduced with permission from [62]. Fig. 14 Left, standard rainbow color scale applied to a topographic data; right, a normalized rainbow color scale applied to the same data. Reproduced with permission from [62]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 15 On the left, color scale adapted for a segmentation task according to boxplot quartiles; on the right, changing the mapping function allowed a proper comparison of different regions. Reproduced with permission from [63]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) ☆ This article was recommended for publication by Robert van Liere. Technical Section Using color in visualization: A survey Samuel Silva a b ⁎ B. Beatriz Sousa Santos a b Joaquim Madeira a b a Institute of Electronics and Telematics Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 234 370 500; fax: +351 234 370 545. Color mapping is an important technique used in visualization to build visual representations of data and information. With output devices such as computer displays providing a large number of colors, developers sometimes tend to build their visualization to be visually appealing, while forgetting the main goal of clear depiction of the underlying data. Visualization researchers have profited from findings in adjoining areas such as human vision and psychophysics which, combined with their own experience, enabled them to establish guidelines that might help practitioners to select appropriate color scales and adjust the associated color maps, for particular applications. This survey presents an overview on the subject of color scales by focusing on important guidelines, experimental research work and tools proposed to help non-expert users. Keywords Visualization Color scales Color maps Guidelines 1 Introduction Visualization [1] is concerned with representing, manipulating and exploring data and information graphically in such a way as to gain understanding and insight into it, i.e., mapping of data to a visual form that supports human interaction in a workspace for visual sense making [2]. Color mapping is a very important visualization technique, but the choice of the most appropriate color scale to use with a particular data set is not just a matter of choosing a colorful and visually attractive representation. Adding color which does not add additional insight to the visualization can sometimes cause confusion as users try to understand its meaning and should, therefore, be avoided [3]. So, it is particularly important to perform the right choice in order to build visualizations which depict the desired information in a clear way. Throughout the years researchers have studied such issues and, profiting from findings in other fields such as human vision and psychophysics, managed to establish some guidelines which might help users along the process of color scale selection according, for example, to the type of data and task to be performed. Nevertheless, those guidelines are still not always used by visualization builders, and some well-documented problems are still ignored by the visualization community [4]. Extending a previous paper by the authors [5], a brief overview on the subject of color use in visualization is presented, providing information on the main concerns, findings and resulting guidelines, hopefully encouraging researchers to seek new solutions, evaluate the use of color in their visualizations and share their experience, thus contributing to a deeper knowledge on the subject. After a short introduction to color models, this survey focuses on the desired properties for color scales and the use of color representations for univariate and multivariate data, and discusses other factors conditioning the use of color in visualization (e.g., data features, tasks to be accomplished and target audience), while mentioning the guidelines that should drive the choice of appropriate color scales and representations, as well as the advantages of applying such guidelines. Afterwards, experimental research work on the field and some tools proposed to help non-expert users are described. Finally, some conclusions regarding the existing guidelines and their usage are presented. 2 Color models The purpose of a color model is to allow the specification of colors in a standard way. In essence, a color model is a specification of a coordinate system, and a subspace within that system, where each color is represented by a single point [6]. Several color models are described in the literature. Each of them has its own characteristics and is more or less suited to particular tasks. Therefore, before carrying on with the use of color in visualization, it is important to present a general overview on the different ways color can be represented. In general, color models can be divided in two classes: device-dependent, when the model allows the representation of the color gamut of a particular device and the same coordinates can represent slightly different colors depending on the device features; and device-independent, when the model provides a representation of color using a coordinate system independent of any output device. A brief description of these two classes is presented in the following subsections. For extended information on the subject of color models and a wider range of references the reader is forwarded to Bratkova et al. [7], which presents a new color space for computer graphics. 2.1 Device-dependent color models In the RGB color model each color is defined by adding three primaries: red, green and blue. This is analogous to what happens in a CRT display where the phosphor has similar base chromaticities. Since there is no strict value for the chromaticity of the three primaries, the same RGB coordinates can result in slightly different colors on different output devices. The CMY color model uses cyan, magenta and yellow as primaries, which are the complementary colors of red, green and blue, respectively. Thus, while RGB is an additive color model, i.e., by representing what is added to blackness, CMY is subtractive, representing what is subtracted to white light. This color model is usually used in color printers. In many situations black is added to this model, in order to allow a better representation of darker colors, and such color model is identified as CMYK. Although the RGB color model is based on the way color is represented in a CRT monitor (and thus sometimes called, along with the CMY color model, hardware-oriented [8]), it does not relate well with the way color is intuitively perceived. Thus, as an alternative, two additional (user-oriented) color models have been proposed: HSV (hue, saturation and value) and HSL (hue, saturation and lightness). These are based on the intuitive appeal of a painter's tint, shade, and tone. 2.2 Device-independent color models The three primaries red, green and blue cannot be used to represent all visible colors (at least using only positive values). In 1931 the Commission Internationale de l’Éclairage (CIE) [9] defined a new color model, CIE XYZ to avoid this problem [8]. Three new standard primaries (X, Y and Z) were defined, thus allowing a specification of all visible colors using only positive values. Two additional color models have been defined, derived from CIE XYZ, which are perceptually uniform: CIE LUV and CIE LAB. In a perceptually uniform color model the euclidean distance between a pair of colors in the color space is directly connected with their perceptual distance, i.e., if two pairs of colors have the same euclidean distance among them, their perceptual distance is the same. The first perceptually uniform color system, which was also an influence to CIE LAB, was the Munsell color system [10] which is still in use. 3 Desired properties for color scales Given a sequence of numerical values { v 1 ≤ v 2 , … ≤ v N } represented by colors {c 1, c 2, …, c N }, respectively, it is possible to identify the following desirable properties [11,12] for a color scale: Order—The colors chosen to represent the numerical values must be perceived as having the same order as them, i.e., if the values are ordered, the colors chosen to represent them must also seem ordered. An example can be the representation of a temperature scale by using the notions of cold and warm colors and their proportional mixtures in order to obtain a scale from cold to hot temperatures. It is important to note the special case of nominal data [13]: objects should be distinguishably different but, since they are not ordered, there should be no perceptual ordering in the representation. Uniformity and representative distance—The color representation of two values should convey the distance between them, and colors representing values which equally differ from each other should also seem equally different. Beyond that, it is required that clearly separated values must be represented by distinguishable colors, and that close values must be represented by colors perceived to be closer. This is what Trumbo [12] calls the separation principle. When representing flow information, for example, complementary colors can be used to represent flows in opposite directions and similar colors (with slight differences) to represent flows in the same direction. Levkowitz et al. [11] identify analogous principles proposed by Pizer et al. [14] (associability) and Robertson et al. [15] (separation). Boundaries—If there are no boundaries on the represented numerical data the color scale should not create this effect, i.e., the color scale must be able to represent continuous scales. Rows and columns principle—This is one of the principles proposed by Trumbo [12] which applies only to bivariate information. It states that if it is important to preserve univariate information, then the display parameters must not obscure one another, i.e., rows or columns having a constant value of one variable must have constant hue, saturation, or brightness. For example, using two display primaries (e.g., red and green) goes against this principle. Diagonal principle—The second principle proposed by Trumbo which only applies to bivariate information states that if the detection of positive association of variables is a goal, the displayed colors must be easily identified as belonging to one of the three classes: the ones near the minor diagonal, the ones above it, and the ones below. This could be accomplished with the major diagonal made up of greys, elements of maximum saturation, or constant hue. A hue and lightness scheme violates the diagonal principle [13]. 4 Univariate representations When using a color scale to represent univariate data, each color represents a single scalar value. It can be a continuous color scale, if color varies along the scale in such a way that adjacent colors are similar to one another, or a discontinuous color scale if that does not happen. In what follows some examples of continuous color scales are presented (according to a survey by Rheingans [16]). Additional examples can be found in [17]. 4.1 Color model components Grey scale—This color scale (see Fig. 1 ) maps scalars to brightness. It consists in a variation from black to white, with black representing, in general, the lowest value and white the highest. While this color scale presents some advantages, such as an easy to perceive ordering (the different and increasing brightness levels), it suffers from the fact that it displays a low contrast between the different colors, which limits its use in quantitative tasks. Rainbow scale—This is one of the most popular color scales used in the visualization literature [4]. It consists in a color path along the different colors of the rainbow, built by varying hue while keeping saturation and contrast at fixed values. For example, in HSV, it consists in a complete rotation around the value (V) axis. Regardless of its popularity, this color scale presents several problems [18]. For example, to some users it might not present an intuitive ordering unless they are familiar with the color progression (light spectrum). The position occupied by some of the colors might also lead to problems. Yellow is present half way through the color scale. This means that if one is interested in depicting extreme values the middle values might interfere, since yellow has an highlighting effect being perceived as brighter than the other colors. Another important aspect whenever using yellow in a visualization is that it has the smallest number of perceived saturation steps [19]. Therefore, users find it harder to distinguish small saturation variations for yellow than, for example, for blue. Another issue might be the fact that this color scale goes from red to violet. Since these colors are quite similar, both extremes will get visually close. To avoid this problem the color scale is usually cut at blue (as seen in Fig. 1). 4.2 Redundant color scales Using multiple display parameters to represent data may have several advantages [16]. First, one can draw benefits from the characteristics of various display parameters in conveying different kinds of information: while brightness is more effective conveying shape, hue is better in providing distinguishable display levels. This kind of redundancy might also help in dealing with situations where one of the parameters becomes ambiguous (e.g., due to a visual deficiency) and is compensated by the others. Finally, using redundancy might also allow a better distinction between values by reinforcing their visual differences. Ware [20] has studied redundant color scales and proved their utility by performing several empirical studies, which led to a suggestion that a color scale varying in both luminance and hue can be used to accurately represent both metric and surface properties. Some examples of such scales are: • Redundant model components—A straightforward redundant scale can be built by mapping data values to both hue and brightness. That kind of color scale has the advantage of being suitable for use by someone with dichromatic color deficiency. • Heated-object scale—This scale represents a compromise between the grey scale and the rainbow scale. It goes from black to white, passing through orange and yellow. This color scale has a stronger perceived natural ordering than the rainbow scale, since it has a monotonic increase in brightness (see Fig. 1). • Linearized optimal color scale—This color scale was introduced by Levkowitz et al. [11] to describe a scale which maximizes the number of JNDs (just noticeable differences) while preserving a natural order (see Fig. 1). 4.3 Double-ended color scales Such color scales are created by joining two monotonically increasing scales at a common end point. For example, one can join a scale from grey to red and a scale from grey to blue, building a scale from red to grey to blue. With such color scales it is possible to visually represent high, low and middle values clearly, since they exhibit three distinct groups of colors. A recent work by Moreland [21] discusses the creation and use of double-ended color scales, for example, as an alternative to the rainbow color scale. 5 Multivariate representations It is common that a single visualization requires the depiction of multivariate data. Multivariate color scales, and color blending and weaving are presented in what follows, as alternatives for representing multivariate data. 5.1 Multivariate color scales In a multivariate color scale two or more data variables are mapped to a single color representing them all. This is the same principle as the one used with redundant scales, but now each display parameter is related with a different variable. Working with the RGB color model, it is possible to map a variable into each one of its components, thus creating a multivariate color scale. For example, Landsat “false color” images are commonly produced by representing three multispectral scanner bands with levels of red, green and blue [22]. Therefore, if the represented bands are highly correlated, the image will be composed of shades of grey, since the three components will have close values. This scheme has the advantage that the extremes of the variable range (black, red, green, blue) are easily detectable. An analogous scheme can be obtained using, for example, the HSL color model. For more details on the possible approaches see [16]. A problem occurs when one needs to decompose the shown colors in their components. How can we detect similarities between areas that have the same value for two components but differ on the third? 5.2 Color blending and weaving Another approach for multivariate representations using color can be to use different color scales for each variable and then blend the results. This approach might also pose a difficulty in identifying individual values for each variable. Urness et al. [23] introduced a technique named color weaving, applied to flow visualization, which consists in coloring the line integral convolution using side-by-side streamlines of the different colors which need to be represented in a region. Fig. 2 shows an example of this technique compared with color blending. A study by Hagh-Shenas et al. [24] compares user performance when analyzing information represented on maps using color weaving and the more traditional color blending (consisting in a linear combination of all the colors which need to overlap in a region). Color weaving is performed by representing the individual colors side-by-side, in a high frequency texture which fills the region. Results show that users perform better when color weaving is used, particularly when more than two colors are being represented. Nevertheless, performance decreases for more than four colors. A possible alternative to extend this limit might be a technique earlier presented by Shenas et al. [25] which uses color and texture. Further work regarding color weaving can be found, for example, in Luboschik et al. [26] who present color weaving applied to scatter plots. Recently, Wang et al. [19] proposed a framework which uses a set of guidelines to support choosing the proper colors for a visualization when color blending is used, e.g., to depict objects that partially overlap and where perceiving the stacking order (which object is in front/back) is important. Whenever two semi-transparent objects overlap, it is easier to perceive which one is in front if their colors have opposite hues (e.g., red and blue) than closer hues (e.g., red and green), since the overlapping region in the first case will exhibit a hue closer to the object which is in front (or at least neutral) while, in the latter case, the overlapping region will present a new (shifted) hue. In situations where more than two objects overlap, a set of hues must be chosen as to avoid any hues which can be generated by mixing other hues in the set: a general rule is to choose opposing hues for the two most important objects and then neutral colors for all others. Furthermore, for situations where multiple objects overlap, using different opacities might also help users to perceive object order by decreasing object opacity from front to back. Saturation might also play an important role in avoiding drastic hue changes in overlapping regions, or in object order perception, by increasing saturation for front objects and decreasing it for back objects. Finally, Wang et al. also found that cold colors (e.g., blue and green) tended to be perceived as being in front, even if their opacity was lower than remaining objects. The opposite was true for warm colors (e.g., red and yellow). This was an unexpected result since it is the opposite effect which is, in general, observed (due to chromostereopsis [3]). For several examples the reader is forwarded to Wang et al. [19]. Chuang et al. [27] extended Wang et al. [19] work. Their main improvement is that they manage to perform blending by preserving the hues of each object and avoiding the appearance of new (shifted) hues, with no need for an initial careful selection of opposing hues. They also introduce the concept of dominant color, i.e., the color that has more impact on the final image as opposed to Wang et al. which always consider the foremost hue to be the most important. 6 Visualization characteristics and color scale selection The color scales and methods used to build a visualization do not depend only on the number of data variables involved. Several factors such as the characteristics of the data, the tasks which need to be accomplished or the target audience are important enough to be considered. 6.1 Data types When designing a visualization (i.e., picking a color scale) care must be taken to ensure that the most striking features of the image reflect the most important features of the data. If a representation catches the user's attention with unimportant features of the data, more interesting features might be missed [16]. Bright colors, sharp boundaries, or high saturation areas will most likely catch the user's attention. Thus, it is important to consider the data that will be represented and their type, and know what is more important: for example, to call attention to middle values or to positive/negative deviations from a zero (threshold). It is possible to distinguish between four data types [28]: Nominal data—For nominal data no mathematical operations are possible, since the value assigned to a particular measurement represents a name. An example is the categorization of different lung diseases with numerical labels 1, 2, 3 and 4: no mathematical operation is meaningful on these data. Here, numbers are identifiers. As noted above, the representation used for these kinds of data should not implicitly order it. A bad choice would be to use a grey scale since users intuitively perceive an order from dark to bright or vice-versa [4]. Fig. 3 shows several segmented color scales provided by ColorBrewer [29] (part of its qualitative color scales set) which could be used to represent nominal data. The number of colors used to represent nominal data should be restricted to seven or less [3]. This guideline is based on two constraints: users ability to discriminate between colors and their ability to remember the meaning of each color while looking into the visualization. Ordinal data—With ordinal data, values are assigned to measurements (for example), but no assumption is made about the spacing in-between such measurements, i.e., there can exist a numbering of 1, 2, 3 and 4 but the distance between elements 1 and 2 cannot be assumed to be equal to the distance between 3 and 4. Ordinal data are inherently discrete [30]. The used representation should allow discrimination between objects and the perception of their relative order. The quite common rainbow scale, for example, would not be a good idea to represent ordinal data. Even though it can be said that it is an ordered scale, if we consider wavelengths, it is not perceptually ordered [31,32,4]. Fig. 4 shows several examples of suitable segmented scales provided by ColorBrewer [29] (part of the sequential color scales set). Interval data—For interval data, the numerically equal distances between values are assumed to be actually equal. These kinds of data are commonly a (experimental) measure such as temperature, etc. The used representation should account for this: equal steps in data values should correspond to equally perceived magnitude in the representation. A perceptually uniform color space can be used to help choosing the appropriate colors. Ratio data—On ratio data, ratios between values are assumed to be equal and values increase/decrease monotonically about a true zero or threshold. This feature should be preserved in the representation. An example is absolute temperature measured using the Kelvin scale. If the ratio data have a relevant zero crossing, it can be preserved by using a double-ended color scale with a transition at zero. If the data are to be represented by a segmented color scale, then it is probably a good idea to have an even number of steps (with a transition at the zero level). 6.2 Spatial frequency An important issue to consider when choosing a color scale is human spatial vision. The luminance and saturation mechanisms in human vision play an important role in spatial sensitivity, but they have different characteristics. The human visual system accurately processes high-resolution images, or data which varies rapidly over an area, if that spatial variation is represented as a variation in luminance, i.e., the luminance channels are responsible for processing high spatial frequency information. This means that, when representing data with a high spatial frequency, it is a good idea to use a color scale which provides a strong luminance variation across the data range [33]. On the other hand, the saturation mechanisms in human vision are more sensitive to low spatial frequency variations. Such kind of effects are illustrated in Fig. 5 . On the top row, a frequency modulated grating beginning at one cycle per image and increasing in spatial frequency is presented (the corresponding waveform is presented on the first column). The variation is represented using a saturation varying color scale (in the center) and a luminance varying color scale (on the right). Notice how the saturation-variation color scale makes the sinusoidal variation more visible, at the low frequency end of the spectrum, than the luminance varying color scale. On the bottom row, the opposite happens: with the saturation varying color scale (in the center) you can only observe the first few cycles of the frequency modulated grating, observing twice as many when using the luminance varying color scale (on the right). Looking, for example, at interval and ratio data, both luminance and saturation varying color scales can produce the effect of having equal steps in data to be represented by equal perceived steps on the color scale, but the first will most certainly be more adequate to high spatial frequency data variations, while the second will be more suited for low spatial frequency variations. 6.3 Task/goals The goal of a particular visualization is an important issue when choosing a color scale. Tasks which require the judgment of metric quantities in the data tend to be better accomplished with color scales which do not vary monotonically in the opponent color channels (brightness, red–green, yellow–blue). On the other hand, tasks involving qualitative judgments about value distribution shape are better served with color scales varying systematically in brightness, allowing our visual system to employ familiar shape-from-shading mechanisms [16]. In situations where the perceived size of an object is important, care must be taken in how the object is colored. An early study by Tedford et al. [34] found a significant color-size effect leading to a conclusion that warm colors such as red, orange and yellow appear larger than cool colors like green. In another study, by Cleveland et al. [35], users were asked to judge, on a map with equal colored areas in red and green, which one was the largest: the average observers considered the red areas were larger. The obtained results suggest also that the color–size effect grows stronger for very saturated colors, which indicates that these colors might not be the better choice for tasks where the user is expected to make judgments about size. Considering the kind of task to perform, the color scale can be designed accordingly [13]. For example, regarding segmentation tasks, some of the rules used to create isomorphic color scales for ratio and interval data are also useful for creating maps for segmented data. In high spacial frequency data, luminance can be used to convey monotonicity; in low spacial frequency data monotonicity can be conveyed through the saturation component. When creating a segmented color scale, it is necessary that the segments be discriminable from one another. This will limit the number of steps which can be represented. Bergman et al. [33] state that a higher number of steps can be effectively discriminated for low spatial frequency data than for high spatial frequency. In Fig. 6 , on the left side, a five-level segmented color scale is used and, on the right side, a ten-level segmented color scale is used. They are applied to low spatial frequency data (top) and high spatial frequency data (bottom). For the low spatial frequency data (top row), having additional levels provides additional information. For this particular case, additional features of the earth's magnetic field are revealed (notice the southern hemisphere). On the contrary, on the bottom row, showing high spatial frequency cloud fraction observations, additional features are not revealed by increasing the number of color scale steps. Another notable example concerns highlight tasks. The principles which should guide the selection of a color scale for highlighting particular features in the data can be found, for example, in Julesz [36]. Using those principles, it is possible to design color scales which draw attention to a particular range in the data. When dealing with high spatial frequency data, for example, a possible approach can be that of using varying saturation to represent the information (maintaining constant luminance), while modifying the hue in the regions needing to be highlighted. The work of D’Zmura [37] and later work by Bauer et al. [38,39] have treated the topic of linear separability, which can also help in highlighting tasks. When a target has a color which can be separated from all other background colors with a single straight line in color space, it will be easier to detect, i.e., its perceived difference from all others will be clearer. Another important aspect regarding hue is that similar hues are perceived as part of the same group [19] which leads to, for example, separate regions on the display having similar hues being perceived as having a connection. This is useful if the goal of the visualization is to detect groups within the global data. The different groups can be represented by different (more distant) hues. Nevertheless, it should be noted that color should probably be reserved for the most important groups (while using other graphical dimensions as shape and size for other groupings) or this advantage will be reduced [40]. Recently, Tominsky et al. [41], based on a task model proposed by Andrienko et al. [42], have presented several strategies for adapting the color mapping to specific tasks such as segmentation, localization (highlight) and comparison. For lookup tasks, for example, they use two techniques, histogram equalization and box-whisker plot adaptation, in order to build a color mapping which assigns a higher number of distinguishable colors to the value ranges where more data values are concentrated. 6.4 Visualization type It is important to consider the whole visualization during the process of color scale design for the individual elements. For example, 3D visualizations have different constraints than those imposed by 2D visualizations. A good example of the problems that can occur is related with shading: if users use shading cues to judge the 3D shape of a representation object (e.g., an isosurface); a brightness varying color scale might interfere with the brightness values resulting from the shading calculations. Nonetheless, a brightness varying scale may be used in planar objects on a 3D scene. Due to the way human vision focuses on the different colors [3] in a 2D visualization a red region is perceived as closer to the user than a blue region thus creating a depth effect. Therefore, adjacent regions of strong blue and red should be avoided to prevent this effect. Another issue might arise from the requirement of displaying multiple variables in the same visualization. The used color scales should not generally overlap, with the representation for each variable interfering with the others as little as possible. For this purpose, the weaving techniques reviewed above might provide a solution. When representing multivariate data, motion can also play an important role as it provides an additional dimension. Even though color is commonly considered irrelevant in the perception of object movement, Weiskopf [43] proposes a set of five guidelines, supported by a set of psychophysical results described in the literature, which concern the use of color to better convey motion in visualization. Those guidelines focus on improving motion perception and the discrimination of motion direction, on using moving color patterns to highlight and visually group data, and on the influence of the chromatic and achromatic channels in apparent speed perception. The author also proposes two calibration procedures, which are necessary for the proper application of some of the proposed guidelines, regarding isoluminance and speed calibration. Application examples are also presented concerning data grouping and flow visualization. It is also important to note that motion can sometimes be more effective to call users attention to a particular element of the visualization than just changing its color as shown by Bartram et al. [44,45] and therefore using both motion and color in concurrent ways, e.g., to highlight two objects, one using motion and the other using color (as opposed to Weiskopf's work where color is used to help convey motion), might lead to problems, for example, with the motion highlight diverting attention from the color highlight. 6.5 Audience, cultural connotations and context It is important to have information about the target audience of a visualization, since it can provide some clues for color scale design. For example, conventions in one application area might place blue/violet colors of a spectrum scale at the low end (in order of increasing wavelength), while in another they may be placed at the high end (in order of increasing frequency) [16]. Therefore, paying attention to area conventions may make the process of designing the color scale easier and help avoid unintentional breaks with viewer expectations. Furthermore, vision associated problems must also be considered, e.g., older people tend to be less sensitive to color. But one of the most common problems is color blindness. This is an important factor to have in mind, since it can seriously influence user performance by limiting the amount of information that can be extracted from color representations. This can be overcome by providing several parameters which users can adjust, while using hue as the only way of encoding information must be avoided. The literature [46] also describes several tests which can be used to detect color blindness problems in users. Color can also have strong cultural connotations varying from culture to culture. Following such cultural conventions it is possible, for example, to reduce the cognitive load on the viewer or use connotations that suggest natural linkings between a variable or a variable value and the color used to represent it. For example, for an USA audience the color green is connected/associated with the color of money; a natural connotation, when visualizing temperatures can be that of high temperatures represented in red and low temperatures in blue [16]. And what about the influence of culture in color cognition? Without going into much detail (given the complexity of the subject), there is active discussion regarding how culture and language might influence color cognition. On one side (among others), Berlin and Kay [48] argue that color cognition is innate and that color category perception/cognition is universal regardless of the language spoken. This is called the universalist view. On the other side, the relativist view (e.g., Saunders et al. [49]) argues that color cognition is a much more culture-related phenomenon and has pointed several flaws in Berlin and Kay's study regarding the methodology used and several a priori assumptions. Salomon et al. [31] state that cultural differences limit the number and categories of color recognized by an individual. Recently, Regier and Kay [50] argued that a purely universalist or relativist view on this subject cannot stand against recent findings and, thus, a new view gathering ideas from both sides might be a more proper approach. Regarding color categories, Kawai et al. [51] have shown that the named category in which each individual color is inserted (by people) can influence perceived color difference, i.e., if two colors belong to the same named category (e.g., blue) their perceived difference will appear smaller than the perceived difference between two colors, at the same euclidean distance from each other, but in different color categories (e.g., blue and purple). For a good illustration of these effects and a description of obtained experimental results see Healey et al. [52]. The question remains regarding how the relativist view, considering color cognition is influenced by cultural issues, would influence these findings across cultures. It is also important to understand the influence of external conditions and how adaptation mechanisms of the eye work: for example, the negative afterimage of what has been previously seen can result in visual stress from prolonged viewing; as well as the phenomenon of time dependent color perception influenced, for instance, by different environmental lightings [53] and display device properties. Providing adjustable parameters can be a good solution to deal with different output device characteristics. For instance, it is important to remember that the same RGB coordinates may result in slightly different colors in different output devices and, therefore, to render colors accurately some calibration process might be necessary. Furthermore, the display device might influence our choice of color scale to meet, for example, power consumption goals in portable devices. Researchers have been striving to propose color scales which lower display device power consumption. A recent example is the work of Chuang et al. [54] which propose color scales allowing up to a 40% save in energy. 7 Learning through experimentation In order to apply theoretical principles coming from other areas (such as psychophysics), verify the applicability of new principles, and find clues for the definition of new ones, many researchers have been conducting user studies [55]. Human color vision, a subject well studied (for more than a century now), provides strong clues for using color in visualization. However, the choice of colors for a particular task is more difficult, as it is far more complex, than the simple displays used by the experimental psychologists. So, experiments are necessary to fill the gap between theory and practice [55]. The main goal is to use well established theories to build design guidelines and then use an experiment to validate the guidelines in an applied setting. 7.1 Which Blair project The work of Rogowitz et al. [47] presents a method which uses visual judgments to perceptually evaluate color scales. Since the literature points out that color scales which monotonically increase in luminance are good candidates for representing the magnitude of continuous data, the proposed method was designed to identify scales that include a monotonic luminance component. For that purpose, an image of a human face was used (Tony Blair), since it provides a structured visual pattern with gradual variations in luminance across its surface, thus providing the identification of irregular variations in the luminance of color scales. The experiment started from eight color scales (shown in Fig. 7 a) and, for each of them, seven additional scales where created, each subtending 25% of the original scale, which gave a total of 64 color scales. Fig. 7b shows the eight overlapping CIELAB greyscales created and how they looked when applied to Tony Blair's image. During the experiment, users were shown 32 pictures randomly ordered for each observer (Fig. 7c shows an example of part of one of the image matrices used). They then had to rate each picture according to the degree to which the image appeared to be a recognizable photograph of a face. The obtained results seemed to show that the proposed method may function as a quick procedure of identifying color scales with monotonically increasing luminance with the advantage that it does not require display calibration or lengthy psychophysical procedures. 7.2 Face-based luminance matching The work of Kindlmann et al. [56] is similar to the one presented by Rogowitz et al. [47]. They address the problem of color scale luminance control by proposing a novel technique for luminance matching. Their technique, given a fixed reference color, and a test color with brightness varied by the user, allows matching the luminance of both. They use images of faces in their experiment since they want to take advantage of humans being good at recognizing faces, due to brain circuitry dedicated to this process [57]. Their method starts by a black-and-white thresholded image of a human face: shadows in black and directly lit areas in white. The test pattern is then composed by the images in Fig. 8 which also include a reverse image. Next, they replace black with a shade of grey and white with a color. According to the luminance of the colored region, in comparison with the luminance of the chosen grey, one of the faces will appear positive and the other negative. For example, in Fig. 8 the left face is positive. Therefore, to match the luminance between the two regions it is only necessary to adjust the intensity of the grey or color region until the point where none of the faces appears positive or negative. The authors claim that their method provides very good results and enables the creation of color scales, with any pre-determined pattern of luminance, in devices (e.g., monitors) which are not calibrated. 7.3 Other examples Other examples are the work of Ware [20], which leads to some rules guiding the process of color scale construction, of Healey [58], which presents a technique for effectively choosing multiple colors for use during data visualization, and of Montag [59], where the performance in judging values in univariate maps encoded using five different color scales is tested. 8 Auxiliary tools and methods During the past few years some efforts have been made in order to provide users (in particular non-experts) with tools and methods which allow them to select an appropriate color scale for their particular visualization purpose. 8.1 Dynamic exploration of color mappings Rheingans et al. [60] propose a tool which allows the exploration of data sets by interactively manipulating the color scale. On the upper left of the screen appears the image space which shows the currently selected color scale applied to the data. In the center of the screen a 3D color space appears and a curve, within it, shows the path defining the sequence of colors composing the color scale. As the path in the 3D color space is modified, the image is dynamically changed accordingly. 8.2 PRAVDAColor Bergman et al. [33] present a tool called PRAVDAColor which focuses on helping users to select color scales. With that purpose, they have built a library of color scales and defined a set of perceptual rules which allow selecting appropriate maps according to the structure of the data and visualization goal. They present a taxonomy for color scale selection which guides their work: starting from the type (ratio, interval, ordinal or nominal) and spatial frequency of the data, and according to the representation task (isomorphic, segmentation or highlight), several guidelines are proposed (see Bergman et al. [33]). Fig. 9 shows PRAVDAColor integrated in the Data Explorer environment. With PRAVDAColor the user is presented a set of color scales judged appropriate to the data set (based essentially on the data spatial frequency, but also on the presence/absence of a zero crossing in order to distinguish between ratio and interval data) and task. At first, one of those color scales is automatically applied to the data in order to produce a first representation, then the user can freely apply any other of the available color scales to the data. It is also possible to control the way the color scale is mapped onto the data by defining to which colors the data minimum, maximum and midpoint values will be mapped. To build the color scales another tool is also available (similar to the one proposed by [60]). 1 1 For recent information on 3D color pickers see Wu et al. [61]. It is a 3D color scale builder. The user can interactively build a path along a hue–lightness–saturation (HLS) double cone (Fig. 10 ). This tool provides undo and move functions for editing that path. It is then possible to use the created color scale along the Data Explorer pipeline. The output color scale can be a linear interpolation between the selected points on the HLS space, or the user may obtain a discrete color scale, with equally sized segments, for each selected point (which may be suitable for segmented representation tasks). 8.3 ColorBrewer ColorBrewer [29] (recently updated to its version 2.0), developed using Macromedia's Flash, is not exactly a tool to create color scales or directly decide the better color scale for a data set. Instead, it provides an environment and a set of color scales to help users choose the best color options for maps (see Fig. 11 ). The process starts by defining the number of data classes the user wants to represent. Then, he chooses the type of legend: sequential, diverging or qualitative. After that, some legends are presented using different color schemes and the user can choose among them. An example map is then colored according to the user's choice. It is possible to modify several map parameters: activate/deactivate a road network, city symbols, or region borders and their colors (using the interface presented in Fig. 12 ); zoom; and modify the color with which each of these parameters appear. This tool also provides an interesting feature: it informs the user about some properties of the chosen representation, namely: 1. If it is suitable for color blind people. 2. If it will withstand black and white photocopying. 3. If it is suitable to view on LCD monitors. 4. If it is color printing friendly (according to tests made on some printers). This information is provided by the symbols presented on the left of Fig. 12 (a red cross over a symbol means that the chosen color scheme does not support that property). In the interface area presented on the right of Fig. 12 it is possible to see the chosen color scheme and obtain the parameters defining each color using one of two color models (RGB and CMYK). The appearance of roads, cities and region borders can also be enable/disabled in this area and the color for each of these elements can be chosen. A new feature in ColorBrewer 2.0 is the ability to select a background for the map (a solid color or terrain data) and then choose the level of transparency applied to the overlayed data. The limitation of this tool is the fact that there is no possibility of using a different data set to perform the testing. 8.4 Self-corrected perceptual colormaps Gresh [62] presents an algorithm which, given a particular color scale, transforms it into one that is more perceptually uniform, i.e., equal steps in data values are equally perceived. The perception function for a particular user on a given monitor is experimentally measured and then, based on those results, the color scale is modified to make it as uniform as possible. Fig. 13 shows the original measured perception function and the perception function measured for a normalized rainbow color scale. Notice how the new color scale presents a flatter perception function. Fig. 14 shows, on the left, a standard rainbow color scale applied to a topographic data set and, on the right, the same data set colored with the obtained normalized rainbow color scale. It is possible to perceive more detail in the image on the right, specially in the blue region and a reduction of the green region. 8.5 Enhancing visual exploration by appropriate color coding The work of Schulze-Wollgast et al. [63] extends the methods proposed by Bergman et al. [33] by extracting statistical metadata from the data set, and then using that information to adapt the chosen color scale. The used metadata consists in the average, median, mode, minimum, maximum, skewness and quartiles of the data set. While all these values can be computed for quantitative data, only a subset can be computed for ordinal and nominal data. In the latter case, only the mode can be computed. Based on such data several automatic color scale modifications may occur which include adjusting control points for setting an appropriate mapping function. Fig. 15 shows how a color scale has been adjusted for a segmentation task by positioning control points according to boxplot quartile position. Changing the mapping function may be necessary to correctly deal with certain data distributions. Linear interpolation between control points is the most common approach, but can lead to problems if the data are not uniformly distributed (e.g., due to outliers), in which case the scale is stretched leading to unnoticeable differences between close values. This is solved by using nonlinear mapping functions. The decision is supported by analysing the skewness of the data distribution: if it is positive an exponential mapping function is used; if it is negative a logarithmic mapping function. Fig. 15, on the right, shows how changing the mapping function allows a better comparison among regions. Another feature presented by Schulze-Wollgast et al. [63] is a color legend which includes a boxplot side-by-side with the color scale, thus providing a better insight on the data distribution. This feature can be observed in Fig. 15. 8.6 VisCheck and Daltonize VisCheck [64] is a tool which shows how an image or site is seen by an user with some kind of colorblindness. It is possible to choose among three different types of vision deficiencies, deuteranope (red/green deficit), protanope (another red/green deficit) and tritanope (blue/yellow deficit), and then see how it influences the perceived colors. Daltonize [65] is a tool which allows correcting a particular image in order to allow a better perception of color differences by users with vision deficiencies. 8.7 Other examples Other examples are the work of Hyun [66], which deals with the creation and usage of non-linear color scales, and of Ventura et al. [67], which deals with the problem of ordered/unordered color scales, and its generation in a device-independent color space, through a computer-aided color coding system based on fundamental principles of human vision. 9 Conclusion Given the importance of color in data and information visualization, an overview of relevant work regarding the use of color in visualization scenarios is presented. From surveyed literature, it is clear that more detailed knowledge is of paramount importance for an informed use of color in visualization. The guidelines proposed by several authors do not provide a solution for all scenarios, but help users to understand the advantages and disadvantages of using particular color scales in specific situations, and contribute for a greater awareness to possible issues in their visualizations. The fact that these guidelines are produced with the support of experimental work is very important, as it increases their value and helps in understanding the subjacent concepts and ideas. The tools proposed to help create color scales which are suited for particular data or visualization goals, or which provide environments where color scales can be applied to data using specific taxonomies, allow that several of those guidelines be transparently (or, at least, with some support) applied by users. But, even though these guidelines and tools exist, they are not being systematically used by practitioners. In fact, one of the most problematic color scales known (the rainbow color scale) is still the most used in visualization tools [4]. Therefore, efforts must be made to motivate the community to pay more attention to the issues related with color usage in visualization, and to justify their choices with more than an aesthetic motive. The surveyed literature also reveals how important it is to learn with the work of others and will hopefully motivate researchers to share their experiences in different application areas where visualization is present. By presenting this survey, we aim to provide a contribution to such efforts. Acknowledgment The first author's work is funded by grant SFRH/BD/38073/2007 awarded by the Portuguese Science and Technology Foundation (FCT). References [1] K. Brodlie L. Carpenter R. Earnshaw J. Gallop R. Hubbold A. Mumford Scientific visualization, techniques and applications 1992 Springer [2] S. Card J. Mackinlay B. Shneiderman Readings in information visualization: using vision to think 1999 Morgan Kauffman Publishers [3] L. MacDonald Using color effectively in computer graphics IEEE Computer Graphics & Applications 19 4 1999 20 35 [4] D. Borland R. Taylor Rainbow color map (still) considered harmful IEEE Computer Graphics & Applications 27 2 2007 14 17 [5] S. Silva J. Madeira B. Sousa Santos There is more to color scales than meets the eye: a review on the use of color in visualization Proceedings of the 11th international conference on information visualization (IV07) 2007 IEEE Computer Society Zurich, Switzerland 943 948 [6] R. Gonzalez R. Woods Digital image processing 2002 Addison-Wesley [7] M. Bratkova S. Boulos P. Shirley oRGB pratical opponent color space for computer graphics IEEE Computer Graphics & Applications 29 1 2009 42 55 [8] J.D. Foley A. van Dam S.K. Feiner J.F. Hughes Computer graphics: principles and practice 1990 Addison-Wesley [9] CIE, Comission internationale de l’eclairage 〈 〉 , online March 2010. [10] E. Landa M. Fairchild Charting color for the eye of the beholder American Scientist 93 5 2005 436 443 [11] H. Levkowitz G. Herman Color scales for image data IEEE Computer Graphics & Applications 12 1 1992 72 80 [12] B. Trumbo Theory for coloring bivariate statistical maps The American Statistician 35 4 1981 220 226 [13] B. Rogowitz L. Treinish How not to lie with visualization Computers in Physics 10 3 1996 268 273 [14] Pizer SM, Zimmerman JB, Johnston RE. Contrast transmission in medical image display. In: Proceedings of the 1st international symposium on medical imaging and interpretation; 1982. p. 2–9. [15] P.K. Robertson J.F. O’Callaghan The generation of color sequences for univariate and bivariate mapping IEEE Computer Graphics & Applications 6 2 1986 24 32 [16] Rheingans P. Task-based color scale design. In: Proceedings of the SPIE—28th AIPR workshop: 3D visualization for data exploration and decision making, vol. 3905; 2000. p. 35–43. [17] Bourke P. Color ramping for data visualization 〈 〉 , online March 2010. [18] B. Rogowitz L. Treinish Data visualization: the end of the rainbow IEEE Spectrum 35 12 1998 52 59 [19] L. Wang J. Giesen K. McDonnell P. Zoliker K. Mueller Color design for illustrative visualization IEEE Transactions on Visualization and Computer Graphics 14 6 2008 1739 1754 [20] C. Ware Color sequences for univariate maps: theory, experiments, and principles IEEE Computer Graphics & Applications 8 5 1988 41 49 [21] Moreland K. Diverging colormaps for scientific visualization. In: Proceedings of the 5th international symposium on visual computing. Lecture Notes in Compuer Science, vol. 5876; 2009. p. 92–103. [22] P.K. Robertson Visualizing color gamuts: a user interface for the effective use of perceptual color spaces in data displays IEEE Computer Graphics & Applications 8 5 1988 50 63 [23] Urness T, Interrante V, Marusic I, Longmire E, Ganapathisubramani B. Effectively visualizing multi-valued flow data using color and texture. In: Proceedings of the IEEE Visualization 2003 (VIS 2003); 2003. p. 115–21 [24] H. Hagh-Shenas S. Kim V. Interrante C.G. Healey Weaving versus blending: a quantitative assessment of the information carrying capacities of two alternative methods for conveying multivariate data with color IEEE Transactions on Visualization and Computer Graphics 13 6 2007 1270 1277 [25] Shenas H, Interrante V. Compositing color with texture for multi-variate visualization. In: Proceedings of the 3rd international conference on computer graphics and interactive techniques in Australasia and South East Asia; 2005. p. 443–6 [26] Luboschik M, Radloff A, Schumann H. A new weaving technique for handling overlapping regions. In: Proceedings of the international conference on advanced visual interfaces (AVI’10); 2010. p. 25–32. [27] J. Chuang D. Weiskopf T. Möller Hue-preserving color blending IEEE Transactions on Visualization and Computer Graphics 15 6 2009 1275 1282 [28] R. Spence Information visualization: design for interaction 2nd ed 2006 Prentice Hall [29] Brewer CA. ColorBrewer 2.0 〈 〉 , online March 2010. [30] Rogowitz B, Treinish L. Why should engineers and scientists be worried about color? 〈 〉 , online March 2010. [31] G. Salomon New uses for color B. Laurel The art of human computer interface design 1990 Addison-Wesley 269 278 [32] A. Light P.J. Bartlein The end of the rainbow? Color schemes for improved data graphics EOS Transactions American Geophysical Union 85 40 2004 385 391 [33] Bergman LD, Rogowitz BE, Treimish LA. A rule-based tool for assisting color map selection. In: Proceedings of the IEEE visualization ’95, 1995. p. 118–25. [34] W.H. Tedford S.L. Berquist W.E. Flynn The size-color illusion The Journal of General Psychology 97 1977 145 150 [35] W.S. Cleveland R. McGill A color-caused optical illusion on a statistical graph The American Statistician 37 2 1983 101 105 [36] B.T. Julesz The elements of texture perception, and their interactions Nature 290 12 1981 91 97 [37] M. D’Zmura Color in visual search Vision Research 31 6 1991 951 966 [38] B. Bauer P. Jolicouer W.B. Cowan Visual search for color targets that are or are not linearly-separable from distractors Vision Research 36 1996 1439 1446 [39] B. Bauer P. Jolicouer W.B. Cowan The linear separability effect in color visual search: ruling out the additive color hypothesis Perception and Psychophysics 60 6 1998 1083 1093 [40] Color usage research lab: Using color in information display graphics 〈 〉 , online September 2010. [41] Tominski C, Fuch G, Schumann H. Task-driven color coding, In: Proceedings of the 12th international conference on information visualization (IV08); 2008. p. 373–80 [42] N. Andrienko G. Andrienko Exploratory analysis of spatial and temporal data 2006 Springer Berlin, Germany [43] Weiskopf D. On the role of color in the perception of motion in animated visualizations. In: Proceedings of the IEEE visualization 2004 (VIS 2004); 2004, p. 305–12. [44] Bartram L, Ware C, Calvert T. Moving icons: detection and distraction. In: Proceedings of the international conference on human–computer interaction (INTERACT 2001); 2001. p. 157–65. [45] L. Bartram C. Ware T. Calvert Filtering and integrating visual information with motion Information Visualization 1 1 2002 66 79 [46] G. Meyer D. Greenberg Color defective vision and computer graphics displays IEEE Computer Graphics & Applications 8 5 1988 28 40 [47] Rogowitz B, Kalvin AD. The which blair project: a quick visual method for evaluating perceptual color maps. In: Proceedings of the IEEE visualization 2001 (VIS 2001); 2001. p. 21–6. [48] B. Berlin P. Kay Basic color terms: their universality and evolution 1991 Cambridge University Press [49] B. Saunders J. Brakel The trajectory of color Perceptions on Science 10 3 2002 302 355 [50] T. Regier P. Kay Language, thought and color: Whorf was half right Trends in Cognitive Sciences 13 10 2009 439 446 [51] Kawai M, Uchikawa K, Ujike H. Influence of color category in visual search. In: Proceedings of the annual meeting association for research in vision and ophtalmology; 1995. p. 2991. [52] C.G. Healey J.T. Enns Large datasets at a glance: combining textures and colors in scientific visualization IEEE Transactions on Visualization and Computer Graphics 5 2 1999 145 167 [53] R. Gregory Eye and brain: the psychology of seeing 5th ed 1998 Oxford University Press p. 133–5 [54] J. Chuang D. Weiskopf T. Möller Energy aware color sets Computer Graphics Forum (EUROGRAPHICS 2009) 28 2 2009 203 211 [55] R. Kosara C. Healey V. Interrante D. Laidlaw C. Ware Visualization viewpoints—user studies: why, how, and when? IEEE Computer Graphics & Applications 23 4 2003 20 25 [56] Kindlmann G, Reinhard E, Creem S. Face-based luminance matching for perceptual colormap generation. In: Proceedings of the IEEE visualization 2002 (VIS 2002); 2002. [57] Biederman I, Kalocsai P. 1998. Neural and psychological analysis of object and face recognition. In: Face recognition: from theory to applications. New York: Springer-Verlag, 1998, pp. 3–25 [58] Healey C. Choosing effective colours for data visualization. In: Proceedings of the IEEE visualization ’96; 1996. p. 263–70. [59] Montag ED. The use of color in multidimensional graphical information display. In: Proceedings of the IS&T/SID 7th color imaging conference; 1999. p. 222–6. [60] P. Rheingans B. Tebbs A tool for dynamic explorations of color mappings ACM Computer Graphics 24 2 1990 145 146 [61] Wu Y, Takatsuka M. Three dimensional colour pickers. In: Proceedings of the 2005 Asia–Pacific symposium on information visualization, vol. 45; 2005. p. 107–14. [62] Gresh D. Self-corrected perceptual colormaps 〈 〉 , online March 2010. [63] Shulze-Wollgast P, Tominski C, Schumann H. Enhancing visual exploration by appropriate color coding. In: Proceedings of the international conference in central Europe on graphics, visualization and computer vision (WSCG’05); 2005. p. 203–10. [64] Dougherty B, Wade A. Vischeck 〈 〉 , online March 2010. [65] Dougherty B, Wade A. Daltonize 〈 〉 , online March 2010. [66] Hyun Y. Nonlinear color scales for interactive exploration 〈 〉 , online March 2010. [67] Ventura A, Schettini R. Computer-aided color coding for data display. In: Proceedings of the 11th IAPR international conference on pattern recognition; 1992. p. 29–32. "
    },
    {
        "doc_title": "CardioAnalyser: A software tool for segmentation and analysis of the left ventricle from 4D MDCT images of the heart",
        "doc_scopus_id": "78449288239",
        "doc_doi": "10.1109/IV.2010.91",
        "doc_eid": "2-s2.0-78449288239",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "4D",
            "Left ventricles",
            "MDCT",
            "Segmentation",
            "Software tool"
        ],
        "doc_abstract": "Cardiac angiography using multiple detector row computerized tomography (MDCT) scanners provides 3D data concerning the heart and, in particular, the left ventricle (LV), for different cardiac phases along one cardiac cycle. Exploring this data for LV function analysis is not an easy task, given the amount of data and time involved in segmenting (or revising results provided by automatic segmentation methods) up to 12 cardiac phases. CardioAnalyser, a tool for 4D LV segmentation from MDCT data which provides a protocol to help perform LV segmentation of all cardiac phases available is presented. It uses an automatic segmentation algorithm along with a procedure which guides the user through the process. Its main goal is to reuse as much information as possible from one cardiac phase to the next in order to reduce segmentation time and the amount of user interaction. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Wiimote as an input device in google earth visualization and navigation: A user study comparing two alternatives",
        "doc_scopus_id": "78449283947",
        "doc_doi": "10.1109/IV.2010.72",
        "doc_eid": "2-s2.0-78449283947",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Google earth",
            "Input devices",
            "Usability",
            "User study",
            "Wiimote"
        ],
        "doc_abstract": "This paper presents a user study performed to compare the usability of the Wiimote as an input device to visualize information and navigate in Google Earth using two different configurations. This study had the collaboration of 15 participants which performed a set of tasks using the Wiimote as an input device while the image was projected on a common projection screen, as well as a mouse on a desktop. Results show that most users clearly preferred one of the Wiimote configurations over the other, and over the mouse; moreover, they had better performances using the preferred configuration, and found it easier to use. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A 3D tool for left ventricle segmentation editing",
        "doc_scopus_id": "77955377969",
        "doc_doi": "10.1007/978-3-642-13775-4_9",
        "doc_eid": "2-s2.0-77955377969",
        "doc_date": "2010-08-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3-D image",
            "3D segmentation",
            "Anatomical structures",
            "Application area",
            "Automatic method",
            "CT scanners",
            "Editing tools",
            "Left ventricles",
            "Quantitative measurement",
            "Robust segmentation",
            "Time instances"
        ],
        "doc_abstract": "Image segmentation has a very important role in many application areas, such as medical imaging. Even robust segmentation methods cannot deal with the wide range of variation observed, for example, in shape and orientation of an anatomical structure. Given the need to accomplish accurate segmentations in order to perform quantitative measurements or compare structures in different time instances, it is important to have tools which allow easy segmentation editing/correction by experts. In 3D images (e.g., obtained using CT scanners) performing segmentation editing of regions which span several slices might be a tiresome task if it has to be done slice-by-slice with a 2D tool. This article presents a 3D segmentation editing tool, to be applied to left ventricle segmentations, which enables radiographers to correct segmentations provided by an automatic method. © 2010 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Processing, visualization and analysis of medical images of the heart: An example of fast prototyping using MeVisLab",
        "doc_scopus_id": "70350558556",
        "doc_doi": "10.1109/VIZ.2009.40",
        "doc_eid": "2-s2.0-70350558556",
        "doc_date": "2009-11-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Fast prototyping",
            "Left ventricles",
            "MDCT images",
            "Medical images",
            "MeVisLab",
            "Prototyping platform",
            "Real problems",
            "Semi-automatic segmentation",
            "Visualization and analysis"
        ],
        "doc_abstract": "Developing and testing new algorithms for medical imaging processing can be a tiresome task as it often requires an additional set of tools to visualize the data and results and perform validation steps along the development. The integration among all these tools is also very important as it speeds all the process allowing the developer to concentrate on the real problems. This article briefly presents how MeVisLab is being used as a prototyping platform to develop a semi-automatic segmentation algorithm to extract the left ventricle from 4D MDCT images of the heart and then build a simple framework to perform a preliminary evaluation of the obtained results. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A perceptual data repository for polygonal meshes",
        "doc_scopus_id": "70350543996",
        "doc_doi": "10.1109/VIZ.2009.41",
        "doc_eid": "2-s2.0-70350543996",
        "doc_date": "2009-11-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Data repositories",
            "Experimental methodology",
            "Observer studies",
            "Perceived quality",
            "Polygonal meshes",
            "Preliminary assessment"
        ],
        "doc_abstract": "A repository containing perceived quality data for polygonal meshes, obtained through observer studies, is presented. It includes information regarding the experimental methodology, protocol and models used, with the purpose of allowing researchers to use it, e.g., for a faster preliminary assessment of their perceived quality metrics without the overhead of designing and performing an observer study. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Left ventricle segmentation from heart MDCT",
        "doc_scopus_id": "68749088804",
        "doc_doi": "10.1007/978-3-642-02172-5_40",
        "doc_eid": "2-s2.0-68749088804",
        "doc_date": "2009-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Left ventricles",
            "Qualitative evaluations",
            "Semiautomatic methods"
        ],
        "doc_abstract": "A semi-automatic method for left ventricle segmentation from MDCT exams is presented. It was developed using ITK and custom modules integrated in the MeVisLab platform. A preliminary qualitative evaluation shows that the provided segmentation, without any tweaking or manual edition, is reasonably close to the ideal segmentation as judged by experienced radiology technicians. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Erratum to \"PolyMeCo-An integrated environment for polygonal mesh analysis and comparison\" [Comput. Graphics 33 (2009) 181-191] (DOI:10.1016/j.cag.2008.09.014)",
        "doc_scopus_id": "71849083306",
        "doc_doi": "10.1016/j.cag.2009.10.003",
        "doc_eid": "2-s2.0-71849083306",
        "doc_date": "2009-01-01",
        "doc_type": "Erratum",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2009-11-04 2009-11-04 2010-04-18T22:56:32 S0097-8493(09)00117-4 S0097849309001174 10.1016/j.cag.2009.10.003 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20091001 20091031 2009 2009-11-04T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype volfirst volissue body affil articletitle auth authfirstini authfull authlast pubtype alllist content subj ssids 0097-8493 00978493 33 33 5 5 Volume 33, Issue 5 7 624 624 200910 October 2009 2009-10-01 2009-10-31 2009 Technical Section simple-article err Copyright © 2009 Elsevier Ltd. All rights reserved. ERRATUMPOLYMECOANINTEGRATEDENVIRONMENTFORPOLYGONALMESHANALYSISCOMPARISONCOMPUTGRAPHICS332009181191 SILVA S 10.1016/j.cag.2008.09.014 S0097849308001271 SILVAX2009X624 SILVAX2009X624XS item S0097-8493(09)00117-4 S0097849309001174 10.1016/j.cag.2009.10.003 271576 2010-09-22T11:52:53.696665-04:00 2009-10-01 2009-10-31 true 97165 MAIN 1 20910 849 656 IMAGE-WEB-PDF 1 CAG 1965 S0097-8493(09)00117-4 10.1016/j.cag.2009.10.003 S0097-8493(08)00127-1 10.1016/j.cag.2008.09.014 Elsevier Ltd Erratum Erratum to “PolyMeCo—An integrated environment for polygonal mesh analysis and comparison” [Comput. Graphics 33 (2009) 181–191] Samuel Silva a b ⁎ Joaquim Madeira a b Beatriz Sousa Santos a b a Institute of Electronics and Telematics Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author at: Institute of Electronics and Telematics Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal. Tel.: +351234370500; fax: +351234370545. The publisher regrets that the above paper was incorrectly published under the Education Section. The correct section for this paper is the “Technical Section”. "
    },
    {
        "doc_title": "PolyMeCo-An integrated environment for polygonal mesh analysis and comparison",
        "doc_scopus_id": "63749095751",
        "doc_doi": "10.1016/j.cag.2008.09.014",
        "doc_eid": "2-s2.0-63749095751",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Analysis and explorations",
            "Application area",
            "Comparison result",
            "Integrated environment",
            "Mesh analysis",
            "Mesh comparison",
            "Mesh simplifications",
            "Processing method"
        ],
        "doc_abstract": "Polygonal meshes are used in several application areas to model different objects and structures. Depending on the application, mesh models sometimes have to be processed to, for instance, reduce their complexity (mesh simplification). Such operations introduce differences regarding the original mesh, whose evaluation is of paramount importance when choosing the processing methods to be applied for a particular purpose. Although some mesh analysis and comparison tools are described in the literature, little attention has been given to the way mesh features and mesh comparison results can be visualized. Moreover, particular functionalities have to be made available to enable systematic use and proper data analysis and exploration. PolyMeCo-a tool for polygonal mesh analysis and comparison-was designed and developed taking the above objectives into account. It enhances the way users analyze features and compare meshes by providing an integrated environment where various mesh quality measures and several visualization options are available and can be used in a coordinated way, thus leading to greater insight into the visualized data. © 2008 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2008-10-17 2008-10-17 2010-04-18T22:56:32 S0097-8493(08)00127-1 S0097849308001271 10.1016/j.cag.2008.09.014 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20090401 20090430 2009 2008-10-17T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst pubtype ref alllist content subj ssids 0097-8493 00978493 33 33 2 2 Volume 33, Issue 2 10 181 191 181 191 200904 April 2009 2009-04-01 2009-04-30 2009 Education article fla Copyright © 2008 Elsevier Ltd. All rights reserved. POLYMECOANINTEGRATEDENVIRONMENTFORPOLYGONALMESHANALYSISCOMPARISON SILVA S 1 Introduction 2 Mesh comparison tools 3 PolyMeCo 3.1 Computational measures 3.2 Representations 3.2.1 Colored mesh 3.2.2 Superimposed meshes 3.2.3 Statistical representations 3.3 Visualization modes 3.3.1 Original vs. processed vs. colored mesh 3.3.2 Extended results 3.3.3 Simultaneous viewing of different measures 3.3.4 Features comparison 3.4 Additional features 3.4.1 Location probe 3.4.2 Light source customization 3.4.3 Workspace saving 3.4.4 Data export 4 User evaluation 4.1 Formal evaluation 4.2 Informal evaluation 5 Application example 6 Availability and community contributions 7 Conclusions and future work Acknowledgments References CIGNONI 2005 125 133 P CIGNONI 1998 167 174 P ROY 2004 127 140 M SILVA 2005 842 847 S PROCEEDINGS9THINTERNATIONALCONFERENCEINFORMATIONVISUALIZATIONIV05LONDONUK POLYMECOAPOLYGONALMESHCOMPARISONTOOL SHEWCHUK 2002 115 126 J PROCEEDINGS11THINTERNATIONALMESHINGROUNDTABLE AGOODLINEARELEMENTINTERPOLATIONCONDITIONINGQUALITYMEASURES BHATIA 1990 309 319 R GRAF 2006 178 183 H PROCEEDINGS2006IEEEINTERNATIONALCONFERENCEVIRTUALENVIRONMENTSHUMANCOMPUTERINTERFACESMEASUREMENTSYSTEMS ADAPTIVEQUALITYMESHINGFORONTHEFLYVOLUMETRICMESHMANIPULATIONSWITHINVIRTUALENVIRONMENTS THEISEL 2004 288 297 H PROCEEDINGS12THPACIFICCONFERENCECOMPUTERGRAPHICSAPPLICATIONS NORMALBASEDESTIMATIONCURVATURETENSORFORTRIANGULARMESHES LEE 2005 659 666 C PROCEEDINGSSIGGRAPH MESHSALIENCY KARNI 2000 279 286 Z PROCEEDINGSSIGGRAPH SPECTRALCOMPRESSIONMESHGEOMETRY SORKINE 2003 42 51 O PROCEEDINGSEUROGRAPHICSSYMPOSIUMGEOMETRYPROCESSING HIGHPASSQUANTIZATIONFORMESHENCODING SOUSASANTOS 2005 15 21 B PROCEEDINGSTHIRDINTERNATIONALCONFERENCEMEDICALINFORMATIONVISUALIZATIONBIOMEDICALVISUALIZATIONMEDIVIS05 COMPARISONMETHODSFORSIMPLIFICATIONMESHMODELSLUNGSUSINGQUALITYINDICESOBSERVERSTUDY SILVA 2007 943 950 S PROCEEDINGS11THINTERNATIONALCONFERENCEINFORMATIONVISUALIZATIONIV07ZURICHSWITZERLAND MORECOLORSCALESMEETSEYEAREVIEWUSECOLORINVISUALIZATION ROGOWITZ 1998 52 59 B BORLAND 2007 14 17 D LEVKOWITZ 1992 72 80 H BOTSCH 2006 M EUROGRAPHICS2006TUTORIALSVIENNAAUSTRIA GEOMETRICMODELINGBASEDTRIANGLEMESHES SOUSASANTOS 2007 133 139 B PROCEEDINGSGEOMETRICMODELINGIMAGING2007NEWADVANCESZURICHSWITZERLAND PRELIMINARYEVALUATIONPOLYMECOAVISUALIZATIONBASEDTOOLFORMESHANALYSISCOMPARISON JACKA 2007 177 186 D PROCEEDINGSAFRIGRAPH ACOMPARISONLINEARSKINNINGTECHNIQUESFORCHARACTERANIMATION SILVA 2007 515 523 F INTERNATIONALCONFERENCECOMPUTATIONALSCIENCEAPPLICATIONSICCSA2007 NSAALGORITHMGEOMETRICALVSVISUALQUALITY PETROU 1999 M IMAGEPROCESSINGFUNDAMENTALS HORN 1984 1671 1686 B SUN 1997 164 168 C BUSTOS 2006 39 54 B REUTER 2006 342 366 M PAULY 2001 379 386 M PROCEEDINGSSIGGRAPH SPECTRALPROCESSINGPOINTSAMPLEDGEOMETRY BERGMAN 1995 118 125 L PROCEEDINGSIEEEVISUALIZATION ARULEBASEDTOOLFORASSISTINGCOLORMAPSELECTION SILVAX2009X181 SILVAX2009X181X191 SILVAX2009X181XS SILVAX2009X181X191XS item S0097-8493(08)00127-1 S0097849308001271 10.1016/j.cag.2008.09.014 271576 2010-09-22T11:48:58.726395-04:00 2009-04-01 2009-04-30 true 2994061 MAIN 11 54303 849 656 IMAGE-WEB-PDF 1 si9 106 6 8 si8 106 6 8 si7 106 6 8 si6 106 6 8 si5 106 6 8 si4 106 6 8 si3 106 6 8 si20 110 12 6 si2 173 11 35 si19 108 12 5 si18 106 6 8 si17 106 6 8 si16 106 6 8 si15 106 6 8 si14 106 6 8 si13 106 6 8 si12 106 6 8 si11 106 6 8 si10 106 6 8 si1 136 8 14 gr1 167537 628 809 gr1 6272 93 120 gr1 1328213 2782 3583 gr14 97839 456 621 gr14 5699 92 125 gr14 661505 2020 2750 gr15 122614 450 621 gr15 7650 91 125 gr15 886076 1992 2750 gr2 19377 101 809 gr2 1302 16 125 gr2 121891 447 3583 gr4 46079 251 226 gr4 5319 93 84 gr4 506954 1111 1000 gr8 47836 196 621 gr8 3167 39 125 gr8 312954 867 2750 gr10 41641 291 391 gr10 14096 163 219 gr10 294577 1288 1733 gr11 18219 134 391 gr11 5569 75 219 gr11 111085 594 1733 gr12 43854 200 809 gr12 5902 54 219 gr12 319445 886 3583 gr13 24863 368 188 gr13 6182 162 83 gr13 148470 1630 833 gr3 89607 258 809 gr3 8952 70 219 gr3 750558 1141 3583 gr5 50097 471 391 gr5 8192 164 136 gr5 371957 2086 1733 gr6 41070 349 391 gr6 11438 163 183 gr6 290957 1547 1733 gr7 64939 406 295 gr7 10330 164 119 gr7 509248 1800 1308 gr9 137937 640 809 gr9 11807 164 207 gr9 1267207 2835 3583 CAG 1889 S0097-8493(08)00127-1 10.1016/j.cag.2008.09.014 Elsevier Ltd Fig. 1 User interface of PolyMeCo: the distribution of a computed measure (mean curvature) is represented using a colored mesh. Fig. 2 PolyMeCo's pipeline. Fig. 3 Computed results are presented using a colored mesh: (a) normal deviation using a rainbow color scale and (b) a blue to cyan color scale; (c) triangle quality using a rainbow color scale and flat shading to color the faces according to the minimum angle. Fig. 4 Principal curvature direction ( κ 2 ) computed at each vertex. Fig. 5 Mesh superposition. Top: original and processed meshes; bottom: mesh superposition using solid rendering depicting a region where the original mesh overlaps the processed one, and mesh superposition with transparency applied to the original mesh, allowing volume comparison. Fig. 6 High saturation value defined to better visualize the representation of curvature over the mesh. Top, original colored mesh and colored mesh with high saturation value definition; bottom, the resulting histogram. Fig. 7 Top, the same mesh colored using three color scales depicting the same results obtained for the geometric distance. Bottom, same mesh but now with the color mapping modified in order to highlight the surface regions with larger distance to the reference mesh. Fig. 8 Comparison using boxplots, as well as numerical values, of the data obtained by computing the same measure for different meshes. Fig. 9 Some visualization modes available in PolyMeCo: (a) original vs. processed vs. colored mesh; (b) extended results; (c) feature comparison. Fig. 10 Comparison using colored meshes of the results obtained by computing the same measure for different meshes: (a) each mesh colored using an individual color map; (b) all meshes colored using a common color map allowing direct comparison among them. Fig. 11 Comparison using histograms of the data obtained with the same measure for different meshes: (a) the histograms are drawn using individual ranges; (b) all histograms are drawn using a common value range allowing direct comparison among them. Fig. 12 A vertex was selected with the Location Probe. Mesh detail showing that vertex along with its direct-neighborhood (a), and the three tabs on the Location Probe showing information about geometry (b), topology (c) and computed measure associated with it (d). Fig. 13 Window where users can customize light source properties. Fig. 14 Geometric distance mapped on the original model for six simplified versions of a model, obtained with NSA and QSlim, depicted using a common color map. Fig. 15 Triangle quality for the same six models presented in Fig. 14, mapped on the simplified models and depicted using colored models and histograms. Equilateral triangles are presented in blue and needle/flat triangles in red. Table 1 Users opinion on general and specific aspects of PolyMeCo (in a scale of 1-complete disagreement to 4-complete agreement) Features Median Is easy to learn 4 Organization is understandable 3 Response time is reasonable 4 Information layout is adequate 3 Terminology is consistent 3 Text is easy to read 4 Messages are clear 3 Feedback is adequate 3 Education PolyMeCo—An integrated environment for polygonal mesh analysis and comparison Samuel Silva a b ⁎ Joaquim Madeira a b Beatriz Sousa Santos a b a Institute of Electronics and Telematics Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351234370500; fax: +351234370545. Polygonal meshes are used in several application areas to model different objects and structures. Depending on the application, mesh models sometimes have to be processed to, for instance, reduce their complexity (mesh simplification). Such operations introduce differences regarding the original mesh, whose evaluation is of paramount importance when choosing the processing methods to be applied for a particular purpose. Although some mesh analysis and comparison tools are described in the literature, little attention has been given to the way mesh features and mesh comparison results can be visualized. Moreover, particular functionalities have to be made available to enable systematic use and proper data analysis and exploration. PolyMeCo—a tool for polygonal mesh analysis and comparison—was designed and developed taking the above objectives into account. It enhances the way users analyze features and compare meshes by providing an integrated environment where various mesh quality measures and several visualization options are available and can be used in a coordinated way, thus leading to greater insight into the visualized data. Keywords Mesh analysis Mesh comparison Data visualization 1 Introduction Models defined using polygonal meshes are commonly used in many application scenarios ranging from entertainment to medical imaging, as well as in numerous scientific visualization and engineering applications. With data complexity growing at a fast pace, it is often necessary to process an original mesh model (e.g., created from data obtained with a range scanner) and reduce the number of its faces, thus requiring less storage space and allowing easier manipulation and faster rendering. Other processing operations are usually carried out, for example, to smooth a mesh surface [1] or enhance particular mesh features [2]. Applying such operations results in meshes that are different from their originals: depending on the application it might be important to assess those differences and verify if they are acceptable. Another important issue is the analysis of particular mesh properties, as surface curvature or the quality of the elements (faces) that compose a mesh. For the purpose of mesh analysis and comparison, several measures have been proposed and there are a few tools presented in the literature [3–6] which allow comparing polygonal meshes; they will be briefly described in the following section. After a careful review of those mesh comparison tools and their usual applications, it was clear that they lacked some features deemed important. They provide numerical descriptors, along with colored meshes which depict the data obtained using some computational measures. Nevertheless, it becomes necessary to have a tool which provides an enhanced environment for systematic mesh analysis and comparison, where several meshes can be loaded during the same work session and data compared using a wide range of measures and proper visualization methods. Developers and general users can then easily analyze the results. We describe a polygonal mesh analysis and comparison tool called PolyMeCo 1 1 A version for test purposes is available at (Fig. 1 ) which offers the above mentioned features; a first prototype was presented in Ref. [7]. The main goal of the work carried out is to integrate enhanced functionalities for interactive visualization and analysis of mesh features and comparison data, in the scope of systematic mesh analysis and comparison. In the following section a short overview of mesh comparison tools that have been proposed in the literature is done. Afterwards, the architecture of PolyMeCo and its main functionalities are presented. Then, the results of some preliminary usability evaluation tests are presented and some research work in which PolyMeCo was used is briefly described (as an application example). Finally, some conclusions and ideas for future work are mentioned. 2 Mesh comparison tools Mesh comparison is usually carried out with the help of dedicated software tools providing the user with numerical data (e.g., minimum, mean and maximum difference values) and visual information (e.g., coloring a mesh according to the difference values measured at each vertex), and allowing the user to choose among several computational measures. A few such tools, which might also allow mesh feature evaluation (e.g., surface curvature), are described in the literature. Metro, developed by Cignoni et al. [3], allows both numerical and visual mesh comparisons. Among the numerical values furnished is data concerning the input mesh characteristics (number of vertices and faces, surface area, mesh volume, etc.), the minimum and maximum geometric distances between two given meshes and their difference in volume. It is also possible to view a mesh colored according to the results obtained. Also provided is the computation of the Hausdorff distance. The tool described by Zhou et al. [4] introduces some additional measures (namely, to compute surface curvature) and care was taken to offer several visualization techniques, including side-by-side viewing of the compared meshes, box-glyphs and animations. MESH, developed by Aspert et al. [5], uses the Hausdorff distance to measure the difference between two mesh models. It provides several numerical values, namely the main characteristics of the input meshes and the minimum, mean, maximum and RMS values of the computed differences. It is also possible to view a mesh colored according to such differences. Finally, Roy et al. [6] described a tool called MeshDev which allows the computation of geometric, normal and other attribute differences such as color or texture. Similarly to the other tools, it provides several numerical values characterizing the input meshes and the obtained results. It is also possible to view meshes colored according to such results. 3 PolyMeCo PolyMeCo is a tool developed using C ++ , OpenGL, OpenMesh [8] and the Fox Toolkit [9], which provides an integrated environment where it is possible to analyze and compare several polygonal meshes at once, and view the data obtained for different computational measures using various representation and visualization options. The process of analyzing and comparing meshes in PolyMeCo can be described by the pipeline depicted in Fig. 2 : 1. Meshes are loaded. 2. For some of those meshes, selected properties or difference measures are computed. 3. The obtained data are mapped to suitable representations. 4. The chosen representations are rendered. The user may change parameters along the pipeline in order to, for example, choose another mesh property or difference measure, or change the representation. 3.1 Computational measures PolyMeCo provides several computational measures which support mesh analysis and comparison. Two types of measures are provided: intrinsic properties and difference measures. Intrinsic 2 2 Here used in the sense of a property which is directly computed from mesh data, irrespective of any other reference mesh; not in the differential geometry sense as in “intrinsic curvature”. properties allow measuring a particular property of a mesh. PolyMeCo provides the following: • Smoothness: for each vertex, the distance to the centroid of its direct neighbors (one-ring vertices). • Triangle quality: the minimum angle for each mesh face, which gives an idea of how close it is to the equilateral; as well as the measures proposed by Shewchuk [10], Bhatia [11] and Graf et al. [12]. • Mean, Gaussian and principal curvatures: using the methods proposed by Theisel et al. [13]. • Saliency: using the method proposed by Lee et al. [14] which identifies mesh areas judged to be of great interest, i.e., presenting a larger number of features. Difference measures allow the comparison of properties between meshes. PolyMeCo provides the following: • Geometric distance and normal deviation: using the methods proposed by Roy et al. [6]. • Mean and Gaussian curvature deviations: using the methods proposed by Theisel et al. [13] for curvature computation and the method proposed by Roy et al. [6] for surface sampling. • Visual differences: using the method proposed by Karni et al. [15] and its enhancements proposed by Sorkine et al. [16]. • Mixed measure: based on the results presented in Sousa Santos et al. [17], mixing geometric distance and normal deviation using a blending factor which depends on the simplification ratio. Notice that, although this might be unusual, PolyMeCo does not impose any constraints on the comparison between meshes representing distinct objects. 3.2 Representations The easiest way for providing the user with information about the computed measures is by presenting some numerical values that characterize them (e.g., minimum, mean and maximum values) and/or the meshes (e.g., number of vertices and faces, bounding box diagonal and surface area) for which they were computed. Another simple approach is that of depicting a mesh and allowing the user to visually inspect it using different projections, views or rendering options (e.g., flat shading, wireframe, etc.). Apart from these, PolyMeCo provides the set of representations described next. 3.2.1 Colored mesh Through a colored mesh it is possible to show the user the distribution of the data obtained with a particular measure: each vertex/face is colored according to the corresponding value. This coloring can be done by mapping the values range to a particular color scale. A usable color scale must respect a set of principles and be properly chosen in order to enhance the insight it provides on the data it refers to [18]. It is also important to understand its disadvantages. For instance, the commonly used rainbow color scale is not the proper choice in many application scenarios according to Rogowitz et al. [19] and, more recently, Borland et al. [20]. As a first approach towards a proper use of color in PolyMeCo, several color scales described in the literature are provided, namely the rainbow, greyscale, linearized greyscale, blue-to-cyan, blue-to-yellow, linearized optimal [21] and heated-object color scales. Currently, the user is allowed to choose one color scale from this set: this is still a naive approach to a correct use of color as it fully depends on user judgment, nevertheless it provides some alternatives which can be explored. Fig. 3 (a) and (b) present the normal deviation values distribution using rainbow and blue to cyan color scales. Fig. 3(c) shows part of a mesh colored according to the computed triangle quality. For some computational measures (such as surface curvature) it may be important to display not only absolute values but also directions. This is performed by drawing a small line segment centered on the corresponding vertex and having the appropriate direction, which can be enabled or disabled at any time. Fig. 4 shows the directions of the second principal curvature superimposed on a colored mesh. 3.2.2 Superimposed meshes It is possible to visualize an original reference mesh superimposed on one of its processed versions. A first choice is to render both in a “solid” way, using different colors. This allows identifying areas where, for example, the processed mesh overlaps the original. An alternative is to render the original mesh with some degree of transparency, in order to let the user perceive any differences in volume between the two. Fig. 5 shows these two alternatives. In addition, it is also possible to visualize both meshes rendered in wireframe. 3.2.3 Statistical representations Although mesh coloring gives a good idea of the distribution of the values obtained using a computational measure, showing where they occur, a global idea of this distribution can be difficult to obtain, due to the impossibility of viewing the whole mesh simultaneously. The provided statistical representations may help to better understand and compare distributions of computed measures: • Histograms—Histograms provide complementary information: the user can, at a glance, get an idea of the global characteristics of the computed data distribution. A problem that may occur when assigning a range of values to a color scale is that of range stretching: if outliers are present in the data, they may stretch the range too much and not allow perceptible differences between close values. In order to tackle this problem, the histogram widget provides a way of choosing the range of values that will be represented by the chosen color scale. By using a slider, the user can choose a high saturation value, i.e., all values above it will be represented using the color to which the maximum value is mapped. A similar slider is available for defining a low saturation value, all values below it being represented by the color to which the minimum is mapped. Thus, the first class of the histogram will correspond to the low saturated values and the last class to the high saturated values. Fig. 6 illustrates this procedure showing a mesh colored according to its mean curvature. Due to the presence of outliers in the data the curvature representation is not perceptible. By defining a high saturation value using the slider provided on the histogram widget, it is possible to better perceive the curvature distribution. Note that the histogram bars are also colored with the chosen color mapping. This feature can also be used to highlight values in a colored mesh. Fig. 7 (top) shows the same mesh colored using different color scales, depicting the geometric distance to a reference mesh. Bottom, the same results are presented but the color mapping has been modified by adjusting the high saturation value. Notice how surface regions with larger associated geometric distance are easily identified. • Boxplots—Boxplots are very useful to compare data sets, since they allow comparing and analyzing the symmetries and ranges of the data, and detect the presence of outliers (Fig. 8 ); thus, they can help set the saturation values on the histogram widget. 3.3 Visualization modes Visualization modes use the available representations in order to provide ways of better analyzing and comparing mesh properties. The main visualization modes provided by PolyMeCo are presented next. 3.3.1 Original vs. processed vs. colored mesh In this visualization mode an original mesh, a processed mesh and a mesh colored according to the selected measure are presented. This allows the visual comparison of the two meshes, for example, to quickly identify areas where computed values are higher/lower, and then perform a more detailed analysis. It is possible to manipulate any of the meshes and change its position, orientation and scale factor. This manipulation is applied to all displayed meshes. Fig. 9 (a) shows this visualization mode. 3.3.2 Extended results This visualization mode allows the simultaneous presentation of a colored mesh, a histogram and a boxplot depicting the data obtained using the selected measure. It is possible to define high and low saturation values, using the sliders below the histogram, as previously described, in order to enhance the way results are presented using the colored mesh. Fig. 9(b) shows this visualization mode being used to analyze the normal deviation between two meshes. 3.3.3 Simultaneous viewing of different measures This mode allows the user to visualize, for one mesh, the data distributions obtained with different measures. This mode can help the user to have a clearer idea of the obtained data, and provides the possibility of comparing between them and finding, for example, similarities in their behavior. The presented meshes remain synchronized, i.e., with similar position, orientation and scale factor. 3.3.4 Features comparison This mode (Fig. 9(c)) allows the visualization of data distributions obtained with the same computational measure for several meshes. This can be useful to study different processing algorithms and compare the obtained results. Once again, the presented meshes remain synchronized. When using this visualization mode one must not forget important details. Each colored mesh uses the full range of the color scale to represent the data, i.e., for each colored mesh the “last” color of the color scale is used to represent the maximum value obtained for that particular mesh. This kind of coloring can still be useful to compare value incidence in each mesh, but it is not possible to directly compare distributions among all colored meshes. In order to allow this kind of comparison (not possible with any of the tools described earlier), PolyMeCo provides the option of using a common color mapping for all meshes. The last color of the color scale is now used to represent the maximum obtained value among all the compared meshes. The same happens with the histograms. The minimum is always considered to be zero as all representations are unsigned. Fig. 10 (a) allows comparing results using an individual color map for each mesh. Notice that it is not clear which is the one that has higher associated values. In Fig. 10(b) a common color map is used to color all the meshes, thus allowing for an easier and more accurate comparison. Side-by-side comparison can also be done using histograms: again, direct comparison among them can only be correctly performed when using a common value range. Fig. 11 shows the histograms drawn using their individual ranges, and then using a common value range. Comparison can also be done by using boxplots as shown in Fig. 8. With boxplots it is easy to compare the ranges and symmetry among all the results. 3.4 Additional features Besides different representations and visualization modes PolyMeCo provides a set of additional features in order to help inspecting meshes and results more efficiently, and allow gathering the resulting data for further analysis and presentation. These features are presented next. 3.4.1 Location probe A probe tool allows the user to obtain information regarding a particular mesh vertex. The provided information concerns geometry (vertex coordinates), topology (vertex neighborhoods and their properties) and associated data values, such as those obtained using a particular measure. Fig. 12 shows some details of the Location Probe window and its usage. When a mesh vertex is selected, a green sphere appears at its location along with a highlight of its neighbors. The user can define the number of visible neighborhood rings on the Topology tab and read information about each neighborhood (one-ring, two-ring, etc.), such as the number of vertices it includes. The information concerning vertex coordinates can be found in the Geometry tab and the value computed for it, using one of the measures, is presented in the Results tab. 3.4.2 Light source customization Scene illumination can play an important role in the analysis of a mesh surface and has been used in applications, such as the automotive industry, to inspect the quality of polygonal meshes [22]. As a first step, PolyMeCo provides the user with several functionalities to view and change the light source properties of the scene. It is possible to increase the number of light sources (up to three) and control their position as well as the ambient, diffuse and specular components. Light source orientation is always towards the origin, as can be seen in the representation appearing on the top of the window presented in Fig. 13 where cones represent the light sources and the sphere represents the illuminated object. The chosen light source properties are applied to the sphere thus allowing the user to have a better perception of the illumination results. Due to performance issues, specially when working with complex meshes, light source changes are only applied to the visualized mesh when the user presses the Apply Settings button. The light source properties set by the user are only applied when viewing original or processed meshes. In order not to interfere with the representations which use color (light source color could change perceived mesh surface colors), the light source properties for those scenes are pre-defined and cannot be changed. A desirable improvement would be to enhance this feature in order to allow surface examination using isophotes [22], in which case, the light sources ought to be modeled differently to obtain the desired effects on the surface. 3.4.3 Workspace saving A work session in PolyMeCo can involve a considerable number of meshes and the computation of many measures which, for complex meshes, can take a long time. Thus, it is important to store the contents of the workspace for future analysis or to be able to resume the work at a later time: PolyMeCo allows complete workspace loading and saving. A summary of the data contained in the workspace is present in the main workspace file which is written in XML, and it is possible to use XSLT style sheets to view it in, for example, a web browser. This allows viewing the main results on any computer, even without PolyMeCo installed. 3.4.4 Data export It may be necessary to compare the values obtained for each vertex/face using a particular measure with theoretical values (e.g., curvature) or to entail a more elaborate statistical analysis. While performing systematic analysis and comparison of mesh processing algorithms a common task is that of building tables with the data obtained with the computed measures. Yet, the usage of a large number of meshes and measures turns this into a tiresome task. To cope with this PolyMeCo provides data export. When exporting data from a particular measure the values computed for each vertex/face are written to a file; when exporting the data obtained by computing several measures for a single mesh a table is stored containing the minimum, mean, maximum and variance values for each measure; finally, when exporting the data concerning several measures computed for several meshes, a table is stored containing the mean values for each measure and for each mesh. All these files can then be opened, for example, in a spreadsheet tool. 4 User evaluation User evaluation is of paramount importance when developing an interactive tool. A formative evaluation intended to inform the development of a new version of PolyMeCo has been carried out and allowed the detection of some problems, providing clues to improve some features and gathering new ideas for future versions. Until now two types of evaluation have been performed: formal and informal evaluation. A brief description of the used methods and results is presented next. For a more detailed explanation see Sousa Santos et al. [23]. 4.1 Formal evaluation Heuristic evaluation was performed providing a list of usability problems, severity ratings and clues to their solution. Observation sessions were performed with the participation of 35 users who had to complete a list of tasks including, for example: • Load a given mesh; • Set the scene illumination parameters; • Inspect the mesh and locate a particular detail; • Obtain a colored mesh using a given measure; • Select a given color scale; • Find the value of the measure at a given vertex; • Obtain a new color mapping for the same color scale; • Compare results for meshes obtained with different processing methods using a given measure. In general all users managed to finish the tasks in a very short time (a few seconds). Only two of the tasks took longer or were not completed. They were related to the Location Probe: when trying to obtain information regarding a particular mesh vertex, the users had some difficulty in selecting it. This result clearly reveals a problem which needs further investigation. A possible solution can be that of activating the wireframe view. After completing the tasks, users gave their opinion concerning some tool features through a questionnaire. Table 1 shows the median values for each item expressed in a scale from 1 to 4. These values convey a positive opinion of PolyMeCo, even though it was the first time all users worked with such a tool. 4.2 Informal evaluation A less structured approach was used for informal evaluation performed with 20 users. In an initial 2h session the goals and main features of PolyMeCo were presented. Then, a period of questions and answers was allowed so that participants could better understand the overall purpose of the tool and the intended evaluation. After this, they installed the prototype on their computers and were given some time to use it on their own. Users were also challenged to think how they would use the tool if they had to conduct research concerning mesh processing methods. Then they were asked for suggestions on how to improve the usability of the tool and on which new useful functionalities should be included. A week later, in another session, the users were asked to further comment on the tool and to answer the same questionnaire about PolyMeCo and its features. During the evaluation sessions PolyMeCo was used in a more committed manner than we had previously anticipated, and users gave plenty of suggestions concerning interesting new functionality to include or ways to improve the usability of the tool. The results concerning their reaction and opinion about PolyMeCo, collected through the questionnaire, were very similar to the results obtained from the formal evaluation participants, conveying a positive opinion, as well. 5 Application example A task which can be performed using PolyMeCo is the comparison among different mesh simplification methods in order to identify the one which provides meshes of greater quality. As described by Silva et al. [24], PolyMeCo was used to compare between two simplification methods: QSlim and NSA. The analysis started with a visual inspection of the meshes using different light source configurations which allowed the detection of some surface artifacts. Then, several measures were computed and the results analyzed using different visualization modes, in particular the Feature Comparison visualization mode which allowed an easy comparison among results using colored meshes. For the mesh models used, QSlim obtained the smallest geometric distance for all simplified meshes but mesh quality, as expressed by the triangle quality measures (minimum angle and those proposed by Shewchuk [10], Bhatia [11] and Graf et al.[12]), was, in general, favorable to the meshes obtained using NSA. This shows the importance of analyzing mesh quality with more than one computational measure and how important it is to have systematic ways of visualizing meshes and the distribution of measure values. Fig. 14 shows the geometric distance for six models created using NSA and QSlim and three simplification levels. By using a common color map it is possible to directly compare the models and verify which ones have the largest distances to the original (NSA's). Although a widely used measure, the geometric distance did not show evidence of the problems which can be observed in Fig. 15 : the triangle quality shows that the models created with QSlim have triangles with lower quality, evidence that something may be wrong, and a visual inspection reveals strange triangulations covering the center hole. Although clearly needing further study, this analysis seems to show that QSlim might not be the proper choice for some kinds of mesh models due to the resulting artifacts and poor element (face) quality. The main contribution of PolyMeCo was clearly the way its integrated environment allowed easy computation of measures for several meshes at once, while providing interactive exploration of the obtained data using the available visualization modes. In any other tool described in the literature for mesh analysis and comparison, such comparison would have been more difficult. One must not forget that in those tools only two meshes are compared at a time and there is no possibility of comparing among the results obtained for a set of meshes, except for the numerical values, or a third party application is used to visualize the colored meshes with the already mentioned problem of independent color mappings. Having all the computational measures, meshes, and visualization options in one tool motivated data exploration, which we believe might promote a more complete and accurate analysis fostering further insight. For a more detailed description of this case study and a more complete analysis of the results see Silva et al. [24]. 6 Availability and community contributions The current version of PolyMeCo is freely available on the web at as well as some support documentation and a discussion forum. It is our purpose to open PolyMeCo to external contributions which, we believe, will result in an improved and more useful tool. PolyMeCo's main features (visualization modes, information windows, computational measures and representations) are developed as plugins. This allows users to develop and integrate their own plugins in PolyMeCo, e.g., to add a new metric. A “Plugin Kit” is freely available which provides a workspace for plugin development along with some examples of each kind of plugin supported. Also, if developers intend to improve existing functionalities or make changes in the core code of PolyMeCo, the source code can be requested. 7 Conclusions and future work PolyMeCo has several innovative features that distinguish it from similar tools presented in the literature: 1. Integrated environment where several meshes can be analyzed/compared simultaneously. 2. Wider range of computational measures. 3. Customization of light source properties. 4. Several data representations and visualization modes to provide alternative visualizations of the data. 5. Feature comparison using common color mapping. 6. Location probe to obtain specific information about the computed data. 7. Workspace saving and data export. The integrated environment allows working with several meshes simultaneously, thus speeding the analysis and comparison process. After loading all the meshes, the user can easily compute several measures and analyze the obtained data. An important feature is that no third-party application is needed to visualize the data, as happens with other tools, such as MeshDev [6]. While working with representations using color (several alternative color scales are available), the user can adjust the color mapping when in the presence of data containing outliers (in order to eliminate them from the color mapping) or to highlight specific values. Among the available visualization modes the Feature Comparison mode is of paramount importance, as it allows comparing meshes using color representations (or histograms). This is performed by allowing the definition of a common color mapping among all meshes. Without this innovative feature colored meshes can only be used to compare measures incidence among meshes, but never their relative values, as is performed in [4]. By providing a location probe, PolyMeCo allows the user to obtain information regarding vertices, namely their coordinates, neighborhood and associated values according to a measure, which is a feature not available in any of the tools described in the literature. Performing mesh analysis and comparison may require storing the computed data for future reference or to continue the analysis later. As some measures take quite some time to compute, it would be a tedious task to compute them whenever they were necessary. For this reason, PolyMeCo allows saving the current workspace (i.e., all loaded meshes and measures) and restore it later. It is also possible to export the data obtained to allow, for example, further analysis using a different tool or to build tables for data comparison and presentation. The presented application example, as well as research work carried out by other authors (see Jacka et al. [25], Silva et al. [26] and Distler [27]), show that PolyMeCo is, indeed, a helpful tool in its field. Users have expressed their appreciation towards PolyMeCo, stating that one of its advantages is the fact that it allows experimenting with the different measures and visualization modes in a very easy way. By having a plugin based architecture, PolyMeCo also provides a clean and easy way of extending its features. We believe that all the characteristics described above establish PolyMeCo as a tool for systematic mesh analysis and comparison, not only for mesh processing algorithm developers but also for those who perform mesh processing for particular purposes. Still, it can be enhanced in many ways. The Location Probe can be improved in order to provide information regarding all the loaded meshes (not only those depicting analysis/comparison data). New computational measures may be added. All the mesh comparisons performed in PolyMeCo consider that the compared meshes have their principal axes aligned and that all differences are due to surface distortion. Therefore, a mesh compared with a version of itself rotated/translated in 3D will result in significant differences, e.g., large geometric distances when the difference is only due to affine transformations. These differences should be reported to the user, but stating their nature. To deal with this, it might be a good idea to use methods such as principal component analysis [28] and the extended Gaussian image [29,30]. It would also be interesting to test if 3D shape descriptors [31,32], usually used in retrieving meshes from databases, can be used to compare polygonal meshes. Another interesting feature to add might be that of spectral analysis of mesh surfaces [33,34], in a similar approach to that provided by Fourier analysis regarding, for example, audio signals. It would be desirable that PolyMeCo somehow suggested the more appropriate color scale for each particular case based, among others, on the spectral features of the data and on the type of task the user wants to accomplish [35]. This would be an important step forward towards a more complete application of the principles described in Silva et al. [18]. Acknowledgments The authors would like to thank Michaël Roy for providing the source code for MeshDev and all the reviewers for their valuable comments and suggestions. The first author would like to thank research unit 127/94 IEETA of the University of Aveiro for the grant that allowed his work. References [1] Taubin G. Geometric signal processing on polygonal meshes. In: Eurographics 2000—state-of-the-art report, Interlaken, Switzerland, 2000. [2] P. Cignoni R. Scopigno M. Tarini A simple normal enhancement technique for interactive non-photorealistic renderings Computers & Graphics 29 1 2005 125 133 [3] P. Cignoni C. Rocchini R. Scopigno Metro: measuring error on simplified surfaces Computer Graphics Forum 17 2 1998 167 174 [4] Zhou L, Pang A. Metrics and visualization tools for surface mesh comparison. In: Proceedings of the SPIE 2001, visual data exploration and analysis VIII, vol. 4302, 2001. p. 99–110. [5] Aspert N, Santa-Cruz D, Ebrahimi T. MESH: measuring errors between surfaces using the Hausdorff distance. In: Proceedings of the IEEE international conference on multimedia and expo 2002, Lausanne, Switzerland, vol. 1, 2002. p. 705–8. [6] M. Roy S. Foufou F. Truchetet Mesh comparison using attribute deviation metric International Journal of Image and Graphics 4 1 2004 127 140 [7] S. Silva J. Madeira B. Sousa Santos PolyMeCo—a polygonal mesh comparison tool Proceedings of the 9th international conference on information visualization (IV05), London, UK 2005 IEEE Computer Society 842 847 [8] Botsch M, Steinberg S, Bischoff S, Kobbelt L. OpenMesh—a generic and efficient polygon mesh data structure. In: First OpenSG symposium, Darmstadt, Germany, 2002. [9] Fox toolkit, in 〈 〉 (online October 2008). [10] J.R. Shewchuk What is a good linear element? Interpolation, conditioning, and quality measures Proceedings of the 11th international meshing roundtable 2002 115 126 [11] R.P. Bhatia K.L. Lawrence Two-dimensional finite element mesh generation based on stripwise automatic triangulation Computers and Structures 36 1990 309 319 [12] H. Graf S.P. Serna A. Stork Adaptive quality meshing for on-the-fly volumetric mesh manipulations within virtual environments Proceedings of the 2006 IEEE international conference virtual environments, human–computer interfaces and measurement systems 2006 178 183 [13] H. Theisel C. Rössl R. Zayer H.-P. Seidel Normal based estimation of the curvature tensor for triangular meshes Proceedings of the 12th Pacific conference on computer graphics and applications 2004 288 297 [14] C. Lee A. Varshney D. Jacobs Mesh saliency Proceedings of the SIGGRAPH 2005 659 666 [15] Z. Karni C. Gotsman Spectral compression of mesh geometry Proceedings of the SIGGRAPH 2000 279 286 [16] O. Sorkine D. Cohen-Or S. Toledo High-pass quantization for mesh encoding Proceedings of the Eurographics symposium on geometry processing 2003 42 51 [17] B. Sousa Santos S. Silva C. Ferreira J. Madeira Comparison of methods for the simplification of mesh models of the lungs using quality indices and an observer study Proceedings of the third international conference on medical information visualization—biomedical visualization (MediVis05) 2005 15 21 [18] S. Silva J. Madeira B. Sousa Santos There is more to color scales than meets the eye: a review on the use of color in visualization Proceedings of the 11th international conference on information visualization (IV07), Zurich, Switzerland 2007 IEEE Computer Society 943 950 [19] B. Rogowitz L. Treinish Data visualization: the end of the rainbow IEEE Spectrum 35 12 1998 52 59 [20] D. Borland R. Taylor Rainbow color map (still) considered harmful IEEE Computer Graphics & Applications 27 2 2007 14 17 [21] H. Levkowitz G. Herman Color scales for image data IEEE Computer Graphics & Applications 12 1 1992 72 80 [22] M. Botsch M. Pauly C. Rössl S. Bischoff L. Kobbelt Geometric modeling based on triangle meshes Eurographics 2006—tutorials, Vienna, Austria 2006 [23] B. Sousa Santos S. Silva L. Teixeira C. Ferreira P. Dias J. Madeira Preliminary evaluation of PolyMeCo: a visualization based tool for mesh analysis and comparison Proceedings of geometric modeling and imaging 2007—new advances, Zurich, Switzerland 2007 IEEE Computer Society 133 139 [24] Silva S, Silva F, Madeira J, Sousa Santos B. Evaluation of mesh simplification algorithms using PolyMeCo: a case study. In: Proceedings of the SPIE. Visualization and data analysis 2007, San José, California, USA, vol. 6495, p. 54950D, 2007. [25] D. Jacka A. Reid B. Merry J. Gain A comparison of linear skinning techniques for character animation Proceedings of the AFRIGRAPH 2007 177 186 [26] F. Silva NSA algorithm: geometrical vs. visual quality International conference computational science and its applications ICCSA 2007 2007 515 523 [27] Distler P. Beschreibung und Bewertung von Algorithmen für die Tesselierung von CAD-Modellen. Diplom-Arbeit, Fachbereich Maschinenbau, TU Darmstadt, Germany, 2007. [28] M. Petrou P. Bosdogianni Image processing: the fundamentals 1999 Wiley New York [29] B.K.P. Horn Extended Gaussian images Proceedings of IEEE 72 12 1984 1671 1686 [30] C. Sun J. Sherrah 3D symmetry detection using the extended Gaussian image IEEE Transactions on Pattern Analysis and Machine Intelligence 19 2 1997 164 168 [31] B. Bustos D. Keim D. Saupe T. Schreck D. Vranic An experimental effectiveness comparison of methods for 3D similarity search International Journal on Digital Libraries 6 1 2006 39 54 [32] M. Reuter F.-E. Wolter N. Peinecke Laplace–Beltrami spectra as shape-DNA of surfaces and solids Computer-Aided Design 38 2006 342 366 [33] M. Pauly M. Gross Spectral processing of point-sampled geometry Proceedings of the SIGGRAPH 2001 379 386 [34] Sorkine O. Laplacian mesh processing. In: Eurographics 2005—state-of-the-art report, Dublin, Ireland, 2005. [35] L. Bergman B. Rogowitz L. Treinish A rule-based tool for assisting color map selection Proceedings of IEEE visualization 1995 118 125 "
    },
    {
        "doc_title": "Head-mounted display versus desktop for 3D navigation in virtual reality: A user study",
        "doc_scopus_id": "58049208308",
        "doc_doi": "10.1007/s11042-008-0223-2",
        "doc_eid": "2-s2.0-58049208308",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "3d navigations",
            "Application areas",
            "Design and developments",
            "Game scenarios",
            "Head mounted display (HMD)",
            "Navigation tasks",
            "Set-ups",
            "Short periods",
            "Usability evaluations",
            "User evaluations",
            "User performances",
            "User study",
            "Virtual environments (VE)",
            "Vr systems"
        ],
        "doc_abstract": "Virtual Reality (VR) has been constantly evolving since its early days, and is now a fundamental technology in different application areas. User evaluation is a crucial step in the design and development of VR systems that do respond to users' needs, as well as for identifying applications that indeed gain from the use of such technology. Yet, there is not much work reported concerning usability evaluation and validation of VR systems, when compared with the traditional desktop setup. The paper presents a user study performed, as a first step, for the evaluation of a low-cost VR system using a Head-Mounted Display (HMD). That system was compared to a traditional desktop setup through an experiment that assessed user performance, when carrying out navigation tasks in a game scenario for a short period. The results show that, although users were generally satisfied with the VR system, and found the HMD interaction intuitive and natural, most performed better with the desktop setup. © 2008 Springer Science+Business Media, LLC.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quality perception of simplified models: NSA VS. Qslim",
        "doc_scopus_id": "55649085269",
        "doc_doi": null,
        "doc_eid": "2-s2.0-55649085269",
        "doc_date": "2008-11-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Execution times",
            "Geometrical errors",
            "Meshes",
            "Quality perception",
            "Simplification algorithms",
            "Simplified models"
        ],
        "doc_abstract": "Quality perception of simplified models is an important aspect for several applications. But, it is normally evaluated only based on the analysis of geometrical errors of the simplified models. However, the analysis of geometrical errors is not enough to evaluate the quality of the simplified models. The quality of the simplified models and the execution times are the main aspects that distinguish the simplification algorithms. These algorithms are of great interest in a variety of areas, since they allow the replacement of large models by approximations with far fewer cells for manipulation and visualization purposes. This paper presents a comparison between two simplification algorithms, NSA and Qslim, and shows the difference between geometrical and graphical quality of the models. The comparison of results was made using the PolyMeCo tool, which enables the analysis and comparison of meshes by providing an environment where several visualization options and metrics are available and can be used in a coordinated way.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Usability Evaluation in Virtual Reality: A User Study Comparing Three Different Setups",
        "doc_scopus_id": "85119835829",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85119835829",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Head-mounted-displays",
            "Low-costs",
            "Performance",
            "Usability evaluation",
            "User performance",
            "User study",
            "VR systems"
        ],
        "doc_abstract": "© The Eurographics Association 2008.We describe a user study comparing a low cost VR system using a Head-Mounted-Display (HMD) to a desktop and another setup where the image is projected on a screen. Eighteen participants played the same game in the three platforms. Results show that users generally did not like the setup using a screen and the best performances were obtained with the desktop configuration. This result could be due to the fact that most users were gamers used to the interaction through keyboard/mouse. Still, we noticed that user performance in the HMD setup was not dramatically worse and that users do not collide as often with walls.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Perceived quality assessment of polygonal meshes using observer studies: A new extended protocol",
        "doc_scopus_id": "41149133313",
        "doc_doi": "10.1117/12.766527",
        "doc_eid": "2-s2.0-41149133313",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Mesh quality",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "The complexity of a polygonal mesh is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, only a few observer studies are reported comparing the perceived quality of the simplified meshes, and it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting mesh, as perceived by the final users. Similar issues occur regarding other mesh processing methods such as smoothing. Mesh quality indices are the obvious less costly alternative to user studies, but it is also not clear how they relate to perceived quality, and which indices best describe the users behavior. This paper describes on going work concerning the evaluation of perceived quality of polygonal meshes using observer studies, while looking for a quality index which estimates user performance. In particular, given some results obtained in previous studies, a new experimental protocol was designed and a study involving 55 users was carried out, which allowed their validation, as well as further insight regarding mesh quality, as perceived by human observers. © 2008 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "There is more to color scales than meets the eye: A review on the use of color in visualization",
        "doc_scopus_id": "35348918980",
        "doc_doi": "10.1109/IV.2007.113",
        "doc_eid": "2-s2.0-35348918980",
        "doc_date": "2007-10-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Color mapping",
            "Color scale selection"
        ],
        "doc_abstract": "The appropriate use of color in Visualization is a very important subject. The choice of the proper color scale to use with a particular data set is not just a matter of choosing the prettiest representation. Throughout the years researchers have studied this subject and managed to propose guidelines which help users along the process of color scale selection. This article presents a brief overview on the subject focusing on the desired properties for color scales, the guidelines that should drive their choice, the advantages of applying those guidelines, the experimental research work on the field, and the tools proposed to help non-expert users. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preliminary usability evaluation of PolyMeCo: A visualization based tool for mesh analysis and comparison",
        "doc_scopus_id": "35048822035",
        "doc_doi": "10.1109/GMAI.2007.27",
        "doc_eid": "2-s2.0-35048822035",
        "doc_date": "2007-10-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Polygonal meshes",
            "Usability evaluation",
            "Visualization based tool"
        ],
        "doc_abstract": "An overall description of the methods used and the results obtained in the on-going evaluation of PolyMeCo - a mesh analysis and comparison tool - is presented. We are trying to evaluate some aspects of both the user interface and the visualization techniques implemented. Heuristic evaluation, observation and querying techniques were used and produced encouraging preliminary results, which provided new ideas, as well as information, that will inform the development of a more usable version of PolyMeCo, including new functionality. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of mesh simplification algorithms using PolyMeCo: A case study",
        "doc_scopus_id": "34249048328",
        "doc_doi": "10.1117/12.704547",
        "doc_eid": "2-s2.0-34249048328",
        "doc_date": "2007-05-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Polygonal meshes",
            "Quality evaluation",
            "Simplification algorithms",
            "Systematic evaluation"
        ],
        "doc_abstract": "Polygonal meshes are used in many application scenarios. Often the generated meshes are too complex not allowing proper interaction, visualization or transmission through a network. To tackle this problem, simplification methods can be used to generate less complex versions of those meshes. For this purpose many methods have been proposed in the literature and it is of paramount importance that each new method be compared with its predecessors, thus allowing quality assessment of the solution it provides. This systematic evaluation of each new method requires tools which provide all the necessary features (ranging from quality measures to visualization methods) to help users gain greater insight into the data. This article presents the comparison of two simplification algorithms, NSA and QSlim, using PolyMeCo, a tool which enhances the way users perform mesh analysis and comparison, by providing an environment where several visualization options are available and can be used in a coordinated way. © 2007 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of methods for the simplification of mesh models using quality indices and an observer study",
        "doc_scopus_id": "34548273966",
        "doc_doi": "10.1117/12.704098",
        "doc_eid": "2-s2.0-34548273966",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Mesh models",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "The complexity of a polygonal mesh model is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, only a few observer studies are reported comparing them regarding the perceived quality of the obtained simplified meshes, and it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting model, as perceived by the final users. Mesh quality indices are the obvious less costly alternative to user studies, but it is also not clear how they relate to perceived quality, and which indices best describe the users behavior. Following on earlier work carried out by the authors, but only for mesh models of the lungs, a comparison among the results of three simplification methods was performed through (1) quality indices and (2) a controlled experiment involving 65 observers, for a set of five reference mesh models of different kinds. These were simplified using two methods provided by the OpenMesh library - one using error quadrics, the other additionally using a normal flipping criterion - and also by the widely used QSlim method, for two simplification levels: 50% and 20% of the original number of faces. The main goal was to ascertain whether the findings previously obtained for lung models, through quality indices and a study with 32 observers, could be generalized to other types of models and confirmed for a larger number of observers. Data obtained using the quality indices and the results of the controlled experiment were compared and do confirm that some quality indices (e.g., geometric distance and normal deviation, as well as a new proposed weighted index) can be used, in specific circumstances, as reasonable estimators of the user perceived quality of mesh models. © 2007 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Biomechanics modeling of human musculoskeleial system using adams multibody dynamics package",
        "doc_scopus_id": "33847221489",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33847221489",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Adams multibody dynamics package",
            "Mechanical model",
            "Model assembly"
        ],
        "doc_abstract": "The aim of our study was to estimate the muscle skeletal load of human activities in the car industry, for that purpose MSC Adams Lifemodeler was used to build a mechanical model for a work place simulation were an electric screw driving was used. The data for model assembly was provided by video captured recordings, electromyography measurements and anthropometric data of workers. The task chosen for simulation was one of the more demanding, presenting a high-risk level for musculoskeletal disorders, considering the plant ergonomic department classification. The model allows the calculation of bone forces and joint moments of force. This will allow, in the future, to obtain the values of muscle skeletal action forces in wrist forearm and shoulder, caused by the action of the electric screw-driving tool. Results presented are from an ongoing study.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Perceived Quality of Simplified Polygonal Meshes: Evaluation using Observer Studies",
        "doc_scopus_id": "34249048115",
        "doc_doi": null,
        "doc_eid": "2-s2.0-34249048115",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Controlled experiment",
            "Generic modeling",
            "Lung model",
            "Mesh modeling",
            "Mesh simplifications",
            "Observer studies",
            "Perceived quality",
            "Polygonal mesh models",
            "Polygonal meshes",
            "Simplification method"
        ],
        "doc_abstract": "© The Eurographics Association 2006.The complexity of a polygonal mesh model is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting mesh, as perceived by the final users. Following on work carried out by the authors, but only for mesh models of the lungs [SSSMF05, SSFM05], a comparison among the results of three mesh simplification methods, for a few generic models and two simplification levels, was performed through a controlled experiment involving 65 observers. The goal was to ascertain whether the main findings previously obtained for lung models, through a study with 32 subjects, could be generalized to other types of models and confirmed for a larger number of observers. This was verified through the analysis of the data collected from the experiment, which shows that, regarding perceived quality, users are indeed sensitive to the mesh simplification method used and that this sensitivity varies with the simplification level.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "POLYMECO - A polygonal mesh comparison tool",
        "doc_scopus_id": "33749052275",
        "doc_doi": "10.1109/IV.2005.98",
        "doc_eid": "2-s2.0-33749052275",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Mesh analysis",
            "Mesh processing",
            "Polygonal meshes"
        ],
        "doc_abstract": "Polygonal meshes are used in many areas to model different objects and structures. Depending on their applications, they sometimes have to be processed to, for instance, reduce their complexity (simplification). This mesh processing introduces error, whose evaluation is important when choosing the kind of processing that is to be done for a particular application. Although some mesh comparison tools are described in the literature, little attention has been given to the way results are presented. A tool is presented which enhances the way users perform mesh analysis and comparison, by providing an environment where several visualization options are available and can be used in a coordinated way. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of methods for the simplification of mesh models of the lungs using quality indices and an observer study",
        "doc_scopus_id": "33746134806",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33746134806",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Lungs",
            "Mesh models",
            "Normal flipping criterion",
            "Quality indices"
        ],
        "doc_abstract": "This paper presents a comparison among mesh simplification methods performed through quality indices and a controlled experiment involving 32 observers. The simplification methods: OpenMesh, OpenMesh with a normal flipping criterion and QSlim, were compared at two simplification levels: 50% and 20% of the original number of faces. Results obtained using the quality indices and the controlled experiment were compared and show that some quality indices can be used as reasonable estimators of the observers' performance in specific circumstances.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing three methods for simplifying mesh models of the lungs: An observer test to assess perceived quality",
        "doc_scopus_id": "24644483667",
        "doc_doi": "10.1117/12.593906",
        "doc_eid": "2-s2.0-24644483667",
        "doc_date": "2005-09-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Evaluation",
            "Interaction",
            "Lungs",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "Meshes are currently used to model objects, namely human organs and other structures. However, if they have a large number of triangles, their rendering times may not be adequate to allow interactive visualization, a mostly desirable feature in some diagnosis (or, more generally, decision) scenarios, where the choice of adequate views is important. In this case, a possible solution consists in showing a simplified version while the user interactively chooses the viewpoint and, then, a fully detailed version of the model to support its analysis. To tackle this problem, simplification methods can be used to generate less complex versions of meshes. While several simplification methods have been developed and reported in the literature, only a few studies compare them concerning the perceived quality of the obtained simplified meshes. This work describes an experiment conducted with human observers in order to compare three different simplification methods used to simplify mesh models of the lungs. We intended to study if any of these methods allows a better-perceived quality for the same simplification rate. A protocol was developed in order to measure these aspects. The results presented were obtained from 32 human observers. The comparison between the three mesh simplification methods was first performed through an Exploratory Data Analysis and the significance of this comparison was then established using other statistical methods. Moreover, the influence on the observers' performances of some other factors was also investigated.",
        "available": false,
        "clean_text": ""
    }
]