[
    {
        "doc_title": "Augmented reality situated visualization in decision-making",
        "doc_scopus_id": "85107332174",
        "doc_doi": "10.1007/s11042-021-10971-4",
        "doc_eid": "2-s2.0-85107332174",
        "doc_date": "2022-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Data representations",
            "Decision making process",
            "Decision support system (dss)",
            "Design and implementations",
            "In contexts",
            "Literature analysis",
            "Scientific fields",
            "Visual data representation"
        ],
        "doc_abstract": "© 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.Decision-making processes and decision support systems (DSS) have been improved by a variety of methods originated from several scientific fields, such as information science and artificial intelligence (AI). Situated visualization (SV) allows presenting visual data representations in context and may support better DSS. Its main characteristic is to display data representations near the data referent. As augmented reality (AR) is becoming more mature, affordable, and widespread, using it as a tool for SV becomes viable in several situations. Moreover, it may provide a positive contribution to more effective and efficient decision-making, as the users have contextual, relevant, and appropriate information that fosters more informed choices. As new challenges and opportunities arise, it is important to understand the relevance of intertwining these fields. Based on literature analysis, this paper introduces the main concepts involved, and, through practical examples, addresses and discusses current areas of application, benefits, challenges, and opportunities of using SV through AR to visualize data in context to support better decision-making processes. In the end, a set of guidelines for the design and implementation of DSS based on situated augmented reality are proposed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing Desktop vs. Mobile Interaction for the Creation of Pervasive Augmented Reality Experiences",
        "doc_scopus_id": "85127690437",
        "doc_doi": "10.3390/jimaging8030079",
        "doc_eid": "2-s2.0-85127690437",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.This paper presents an evaluation and comparison of interaction methods for the configuration and visualization of pervasive Augmented Reality (AR) experiences using two different platforms: desktop and mobile. AR experiences consist of the enhancement of real-world environments by superimposing additional layers of information, real-time interaction, and accurate 3D registration of virtual and real objects. Pervasive AR extends this concept through experiences that are continuous in space, being aware of and responsive to the user’s context and pose. Currently, the time and technical expertise required to create such applications are the main reasons preventing its widespread use. As such, authoring tools which facilitate the development and configuration of pervasive AR experiences have become progressively more relevant. Their operation often involves the navigation of the real-world scene and the use of the AR equipment itself to add the augmented information within the environment. The proposed experimental tool makes use of 3D scans from physical environments to provide a reconstructed digital replica of such spaces for a desktop-based method, and to enable positional tracking for a mobile-based one. While the desktop platform represents a non-immersive setting, the mobile one provides continuous AR in the physical environment. Both versions can be used to place virtual content and ultimately configure an AR experience. The authoring capabilities of the different platforms were compared by conducting a user study focused on evaluating their usability. Although the AR interface was generally considered more intuitive, the desktop platform shows promise in several aspects, such as remote configuration, lower required effort, and overall better scalability.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Robust 3D-Based Color Correction Approach for Texture Mapping Applications",
        "doc_scopus_id": "85125064447",
        "doc_doi": "10.3390/s22051730",
        "doc_eid": "2-s2.0-85125064447",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3d-modeling",
            "Color mapping function",
            "Colour corrections",
            "Correction approaches",
            "Image histograms",
            "Joint image histogram",
            "Mapping applications",
            "Multiple image",
            "Point-clouds",
            "Texture mapping"
        ],
        "doc_abstract": "© 2022 by the authorsLicensee MDPI, Basel, Switzerland.Texture mapping of 3D models using multiple images often results in textured meshes with unappealing visual artifacts known as texture seams. These artifacts can be more or less visible, depending on the color similarity between the used images. The main goal of this work is to produce textured meshes free of texture seams through a process of color correcting all images of the scene. To accomplish this goal, we propose two contributions to the state-of-the-art of color correction: a pairwise-based methodology, capable of color correcting multiple images from the same scene; the application of 3D information from the scene, namely meshes and point clouds, to build a filtering procedure, in order to produce a more reliable spatial registration between images, thereby increasing the robustness of the color correction procedure. We also present a texture mapping pipeline that receives uncorrected images, an untextured mesh, and point clouds as inputs, producing a final textured mesh and color corrected images as output. Results include a comparison with four other color correction approaches. These show that the proposed approach outperforms all others, both in qualitative and quantitative metrics. The proposed approach enhances the visual quality of textured meshes by eliminating most of the texture seams.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Does Remote Expert Representation really matters: A comparison of Video and AR-based Guidance",
        "doc_scopus_id": "85129657701",
        "doc_doi": "10.1109/VRW55335.2022.00208",
        "doc_eid": "2-s2.0-85129657701",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Human computer interaction",
            "Human computer interaction design and evalution method",
            "Human-centered computing",
            "Human-computer-interaction designs",
            "Interaction paradigm",
            "Mixed/augmented reality",
            "Remote experts",
            "User study"
        ],
        "doc_abstract": "© 2022 IEEE.This work describes a user study aimed at understanding how the remote expert representation affects the sense of social presence in scenarios of remote guidance. We compared a traditional video chat solution with an Augmented Reality (AR) annotation tool. These were selected due to ongoing research with partners from the industry sector, following the insights of a participatory design process. A well defined-problem was used, i.e., a synchronous maintenance task with 4 completion stages that required a remote expert using a computer to guide 26 on-site participants wielding a handheld device. The results of the study are described and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Whac-A-Mole: Exploring Virtual Reality (VR) for Upper-Limb Post-Stroke Physical Rehabilitation based on Participatory Design and Serious Games",
        "doc_scopus_id": "85129620414",
        "doc_doi": "10.1109/VRW55335.2022.00209",
        "doc_eid": "2-s2.0-85129620414",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Human computer interaction",
            "Human computer interaction design and evalution method",
            "Human-centered computing",
            "Human-computer-interaction designs",
            "Interaction paradigm",
            "Mixed/augmented reality",
            "Physical rehabilitation",
            "User study"
        ],
        "doc_abstract": "© 2022 IEEE.This paper describes a Human-Centered Design process aimed at understanding how Virtual Reality (VR) can assist in post-stroke physical rehabilitation. Based on insights from stroke survivors and healthcare professionals, a serious game prototype is proposed. We focused on upper-limb rehabilitation, which inspired the game narrative and the movements users must perform. The game supports two modes: 1-normal version*users can use any arm to pick a virtual hammer and hit objects; 2-mirror version*converts a traditional approach to VR, providing the illusion that the arm affected by the stroke is moving. These were evaluated through a user study.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using 3D Reconstruction to create Pervasive Augmented Reality Experiences: A comparison",
        "doc_scopus_id": "85129618187",
        "doc_doi": "10.1109/VRW55335.2022.00207",
        "doc_eid": "2-s2.0-85129618187",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Human computer interaction",
            "Human computer interaction design and evalution method",
            "Human-centered computing",
            "Human-computer-interaction designs",
            "Interaction paradigm",
            "Mixed/augmented reality",
            "Physical environments",
            "User study"
        ],
        "doc_abstract": "© 2022 IEEE.This paper presents a prototype for configuration and visualization of Pervasive Augmented Reality (AR) experiences using two versions: desktop and mobile. It makes use of 3D scans from physical environments to provide a reconstructed digital representation of such spaces to the desktop version and enable positional tracking for the mobile. While the desktop presents a non-immersive setting, the mobile provides continuous AR in the physical environment. Both versions can be used to place virtual content and ultimately configure an AR experience. The authoring capabilities of the proposed solution were compared by conducting a user study focused on evaluating their usability.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the Use of Virtual Reality for Medical Imaging Visualization",
        "doc_scopus_id": "85111544006",
        "doc_doi": "10.1007/s10278-021-00480-z",
        "doc_eid": "2-s2.0-85111544006",
        "doc_date": "2021-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Advanced visualizations",
            "Development frameworks",
            "Development tools",
            "Medical analysis",
            "Medical imaging visualization",
            "Real time performance",
            "Standard data format",
            "Surgical planning",
            "Augmented Reality",
            "Diagnostic Imaging",
            "Humans",
            "Radiography",
            "Software",
            "Virtual Reality"
        ],
        "doc_abstract": "© 2021, Society for Imaging Informatics in Medicine.Advanced visualization of medical imaging has been a motive for research due to its value for disease analysis, surgical planning, and academical training. More recently, attention has been turning toward mixed reality as a means to deliver more interactive and realistic medical experiences. However, there are still many limitations to the use of virtual reality for specific scenarios. Our intent is to study the current usage of this technology and assess the potential of related development tools for clinical contexts. This paper focuses on virtual reality as an alternative to today’s majority of slice-based medical analysis workstations, bringing more immersive three-dimensional experiences that could help in cross-slice analysis. We determine the key features a virtual reality software should support and present today’s software tools and frameworks for researchers that intend to work on immersive medical imaging visualization. Such solutions are assessed to understand their ability to address existing challenges of the field. It was understood that most development frameworks rely on well-established toolkits specialized for healthcare and standard data formats such as DICOM. Also, game engines prove to be adequate means of combining software modules for improved results. Virtual reality seems to remain a promising technology for medical analysis but has not yet achieved its true potential. Our results suggest that prerequisites such as real-time performance and minimum latency pose the greatest limitations for clinical adoption and need to be addressed. There is also a need for further research comparing mixed realities and currently used technologies.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using augmented reality for industrial quality assurance: a shop floor user study",
        "doc_scopus_id": "85105164742",
        "doc_doi": "10.1007/s00170-021-07049-8",
        "doc_eid": "2-s2.0-85105164742",
        "doc_date": "2021-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Industrial production",
            "Industrial scenarios",
            "Laboratory conditions",
            "Quality control tests",
            "Real-time validation",
            "Risk of human error",
            "Sustainable solution",
            "Video instructions"
        ],
        "doc_abstract": "© 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.Quality control procedures are essential in many industrial production pipelines. These repetitive and precise tasks are frequently complex, including several steps that must be performed correctly by different operators. To facilitate these, quality control tests are often documented with static media like video recordings, photos, or diagrams. However, the need for the operator to divide attention between the visual instructions and the task, and the lack of feedback lead to slow processes, with potential for improvement. By using augmented reality (AR), operators can focus on the task at hand while receiving visual feedback where it is needed. Nevertheless, existing prototypes are still at early stages, being tested only in laboratory conditions, far from mimicking real scenarios. The major contributions of this work are twofold: first, we present an AR-based quality control system capable of generating virtual content to guide operators by overlaying information in a video stream while performing real-time validation. The system evaluates the current status of the procedure to ensure the automatic progression to the next phase. Second, an evaluation was conducted in an industrial shop floor during 1 week, with seven operators to verify if the system was robust and understand possible efficiency gains when compared to the alternative, i.e., video instructions. Results showed that AR had a significant impact in procedures’ execution time (reduction of 36%), while reducing the risk of human errors, which means AR technologies may represent a profitable and sustainable solution when applied to real-world industrial scenarios, in the long run.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Robust texture mapping using rgb-d cameras",
        "doc_scopus_id": "85105402165",
        "doc_doi": "10.3390/s21093248",
        "doc_eid": "2-s2.0-85105402165",
        "doc_date": "2021-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D meshes",
            "Camera misalignments",
            "Camera pose estimation",
            "Estimation methodologies",
            "Pose estimation errors",
            "Rgb-d cameras",
            "Texture mapping",
            "Visual artifacts"
        ],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.The creation of a textured 3D mesh from a set of RGD-D images often results in textured meshes that yield unappealing visual artifacts. The main cause is the misalignments between the RGB-D images due to inaccurate camera pose estimations. While there are many works that focus on improving those estimates, the fact is that this is a cumbersome problem, in particular due to the accumulation of pose estimation errors. In this work, we conjecture that camera poses estimation methodologies will always display non-neglectable errors. Hence, the need for more robust texture mapping methodologies, capable of producing quality textures even in considerable camera misalignments scenarios. To this end, we argue that use of the depth data from RGB-D images can be an invaluable help to confer such robustness to the texture mapping process. Results show that the complete texture mapping procedure proposed in this paper is able to significantly improve the quality of the produced textured 3D meshes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Remote asynchronous collaboration in maintenance scenarios using augmented reality and annotations",
        "doc_scopus_id": "85105968777",
        "doc_doi": "10.1109/VRW52623.2021.00166",
        "doc_eid": "2-s2.0-85105968777",
        "doc_date": "2021-03-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Asynchronous collaboration",
            "Collaborative approach",
            "Maintenance scenario",
            "Maintenance tasks",
            "Remote experts",
            "Shared understanding",
            "Spatial informations",
            "User study"
        ],
        "doc_abstract": "© 2021 IEEE.This paper presents an Augmented Reality (AR) remote collaborative approach making use of different stabilized annotation features, part of an ongoing research with partners from the industry. It enables a remote expert to assist an on-site technician during asynchronous maintenance tasks. To foster the creation of a shared understanding, the on-site technician uses mobile AR, allowing the identification of issues, while the remote expert uses a computer to share annotations and provide spatial information about objects, events and areas of interest. The results of a pilot user study to evaluate asynchronous collaborative aspects while using the approach are also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual vs auditory augmented reality for indoor guidance",
        "doc_scopus_id": "85102970265",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85102970265",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Auditory stimuli",
            "Indoor navigation system",
            "Indoor navigation tools",
            "Performance outcome",
            "Presenting informations",
            "Sensory channels",
            "Subjective perceptions",
            "Visual impairment"
        ],
        "doc_abstract": "Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.Indoor navigation systems are not widely used due to the lack of effective indoor tracking technology. Augmented Reality (AR) is a natural medium for presenting information in indoor navigation tools. However, augmenting the environment with visual stimuli may not always be the most appropriate method to guide users, e.g., when they are performing some other visual task or they suffer from visual impairments. This paper presents an AR app to support visual and auditory stimuli that we have developed for indoor guidance. A study (N=20) confirms that the participants reached the target when using two types of stimuli, visual and auditory. The AR visual stimuli outperformed the auditory stimuli in terms of time and overall distance travelled. However, the auditory stimuli forced the participants to pay more attention, and this resulted in better memorization of the route. These performance outcomes were independent of gender and age. Therefore, in addition to being easy to use, auditory stimuli promote route retention and show potential in situations in which vision cannot be used as the primary sensory channel or when spatial memory retention is important. We also found that perceived physical and mental efforts affect the subjective perception about the AR guidance app.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Experience: Quality Assessment and Improvement on a Forest Fire Dataset",
        "doc_scopus_id": "85100401157",
        "doc_doi": "10.1145/3428155",
        "doc_eid": "2-s2.0-85100401157",
        "doc_date": "2021-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Assessment and improvement",
            "Controlled fires",
            "Geometric operations",
            "Graph analysis",
            "Natural phenomena",
            "Quality assessment",
            "Segmentation techniques",
            "Video tracking"
        ],
        "doc_abstract": "© 2021 ACM.Spatiooral data can be used to study and simulate the movement and behavior of objects and natural phenomena. However, the use of real-world data raises several challenges related to its acquisition, representation, and quality. This article presents a data cleaning process, based on consistency rules and checks, that uses geometric operations to detect and remove outliers or inaccurate data in a spatiooral series. The proposal consists of selecting key frames and applying the process iteratively until the data have the desired quality. The case study consists of extracting and cleaning spatiooral data from a video tracking the propagation of a controlled fire captured using drones. The source data was generated using segmentation techniques to obtain the regions representing the burned area across time. The main issues concern noisy data (e.g., the height of flames is highly variable) and occlusion due to smoke. The results show that the quality assessment and improvement method proposed in this work can identify and remove inconsistencies from a dataset of more than 22,500 polygons in just a few iterations. The quality of the corrected dataset is verified using metrics and graph analysis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using Augmented Reality and Step by Step Verification in Industrial Quality Control",
        "doc_scopus_id": "85091118569",
        "doc_doi": "10.1007/978-3-030-58282-1_55",
        "doc_eid": "2-s2.0-85091118569",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Computer vision techniques",
            "Human demonstrations",
            "Industrial scenarios",
            "Manual intervention",
            "Quality control tests",
            "Real-time validation",
            "Step-by-step instructions",
            "Validation process"
        ],
        "doc_abstract": "© 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.Quality control procedures are extremely important among industrial applications. Generally, these tasks include many repetitive tasks that require manual intervention. Given their complexity, quality control tests are often detailed in video recordings, paper instructions, photos or diagrams to guide workers throughout the process. Augmented Reality (AR) has been making significant progress in the last decades, becoming mature enough to be used in industrial scenarios. While some AR systems have been proposed to support quality control procedures, most of them only present information to workers but do not track or validate the process in real-time being used only to guide it. Another limitation of existing systems is the generation of virtual instructions used by AR systems to guide the operator. In this work, we propose an AR-based tool to guide users by overlaying information in a video stream while performing real-time validation during the execution of quality control procedures. The main objective is to provide dynamic support and decrease the mental workload needed to complete the procedure as well as the number of errors, facilitating the procedure execution by untrained workers. Besides this, the tool allows to create virtual content that can be used to generate step-by-step instructions automatically based on human demonstrations. By making the virtual instruction creation effortlessly it is possible to eliminate the user’s need for memorizing new instructions with each change of the product lines. While presenting task relevant information the system uses computer vision techniques to keep track of the procedure stage, verifying its completion and switching automatically to the next step without requiring any interaction from the user. A comparison between the time taken to perform the procedure with and without validation was made. The results show that the validation process would confer the process a significant efficiency boost, while avoiding possible human errors.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring Annotations and Hand Tracking in Augmented Reality for Remote Collaboration",
        "doc_scopus_id": "85091104420",
        "doc_doi": "10.1007/978-3-030-58282-1_14",
        "doc_eid": "2-s2.0-85091104420",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Adaptive capabilities",
            "Head mounted displays",
            "Human-centered designs",
            "Maintenance procedures",
            "Professional experiences",
            "Real-time video streams",
            "Remote collaboration",
            "Visualization of information"
        ],
        "doc_abstract": "© 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.Collaboration among remotely distributed professionals is often required in a maintenance context. Professionals need mechanisms with adaptive capabilities to enable knowledge transfer, since the necessary experience and expertise are usually spread among different professionals. To provide a shared understanding, Augmented Reality (AR) has been explored. In this paper, which is part of ongoing research using a human-centered design approach with partners from the industry sector, we describe a framework using annotations to improve the shared perceived realities of different professionals. The framework allows manually freezing the on-site professional context and sharing it with a remote expert to create annotations. Then, the on-site professional can visualize instructions through aligned and anchored annotations, using a see-through Head Mounted Display (HMD). In addition, annotations based on real-time video stream from a remote expert are also available. Hand tracking is used to manipulate the annotations, enabling the adjustment of their position and scale in the real-world according to the context, thus enriching the on-site professional experience and improving visualization of information while conducting maintenance procedures suggested by a remote expert.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interaction with Virtual Content using Augmented Reality: A User Study in Assembly Procedures",
        "doc_scopus_id": "85095832791",
        "doc_doi": "10.1145/3427324",
        "doc_eid": "2-s2.0-85095832791",
        "doc_date": "2020-11-04",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Social Sciences (miscellaneous)",
                "area_abbreviation": "SOCI",
                "area_code": "3301"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Assembly tasks",
            "Controlled experiment",
            "Interaction design",
            "Interaction methods",
            "Lego block",
            "Limited resolution",
            "Manipulation of virtual objects",
            "See-through HMD"
        ],
        "doc_abstract": "© 2020 ACM.Assembly procedures are a common task in several domains of application. Augmented Reality (AR) has been considered as having great potential in assisting users while performing such tasks. However, poor interaction design and lack of studies, often results in complex and hard to use AR systems. This paper considers three different interaction methods for assembly procedures (Touch gestures in a mobile device; Mobile Device movements; 3D Controllers and See-through HMD). It also describes a controlled experiment aimed at comparing acceptance and usability between these methods in an assembly task using Lego blocks. The main conclusions are that participants were faster using the 3D controllers and Video see-through HMD. Participants also preferred the HMD condition, even though some reported light symptoms of nausea, sickness and/or disorientation, probably due to limited resolution of the HMD cameras used in the video see-through setting and some latency issues. In addition, although some research claims that manipulation of virtual objects with movements of the mobile device can be considered as natural, this condition was the least preferred by the participants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-sensor extrinsic calibration using an extended set of pairwise geometric transformations",
        "doc_scopus_id": "85096588033",
        "doc_doi": "10.3390/s20236717",
        "doc_eid": "2-s2.0-85096588033",
        "doc_date": "2020-11-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Automatic method",
            "Calibration procedure",
            "Calibration process",
            "Experimental methods",
            "Extrinsic calibration",
            "Geometric transformations",
            "Human intervention",
            "Multiple sensors"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Systems composed of multiple sensors for exteroceptive perception are becoming increasingly common, such as mobile robots or highly monitored spaces. However, to combine and fuse those sensors to create a larger and more robust representation of the perceived scene, the sensors need to be properly registered among them, that is, all relative geometric transformations must be known. This calibration procedure is challenging as, traditionally, human intervention is required in variate extents. This paper proposes a nearly automatic method where the best set of geometric transformations among any number of sensors is obtained by processing and combining the individual pairwise transformations obtained from an experimental method. Besides eliminating some experimental outliers with a standard criterion, the method exploits the possibility of obtaining better geometric transformations between all pairs of sensors by combining them within some restrictions to obtain a more precise transformation, and thus a better calibration. Although other data sources are possible, in this approach, 3D point clouds are obtained by each sensor, which correspond to the successive centers of a moving ball its field of view. The method can be applied to any sensors able to detect the ball and the 3D position of its center, namely, LIDARs, mono cameras (visual or infrared), stereo cameras, and TOF cameras. Results demonstrate that calibration is improved when compared to methods in previous works that do not address the outliers problem and, depending on the context, as explained in the results section, the multi-pairwise technique can be used in two different methodologies to reduce uncertainty in the calibration process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Conceptual Model for Data Collection and Analysis for AR-based Remote Collaboration Evaluation",
        "doc_scopus_id": "85099568572",
        "doc_doi": "10.1109/ISMAR-Adjunct51615.2020.00016",
        "doc_eid": "2-s2.0-85099568572",
        "doc_date": "2020-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Collaborative process",
            "Conceptual model",
            "Data collection",
            "Enabling technologies",
            "Holistic evaluations",
            "Novel methods",
            "Remote collaboration",
            "Team performance"
        ],
        "doc_abstract": "© 2020 IEEE.A significant effort has been devoted to the creation of the enabling technology and in the proposal of novel methods to support remote collaboration using Augmented Reality (AR), given the novelty of the field. As the field progresses to focus on the nuances of supporting collaboration and with the growing number of prototypes mediated by AR, the characterization and evaluation of the collaborative process becomes an essential, but difficult endeavor. Evaluation is particularly challenging in this multifaceted context involving many aspects that may influence the way collaboration occurs. Therefore, it is essential the existence of holistic evaluation strategies that monitor the use and performance of the proposed solutions regarding the team, its members, and the technology, allowing adequate characterization and report of collaborative processes. As a contribute, we propose a conceptual model for multi-user data collection and analysis that monitors several collaboration aspects: individual and team performance, behaviour and level of collaboration, as well as contextual data in scenarios of remote collaboration using AR-based solutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DeepRings: A Concentric-Ring Based Visualization to Understand Deep Learning Models",
        "doc_scopus_id": "85102927129",
        "doc_doi": "10.1109/IV51561.2020.00054",
        "doc_eid": "2-s2.0-85102927129",
        "doc_date": "2020-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Artificial intelligent",
            "Concentric rings",
            "Global perspective",
            "Learning models",
            "Network features",
            "Neural network predictions",
            "Potential problems",
            "Real world environments"
        ],
        "doc_abstract": "© 2020 IEEE.Artificial Intelligent (AI) techniques, such as ma-chine learning (ML), have been making significant progress over the past decade. Many systems have been applied in sensitive tasks involving critical infrastructures which affect human well-being or health. Before deploying an AI system, it is necessary to validate its behavior and guarantee that it will continue to perform as expected when deployed in a real-world environment. For this reason, it is important to comprehend specific aspects of such systems. For example, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images via feature visualization often focuses on explaining predictions for neurons of one single convolutional layer. Not presenting a global perspective over the features learned by the model leads the user to miss the bigger picture. In this work we focus on providing a representation based on the structure of deep neural networks. It presents a visualization able to give the user a global perspective over the feature maps of a convolutional neural network (CNN) in a single image, revealing potential problems of the learning representations present in the network feature maps.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "FICAvis: Data Visualization to Prevent University Dropout",
        "doc_scopus_id": "85102922172",
        "doc_doi": "10.1109/IV51561.2020.00034",
        "doc_eid": "2-s2.0-85102922172",
        "doc_date": "2020-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Creation process",
            "Failure factors",
            "Interactive exploration",
            "Participatory design",
            "Prototype development",
            "Requirements elicitation",
            "Risk indicators",
            "University students"
        ],
        "doc_abstract": "© 2020 IEEE.The FICA project-Tools for Identifying and Combating Dropout-started at the University of Aveiro in 2015 with the aim to help reduce and prevent dropouts and increase academic success among university students. Within the project a signicant amount of data is provided to different University stakeholders to monitor academic issues, however, these data are currently provided in large tables, a format difficult to analyze. In this paper, we present the main aspects of the data, the users and contexts of use. We also propose an approach to allow the visual and interactive exploration of the FICA project data to help monitor the path of the students and identify risk indicators and failure factors that can lead to critical situations such as dropout. A solution developed using the participatory design methodology is presented, detailing all stages of its creation process, from the requirements elicitation based on focus groups and interviews, design and prototype development in Power BI to its evaluation. Some suggestions for future work are also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Egocentric viewpoint in mixed reality situated visualization: Challenges and opportunities",
        "doc_scopus_id": "85102917072",
        "doc_doi": "10.1109/IV51561.2020.00012",
        "doc_eid": "2-s2.0-85102917072",
        "doc_date": "2020-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 IEEE.Mixed reality (MR) is well suited for situated visualization (SV), a method to represent data in a context, with potential in many situations. However, MR-based visualizations are commonly constrained to the users' single egocentric viewpoint reducing their ability to explore all the available information. This article discusses the main limitations and challenges of this approach based on the analysis of existing literature and identifies opportunities, as well as relevant aspects that must be considered when devising new methods aimed at overcoming those limitations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach",
        "doc_scopus_id": "85085374137",
        "doc_doi": "10.1016/j.robot.2020.103558",
        "doc_eid": "2-s2.0-85085374137",
        "doc_date": "2020-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Calibration procedure",
            "Extrinsic calibration",
            "Multi-modal approach",
            "Multiple sensors",
            "Objective functions",
            "Robot operating systems (ROS)",
            "Simultaneous optimization",
            "State-of-the-art approach"
        ],
        "doc_abstract": "© 2020 Elsevier B.V.This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2020-05-19 2020-05-19 2020-05-28 2020-05-28 2020-08-03T02:56:00 S0921-8890(20)30398-5 S0921889020303985 10.1016/j.robot.2020.103558 S300 S300.1 FULL-TEXT 2020-08-03T02:04:37.606251Z 0 0 20200901 20200930 2020 2020-05-19T15:45:05.788726Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst primabst ref vitae 0921-8890 09218890 true 131 131 C Volume 131 7 103558 103558 103558 202009 September 2020 2020-09-01 2020-09-30 2020 article fla © 2020 Elsevier B.V. All rights reserved. AROSFRAMEWORKFOREXTRINSICCALIBRATIONINTELLIGENTVEHICLESAMULTISENSORMULTIMODALAPPROACH OLIVEIRA M 1 Introduction 2 ROS based calibration setup 2.1 Configuration of the calibration procedure 2.2 Interactive positioning of sensors 2.3 Interactive data labelling 2.4 Collecting data 2.5 Sensor poses from partial transformations 3 Calibration procedure 3.1 Optimization parameters 3.2 Objective function 3.2.1 Camera sub-function 3.2.2 Laser sub-function 3.3 Sensors pose calibration: Optimization 4 Results 4.1 Camera to camera 4.2 Complete system calibration 5 Conclusions and future work Acknowledgements References MUELLER 2017 1 6 G 2017IEEE20THINTCONFINTELLIGENTTRANSPORTATIONSYSTEMSITSC CONTINUOUSSTEREOCAMERACALIBRATIONINURBANSCENARIOS WU 2015 2638 2642 L 2015IEEEINTCONFMECHATRONICSAUTOMATIONICMA BINOCULARSTEREOVISIONCAMERACALIBRATION ROUSU 2016 896 900 L 2016IEEEADVANCEDINFORMATIONMANAGEMENTCOMMUNICATESELECTRONICAUTOMATIONCONTROLCONFIMCEC AUTOMATICCALIBRATIONSYSTEMFORBINOCULARSTEREOIMAGING LING 2016 1771 1778 Y 2016IEEERSJINTCONFINTELLIGENTROBOTSSYSTEMSIROS HIGHPRECISIONONLINEMARKERLESSSTEREOEXTRINSICCALIBRATION DINH 2019 815 826 V VASCONCELOS 2012 2097 2107 F PEREIRA 2016 326 337 M ALMEIDA 2012 312 319 M IMAGEANALYSISRECOGNITION 3D2DLASERRANGEFINDERCALIBRATIONUSINGACONICBASEDGEOMETRYSHAPE GUINDEL 2017 1 6 C 2017IEEE20THINTERNATIONALCONFERENCEINTELLIGENTTRANSPORTATIONSYSTEMSITSC AUTOMATICEXTRINSICCALIBRATIONFORLIDARSTEREOVEHICLESENSORSETUPS KWON 2018 1451 1454 Y 201818THINTCONFCONTROLAUTOMATIONSYSTEMSICCAS AUTOMATICSPHEREDETECTIONFOREXTRINSICCALIBRATIONMULTIPLERGBDCAMERAS KHAN 2016 1960 1965 A 2016IEEEINTCONFROBOTICSBIOMIMETICSROBIO CALIBRATIONACTIVEBINOCULARRGBDVISIONSYSTEMSFORDUALARMROBOTS BASSO 2018 1315 1332 F QIAO 2013 253 256 Y 2013INTCONFCOMPUTATIONALPROBLEMSOLVINGICCP ANEWAPPROACHSELFCALIBRATIONHANDEYEVISIONSYSTEMS ZHANG 2011 1 6 C 2011IEEEINTCONFMULTIMEDIAEXPO CALIBRATIONBETWEENDEPTHCOLORSENSORSFORCOMMODITYDEPTHCAMERAS CHEN 2019 2685 2694 G QILONGZHANG 2004 2301 2306 G 2004IEEERSJINTCONFINTELLIGENTROBOTSSYSTEMSIROSIEEECATNO04CH37566VOL3 EXTRINSICCALIBRATIONACAMERALASERRANGEFINDERIMPROVESCAMERACALIBRATION HASELICH 2012 25 28 M 2012IEEEINTCONFEMERGINGSIGNALPROCESSINGAPPLICATIONS CALIBRATIONMULTIPLECAMERASA3DLASERRANGEFINDER CHEN 2016 448 453 Z 20169THINTCONGRESSIMAGESIGNALPROCESSINGBIOMEDICALENGINEERINGINFORMATICSCISPBMEI EXTRINSICCALIBRATIONALASERRANGEFINDERACAMERABASEDAUTOMATICDETECTIONLINEFEATURE VELAS 2014 M CALIBRATIONRGBCAMERAVELODYNELIDAR LEE 2017 64 69 G 2017IEEEINTCONFMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMSMFI CALIBRATIONVLP16LIDARMULTIVIEWCAMERASUSINGABALLFOR360DEGREE3DCOLORMAPACQUISITION LEVINSON 2013 J ROBOTICSSCIENCESYSTEMS AUTOMATICONLINECALIBRATIONCAMERASLASERS DEZHIGAO 2010 6211 6215 J 20108THWORLDCONGRESSINTELLIGENTCONTROLAUTOMATION AMETHODSPATIALCALIBRATIONFORCAMERARADAR SANTOS 2010 1421 1427 V 13THINTIEEECONFINTELLIGENTTRANSPORTATIONSYSTEMS ATLASCARTECHNOLOGIESFORACOMPUTERASSISTEDDRIVINGSYSTEMBOARDACOMMONAUTOMOBILE LIAO 2017 305 310 Y 20172NDINTCONFADVANCEDROBOTICSMECHATRONICSICARM JOINTKINECTMULTIPLEEXTERNALCAMERASSIMULTANEOUSCALIBRATION REHDER 2016 383 398 J PRADEEP 2014 211 225 V EXPERIMENTALROBOTICS12THINTSYMPOSIUMEXPERIMENTALROBOTICS CALIBRATINGAMULTIARMMULTISENSORROBOTABUNDLEADJUSTMENTAPPROACH OLIVEIRA 2020 203 215 M ROBOT2019FOURTHIBERIANROBOTICSCONFERENCE AGENERALAPPROACHEXTRINSICCALIBRATIONINTELLIGENTVEHICLESUSINGROS BRADSKI 2000 G QUIGLEY 2009 M ICRAWORKSHOPOPENSOURCESOFTWARE ROSOPENSOURCEROBOTOPERATINGSYSTEM FOOTE 2013 1 6 T 2013IEEECONFERENCETECHNOLOGIESFORPRACTICALROBOTAPPLICATIONSTEPRA TFTRANSFORMLIBRARY HORNEGGER 1999 640 647 J PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCECOMPUTERVISIONVOL1 REPRESENTATIONISSUESINMLESTIMATIONCAMERAMOTION AGARWAL 2010 29 42 S BUNDLEADJUSTMENTINLARGE OLIVEIRAX2020X103558 OLIVEIRAX2020X103558XM 2022-05-28T00:00:00.000Z 2022-05-28T00:00:00.000Z © 2020 Elsevier B.V. All rights reserved. 2020-05-03T22:04:14.426Z FCT Fundação para a Ciência e a Tecnologia CYTED CYTED Ciencia y Tecnología para el Desarrollo item S0921-8890(20)30398-5 S0921889020303985 10.1016/j.robot.2020.103558 271599 2020-08-03T02:04:37.606251Z 2020-09-01 2020-09-30 true 2168899 MAIN 11 62586 849 656 IMAGE-WEB-PDF 1 gr3 35928 345 552 gr9 19670 275 339 pic2 6890 131 112 gr1 33542 289 490 gr12 23770 202 495 gr11 36248 266 489 gr7 37441 309 371 fx1002 8338 79 360 gr4 28117 314 378 gr6 18379 143 385 fx1001 8242 73 367 gr5 45234 242 376 gr2 42926 236 553 gr8 52449 600 489 gr10 49550 264 489 fx1003 6813 77 283 gr3 8344 137 219 gr9 6226 164 202 pic2 19499 164 140 gr1 5132 129 219 gr12 5757 89 219 gr11 7781 119 219 gr7 20274 163 196 fx1002 2821 48 219 gr4 6100 163 197 gr6 10154 81 219 fx1001 2829 43 219 gr5 25359 141 219 gr2 10115 94 219 gr8 4814 163 133 gr10 11178 118 219 fx1003 3134 60 219 gr3 277021 1527 2446 gr9 136675 1219 1500 pic2 61746 583 499 gr1 243828 1282 2171 gr12 231425 895 2191 gr11 268102 1178 2168 gr7 362638 1370 1643 fx1002 30951 210 958 gr4 207176 1390 1675 gr6 147160 632 1706 fx1001 30843 193 975 gr5 416481 1072 1667 gr2 344316 1047 2451 gr8 390098 2658 2167 gr10 424428 1169 2168 fx1003 24651 206 753 si115 6585 si97 32421 si36 1420 si95 6165 si111 9455 si44 6967 si7 7552 si94 1102 si42 7085 si121 31424 si5 7072 si74 5953 si89 2139 si119 9249 si71 1806 si10 1146 si30 8639 si12 235 si35 7139 si8 12045 si37 8297 si54 6126 si16 25662 si41 1679 si117 27686 si80 14084 si88 1722 si51 1990 si91 10025 si85 7503 si6 5529 si110 14513 si21 22352 si26 1589 si32 5724 si45 21948 si47 2672 si24 1842 si27 1665 si40 5395 si112 8789 si118 8959 si18 5806 si55 7274 si59 21073 si1 1359 si87 1814 si70 2168 si38 3203 si52 22498 si83 5598 si53 5544 si77 23977 si4 916 si49 2727 si17 4071 si90 10327 si3 3897 si50 3168 si29 1813 si39 21584 si22 37437 si48 1268 si114 16710 si34 1274 si23 2199 si28 1675 si92 3705 si25 1781 si75 36143 si31 1407 am false 3012923 ROBOT 103558 103558 S0921-8890(20)30398-5 10.1016/j.robot.2020.103558 Elsevier B.V. Fig. 1 Two methodologies for solving the calibration of complex systems using pair-wise approaches: (a) sequential pairwise; (b) one level pyramid using a reference sensor. The estimated transformations use the arrangements shown in solid colour arrows. Other possible arrangements are presented in dashed grey lines. Note that in both cases only a subset of the available transformations is used. Fig. 2 The proposed calibration procedure: (a) initialization from xacro files and interactive first guess; (b) data labelling and collecting. Fig. 3 Interactive labelling of 2D LiDAR data: (a) creation of interactive marker on the sensor body, (b) dragging and dropping the marker on top of the data cluster containing the chessboard plane, (c) and (d) subsequent automatic tracking of the chessboard plane. Fig. 4 Conceptual transformation graph for a complex robotic system. Each sensor has a respective calibration partial transformation, denoted by the solid edges. Dashed edges contain transformations which are not optimized (they may be static or dynamic). Each sensor has a corresponding link to which the data it collects is attached, denoted in the figure by the solid thin ellipses. Very few approaches in the literature are capable of calibrating such a system while preserving the initial structure of the graph of transformations. Fig. 5 Example of reprojection of chessboard corners during the optimization procedure: squares denote the position of the detected chessboard corners (ground truth points); crosses denote the initial position of each projected corner; points denote the current position of the projected corners. Fig. 6 Chessboard: graphics visualization of the created grid and boundary correspondent to the real chessboard (a); image of the real chessboard (b). Fig. 7 ATLASCAR2: Autonomous vehicle from the Department of Mechanical Engineering of the University of Aveiro; the sensors are indicated by the Ellipses. Fig. 8 Pixel coordinates errors between projected (expected) chessboard corners and the ground truth indexes, from top left camera to top right camera, for each collection, before the optimization procedure (a) and after the optimization procedure (b). Fig. 9 Flowchart representing the results comparison structure. Each ellipse represents a JSON file and each rectangle identifies a programmed application. Fig. 10 Pixel coordinate errors between projected and expected chessboard corners. The kalibr results are not visible because of the selection of the axes range. Fig. 11 Average error per sensor during a full system calibration procedure. Errors for cameras in pixels, for LiDARs in metres. Fig. 12 Left laser (dots surrounded by red circles) and right laser (dots surrounded by green circles) data overlaid onto a representation of the chessboard, taking in consideration the pose of the chessboard and the LiDARs as estimated by the calibration for one particular collection: (a) and (b) the start of the optimization (initial guess); (c) and (d) the end of the optimization (calibration results). Table 1 Average errors and standard deviations along both directions, before and after the optimization. Values in pixels. Values Average error Standard deviation Initial Final Initial Final x error 2.25 1.64 2.68 1.69 y error 17.09 0.53 3.32 0.62 Both 17.34 1.83 8.71 1.51 Table 2 Average errors and standard deviations, in pixels, for the distances in x axis and y axis, for the proposed approach, the OpenCV stereo calibration and the kalibr calibration method. For kalibr, two datasets were used for training: the train dataset, which was also used to train all other approaches, and the test dataset, which was used to evaluate all approaches. Values in pixels. Calibration method Average error Standard deviation x y x y Proposed approach (left) 2.218 1.633 1.223 0.584 Proposed approach (right) 2.080 1.797 1.253 0.608 OpenCV stereo calibrate 1.251 0.903 1.509 0.767 Kalibr (train) [26] 67.383 8.887 0.832 1.722 Kalibr (test) [26] 1.187 17.999 1.369 2.225 A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach Miguel Oliveira a b Afonso Castro b ⁎ Tiago Madeira a Eurico Pedrosa a Paulo Dias a c Vítor Santos a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal, Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro b Department of Mechanical Engineering, University of Aveiro, Portugal Department of Mechanical Engineering, University of Aveiro Portugal Department of Mechanical Engineering, University of Aveiro c Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro ⁎ Corresponding author. This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles, which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups. Keywords Extrinsic calibration ROS Optimization Bundle adjustment Intelligent vehicles OpenCV 1 Introduction Intelligent vehicles require a considerable amount of on-board sensors, often of multiple modalities (e.g. camera, Light Detection And Ranging (LiDAR), etc.) in order to operate consistently. The combination of the data collected by these sensors requires a transformation or projection of data from one sensor coordinate frame to another. The process of estimating these transformations between sensor coordinate systems is called extrinsic calibration. An extrinsic calibration between two sensors requires an association of data from one sensor to the data of another. By knowing these data associations, an optimization procedure can be formulated to estimate the parameters of the transformation between those sensors that minimizes the distance between associations. Most calibration approaches make use of calibration patterns, i.e., objects that are robustly and accurately detected by distinct sensor modalities. Although there have been many solutions available in the literature, on the topic of calibration, there is no straightforward solution for the calibration of multiple sensors in intelligent vehicles, or robots in general. There are multiple factors that contribute to this lack of solutions. The majority of works on calibration focus on sensor to sensor pairwise calibrations: between only cameras [1–5] or between cameras and LiDARs [6–10]. When considering pairwise combinations of sensors, there are several possibilities, according to the modality of each of the sensors in the pair. Most of them have been addressed in the literature: RGB to RGB camera calibration [1–5,7]; RGB to depth camera (RGB-D cameras) calibration [11–16]; camera to 2D LiDAR [6,7,10,15,17–20]; 2D LiDAR to 3D LiDAR [8]; camera to 3D LiDAR [10,21,22]; and camera to radar [23]. Nonetheless, all these approaches have the obvious shortcoming of operating only with a single pair of sensors, which is not directly applicable to the case of intelligent vehicles, or more complex robotic systems in general. To be applicable in those cases, pairwise approaches must be arranged in a graph-like sequential procedure, in which one sensor calibrates with another, that then relates to a third sensor, and so forth. Another option is to establish one sensor as the reference sensor and link all other sensors to it. In this case, the graph of transformations between sensors results in a one level pyramid, which contains the reference sensor on top and all other sensors at the base. One example is [7], in which a methodology for calibrating the ATLASCAR2 autonomous vehicle [24] is proposed, wherein all sensors are paired with a reference sensor. Sequential pairwise approaches have three major shortcomings: (i) transformations are estimated using only data provided from the selected sensor tandem, despite the fact that data from additional sensors could be available and prove relevant to the overall accuracy of the calibration procedure; (ii) sensitivity to cumulative errors, since the transformations are computed in a sequence (in fact, this does not occur in [7], since the pose of a specific sensor only depends on the reference sensor pose); (iii) structure of transformation graph is enforced by the nature of the calibration procedure, rather than being defined by the preference of the programmer, which could compromise some robot functionalities. Fig. 1 shows a conceptual example in which these problems are visible. There are a few works which address the problem of calibration from a multi-sensor, simultaneous optimization, perspective. In [25], a joint objective function is proposed to simultaneously calibrate three RGB cameras with respect to an RGB-D camera. Authors report a significant improvement in the accuracy of the calibration. In [26], an approach for joint estimation of both temporal offsets and spatial transformations between sensors is presented. This approach is one of few that is not designed for a particular set of sensors, since its methodology does not rely on unique properties of specific sensors. It is able to calibrate systems containing both cameras and LiDARs. Moreover, the approach does not require the usage of calibration patterns for the LiDARs, using the planes present in the scene for that purpose. In [27], a joint calibration of the joint offsets and the sensors locations for a PR2 robot is proposed. This method takes sensor uncertainty into account and is modelled in a similar way to the bundle adjustment problem. Our approach is similar to [27], in the sense that we also employ a bundle adjustment-like optimization procedure. However, our approach is not focused on a single robotic platform, rather it is a general approach that is applicable to any robotic system, which also relates it with [26]. This paper is an extension of [28], where the general approach was originally proposed. This extension focuses on the comparison of this work against state of the art calibration approaches, i.e. methodologies provided by Open Source Computer Vision Library (OpenCV) [29] as well as the calibration method from [26]. Robot Operating System ROS [30] based architectures are the standard when developing robots. There are several ROS based calibration packages available. 1 1 2 2 3 3 In addition, some approaches are well integrated with ROS since the input data for the calibration is provided as a rosbag file. Despite this, no approach provides a complete solution for the calibration of intelligent vehicles. Thus, the seamless integration with ROS became a core component of the proposed approach. To that end, the proposed calibration procedure is self-configured using the standard ROS robot description files, the Unified Robot Description Format (URDF), and provide several tools for sensor positioning and data labelling based on RVIZ interactive markers. The remainder of this paper is organized as follows: Section 2 describes the methodologies used to set up an optimization procedure which calibrates the system. In this section, several auxiliary tools for labelling data and positioning sensors are described; Section 3 details the optimization procedure and how it is cast as a bundle adjustment problem; Section 4 provides comparisons with established OpenCV calibration methodologies; finally, Section 5 provides conclusions and future work. 2 ROS based calibration setup A schematic of the proposed calibration procedure is displayed in Fig. 2. It consists of five components: configuration; interactive positioning of sensors; interactive labelling of data; collection of data; and finally, the optimization procedure. Each component will be described in detail in the following subsections. 2.1 Configuration of the calibration procedure Robotic platforms are described in ROS using a xml file called URDF. We propose to extend the URDF description files of a robot in order to provide information necessary for configuring how the calibration should be carried out. A new URDF element, named calibration, is introduced specifically for the purpose of calibrating. Each calibration element describes a sensor to be calibrated. The element contains information about the calibration parent and child links, which define the partial transformation that is optimized. 2.2 Interactive positioning of sensors Optimization procedures suffer from the known problem of local minima. This problem tends to occur when the initial solution is far from the optimal parameter configuration. Thus, it is expected that, by ensuring an accurate first guess for the sensor poses, there is less likelihood of falling into local minima. We propose to solve this problem in an interactive fashion: the system parses the URDF robot description and creates an rviz interactive marker associated with each sensor. It is then possible to move and rotate the interactive markers. This provides a simple, interactive method to manually calibrate the system or, alternatively, to easily generate plausible first guesses for the poses of the sensors. Real time visual feedback is provided by the observation of the bodies of the robot model (e.g. where a LiDAR is placed w.r.t. the vehicle), and also by the data measured by the sensors (e.g. how well the measurements from two LiDARs match). An example of this procedure can be watched at 2.3 Interactive data labelling Since the goal is to propose a calibration procedure that operates on multi-modal data, a calibration pattern adequate to all available sensor modalities must be selected. A chessboard pattern is a common calibration pattern, in particular for RGB and RGB-D cameras. To label image data, one of the many available image-based chessboards detectors is used (Find Chessboard Corners OpenCV function 4 4 ). In the case of 2D LiDAR data, it is not possible to robustly detect the chessboard, since there are often multiple planes in the scene derived from other structures, such as walls and doors. To solve this, we propose an interactive approach which requires minimal user intervention: rviz interactive markers are positioned along the LiDAR measurement planes and the user drags the marker to indicate where in the data the chessboard is observed. This is done by clustering the LiDAR data, and selecting the cluster which is closer to the marker. This interactive procedure is done only once, since it is then possible to track the chessboard robustly. Fig. 3 shows an example of the labelling of 2D LiDAR data. This interactive data labelling procedure is showcased in 2.4 Collecting data Usually, different sensors stream data at different frequencies. However, to compute the associations between the data of multiple sensors, temporal synchronization is required. While some approaches require hardware synchronization to operate [26], in the current method this is solved trivially by collecting data (and the corresponding labels) at user defined moments in which the scene has remained static for a certain period of time. In static scenes, the problem of data desynchronization is not observable, which warrants the assumption that for each captured collection the sensor data is synchronized. We refer to these snapshot recordings of multi-sensor data as data collections. This information is stored in a JSON file that will be read by the optimization procedure afterwards. The JSON file contains abstract information about the sensors, such as the sensor transformation chain, among others, and specific information about each collection, i.e., sensor data, partial transformations, and data labels. It is important to note that the set of collections should contain as many different poses as possible. As such, collections should preferably have different distances and orientations w.r.t. the chessboard so that the calibration becomes more reliable. This concern is common to the majority of calibration procedures. 2.5 Sensor poses from partial transformations The representation of a complex, multi-sensor system requires the creation of a transformation graph. For this purpose, ROS uses a graph tree referred to as tf tree [31]. One critical factor for any calibration procedure is that it should not change the structure of that existing tf tree. The reason for this is that the tf tree, derived from the URDF files by the robot state publisher, 5 5 also supports additional functionalities, such as robot visualization or collision detection. If the tf tree changes due to the calibration, those functionalities may be compromised or require some redesigning. To accomplish this, we propose to compute the pose of any particular sensor (i.e., the transformation from the Reference Link, also known as World, to that Sensor) as an aggregate transformation A , obtained after the chain of transformations for that particular sensor, extracted from the topology of the tf tree: (1) where i T i + 1 represents the partial transformation from the ith to the i + 1 link, and p a r e n t and c h i l d are the indexes of the calibration parent and calibration child links in the sensor chain, respectively. Our approach preserves the predefined structure of the tf tree, since, during optimization, only one partial transformation contained in the chain is altered (the one in blue in Eq. (1)). This computation is performed within the optimization’s cost function. Therefore, a change in one partial transformation affects the global sensor pose, and consequently, the error to minimize. The optimization may target multiple links of each chain, and is agnostic to whether the remaining links are static or dynamic, since all existing partial transformations are stored for each data collection. To the best of our knowledge, our approach is one of few which maintains the structure of the transformation graph before and after optimization. This is a feature that is often overlooked, yet it is of critical practical importance for the selection of a calibration framework. Taking the example of Fig. 4, consider that Sensor 1 is mounted on top of a pan and tilt unit, where LinkA T Link C corresponds to the pan movement, and LinkC T Sensor 1 represents the tilt motion. For this particular case, Eq. (1) becomes: (2) where I is the identity matrix (since there are no prior links), and the pan and tilt motions are coloured in red to denote that these transformations are dynamic and, as a consequence, may also change from collection to collection. Another example is the one of Sensor 2: it contains an aggregate transformation that also includes the partial transformation optimized w.r.t. Sensor 1, resulting in the following aggregate transformation: (3) which is also directly derived from Eq. (1). Such complex arrangements are seldom tackled by a single calibration approach, even less in a transparent way by the same general formalism. The proposed optimization of partial transformations achieves this goal. We consider the ability to preserve the structure of the tf tree as a key feature of the proposed framework: from a practical standpoint, since it facilitates the integration into ROS, both before and after the optimization; and, moreover, from a conceptual perspective, since this formulation is general and adequate to handle most calibration scenarios. 3 Calibration procedure The goal of general optimization procedures is to find the parameter configuration that results in the smallest function value. This function, which depends on the optimization parameters Φ is known as the objective function. For the purpose of calibrating the multi-modal sensors of a robotic platform, like the ATLASCAR2 intelligent vehicle, the objective of this optimization is to estimate the pose of each sensor relatively to a reference link (base link for ATLASCAR2 case). 3.1 Optimization parameters An extrinsic calibration translates into a pose estimation. Thus, the set of parameters to optimize, defined as Φ , must contain parameters that together define the pose of each sensor. As discussed in the beginning of Section 2, we propose to maintain the initial structure of the transformation graph, and thus only optimize one partial transformation per sensor. In the example of Fig. 4, these partial transformations are denoted by solid arrows. Since the usage of camera sensors is considered, it is also possible to introduce the intrinsic parameters of each camera in the set Φ . Our goal is to define an objective function that is able to characterize sensors of different modalities. Pairwise methodology for devising the cost function results in complex graphs of exhaustive definition of relationships. For every existing pair of sensors, these relationships must be established according to the modality of each of the sensors, and, although most cases have been addressed in literature, as discussed in Section 1, a problem of scalability remains inherent to such a solution. To address this issue, we propose to structure the cost function in a sensor to calibration pattern paradigm, similar to what is done in bundle adjustment. That is, the positions of 3D points in the scene are jointly refined with the poses of the sensors. These 3D points correspond to the corners of the calibration chessboard. What is optimized is actually the transformation that takes these corners from the frame of reference of the chessboard to the world, for every collection. All variables must have some initial value, so that the optimizer may compute the first error, and start to refine the values in order to obtain the minimum of the cost function. The first guess for each chessboard is obtained by computing the pose of a chessboard detection in one of the cameras available. The output is a transformation from the chessboard reference frame to the camera’s reference frame. Since we already have the first guess for the poses of each sensor, calculated as an aggregate transformation A (see Eq. (1)), to obtain the transformation from the chessboard reference frame to the world (an external and absolute frame), the following calculation is applied: (4) chess T world = camera A world ︷ Eq. (1) ⋅ chess T camera ︷ chess detection , where chess and camera refer to chessboard and camera coordinate frames, respectively. Thus, the set of parameters to be optimized Φ , contains the transformation represented in Eq. (4), for each collection, along with the poses of each sensor: (5) Φ = [ x m = 1 , r m = 1 , i m = 1 , d m = 1 , … , x m = M , r m = M , i m = M , d m = M , ︷ Cameras x n = 1 , r n = 1 , … , x n = N , r n = N , ︷ LiDARs … , ︷ Other modalities x k = 1 , r k = 1 , … , x k = K , r k = K ︷ Calibration object ] where m refers to the mth camera, of the set of M cameras, n refers to the nth LiDAR, of the set of N LiDARs, k refers to the chessboard detection of the kth collection, contained in the set of K collections, x is a translation vector [ t x , t y , t z ] , r is a rotation represented through the axis/angle parameterization [ r 1 , r 2 , r 3 ] (where the vector [ r 1 , r 2 , r 3 ] is used to represent the axis and its norm the angle), i is a vector of a camera’s intrinsic parameters [ f x , f y , c x , c y ], and d is a vector of camera’s distortion coefficients [ d 0 , d 1 , d 2 , d 3 , d 4 ]. The initial estimate for the intrinsic parameters is obtained using any intrinsic camera calibration tool. The axis/angle parameterization was chosen because it has 3 components and 3 degrees of freedom, making it a fair parameterization, since it does not introduce more numerical sensitivity than the one inherent to the problem itself [32]. At this point, there are six parameters per sensor, related to the pose of each one, to be enhanced. These values compose the geometric transformation that will be calibrated. The cost function will compute the residuals based on an error (in pixels for RGB cameras and in millimetres for LiDARs) between the re-projected position of the chessboard, estimated by all transformations, and the position of the calibration pattern detected by each sensor. 3.2 Objective function The cost function for this optimization, F ( Φ ) , can be thought of as the sum of several sub-functions that compose a vector function, where, for every modality of sensor added to the calibration, a new sub-function is defined accordingly, which allows for the minimization of the error associated with the pose of sensors of that modality. Thus, the optimization procedure can be defined as: (6) arg min Φ F ( Φ ) = 1 2 ∑ i f i ( Φ i 1 , … , Φ i k ) 2 where f i ( ⋅ ) is the objective sub-function for the i th sensor with the respective parameters block { Φ i 1 , … , Φ i k } , being k the parameters number of each objective sub-function. In other words, the scalar cost function of this optimization is the sum of the squares of the returned values from a vector function, divided by two. Each different sensor has an inherent sub-function, that depends on the sensor modality. The value of all these sub-functions is a vector with the errors (residuals) associated to the re-projection of the calibration pattern points. Since for the ATLASCAR2 intelligent vehicle we are considering four sensors (two cameras and two 2D LiDARs), the objective function is composed by the vector values of four sub-functions, two of each type. Each different sub-function is detailed in the next sub-sections. 3.2.1 Camera sub-function When the sensors are cameras, their calibration is performed as a bundle adjustment [33], and as such, the sub-function created is based on the average geometric error corresponding to the image distance between a projected point and a detected one. The 3D points corresponding to the corners of the calibration chessboard are captured by one or more cameras in each collection. Each camera is defined by its pose relative to a reference link and intrinsic parameters. After the desired acquisitions are completed, the 3D points are projected from the world into the images and the 2D coordinates are compared to the ones obtained by detection of the calibration pattern in the corresponding images. The positions of the 3D points in the world are obtained by applying the transformation described in Eq. (4) to the chessboard corner points defined in the chessboard detection’s reference frame. The goal of this cost sub-function is to adjust the initial estimation of the camera parameters and the position of the points, in order to minimize the average reprojection error f camera , given by: (7) f camera = ℓ 2 ( x c = 1 , x ˆ c = 1 ) ℓ 2 ( x c = 2 , x ˆ c = 2 ) ⋯ ℓ 2 ( x c = C , x ˆ c = C ) ⊺ where ℓ 2 is the Euclidean distance between two vectors, c denotes the index of the chessboard corners, x c denotes the pixels coordinates of the measured points (given by chessboard detection), and x ˆ c are the projected points, given by the relationship between a 3D point in the world and its projection on an image plane. By knowing the real size of the chessboard squares, the 3D coordinates of all corners relatively to the chess frame can be inferred. Note that the z value will be, for every point, zero, since the chessboard is in the XoY plane. After obtaining the 3D coordinates of all corners in reference to the chessboard frame, the objective function computes the coordinates of the points relatively to the camera link through multiplying by the geometric transformation between the base link (reference frame for the ATLASCAR2 example) and the calibration pattern frame and by the transform between the camera link and the base link: (8) p c a m e r a = camera T world ⋅ world T chess ⋅ p c h e s s where p c h e s s refers to the x , y , z coordinates of a chessboard corner, defined in the local chessboard coordinate frame, and p c a m e r a refers to the x , y , z coordinates of the same chessboard corner, defined in the camera link. In fact, both p c h e s s and p c a m e r a are the homogenized matrices of the coordinates so that Eq. (8) is mathematically correct. Note that the parameters to be optimized define the chessboard to world transformation, and that the world to camera transformation is computed from an aggregate of several partial transformations, one of which is defined by other parameters being optimized; furthermore, the intrinsic matrix is dependent on parameters which are accounted for in the optimization. As is expected, the re-projected points become closer to the ground truth corners during the optimization procedure. Fig. 5 shows the difference between the initial position of the chessboard corners, projected from the 3D world to the camera image, and the final position of these same projected points, after the optimization has been completed. It is possible to observe that the pixels corresponding to the projection of the final position of the points (dots in Fig. 5) almost perfectly match the ground truth points (squares in Fig. 5). 3.2.2 Laser sub-function Finally, for the case of 2D LiDARs, the sub-function only considers the two border points, among all the measurements that are related to the chessboard plane, to compute the error associated to the pose of the LiDAR and the chessboard. In order to calculate the residuals that this cost sub-function should return, the detected points’ 3D coordinates from the chessboard frame are required. During the calibration setup stage, when the information of a time stamp is saved, the ranges of all measurements that the LiDAR is detecting are stored, as well as the information about this same LiDAR and the indexes of the ranges that correspond to the plane where the chessboard is. With the optimization parameters of the chessboard pose and the LiDAR pose (computed accordingly to Eq. (1)), both relative to the base link, the 3D coordinates of each labelled measurement of the point cloud in the chessboard frame are known: (9) x y z 1 chess = chess T world ⋅ world T lidar ⋅ x y z 1 lidar . Finally, with the coordinates from the chessboard frame, of both the first and the last points of the cluster extracted in the labelling stage, it is possible to compute the error evaluated by this cost sub-function. The error is based on the distance between each one of the limit points (the first and the last index) of the selected ranges and the chessboard surface boundaries. There are two computed distances for each point: orthogonal and longitudinal. The orthogonal distance is the z absolute value of the coordinates, in the calibration pattern frame, of the LiDAR data measurement. In an ideal setting, the z value should be zero, since the chessboard plane is on the XoY plane. This is why any value different from zero means that the optimization parameters (sensor pose and chess pose) are not yet correct. The longitudinal distance is the Euclidean distance between the x and y coordinates, in the calibration pattern frame, of the LiDAR data measurement and the x and y coordinates of the closest point that belong to the limit of the physical board that is being detected. In order to compute this distance, it is essential to create a group of points that represent the boundaries of the chessboard. By knowing the size of the board, the size of each chess square, and that the chess frame origin matches with the first (top left) chess corner, the coordinates were calculated and the points of the board boundaries were manually defined. The size of the border between the chess corner grid and the end of the physical chessboard had to be measured so that this step could be implemented. In Fig. 6, we can see the grid of the chess corners and a line around it: that line marks the limit of the board. This solid line has some points within it, which are going to be compared to the LiDAR data measured ones. Again, the optimizer will search for the closest limit point to each one of the studied LiDAR data measurement coordinates and then compute the longitudinal distance. Thus, the LiDAR sub-function f lidar is defined as: (10) f lidar = | z 1 chess | ℓ 2 ( p 1 b o a r d l i m i t , p 1 c h e s s ) | z 2 chess | ℓ 2 ( p 2 b o a r d l i m i t , p 2 c h e s s ) ⊺ where (11) p b o a r d l i m i t = x y boardlimit , (12) p c h e s s = x y chess , and z chess is the third coordinate value of the range measurement points transformed to the chessboard’s coordinate frame. 3.3 Sensors pose calibration: Optimization The cost function F ( Φ ) from Eq. (6) is minimized using a least-squares approach. 6 6 In this work we used the least-squares solver provided by SciPy: Least-squares finds a local minimum of a scalar cost function, with bounds on the variables, by having an m-dimensional real residual function of n real variables. As such, we choose this minimization approach as it is the best fit for our problem. 4 Results To assess the performance of the proposed calibration approach, we used an intelligent vehicle as test bed. The ATLASCAR2 [24] is an electric vehicle (Mitsubishi i-MiEV) with several sensors onboard. In this work four sensors were considered: two 2D LiDARs and two RGB cameras. Thus, two different modalities of sensors are used. The sensors are designated as follows: left laser, right laser, top left camera and top right camera. Fig. 7 shows the ATLASCAR2 vehicle. The proposed approach is used to calibrate the four selected sensors simultaneously. Nonetheless, as discussed above, there are no approaches which provide an off-the-shelf multi-sensor multi-modal calibration. As such, in order to evaluate this approach, we provide comparisons against other pairwise methodologies, which are abundant in the field, as was mentioned in Section 1. Note that, in the following comparisons, the results given by the proposed approach for a particular pair of sensors are obtained using a complete system calibration. On the other hand, the alternative methodologies calibrate a single pair of sensors. In this sense, the comparison methodology is not favourable to the proposed approach, since the other approaches are specialized in the case being evaluated. In the following lines, two tests are detailed: the first is a camera-to-camera evaluation which compares several calibration methods in a pairwise fashion, while the second characterizes the proposed joint optimization over time providing global metrics. 4.1 Camera to camera The methodology used to compute the error of the calibrated poses of the top right camera and the top left camera is based on the distance between pixel coordinates. These coordinates are, on the one hand, the detected chessboard corners (ground truth) of the top right camera and, on other hand, the coordinates of the projections of those corners, to the top left camera, using the transformation between the cameras which is the output of the calibration. To transform pixels from one camera to the other, we start from the projection of the 3D world coordinates to the image of a camera: (13) p = K ⋅ R t ⋅ P where P refers to the 3D homogeneous coordinates of the corners as viewed in the chessboard frame; p is a vector composed by the u , v and w values, in which: x pixel = u ∕ w and y pixel = v ∕ w , allowing for the direct extraction of image coordinates from this vector; R t is the non-homogeneous geometric transformation matrix from the camera frame to the chessboard frame, K represents the camera’s intrinsic matrix. Eq. (13) can be applied to each camera separately. Since the 3D chessboard corner coordinates are defined in the chessboard frame, the value of Z will be 0 for all corners, because they all lie on the XoY plane: Z chess = 0 . As a result, Eq. (13) may be simplified as follows: (14) u v w = f x 0 c x 0 f y c y 0 0 1 ⋅ r 11 r 12 t x r 21 r 22 t y r 31 r 32 t z ⋅ X Y 1 chess corners , which is equivalent to: (15) p camera = K ⋅ camera T chess ′ ⋅ P chess , where the geometric transformation matrix camera T chess ′ is a portion of the camera T chess matrix, as detailed in Eq. (15). We use (15) for both cameras, and relate both expressions by the 3D coordinates of the chessboard corners (which are the same for both cameras), resulting in: (16) p cam2 = K cam2 ⋅ cam2 T chess ′ ⋅ cam1 T chess ′ -1 ⋅ K cam1 -1 ⋅ p cam1 where cam1 and cam2 refer to the top left and top right cameras, respectively. This formulation provides the relation between image coordinates of the chessboard corners for both camera images of each collection. Notice, however, that calibration methods output the transformation between sensors, in this case between cameras, while Eq. (16) requires transformations from the cameras to the chessboard. Some approaches, as for example the proposed approach, also estimate the pose of the chessboards (see parameters of the calibration objects in Eq. (5)). Thus, at first glance, one could think of using these transformations directly in Eq. (16). However, these chessboard poses are estimated for a given training dataset, and cannot be accurately used for other datasets. Moreover, as said before, not all calibration approaches output the pose of the chessboards (e.g. OpenCV stereo calibrate). Instead, calibration approaches provide the transformation between cameras. By arbitrarily selecting one camera from which the chessboard pose is determined through the solvePNP function (we have used cam1, but tests have shown that the alternative provided similar results) and using the transformation cam1 T cam2 estimated by the calibration approaches, it is possible to determine the transformation of the other camera to the chess, as follows: (17) cam2 T chess = cam1 T cam2 -1 ︷ calibration ⋅ cam1 T chess ︷ s o l v e P n P . From this expression the partial matrices cam1 T chess ′ and cam2 T chess ′ are derived. Then, we apply Eq. (16) to compute the corner coordinates on the top right camera image, as projected from the detection of the top left camera image. The error is computed by measuring the difference between expected and projected corner coordinates on the top right camera image: (18) x error y error top right camera = x y projected − x y e x p e c t e d Fig. 8 shows the errors related to the projection of the chessboard corners from the top left camera to the top right camera before and after the optimization of the position and orientation parameters of the cameras. These results can be better evaluated through the calculated mean error and standard deviation values, as shown in Table 1: Next, the proposed approach was compared with other calibration methodologies: stereo calibrate function 7 7 provided by OpenCV and the kalibr calibration method [26]. The kalibr method requires hardware synchronization and receives a bag file as input, unlike the other approaches, which make use of the datasets collected as described in Section 2. Because of this, two different calibrations are provided for kalibr: the first in which the training dataset is used, and a second which uses the test dataset, i.e. the dataset which is used to evaluate all approaches. The results for the proposed approach are presented for two different scenarios, taking into account the camera (top left or top right) which was used for creating the initial values of the chessboard poses (see Eq. (4)). These two variants are used to assess the impact of the selection of the camera for providing initial estimates on the final calibration estimates. In this experiment, calibration of a pair of sensors composed by the top left camera (cam1) and the top right camera (cam2) is evaluated. The dataset used for running the calibration procedures, i.e. the training dataset, is composed of 27 collections (27 images per camera). The test dataset to be used to evaluate the estimated sensor-to-sensor transformations has 15 collections. Images from the train and test datasets are similar. In order to make this comparison fair, the three distinct calibration procedures are given the exact same information. Moreover, the procedures were implemented in such a way that the returned estimated parameters, and remaining data, are organized similarly to the proposed approach. This means that each distinct approach will output a final JSON file with the estimated position and orientation of the sensors. Taking all this into account, a specific tool was created for visualizing the results of the different calibration procedures named Results Visualization, which imports the JSON files outputted by each one of the several approaches. Fig. 9 shows a flowchart of this framework, built specifically to compare the proposed methodology with standard pairwise approaches. Fig. 10 shows the pixel errors of the three distinct calibration approaches. Note that the methodology described above is computed separately for each collection. The performance of the kalibr method is clearly below the other two. We suspect there are several factors contributing to this. The first is that this method requires hardware synchronization, not ensured in the used datasets. Another is that the kalibr method reads data from a bag file and thus we have no control over the images which are selected to run the calibration. In an attempt to address the problem, we ran a kalibr calibration using the test dataset as input (kalibr test in Table 2). It may be that the selection of images is not working well, which in turn causes a poor calibration performance. Also, due to the limited duration of the bag file of the experiment, only around 20 to 30 images have been selected, a total similar to the datasets we have used. It could be that kalibr requires a larger number of images. In any case, we believe these results are not representative of kalibr. The proposed approach and the stereo calibration display similar errors, which means that the proposed approach is on par with a state of the art calibration approach. Moreover, the largest error of each of the compared methodologies occurs for the same collection (in this case, for collection 4, the dark green). This also shows a high degree of consistency between the proposed approach and the stereo calibration. Table 2 shows the average error and the standard deviation of all tested calibration approaches. These results exhibit reprojection errors in the order of some pixels, which is the normal range of values for these methods and experimental setups. Moreover, the obtained values are very similar between the proposed approach and the stereo calibrate. As such, results show that the proposed approach is able to calibrate all sensors on-board the ATLASCAR2 using a single optimization procedure. Furthermore, the accuracy of this joint calibration framework we propose is the same as when using state of the art pairwise calibration methods. 4.2 Complete system calibration This section will provide results concerning a full system calibration. Note that, in Section 4.1, the results focus only on the evaluation of the camera sensors, despite the fact that the complete system was also calibrated. In this section, the goal is to characterize all the sensors and not just the cameras. Because of this, it is not possible to compare the full system calibration (taking into account all the sensors) with other approaches since, as described in Section 1, there is no calibration framework available, in particular a multi-sensor and multi-modal one. Fig. 11 shows the average error per sensor over the cost function evaluations, for a full system calibration test. The average error per sensor is estimated after the several error measurements computed for each particular sensor. For example, a camera cost sub-function returns as many residuals as chessboard corners (see Eq. (7)), while the LiDAR sub sub-function returns four measurements (see Eq. (10)). The average error for camera sensors is provided in pixels, while for LiDAR sensors the error is in metres. The first takeaway is that the optimization is working as intended, since the minimization of the errors of all sensors can be observed. This shows that the multi-sensor, multi-modal optimization (the joint minimization of all the sensor’s parameters) is in fact possible. Furthermore, the final errors values (after the optimization is finished) are around a few pixels for camera sensors (2.8 and 3.3 pixels for the top left camera and the top right camera, respectively), and around a few centimetres for the LiDARs (0.017 and 0.033 metres for the left laser and right laser, respectively). These values are on par with the state of the art, even when considering calibration results for pairwise approaches. Another important insight is the reason why the top left camera residual starts with a low error: Section 3.1, in particular Eq. (4), described how the initial poses of the chessboards were estimated using one camera sensor, which is arbitrarily selected. In this test, the top left camera was selected to produce the initial chessboard pose estimates. Thus, since the corner detection in the top left camera images are used to compute the initial chessboard poses, the reverse procedure of projecting the chessboard corners back to the image results in corner coordinates that are naturally very close to the detections at the beginning of the optimization. Fig. 12 shows the data from the LiDARs along with a representation of the chessboard. For a better visualization, a single collection is displayed. The four images correspond to different stages of the optimization process. It is possible to see an improvement during the calibration (i.e. from Fig. 12(a) and (b), the beginning of the optimization, to (c) and (d), the end of the optimization, since the data from both LiDARs is much closer to the chessboard plane (c) and (d) when compared to (a) and (b). This shows that the proposed approach is also capable of calibrating LiDARs within a joint optimization framework. 5 Conclusions and future work This paper proposes an extrinsic calibration methodology that is general, in the sense that the number of sensors and their modalities are not restricted. The approach is compliant with the ROS framework, having also the advantage of not altering the tf tree. To accomplish this, the problem is formalized as an optimization procedure of a set of partial transformations, which accounts for specific links in the transformation chains of the sensors. Additionally, the work contributes with a set of interactive tools for the positioning of the sensors and labelling of data, which facilitate the creation of a first guess and significantly ease the calibration procedure. Results show that the proposed approach is able to achieve similar accuracy when compared to state of the art methodologies, implemented in OpenCV. Moreover, these results are obtained by performing a complete calibration of the system, rather than one of a single pair of sensors. In other words, the proposed approach calibrates all sensors at once, with similar performance as the pairwise approaches. This confirms that the proposed approach is adequate for the calibration of complex robotic systems, as are most intelligent vehicles. Future work will focus on the extension to additional sensor modalities, e.g., 3D LiDARs, RGB-D cameras, Radio Detection And Ranging (RaDAR), etc. Given the scalability of the proposed framework, it is expected that this should be more or less straightforward. Finally, the ultimate goal is to produce a multi-sensor, multi-modal calibration package that may be released to the community. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This Research was funded by National Funds through the FCT — Foundation for Science and Technology, in the context of the project UIDB/00127/2020, as well as CYTED/TICs4CI — Aplicaciones TICS para Ciudades Inteligentes . References [1] Mueller G.R. Wuensche H. Continuous stereo camera calibration in urban scenarios 2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC) 2017 1 6 10.1109/ITSC.2017.8317675 G. R. Mueller, H. Wuensche, Continuous stereo camera calibration in urban scenarios, in: 2017 IEEE 20th Int. Conf. on Intelligent Transportation Systems (ITSC), 2017, 1–6. DOI: 10.1109/ITSC.2017.8317675. [2] Wu L. Zhu B. Binocular stereovision camera calibration 2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA) 2015 2638 2642 10.1109/ICMA.2015.7237903 L. Wu, B. Zhu, Binocular stereovision camera calibration, in: 2015 IEEE Int. Conf. on Mechatronics and Automation (ICMA), 2015, 2638–2642. DOI: 10.1109/ICMA.2015.7237903. [3] Rou Su L. JingLiang Zhong B. QiaoLiang Li SuWen Qi HuiSheng Zhang TianFu Wang An automatic calibration system for binocular stereo imaging 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC) 2016 896 900 10.1109/IMCEC.2016.7867340 Rou Su, JingLiang Zhong, QiaoLiang Li, SuWen Qi, HuiSheng Zhang, TianFu Wang, An automatic calibration system for binocular stereo imaging, in: 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conf. (IMCEC), 2016, 896–900. DOI: 10.1109/IMCEC.2016.7867340. [4] Ling Y. Shen S. High-precision online markerless stereo extrinsic calibration 2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) 2016 1771 1778 10.1109/IROS.2016.7759283 Y. Ling, S. Shen, High-precision online markerless stereo extrinsic calibration, in: 2016 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2016, 1771–1778. DOI: 10.1109/IROS.2016.7759283. [5] Dinh V.Q. Nguyen T.P. Jeon J.W. Rectification using different types of cameras attached to a vehicle IEEE Trans. Image Process. 28 2 2019 815 826 10.1109/TIP.2018.2870930 V. Q. Dinh, T. P. Nguyen, J. W. Jeon, Rectification using different types of cameras attached to a vehicle, IEEE Trans. on Image Processing 28 (2) (2019) 815–826. DOI: 10.1109/TIP.2018.2870930. [6] Vasconcelos F. Barreto J.P. Nunes U. A minimal solution for the extrinsic Calibration of a Camera and a laser-rangefinder IEEE Trans. Pattern Anal. Mach. Intell. 34 11 2012 2097 2107 F. Vasconcelos, J. P. Barreto, U. Nunes, A minimal solution for the extrinsic calibration of a camera and a laser-rangefinder, IEEE Trans. on Pattern Analysis and Machine Intelligence 34 (11) (2012) 2097–2107. [7] Pereira M. Silva D. Santos V. Dias P. Self calibration of multiple lidars and cameras on autonomous vehicles Robot. Auton. Syst. 83 2016 326 337 M. Pereira, D. Silva, V. Santos, P. Dias, Self calibration of multiple lidars and cameras on autonomous vehicles, Robotics and Autonomous Systems 83 (2016) 326–337. [8] Almeida M. Dias P. Oliveira M. Santos V. 3d-2d laser range finder calibration using a conic based geometry shape Image Analysis and Recognition 2012 312 319 M. Almeida, P. Dias, M. Oliveira, V. Santos, 3d-2d laser range finder calibration using a conic based geometry shape, in: Image Analysis and Recognition, 2012, 312–319. [9] A. Geiger, F. Moosmann, O. Car, B. Schuster, Automatic camera and range sensor calibration using a single shot, in: Proc. IEEE Int. Conf. Robot. Autom., 2012, pp. 3936–3943. [10] Guindel C. Beltrán J. Martín D. García F. Automatic extrinsic calibration for lidar-stereo vehicle sensor setups 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) 2017 1 6 C. Guindel, J. Beltrán, D. Martín, F. García, Automatic extrinsic calibration for lidar-stereo vehicle sensor setups, 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) (2017) 1–6. [11] Kwon Y.C. Jang J.W. Choi O. Automatic sphere detection for extrinsic calibration of multiple rgbd cameras 2018 18th Int. Conf. on Control, Automation and Systems (ICCAS) 2018 1451 1454 Y. C. Kwon, J. W. Jang, O. Choi, Automatic sphere detection for extrinsic calibration of multiple rgbd cameras, in: 2018 18th Int. Conf. on Control, Automation and Systems (ICCAS), 2018, 1451–1454. [12] Khan A. Aragon-Camarasa G. Sun L. Siebert J.P. On the calibration of active binocular and rgbd vision systems for dual-arm robots 2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO) 2016 1960 1965 10.1109/ROBIO.2016.7866616 A. Khan, G. Aragon-Camarasa, L. Sun, J. P. Siebert, On the calibration of active binocular and rgbd vision systems for dual-arm robots, in: 2016 IEEE Int. Conf. on Robotics and Biomimetics (ROBIO), 2016, 1960–1965. DOI: 10.1109/ROBIO.2016.7866616. [13] Basso F. Menegatti E. Pretto A. Robust intrinsic and extrinsic calibration of rgb-d cameras IEEE Trans. Robot. 34 5 2018 1315 1332 10.1109/TRO.2018.2853742 F. Basso, E. Menegatti, A. Pretto, Robust intrinsic and extrinsic calibration of rgb-d cameras, IEEE Trans. on Robotics 34 (5) (2018) 1315–1332. DOI: 10.1109/TRO.2018.2853742. [14] Qiao Y. Tang B. Wang Y. Peng L. A new approach to self-calibration of hand-eye vision systems 2013 Int. Conf. on Computational Problem-Solving (ICCP) 2013 253 256 10.1109/ICCPS.2013.6893596 Y. Qiao, B. Tang, Y. Wang, L. Peng, A new approach to self-calibration of hand-eye vision systems, in: 2013 Int. Conf. on Computational Problem-Solving (ICCP), 2013, 253–256. DOI: 10.1109/ICCPS.2013.6893596. [15] Zhang C. Zhang Z. Calibration between depth and color sensors for commodity depth cameras 2011 IEEE Int. Conf. on Multimedia and Expo 2011 1 6 10.1109/ICME.2011.6012191 C. Zhang, Z. Zhang, Calibration between depth and color sensors for commodity depth cameras, in: 2011 IEEE Int. Conf. on Multimedia and Expo, 2011, 1–6. DOI: 10.1109/ICME.2011.6012191. [16] Chen G. Cui G. Jin Z. Wu F. Chen X. Accurate intrinsic and extrinsic calibration of rgb-d cameras with gp-based depth correction IEEE Sens. J. 19 7 2019 2685 2694 10.1109/JSEN.2018.2889805 G. Chen, G. Cui, Z. Jin, F. Wu, X. Chen, Accurate intrinsic and extrinsic calibration of rgb-d cameras with gp-based depth correction, IEEE Sensors Journal 19 (7) (2019) 2685–2694. DOI: 10.1109/JSEN.2018.2889805. [17] Qilong Zhang G. Pless R. Extrinsic calibration of a camera and laser range finder (improves camera calibration) 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), Vol. 3 2004 2301 2306 10.1109/IROS.2004.1389752 Qilong Zhang, R. Pless, Extrinsic calibration of a camera and laser range finder (improves camera calibration), in: 2004 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), 3, 2004, 2301–2306 3. DOI: 10.1109/IROS.2004.1389752. [18] Häselich M. Bing R. Paulus D. Calibration of multiple cameras to a 3d laser range finder 2012 IEEE Int. Conf. on Emerging Signal Processing Applications 2012 25 28 M. Häselich, R. Bing, D. Paulus, Calibration of multiple cameras to a 3d laser range finder, in: 2012 IEEE Int. Conf. on Emerging Signal Processing Applications, 2012, 25–28. [19] Chen Z. Yang X. Zhang C. Jiang S. Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature 2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI) 2016 448 453 Z. Chen, X. Yang, C. Zhang, S. Jiang, Extrinsic calibration of a laser range finder and a camera based on the automatic detection of line feature, in: 2016 9th Int. Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), 2016, 448–453. [20] Velas M. Spanel M. Materna Z. Herout A. Calibration of rgb camera with velodyne lidar 2014 M. Velas, M. Spanel, Z. Materna, A. Herout, Calibration of rgb camera with velodyne lidar, 2014. [21] Lee G. Lee J. Park S. Calibration of vlp-16 lidar and multi-view cameras using a ball for 360 degree 3d color map acquisition 2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI) 2017 64 69 10.1109/MFI.2017.8170408 G. Lee, J. Lee, S. Park, Calibration of vlp-16 lidar and multi-view cameras using a ball for 360 degree 3d color map acquisition, in: 2017 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI), 2017, 64–69. DOI: 10.1109/MFI.2017.8170408. [22] Levinson J. Thrun S. Automatic online calibration of cameras and lasers Robotics: Science and Systems 2013 J. Levinson, S. Thrun, Automatic online calibration of cameras and lasers, in: Robotics: Science and Systems, 2013. [23] Dezhi Gao J. Duan J. Xining Yang S. Zheng B. A method of spatial calibration for camera and radar 2010 8th World Congress on Intelligent Control and Automation 2010 6211 6215 10.1109/WCICA.2010.5554411 Dezhi Gao, J. Duan, Xining Yang, B. Zheng, A method of spatial calibration for camera and radar, in: 2010 8th World Congress on Intelligent Control and Automation, 2010, 6211–6215. DOI: 10.1109/WCICA.2010.5554411. [24] Santos V. Almeida J. Ávila E. Gameiro D. Oliveira M. Pascoal R. Sabino R. Stein P. Atlascar - technologies for a computer assisted driving system, on board a common automobile 13th Int. IEEE Conf. on Intelligent Transpor Tation Systems 2010 1421 1427 10.1109/ITSC.2010.5625031 V. Santos, J. Almeida, E. vila, D. Gameiro, M. Oliveira, R. Pascoal, R. Sabino, P. Stein, Atlascar - technologies for a computer assisted driving system, on board a common automobile, in: 13th Int. IEEE Conf. on Intelligent Transpor tation Systems, 2010, 1421–1427. DOI: 10.1109/ITSC.2010.5625031. [25] Liao Y. Li G. Ju Z. Liu H. Jiang D. Joint kinect and multiple external cameras simultaneous calibration 2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM) 2017 305 310 10.1109/ICARM.2017.8273179 Y. Liao, G. Li, Z. Ju, H. Liu, D. Jiang, Joint kinect and multiple external cameras simultaneous calibration, in: 2017 2nd Int. Conf. on Advanced Robotics and Mechatronics (ICARM), 2017, 305–310. DOI: 10.1109/ICARM.2017.8273179. [26] Rehder J. Siegwart R. Furgale P. A general approach to spatiotemporal calibration in multisensor systems IEEE Trans. Robot. 32 2 2016 383 398 10.1109/TRO.2016.2529645 J. Rehder, R. Siegwart, P. Furgale, A general approach to spatiotemporal calibration in multisensor systems, IEEE Trans. on Robotics 32 (2) (2016) 383–398. DOI: 10.1109/TRO.2016.2529645. [27] Pradeep V. Konolige K. Berger E. Calibrating a multi-arm multi-sensor robot: A bundle adjustment approach Experimental Robotics: The 12th Int. Symposium on Experimental Robotics 2014 Springer Berlin Heidelberg Berlin, Heidelberg 211 225 10.1007/978-3-642-28572-1˙15 V. Pradeep, K. Konolige, E. Berger, Calibrating a Multi-arm Multi-sensor Robot: A Bundle Adjustment Approach, Springer Berlin Heidelberg, Berlin, Heidelberg, 2014, 211–225. DOI: 10.1007/978-3-642-28572-1˙15. [28] Oliveira M. Castro A. Madeira T. Dias P. Santos V. A general approach to the extrinsic calibration of intelligent vehicles using ros Robot 2019: Fourth Iberian Robotics Conference 2020 Springer International Publishing Cham 203 215 M. Oliveira, A. Castro, T. Madeira, P. Dias, V. Santos, A general approach to the extrinsic calibration of intelligent vehicles using ros, in: Robot 2019: Fourth Iberian Robotics Conference, Springer International Publishing, Cham, 2020, 203–215. [29] Bradski G. The OpenCV Library, Dr. Dobb’s J. Softw. Tools 2000 G. Bradski, The OpenCV Library, Dr. Dobb’s Journal of Software Tools. [30] Quigley M. Conley K. Gerkey B.P. Faust J. Foote T. Leibs J. Wheeler R. Ng A.Y. Ros: an open-source robot operating system ICRA Workshop on Open Source Software 2009 M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, A. Y. Ng, Ros: an open-source robot operating system, in: ICRA Workshop on Open Source Software, 2009. [31] Foote T. Tf: The transform library 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA) 2013 1 6 10.1109/TePRA.2013.6556373 T. Foote, tf: The transform library, in: 2013 IEEE Conference on Technologies for Practical Robot Applications (TePRA), 2013, 1–6. DOI: 10.1109/TePRA.2013.6556373. [32] Hornegger J. Tomasi C. Representation issues in the ML estimation of camera motion Proceedings of the Seventh IEEE International Conference on Computer Vision, Vol. 1 1999 640 647 10.1109/ICCV.1999.791285 J. Hornegger, C. Tomasi, Representation issues in the ml estimation of camera motion, 1, 1999, 640–647 1. DOI: 10.1109/ICCV.1999.791285. [33] Agarwal S. Snavely N. M. Seitz S. Szeliski R. Bundle Adjustment in the Large 2010 29 42 10.1007/978-3-642-15552-9˙3 S. Agarwal, N. Snavely, S. M. Seitz, R. Szeliski, Bundle adjustment in the large, 2010, 29–42. DOI: 10.1007/978-3-642-15552-9˙3. Afonso Castro is a junior web developer. He has an M.Sc. Degree in Mechanical Engineering from the Department of Mechanical Engineering of the University of Aveiro (2019). His master specialization was in robotics and, more precisely, sensor calibration. During his M.Sc. Dissertation development, Afonso Castro has published a research article entitled “A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS” for ROBOT2019: Fourth Iberian Robotics Conference, where he has participated and presented the mentioned work. "
    },
    {
        "doc_title": "2D lidar to kinematic chain calibration using planar features of indoor scenes",
        "doc_scopus_id": "85085949613",
        "doc_doi": "10.1108/IR-09-2019-0201",
        "doc_eid": "2-s2.0-85085949613",
        "doc_date": "2020-08-18",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Calibration procedure",
            "Calibration techniques",
            "Design/methodology/approach",
            "Geometric accuracy",
            "High angular resolutions",
            "Quantitative result",
            "Range measurements",
            "Reconstruction procedure"
        ],
        "doc_abstract": "© 2020, Emerald Publishing Limited.Purpose: 2D laser rangefinders (LRFs) are commonly used sensors in the field of robotics, as they provide accurate range measurements with high angular resolution. These sensors can be coupled with mechanical units which, by granting an additional degree of freedom to the movement of the LRF, enable the 3D perception of a scene. To be successful, this reconstruction procedure requires to evaluate with high accuracy the extrinsic transformation between the LRF and the motorized system. Design/methodology/approach: In this work, a calibration procedure is proposed to evaluate this transformation. The method does not require a predefined marker (commonly used despite its numerous disadvantages), as it uses planar features in the point acquired clouds. Findings: Qualitative inspections show that the proposed method reduces artifacts significantly, which typically appear in point clouds because of inaccurate calibrations. Furthermore, quantitative results and comparisons with a high-resolution 3D scanner demonstrate that the calibrated point cloud represents the geometries present in the scene with much higher accuracy than with the un-calibrated point cloud. Practical implications: The last key point of this work is the comparison of two laser scanners: the lemonbot (authors’) and a commercial FARO scanner. Despite being almost ten times cheaper, the laser scanner was able to achieve similar results in terms of geometric accuracy. Originality/value: This work describes a novel calibration technique that is easy to implement and is able to achieve accurate results. One of its key features is the use of planes to calibrate the extrinsic transformation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sampling strategies to create moving regions from real world observations",
        "doc_scopus_id": "85083039742",
        "doc_doi": "10.1145/3341105.3374019",
        "doc_eid": "2-s2.0-85083039742",
        "doc_date": "2020-03-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Continuous modeling",
            "Interpolation algorithms",
            "Interpolation function",
            "Real-world objects",
            "Sampling strategies",
            "Slice decomposition",
            "Spatial informations",
            "Spatio-temporal data"
        ],
        "doc_abstract": "© 2020 ACM.Spatio-temporal data may be used to represent the evolution of real world objects and phenomena. Such data can be represented in discrete time, which associates spatial information (like position and shape) to time instants, or in continuous time, in which the representation of the evolution of the phenomena is decomposed into slices and interpolation functions are used to estimate the intermediate position and shape at any time. The use of a discrete model may seem more straightforward but a continuous representation provides potential gains in terms of data management, including in compression and spatio-temporal operations. In this work, we study the use of the continuous model to represent deformable moving regions captured at discrete snapshots. We propose strategies to select the observations that should be used to define the time slices of the continuous representation, thus transforming data acquired at discrete steps into a continuous model. We also study how the use of geometry simplification mechanisms may impact on moving regions interpolation quality. We evaluate our proposals using a dataset composed by thousands of aerial bush-fires images. After applying object simplification and slice decomposition, we use interpolation algorithms to generate in-between observations and compare them with real images. The results prove the effectiveness of our proposals and their importance in terms of interpolation accuracy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhancement of RGB-D image alignment using fiducial markers",
        "doc_scopus_id": "85081256441",
        "doc_doi": "10.3390/s20051497",
        "doc_eid": "2-s2.0-85081256441",
        "doc_date": "2020-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D reconstruction",
            "Camera calibration",
            "Fiducial marker",
            "Geometric optimization",
            "Inpainting",
            "Point cloud",
            "Projection of 3D points"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Three-dimensional (3D) reconstruction methods generate a 3D textured model from the combination of data from several captures. As such, the geometrical transformations between these captures are required. The process of computing or refining these transformations is referred to as alignment. It is often a difficult problem to handle, in particular due to a lack of accuracy in the matching of features. We propose an optimization framework that takes advantage of fiducial markers placed in the scene. Since these markers are robustly detected, the problem of incorrect matching of features is overcome. The proposed procedure is capable of enhancing the 3D models created using consumer level RGB-D hand-held cameras, reducing visual artefacts caused by misalignments. One problem inherent to this solution is that the scene is polluted by the markers. Therefore, a tool was developed to allow their removal from the texture of the scene. Results show that our optimization framework is able to significantly reduce alignment errors between captures, which results in visually appealing reconstructions. Furthermore, the markers used to enhance the alignment are seamlessly removed from the final model texture.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Serious games for stroke telerehabilitation of upper limb-a review for future research",
        "doc_scopus_id": "85097394134",
        "doc_doi": "10.5195/ijt.2020.6326",
        "doc_eid": "2-s2.0-85097394134",
        "doc_date": "2020-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Rehabilitation",
                "area_abbreviation": "MEDI",
                "area_code": "2742"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020, University Library System, University of Pittsburgh. All rights reserved.Maintaining appropriate home rehabilitation programs after stroke, with proper adherence and remote monitoring is a challenging task. Virtual reality (VR)-based serious games could be a strategy used in telerehabilitation (TR) to engage patients in an enjoyable and therapeutic approach. The aim of this review was to analyze the background and quality of clinical research on this matter to guide future research. The review was based on research material obtained from PubMed and Cochrane up to April 2020 using the PRISMA approach. The use of VR serious games has shown evidence of efficacy on upper limb TR after stroke, but the evidence strength is still low due to a limited number of randomized controlled trials (RCT), a small number of participants involved, and heterogeneous samples. Although this is a promising strategy to complement conventional rehabilitation, further investigation is needed to strengthen the evidence of effectiveness and support the dissemination of the developed solutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "TIMAIRIS: Autonomous Blank Feeding for Packaging Machines",
        "doc_scopus_id": "85087543982",
        "doc_doi": "10.1007/978-3-030-34507-5_7",
        "doc_eid": "2-s2.0-85087543982",
        "doc_date": "2020-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Alternative solutions",
            "Computer vision system",
            "Current packaging",
            "Different shapes",
            "Industrial environments",
            "Mobile manipulator",
            "Modes of operation",
            "Multi-Modal Interactions"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Current packaging machine vendors do not provide any automated mechanism for blank feeding and the state of the art is to have a human operator dedicated to feed the blank piles to the packaging machine. This is a tedious, repetitive and tiring task. This also results in problems with unintentional errors, such as using the wrong pile of blanks. An alternative solution is the use of a fixed robotic arm surrounded by a protective cage. However, this solution is restricted to a single packaging machine, a unique type of blank shapes and does not cooperate with humans. TIMAIRIS is a joint effort between IMA S.p.A., Italy, (IMA) and the Universidade de Aveiro, Portugal, (UAVR), promoted by the European Robotics Challenges (EuRoC) project. Together, we propose a system based on a mobile manipulator for flexible, autonomous and collaborative blank feeding of packaging machines on industrial shop floor. The system provides a software architecture that allows a mobile robot to take high level decisions on how the task should be executed, which can depend on variables such as the number of packaging machines to feed and the rate of blank consumption at each one. Through a computer vision system, blanks of different shapes and sizes are correctly identified for adequate manipulation. The manipulation of the piles of blanks is performed using a single arm using compliant modes of operation to increase manipulation safety and robustness. Additionally, it has a safe navigation system that allows the robot to be integrated in an industrial environment where humans are present. Finally, it provides an enhanced multimodal interaction between human and robot that can be adapted to the environment and operator characteristics to make communication intuitive, redundant and safe.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Matching-aware shape simplification",
        "doc_scopus_id": "85083571483",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85083571483",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Comparative studies",
            "Image representations",
            "Polygon simplification",
            "Real-world",
            "Relevant vertex",
            "Shape simplification",
            "Simplification algorithms",
            "Spatio-temporal data"
        ],
        "doc_abstract": "Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Current research has shown significant interest in spatio-temporal data. The acquisition of spatio-temporal data usually begins with the segmentation of the objects of interest from raw data, which are then simplified and represented as polygons (contours). However, the simplification is usually performed individually, i.e., one polygon at a time, without considering additional information that can be inferred by looking at the correspondences between the polygons obtained from consecutive snapshots. This can reduce the quality of polygon matching, as the simplification algorithm may choose to remove vertices that would be relevant for the matching and maintain other less relevant ones. This causes undesired situations like unmatched vertices and multiple matched vertices. This paper presents a new methodology for polygon simplification that operates on pairs of shapes. The aim is to reduce the occurrence of unmatched and multiple matched vertices, while maintaining relevant vertices for image representation. We evaluated our method on synthetic and real world data and performed an extensive comparative study with two well-known simplification algorithms. The results show that our method outperforms current simplification algorithms, as it reduces the amount of unmatched vertexes and of vertexes with multiple matches.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS",
        "doc_scopus_id": "85082115002",
        "doc_doi": "10.1007/978-3-030-35990-4_17",
        "doc_eid": "2-s2.0-85082115002",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bundle adjustments",
            "Calibration problems",
            "Calibration process",
            "Extrinsic calibration",
            "Multiple sensors",
            "Objective functions",
            "Robot operating systems (ROS)",
            "Simultaneous optimization"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Intelligent vehicles are complex systems which often accommodate several sensors of different modalities. This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration process. The calibration problem is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Configuration and Use of Pervasive Augmented Reality Interfaces in a Smart Home Context: A Prototype",
        "doc_scopus_id": "85071434188",
        "doc_doi": "10.1007/978-3-030-27928-8_16",
        "doc_eid": "2-s2.0-85071434188",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Context sensitive computing",
            "Dynamic environments",
            "Information display",
            "Intelligent environment",
            "Management and controls",
            "Positioning and tracking",
            "Smart homes",
            "Uninterrupted experiences"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.The Augmented Reality (AR) is believed to be a trending mechanism for intelligent environments since it provides additional layers of information on top of the physical world. Pervasive AR extends this concept through context-awareness, 3D space positioning and tracking mechanisms, resulting in experiences without interruptions. This paper proposes a pervasive AR application to create and configure such experiences, allowing management and control of appliances in a Smart Home context Based on a virtual content re-calibration process, the application is able to place and highlight what in the smart home can be interacted with a high level of accuracy and resilience to changes in dynamic environments. We also present examples of other appliances that can be integrated into the management and control features. Finally, we propose possible scenarios besides the Smart Home where the application might also be used.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Adaptive Augmented Reality User Interfaces Using Face Recognition for Smart Home Control",
        "doc_scopus_id": "85071420584",
        "doc_doi": "10.1007/978-3-030-27928-8_3",
        "doc_eid": "2-s2.0-85071420584",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Adaptive augmented realities",
            "Adaptive user interface",
            "Interaction process",
            "Smart homes",
            "User profile"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Augmented Reality (AR) offers the possibility to present information depending on the context/location: Pointing at a given appliance will automatically present relevant interfaces. In this work, we explore Pervasive AR in the context of the Smart Home: Users may select the control of a given appliance by pointing at it, simplifying the interaction process. This idea is then extended to consider user profile in the loop: face recognition is used to recognize a given user of and adapt the interface not only to location but also to specific users needs. A preliminary study was conducted to evaluate and validate the concept.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a qualitative analysis of interpolation methods for deformable moving regions",
        "doc_scopus_id": "85076959248",
        "doc_doi": "10.1145/3347146.3359368",
        "doc_eid": "2-s2.0-85076959248",
        "doc_date": "2019-11-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Earth-Surface Processes",
                "area_abbreviation": "EART",
                "area_code": "1904"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Interpolation problems",
            "Moving regions",
            "Quality metrics",
            "Spatio-temporal data",
            "Visual analysis"
        ],
        "doc_abstract": "© 2019 Association for Computing Machinery. All rights reserved.Spatio-temporal data on the evolution of real-world phenomena are normally acquired as snapshots in discrete time. The continuous evolution of a phenomenon between observations can be approximated using interpolation methods capable of generating deformable moving regions. Several region interpolation methods have been proposed in the spatio-temporal databases literature, each one with its own characteristics that can be more suited to represent the evolution of specific physical phenomena. In this work, we present SPT Data Lab for the qualitative and quantitative comparison of different region interpolation methods. SPT Data Lab allows users to visualize and refine 2D regions extracted from sequences of observations, execute several region interpolation methods and visualize and compare their results. SPT Data Lab also provides quality metrics that are collected during interpolation, allowing users to assess the quality of different methods. Demonstration participants will experience the application of methods on real-world data that exemplify the importance of using an appropriate method for each use case.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of a mobile augmented reality game application as an outdoor learning tool",
        "doc_scopus_id": "85081588465",
        "doc_doi": "10.4018/IJMBL.2019100105",
        "doc_eid": "2-s2.0-85081588465",
        "doc_date": "2019-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            }
        ],
        "doc_keywords": [
            "Design-based research",
            "Educational game",
            "Mobile Learning",
            "Outdoor learning",
            "User's Evaluation"
        ],
        "doc_abstract": "Copyright © 2019, IGI Global.There is a discussion on the potential of augmented reality (AR), mobile technologies to enhance learning. This article presents: 1) the EduPARK project's first cycle of design-based research for the development of a mobile AR game-like app that aims to promote learning in an urban park, and 2) an experience of students using it in loco. The focus is the students' perceptions regarding the usability and functionality of the app. Data collection involved focus groups, questionnaires and app usage information. Data was submitted to content analysis and descriptive statistics. Results revealed an excellent usability of the EduPARK app, with an average system usability scale of 85.6. Overall, students reported that the app was enjoyable, easy to use and promoted learning; however, improvements and more evaluation experiences are needed to better understand mobile AR gamelike learning in urban parks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Pervasive augmented reality for indoor uninterrupted experiences: A user study",
        "doc_scopus_id": "85072884671",
        "doc_doi": "10.1145/3341162.3343759",
        "doc_eid": "2-s2.0-85072884671",
        "doc_date": "2019-09-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Context sensitive computing",
            "Dynamic environments",
            "Location based",
            "Position and orientations",
            "Real environments",
            "Uninterrupted Experiences",
            "User acceptance",
            "User study"
        ],
        "doc_abstract": "© 2019 Copyright held by the owner/author(s).Augmented Reality (AR) adds additional layers of information on top of real environments. Recently, Pervasive AR extends this concept through an AR experience that is continuous in space, being aware of and responsive to the user’s context and pose (position and orientation). This paper focus on an exploratory user study with 27 participants meant to better understand some aspects of Pervasive AR, such as how users explore, select, recognize and manipulate virtual content in uninterrupted AR experiences, as well as their preferences. The approach used to provide this sort of engaging experiences allows the creation of indoor persistent location-based experiences, with a high level of accuracy and resilience to changes in dynamic environments. Results concerning user acceptance of uninterrupted AR experiences were encouraging. In particular, users were positively impressed by the continuous display of virtual content and were willing to use this technology more often and in different contexts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modeling and representing real-world spatio-temporal data in databases",
        "doc_scopus_id": "85072649872",
        "doc_doi": "10.4230/LIPIcs.COSIT.2019.6",
        "doc_eid": "2-s2.0-85072649872",
        "doc_date": "2019-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Benchmarking methodology",
            "Interpolation problems",
            "Morphing techniques",
            "Moving regions",
            "Natural representation",
            "Research questions",
            "Spatio-temporal data",
            "Spatio-temporal database"
        ],
        "doc_abstract": "© 2019 Schloss Dagstuhl- Leibniz-Zentrum fur Informatik GmbH, Dagstuhl Publishing. All rights reserved.Research in general-purpose spatio-temporal databases has focused mainly on the development of data models and query languages. However, since spatio-temporal data are captured as snapshots, an important research question is how to compute and represent the spatial evolution of the data between observations in databases. Current methods impose constraints to ensure data integrity, but, in some cases, these constraints do not allow the methods to obtain a natural representation of the evolution of spatio-temporal phenomena over time. This paper discusses a different approach where morphing techniques are used to represent the evolution of spatio-temporal data in databases. First, the methods proposed in the spatio-temporal databases literature are presented and their main limitations are discussed with the help of illustrative examples. Then, the paper discusses the use of morphing techniques to handle spatio-temporal data, and the requirements and the challenges that must be investigated to allow the use of these techniques in databases. Finally, a set of examples is presented to compare the approaches investigated in this work. The need for benchmarking methodologies for spatio-temporal databases is also highlighted.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Situated Visualization in the Decision Process through Augmented Reality",
        "doc_scopus_id": "85072281162",
        "doc_doi": "10.1109/IV.2019.00012",
        "doc_eid": "2-s2.0-85072281162",
        "doc_date": "2019-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Cognitive psychology",
            "Data representations",
            "Decision making process",
            "Decision process",
            "Decision support system (dss)",
            "In contexts",
            "Literature analysis"
        ],
        "doc_abstract": "© 2019 IEEE.The decision-making process and the development of decision support systems (DSS) have been enhanced by a variety of methods originated from information science, cognitive psychology and artificial intelligence over the past years. Situated visualization (SV) is a method to present data representations in context. Its main characteristic is to display data representations near the data referent. As augmented reality (AR) is becoming more mature, affordable and widespread, using it as a tool for SV becomes feasible in several situations. In addition, it may provide a positive contribution to more effective and efficient decision-making, as the users have contextual, relevant and appropriate information to endorse their choices. As new challenges and opportunities arise, it is important to understand the relevance of intertwining these fields. Based on a literature analysis, this paper addresses and discusses current areas of application, benefits, challenges and opportunities of using SV through AR to visualize data in context and to support a decision-making process and its importance in future DSS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Monitoring system for patients with cognitive impairment using mobile devices",
        "doc_scopus_id": "85070095847",
        "doc_doi": "10.23919/CISTI.2019.8760807",
        "doc_eid": "2-s2.0-85070095847",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Cognitive impairment",
            "Healthcare institutions",
            "Identification tags",
            "Locating",
            "Monitoring system",
            "Rehabilitation hospitals",
            "Software solution",
            "Validation process"
        ],
        "doc_abstract": "© 2019 AISTI.The identification, monitoring, tracking and locating of patients with cognitive impairment in healthcare institutions has always been a serious problem for these institutions. Most solutions adopted until today rely mainly on human resources and on very simple manual techniques, like, e.g., putting stickers on the patients' clothes. This paper addresses this problem describing a software solution based on the use of mobile devices, as smartphones, smartwatches, and identification tags all linked to an information system. This work was done in close collaboration with a national public rehabilitation hospital center to really understand the underlying problems, to identify the main requirements and challenges, and to help in the testing and validation process. The final prototype demonstrated the viability of the solution and that it could be deployed in a real set-up without significant effort.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing Spatial and Mobile Augmented Reality for Guiding Assembling Procedures with Task Validation",
        "doc_scopus_id": "85068430962",
        "doc_doi": "10.1109/ICARSC.2019.8733642",
        "doc_eid": "2-s2.0-85068430962",
        "doc_date": "2019-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Assembly sequence",
            "Augmented reality systems",
            "Computer vision techniques",
            "Controlled experiment",
            "Mobile augmented reality",
            "Real-time validation",
            "Spatial augmented realities",
            "Validation process"
        ],
        "doc_abstract": "© 2019 IEEE.Assembly tasks are a common situation in many industrial applications. These tasks are often presented on paper or digital manuals containing instructions, photos or diagrams to guide an assembly sequence. While some Augmented Reality (AR) systems have also been proposed to support these processes, only a few track the state of the assembling procedure, validating the process in real-time. In this work, we propose two different AR-based (mobile and spatial AR) methods with real-time validation to provide assistance to users during the execution of an assembly process. The validation process uses computer vision techniques to keep track of the state of the assembly sequence, verifying the completion of each stage and providing information at the end of the assembly. A controlled experiment was used to compare the performance, ease of use, and acceptance of the two AR-based methods proposed. Participants were significantly faster and made fewer errors using the Spatial AR condition. Besides, participants also preferred this condition. In addition, Nasa TLX rating showed that the Spatial AR condition had a slightly lower cognitive load on the participants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Heuristic evaluation in visualization: An empirical study : ition paper",
        "doc_scopus_id": "85062881054",
        "doc_doi": "10.1109/BELIV.2018.8634108",
        "doc_eid": "2-s2.0-85062881054",
        "doc_date": "2019-02-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Empirical studies",
            "Heuristic evaluation",
            "heuristics sets",
            "Potential problems",
            "Usability inspection",
            "Usability problems",
            "Usability tests",
            "Visualization application"
        ],
        "doc_abstract": "© 2018 IEEE.Heuristic evaluation is a usability inspection method that has been adapted to evaluate visualization applications through the development of specific sets of heuristics. This paper presents an empirical study meant to assess the capacity of the method to anticipate the usability issues noticed by users when using a visualization application. The potential usability problems identified by 20 evaluators were compared with the issues found for the same application by 46 users through a usability test, as well as with the fixes recommended by the experimenters observing those users during the test. Results suggest that using some heuristics may have elicited potential problems that none of the users noticed while using the application; on the other hand, users encountered unpredicted usability issues.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Investigating different augmented reality approaches in circuit assembly: A user study",
        "doc_scopus_id": "85072281241",
        "doc_doi": "10.2312/egs.20191011",
        "doc_eid": "2-s2.0-85072281241",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Circuit assembly",
            "Complex task",
            "Controlled experiment",
            "Multi-devices",
            "Traditional approaches",
            "User study"
        ],
        "doc_abstract": "© 2019 The Eurographics Association.Augmented Reality (AR) has been considered as having great potential in assisting performance and training of complex tasks. Assembling electronic circuits is such a task, since many errors may occur, as wrong choice or positioning of components or incorrect wiring and thus using AR approaches may be beneficial. This paper describes a controlled experiment aimed at comparing usability and acceptance of two AR-based approaches (one based on a single device and another approach using two interconnected devices), with a traditional approach using a paper manual in the assembly of an electronic circuit. Participants were significantly faster and made fewer errors while using the AR approaches, and most preferred the multi-device approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using Virtual Reality to Increase Motivation in Poststroke Rehabilitation: VR Therapeutic Mini-Games Help in Poststroke Recovery",
        "doc_scopus_id": "85062846487",
        "doc_doi": "10.1109/MCG.2018.2875630",
        "doc_eid": "2-s2.0-85062846487",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Development process",
            "Fundamental principles",
            "Post-stroke rehabilitation",
            "Rehabilitation programs",
            "Stroke patients",
            "Task-oriented",
            "Upper limbs",
            "Adult",
            "Aged",
            "Female",
            "Humans",
            "Male",
            "Middle Aged",
            "Motivation",
            "Stroke Rehabilitation",
            "Telerehabilitation",
            "Video Games"
        ],
        "doc_abstract": "© 2019 IEEE.Virtual reality (VR) applications meet fundamental principles of rehabilitation: intensity, task oriented training, biofeedback, environments rich in stimuli, and motivation, all pivotal factors for the success of rehabilitation programs. This paper describes the development process of a set of VR minigames developed to increase the motivation of stroke patients while performing repetitive upper limb movements.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Morphological analysis of 3D skull models for ancestry estimation",
        "doc_scopus_id": "85060177532",
        "doc_doi": "10.1109/iV.2018.00104",
        "doc_eid": "2-s2.0-85060177532",
        "doc_date": "2018-12-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "3D models",
            "Computer methods",
            "Direct manipulation",
            "Feature detection",
            "Intra-observer errors",
            "Morphological analysis",
            "Overall process",
            "Semiautomatic methods"
        ],
        "doc_abstract": "© 2018 IEEE.Skull analysis is the main tool used in anthropology to identify several characteristics such as ancestry, sex, and variations between populations. Yet, skull analysis methods used by anthropologists still rely heavily on direct manipulation and measurement of the skulls producing significant inter and intra observer errors. Direct manipulation also involves risks of damaging the specimens while handling. In recent years computer methods for skull analysis that rely on 3D models of skulls acquired with a 3D scanner have been proposed. This approach gives the possibility to perform analysis otherwise not possible, simultaneously easing the overall process of skull analysis and reducing variability. This paper describes the development of automatic and semi-Automatic methods for morphological analysis of 3D skull models through the extraction and classification of structures aiming to support the estimation of ancestry. Results with fifty specimens are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluating and enhancing google tango localization in indoor environments using fiducial markers",
        "doc_scopus_id": "85048857208",
        "doc_doi": "10.1109/ICARSC.2018.8374174",
        "doc_eid": "2-s2.0-85048857208",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.Recent advances in 3D sensing technologies, as well as in inertial measurement technologies, have resulted in significant improvements in the accuracy of the localization of systems that combine all these sensors. Project Tango is one of the most successful examples of such systems. Developed by Google, it integrates in an Android mobile device a set of sensors and software required to provide accurate real-time 3D information when moving the equipment freely in hand. This is making mapping and navigation accessible to the general public, with evident applications in robotics, augmented reality, computer vision and others. The contribution of this paper is towfold: first, we present a thorough evaluation of the localization accuracy of the Tango platform in different conditions; second, we present a fiducial marker-based extension of the Tango localization system, which improves the localization estimates in certain conditions. The paper presents a set of experiments performed to evaluate the position and orientation errors in indoor environments, using Augmented Reality for visualization purposes, with and without area learning, e.g. using a priori information acquired from the environment. In addition, we propose a solution based on the use of additional visual markers, which allows the re-calibration of augmented content in specific locations, to improve tracking accuracy in dynamic environments where spatial and/or illumination changes may occur. A statistical analysis of the results shows that the Tango with area learning and the proposed solution provide a level of accuracy significantly better that the Tango without area learning. Moreover, the proposed solution can overcome some limitations of Tango with area learning when used in dynamic environments.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mobile devices for interaction in immersive virtual environments",
        "doc_scopus_id": "85048875007",
        "doc_doi": "10.1145/3206505.3206526",
        "doc_eid": "2-s2.0-85048875007",
        "doc_date": "2018-05-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "3-d interactions",
            "Flexible interfaces",
            "Head mounted displays",
            "Immersive environment",
            "Immersive virtual environments",
            "Immersive virtual reality",
            "On-board sensors",
            "Virtual representations"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery ACMGamepads and 3D controllers are the main controllers used in most Virtual Environments. Despite being simple to use, these input devices have a number of limitations as fixed layout and difficulty to remember the mapping between buttons and functions. Mobile devices present interesting characteristics that might be valuable in immersive environments: more flexible interfaces, touchscreen combined with onboard sensors that allow new interaction and easy acceptance since these devices are used daily by most users. The work described in this article proposes a solution that uses mobile devices to interact with Immersive Virtual Environments for selection and navigation tasks. The proposed solution uses the mobile device camera to track the Head-Mounted-Display position and present a virtual representation of the mobile device screen; it was tested using an Immersive Virtual Museum as use case. Based on this prototype, a study was performed to compare controller based and mobile based interaction for navigation and selection showing that using mobile devices is viable in this context and offers interesting interaction opportunities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An evaluation of smoothing and remeshing techniques to represent the evolution of real-world phenomena",
        "doc_scopus_id": "85057150958",
        "doc_doi": "10.1007/978-3-030-03801-4_6",
        "doc_eid": "2-s2.0-85057150958",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Deformable moving objects",
            "Delaunay",
            "Edge flips",
            "Mesh quality",
            "Morphing techniques",
            "Real-world",
            "Remeshing method",
            "Remeshing technique"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2018.In this paper we investigate the use of morphing techniques to represent the continuous evolution of deformable moving objects, representing the evolution of real-world phenomena. Our goal is to devise processes capable of generating an approximation of the actual evolution of these objects with a known error. We study the use of different smoothing and remeshing methods and analyze various statistics to establish mesh quality metrics with respect to the quality of the approximation (interpolation). The results of the tests and the statistics that were collected suggest that the quality of the correspondence between the observations has a major influence on the quality and validity of the interpolation, and it is not trivial to compare the quality of the interpolation with respect to the actual evolution of the phenomenon being represented. The Angle-Improving Delaunay Edge-Flips method, overall, obtained the best results, but the Remeshing method seems to be more robust to abrupt changes in the geometry.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of Aruco Markers Using the Quadrilateral Sum Conjuncture",
        "doc_scopus_id": "85049453966",
        "doc_doi": "10.1007/978-3-319-93000-8_41",
        "doc_eid": "2-s2.0-85049453966",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Aruco",
            "Digital image",
            "Fiducial marker",
            "Internal angles",
            "Marker detections",
            "Pose estimation"
        ],
        "doc_abstract": "© 2018, Springer International Publishing AG, part of Springer Nature.Fiducial Markers are heavily used for pose estimation in many applications from robotics to augmented reality. In this paper we present an algorithm for the detection of aruco marker at larger distance. The algorithm uses the quadrilateral sum conjecture and analyzes the sum of the cosine of the internal angles to detect squares at larger distances. Experiments conducted showed that the developed solution was able to improve the detection distance when compared to other methods that use similar marker while keeping similar pose estimation precision.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A framework for the management of deformable moving objects",
        "doc_scopus_id": "85044826926",
        "doc_doi": "10.1007/978-3-319-78208-9_17",
        "doc_eid": "2-s2.0-85044826926",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Civil and Structural Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2205"
            },
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Earth-Surface Processes",
                "area_abbreviation": "EART",
                "area_code": "1904"
            },
            {
                "area_name": "Computers in Earth Sciences",
                "area_abbreviation": "EART",
                "area_code": "1903"
            }
        ],
        "doc_keywords": [
            "Compatible triangulations",
            "Deformable moving objects",
            "Interpolation method",
            "Morphing",
            "Moving objects",
            "Spatio-temporal data",
            "Spatio-temporal database",
            "Triangulated polygons"
        ],
        "doc_abstract": "© Springer International Publishing AG, part of Springer Nature 2018.There is an emergence of a growing number of applications and services based on spatiotemporal data in the most diverse areas of knowledge and human activity. The representation of the continuous evolution of moving regions, i.e., entities (or objects) whose position, shape and extent change continuously over time, is particularly challenging and the methods proposed in the literature to obtain such representation still present some issues. In this paper we present a framework for moving objects, in particular, moving regions, that uses the concept of mesh, i.e., a triangulated polygon, compatible triangulation and rigid interpolation methods to represent the continuous evolution of moving regions over time. We also present a spatiotemporal database extension for PostgreSQL that uses this framework and that allows to store moving objects data in a PostgreSQL database and to analyze and manipulate them using SQL. This extension can be smoothly integrated with PostGIS. Experiments show that our framework works with real data and provides a base for further work and investigation in this area.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Rich and robust human-robot interaction on gesture recognition for assembly tasks",
        "doc_scopus_id": "85026864168",
        "doc_doi": "10.1109/ICARSC.2017.7964069",
        "doc_eid": "2-s2.0-85026864168",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Assembly tasks",
            "Human robot Interaction (HRI)",
            "Manufacturing enterprise",
            "Mobile manipulator",
            "Multiple feature fusion",
            "Puzzle games",
            "Robotics technology",
            "Small and medium sized enterprise"
        ],
        "doc_abstract": "© 2017 IEEE.The adoption of robotics technology has the potential to advance quality, efficiency and safety for manufacturing enterprises, in particular small and medium-sized enterprises. This paper presents a human-robot interaction (HRI) system that enables a robot to receive commands, provide information to a human teammate and ask them a favor. In order to build a robust HRI system based on gesture recognition, three key issues are addressed: richness, multiple feature fusion and failure verification. The developed system has been tested and validated in a realistic lab with a real mobile manipulator and a human teammate to solve a puzzle game.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Effect of hand-avatar in a selection task using a tablet as input device in an immersive virtual environment",
        "doc_scopus_id": "85018939288",
        "doc_doi": "10.1109/3DUI.2017.7893364",
        "doc_eid": "2-s2.0-85018939288",
        "doc_date": "2017-04-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "button selection task",
            "hand-avatar",
            "Immersive virtual environments",
            "Input devices",
            "User study"
        ],
        "doc_abstract": "© 2017 IEEE.How does the virtual representation of the user's hands influence the performance on a button selection task performed in a tablet-based interaction within an immersive virtual environment? To answer this question, we asked 55 participants to use three conditions: no-hand avatar, realistic avatar and translucent avatar. The participants were faster but made slightly more errors while using the no-avatar condition, and considered easier to perform the task with the translucent avatar.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Representation of continuously changing data over time and space: Modeling the shape of spatiotemporal phenomena",
        "doc_scopus_id": "85016818033",
        "doc_doi": "10.1109/eScience.2016.7870891",
        "doc_eid": "2-s2.0-85016818033",
        "doc_date": "2017-03-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Environmental Science (miscellaneous)",
                "area_abbreviation": "ENVI",
                "area_code": "2301"
            },
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Social Sciences (miscellaneous)",
                "area_abbreviation": "SOCI",
                "area_code": "3301"
            },
            {
                "area_name": "Agricultural and Biological Sciences (miscellaneous)",
                "area_abbreviation": "AGRI",
                "area_code": "1101"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Compatible triangulations",
            "Environmental science",
            "Morphing techniques",
            "phenomena",
            "Planar shape morphing",
            "Spatio-temporal data",
            "Spatio-temporal database",
            "Synthetic and real data"
        ],
        "doc_abstract": "© 2016 IEEE.There are numerous technologies and tools to acquire data related to the evolution of spatial phenomena over time. These data are typically organized as sequences of 2D geometric shapes obtained from observations taken at different times. The transformation of such sequences of 2D geometric shapes into spatiotemporal data representations, which can be easily processed and interpreted, has the potential to enable novel applications in fields as diverse as environmental sciences, climate sciences, biology or medicine. This paper focuses on the representation of moving 2D geometric shapes acquired at discrete times using continuous models of time and space. Using morphing techniques based on compatible triangulations, issues regarding the representation of spatiotemporal data in databases, as well as the influence of different design strategies on the fidelity of the approximations with respect to the modelled phenomena, are investigated. An experimental study using synthetic and real data was performed. The findings show that the use of triangulation based interpolation is a promising approach, because it allows creating continuous spatiotemporal representations that are more realistic than those obtained using the solutions proposed in previous work. Open issues regarding the representation of spatiotemporal data in information systems are also highlighted.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards automatic non-metric traits analysis on 3D models of skulls",
        "doc_scopus_id": "85016055539",
        "doc_doi": "10.1109/EPCGI.2016.7851196",
        "doc_eid": "2-s2.0-85016055539",
        "doc_date": "2017-02-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "3-D scanner",
            "3D models",
            "Ancestry",
            "Feature detection",
            "Intra-observer variability",
            "Morphological analysis",
            "New approaches",
            "Non-Metric"
        ],
        "doc_abstract": "© 2016 IEEE.The morphological and metric methods used by anthropologists to assess ancestry can generate results with low repeatability besides damaging the specimens while handling. These problems have led to the development of a new approach based on skulls acquisition with a 3D scanner, using the resulting models to make measurements and morphological analyzes in the CraMs application (Craniometric Measurements). This paper focuses on the development of new methods for the morphological analysis, and the extraction and classification of structures with the objective of reducing inter and intra observer variability. The final aim is to ease the process of estimating the individual's ancestry.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "What should a virtual/augmented reality course be?",
        "doc_scopus_id": "85092203085",
        "doc_doi": "10.2312/eged.20171027",
        "doc_eid": "2-s2.0-85092203085",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Core course",
            "Introductory course",
            "New applications",
            "Virtual and augmented reality"
        ],
        "doc_abstract": "© 2017 The Author(s) Eurographics Proceedings © 2017 The Eurographics Association.Never before has Virtual and Augmented Reality hardware been so affordable allowing so many new applications of these technologies; however, developing these applications implies specific skills that are not usually acquired in core courses in Computer Science/Engineering. In this context, specific courses introducing the basics on these technologies seem to be most relevant. With this panel we intend to foster a discussion concerning what should an introductory course on Virtual/Augmented Reality be as of 2017. A review of the courses described in literature is presented as well as guidelines issued by professional/scientific associations concerning a basic Virtual Reality course identifying a set of relevant aspects to be considered when organizing such a course.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Freehand gesture-based 3D manipulation methods for interaction with large displays",
        "doc_scopus_id": "85025152989",
        "doc_doi": "10.1007/978-3-319-58697-7_10",
        "doc_eid": "2-s2.0-85025152989",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D object manipulations",
            "3D user interface",
            "Hand gesture",
            "Large displays",
            "User study"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Gesture-based 3D interaction is a research topic with application in numerous scenarios which gained relevance with the recent advances in low-cost tracking systems. Yet, it poses many challenges due to its novelty and consequent lack of systematic development methodologies. Developing easy to use and learn gesture-based 3D interfaces is particularly difficult since the most adequate and intuitive gestures are not always obvious and there is often a variety of different gestures used to perform similar actions. This paper presents the development and evaluation of interaction methods to manipulate 3D virtual objects in a large display set-up using freehand gestures detected by a Kinect depth sensor. We describe the implementation of these methods and the user studies conducted to improve them and assess their usability as manipulation methods. Based on the results of these studies we also propose a method that overcomes the lack of roll movement detection by the Kinect and makes simpler the scaling and rotation in all degrees-of-freedom using hand gestures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An exploratory study on the predictive capacity of heuristic evaluation in visualization applications",
        "doc_scopus_id": "85021624109",
        "doc_doi": "10.1007/978-3-319-58071-5_28",
        "doc_eid": "2-s2.0-85021624109",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Exploratory studies",
            "Heuristic evaluation",
            "Information visualization",
            "Potential problems",
            "Predictive capacity",
            "Usability evaluation",
            "User study",
            "Visualization application"
        ],
        "doc_abstract": "© 2017, Springer International Publishing AG.Heuristic evaluation is generally considered as an adequate method to perform formative usability evaluation as it helps identify potential problems from early stages of development and may provide useful results even with a relatively low investment. In particular, the method has been adapted and used to evaluate visualization applications. This paper presents an exploratory study aimed at better understanding the capacity of heuristic evaluation to predict the issues experienced by users when using a visualization application and how to assess it. The main usability potential problems pointed out in a visualization application by 20 evaluators using heuristic evaluation are compared with the problems reported for the same application by 44 users.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Incremental texture mapping for autonomous driving",
        "doc_scopus_id": "84991738411",
        "doc_doi": "10.1016/j.robot.2016.06.009",
        "doc_eid": "2-s2.0-84991738411",
        "doc_date": "2016-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Autonomous driving",
            "Autonomous Vehicles",
            "Constrained Delaunay triangulation",
            "Geometric description",
            "Partial configuration",
            "Scene reconstruction",
            "Texture mapping",
            "Vision-based sensors"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.Autonomous vehicles have a large number of on-board sensors, not only for providing coverage all around the vehicle, but also to ensure multi-modality in the observation of the scene. Because of this, it is not trivial to come up with a single, unique representation that feeds from the data given by all these sensors. We propose an algorithm which is capable of mapping texture collected from vision based sensors onto a geometric description of the scenario constructed from data provided by 3D sensors. The algorithm uses a constrained Delaunay triangulation to produce a mesh which is updated using a specially devised sequence of operations. These enforce a partial configuration of the mesh that avoids bad quality textures and ensures that there are no gaps in the texture. Results show that this algorithm is capable of producing fine quality textures.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-07-05 2016-07-05 2016-08-26 2016-08-26 2016-08-26T17:58:13 S0921-8890(16)30081-1 S0921889016300811 10.1016/j.robot.2016.06.009 S300 S300.1 FULL-TEXT 2016-08-26T13:55:25.23236-04:00 0 0 20161001 20161031 2016 2016-07-05T15:19:52.486451Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 84 84 C Volume 84 10 113 128 113 128 201610 October 2016 2016-10-01 2016-10-31 2016 article fla © 2016 Elsevier B.V. All rights reserved. INCREMENTALTEXTUREMAPPINGFORAUTONOMOUSDRIVING OLIVEIRA M 1 Introduction 2 Related work 3 Proposed approach 3.1 One-shot texture mapping using data dependent triangulation 3.2 Incremental texture mapping 4 Results 4.1 One-shot texture mapping 4.2 Incremental texture mapping 4.3 Projection of a single camera onto the ground plane 4.4 Projection of multiple cameras onto the ground plane 5 Conclusions Acknowledgments References OLIVEIRA 2016 503 515 M ROBOT2015SECONDIBERIANROBOTICSCONFERENCEADVANCESINROBOTICSVOLUME1 SCENEREPRESENTATIONSFORAUTONOMOUSDRIVINGAPPROACHBASEDPOLYGONALPRIMITIVES OLIVEIRA 2016 M HUANG 2011 1595 1601 A SEGAL 1992 249 252 M DEBEVEC 1998 P EFFICIENTVIEWDEPENDENTIMAGEBASEDRENDERINGPROJECTIVETEXTUREMAPPINGTECHREP RIPPA 1992 257 270 S GARLAND 1995 M FASTPOLYGONALAPPROXIMATIONTERRAINSHEIGHTFIELDSTECHREPCMUCS95181 SCHATZL 2001 309 321 R GEOMETRICMODELLING DATADEPENDENTTRIANGULATIONINPLANEADAPTIVEKNOTPLACEMENT DEMARET 2006 1604 1616 L SAPPA 2007 23010 A DYN 1992 179 192 N SCHUMAKER 1993 329 345 L OPENGL 2005 OPENGLRPROGRAMMINGGUIDEOFFICIALGUIDELEARNINGOPENGLRVERSION2 BLYTHE 2006 724 734 D LEHNER 2007 178 187 B GILECTURENOTESININFORMATICSVISUALIZATIONLARGEUNSTRUCTUREDDATASETS SURVEYTECHNIQUESFORDATADEPENDENTTRIANGULATIONS CERVENANSKSY 2010 125 135 M SVALBE 1989 941 950 I YVINEC 2012 M CGALUSERREFERENCEMANUAL40EDITION 2DTRIANGULATIONS SHEWCHUK 2008 580 637 J MOLLER 1997 25 30 T CHANG 2009 235 240 J FOGEL 2012 E CGALUSERREFERENCEMANUAL40EDITION 2DREGULARIZEDBOOLEANSETOPERATIONS OLIVEIRAX2016X113 OLIVEIRAX2016X113X128 OLIVEIRAX2016X113XM OLIVEIRAX2016X113X128XM 2018-08-26T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0921-8890(16)30081-1 S0921889016300811 10.1016/j.robot.2016.06.009 271599 2016-08-26T13:11:12.895259-04:00 2016-10-01 2016-10-31 true 8641462 MAIN 16 49999 849 656 IMAGE-WEB-PDF 1 gr4 26663 164 127 gr7 30032 163 206 gr8 29537 164 203 pic5 33051 163 140 gr2 30967 164 169 pic2 25133 163 140 gr12 33037 164 197 gr1 29767 164 183 gr5 20010 163 140 gr13 41383 163 154 pic3 22237 163 140 gr6 26630 164 214 gr9 23491 52 219 pic4 22871 164 141 gr11 23784 163 122 pic1 23187 164 141 gr3 25322 113 219 gr10 38213 164 168 gr4 152384 649 502 gr7 192344 600 756 gr8 183020 594 736 pic5 39234 132 113 gr2 149230 587 606 pic2 38908 132 113 gr12 193022 529 637 gr1 129052 533 596 gr5 164313 882 756 gr13 293812 664 626 pic3 35322 132 113 gr6 155924 579 755 gr9 111172 156 656 pic4 35061 131 113 gr11 208963 834 624 pic1 35749 131 113 gr3 64157 312 606 gr10 203603 571 585 si123 161 13 16 si56 159 12 17 si21 144 13 13 si96 4249 133 353 si71 222 15 36 si107 724 39 134 si121 2455 64 382 si166 300 19 45 si81 578 15 150 si53 159 13 17 si17 188 13 42 si184 390 15 87 si75 112 11 6 si46 401 15 87 si147 201 14 29 si37 117 12 6 si168 265 19 37 si203 401 15 87 si134 149 13 14 si39 264 15 39 si68 1486 36 284 si88 399 15 64 si78 136 10 16 si115 127 10 11 si114 405 15 80 si201 400 15 87 si163 276 19 37 si10 178 12 23 si124 151 13 16 si181 346 19 51 si202 407 15 87 si152 332 21 61 si122 154 13 16 si63 170 11 16 si66 229 13 39 si199 391 15 87 si11 195 12 29 si126 115 7 10 si119 187 14 27 si8 134 10 11 si127 1424 16 390 si110 277 16 40 si99 1388 41 292 si196 168 12 35 si132 470 15 148 si9 160 12 17 si57 922 16 247 si73 151 14 14 si54 169 13 18 si159 183 11 41 si69 170 14 19 si83 330 15 62 si205 397 15 87 si2 126 11 11 si125 713 15 145 si61 566 15 183 si80 142 10 17 si148 200 14 29 si16 178 13 40 si197 388 15 87 si108 147 14 11 si145 230 13 41 si178 287 19 36 si183 496 15 136 si207 398 15 87 si176 324 19 45 si34 145 13 13 si18 188 13 41 si170 356 19 57 si1 202 12 40 si29 136 11 12 si174 324 19 45 si140 233 13 39 si30 131 11 10 si206 403 15 87 si164 256 19 36 si135 944 17 283 si133 794 15 171 si74 699 21 173 si62 1004 51 189 si175 330 19 45 si43 273 15 52 si67 1107 15 328 si154 358 23 60 si72 143 14 14 si100 1647 41 383 si200 183 12 36 si161 318 19 45 si120 255 14 43 si60 243 17 36 si153 351 23 61 si90 140 11 10 si59 286 11 72 si91 1560 31 389 si47 392 15 87 si41 187 12 36 si70 351 15 76 si65 155 10 20 si77 326 24 62 si40 192 12 36 si195 212 12 40 si20 134 13 12 si117 972 16 249 si187 395 15 87 si105 216 15 27 si186 386 15 87 si38 114 10 8 si162 272 19 36 si112 135 12 10 si3 141 11 12 si130 167 14 16 si44 326 15 65 si50 189 13 41 si129 163 14 16 si182 381 15 87 si137 223 13 40 si131 234 13 39 si104 210 13 41 si64 160 10 19 si102 113 10 10 si146 193 14 28 si22 141 13 13 si177 283 19 37 si103 2739 70 382 si55 149 12 16 si85 160 21 14 si98 577 17 148 si58 131 8 11 si45 379 15 94 si76 123 14 8 si128 248 14 39 si158 1670 44 346 si179 287 19 37 si82 146 11 13 si106 285 15 51 si97 500 15 141 si79 145 10 17 si139 221 13 41 si167 231 13 40 si198 395 15 87 ROBOT 2656 S0921-8890(16)30081-1 10.1016/j.robot.2016.06.009 Elsevier B.V. Fig. 1 An example from the MIT data-set: three projections are collected over a period of time and mapped to a wall panel (GPP k = 4 , in blue): (a) positions of the vehicle at the time each projection is collected; (b) image from front camera, at location C ; (c), front camera, intermediate location; (d) front camera, location D ; (e) left camera, location D . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Triangulated meshes (top) and textures (bottom) created separately for each of the projections shown in the example of Fig. 1: (a) front camera, location C ; (b) front camera, intermediate location; (c) front camera, location D ; (d) left camera, location D . Fig. 3 Textures obtained using different fusion strategies: (a) option (1), average textures from local meshes; (b) option (2), insert vertices from all local meshes; (c) option (3), insert triangles of better quality, removing overlapping triangles. Fig. 4 A diagram showing the main components of the proposed system. Fig. 5 Triangle overlap test: (a) intersection returns points, overlap true; (b) intersection returns line segments, overlap true; (c) intersection returns polygons, overlap true; (d) intersection returns empty, overlap false; (e) intersection returns points, overlap false; (f) intersection returns line segments, overlap false. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 The insertion operation: (a) candidate triangle and initial mesh; (b) insertion of candidate triangle’s vertices; (c) insertion of the candidate triangle’s vertices and constraints; (d) preparation of the mesh followed by the insertion of the candidate triangle’s vertices and constraints. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 7 The insertion operation, example 1: (a) candidate triangles and initial mesh M ; (b) first insertion, mesh M ∗ ; (c) second insertion, mesh M ∗ ∗ ; (d) third insertion, mesh M ∗ ∗ ∗ . Fig. 8 The projection parent status of each triangle (same example as in Fig. 7): (a) candidate triangles and initial mesh M ; (b) first insertion, mesh M ∗ ; (c) second insertion, mesh M ∗ ∗ ; (d) third insertion, mesh M ∗ ∗ ∗ . Fig. 9 One-shot texture mapping: (a) image with line segments detected (red lines); (b) arbitrary Delaunay triangulation; (c) proposed approach, using a constrained Delaunay triangulation. Constrained edges marked in red. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 10 The evolution of the global primitive mesh (top) as well as the texture (bottom): (a) time t = t 1 ; (b) time t = t 2 ; (c) time t = t 3 , insertion of front center camera; (d) time t = t 3 , insertion of front left camera. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 11 Mapping of a single camera to the ground plane: (first row) front center camera; (second row) rear center camera; (third row) front left camera; (left column) global primitive meshes; (right column) contribution of each projection to the total number of triangles in the global mesh. Colors denote each of the projections, i.e., black is time t 1 , red is time t 2 and yellow is time t 3 . Blue triangles are orphan triangles. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Distribution of triangles according to projection, for an example with five cameras. Three time instants (15 projections in total) are considered; (a) t 1 ; (b) t 2 ; (c) t 3 ; (d) percentage of triangles by parent projection. Projections are colored with a black to yellow color coding, denoting oldest to newest projections. Blue color denotes orphan triangles. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 Images and local triangulated meshes for all projections shown in Fig. 12: First row front center teleobjective camera; Second row front center camera; Third row, front right camera; Fourth row, front left camera; Fifth row, rear center camera; Left column: time t 1 , (Fig. 11(a)); Middle column: time t 2 , (Fig. 11(b)); Right column: time t 3 , (Fig. 11(c)). Incremental texture mapping for autonomous driving Miguel Oliveira a b ⁎ Vitor Santos b Angel D. Sappa c d Paulo Dias b A. Paulo Moreira a e a INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal b IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro Campus Universitário de Santiago Aveiro 3810-193 Portugal c Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS, Campus Gustavo Galindo, Km 30.5 Vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Escuela Superior Politécnica del Litoral, ESPOL, Facultad de Ingeniería en Electricidad y Computación, CIDIS Campus Gustavo Galindo Km 30.5 Vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador d Computer Vision Center, Campus UAB, 08193 Bellaterra, Barcelona, Spain Computer Vision Center, Campus UAB Bellaterra Barcelona 08193 Spain e FEUP - Faculty of Engineering, University of Porto, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal FEUP - Faculty of Engineering, University of Porto R. Dr. Roberto Frias s/n Porto 4200-465 Portugal ⁎ Corresponding author at: INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal. INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal Autonomous vehicles have a large number of on-board sensors, not only for providing coverage all around the vehicle, but also to ensure multi-modality in the observation of the scene. Because of this, it is not trivial to come up with a single, unique representation that feeds from the data given by all these sensors. We propose an algorithm which is capable of mapping texture collected from vision based sensors onto a geometric description of the scenario constructed from data provided by 3D sensors. The algorithm uses a constrained Delaunay triangulation to produce a mesh which is updated using a specially devised sequence of operations. These enforce a partial configuration of the mesh that avoids bad quality textures and ensures that there are no gaps in the texture. Results show that this algorithm is capable of producing fine quality textures. Keywords Scene reconstruction Autonomous driving Texture mapping 1 Introduction Autonomous vehicles often have a very large number of sensors mounted on-board. This is due to the need to observe the environment all around the vehicle, but also because vehicles must observe the scene with sensors of different nature. Mainly, sensors are divided into two groups: range sensors and vision based sensors. Sensors of the first group provide 3D measurements of the scene. On the other hand, vision based sensors collect photometric information of the scene. Due to the large number of sensors on-board these vehicles, it is not trivial to combine data from these sensors into a unique representation of the scene. Given that these sensors provide a continuous stream of data over time, and that they are displaced by the movement of the vehicle, then it follows that the representation of the scene must also be dynamic, in the sense that it must evolve to represent novel information collected at later stages of the mission. Note that, given a continuous throughput of images, the most recent image is not necessarily the best image to be used for texture mapping. For example, if the vehicle is moving away from an object, a camera on the rear side of the vehicle will produce images with decreasing quality. Rather, what is required is an algorithm that produces a scene representation at the early stages of a mission (because this might be immediately required for other tasks such as navigation, planning, etc.), but the later on is also capable of evaluating newly acquired images to assess whether or not these images are better than the previously used for mapping the texture. We refer to this as incremental texture mapping. In [1], an algorithm for creating and incrementally updating a geometrical representation of the scenario is presented. This work was later extended in [2]. The approach is based on Geometric Polygonal Primitives (GPP), and is shown to be capable of providing an accurate geometric description of the scenario. It uses data from range sensors only, and the geometric description changes to accommodate novel sensor data. In this paper we use the results given by the approach described in [2]. This means that we consider that there is, at all times, a geometric description of the scenario which is constantly evolving. In this paper, we focus on how the vision based sensors can be used to enrich the description of the scenario. In other words, we propose to use the images from the cameras on-board the vehicle to produce texture, which may be added to the 3D description of the environment. Note that, as in the case of the range sensors, the vision based sensors also produce a continuous stream of information which must be integrated in order to create a unique photometric description of the scenario. In this paper, we propose an approach which is capable of incrementally updating texture mapped onto GPPs. The following lines show an example in which the need for incremental texture mapping becomes clear. For testing and evaluation purposes, we use a data-set from the Massachusetts Institute of Technology (MIT) Team, taken from their participation in the DARPA Urban Challenge [3]. A small 40 s sequence was cropped from the MIT data-set. This sequence is referred to as the MIT sequence, and five key locations ( A through E where marked in the sequence (see [2] for details). The approach described in [2] produces a description of the geometric structure of the environment observed by the vehicle’s sensors. This description is given in the form of Geometric Polygonal Primitives (GPP), i.e., a list of polygons. Note that, as pointed out in [2] the geometric description of the scene is dynamic, since it may change whenever novel sensor information is collected. An example is presented in Fig. 1 where the vehicle travels from location C to location D of the MIT sequence. Images are collected at three locations: location C at mission time t 0 , location D at mission time t 2 and an intermediate location between those two at mission time t 1 (Fig. 1(a) shows the vehicle at each location). Consider a camera of index l , that produces an image which may virtually be projected to any GPP (i.e., to one of the polygons that constitute the geometric description of the scene), at any given mission time t . The term projection is defined as an image captured from a camera that can be used to map some texture to one of the polygons contained in the geometric description of the scene, and is denoted as C { k , l , t } . The data-set contains five color cameras (see [2,4] for details). Without loss of generality, in this example only images from two cameras are used: front center ( l = 0 ) and front left ( l = 3 ), and only a single GPP (index k = 4 ) is employed, which corresponds to the wall panel in front of the vehicle (in blue, left side of Fig. 1(a)). Note that, under the constraints defined above, i.e., k = { 4 } , l = { 0 , 3 } and t = { t 0 , t 1 , t 2 } , there are a total of six possible projections. However, two of these projections are empty, namely C { k = 4 , l = 3 , t = t 0 } and C { k = 4 , l = 3 , t = t 1 } . This is because the left camera ( l = 3 ) does not see the wall panel ( k = 4 ) in the first two locations ( t = t 0 and t = t 1 ). This can be observed in Fig. 1(a), which shows that the vehicle turns right at location D , and only then the left side camera is pointed in the direction of the wall panel. The images from the remaining four projections are shown in Fig. 1(b)–(e). As the vehicle approaches the wall panel, it collects images with higher resolution and better quality of that surface. Our goal is to study how a low resolution texture created when the vehicle was distant from the surface may evolve to a higher resolution texture once the vehicle comes closer to the panel. In other words, how can the texture of a surface be incrementally refined. Note that we assume that an accurate localization is available at all times. In the case of the MIT dataset, localization is provided by an Applanix POS-LV 220 system, 1 1 which includes a GPS, an inertial measurement unit and a wheel encoder. This is a very accurate system which publishes the 6 DOF pose of the vehicle at high frequencies (100 Hz). Thus, it is possible to gather the pose of each of the onboard cameras at any point in time. Obviously, a less accurate ego motion estimation should influence the mapping of texture. However, a detailed analysis of the impact of other ego motion estimation systems is out of the scope of the current paper. The remainder of the paper is organized as follows: related work is presented in Section 2; the proposed approach is described in Section 3 and, finally, results and conclusions are given in Sections 4 and 5. 2 Related work Texture mapping is a technique for mapping a 2D image onto a 3D surface by transforming color data so that it conforms to the surface plot. It allows the application of texture such as tiles or wood grain, to a surface without performing the geometric modeling necessary to create a surface with these features, or, in other words, without computing the projection of every pixel in the image onto the surface. The color data can also be any image, such as a picture taken by a camera. Texture mapping is performed over convex polygons, most commonly on triangles. Let X 1 , X 2 be the coordinates of the vertices 1 and 2 in 3D space. The coordinates u 1 , u 2 of the pixels that correspond to those vertices in the image plane can be obtained using direct projection: (1) u i = projection ( X i ) , ∀ i ∈ { 1 , 2 } . Let α be a parameter 0 < α < 1 , that indicates how a given vertex is positioned along the X 1 X 2 ¯ line segment. Texture mapping interpolates the color value for any vertices along the line segment as follows: (2) u α = ( 1 − α ) ⋅ u 0 + α ⋅ u 1 , which is of course a linear interpolation. When this kind of linear interpolation is used, the texture mapping is referred to as affine texture mapping. A linear interpolation works fine when the image plane and the projection plane are parallel. However, when this does not occur, the projection shows some artifacts that derive from the assumption that a linear interpolation can be used. This is a well documented problem, and is discussed in several works [5,6]. The solution to this problem is called view dependent texture mapping, and it consists of making texture mapping account for the position of the vertexes in 3D space, rather than simply interpolating a 2D triangle. This achieves the correct visual effect, but it is slower to calculate. Instead of interpolating the texture coordinates directly, the coordinates are divided by their depth (relative to the viewer), and the reciprocal of the depth value is also interpolated and used to recover the perspective corrected coordinate. This correction operates so that in parts of the polygon that are closer to the viewer, the difference from pixel to pixel between texture coordinates is smaller (stretching the texture wider), and in parts that are farther away this difference is larger (compressing the texture). View dependent texture mapping can be formulated as: (3) u α = ( 1 − α ) ⋅ u 0 w 0 + α ⋅ u 1 w 1 ( 1 − α ) ⋅ 1 w 0 + α ⋅ 1 w 1 . The solution proposed in Eq. (3) is capable of producing accurate mapping for texture. View dependent texture mapping is significantly slower when compared to affine texture mapping. Since triangles are the atomic entities of texture mapping, triangulation methodologies are an important part of the process. In this scope, Data Dependent Triangulation (DDT) algorithms are of particular interest since they can produce triangulated meshes which are ideal for texture mapping. The goal of a DDT is, on the one hand, to obtain the best approximation possible, and on the other to reduce the number of triangles and in turn the memory load. Consequently, the number of vertices should be kept as small as possible to speed up processing and reduce memory load. The two variables that should be tuned to achieve a good approximation are then the position of the vertices and the connections between them. Even if we decide to fix the number of triangles and vertices, the possible combinations of the connections between vertices are usually very large. Hence, an exhaustive search of all possible combinations is not possible. Also, no assumptions should be made on the optimal shape or size of the triangles. One might tend to assume long, thin triangles are not adequate but in fact that depends on the nature of the image [7]. If the image contains high gradient long feature such as poles or trees, such triangles could be well suited to represent these regions. DDT algorithms can be divided into refinement, decimation, or modification approaches. In refinement approaches, the starting point for the algorithm is a very coarse triangular mesh that is then refined. The mesh is refined by inserting new vertices. Since the number of possible positions where vertices can be inserted is very high, authors make use of heuristics to limit the number of options. The greedy refinement algorithm proposed in [8] works by inserting vertices into a triangulated mesh. In every step, a new vertex is inserted at the position of the largest distance between the approximation and the data provided in the image. In [9], the choice of which are the triangles to decimate is based on the high curvature of the data, and the positions where new vertices are to be inserted are locations with high proximity to the data. These methods have the drawback of tending to a local optima. Decimation approaches are the opposite of refinement meshes. The algorithms start from a very fine mesh and try to remove vertices and collapse triangles as they iterate. In [10] the initial triangulation is a full triangulation where each pixel is a vertex in the mesh. The algorithm then decimates the mesh by collapsing one of the edges of the mesh. The edge to collapse is the one that implicates less increase in the approximation error. Similar approaches were proposed in [11,12]. Finally, modification strategies start from a random arbitrary mesh and try to improve it by performing modification operations. These modification operations usually are edge swaps and the number of vertices in the initial mesh remains the same. It is the case of the algorithms proposed in [13,14]. Both propose different criteria for the selection of which are the edges that should be swapped. Given a triangulated mesh of a surface and an image registered to that surface, it is possible to map the texture from the image onto the surface using classic texture mapping approaches. Fig. 2 shows the triangulated meshes and textures produced using classical texture mapping, for each of the four projections displayed in Fig. 1. These mappings are computed independently for each location. As expected, textures that derive from projections taken closer to the surface (e.g., Fig. 2(c) and (d)) have better quality when compared to projections taken far away from the surface (e.g., Fig. 2(a) and (b)). The question is how to create an unique triangulated mesh and texture and how it should be updated with local triangulated meshes from novel projections. Let us assume that there is a way of assessing the quality of each projection and in particular of each triangle in each mesh, so that it is possible to rank the triangles with respect to their quality. At first sight, several strategies can be used to fuse the textures, namely: (1) average the textures produced by local meshes; (2) insert vertices from new triangles into the existing mesh and re triangulate, provided that these triangles yield better quality; (3) remove the triangles of the existing mesh that overlap the triangle (of better quality) to be inserted, and then insert the triangle. Option (1) consists of averaging the textures provided by each local mesh. This could be achieved by setting the alpha channel of all local meshes so that they average out. Additionally, the average could be weighted by the quality of the triangles, although this was not tested in this work. The primitive would have several layers, each with a given local triangulated mesh belonging to each projection. Fig. 3 (a) shows the results obtained using this strategy. Visually, results are not appealing. Another disadvantage concerns the need to store all local meshes, which is highly inefficient in terms of memory. As seen in Fig. 2, there are textures with much better quality than others. To average good textures with bad textures does not seem to make sense. Option (2) proposes to address the problem by considering the vertices of the meshes only (rather than the triangles). Each vertex in the new mesh is added to the current mesh. This results in a super mesh containing all the vertices of the two previous meshes in which the triangles (the configuration of the mesh) are defined arbitrarily. The idea is to fuse using an additive strategy. Fig. 3(b) shows the results obtained using this approach. Again, results are not visually appealing. In option (3), an alternative to averaging is considered: a winner takes all strategy. The idea is to select, for each region in the surface, a single triangle which will provide the texture. This selection can be done using the quality of each triangle. Note that, whenever a triangle to be inserted overlaps some triangles in the existing mesh, these triangles must first be removed so that the mesh preserves its configuration and thus the quality of the texture mapping. Results obtained from this method are shown in Fig. 3(c). Textures are visually appealing. Artifacts present in the average and the additive strategies are not visible. There is one problem however: removed triangles often overlap triangles to be inserted in just a portion of their area (partial overlap). When deleted, these triangles leave empty spaces where no texture is defined. This is visible in Fig. 3(c). None of the strategies discussed provides textures of sufficient quality. Thus the problem of incrementally updating the texture is not trivial. In the following sections, an approach is presented which is capable of generating higher quality textures. 3 Proposed approach Fig. 4 shows a diagram which describes the functioning of the system. The following sections will describe these components in detail. First, a one-shot texture mapping based in DDT is presented in Section 3.1. Here, we propose an algorithm based on the extraction of edges in the image and the construction of a constrained Delaunay triangulation, which is operated as a DDT and is very efficient. Then, the incremental texture mapping approach is presented in Section 3.2. In this case, we propose a sequence of atomic operations to conduct the insertion of a new triangle in a triangulated mesh, which minimizes the changes in the configuration of the mesh. 3.1 One-shot texture mapping using data dependent triangulation In this paper, we propose an alternative solution to view dependent texture mapping. One reason for this is that the objective of this work is to develop a mechanism for mapping texture from images onto GPP (see [2]). Those geometric primitives consist of polygons, instead of the traditional triangles. The mapping of photometric properties can be performed by mapping triangles in image space to 3D space. These procedures are executed in Graphical Processing Units (GPUs), and programmed using OpenGL [15], Direct3D [16] or other graphics libraries. These libraries also have the functionalities of mapping convex polygons, but in fact these are mere high level functions that decompose the polygons in an arbitrary way into sets of triangles and then map texture onto those triangles. We argue that, if we control the process of triangulation in such a way that the edges in the images used in the projection are aligned with the edges of the triangles in 3D space, the distortion produced by linear texture mapping is not visible, and thus, linear texture mapping may be used instead of view dependent triangulation, which is much slower. In other words, if the triangles are especially defined so that their faces represent smooth regions with constant color then, a linear texture mapping over these could in fact provide accurate projections. This procedure of creating a triangulated mesh which accommodates some input data is called DDT [17], and the mapping of images using this technique will be referred to as DDT mapping as opposed to texture mapping. Unlike in standard texture mapping approaches, where the triangulation is executed in the 3D space, DDT triangulation operates in the image space, and only after those 2D triangles are mapped onto the 3D space. Although there are many approaches in the literature to the data dependent triangulation problem, most of them are focused on the fact that such a triangulated mesh is capable of producing very good data compression ratios with respect to the real image, while still maintaining low approximation errors. Real time performance of the algorithms has seldom been debated, with authors reporting processing times of over three seconds for 512×512 images. The exception was the study conducted in [18], where DDT was parallelized, resulting in a significant speed up. We propose a simple procedure similar to [12]: edges are detected using a Hough lines detector [19] extended to obtain a description of line segments instead of lines (e.g., see [20,21]). The triangulation is a Delaunay triangulation [22]: let the image be described by M line segments with starting points s m and endpoints e m , where each detected line segment is defined as s m e m ¯ . The Delaunay triangulation (Delaunay) receives the starting and endpoints as input to define the vertices of the triangulated mesh: (4) t = Delaunay ( { s 0 , e 0 , s 1 , e 1 , … , s M − 1 , e M − 1 } ) , where t is the resulting triangulated mesh. A constrained Delaunay triangulation is a generalization of the Delaunay triangulation where line segments may be imposed as belonging to the triangulated mesh (initially proposed by [23] for 2D spaces, later generalized to N dimensional spaces by [24]). A constrained Delaunay triangulation (cDelaunay) requires two inputs, a list of points and a list of line segments (also called constraints): (5) t = cDelaunay ( { s 0 , e 0 , … , s M − 1 , e M − 1 } , { s 0 e 0 ¯ , … , s M − 1 e M − 1 ¯ } ) . In brief, what we propose is a technique in which a constrained Delaunay triangulation is executed on the image space, having as input the line segments given by a line segment detection algorithm based on hough lines. 3.2 Incremental texture mapping Section 3.1 described how a constrained Delaunay triangulation may be used to produce a data dependent triangulated mesh that conforms with edges previously detected in the image. Note that this is a one-camera, one-shot approach, since it does not consider how to map more than one image. In reality, there is always a large set of images available to use for texture mapping, either from multiple cameras or from a unique camera at different times. This section addresses this problem of merging multiple projections into a single representation. As described in Section 3.1, a DDT triangulation is executed for each image used in a projection. Thus, there will be a triangulated mesh (a list of triangles) for each image (for each projection), to which we refer as local triangulated mesh. Local triangulated meshes for the example of Fig. 1 are shown in Fig. 2. Let M be the global triangulated mesh, defined in R 2 , so that only one global mesh exists per each primitive. This global mesh should be updated when new projections are collected or, in other words, when novel local meshes are received, i.e. it should contain the result of the fusion of the several textures. A local triangulated mesh from projection index j = { k , l , t } (i.e., form a given combination of k , l , t ) is denoted as T j . Local triangulated meshes contain T j number of triangles. Individual triangles are denoted as T i j , ∀ i ∈ { 0 , 1 , … , T { k , l , t } } , when indicating the i th triangle of the local mesh j , or as T { v 1 , v 2 , v 3 } j , in the case the vertices v 1 , v 2 and v 3 are specified. Likewise, triangles in the global mesh are notated as M n , ∀ n ∈ { 0 , 1 , … , N } , where N is the number of triangles in the global mesh. When the vertices of the triangles are specified, then the notation M { V 1 , V 2 , V 3 } is used. To continuously fuse local triangulated meshes from new projections onto the global mesh, we propose a mechanism which iterates all the triangles in the local projection mesh and decides whether they should be inserted in the global mesh by computing the benefit of this operation to the overall quality of the global mesh. At iteration i , triangle T i j from the local projection mesh is referred to as candidate triangle. First, the algorithm assesses if there is overlap between ( T i j ) and any of the existing triangles in the global mesh M n , ∀ n ∈ { 0 , 1 , … , N } . Let intr ( A , B ) be a function that tests intersection between triangles A and B . The test can be written as: (6) do_intersect = intr ( T { V 1 , V 2 , V 3 } j , M n ) , ∀ n ∈ { 0 , 1 , … , N } , where N corresponds to the total number of triangles in the global mesh M . The intersection of two triangles can result in an empty set, whenever there is no intersection, in a point, a line segment, or a polygon. There are several approaches to triangle triangle intersection tests, that provide fast and efficient algorithms [25–27]. Note that there is a distinction between overlap and intersection: what must be assessed is whether or not an insertion of the candidate triangle onto the global mesh will change its configuration. Thus, an overlap test is not the same as an intersection test, since there are some cases where the triangles do intersect but the mesh configuration is not altered. The overlap test is based on a set of rules that analyse the return of the intersection function (intr, implementation from [28]), between candidate triangle T i j and global mesh triangle M { V 1 , V 2 , V 3 } . It returns yes if the triangles overlap or no otherwise. The algorithm is detailed in Eqs. (7)–(9): (7) { no ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = ∅ yes ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = list of polygons Goto (8) ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = points X Goto (9) ⇐ intr ( T i j , M { V 1 , V 2 , V 3 } ) = line segment L where X = { X 0 , X 1 , … , X N } and L = { S 0 E 0 ¯ , … , S N E N ¯ } ; (8) { no ⇐ ∃ V g : V g = X o , ∀ V g ∈ { V 1 , V 2 , V 3 } , ∀ o ∈ { 0 , 1 , … , N } yes ⇐ otherwise (9) { no ⇐ ∃ V g : V g = S o ∧ ∃ V h : V h = E o , ∀ V g , V h ∈ { V 1 , V 2 , V 3 } , ∀ o ∈ { 0 , 1 , … , N } yes ⇐ otherwise . Fig. 5 (a) shows a global primitive mesh M that contains a single triangle (in blue), and a candidate triangle (in red). In each case, the geometries returned by the intersection function are as follows: an empty set (d), points (a) and (e), line segments (b) and (f), and polygons (c). If we consider the cases where the insertion of the candidate triangle (in red) does not change the configuration of the already existing global mesh (in this case, the initial global mesh is composed of a single triangle, in blue), we can say that in case (a), (b) and (c) the mesh would be altered, and that, in cases (d), (e) and (f) the mesh would remain unaltered. The overlap test returns a list L of indices of triangles from the global mesh which overlap with the candidate triangle. From this, the benefit of inserting the candidate triangle in the global mesh is assessed. In this context, benefit is defined as an improvement in the quality of texture and estimated as follows: (10) { beneficial ⇐ L is empty otherwise { beneficial , ⇐ q ( T i j ) > α ⋅ max ( q ( M g ) ) , ∀ g ∈ L not beneficial , ⇐ otherwise where α ≥ 1 is a user defined cost parameter, which defines how much better the quality of candidate triangle must be to any other triangle it overlaps, in order for the insertion to be considered beneficial, and q ( ⋅ ) is an arbitrary function that returns the estimated quality of each triangle. In this work we define quality as directly proportional to the resolution of the texture. An image provides a texture of higher resolution when it is closer to the GPP. In addition to this, the focal distance of the camera should be taken into account. Thus, the quality of a triangle T j = { k , l , t } is proposed as follows: (11) q ( T { k , l , t } ) = f l D { k , l , t } , where f l is the focal distance of camera index l and D { k , l , t } is the distance between the camera l and the GPP k computed at time t . When the insertion of a candidate triangle is considered to be beneficial, the next step is to execute the insertion in the global mesh. The global primitive mesh is built as a constrained Delaunay triangulation. Hence, a description of the mesh contains a set of vertices, edges and constraints (implementation from [22] is used). After insertion, the candidate triangle should be preserved on the updated mesh (since it had a larger quality than any global mesh triangle it overlapped). The configuration of all other non overlapped global mesh triangles should also be preserved. In order to comply with those objectives, the insertion of a candidate triangle is composed of a set of atomic operations executed in sequence. Fig. 6 will be used to demonstrate why the proposed set of operations is required, by comparing it to other possibilities. Fig. 6(a) shows the initial situation: an existing mesh in blue must be altered by the insertion of a candidate triangle in red. The blue triangle has all three edges constrained (blue squares, in Fig. 6(a)). Let insert ( V , M ) be a function that inserts vertex V into mesh M . (12) M ∗ = insert ( V , M ) , ∀ V ∈ { V a , V b , V c } , where M ∗ is the updated mesh. Fig. 6(b) shows the updated mesh after the insertion of the three vertices, indices 4 , 5 and 6 (see vertices indices in the Fig. 6). The updated mesh does not preserve the configuration of the candidate triangle. In other words, there is no triangle with vertices 4 , 5 , 6 in the updated mesh. The expression that asserts if the configuration of the candidate triangle is preserved can be stated as follows: (13) { preserve T { V a , V b , V c } j , if ( ∃ M { V d , V e , V f } ∗ ∈ M ∗ ) : V d = V a ∧ V e = V b ∧ V f = V c do not preserve T { V a , V b , V c } j , otherwise . One of the reasons why the simple insertion of the vertices does not work is that the existing mesh had some constrained edges. After the mesh is updated, these constraints continue to exist (see squares on edges 1–2, 2–3, and 1–3 in Fig. 6(b)). The configuration of the candidate triangle is not kept because no constraints over the edges of that triangle are set. Hence, the second alternative is to execute an additional operation on top of the insertion of vertices V a , V b and V c . Let add_constraint ( e , M ) be a function that adds a constraint on edge e . The operation can be expressed as follows: (14) M ∗ = add_constraint ( e , M ) , ∀ e ∈ { V a – V b , V b – V c , V a – V c } where V a – V b denotes the edge defined between vertices V a and V b . Fig. 6(c) shows the updated mesh after this procedure is executed. Also in this case the configuration of the candidate triangle is not preserved. The reason is that there are conflicting constraints inserted in the mesh. For example, initially, the global mesh had a constraint over edge 1–2 (see indices in Fig. 6). At the same time the constraint V a – V b is inserted into the mesh. Since these two constraints intersect, a new vertex is created at the intersection point (vertex 7). Since a vertex is created at the intersection of the two initial constrained edges, four new edges are created (edges 4–7, 7–8, 1–7 and 7–10). All of these edges are constrained. From Fig. 6(c), one can see that the overall result of this approach is that neither the candidate triangle nor the existing mesh is preserved. The reason is that contradictory (intersecting) constraints are inserted in the mesh. The solution is to remove the constraints from edges in the global mesh that intersect edges from the candidate triangle, prior to inserting the vertices and constraints of the candidate triangle. Let E = { e 0 , e 1 , … , e N , } be the list of the global mesh constrained edges that intersect any of the candidate triangle’s edges, and remove_constraint ( e , M ) a function that removes the constraint from edge e in the mesh M. The prepared mesh M ′ is obtained as follows: (15) M ′ = remove_constraint ( e , M ) , ∀ e ∈ E , and after this, the operations described in Eqs. (12) and (14) are executed. Fig. 6(d) shows the results of this approach. The mesh preparation stage detected the following intersections (indices in Fig. 6(a) and (d)): V a – V b intersects with V 1 – V 2 , V a – V b intersects with V 1 – V 3 , V b – V c intersects with V 1 – V 2 and V b – V c intersects with V 1 – V 3 . As a result, the constraints of edges V 1 – V 2 and V 2 – V 3 are removed. Note that in Fig. 6(d), the prepared mesh (not the initial global mesh) is shown in blue, and those constraints no longer appear. More important, the candidate triangle’s configuration is preserved (triangle 4–5–6). In this particular case, the initial configuration of the mesh is lost, since there was overlap between the candidate triangle and the initial mesh triangle. We now show an example of continuous update of the global mesh: three new projections ( C j = 1 , C j = 2 , C j = 3 ) are available to update to the initial mesh M . The projections are mapped sequentially, generating updated meshes M ∗ , M ∗ ∗ , etc. Each projection contains a single triangle to map to the global mesh. Triangles T { V a , V b , V c } 1 , T { V d , V e , V f } 2 and T { V g , V h , V i } 3 , correspond to projections C j = 1 , C j = 2 , C j = 3 , respectively. The quality of the triangles is such that the following holds: (16) q ( M n ) < q ( T { V a , V b , V c } 1 ) < q ( T { V d , V e , V f } 2 ) < q ( T { V g , V h , V i } 3 ) ∀ M n ∈ M , and the mesh update cost parameter is α = 1 , which means that there is no cost associated to the updating of the mesh (see Eq. (10)). In other words, the insertion of all three candidate triangles is considered beneficial. The initial mesh is shown in Fig. 7 (a), along with the three candidate triangles. Fig. 7(b) shows the mesh after the insertion of the first candidate triangle, i.e., M ∗ . Since there is no overlap, the candidate triangle is added to the mesh M { 4 , 5 , 8 } ∗ , and edges M { 4 – 5 } ∗ , M { 5 – 8 } ∗ , and M { 4 – 8 } ∗ are constrained. Also, since there was no overlap detected, the initial configuration of the mesh is preserved. The result of the second insertion is shown in Fig. 7(c). In this case, there is overlap between candidate triangle T { V d , V e , V f } 2 (seen in Fig. 7(a)) and triangle M { 2 , 5 , 7 } ∗ (seen in Fig. 7(b)). An intersection between edges V d – V e and edge M { 5 – 7 } ∗ (seen in Fig. 7(b)), is detected. As a result, the constraint from edge M { 5 – 7 } ∗ is removed. The insertion results in a new triangle M { 9 , 10 , 11 } ∗ ∗ . Note also that the overlapping triangle M { 2 , 5 , 7 } ∗ was not preserved, i.e., it does not exist in the new mesh M ∗ ∗ . Finally, the third insertion detects that triangle T { V g , V h , V i } 3 overlaps triangles M { 3 , 4 , 5 } ∗ ∗ , M { 4 , 5 , 8 } ∗ ∗ and M { 2 , 3 , 5 } ∗ ∗ . Edges M { 3 – 4 } ∗ ∗ , M { 4 – 5 } ∗ ∗ and M { 3 – 5 } ∗ ∗ intersect the edges of T { V g , V h , V i } 3 which is why their constraints are removed (actually, in this case they disappear after the candidate triangle is inserted). The insertion of candidate triangles sometimes creates not only the candidate triangle itself, but also some additional triangles on the mesh. It is the case, for example, of triangle M { 7 , 9 , 10 } ∗ ∗ . We refer to this type of triangles as orphan triangles, meaning they have no parent projection. These are shown in grey color in Fig. 8 . Unlike triangles with parent projections, these triangles do not belong to any projection and thus they do not derive from the DDT triangulation executed over an image of some projection. Because of this, there is no guarantee that these orphan triangles are compliant with edges in the projection images. For this reason, we propose that orphan triangles are set to have the quality 0. In summary, this approach for the update of a global primitive mesh consists of a set of procedures that are capable of updating the mesh whenever new, better quality triangles are available for insertion, but at the same time the mechanism is capable of filling the gaps left empty using orphan triangles. 4 Results This section shows results both from one-shot texture mapping using DDTs, as well as results from the algorithm proposed to conduct incremental texture mapping. 4.1 One-shot texture mapping Fig. 9 (a) shows the detection of line segments in an image. Fig. 9(b) displays the result of a Delaunay triangulation with arbitrary configuration, e.g. computed by giving only the vertices as input (green dots in Fig. 9(a)). Because the triangulated mesh has an arbitrary configuration, triangle often contain areas with multiple textures. This would cause problems when using affine texture mapping. Notice the large triangle that covers part of the roof of the building, as well as a portion of the sky. This triangle contains a significant change in color and thus its affine texture mapping would result inaccurate. Fig. 9(c) shows the result of the proposed DDT approach, where a constrained Delaunay triangulation is used. This triangulation is computed using as input the vertices as in the previous case but also the detected line segments (red lines in Fig. 9(a), constrained edges also shown in (c) with red lines). In this case, the large triangle described above does not exist. In fact, there are no triangles which contain both sky and roof. Thus, we can argue that the proposed approach creates a mesh in which triangles contain smooth color transitions. The next section addresses the incremental update of these triangulation meshes. 4.2 Incremental texture mapping To show the results of incremental texture mapping, we recover the example of Section 1 (see Fig. 1): the vehicle approaches a wall panel, which has the word START written on it and collects four images in sequence (color coded black-red-orange-yellow in Fig. 10 ). The global mesh is created with the first image and then updated three times. The global triangulated mesh at each iteration is shown in Fig. 10 (top). Textures for each of these cases are show in Fig. 10 (bottom). Projection C { k = 4 , l = 1 , t = t 1 } is used to create the global mesh. Thus, the global mesh is composed only of triangles with parent projection C { k = 4 , l = front center , t = t 1 } (black triangles in Fig. 10(a)). Then, a new projection C { k = 4 , l = 1 , t = t 2 } becomes available. The global mesh is updated (Fig. 10(b)), and now contains a majority of triangles from C { k = 4 , l = 1 , t = t 2 } (red triangles). Then projection C { k = 4 , l = 1 , t = t 3 } is mapped. Since only a right side portion of the primitive is seen, orange triangles can be observed on the right side of Fig. 10(c)), while the left side retains red colored triangles from previous projections. Orphan triangles (in blue) are generated to fill the gaps that appear between the triangles with parent projections. Finally, projection C { k = 4 , l = 3 , t = t 4 } is used. This image views only the left portion of the wall panel. As such, we can see yellow triangles on the left side of (Fig. 10(d)). This example shows how the proposed mechanism is capable of creating and maintaining a global triangulated mesh which is used for enhancing the texture mapped onto the GPPs whenever new (and better) images are collected. 4.3 Projection of a single camera onto the ground plane This section shows three examples of how the global primitive mesh evolves when using a single camera to map a single primitive. We consider a similar scene to the one presented in Fig. 1. Throughout the three time instants t = t 1 , t = t 2 and t = t 3 , the vehicle is moving forward. From t 1 to t 2 the vehicle drives straight, and from t 2 to t 3 the vehicle turns slightly to the right. In this case the primitive that represents the ground plane is used for texture mapping ( k = 0 ). As a consequence, there is always a portion of the images from the projections that view the ground. In other words, at all instants any of the cameras view a portion of the ground, since they are pointed downwards. We will consider three different cases, each generating a unique scene representation: In the first case only the front center camera ( l = 1 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 1 , t = t 1 } , C { k = 0 , l = 1 , t = t 2 } and C { k = 0 , l = 1 , t = t 3 } ; In the second case only the rear center camera ( l = 4 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 4 , t = t 1 } , C { k = 0 , l = 4 , t = t 2 } and C { k = 0 , l = 4 , t = t 3 } ; In the third case only the front left camera ( l = 3 ) is used for projection. Hence there will be three projections: C { k = 0 , l = 3 , t = t 1 } , C { k = 0 , l = 3 , t = t 2 } and C { k = 0 , l = 3 , t = t 3 } . The final global primitive meshes (those obtained after inserting projections at times t 1 , t 2 and t 3 ) for each case are displayed in Fig. 11 (left column). Fig. 11 (right column) shows the percentage of triangles of the global mesh that belong to each projection, as a function of the mission time. Note that the final position of the vehicle (which is the same for all cases) is depicted in the images, and bear in mind that, during this sequence, the vehicle moves forward from the right to the left. The triangles of the global primitive mesh are shown in colors, where each color corresponds to a particular projection. Fig. 11 (first row, left) shows the distribution of triangles according to the parent projection. In this case, the images are provided by the front center camera. As the vehicle moves forward, the ground in front of the vehicle that has been previously mapped by previous projections is now visible in images at a closer range. This leads to the effect that more recent projections tend to override older projections, i.e., red color ( t 2 ) overrides black color ( t 1 ), and yellow color ( t 3 ) overrides the other two. Fig. 11 (first row, right) shows that at time t 1 , only triangles from the first projection (black) and orphan triangles (blue) exist. Then, at time t 2 , the triangles from the second projection (red) are added to the global mesh. As a consequence, the percentage of triangles from the first projection (black) decreases. At time t 3 , the third projection again takes the major slice of percentage with respect to the previous two projections. In front facing cameras, when the vehicle is moving forward, more recent projections tend to contribute with a larger portion of the total triangles in the global mesh. The second case is shown in 11 (second row, left). Here, since the camera is facing the rear side of the vehicle, the opposite phenomena occurs: the vehicle is moving away from the ground behind it, and thus older projections were taken at closer distances to the ground. As a consequence, the red color (projection at t 2 ) overrides the yellow color (projection at t 3 ), and the black color (projection at t 1 , the oldest one) overrides all others. This is observable in Fig. 11 (second row, right), where the first projection (black) is, at all times, the one with the largest percentage of the triangles. Fig. 11 (third row, left) shows the third case. Here, since the camera is facing the left side of the vehicle, a hybrid phenomena takes place. For each projection, there is always a portion of the triangles, i.e., those that map the ground directly in front of the camera for that particular instant, that have a higher projection quality when compared to others. Fig. 11 (third row, right) also shows this tendency: the percentage of projections tends to be the same for all projections, which is why the second projection (red) when first mapped at time t 2 achieves approximately the same percentage of triangles as the first projection (black). They continue to have similar percentages also at time t 3 . At time t 3 , the third projection (yellow) obtained a higher value of percentage because the vehicle as turned slightly to the right and the left camera faced an area of the ground that was not previously mapped by any of the previous projections. 4.4 Projection of multiple cameras onto the ground plane The examples given in Section 4.3 have shown that the proposed algorithm is capable of handling multiple projections, correctly determining which are the best quality projections to map onto the global mesh. Nonetheless, those examples were simplified since only one camera was considered to provide projections in each case. In this section, the five cameras onboard the Talos are used to provide projection to be mapped onto the ground plane. The same sequence is used: the vehicle is moving forward and three time instants are used to generate projections. Each time instant t 1 , t 2 and t 3 generates five projections, one for each camera. Fig. 12 (a) shows the state of the global mesh after time t 1 . Five projections are contained in the mesh. At time t 2 , the global mesh incorporates many of the projections that are computed at this time (Fig. 12(b)). The same occurs at time t 3 (Fig. 12(c)). Note that these images are not exactly the same as those in Fig. 11, because in that case only the final global projection mesh was shown for three different examples. Here, we show the state of a single global projection mesh at times t 1 , t 2 and t 3 . Thus, in this case it is possible to see how the mesh evolved as more projections became available. The resulting mesh is an intricate mosaic of triangles coming from several projections. At time t 1 , the area of projection from the rear center camera was not connected to the areas of projection of the other cameras. Note that the red triangles in Fig. 12(a) are not connected to any triangle with a parent projection, only to orphan triangles. This unmapped region corresponds to the ground that was below the vehicle at time t 1 . Obviously, there is no coverage from the cameras for that area, so the system handles this by defining orphan triangles (blue) to cover that area. At time t 2 , the vehicle has moved forward, and the uncovered ground is now visible from the rear camera. Hence, the areas mapped by the rear cameras connect to the areas mapped by the other cameras, as seen in Fig. 12(b). At time t 3 , since the vehicle has turned to the right, the rear camera now views a different portion of the ground that had not been captured by any other camera. Note, in Fig. 12(c), how the triangles of the rear camera (the brightest yellow at the bottom right side) map a region that was not seen before and was previously covered only by orphan (blue) triangles. Fig. 12(d) shows the percentage of triangles of each projection as a function of the mission time. As each time instant, only newly acquired projections are used to update the mesh. Hence, triangles from previous iterations, if removed, will not be retested for insertion. That means each triangles is tested for insertion a single time. If a triangle is removed, it will never again be reinserted. This can be observed in the Figure, since none of the projections increases the percentage of triangles it contains. Fig. 13 shows the fifteen images used to compute these representations. As the vehicle moves and turns around, more and more of the ground that had not been viewed before is covered by new projections. This is a clear example of why integrating several projections over time is advantageous. A composite photometric description of the environment can be obtained that was impossible to compute without the capability of integrating multiple projections over time in an incremental fashion. The incremental texture mapping of an entire scenario can be observed in The scenario is composed of the entire (MIT) sequence. All five cameras onboard the Talos vehicle are used as input to the texture mapping. Geometric primitives are represented in the environment by the blue–green polygons. A blue to green colormap is used to color the primitives according to their index, the more recently detected the primitive, the closer to green it is. Photometry is represented by the texture mapped onto the primitives. Note that at each of the time instants new projections will update the global meshes of the detected polygonal primitives. Hence the scenario representation will evolve photometrically over time. Furthermore, also the geometric representation will evolve over time (see [2] for details). For a better visualization, the primitive that represents the ground plane is not textured in the video. 5 Conclusions This paper addressed the problem of how to create and update a triangulated mesh. These meshes are used for texture mapping surfaces in 3D, and the input are images collected from cameras mounted on-board a vehicle. The geometric structure onto which texture is mapped is described in detail in [2] and given as a list of polygons. Because the atomic entities of the 3D structure are defined as polygons (rather than triangles), it is possible to perform a triangulation of the convex hull of that polygon, as opposed to having an arbitrary triangulation. This triangulation is computed in the image space, and is defined as a constrained Delaunay triangulation. This makes it possible to impose line segments as constrained edges in the triangulation, which creates triangles with smooth color transitions. This, in turn, makes it possible to use affine texture mapping. Incremental texture mapping is done by creating and updating a global triangulated mesh per geometric primitive. The update of this mesh is done using meshes created from projections. In this paper, we have proposed a sequence of operations which are used for inserting triangles from the projection mesh into the global triangulation mesh. This procedure ensures that the inserted triangles maintain their configuration as well as the existing triangles which do not overlap the inserted triangles. Furthermore, the proposed algorithm fills the gaps in the mesh where there are to triangles with parent projections with orphan triangles. Using this mechanism, the holes that could exist between textures of different projections are replaced by orphan triangles where texture is interpolated, resulting in a better overall quality of the texture. To the best of our knowledge, this is the first approach in this field capable of fusing images continuously and in an incremental fashion in order to generate a single texture of good quality. Acknowledgments This work has been supported by the Portuguese Foundation for Science and Technology “Fundação para a Ciência e Tecnologia” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador”, reference CEB-02502014. References [1] M. Oliveira V. Santos A.D. Sappa P. Dias Scene representations for autonomous driving: An approach based on polygonal primitives Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Volume 1 2016 Springer International Publishing Cham 503 515 (Chapter) [2] M. Oliveira V. Santos A. Sappa P. Dias A.P. Moreira Incremental scenario representations for autonomous driving using geometric polygonal primitives Robot. Auton. Syst. 2016 10.1016/j.robot.2016.05.011 in press [3] A.S. Huang M. Antone E. Olson L. Fletcher D. Moore S. Teller J. Leonard A High-rate, Heterogeneous data set from the DARPA urban challenge Int. J. Robot. Res. 29 13 2011 1595 1601 [4] A. Huang, E. Olson, D. Moore, LCM: Lightweight communications and marshalling, in: 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, 2010, pp. 4057–4062. [5] M. Segal C. Korobkin R. van Widenfelt J. Foran P. Haeberli Fast shadows and lighting effects using texture mapping SIGGRAPH Comput. Graph. 26 2 1992 249 252 [6] P. Debevec Y. Yu G. Boshokov Efficient view-dependent image-based rendering with projective texture-mapping, Tech. Rep. 1998 [7] S. Rippa Long and thin triangles can be good for linear interpolation SIAM J. Numer. Anal. 29 1 1992 257 270 [8] M. Garland P.S. Heckbert Fast polygonal approximation of terrains and height fields, Tech. Rep. CMU-CS-95-181 1995 [9] R. Schätzl H. Hagen J.C. Barnes B. Hamann K.I. Joy Data-dependent triangulation in the plane with adaptive knot placement Geometric Modelling 2001 309 321 [10] H. Hoppe, Progressive meshes, in: Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’96, 1996, pp. 99–108. [11] L. Demaret N. Dyn A. Iske Image compression by linear splines over adaptive triangulations Signal Process. 86 7 2006 1604 1616 [12] A.D. Sappa M.A. García Coarse-to-fine approximation of range images with bounded error adaptive triangular meshes J. Electron. Imaging 16 2 2007 23010 [13] N. Dyn D. Levin S. Rippa Boundary correction for piecewise linear interpolation defined over data-dependent triangulations J. Comput. Appl. Math. 39 2 1992 179 192 [14] L.L. Schumaker Computing optimal triangulations using simulated annealing Comput. Aided Geom. Design 10 3–4 1993 329 345 [15] Opengl D. Shreiner M. Woo J. Neider T. Davis OpenGL(R) Programming Guide: The Official Guide to Learning OpenGL(R), Version 2 fifth ed. 2005 Addison-Wesley Professional [16] D. Blythe The direct3d 10 system ACM Trans. Graph. 25 3 2006 724 734 [17] B. Lehner G. Umlauf B. Hamann Survey of techniques for data-dependent triangulations H. Hagen M. Hering-Bertram C. Garth GI Lecture Notes in Informatics, Visualization of Large and Unstructured Data Sets 2007 178 187 [18] M. Cervenansksy Z. Toth J. Starinsky A. Ferko M. Sramek Parallel gpu-based data-dependent triangulations Comput. Graph. 34 2 2010 125 135 [19] I. Svalbe Natural representations for straight lines and the hough transform on discrete arrays IEEE Trans. Pattern Anal. Mach. Intell. 11 9 1989 941 950 [20] V. Kamat, S. Ganesan, A robust hough transform technique for description of multiple line segments in an image, in: 1998 International Conference on Image Processing, 1998. ICIP 98. Proceedings. Vol. 1, 1998, vol. 1, pp. 216–220. [21] R. Guerreiro, P. Aguiar, Incremental local hough transform for line segment extraction, in: 2011 18th IEEE International Conference on Image Processing, ICIP, 2011, pp. 2841–2844. [22] M. Yvinec 2D triangulations CGAL User and Reference Manual, 4.0 Edition 2012 CGAL Editorial Board [23] L.P. Chew, Constrained delaunay triangulations, in: Proceedings of the Third Annual Symposium on Computational Geometry, SCG ’87, 1987, pp. 215–222. [24] J.R. Shewchuk General-dimensional constrained delaunay and constrained regular triangulations i: Combinatorial properties Discrete Comput. Geom. 39 1 2008 580 637 [25] T. Moller A fast triangle-triangle intersection test J. Graph. Tools 2 1997 25 30 [26] J.-W. Chang M.-S. Kim Efficient triangle–triangle intersection test for OBB-based collision detection Comput. Graph. 33 3 2009 235 240 [27] A.D. Sappa, M.A. García, Incremental multiview integration of range images, in: ICPR, 2000, pp. 1546–1549. [28] E. Fogel R. Wein B. Zukerman D. Halperin 2D regularized Boolean set-operations CGAL User and Reference Manual, 4.0 Edition 2012 CGAL Editorial Board Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Vítor Santos obtained a 5 year degree in Electronics Engineering and Telecommunications in 1989, at the University of Aveiro, Portugal, where he later obtained a Ph.D. in Electrical Engineering in 1995. He was awarded fellowships to pursue research in mobile robotics during 1990 1994 at the Joint Research Center, Italy. He is currently Associate Professor at the University of Aveiro and lectures courses related to advanced perception and robotics, and has managed research activity on mobile robotics, advanced perception and humanoid robotics, with the supervision or cosupervision of more than 100 graduate and undergraduate students, and more that 120 publications in conferences, books and journals. At the University of Aveiro he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and has coordinated the development of ATLASCAR, the first real car with autonomous navigation capabilities in Portugal. He is one of the founders of Portuguese Robotics Open in 2001 where he has kept active participation ever since. He is also cofounder of the Portuguese Society of Robotics, and participated several times in its management since its foundation in 2006. His current interests extend to humanoid robotics and the application of techniques from perception and mobile robotics to autonomy and safety in ADAS contexts. Angel Domingo Sappa (S’1994–M’00–SM’12) received the Electromechanical Engineering degree from National University of La Pampa, General Pico, Argentina, in 1995, and the Ph.D. degree in Industrial Engineering from the Polytechnic University of Catalonia, Barcelona, Spain, in 1999. In 2003, after holding research positions in France, the UK, and Greece, he joined the Computer Vision Center, Barcelona, where he is currently a Senior Researcher. He is a member of the Advanced Driver Assistance Systems Group. His research interests span a broad spectrum within the 2D and 3D image processing. His current research focuses on stereoimage processing and analysis, 3D modelling, and dense optical flow estimation. Paulo Dias graduated from the University of Aveiro Portugal in 1998 and started working in 3D reconstruction at the European Joint research Centre in Italy. In September 2003, he concluded his Ph.D. with the thesis “3D Reconstruction of real World Scenes Using Laser and Intensity Data”. He is currently an assistant professor within the Department of Electronics Telecommunications and Informatics (DETI) and is involved in several works and projects within the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) related to 3D Reconstruction, Virtual Reality, Computer Vision, Computer Graphics, Visualization and Combination and Fusion of data from multiple sensors. António Paulo Moreira graduated with a degree in electrical engineering at the University of Oporto, in 1986. He then pursued graduate studies at University of Porto, obtaining a M.Sc. degree in electrical engineering—systems in 1991 and a Ph.D. degree in electrical engineering in 1998. Presently, he is an Associate Professor at the Faculty of Engineering of the University of Porto and researcher and manager of the Robotics and Intelligent Systems Centre at INESC TEC. His main research interests are process control and robotics. "
    },
    {
        "doc_title": "Incremental scenario representations for autonomous driving using geometric polygonal primitives",
        "doc_scopus_id": "85006515693",
        "doc_doi": "10.1016/j.robot.2016.05.011",
        "doc_eid": "2-s2.0-85006515693",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3d representations",
            "Autonomous Vehicles",
            "Geometric structure",
            "Point cloud",
            "Polygonal primitives",
            "Range measurements",
            "Reconstruction techniques",
            "Scene reconstruction"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.When an autonomous vehicle is traveling through some scenario it receives a continuous stream of sensor data. This sensor data arrives in an asynchronous fashion and often contains overlapping or redundant information. Thus, it is not trivial how a representation of the environment observed by the vehicle can be created and updated over time. This paper presents a novel methodology to compute an incremental 3D representation of a scenario from 3D range measurements. We propose to use macro scale polygonal primitives to model the scenario. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Furthermore, we propose mechanisms designed to update the geometric polygonal primitives over time whenever fresh sensor data is collected. Results show that the approach is capable of producing accurate descriptions of the scene, and that it is computationally very efficient when compared to other reconstruction techniques.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-06-07 2016-06-07 2016-08-04 2016-08-04 2016-08-04T18:32:25 S0921-8890(16)30060-4 S0921889016300604 10.1016/j.robot.2016.05.011 S300 S300.1 FULL-TEXT 2020-11-22T06:55:28.343358Z 0 0 20160901 20160930 2016 2016-06-07T15:16:25.691999Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid orcid primabst ref vitae 0921-8890 09218890 true 83 83 C Volume 83 27 312 325 312 325 201609 September 2016 2016-09-01 2016-09-30 2016 article fla © 2016 Elsevier B.V. All rights reserved. INCREMENTALSCENARIOREPRESENTATIONSFORAUTONOMOUSDRIVINGUSINGGEOMETRICPOLYGONALPRIMITIVES OLIVEIRA M 1 Introduction 2 Related work 3 Proposed approach 3.1 One-shot scene reconstruction 3.2 Incremental scenario reconstruction 4 Results 4.1 One-shot reconstruction 4.2 Incremental reconstruction 4.3 Comparison of approaches with and without expansion 5 Conclusions Acknowledgments References BIRK 2009 53 60 A BURGARD 2009 757 758 W OLIVEIRA 2016 M HUANG 2011 1595 1601 A ONIGA 2010 1172 1182 F ZHOU 2011 669 681 K THRUN 2006 661 692 S URMSON 2006 467 508 C URMSON 2008 425 466 C MONTEMERLO 2008 M BACHA 2008 467 492 A CHEN 2011 762 775 Y DEMEDEIROSBRITO 2008 1130 1140 A BERNARDINI 1999 349 359 F BYKAT 1978 296 298 A BARBER 1996 469 483 C AICHHOLZER 1995 752 761 O OLIVEIRAX2016X312 OLIVEIRAX2016X312X325 OLIVEIRAX2016X312XM OLIVEIRAX2016X312X325XM 2018-08-04T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. 2020-11-19T10:33:14.232Z S0921889016300604 Portuguese Foundation for Science and Technology FCT Fundação para a Ciência e a Tecnologia ERDF European Regional Development Fund FEDER European Regional Development Fund Spanish Government TIN2014-56919-C3-2-R Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación CEB-02502014 SENESCYT Secretaría de Educación Superior, Ciencia, Tecnología e Innovación Operational Programme for Competitiveness and Internationalisation Fundação para a Ciência e Tecnologia POCI-01-0145-FEDER-006961 SFRH/BD/43203/2008 UID/CEC/00127/2013 SFRH/BPD/109651/2015 INCT-EN Instituto Nacional de Ciência e Tecnologia para Excitotoxicidade e Neuroproteção COMPETE POFC Programa Operacional Temático Factores de Competitividade This work has been supported by the Portuguese Foundation for Science and Technology “ Fundação para a Ciência e Tecnologia ” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador ”, reference CEB-02502014 . item S0921-8890(16)30060-4 S0921889016300604 10.1016/j.robot.2016.05.011 271599 2016-08-04T15:40:51.774542-04:00 2016-09-01 2016-09-30 true 5888810 MAIN 14 58041 849 656 IMAGE-WEB-PDF 1 fx5 12830 69 83 gr4 17555 76 219 gr2 28101 119 219 fx1 23862 150 219 gr14 15759 58 219 fx6 24677 164 125 fx3 12077 56 49 pic5 33838 164 140 pic2 25222 164 141 gr15 17370 87 219 gr8 14416 27 219 fx4 12819 69 83 gr12 27038 127 219 gr1 21146 87 219 fx2 12077 56 49 gr5 22864 164 134 gr13 16887 164 205 pic3 23197 163 140 gr7 30380 136 219 gr6 53566 152 219 gr9 16879 87 219 pic4 25191 164 141 gr11 29235 115 219 pic1 24035 163 140 gr16 24620 131 219 gr3 18915 65 219 gr10 22449 111 219 fx5 29274 15 18 gr4 68423 200 580 gr2 68835 205 376 fx1 97662 388 565 gr14 73492 200 757 fx6 187395 742 566 fx3 28364 13 11 pic5 40387 131 112 pic2 38463 131 113 gr15 65788 226 572 gr8 45793 78 631 fx4 29234 15 18 gr12 149219 356 616 gr1 83494 246 619 fx2 28364 13 11 gr5 160412 724 591 gr13 51934 475 593 pic3 36162 132 113 gr7 132506 412 662 gr6 89241 236 339 gr9 61854 228 573 pic4 34967 131 113 gr11 148501 300 572 pic1 37492 132 113 gr16 101169 373 625 gr3 81281 161 545 gr10 75175 289 570 si25 624 18 180 si21 159 15 10 si46 131 11 12 si4 139 12 10 si14 135 12 10 si10 112 11 6 si26 165 17 13 si15 182 15 22 si11 574 21 138 si12 152 11 15 si9 180 18 14 si51 661 21 142 si54 574 15 150 si2 165 13 17 si48 1945 41 389 si16 225 15 35 si34 133 11 11 si18 735 31 184 si1 309 18 50 si47 344 15 58 si41 117 10 11 si6 397 18 68 si27 143 13 12 si13 166 15 18 si36 159 15 11 si42 131 15 11 si5 318 18 49 si49 207 12 25 si3 114 10 8 si50 165 11 21 si24 174 14 22 si35 170 14 19 si22 195 14 27 si55 568 15 149 si45 139 11 13 si33 123 8 10 si32 120 8 10 si19 844 12 234 ROBOT 2641 S0921-8890(16)30060-4 10.1016/j.robot.2016.05.011 Elsevier B.V. Fig. 1 Plane detection examples using RANSAC: (a) five best RANSAC candidates for the input point cloud in grey; (b) a detail of (a); (c) without using clustering; (d) using clustering. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Scenario reconstruction approach (1): BPA surface reconstruction over the accumulated point cloud of three scenes. Fig. 3 Scenario reconstruction approach (2): Accumulation of scene reconstructions over multiple scenes, each marked with a different color; (a) BPA; (b) GPP 2, shown without the ground plane for an easier visualization. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 4 Orthogonal component of the expansion operation: (a) points are tested for their orthogonal distance to the support plane; (b) included points are projected to the support plane. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 Successive iterations of the longitudinal expansion of GPPs. Solid lines in color represent the convex hull at the start of a given iteration, dashed lines the expanded polygon. Crosses over points mean they were added to the polygon in a given expansion: (a) initial situation; (b), (c), (d), (e) and (f) iterations 0–4, respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 MIT sequence. Fig. 7 Location C of the MIT sequence: (a) 3d view; (b) top view; (c) satellite view of the location; (d) front 6 mm camera; (e) front; (f) rear; (g) left (h) right. Fig. 8 Detection of geometric polygonal primitives in the data-sets of the MIT sequence: (a) location C; (b) location D. Fig. 9 Cascade processing analysis for MIT sequence locations A through E: (a) the number of points left to process for a given input point cloud, as a function of the index of the detected primitive; (b) the time it takes to perform the detection of each of the geometric polygonal primitives as a function of the primitives index. Fig. 10 Reconstruction of location E of MIT sequence: (a) BPA; (b) GT; (c) POIS; (d) GPP2. The observation of Fig. 16(e) may help the reader better understand the viewpoints in these images. Fig. 11 Qualitative analysis of the one sided Hausdorff distance in location C sequence 1: (a) GT; (b) POIS; (c) GPP 1; (d) GPP 2; A Red–Green–Blue color map is used to code the distance. Red represents zero distance and blue maximum distance. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Results from the Hausdorff distance obtained when using alternatives for the GPP 2 method for location E, sequence 1: (a) the standard GPP 2, with ground plane and convex hull; (b) discarded ground plane, convex hull; (c) with ground plane, concave hull; (d) discarded ground plane, concave hull. A Red–Green–Blue color map is used to code the distance. Red represents zero distance and blue maximum distance. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 GPP reconstruction for sequence 1: (a) The reconstructed scenario after the input point cloud of location A is received; (b) after location B; (c) after location C; (d) after location D; (e) after location E. Fig. 14 (a) Comparison between the number of polygons generated by the algorithm using expansion and not using expansion through sequence 1; (b) number of points to use as input to the detection in both cases; (c) Total area of the detected polygons. Fig. 15 Analysis of the evolution of each of the polygons through sequence 1. Only pair index polygons are shown to simplify the graphs: (a) number of support points per polygon; (b) total area of the primitives. Fig. 16 Evolution of polygonal primitive 0 (the ground plane) through sequence 1: (a) location A; (b) location B; (c) location C; (d) location D; (e) location E. Table 1 LIDAR sensor systems mounted on some autonomous vehicle systems of recent years. Institution Vehicle Ref. 3D sensor type Total a 2D laser 3D laser Stanford U. Stanley b [11] 5 × Sick LMS 291 – 67.5 CMU Sandstorm b [12] 3 × Sick LMS 291 – 50.5 Highlander b Riegl Q140i – CMU Boss b [13] 6 × Sick LMS 291 Vel. HDL-64 2305.0 2 × Continental ISF 172 – 2 × IBEO Alasca XT – Stanford U. Junior c [14] 4 × SICK LMS 291 Vel. HDL-64 2278.0 2 × IBEO Alasca XT – Virginia Tech Odin c [15] 4 × Sick LMS 291 – 90.0 2 × IBEO Alasca XT – IBEO Alasca AO – MIT Talos c [6] 12 × Sick LMS 291 Vel. HDL-64 2361.2 U. Munich MuCar-3 d [16] – Vel. HDL-64 2200.0 Google Driverless Car [17] – Vel. HDL-64 2200.0 a Estimation of total 3D data throughput of all LIDAR sensors mounted on the vehicle, given as points × 10 3 / s . b These vehicles participated in the Defense Advanced Research Projects Agency (DARPA) Grand Challenge 2006. c These vehicles participated in the DARPA Urban Challenge 2007. d This vehicle participated in the Civilian European Land Robot Trial (ELROB) Trial 2009. Table 2 Information on each of the locations defined in the MIT sequence. Columns description: pt , number of points; size, memory size in mega bytes; t , mission time in seconds; d , traveled distance in meters. Location name Location snapshot Sequence accumulated pt ( × 10 6 ) Size (MB) a pt ( × 10 6 ) Size (MB) a t (s) d (m) A 1.3 15.6 1.3 15.6 1 0 B 1.3 15.6 13.0 156.0 11 75 C 1.3 15.6 26.0 312.0 21 125 D 1.3 15.6 39.0 468.0 31 140 E 1.3 15.6 52.0 624.0 41 190 a Computed from the number of points times the three xyz dimensions times the four bytes for each dimension (type float32). It is an approximate value since additional data is present in the message, such as the time stamp, the coordinate frame identification, etc. Table 3 Comparison of the computation time of several approaches for surface reconstruction on the MIT sequence. Results were obtained with an i7-860 2.8 GHz quad core processor, Ubuntu operating system. Sequence/Location Processing time (s) BPA GT POIS GPP 1 GPP 2 A 659.0 154.0 63.2 16.3 27.3 B 752.9 157.5 61.6 25.3 17.4 C 488.2 156.3 56.3 13.5 49.4 D 480.4 142.4 52.6 25.2 25.2 E 558.8 149.0 57.9 47.4 58.1 Average 585.9 151.8 58.3 25.5 35.5 Table 4 Comparison of the accuracy of the several approaches using BPA results as ground truth and Hausdorff distance as metric. Location Hausdorff distance (m) GT POIS GPP 1 GPP 2 max mean max mean max mean max mean A 11.7 0.15 14.0 1.39 7.6 1.02 7.6 0.87 B 11.8 0.12 14.1 1.39 12.7 0.94 12.6 0.81 C 12.7 0.18 13.9 1.06 8.9 0.87 8.9 0.69 D 13.8 0.10 13.9 1.90 7.6 0.86 7.6 0.69 E 12.5 0.14 14.0 1.42 14.0 1.25 14.0 1.11 Average 12.5 0.14 13.9 1.43 10.2 0.99 10.1 0.83 Table 5 Comparison of the Hausdorff distance accuracy of the GPP 2 approach using: the standard approach, convex hull and ground plane included (also in Table 4); the convex hull with no ground plane included; the concave hull with ground plane; and the concave hull without ground plane. GPP 2 Hausdorff distance (m) Hull Convex Convex Concave Concave Ground plane Included Not included Included Not included Location max mean max mean max mean max mean A 7.6 0.87 1.8 0.15 6.8 0.71 1.2 0.13 B 12.6 0.81 1.5 0.11 12.6 0.53 1.1 0.08 C 8.9 0.69 1.9 0.16 6.6 0.52 1.9 0.12 D 7.6 0.69 2.2 0.14 7.3 0.59 2.1 0.11 E 14.0 1.11 1.7 0.10 8.8 0.32 1.4 0.08 Average 10.1 0.83 1.8 0.13 8.4 0.53 1.5 0.10 Table 6 Online videos showing the system running for the MIT sequence. The non incremental approach corresponds to the continuous processing without using the expansion mechanism. It is possible to observe that because there is no expansion, the primitives are duplicated. URL Description Input data from MIT sequence Incremental approach Non incremental approach Incremental scenario representations for autonomous driving using geometric polygonal primitives Miguel Oliveira a b ⁎ Vitor Santos b Angel D. Sappa c d Paulo Dias b A. Paulo Moreira a e a INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal b IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal IEETA - Institute of Electronics and Informatics Engineering of Aveiro, Universidade de Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal c Facultad de Ingeniería en Electricidad y Computación, Escuela Superior Politécnica del Litoral, ESPOL, Campus Gustavo Galindo, Km 30.5 vía Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador Facultad de Ingeniería en Electricidad y Computación, Escuela Superior Politécnica del Litoral, ESPOL, Campus Gustavo Galindo Km 30.5 vía Perimetral P.O. Box 09-01-5863 Guayaquil Ecuador d Computer Vision Center, Campus UAB, 08193 Bellaterra, Barcelona, Spain Computer Vision Center, Campus UAB Bellaterra Barcelona 08193 Spain e FEUP - Faculty of Engineering, University of Porto, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal FEUP - Faculty of Engineering, University of Porto R. Dr. Roberto Frias s/n Porto 4200-465 Portugal ⁎ Corresponding author at: INESC TEC - INESC Technology and Science, R. Dr. Roberto Frias s/n, 4200-465 Porto, Portugal. INESC TEC - INESC Technology and Science R. Dr. Roberto Frias s/n Porto 4200-465 Portugal When an autonomous vehicle is traveling through some scenario it receives a continuous stream of sensor data. This sensor data arrives in an asynchronous fashion and often contains overlapping or redundant information. Thus, it is not trivial how a representation of the environment observed by the vehicle can be created and updated over time. This paper presents a novel methodology to compute an incremental 3D representation of a scenario from 3D range measurements. We propose to use macro scale polygonal primitives to model the scenario. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Furthermore, we propose mechanisms designed to update the geometric polygonal primitives over time whenever fresh sensor data is collected. Results show that the approach is capable of producing accurate descriptions of the scene, and that it is computationally very efficient when compared to other reconstruction techniques. Keywords Incremental scene reconstruction Point clouds Autonomous vehicles Polygonal primitives 1 Introduction Recent research in the fields of pattern recognition suggest that the usage of 3D sensors improves the effectiveness of perception, “since it supports good situation awareness for motion level tele-operation as well as higher level intelligent autonomous functions” [1]. Nowadays, autonomous robotic systems have at their disposal a new generation of 3D sensors, which provide 3D data of unprecedented quality [2]. In robotic systems, 3D data is used to compute some form of internal representation of the environment. In this paper, we refer to this as 3D scene representation or simply 3D representation. The improvement of 3D data available to robotic systems should pave the road for more comprehensive 3D representations. In turn, advanced 3D representations of the scenes are expected to play a major role in future robotic applications since they support a wide variety of tasks, including navigation, localization, and perception [3]. In summary, the improvement in the quality of 3D data clearly opens the possibility of building more complex scene representations. In turn, more advanced scene representations will surely have a positive impact on the overall performance of robotic systems. Despite this, complex scene representations have not yet been substantiated into robotic applications. The problem is how to process the large amounts of 3D data. In this context, classical computer graphics algorithms are not optimized to operate in real time, which is a non-negotiable requirement of the majority of robotic applications. Unless novel and efficient methodologies that produce compact, yet elaborate scene representations, are introduced by the research community, robotic systems are limited to mapping the scenes in classical 2D or 2.5D representations or are restricted to off-line applications. Very frequently, the scenarios where autonomous systems operate are urban locations or buildings. Such scenes are often characterized for having a large number of well defined geometric structures. In outdoor scenarios, these geometric structures could be road surfaces or buildings, while in indoor scenarios they may be furniture, walls, stairs, etc. We refer to the scale of these structures as a macro scale, meaning that 3D sensor may collect thousands of measurements of those structures in a single scan. A scene representation is defined by the surface primitive that is employed. For example, triangulation approaches make use of triangle primitives, while other approaches such as Poisson surface reconstruction resort to implicit surfaces. Triangulation approaches generate surface primitives that are considered to have a micro scale, since a geometric structure of the scene could contain hundreds or thousands of triangles. Micro scale primitives are inadequate to model large scale environments because they are not compact enough. In this paper, we present a novel methodology to compute a 3D scene representation. The algorithm uses macro scale polygonal primitives to model the scene. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. The proposed representation addresses the problems that were raised in the previous lines: the representation is compact and can be computed much faster than most others, while at the same time providing a sufficiently accurate geometric representation of the scene from the point of view of the tasks required by an autonomous system. The second problem addressed in this paper is the reconstruction of large scale scenarios from a continuous throughput of massive amounts of 3D data. We will use the distinction between the terms scene and scenario. Let scenario refer to a particular location that should be reconstructed, e.g., a city, a road or a building. By scene, we refer to the portion of the scenario that is viewed by the vehicle at a particular time. Thus, the scenario is an integration of scenes over time. In the case of large scale scenarios, the compactness of a given scene representation is even more important. In this paper, we focus also on how the representation may evolve by integrating 3D data from multiple measurements over time. This is an extended version of [4]. The new material covers mostly the incremental part of the geometric reconstruction. There is also the possibility of adding texture to the geometric scene description. For further details on this see [5]. For testing and evaluation purposes, we use a data-set from the Massachusetts Institute of Technology (MIT) Team, taken from their participation in the DARPA Urban Challenge [6]. From this data-set we have extracted a 40 s sequence which will be used to assess the proposed algorithms. For the remainder of the paper, this sequence is referred to as MIT sequence. Using this data-set, we aim at reconstructing large portions of the urban environment in which the competition took place. The remainder of this paper is organized as follows: Section 2 reviews the state of the art, Section 3 presents the proposed approach. Results are given in Section 4 and conclusions in Section 5. 2 Related work At first glance, it would seem plain to translate the improvement on the quality of the 3D data into an enhancement of the 3D representations. However, the fact is that the majority of the robotic systems, namely autonomous vehicles, continue to rely on classic 2D or 2.5D scene representations [7], such as occupancy grids [8] or elevation maps [9], or use discretized grid-like approaches as in octrees [10]. The reason for that is that autonomous vehicles commonly require a large array of sensors installed on-board and, as a consequence, collect large amounts of range measurements every second. Table 1 shows an estimate of the amount of 3D data (measured by LIDAR systems alone) generated by several autonomous vehicles. Simplified 2D or 2.5D representations are used so that they can be computed in real time using large amounts of data. More advanced 3D representations have not been introduced in robotics because they fail to abide to the requirements of real time processing using the 3D data produced by new generation LIDAR sensors. One example of this is the methodologies used in the computer graphics research field: traditional algorithms such as building of triangular meshes are unable to operate in real time with the throughput of data provided by new generation 3D sensors. Some authors have tried to optimize triangulation algorithms (e.g., [2,7]), and they report near real time performances. Note that these results were obtained using point clouds from a Microsoft Kinect 3D camera. 1 1 On the other hand, the results provided towards the end of this paper are obtained using point clouds from a Velodyne HDL-64E Lidar, 2 2 and therefore results are not directly comparable. Scene reconstruction is defined as the computation of a geometric 3D model from multiple measurements. These measurements could be obtained from stereo systems, range sensors, etc. Scene reconstruction may also include the texturing of the generated 3D model. Scene reconstruction methodologies are grouped into two different approaches: surface based representations or volumetric occupancy representations. In the first, the underlying surfaces of the scene that generated the range measurements are estimated, while in the second, the range measurements are grouped into cells of a grid, and are then labeled free or occupied. Traditional surface based representations include several 3D triangulations methodologies, such as 3D Delaunay triangulation [18], or Ball Pivoting Algorithm (BPA) [19]. There are also some alternative higher order surface representation methods such as Poisson surface reconstruction [20], Orientation Inference Framework [21] or learning approaches [22]. However, most of these methods do not tackle well noisy range measurements and, above all, since these methods involve a large number of nearest neighbor queries, they are very slow to compute. One attempt to accelerate the triangulation of point clouds was done in [7], but authors report they have only achieved near real time. Volumetric occupancy representations include occupancy grids [8], elevation maps [9], multi-level surface maps [23] or octrees [10]. While these representations are easier to compute, they do not provide accurate information about the geometry of the scene. The BPA triangulation was proposed in [24]. The BPA computes a triangle mesh interpolating a given point cloud. The principle of the BPA is very simple: Three points form a triangle if a ball of a user-specified radius touches them without containing any other point. Starting with a seed triangle, the ball pivots around an edge (i.e., it revolves around the edge while keeping in contact with the edge’s endpoints) until it touches another point, forming another triangle. The process continues until all reachable edges have been tried, and then starts from another seed triangle, until all points have been considered. Although all range points are considered in the computation of the mesh, which accounts for the accuracy of the methodology, this fact also hampers the computational performance of the algorithm. The Greedy Triangulation (GT) is an approach designed for fast surface reconstruction from large noisy data sets [7]. Given an unorganized 3D point cloud, the algorithm recreates the underlying surface’s geometrical properties using data resampling and a robust triangulation algorithm, the authors claim to achieve near real time. For resulting smooth surfaces, the data is resampled with variable densities according to previously estimated surface curvatures. One of the advantages of this method is that, since a greedy search is executed, it is expected to be faster than other standard triangulation approaches. Poisson surface reconstruction was initially proposed in [25]. In this approach, surface reconstruction of a point cloud with estimated normals is viewed as a spatial Poisson problem. The Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, a Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. The work from [26] proposes an expectation maximization based method for producing a scene representation based on planes, rather than polygons. Also, this method is not intended for real time applications, since creating a scene representation may take up to twenty minutes. Finally, this work does not discuss how new sensor measurements could be integrated in the existing representation, therefore not focusing on the incremental part of scenario reconstruction. 3 Proposed approach In this section we will explain in detail the methodologies of our approach. First, we describe the scene reconstruction algorithm (see Section 3.1) and then, in Section 3.2, we describe how a scenario is created over time from a continuous throughput of 3D data. 3.1 One-shot scene reconstruction This work proposes to explore the usage of Geometric Polygonal Primitives (GPP) to perform scene reconstruction. In other words, the idea is to describe a scene by a list of macro scale polygons. The detection of geometric polygonal primitives is simple when compared to the detection of other more complex primitives. Furthermore, given that road environments are often geometrically structured, it seems feasible to represent the 3D structure with a set of planar polygons. In addition to that, polygons are compact representations: geometric polygonal primitives are described by a support plane and a bounding polygon. Let G i represent the i th polygonal geometric primitive of a given scene, with the support plane Hessian coefficients denoted by G p i = [ a i b i c i d i ] . The search for the support plane is done on a given input point cloud P using a Random Sample Consensus (RANSAC) procedure [27]. Note that there are other algorithms which are more efficient for detecting planes in 3D data [28]. However, an analysis of the impact of these alternative approaches is out of the scope of the current paper. RANSAC is an iterative method to estimate parameters of a mathematical model from a set of observed data points. The assumption is that data consists of inliers, i.e., data whose distribution can be explained by some set of model parameters, and outliers, data that does not fit the model. The input to the RANSAC algorithm is a set of observed data values, a parameterized model which is fitted to the observations, and the output are the model parameters, i.e., in the case of detecting the support plane of polygonal primitives, the Hessian coefficients. Fig. 1 (a) shows in different colors the inlier points of the five best candidates of a RANSAC search. Wall like structures are correctly detected. Fig. 1(c) shows the inliers (signaled in green) of a RANSAC plane detection. In this case, range measurements from two separate walls have been signaled as inliers of a single support plane. To address this issue, the set of inliers of each support plane hypothesis is used as input to a clustering procedure. By using the proposed clustering algorithm, it is possible to separate the two walls into separate polygons, as shown in Fig. 1(d). Polygons are computed using a 2D convex hull operation on the (clustered) RANSAC inliers. In this work, the implementation provided in [29] is used to compute the 2D convex hull, based on a non recursive version of [30,31]. To increase the efficiency of the algorithm, we propose to conduct the detection of polygonal primitives in a cascade like processing configuration. This should be more efficient and fast to process. The input point cloud contains a large amount of 3D points: we assume each 3D point can only belong to a single polygonal primitive. Let S k be the point cloud containing the support points of primitive k , and P k be the input point cloud in which the primitive was searched. The input point cloud for the search of the next primitive, P k + 1 , is obtained by removing the support points of primitive k : (1) P k + 1 = { ∈ P k ∣ ∉ S k } . Since every iteration of primitive detection will conduct a search on a smaller point cloud, it is expected that the cascade configuration is capable of reducing the processing time. Algorithm 1 details the complete procedure for the detection of a set of polygonal primitives given a point cloud. 3.2 Incremental scenario reconstruction We use the term scenario reconstruction to designate an incremental process of reconstruction of scenes from a continuous stream of sensor data. At first sight, there are three alternatives for performing scenario refinement: (1) store raw measurements and, in the end, reconstruct using the entire data; (2) reconstruct periodically and fuse partial scene reconstructions; (3) reconstruct with the first input data then make the representation evolve as new data arrives. Approach (1) is the most immediate, since there are well known surface reconstruction algorithms which reconstruct a surface from a single point cloud. This approach merely merges all input point clouds into a single point cloud, the accumulated point cloud, and then, standard surface reconstruction algorithms may be applied using the accumulated point cloud as input. One downside of this method is that the process of reconstruction can only begin after all point clouds have been collected. This is not suited for usage in real time applications. Furthermore, the amount of data that results from the accumulation of point clouds should be very large, which in turn might cause problems for reconstruction algorithms. Fig. 2 shows an example of a scenario reconstruction (with BPA) using as input the accumulated point clouds of three scenes. This reconstruction took over 2 h to complete. Alternative (2) proposes a fusion of (partially) reconstructed scenes for each of the locations. Taking the example of Fig. 2: if the three scenes are reconstructed independently using BPA, the overall process takes 488.2 + 480.4 + 558.8 = 1487.4 s (this is without considering the overhead attached to the process of fusion). Fig. 3 shows the reconstructed scenarios obtained using this alternative. The downsides of this alternative are the need to define a strategy to fuse the reconstructed scenes in order to obtain a global scenario representation. In addition to this, note that the reconstructed scenes overlap. In overlapping regions, reconstruction is carried out multiple times without originating an improved description, therefore waisting computational resources. Alternative (3) proposes an incremental scenario reconstruction. Unlike in the other two approaches, in this case, the reconstruction of a given scenario receives as input not only the current sensor data (a point cloud), but also a description of the scenario as seen by preceding reconstructions. This method appears to be more interesting than the others, since a scenario representation is updated or refined when new sensor data arrives. An illustrative example: a vehicle is moving on a road and there is a long wall on the side of the road. At the beginning of the road, sensors see only a portion of the wall and the reconstructed surface corresponds only to the visible part of the wall. As the vehicle moves forward, additional areas of the wall become visible to the sensors. These additional range measurements of the wall should be used to update the already existing shape primitive that represents the wall, rather than to create a new shape primitive which represents the new visible portion of the wall, and that overlaps, to some extent, the first wall primitive. In the following lines, we present mechanisms that enable a scene representation based on GPPs to be incrementally refined from novel point cloud data. To update the representation, an operation called expansion is executed for each of the existing GPPs. The expansion receives as input a list of points (the 3D point cloud) as well as the definition of the polygon that is to be expanded. It is composed of two parts, a perpendicular and a longitudinal expansion, and is defined as follows (see Fig. 4 ): Let P represent an input point cloud that is received at a given time and that contains several points (triangles and diamonds in Fig. 4(a)), and the scenario representation that was previously computed, being composed of a single primitive G (black solid line polygon in Fig. 4(a)). The primitive has a support plane, as well as a local coordinate system represented by red–green–blue lines. The first step is to compute a new point cloud P ort that is given by the points of P whose distance to the plane is smaller than the perpendicular expansion threshold T ort : (2) P ort = { ∈ P ∣ d j < T ort } , where d j is the distance of point to the support plane of G . Only points that lie close to the primitives support plane are stored in P ort and used in the next steps. In Fig. 4(a), some points are included in P ort (triangles) and others discarded (diamonds). Then, the points in P ort are projected into the primitives support plane and their coordinates transformed into the primitives local coordinate frame. In this local reference frame, the projected points always have z value equal to zero, which is why only the x and y coordinates are stored, i.e., points are defined in R 2 . Let J be the point cloud that contains the x and y coordinates of the projected points viewed from the primitive local coordinate system. Fig. 4(b) shows the projections of the triangles of Fig. 4(a) to the support plane (circles). This process is called the orthogonal part of the expansion. From here onward, all computations are performed in R 2 , which significantly speeds up the computation. The second part of the expansion is referred to as longitudinal expansion. Fig. 5 (a) shows an example point cloud J that contains several points. Let us consider that some of these points actually belong to the same object that the primitive represents (circles in Fig. 5(a)), and others do not (squares in Fig. 5(a)). Since these points are obtained from new data, not all of them are contained inside the bounding polygon of the corresponding primitives. Therefore, the primitive should expand to accommodate these new points. To do this, an iterative process is proposed. The first step is to offset the existing bounding polygon of the primitive. The algorithm used was introduced in [32] and the implementation is from [33]. The offsetting operation generates a new larger polygon, and in every iteration, the polygon grows as detailed next. The bounding polygon of the primitive is referred to as P , and the grown or extended polygon is referred to P ˆ . Then, all points in J are tested to see if they are inside P ˆ . The final stage is to compute a new convex hull. This new convex hull is computed from the point set that contains both the points of the previous convex hull and the points to which the polygon expanded to. The process is repeated using the newly computed convex hull as the starting hull. The iterative expansion stops when the extended polygon does not contain points inside it. Fig. 5 shows an example of an iterative longitudinal expansion. Fig. 5(a) shows the initial situation; Fig. 5(b) shows the start of the iterative process. The expanded polygon (dashed line), is offset from the initial bounding polygon (solid blue line). Points are tested to see whether they are inside the offset polygon (annotated with crosses). The process repeats until, in the fifth iteration (Fig. 5(f)), no new points are found inside the offset polygon. This causes the iterative search to finish. The expansion operation changes the polygon from its initial state Fig. 5(a) to a new shape, solid magenta line in Fig. 5(f). We propose to conduct the expansion of the current primitives using also a cascade configuration, based on the following reasoning. Each range measurement is obtained from a laser beam reflection of a single physical object. Thus, we can assume that each 3D point is explained by a GPP. Under this assumption, the points that have been assigned to a given primitive by an expansion operation, can only belong to that primitive and no other. Because of this, expanded points are removed from the input point cloud and are not a part of subsequent expansions (of other primitives) nor part of detections of new primitives. Since all the points that are taken by the expansion of a primitive are removed from the input point cloud, the subsequent expansions or searches are accelerated since that less points need to be analyzed. In terms of configuration, a cascade processing recommends that the faster stages are computed first. The expansion of primitives is faster than the detection. Because of this, when a new input point cloud is received, all existing primitives are first expanded and only then the remainder non expanded points are used for searching new primitives. Algorithm 2 describes the architecture of the complete algorithm for the geometric polygonal primitives representation computation, including both the detection and expansion operations. 4 Results In order to evaluate the proposed 3D processing techniques, a complete data-set with 3D laser data, cameras and accurate egomotion is required. The MIT autonomous vehicle Talos competed in the DARPA Urban Challenge and achieved fourth overall place. The data logged by the robot is publicly available [6]. In total, the MIT logs sum up to 315 GB of data. We have cropped a small sequence of 40 s (200 m of vehicle movement) at the start of the race (see Fig. 6 ). The sequence contains a continuous stream of sensor data, but in addition we have marked five locations (A through E) which are used to facilitate the analysis of the results. Additional information on each location is given in Table 2 . Fig. 7 shows images from all cameras, isometric and top views of the 3D data, and a satellite photograph of location C. The proposed approach is evaluated by analyzing how the scenario contained in this MIT sequence is reconstructed. Fig. 8 shows the polygonal primitives detected at locations C and D. It is possible to observe that most of the relevant planes are picked up by the algorithm. 4.1 One-shot reconstruction The detection of polygonal primitives is operated in a cascade-like configuration. In other words, the algorithm will search for polygonal primitives on a given input point cloud. After the first primitive is found, all the range measurements that are explained by that primitive are removed from the input point cloud. The second primitive is then searched in a smaller point cloud and so on. Since the search for a primitive is done over a decreasing size point cloud, it is expected that the search becomes faster as the primitives are extracted. In Fig. 9 an analysis of the computation time of each primitive is displayed. Primitives with higher numbers are detected in posterior phases. Fig. 9(a) shows the number of points remaining in the input point cloud as a function of the polygon number. Results are shown for all locations in the MIT sequence. The number of detected primitives varies from location to location. It is also possible to observe that, as expected, the number of remaining points decreases with the increase in the number of detected primitives. Also, the reduction in the number of points is larger for primitives detected earlier in the process. Hence, since the algorithm tends to remove the largest portion of points at the early stages of the cascade processing, this means that the latter stages will also be more efficient to compute. The reason for this behavior is the RANSAC algorithm. Because RANSAC will search for the larger consensus, it will most likely select planes that are supported by a greater number of points. In this way, RANSAC tends to select first polygons with the largest amount of primitive support points. As a consequence, the largest decreases in the input point cloud occur early in the cascade, which in turn accelerates the subsequent detection stages of the cascade. The detection time per primitive is shown in Fig. 9(b). The detection time tends to decrease with the increase in polygon number, for the reasons that were previously reported. We compare the proposed approach with three surface reconstruction methodologies: Ball Pivoting Algorithm (BPA) [24], Greedy Triangulation (GT) [7] and Poisson Surface Reconstruction (POIS) [25]. Two different parameter configurations for the proposed approach are used. In the first GPP 1, parameters are set so that only very large polygons are detected. Processing time is faster, since a lot of polygons are discarded but, on the other hand, the accuracy or completeness of the scene representation is not very good. The second alternative, GPP 2, is configured so that even small polygons are detected, which should provide a more accurate scene description at the cost of a higher computation time. Fig. 10 (a) shows that the BPA method Fig. 10(d) shows results from the GPP. Since our approach uses primitives to define macro size structures, the number of polygons used to represent the scene is small. Even though, it can be said that the most relevant polygons are part of the representation. Table 3 shows the computation times that each algorithm took to reconstruct each of the locations in the sequence. The GPP methodology is the fastest one. This efficiency is related to the simplicity of the computed representation, and to the fact that RANSAC analyzes only a small sample of points in the input point cloud, which means that not all input points are visited in order to reconstruct the scene, as is the case with the slower triangulation approaches. To measure the accuracy of each reconstruction approach, the results obtained by BPA (the most accurate algorithm) are used as reference. Let X and Y be two meshes. The Hausdorff distance between those meshes d H ( X , Y ) is computed as: (3) d H ( X , Y ) = max ( sup x ∈ X ( inf y ∈ Y d ( x , y ) ) , sup y ∈ Y ( inf x ∈ X d ( x , y ) ) ) , where sup and inf are the supremum and infimum, respectively. In this particular case, a variation of the Hausdorff distance, called the one sided Hausdorff distance is used where only the sup x ∈ X ( inf y ∈ Y d ( x , y ) ) part is computed, because we only wish to measure how distant is each approach to the ground truth and not the other way around. In this case, the X meshes are given by each of the algorithms and the Y mesh is supplied by the ground truth mesh BPA. Table 4 shows the Hausdorff distance values obtained by GT, POIS, GPP 1 and GPP 2 using BPA meshes as ground truth. The algorithm that obtains the best mean results is GT with an average error of 0.14 m. The accuracy exhibited by the GPP 1 and GPP 2 approaches is about 0.99 and 0.83 m, respectively. Fig. 11 shows a graphical representation of the error for all of the approaches. For each approach, the output mesh has been sampled and the points are shown with color associated to the computed one sided Hausdorff distance of each point. A Red–Green–Blue colormap is used to code the distance. Red represents zero distance and blue maximum distance. In Fig. 11(a), corresponding to the GT approach, almost all points have red color, resulting in low mean error. The POIS approach, represented in Fig. 11(b), shows many points in blue and green color, e.g., points whose minimum distance to the ground truth sampled points was very large. This is why POIS shows low accuracy values. In the case of the GPP approaches, 11(c) and (d), some regions of the sampled points are more prone to have large error distances, while those in red seem to perfectly fit the ground truth mesh. The reason for this is that the BPA methodology, that was selected to serve as ground truth, does not perform interpolation over occluded areas, as the GPP approaches do. Most of the errors appear in the polygonal primitive that represents the ground plane; that occurs because this is the one that suffers more from occlusion from other planes. Errors may also result from the usage of convex hulls to compute the boundary polygons. We have investigated this by using alternatives to the proposed approach where the ground plane polygon is suppressed, and where concave hulls are used. Results are shown in Table 5 . We can observe that, with the option of ground plane suppression and concave hull, the mean accuracy of GPP 2 improves to 0.1 m. Fig. 12 shows a visual analysis of the Hausdorff distance errors for these variations of GPP 2. It is possible to observe that regions with error, e.g., in blue and green, decrease considerably when the concave hull is used, but in particular when the ground plane polygon is discarded. 4.2 Incremental reconstruction This section presents several results and analysis of the expansion mechanism of the geometric polygonal primitives. The polygonal primitives algorithm without the expansion mechanism is compared against the same algorithm using the expansion mechanism. These two algorithms will be referred to as “with expansion” and “without expansion”. By using this evaluation, it is possible to assess what are the benefits or disadvantages of the expansion mechanism. All five locations of sequence 1 are used to obtain results. Parameters used in the detection are similar to the GPP 1 set. Fig. 13 shows a reconstruction of the scene using the expansion mechanism. The state of the reconstructed scenario at each location is shown. One clear advantage of this representation is that there are no overlapping primitives. A qualitative analysis of the results present in Fig. 13 shows that the most important features of the scenario are contained in the representation, especially if the task in mind is navigation. To evaluate the computational complexity of the proposed algorithms, we measure the amount of detail of the representation that they produce when given a fixed amount of time to reconstruct a scene. Fig. 14 (a) shows the number of polygons created in both algorithms. Although at the beginning of the sequence both algorithms generate a similar number of primitives, after some locations the algorithm without expansion shows a greater number of primitives. The explanation is that since the algorithm without expansion does not compare the stored primitives with the new data, it ends up duplicating primitives. On the contrary, when using the expansion mechanism, the duplication of primitives is avoided which leads to a smaller number of primitives. Note that just because a representation has more primitives it is not necessarily better. In fact, if there are duplicated primitives, the representation may be worse than another with a smaller number of non duplicated primitives. Fig. 14(b) shows the number of points that are given as input for the detection mechanism. In the case of the algorithm without expansion, all of the input points are passed on to the detection component, i.e., approximately 400000 points per scan. When the expansion is active, a large number of points are explained by the expansion component and are not fed into the detection. In conclusion, the expansion mechanism takes a small amount of time when compared to the detection and other processes, and is able to quickly explain a large portion of the input points, thus filtering out many points which are not sent to other slower components. Finally, in Fig. 14(c), the total accumulated area of the primitives is shown. There is a clear difference between the with and without expansion approaches. This difference is due to the duplication of primitives in the case of the without expansion approach. 4.3 Comparison of approaches with and without expansion Next, we focus on characterizing how the polygonal primitives evolve. Only the algorithm with expansion is portrayed, since in the without expansion approach primitives are static. Fig. 15 (a) shows the number of support points assigned to each primitive. Only primitives of even index are shown. We can see that primitives are initialized in different locations in the sequence: at location A primitive 0 is created, while primitives 2, 4 and 6 are created at location B. These results show that most primitives significantly increase their number of support points throughout the sequence: Primitive 0 was detected at location A with 0.4 × 50 × 10 4 = 200 kpoints, and at location E it already supported 1.4 × 50 × 10 4 = 700 kpoints. In other words, it increased the number of support points by 350%. Another example, primitive 10, detected at location D with 3 kpoints, has at location E around 7 kpoints. A 230% increase between consecutive locations. The same analysis holds when considering the area of the primitives (see Fig. 15(b)): significant increases in the area of the primitives bounding polygons are also observed. All these observations, both in number of support points as well as in terms of area, show that polygons grow considerably after being detected. If these primitives were not expanded, the additional support points and area would have to be handled by a detection mechanism. Fig. 16 shows how primitive 0, i.e., the ground plane primitive, evolves over sequence 1. The primitive expands at every iteration to accommodate newly observed data points that belong to the ground plane. Table 6 provides the links for some videos that show how the system processes the data stream from the MIT sequence. It is possible to see the difference between an incremental versus a non incremental (without expansion) approach. 5 Conclusions This paper proposes a novel approach to produce scene representations using the array of sensors on-board autonomous vehicles. Since roads are semi structured environments with a great deal of macro size geometric structures, we argue that the use of polygonal primitives is well suited to describe these scenes. Furthermore, we propose mechanisms designed to update the polygonal primitives as new sensor data is collected. Results have shown that the proposed approach is capable of producing accurate descriptions of the scene, and that it is considerably faster than all the approaches used in this evaluation. The proposed expansion mechanism updates previous descriptions of the scene, therefore not creating duplicate representations of the same objects. In addition to this, the expansion mechanism is capable of efficiently filtering out data points that otherwise would be handled by (slower) detection mechanisms. Future work will include the addition of texture on the polygons generated by the proposed algorithm. In this way, we expect to have the means to produce scene representations that can be used not only for standard task such as obstacle detection and motion planning, but also for more complex endeavors such as recognizing patterns in the scene. Acknowledgments This work has been supported by the Portuguese Foundation for Science and Technology “Fundação para a Ciência e Tecnologia” (FTC), under grant agreements SFRH/BD/43203/2008 and SFRH/BPD/109651/2015 and projects POCI-01-0145-FEDER-006961 and UID/CEC/00127/2013. This work was also financed by the ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020. A. Sappa has been partially supported by the Spanish Government under Project TIN2014-56919-C3-2-R and the PROMETEO Project of the “Secretaría Nacional de Educación Superior, Ciencia, Tecnología e Innovación de la República del Ecuador”, reference CEB-02502014. References [1] A. Birk N. Vaskevicius K. Pathak S. Schwertfeger J. Poppinga H. Buelow 3-d perception and modeling IEEE Robot. Autom. Mag. 16 4 2009 53 60 [2] R.B. Rusu, S. Cousins, 3D is here: Point Cloud Library (PCL), in: IEEE International Conference on Robotics and Automation, ICRA, Shanghai, China, 2011. [3] W. Burgard P. Pfaff Editorial: Three-dimensional mapping, part 1 J. Field Robot. 26 10 2009 757 758 [4] M. Oliveira, V. Santos, A.D. Sappa, P. Dias, Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Volume 1, Springer International Publishing, Cham, 2016, Ch. Scene Representations for Autonomous Driving: An Approach Based on Polygonal Primitives, pp. 503–515. [5] M. Oliveira V. Santos A. Sappa P. Dias A.P. Moreira Incremental texture mapping for autonomous driving Robot. Auton. Syst. 2016 (submitted January 2016) [6] A.S. Huang M. Antone E. Olson L. Fletcher D. Moore S. Teller J. Leonard A High-rate, Heterogeneous Data Set from the DARPA Urban Challenge Int. J. Robot. Res. 29 13 2011 1595 1601 [7] Z.C. Marton, R.B. Rusu, M. Beetz, On fast surface reconstruction methods for large and noisy datasets, in: Proceedings of the IEEE International Conference on Robotics and Automation, ICRA, Kobe, Japan, 2009. [8] T. Weiss, B. Schiele, K. Dietmayer, Robust driving path detection in urban and highway scenarios using a laser scanner and online occupancy grids, in: Intelligent Vehicles Symposium, 2007 IEEE, 2007, pp. 184–189. [9] F. Oniga S. Nedevschi Processing dense stereo data using elevation maps: Road surface, traffic isle, and obstacle detection IEEE Trans. Veh. Technol. 59 3 2010 1172 1182 [10] K. Zhou M. Gong X. Huang B. Guo Data-parallel octrees for surface reconstruction IEEE Trans. Vis. Comput. Graphics 17 5 2011 669 681 [11] S. Thrun M. Montemerlo H. Dahlkamp D. Stavens A. Aron J. Diebel P. Fong J. Gale M. Halpenny G. Hoffmann K. Lau C.M. Oakley M. Palatucci V. Pratt P. Stang S. Strohband C. Dupont L.-E. Jendrossek C. Koelen C. Markey C. Rummel J. van Niekerk E. Jensen P. Alessandrini G.R. Bradski B. Davies S. Ettinger A. Kaehler A.V. Nefian P. Mahoney Stanley: The robot that won the darpa grand challenge J. Field Robot. 23 9 2006 661 692 [12] C. Urmson J. Anhalt D. Bartz M. Clark T. Galatali A. Gutierrez S. Harbaugh J. Johnston H. Kato P.L. Koon W. Messner N. Miller A. Mosher K. Peterson C. Ragusa D. Ray B.K. Smith J.M. Snider S. Spiker J.C. Struble J. Ziglar W.R.L. Whittaker A robust approach to high-speed navigation for unrehearsed desert terrain J. Field Robot. 23 8 2006 467 508 [13] C. Urmson J. Anhalt H. Bae J.A.D. Bagnell C.R. Baker R.E. Bittner T. Brown M.N. Clark M. Darms D. Demitrish J.M. Dolan D. Duggins D. Ferguson T. Galatali C.M. Geyer M. Gittleman S. Harbaugh M. Hebert T. Howard S. Kolski M. Likhachev B. Litkouhi A. Kelly M. McNaughton N. Miller J. Nickolaou K. Peterson B. Pilnick R. Rajkumar P. Rybski V. Sadekar B. Salesky Y.-W. Seo S. Singh J.M. Snider J.C. Struble A.T. Stentz M. Taylor W.R.L. Whittaker Z. Wolkowicki W. Zhang J. Ziglar Autonomous driving in urban environments: Boss and the urban challenge J. Field Robot. 25 8 2008 425 466 Special Issue on the 2007 DARPA Urban Challenge, Part I [14] M. Montemerlo J. Becker S. Bhat H. Dahlkamp D. Dolgov S. Ettinger D. Haehnel T. Hilden G. Hoffmann B. Huhnke D. Johnston S. Klumpp D. Langer A. Levandowski J. Levinson J. Marcil D. Orenstein J. Paefgen I. Penny A. Petrovskaya M. Pflueger G. Stanek D. Stavens A. Vogt S. Thrun Junior: The stanford entry in the urban challenge J. Field Robotics 2008 [15] A. Bacha C. Bauman R. Faruque M. Fleming C. Terwelp C. Reinholtz D. Hong A. Wicks T. Alberi D. Anderson S. Cacciola P. Currier A. Dalton J. Farmer J. Hurdus S. Kimmel P. King A. Taylor D.V. Covern M. Webster Odin: Team victortango’s entry in the darpa urban challenge J. Field Robot. 25 8 2008 467 492 [16] T. Luettel, M. Himmelsbach, F. von Hundelshausen, M. Manz, A. Mueller, H.-J. Wuensche, Autonomous Offroad Navigation Under Poor GPS Conditions, in: Proceedings of 3rd Workshop On Planning, Perception and Navigation for Intelligent Vehicles, PPNIV, IEEE/RSJ International Conference on Intelligent Robots and Systems, St. Louis, MO, USA, 2009. [17] Wikipedia, Google driverless car—Wikipedia, the free encyclopedia, [Online; accessed November 2015], 2015. [18] R. Jovanovic, R. Lorentz, Compression of volumetric data using 3d delaunay triangulation, in: 2011 4th International Conference on Modeling, Simulation and Applied Optimization, ICMSAO, 2011, pp. 1–5. [19] A. Specht, M. Devy, Surface segmentation using a modified ball-pivoting algorithm, in: 2004 International Conference on Image Processing, 2004. ICIP’04. Vol. 3, 2004, pp. 1931–1934. [20] C. Yin, D. Gang, C. Zhi-quan, L. Hong-hua, L. Jun, J. Shi-yao, An algorithm of cuda-based poisson surface reconstruction, in: 2010 International Conference on Audio Language and Image Processing, ICALIP, 2010, pp. 203–207. [21] Y.-L. Chen S.-H. Lai An orientation inference framework for surface reconstruction from unorganized point clouds IEEE Trans. Image Process. 20 3 2011 762 775 [22] A. de Medeiros Brito A. Doria Neto J. Dantas de Melo L. Garcia Goncalves An adaptive learning approach for 3-d surface reconstruction from point clouds IEEE Trans. Neural Netw. 19 6 2008 1130 1140 [23] C. Rivadeneyra, I. Miller, J. Schoenberg, M. Campbell, Probabilistic estimation of multi-level terrain maps, in: IEEE International Conference on Robotics and Automation, 2009. ICRA’09. 2009, pp. 1643–1648. [24] F. Bernardini J. Mittleman H. Rushmeier C. Silva G. Taubin The ball-pivoting algorithm for surface reconstruction IEEE Trans. Vis. Comput. Graphics 5 4 1999 349 359 [25] M. Kazhdan, M. Bolitho, H. Hoppe, Poisson surface reconstruction, in: SGP’06: Proceedings of the Fourth Eurographics Symposium on Geometry Processing, Eurographics Association, 2006, pp. 61–70. [26] R. Triebel, W. Burgard, F. Dellaert, Using hierarchical em to extract planes from 3d range scans, in: Proceedings of the 2005 IEEE International Conference on Robotics and Automation, 2005, pp. 4437–4442. [27] M.A. Fischler, R.C. Bolles, Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography, in: ACM, Los Angeles, California, 1981. [28] A. Nurunnabi, D. Belton, G. West, Robust segmentation for multiple planar surface extraction in laser scanning 3d point cloud data, in: 2012 21st International Conference on Pattern Recognition, ICPR, 2012, pp. 1367–1370. [29] S. Hert, S. Schirra, 2D convex hulls and extreme points, in: CGAL User and Reference Manual, 4.0 Edition, CGAL Editorial Board, 2012. [30] A. Bykat Convex hull of a finite set of points in two dimensions Inform. Process. Lett. 7 1978 296 298 [31] C.B. Barber D.P. Dobkin H. Huhdanpaa The quickhull algorithm for convex hulls ACM Trans. Math. Software 22 4 1996 469 483 [32] O. Aichholzer F. Aurenhammer D. Alberts B. Gartner A novel type of skeleton for polygons J.UCS 1 12 1995 752 761 [33] F. Cacciola, 2D straight skeleton and polygon offsetting, in: CGAL User and Reference Manual, 4.0 Edition, CGAL Editorial Board, 2012. Miguel Oliveira received the Mechanical Engineering and M.Sc. in Mechanical Engineering degrees from the University of Aveiro, Portugal, in 2004 and 2007, where later in 2013 he obtained the Ph.D. in Mechanical Engineering specialization in Robotics, on the topic of autonomous driving systems. Currently he is a researcher at the Institute of Electronics and Telematics Engineering of Aveiro, Portugal, where he works on visual object recognition in open-ended domains. His research interests include multimodal sensor fusion, computer vision and robotics. Vítor Santos obtained a 5 year degree in Electronics Engineering and Telecommunications in 1989, at the University of Aveiro, Portugal, where he later obtained a Ph.D. in Electrical Engineering in 1995. He was awarded fellowships to pursue research in mobile robotics during 1990–1994 at the Joint Research Center, Italy. He his currently Associate Professor at the University of Aveiro and lectures courses related to advanced perception and robotics, and has managed research activity on mobile robotics, advanced perception and humanoid robotics, with the supervision or cosupervision of more than 100 graduate and undergraduate students, and more that 120 publications in conferences, books and journals. At the University of Aveiro he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and has coordinated the development of ATLASCAR, the first real car with autonomous navigation capabilities in Portugal. He is one of the founders of Portuguese Robotics Open in 2001 where he has kept active participation ever since. He his also cofounder of the Portuguese Society of Robotics, and participated several times in its management since its foundation in 2006. His current interests extend to humanoid robotics and the application of techniques from perception and mobile robotics to autonomy and safety in ADAS contexts. Angel Domingo Sappa (S’94-M’00-SM’12) received the Electromechanical Engineering degree from National University of La Pampa, General Pico, Argentina, in 1995, and the Ph.D. degree in Industrial Engineering from the Polytechnic University of Catalonia, Barcelona, Spain, in 1999. In 2003, after holding research positions in France, the UK, and Greece, he joined the Computer Vision Center, Barcelona, where he is currently a Senior Researcher. He is a member of the Advanced Driver Assistance Systems Group. His research interests span a broad spectrum within the 2D and 3D image processing. His current research focuses on stereoimage processing and analysis, 3D modeling, and dense optical flow estimation. Paulo Dias graduated from the University of Aveiro Portugal in 1998 and started working in 3D reconstruction at the European Joint research Centre in Italy. In September 2003, he concluded his Ph.D. with the thesis “3D Reconstruction of real World Scenes Using Laser and Intensity Data”. He is currently an assistant professor within the Department of Electronics Telecommunications and Informatics (DETI) and is involved in several works and projects within the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) related to 3D Reconstruction, Virtual Reality, Computer Vision, Computer Graphics, Visualization and Combination and Fusion of data from multiple sensors. António Paulo Moreira graduated with a degree in electrical engineering at the University of Oporto, in 1986. He then pursued graduate studies at University of Porto, obtaining an M.Sc. degree in electrical engineering-systems in 1991 and a Ph.D. degree in electrical engineering in 1998. Presently, he is an Associate Professor at the Faculty of Engineering of the University of Porto and researcher and manager of the Robotics and Intelligent Systems Centre at INESC TEC. His main research interests are process control and robotics. "
    },
    {
        "doc_title": "Self calibration of multiple LIDARs and cameras on autonomous vehicles",
        "doc_scopus_id": "84977663236",
        "doc_doi": "10.1016/j.robot.2016.05.010",
        "doc_eid": "2-s2.0-84977663236",
        "doc_date": "2016-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "3D data",
            "Automatic calibration",
            "Autonomous navigation",
            "Autonomous Vehicles",
            "Extrinsic calibration",
            "Point cloud",
            "Real world environments",
            "Rigid body transformation"
        ],
        "doc_abstract": "© 2016 Elsevier B.V.Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2016-05-31 2016-05-31 2016-08-04 2016-08-04 2016-08-04T18:32:25 S0921-8890(16)30017-3 S0921889016300173 10.1016/j.robot.2016.05.010 S300 S300.1 FULL-TEXT 2016-08-04T16:53:09.271769-04:00 0 0 20160901 20160930 2016 2016-05-31T15:48:20.983401Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 83 83 C Volume 83 28 326 337 326 337 201609 September 2016 2016-09-01 2016-09-30 2016 article fla © 2016 Elsevier B.V. All rights reserved. SELFCALIBRATIONMULTIPLELIDARSCAMERASAUTONOMOUSVEHICLES PEREIRA M 1 Introduction 1.1 Context of the work 1.2 Related work 2 Proposed solution 3 Ball detection algorithms 3.1 Sphere center detection in 3D data: SwissRanger 3.2 Sphere center detection on LIDAR lasers 3.2.1 Segmentation 3.2.2 Circle detection 3.2.3 Calculation of the circle properties 3.2.4 Calculation of the center of the ball 3.3 Ball detection on cameras 4 Calibration 4.1 3D rigid transformation 4.2 External camera calibration 5 Results 5.1 Consistency of ball detection 5.2 Consistency of the geometric transformation 5.3 Validation of the method 5.3.1 Grid pattern calibration 5.3.2 Calibration with random acquisition spots 6 Conclusions and future work References DIAS 2009 J DISTANCIOMETRO3DBASEADONUMAUNIDADELASER2DEMMOVIMENTOCONTINUO LEVINSON 2013 J ROBOTICSSCIENCESYSTEMSRSS AUTOMATICONLINECALIBRATIONCAMERASLASERS ALMEIDA 2012 312 319 M IMAGEANALYSISRECOGNITION 3D2DLASERRANGEFINDERCALIBRATIONUSINGACONICBASEDGEOMETRYSHAPE FERNANDEZMORAL 2015 E BRADSKI 2000 120 125 G FISCHLER 1981 381 395 M COIMBRA 2013 D LIDARTARGETDETECTIONSEGMENTATIONINROADENVIRONMENT RAMER 1972 244 256 U DOUGLAS 1973 112 122 D ARUN 1987 698 700 K HORN 1987 629 642 B PEREIRAX2016X326 PEREIRAX2016X326X337 PEREIRAX2016X326XM PEREIRAX2016X326X337XM 2018-08-04T00:00:00Z UnderEmbargo © 2016 Elsevier B.V. All rights reserved. item S0921-8890(16)30017-3 S0921889016300173 10.1016/j.robot.2016.05.010 271599 2016-08-04T15:40:51.784284-04:00 2016-09-01 2016-09-30 true 3811601 MAIN 12 55474 849 656 IMAGE-WEB-PDF 1 gr4 16067 110 219 gr2 18808 143 219 pic2 29748 163 140 gr18a 15625 164 215 gr20c 20835 164 108 gr17b 20252 119 219 gr14a 26766 78 219 gr20a 24661 132 219 gr20b 23129 114 219 gr18b 36240 164 218 gr12 21003 69 219 gr1 27257 87 219 gr5 13945 164 171 gr13 39139 134 219 gr14c 19998 164 192 gr17a 21932 129 219 pic3 32172 163 140 gr7 14362 80 219 gr15a 15531 156 219 gr14b 14733 148 219 gr8b 35489 164 218 gr17c 17546 163 119 gr6 24107 163 164 gr9 16887 90 219 gr19 38732 164 218 pic4 34270 163 140 gr11 17847 69 219 pic1 34474 164 140 gr15b 36304 164 218 gr16 36306 164 218 gr8a 37620 164 218 gr3 14494 163 174 gr10 16804 87 219 gr4 39043 107 213 gr2 50725 205 313 pic2 40944 132 113 gr18a 46094 258 339 gr20c 64979 460 304 gr17b 47136 180 333 gr14a 71656 181 507 gr20a 50139 181 299 gr20b 48819 154 296 gr18b 53590 255 339 gr12 70597 195 621 gr1 75208 193 489 gr5 39533 272 284 gr13 70288 208 339 gr14c 78393 372 435 gr17a 49542 194 329 pic3 40110 132 113 gr7 39013 110 303 gr15a 44676 242 339 gr14b 40662 228 338 gr8b 51430 255 339 gr17c 58851 448 327 gr6 38078 174 175 gr9 51276 154 376 gr19 52883 229 305 pic4 42468 132 113 gr11 64615 150 480 pic1 40842 132 113 gr15b 53183 255 339 gr16 53502 255 339 gr8a 55724 255 339 gr3 39806 265 282 gr10 51316 149 376 si25 222 11 30 si56 835 35 212 si21 213 11 28 si96 159 11 17 si111 224 11 30 si95 126 11 11 si121 152 14 15 si31 124 8 10 si81 655 41 125 si53 245 14 48 si17 514 14 127 si75 398 19 70 si86 131 8 11 si46 248 15 37 si37 595 15 128 si7 580 15 124 si39 397 21 76 si4 231 11 31 si88 389 16 78 si14 162 14 16 si78 253 16 36 si115 167 12 34 si114 112 11 6 si10 197 14 22 si26 226 11 30 si15 162 14 16 si66 412 17 100 si8 401 15 72 si110 252 12 33 si99 830 45 164 si9 159 11 19 si87 141 11 12 si73 1203 21 303 si51 479 19 112 si54 390 15 76 si69 671 15 170 si83 321 15 60 si2 200 11 28 si61 665 15 174 si80 699 41 127 si48 151 11 15 si16 159 14 16 si108 957 17 195 si116 139 11 13 si89 154 14 16 si34 666 22 148 si18 303 14 48 si1 139 12 10 si30 126 11 10 si94 1153 63 150 si74 426 17 87 si62 586 21 147 si43 335 17 70 si72 715 17 204 si100 303 14 61 si120 162 14 17 si60 155 11 16 si90 458 39 80 si59 146 11 15 si47 258 15 36 si41 284 14 74 si70 175 11 22 si6 204 11 30 si77 254 12 39 si27 184 11 22 si40 271 14 74 si20 130 9 12 si13 155 14 15 si117 131 11 12 si36 170 14 19 si42 313 13 69 si5 653 15 135 si105 173 16 15 si38 350 21 74 si112 147 11 15 si49 143 11 14 si3 201 11 23 si44 388 19 78 si50 141 11 11 si24 152 11 15 si35 961 31 216 si104 166 14 15 si64 154 13 13 si52 788 17 245 si22 205 11 23 si55 821 35 206 si85 164 13 16 si98 146 11 13 si45 421 17 88 si33 480 15 134 si76 243 15 40 si32 135 11 11 si82 210 16 24 si106 164 14 15 si97 160 12 16 si109 224 12 26 si19 153 11 14 si84 159 11 16 si79 517 23 119 si23 219 11 30 si93 372 15 71 ROBOT 2640 S0921-8890(16)30017-3 10.1016/j.robot.2016.05.010 Elsevier B.V. Fig. 1 ATLASCAR, a Ford Escort SW 98 adapted for autonomous driving capabilities on the left. Sensors used in this work—Sick LMS151 and LD-MRS40001, Point Grey camera and SwissRanger—on the right. Fig. 2 Detection in a SwissRanger cloud of points. Fig. 3 Result of the segmentation in a scan of the sensor Sick LMS151. Fig. 4 Congruent angles of points on an arc in respect to the extremes. Fig. 5 Result of the circle detection in real data from the sensor Sick LMS151. Fig. 6 Example of the cross-section of the sphere at a distance d from the sphere’s center. Fig. 7 Detection of the ball in a 2D scan from the Sick LMS151. Fig. 8 Outdoors ball detection with a Point Grey camera. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 8(a) (a) Initial camera image. Fig. 8(b) (b) Circle is the ball contour; plus sign is the ball center. Fig. 9 Standard deviation of ball center detection for several distances. Fig. 10 Standard deviation of the translation between the three sensor pairs. Fig. 11 Standard deviation of the Euler angles between the three sensor pairs. Fig. 12 Real layout of the sensors on positions detected by the calibration process. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 Setup used on the validation test and the sensor positions. Fig. 14 Real world and Sick LMS151(A) grid pattern. Fig. 14(a) (a) Real world grid points used for calibration. Fig. 14(b) (b) Sick LMS151(A) ball centers point cloud. Fig. 14(c) (c) Vertical and horizontal distance between grid points. Fig. 15 Uncalibrated point clouds in their respective coordinate systems. Fig. 15(a) (a) Point clouds to be used with the 3D rigid transform method. Fig. 15(b) (b) Ball centers to be used with the external camera calibration method. Fig. 16 Ball centers in the camera coordinate system (green plus sign) and projected ball centers from Sick LMS151(A) (red points). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 17 Sensor placement in a Ford Escort SW 98 3D model after calibration. Fig. 17(a) (a) 3D view. Fig. 17(b) (b) Front view. Fig. 17(c) (c) Top view with calibrated point clouds. Fig. 18 Uncalibrated point clouds in their respective coordinate systems. Fig. 18(a) (a) Point clouds to be used with the 3D rigid transform method. Fig. 18(b) (b) Ball centers to be used with the external camera calibration method. Fig. 19 Ball centers in the camera coordinate system (green plus sign) and projected ball centers from Sick LMS151(A) (red points). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 20 Sensor placement in a Ford Escort SW 98 3D model after calibration. Fig. 20(a) (a) 3D view. Fig. 20(b) (b) Front view. Fig. 20(c) (c) Top view with calibrated point clouds. Table 1 Properties of the main 3D range sensors onboard the ATLASCAR. Sick LD-MRS400001 Sick LMS151 SwissRanger sr4000 Scan planes 4, with full vertical aperture of 3.2 ° 1 Field of view 2 scan planes: 85 ° 270 ° 43.6 ° ( h ) × 34.6 ° ( v ) 2 scan planes: 110 ° Scanning frequency 12.5 Hz/25 Hz/50 Hz 25 Hz/50 Hz Angular resolution 0.125 ° / 0.25 ° / 0.5 ° 0.25 ° / 0.5 ° Operating range 0.5–250 m 0.5–50 m 0.1–5.0 m Statistical error ( 1 σ ) ±100 mm ±12 mm ±10 mm Table 2 Standard deviation and absolute mean error for each sensor calibration, using the 3D rigid transform method, against Sick LMS151(A). Absolute mean error (cm) Standard deviation (cm) Sick LMS151 5.9 3.0 Sick LD-MRS 13.4 10.1 Point Grey camera 18.4 8.3 Table 3 Standard deviation and absolute mean error of each sensor calibration, using the 3D rigid transform method, against Sick LMS151(A). Absolute mean error (cm) Standard deviation (cm) Sick LMS151 6.8 3.4 Sick LD-MRS 14.8 8.2 Point Grey camera 16.8 9.8 Self calibration of multiple LIDARs and cameras on autonomous vehicles Marcelo Pereira a David Silva a Vitor Santos a c ⁎ Paulo Dias b c a Department of Mechanical Engineering, University of Aveiro, Portugal Department of Mechanical Engineering, University of Aveiro Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro Portugal c IEETA-Institute for Electronics Engineering and Informatics of Aveiro, University of Aveiro, Portugal IEETA-Institute for Electronics Engineering and Informatics of Aveiro, University of Aveiro Portugal ⁎ Corresponding author. Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity. Keywords Extrinsic calibration Point cloud 3D data fitting 1 Introduction Many vehicles with autonomous navigation capabilities, and also many advanced drivers assistance systems, rely on LIDAR (Light Detection And Ranging) and vision based sensors. Moreover, most of the developed systems use multiple sensors simultaneously. Thus, when there is more than one sensor in the same setup, a calibration procedure must take place to combine data from different sensors in a common reference frame. In order to solve that necessity, this work presents a new extrinsic calibration method. Differently from the majority of existing methods, which are manual or semi-automatic, the proposed method is automatic, with no requirement of manual measurements or manual correspondences between sensors data. Instead of using those approaches, a ball is used as a calibration target allowing the detection of its center by all sensors and then perform the several registration steps. To estimate the full transformation between the sensors, at least three 3D point correspondences between the sensors are needed, but more may be used for precision purposes. These points can be estimated during the ball movement, which furthermore increases the accuracy of the method as a significant number of points can be obtained along that path. 1.1 Context of the work This work is part of the ATLASCAR project [1], carried out at the University of Aveiro, whose main purpose is the research and development of solutions on autonomous driving and Advanced Driver Assistance System (ADAS). For that goal, a common Ford Escort car is equipped with a rich set of sensors dedicated mainly to the perception of the surrounding environment (Fig. 1 ). The car is equipped with several exteroceptive sensors, namely a stereo camera, a 3D LIDAR, a foveated vision unit and two planar laser range finders. The planar lasers already installed on the car are two Sick LMS151, and a custom made 3D LIDAR using a Sick LMS200 in a rotating configuration adapted from [2]. Additionally, a new multi-layer LIDAR (sick LD-MRS 400001) and two Point Grey cameras are available as well as a SwissRanger 3D TOF used occasionally in some experiments and contexts. This paper will focus on the sensors illustrated on the right side of Fig. 1, and Table 1 presents some of the 3D range sensors main properties. 1.2 Related work Over the past several years, a number of proposed solutions for the calibration between a camera and a laser were introduced, including some automatic on-line calibration solutions as presented in [3]. In [4] a method to estimate the motion of a camera–laser fusion system was developed that projects the laser points onto the images. Features are then selected using the Kanade–Lucas–Tomasi feature tracker [5] and tracked between frames to be used as 3D–2D correspondences for the motion estimation using a three-point method based in the algorithm developed by Bock et al. [6]. In [7] a plane with a printed black ring and a circular perforation is used to solve the extrinsic calibration between a camera and a multi-layer LIDAR; the method consists of estimating different poses of the calibration target detected simultaneously by the camera and the multi-layer LIDAR, resulting in a set of point correspondences between frames (circle centers of each pose), that are used to compute the extrinsic calibration by using the singular value decomposition (SVD) along with the Iterative Closest Point (ICP) algorithm to refine the resulting transformation. A similar approach is used in [8] to calibrate the same set of sensors; a planar triangle plane is used as target to extract correspondences between sensors, and the extrinsic calibration between sensors is solved using the Levenberg–Marquardt algorithm that projects the laser points into the image. There are also some other studies for the specific problem of calibration between LIDAR sensors, however, there is still room for improvement since no fully automatic method using perception sensor information, and adapted to any configuration, is available. Previous works on the ATLASCAR project used a technique that also uses a calibration target, but it requires manual input from the user [9]. Other authors developed algorithms to calibrate one [10] or two [11] single-beam LIDARs within the body frame; both methods use approaches that rely on establishing feature correspondences between the individual observations by preparing the environment with laser-reflective tape, which additionally requires an intensity threshold for correspondences and some initial parameters. A more recent method [12] is based on the observation of perpendicular planes; this calibration process is constrained by imposing co-planarity and perpendicularity constrains on the line segments extracted by the different laser scanners; despite being an automatic calibration method, it does not provide the versatility of the method presented in this paper. Finally an approach using a sphere target to perform extrinsic calibration of multiple 3D cameras was presented in [13], which presents some similarities with the current proposal since the user also moves the target to different positions and heights within a shared viewing area. In that process, the algorithm automatically detects the center of the ball in the data from each camera, and then uses those centers as corresponding points to estimate the relative positions of the 3D sensors. However, this approach [13] is only used with Kinect sensors with a smaller ball and a reduced working range. 2 Proposed solution The proposed solution in this paper is to estimate the rigid body transform between different sensors (SICK LMS151 and LD-MRS400001, Point Grey camera and SwissRanger) using a ball as calibration target. The only restriction of the calibration target is about its size (diameter). The size of the calibration target is related to the angular resolution of the sensors used; after some empirical experiments, it was concluded that the ball must have a diameter large enough for the sensors to have at least 8 measurements of the target at 5 m. The approach used to obtain the calibration among all the devices is achieved in three stages. First, each sensor must detect the ball; then, the ball is placed in motion in front of the sensors allowing them to detect its center along successive positions, creating a synchronized point cloud of detected centers for each of the sensors. The condition to consider a new reference point (ball center) for each point cloud is that each new point is separated from the previous one by a minimum distance pre-defined to 15 cm, but can be defined by the user. Finally, a sensor is chosen as reference and the remainder are calibrated relatively to it, one at a time, by using our own developed algorithms or ones available on the Point Cloud Library (PCL) [14] or on OpenCV [15]. 3 Ball detection algorithms The method to find the center of the ball depends on the type of data. In the following sections the methods used for the ball detection in the different sensors are described. 3.1 Sphere center detection in 3D data: SwissRanger The ball recognition using the SwissRanger is achieved using the PCL segmentation capabilities, namely the sample consensus module. The Random Sample Consensus (RANSAC) [16], which is an iterative method used to estimate parameters of a mathematical model from a set of data containing outliers is the algorithm applied to the point cloud measured with the SwissRanger. In this case, the model is a sphere, and so the resulting parameters are the coordinates of the center of the ball and its radius. Fig. 2 shows the detected ball computed after applying the RANSAC algorithm to the point cloud. 3.2 Sphere center detection on LIDAR lasers The method for 2D data is based on finding circular arcs, taking advantage of the particularity that any planar section of the ball is a circle. Thus, the process to detect the center of the ball is divided in the following sequence: segmentation of the laser scan, circle detection and calculation of its properties (center coordinates and radius), and calculation of the center of the ball given its diameter. It is important to mention that in 2D scans, due to the symmetry of the ball, there is an ambiguity relatively to which of the hemispheres belongs the detected circle, since every section of the ball has a symmetric section relatively to the hemisphere. This generates two solutions for the center of the ball (one above and another below the detected section) and, consequently, a priori information about the position of the sensor relatively to the ball is required. For the Sick LD-MRS, the idea was to solve this ambiguity problem taking advantage of its multi-layer technology, however, it was verified that when all the laser scans were on the lower ball hemisphere, the lowest scan did not measure the smallest circle diameter as expected, due to its own intrinsic error. Thus, it is required to give a priori information about is the sensor position relatively to the ball—below or above the ball’s equator. 3.2.1 Segmentation Segmentation is a very important part before the calibration process. The main goal is to cluster the point cloud in subsets of points which have high probability to belong to the same object through detected discontinuities in the laser data sequence, which are called break-points. Several methods are available to perform 2D point cloud segmentation. Based on the work of Coimbra [17], the Spatial Nearest Neighbor (SNN) is used since it has the most consistent results over different tested scenes. The SNN is a recursive algorithm where the distance between a point and all the other points that are not yet assigned to a cluster is computed. If that distance is smaller than a certain threshold the points are assigned to that cluster. Fig. 3 shows the result of the segmentation in a scan and the cluster related to the ball. The only variable in this algorithm is the threshold value D th , so it is expected that for a higher D th the result will be larger clusters, and for a smaller D th the result will be smaller clusters. 3.2.2 Circle detection The method used for circle detection is inspired on a work developed for line, arc/circle and leg detection from laser scan data [18]. The circle detection makes use of a technique named Internal Angle Variance (IAV), which is based on the trigonometric property that every point in an arc has congruent angles (angles that have the same amplitude) in respect to the extremes. This property can be verified in Fig. 4 . Let P 1 and P 4 be the extremes of the arc, and P 2 and P 3 random points belonging to the same arc. Then ∠ P 1 P 2 P 4 = ∠ P 1 P 3 P 4 because both angles measure one-half of ∠ P 1 O P 4 . The detection of circles involves calculating the mean of the aperture angle ( m ̄ ) between the extreme points and the remainder points of a cluster, as well as its standard deviation ( σ ). In an earlier approach, and considering that the scan covers approximately half a circle, values of standard deviation smaller than 8.6 ° and values of mean aperture between 90 ° and 135 ° are used to define a positive detection. However, those values depend on two factors: the error associated to the sensors, and how much of the circle is covered by the scan. Thus, after analyzing the results empirically, the values were adjusted to standard deviations under 5 ° , and values of mean aperture between 105 ° and 138 ° for the sick LMS151, and 10 ° and between 110 ° and 135 ° , respectively for the sick LD-MRS400001. These adjustments allowed to obtain the best results avoiding false circle detections. Considering that a segment S has n points ( P ), say S = { P 1 , P 2 , … , P n } , the mean and standard deviation of the angle are calculated as follows: m ̄ = 1 n − 2 ∑ i = 2 n − 1 ∠ P 1 P i P n and σ = 1 n − 2 ∑ i = 2 n − 1 ( ∠ P 1 P i P n − m ̄ ) 2 . 3.2.3 Calculation of the circle properties The calculation of the center and radius of the circle uses the method of least squares to find the circle that best fits the points. Given a finite set of points in R 2 , say { ( x i , y i ) | 0 ≤ i < N } , first calculate their mean values by x ̄ = 1 N ∑ i x i and y ̄ = 1 N ∑ i y i . Let u i = x i − x ̄ , v i = y i − y ̄ for 0 ≤ i < N , and define S u = ∑ i u i , S u u = ∑ i u i 2 , S u v = ∑ i u i v i , etc. The problem is then to solve first in ( u , v ) coordinates, and then transform back to ( x , y ) . Considering that the circle has center ( u c , v c ) and radius R , the main goal is to minimize S = ∑ i g ( u i , v i ) 2 , where g ( u , v ) = ( u − u c ) 2 + ( v − v c ) 2 − α , and α = R 2 . To do that, it is necessary to differentiate S ( α , u c , v c ) in order to the two variables, resulting in the following expressions: (1) u c S u u + v c S u v = 1 2 ( S u u u + S u v v ) and (2) u c S u v + v c S v v = 1 2 ( S v v v + S v u u ) . Solving expressions (1) and (2) simultaneously allows to obtain ( u c , v c ). Then, the center ( x c , y c ) of the circle in the original coordinate system can be obtained as: ( x c , y c ) = ( u c , v c ) + ( x ̄ , y ̄ ) , being α = u c 2 + v c 2 + S u u + S v v N . The result of the combination of circle detection with the properties of the circle is illustrated in Fig. 5 . 3.2.4 Calculation of the center of the ball After knowing all the properties of the circle, it is possible to calculate the center of the ball through trigonometric relations as shown in Fig. 6 , where R is the radius of the ball, R ′ the radius of the circle and d the distance between the center of the circle and the center of the ball, with d = R 2 − R ′ 2 . Taking into account the ambiguity for the 2D lasers mentioned in Section 3.2, and considering that the center of the circle is at ( x c , y c ), the coordinates of the center of the ball for this laser are defined as follows: ( X c , Y c , Z c ) = ( x c , y c , ± d ) . Fig. 7 illustrates an example of the ball detection in a 2D scan. A similar methodology is applied to each of the four layers from the Sick LD-MRS laser sensor and a circle has to be successfully detected in each layer. Thus, in order to simplify the calculations in the 3D multi-layer laser, the coordinates of the circle center are transformed into the X Y plane for each layer; then for each layer, the center of the ball is calculated in the same way as in the 2D laser. After this, the centers of the ball are transformed back into the respective original plane. At this point, for each layer, the center of the ball was calculated, which means that there are as many centers as layers. Thus, statistically, the center of the ball is obtained by calculating the mean of all the centers. Considering that there are n layers and the different circles have ( x c 1 , y c 1 , z c 1 ) , … , ( x c n , y c n , z c n ) coordinates, the center mean coordinates are defined as follows: ( X c , Y c , Z c ) = ( 1 n ∑ i = 1 n x c i , 1 n ∑ i = 1 n y c i , 1 n ∑ i = 1 n z c i ) . 3.3 Ball detection on cameras Giving continuity to the work presented in [19], the integration of the cameras on the calibration process was also addressed, initially with a non-uniform color ball (see Fig. 12), which complicated its detection, therefore the ball was replaced by a uniform color (red) ball. The first approach to detect a single color ball in 2D images was based on the Hough Transform implementation available in the OpenCV library. The Hough Transform is a method used to extract features of specific shapes in images. Since the calibration target is a ball, the Hough Circle Transform was used to find points in the image that best fit the ball, returning its parameters (apparent radius in pixels R pix = D pix / 2 , and center in pixels ( x c pix , y c pix ) ). In order to detect the ball with Hough Circle Transform a total of five parameters had to be specified: 1. Ball color in HSV; 2. Canny threshold for edge detection; 3. Accumulator threshold for center detection; 4. Minimum circle radius in pixels; 5. Maximum circle radius in pixels. In practice, the method proved to be unreliable, since constant adjustments to minimum and maximum circle radius were required to detect the ball when its position changed. Therefore a second algorithm was implemented. This second approach is based on the Ramer–Douglas–Peucker algorithm [20,21], implemented in the OpenCV approxPolyDP function. When given a set of points that defines a curve, the algorithm finds a similar curve with fewer points, an approximated curve. The initial set of points that define a curve are obtained from the ball’s contour. Approximated contours with 6 or more vertices are considered potential circles. The area limited by the original contour is calculated ( A r . cont ) and an up-right bounding rectangle is computed in order to know its width ( w b . rect ) and height ( h b . rect ). A circle is positively detected when: (i) the bounding rectangle has similar height and width (3); and (ii) the area limited by the contour is similar to the area of a circle with radius R pix = w b . rect + h b . rect 4 (4). Thresholds on Eqs. (3) and (4) were obtained through empirical experiments. (3) | 1 − w b . rect h b . rect | ⩽ 0.2 . (4) | 1 − A r . cont π r 2 | ⩽ 0.2 . Fig. 8 (b) shows the result of applying this method to the image in Fig. 8(a), successfully detecting the red ball and calculating its radius ( R pix ) and center ( x c pix , y c pix ). It is also much more reliable than Hough Transforms since only the ball’s color needs to be specified at the start of the calibration process. Knowing the intrinsic parameters of the camera (obtained previously with a traditional chessboard calibration method like the one available in OpenCV) with focal length α x and α y approximately equal to α , the real world ball diameter ( D ) and the ball diameter in pixels ( D pix = 2 R pix ), it is possible to determine the distance from the camera to the ball ( Z c ) through the following equation, (5) Z c = α D D pix . Knowing Z c , the center of the ball coordinates on the image ( x c pix , y c pix ), and once again using the camera intrinsic parameters, it is possible to solve Eq. (6) for the ball center coordinates in the camera coordinate frame ( X c , Y c , Z c ) . (6) Z c [ x c pix y c pix 1 ] = K [ X c Y c Z c 1 ] . A Stereo configuration with two Point Grey cameras was also implemented however, tests revealed large calibration errors and as such this paper will focus on monocular cameras. 4 Calibration 4.1 3D rigid transformation The purpose of 3D transformation estimation is to align the point clouds obtained by each sensor. This alignment results on the relative pose (position and orientation) between sensors in a global coordinate frame, such that the overlapping areas between the point clouds match as well as possible. To compute the transformation between sensors an Absolute Orientation algorithm is used. A variant of the Iterative Closest Point (ICP) algorithm is used to estimate the 3D translation and rotation between a pair of point clouds, available on PCL. The objective is to solve the rigid transformation T that minimizes the error of the point pairs. For that purpose, the ICP algorithm provided by PCL has two error minimization metrics: point-to-point and point-to-plane. For this work the point-to-point (7) metric was applied, where p n and q n are 3D points of N pair correspondences from the source and target cloud. (7) E ( T ) = ∑ n = 1 N ‖ T p n − q n ‖ 2 . Minimizing the sum of the Euclidean distances between corresponding points is a least-square problem, which can be solved by using a closed-form solution, based on the Singular Value Decomposition (SVD) method proposed in [22,23]. The method is used to estimate the lasers and camera 6 DOF pose (roll, pitch, yaw, X, Y and Z) from their 3D point clouds consisting of ball center position ( X c , Y c , Z c ) in their respective coordinate systems. 4.2 External camera calibration External camera calibration can be used as an alternative to the 3D rigid transformation for cameras. Similarly to the 3D rigid transformation method, a 3D target cloud is used, however instead of using the ball center position in the camera coordinate system ( X c , Y c , Z c ) as the source cloud, this method uses the ball coordinates in a reference 3D sensor (a LMS for example) and the pixel center coordinates in the image coordinate system ( x c pix , y c pix ) to evaluate the extrinsic parameters of the camera. Pose estimation of a calibrated camera with a set of 3D points and their corresponding 2D image projections is a Perspective-n-Point problem. The problem is solved resorting to an iterative method based on Levenberg–Marquardt optimization algorithm, finding a pose that minimizes reprojection error. The implementation used was the one available in the solvePnP function of OpenCV. Given the frequent existence of outliers in the source and target pointclouds, more stable results were obtained using the available RANSAC version of the same algorithm (solvePnPRansac). 5 Results Several experiments were carried out and results are focused in the evaluation of the following three fronts: (i) consistency of the ball detection for each laser sensor and SwissRanger; (ii) consistency in the 3D transformation estimation depending on the number of points used for each laser and SwissRanger; and (iii) global validation of the method for laser sensors and PointGrey cameras. The experiments carried out do not provide an absolute validation of the resulting estimated transformations from the calibration procedure, nevertheless, the obtained results demonstrate the validity of the method. 5.1 Consistency of ball detection In the ball detection consistency test, and in similar conditions, the ball was placed statically at different distances from the 3D sensors and, for each position, 500 samples of coordinates of the ball center were acquired. From these samples, a mean and a standard deviation were calculated. Fig. 9 shows the standard deviations of the measurements of the three sensors used on this test. Since the camera is not returning directly 3D information, it was not considered in this test. The results show that the variation is consistent with the error associated to each sensor as shown in Table 1 (below 10 cm for the Sick LD-MRS, and about 1 cm for the remaining sensors). In the case of the SwissRanger a greater variation is verified for the closest and furthest distances; it may be due to the proximity of the ball and a lower density of points for further distances, but a more detailed study for confirmation is needed. Nonetheless, the standard deviation in the detection of the ball is not larger than the error associated to the standalone sensors, indicating that the method does not introduce new measurement errors. For the Sick LD-MRS, even with results in accordance with its associated error, it is difficult to evaluate the interference of the distance of the ball on its detection, which can be possibly explained by the fluctuations in the data provided by the sensor; for example, it is possible to have a set of 500 measurements of a static object at 10 m, where 80% of the measurements have a deviation of ±0.01 m and the remainder 20% have a deviation of ±0.08 m. On the other hand, for the exact same conditions, if a new set of 500 measurements is obtained, a ratio of 50%/50% may be found instead. 5.2 Consistency of the geometric transformation In the second set of tests, a calibration among all the 3D sensors was performed using always the same setup; this was done by using different sizes of point clouds (number of points), where each point of the cloud corresponds to a different position of the ball along its motion in front of the sensors. Thus, for each size of the point clouds, a set of 20 calibrations was performed with the respective matrix of the estimated rigid transformation. The analysis of results compares the translation and rotation; the translation is obtained directly from the matrix of the associated rigid geometric transformation; considering the rotation matrix ( R ) from calibration and R x , R y and R z the generic rotations around each axis, R is defined as R = R x ( Roll ) R y ( Pitch ) R z ( Y aw ) , which allows to be solved and obtain the Euler angles ( Roll , Pitch , Y aw ). Then, as in the first test, a mean and a standard deviation of the translation and Euler angles are calculated, with the difference that the Euler angles are analyzed individually. Fig. 10 shows the results for the translation, and Fig. 11 presents the results for the Euler angles. Fig. 12 shows the setup used for the calibration and the estimated positions of the sensors after being calibrated, it can noted that in this setup the ball does not need to be in a single color since all the data is acquired directly with 3D sensors. As expected, the standard deviation decreases with the number of points, and stabilizes at around 20 points; for this test the minimum distance between consecutive points was 10 cm. The mean variation from point clouds of 20 points is lower than 10 cm and about 4 ° or 5 ° , which, once again, is in the range of the standalone sensors errors. 5.3 Validation of the method The difficulty of having a ground-truth for the sensors relative position complicates the absolute validation of the method, which would allow the comparison of the estimated transformation with the exact transformation among sensors. It is very difficult to guarantee with precision if the obtained calibration is correct, given the difficulty to measure exactly the correct pose between pairs of sensors. This problem is particularly difficult with rotations (it is possible to have a fairly reasonable evaluation of the translation with simple measurements using a measuring tape), which is critical, since small errors in rotation may result in large error in the final registered data. With this problem in mind, our goal has been to perform a calibration of all the devices with respect to the sensor with better accuracy. Then, the point cloud of the reference sensor is used as ground-truth, and on the remainder sensors the resulting transformation from the calibration is applied. This allows to evaluate how well the point clouds fit in the reference point cloud by calculating the absolute mean error and the standard deviation of the corresponding points. This test was performed with a full size vehicle (ATLASCAR) to evaluate the method on real conditions. And since these tests took place outdoors, the devices used are the ones that will have some functionality on the ATLASCAR project, which means that the set of sensors does not include the SwissRanger, since sunlight interferes with the infrared light it projects. The sensors used in this experiment are: the Sick LD-MRS, the two Sick LMS151 and a Point Grey camera. One of the Sick LMS151 was chosen as reference (henceforth named Sick LMS151(A)) and the remainder are calibrated relatively to it. The experience has the following steps: 1. Choose one of the sensors as reference (in this case the Sick LMS151(A) was chosen given its higher accuracy); 2. Point cloud acquisition for each sensor and calibrate them with respect to the reference sensor; 3. Apply the resulting transformations to the respective point cloud of each sensor; 4. Results evaluation: (a) For transformations calculated with the 3D rigid transformation method, described in Section 4.1, the mean euclidean distance and respective standard deviation between corresponding points of the source point cloud and target point cloud (LMS151(A)) is used to evaluate its results; (b) For the LMS151(A) to camera transformation, calculated by the external camera calibration method as described in Section 4.2, the mean reprojection error is used to evaluate its results; (c) Visual assessment. The acquisition of the point clouds was performed with two different approaches: (i) following a grid-pattern and, (ii) acquiring random points during the motion of the ball in front of the sensors. The idea of acquiring a pattern is to indirectly establish a better ground-truth. The acquisition of random points is intended to evaluate the calibration method in a real world scenario. Fig. 13 shows the setup mounted on ATLASCAR, where the Sick LD-MRS is placed on the top of a box since there is not yet a support on the car for it; nonetheless, its final location will not be much different from where it is shown in Fig. 13. 5.3.1 Grid pattern calibration To perform the calibration with a grid pattern, 15 markers were positioned on the ground, in a 3×5 grid arrangement, as can be seen in Fig. 14 (a). The ball was then placed on top of each marker and a point cloud was acquired for each sensor. In order to evaluate the grid pattern, Fig. 14(b) shows the acquired point cloud for Sick LMS151(A) (reference sensor) overlayed by a virtual grid. With a perfect grid and laser, the distance between a ball center on grid point i and a ball center on grid point i + 1 , would be 1 m. To evaluate this condition, the vertical and horizontal euclidean distance between ball centers and ball centers was calculated and is represented on Fig. 14(c), which shows that the mean overall distance between ball centers is 1.027 m, only 2.7 cm above what would be expected from a perfect laser and grid. The acquired point clouds for each sensor are presented on Fig. 15 (a) with respect to their own coordinate system. The 3D rigid transformation method is applied to estimate the geometric transformations between the Sick LMS151(A) and the other sensors, whose point clouds are displayed on Fig. 15(a). Regarding the camera, as discussed in Section 4, there is an alternative method, based on the external camera calibration (Section 4.2), which resorts to a point cloud of ball centers in the image coordinate system (see Fig. 15(b)) and the 3D point cloud from Sick LMS151(A). Resulting transformations from both methods are then applied to the point cloud and pose of each sensor. To evaluate the quality of the point cloud fitting obtained through the 3D rigid transformation method, we present on Table 2 , the mean absolute error and standard deviation, based on the euclidean distance between corresponding points of a sensor point cloud and the Sick LMS151(A) point cloud. For both Sick laser sensors the results from Table 2 are expected since they do not differ much from results obtained in Section 5.1 and their own intrinsic errors. Concerning the Point Grey camera, Table 2 shows an error larger than expected, indicating that the point cloud fitting is not as good as with the lasers. Concerning the external camera calibration method, the same metric cannot be applied since the camera’s point cloud is in the image coordinate system. However, it is possible to project the Sick LMS151(A) point cloud to the image coordinate system, using the geometric transformation obtained by this method. Thus, making it also possible to compute the mean reprojection error of such transformation—in this case the reprojection error is 3.7 pixels. Fig. 16 shows the initial ball centers detected by the camera and the projected 3D Sick LMS151(A) points. To visually assess the lasers and camera pose relative to the car as well as the point cloud alignment, Fig. 17 is presented. To ease the visual assessment a Ford Escort SW 98 3D model (same as the ATLASCAR) was placed in the scene. However, the model does not include any of the extra equipment used on the actual car, like the supports for the sensors. In Fig. 17 sensors are represented by arrows; for lasers the arrow points in the X axis direction of their own frame and for cameras it points in the their Z axis direction. A comparison between Figs. 13 and 17 shows that the calibration results for the Sick LD-MRS is very close to its real position, however the Sick LMS151 is a few centimeters above its real position. Regarding the two methods used to estimate the camera pose, comparing Figs. 13 and 17, we conclude that the external camera calibration method yielded better results. The source for such a significant error with the 3D rigid transform method is probably related to the fact that the ball center coordinates in the camera coordinate system, ( X c , Y c , Z c ) , are calculated based on the detected diameter. Since Z c is inversely proportional to the detected diameter, Eq. (5) and is then used to calculate X c and Y c , Eq. (6). Even a slight variation of one or two pixels in the detected ball diameter during calibration, causes errors in the range of centimeters in all X c , Y c and Z c coordinates. It was observed that even though the diameter frequently suffers small variations, the detected ball center was much more stable, which explains the better results with the external camera calibration method. 5.3.2 Calibration with random acquisition spots The calibration method does not need to follow any particular pattern with the corresponding points. Thus, this new test is intended to evaluate a calibration using points in random positions, allowing for a higher point density. To take advantage of this, we chose to increase the number of points per point cloud to 25. The acquired point clouds for each sensor with respect to their own frame are shown in Fig. 18 (a) for point clouds that use the 3D transform method; Fig. 18(b) shows the detected ball centers in the image coordinate system to be used with the external camera calibration method. Again, we chose Sick LMS151(A) as the reference sensor. After the calibration process, the estimated transformation was applied on the corresponding point cloud. Then, as in the previous test, the point cloud fitting mean absolute errors and standard deviation for the 3D rigid transform method were computed and are presented on Table 3 . Regarding the external camera calibration method, similarly to the grid pattern test, Sick’s LMS151(A) 3D point cloud was projected to the image plane and a mean reprojection error of 3.3 pixels was obtained; Fig. 19 shows the reprojection error. The final arrangement of the sensor after calibration (with the Ford Escort SW 98 3D model) is shown on Fig. 20 . Comparing Figs. 13, 17 and 20, Sick’s LMS151 position and the Point Grey camera position, obtained from the 3D rigid transform method, have improved on this test, while Sick’s LD-MRS position is identical on both tests. The overall improvement on 3D rigid transform results is likely due to the increase in points per point cloud. The external camera calibration method results appear to be slightly worse than the 3D rigid transform method. However, since the 3D model is not an exact replica of ATLASCAR, no conclusion can be drawn from Fig. 20 about which method is more accurate. 6 Conclusions and future work This paper proposes a new automatic calibration methodology to seamlessly calibrate different 2D/3D sensors and cameras by using a ball as calibration target. In the current version, the only a priori parameter required is the relative position between laser scanners and the ball—whether it is placed above or below its equator. The good results on the consistency tests for the ball detection and transformation estimation were confirmed with the validation tests by the small errors that were obtained on the calibration of the LIDAR lasers. Regarding the camera, tests showed that both 3D rigid transformation method and external camera calibration method were capable of yielding good results. Although the external camera calibration was the most consistent of the two. Future work includes improving the camera calibration results and designing an absolute ground-truth experiment to evaluate the estimated transformations. It is also planned to impose full 3D motion paths on the ball, possibly hopping or bouncing in the sensors fields, to solve all ambiguities for any positions of the sensors, and thus requiring no a priori information on hemisphere position relative to lasers. Also, it would be interesting to use external high precision devices (for example a motion capture setup) for a better validation of the method. Finally, in order to simplify the use of the method, an interface is to be developed to be a more user friendly method, including possibly a software package to be released for the ROS community. References [1] V. Santos, J. Almeida, E. Avila, D. Gameiro, M. Oliveira, R. Pascoal, R. Sabino, P. Stein, ATLASCAR—Technologies for a computer assisted driving system on board a common automobile, in: 13th International IEEE Conference on Intelligent Transportation Systems, ITSC2010, Madeira Island, Portugal, 2010. [2] J. Dias Distanciómetro 3D baseado numa unidade laser 2D em movimento contínuo (Masters’ thesis) 2009 Universidade de Aveiro [3] J. Levinson S. Thrun Automatic online calibration of cameras and lasers Robotics: Science and Systems (RSS) 2013 [4] Y. Bok, D. Choi, I. Kweon, Generalized laser three-point algorithm for motion estimation of camera-laser fusion system, in: IEEE International Conference on Robotics and Automation, ICRA, 2013, pp. 2880–2887. [5] J. Shi, C. Tomasi, Good features to track, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1994, pp. 593–600. [6] Y. Bok, D.-G. Choi, Y. Jeong, I.S. Kweon, Capturing citylevel scenes with a synchronized camera-laser fusion sensor, in: Proceedings of the IEEE/RSf International Conference on Intelligent Robots and Systems, 2011, pp. 4436–4441. [7] S.A. Rodriguez F., V. Fremont, P. Bonnifait, Extrinsic calibration between a multi-layer lidar and a camera, in: IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, 2008, pp. 214–219. [8] S. Debattisti, L. Mazzei, M. Panciroli, Automated extrinsic laser and camera inter-calibration using triangular targets, in: IEEE Intelligent Vehicles Symposium, IV, 2013, pp. 696–701. [9] M. Almeida P. Dias M. Oliveira V. Santos 3D-2D Laser Range Finder calibration using a conic based geometry shape Aurélio Campilho Mohamed Kamel. Image Analysis and Recognition 2012 Springer Verlag 312 319 [10] J. Underwood, A. Hill, S. Scheding, Calibration of range sensor pose on mobile platforms, in: IEEE/RSJ International Conference on Intelligent Robots and Systems, 2007, pp. 3866–3871. [11] G. Chao, J. Spletzer, On-line calibration of multiple LIDARs on a mobile vehicle platform, in: IEEE International Conference on Robotics and Automation, ICRA, 2010, pp. 279–284. [12] E. Fernandez-Moral J. Gonzalez-Jiménez V. Arévalo Extrinsic calibration of 2D laser rangefinders from perpendicular plane observations Int. J. Robot. Res. 2015 [13] M. Ruan, D. Huber, Extrinsic calibration of 3D sensors using a spherical target, in: Second International Conference on 3D Vision, 3DV, 2014, pp. 187–193. [14] R.B. Rusu, S. Cousins, 3D is here: Point cloud library (PCL), in: IEEE International Conference on Robotics and Automation, ICRA, 2011, pp. 1–4. [15] G. Bradski The OpenCV Library Dr Dobbs J. Softw. Tools 25 2000 120 125 [16] M.A. Fischler R.C. Bolles Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography Commun. ACM 24 6 1981 381 395 [17] D. Coimbra LIDAR Target Detection and Segmentation in Road Environment (Masters’ thesis) 2013 Universidade de Aveiro [18] J. Xavier, M. Pacheco, D. Castro, A. Ruano, Fast line, arc/circle and leg detection from laser scan data in a Player driver, in: Proceedings of the 2005 IEEE International Conference on Robotics and Automation, ICRA, 2005, pp. 3930–3935. [19] M. Pereira, V. Santos, P. Dias, Automatic calibration of multiple LIDAR sensors using a moving sphere as target, in: Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Vol. 1, 2015, pp. 477–486. [20] U. Ramer An iterative procedure for the polygonal approximation of plane curves Comput. Graph. Image Process. 1 3 1972 244 256 [21] D.H. Douglas T.K. Peucker Algorithms for the reduction of the number of points required to represent a digitized line or its caricature Cartogr.: Int. J. Geogr. Inf. Geovisualization 10 2 1973 112 122 [22] K.S. Arun T.S. Huang S.D. Blostein Least-squares fitting of two 3-D point sets IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 9 5 1987 698 700 [23] B.K.P. Horn Closed-form solution of absolute orientation using unit quaternions J. Opt. Soc. Am. A 4 4 1987 629 642 Marcelo Pereira obtained his Master’s Degree in Mechanical Engineering in 2015 at the University of Aveiro, Portugal. During his later graduation studies he focused especially on applications in automation, computer vision robotics, and programming in the ROS framework. His first technical publication took place in the ROBOT2015 conference. He played futsal during three years in Atlético Clube do Luso, being team captain in his last year. His current interests are related to robotics applications, application of perception systems in autonomous navigation and vision systems. David Silva is currently writing his Master’s Degree Thesis in Mechanical Engineering at the University of Aveiro, Portugal, within the Laboratory for Automation and Robotics (LAR). His Thesis is entitled “Multisensory navigation based on LIDAR for ATLASCAR”. During his Master’s Degree he specialized in programming, computer vision, automation, robotics and computerized numerical control. Vitor Santos obtained a 5-year degree in Electronics Engineering and Telecommunications in 1989, at the University of Aveiro, Portugal, where he later obtained a Ph.D. in Electrical Engineering in 1995. He was awarded fellowships to pursue research in mobile robotics during 1990–1994 at the Joint Research Center, Italy. He is currently Associate Professor at the University of Aveiro and lectures courses related to advanced perception and robotics, and has managed research activity on mobile robotics, advanced perception and humanoid robotics, with the supervision or co-supervision of more than 100 graduate and undergraduate students, and more that 120 publications in conferences, books and journals. At the University of Aveiro he has coordinated the ATLAS project for mobile robot competition that achieved 6 first prizes in the annual Autonomous Driving competition and has coordinated the development of ATLASCAR, the first real car with autonomous navigation capabilities in Portugal. He is one of the founders of Portuguese Robotics Open in 2001 where he has kept active participation ever since. He is also co-founder of the Portuguese Society of Robotics, and participated several times in its management since its foundation in 2006. His current interests extend to humanoid robotics and the application of techniques from perception and mobile robotics to autonomy and safety in ADAS contexts. Paulo Dias graduated from the University of Aveiro Portugal in 1998 and started working in 3D reconstruction at the European Joint research Centre in Italy. In September 2003, he concluded his Ph.D. with the thesis “3D Reconstruction of real World Scenes Using Laser and Intensity Data”. He is currently an assistant professor within the Department of Electronics Telecommunications and Informatics (DETI) and is involved in several works and projects within the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) related to 3D Reconstruction, Virtual Reality, Computer Vision, Computer Graphics, Visualization and Combination and Fusion of data from multiple sensors. "
    },
    {
        "doc_title": "Developing 3D freehand gesture-based interaction methods for virtual walkthroughs: Using an iterative approach",
        "doc_scopus_id": "85014148227",
        "doc_doi": "10.4018/978-1-5225-0435-1.ch003",
        "doc_eid": "2-s2.0-85014148227",
        "doc_date": "2016-06-29",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D interactions",
            "Controlled experiment",
            "Development methodology",
            "Gesture tracking",
            "Gesture-based interaction",
            "Iterative approach",
            "Navigation methods",
            "Research topics"
        ],
        "doc_abstract": "© 2016 by IGI Global. All rights reserved.Gesture-based 3D interaction has been considered a relevant research topic as it has a natural application in several scenarios. Yet, it presents several challenges due to its novelty and consequential lack of systematic development methodologies, as well as to inherent usability related problems. Moreover, it is not always obvious which are the most adequate and intuitive gestures, and users may use a variety of different gestures to perform similar actions. This chapter describes how spatial freehand gesture based navigation methods were developed to be used in virtual walkthroughs meant to be experienced in large displays using a depth sensor for gesture tracking. Several iterations of design, implementation, user tests, and controlled experiments performed as formative and summative evaluation to improve, validate, and compare the methods are presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gesture interactions for virtual immersive environments: Navigation, selection and manipulation",
        "doc_scopus_id": "84978891255",
        "doc_doi": "10.1007/978-3-319-39907-2_20",
        "doc_eid": "2-s2.0-84978891255",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3duis",
            "Gestural interaction",
            "Gesture interaction",
            "Gesture-based interaction",
            "Immersive environment",
            "Kinect",
            "Learning difficulties",
            "User study"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.This paper presents an extension to a Platform for Setting-Up Virtual environments with the purpose of allowing gesture interaction. The proposed solution maintains the flexibility of the original framework as well as content association (PDF, Video, Text), but allows new interactions based on gestures. An important feature is the one to one navigational input based on Kinect skeleton tracking. The framework was used to configure a virtual museum art installation using a real museum room where the user can move freely and interact with virtual contents by adding and manipulating 3D models. Two user studies were performed to compare gestures against button-controlled interactions for navigation and 3D manipulation. Most users preferred the Kinect-based navigation and gesture-based interaction despite some learning difficulties and tracking problems. Regarding manipulation, the gesture-based method was significantly faster with similar accuracy when compared to the controller. On the other hand, when dealing with rotations, the controller-based method was faster.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Living globe: Tridimensional interactive visualization of world demographic data",
        "doc_scopus_id": "84978891214",
        "doc_doi": "10.1007/978-3-319-40349-6_2",
        "doc_eid": "2-s2.0-84978891214",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D globe",
            "Demographic data",
            "Heuristic evaluation",
            "Information visualization",
            "Usability evaluation",
            "WebGL Globe"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.This paper presents Living Globe, an application for visualization of demographic data supporting the temporal comparison of data from several countries represented on a 3D globe. Living Globe allows the visual exploration of the following demographic data: total population, population density and growth, crude birth and death rates, life expectancy, net migration and population percentage of different age groups. While offering unexperienced users a default mapping of these data variables into visual variables, Living Globe allows more advanced users to select the mapping, increasing its flexibility. The main aspects of the Living Globe model and prototype are described as well as the evaluation results obtained using heuristic evaluation and usability testing. Some conclusions and ideas for future work are also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A ground truth vision system for robotic soccer",
        "doc_scopus_id": "84969962024",
        "doc_doi": "10.5220/0005817506840689",
        "doc_eid": "2-s2.0-84969962024",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Biped walking",
            "Ground truth",
            "Monitoring system",
            "Perception systems",
            "Robotic soccer",
            "Robotic soccer game",
            "Robotic vision",
            "Vision systems"
        ],
        "doc_abstract": "© Copyright 2016 by SCITEPRESS -Science and Technology Publications, Lda. All rights reserved.Robotic soccer represents an innovative and appealing test bed for the most recent advances in multi-agent systems, artificial intelligence, perception and navigation and biped walking. The main sensorial element of a soccer robot must be its perception system, most of the times based on a digital camera, through which the robot analyses the surrounding world and performs accordingly. Up to this date, the validation of the vision system of a soccer robots can only be related to the way the robot and its team mates interpret the surroundings, relative to their owns. In this paper we propose an external monitoring vision system that can act as a ground truth system for the validations of the objects of interest of a robotic soccer game, mainly robots and ball. The system we present is made of two to four digital cameras, strategically positioned above the soccer field. We present preliminary results regarding the accuracy of the detection of a soccer ball, which proves that such a system can indeed be used as a provider for ground truth ball positions on the field during a robotic soccer game.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using Heuristic Evaluation to Foster Visualization Analysis and Design Skills",
        "doc_scopus_id": "84962627039",
        "doc_doi": "10.1109/MCG.2016.7",
        "doc_eid": "2-s2.0-84962627039",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Computer graphics education",
            "Design visualization",
            "Heuristic evaluation",
            "Information visualization",
            "Systematic analysis method",
            "Usability evaluation methods",
            "Visualization analysis",
            "Visualization application"
        ],
        "doc_abstract": "© 1981-2012 IEEE.In an effort to develop visualization analysis and design skills in master's level information visualization students, the authors use a well-known analytical usability evaluation method, heuristic evaluation, with different sets of heuristics to teach students to analyze visualization applications. The proposed approach, used for three consecutive years, has successfully stimulated critical analysis and discussion sessions as well as helped raise students' awareness concerning the benefit of using systematic analysis methods and the strategies and guidelines that should be used to design visualization applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Representations for autonomous driving: An approach based on polygonal primitives",
        "doc_scopus_id": "84951868739",
        "doc_doi": "10.1007/978-3-319-27146-0_39",
        "doc_eid": "2-s2.0-84951868739",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3-D scene representation",
            "Autonomous driving",
            "Autonomous Vehicles",
            "Geometric structure",
            "Macro scale",
            "Novel methodology",
            "Point cloud",
            "Scene reconstruction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.In this paper, we present a novel methodology to compute a 3D scene representation. The algorithm uses macro scale polygonal primitives to model the scene. This means that the representation of the scene is given as a list of large scale polygons that describe the geometric structure of the environment. Results show that the approach is capable of producing accurate descriptions of the scene. In addition, the algorithm is very efficient when compared to other techniques.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic calibration of multiple LIDAR sensors using a moving sphere as target",
        "doc_scopus_id": "84951782220",
        "doc_doi": "10.1007/978-3-319-27146-0_37",
        "doc_eid": "2-s2.0-84951782220",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D data",
            "Automatic calibration",
            "Automatic method",
            "Calibration system",
            "Fitting techniques",
            "Point cloud",
            "Rigid body transformation",
            "Sensor calibration"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.The number of LIDAR sensors installed in robotic vehicles has been increasing, which is a situation that reinforces the concern of sensor calibration. Most calibration systems rely on manual or semi-automatic interactive procedures, but fully automatic methods are still missing due to the variability of the nearby objects with the point of view. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. This is indeed feasible if a ball is placed in motion in front of the set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation between all pairs of sensors. This paper proposes and describes such a method with encouraging preliminary results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "CraMs: Craniometric analysis application using 3d skull models",
        "doc_scopus_id": "84961700580",
        "doc_doi": "10.1109/MCG.2015.136",
        "doc_eid": "2-s2.0-84961700580",
        "doc_date": "2015-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "3D models",
            "Bone damages",
            "Initial guess",
            "Interobserver variability",
            "Point detection",
            "Points of interest",
            "Skull models",
            "Anatomic Landmarks",
            "Cephalometry",
            "Computer Graphics",
            "Humans",
            "Image Interpretation, Computer-Assisted",
            "Imaging, Three-Dimensional",
            "Models, Anatomic",
            "Reproducibility of Results",
            "Sensitivity and Specificity",
            "Skull",
            "Software"
        ],
        "doc_abstract": "© 1981-2012 IEEE.Craniometric analysis plays an important role in anthropology studies and forensics. This paper presents CraMs, an application using a new craniometric approach based on 3D models of the skull. The main objective is to obtain, through a process supervised by anthropologists, the main points of interest used to compute craniometric measurements. The application aids this process by analyzing the skull geometry and automatically providing points of interest. The application also allows for semiautomatic point detection, where the user provides an initial guess that might be refined based on the curvature of the skull, as well as the manual selection of any other points of interest. Moreover, results comparing measurements obtained with CraMs and traditional craniometry methods on eight skulls suggest that the application provides comparable craniometric measurements and lower inter-observer variability. This approach offers advantages such as an easier access to skulls with no risk of bone damage and the possibility of defining new measurements based on morphology or other skull characteristics, which are not possible using traditional methods.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of aerial balls in robotic soccer using a mixture of color and depth information",
        "doc_scopus_id": "84933059765",
        "doc_doi": "10.1109/ICARSC.2015.13",
        "doc_eid": "2-s2.0-84933059765",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Ball detection",
            "Color classification",
            "League competition",
            "Omni-directional vision",
            "Omnidirectional cameras",
            "Real-time application",
            "Robotic vision",
            "Trajectory calculations"
        ],
        "doc_abstract": "© 2015 IEEE.Detection of aerial objects is a difficult problem to tackle given the dynamics and speed of a flying object. The problem is even more difficult when considering a noncontrolled environment, where the predominance of a given color is not guaranteed, and/or when the vision system is located on a moving platform. Taking as an example the game of robotic soccer promoted by the RoboCup Federation, most of the teams participating in the soccer competitions detect the objects in the environment using an omni directional camera. Omni directional vision systems only detect the ball when it is on the ground, and thus precise information on the ball position when in the air is lost. In this paper we present a novel approach for 3D ball detection in which we use the color information to identify ball candidates and the 3D data for filtering the relevant color information. The main advantage of our approach is the low processing time, being thus suitable for real-time applications. We present experimental results showing the effectiveness of the proposed algorithm. Moreover, this approach was already used in the last official RoboCup Middle Size League competition. The goalkeeper was able to move to a right position in order to defend a goal, in situations where the ball was flying towards the goal.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A virtual and augmented reality course based on inexpensive interaction devices and displays",
        "doc_scopus_id": "85063434058",
        "doc_doi": "10.2312/eged.20151022",
        "doc_eid": "2-s2.0-85063434058",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Computer science and engineerings",
            "Course organization",
            "Graduate level course",
            "Interaction devices",
            "Practical projects",
            "Technical skills",
            "Techniques and tools",
            "Virtual and augmented reality"
        ],
        "doc_abstract": "© The Eurographics Association 2015.In the last years a plethora of affordable displays, sensors, and interaction devices has reached the market, fostering the application of Virtual and Augmented Reality to many new situations. Yet, creating such applications requires a good understanding of the field and specific technical skills typically not provided by current Computer Science and Engineering education. This paper presents a graduate level course offered to MSc. Programs in Computer and Electrical Engineering which introduces the main concepts, techniques and tools in Virtual and Augmented Reality. The aim is to provide students with enough background to understand, design, implement and test such applications. The course organization, the main issues addressed and bibliography, the sensors, interaction devices and displays used, and a sample of the practical projects are briefly described. Major issues are discussed and conclusions are drawn.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of aerial balls using a Kinect sensor",
        "doc_scopus_id": "84958548108",
        "doc_doi": "10.1007/978-3-319-18615-3_44",
        "doc_eid": "2-s2.0-84958548108",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Detection of objects in the air is a difficult problem to tackle given the dynamics and speed of a flying object. The problem is even more difficult when considering a non-controlled environment where the predominance of a given color is not guaranteed, and/or when the vision system is located on a moving platform. As an example, most of the Middle Size League teams in RoboCup competition detect the objects in the environment using an omni directional camera that only detects the ball when in the ground, and losing any precise information of the ball position when in the air. In this paper we present a first approach towards the detection of a ball flying using a Kinect camera as sensor. The approach only uses 3D data and does not consider, at this time, any additional intensity information. The objective at this stage is to evaluate how useful is the use of 3D information in the Middle Size League context. A simple algorithm to detect a flying ball and evaluate its trajectory was implemented and preliminary results are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Developing and evaluating two gestural-based virtual environment navigation methods for large displays",
        "doc_scopus_id": "84947297363",
        "doc_doi": "10.1007/978-3-319-20804-6_13",
        "doc_eid": "2-s2.0-84947297363",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3DUIs",
            "Depth sensors",
            "Gestural interaction",
            "Large displays",
            "Navigation in virtual en-vironments",
            "Navigation methods",
            "User study"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.In this paper we present two methods to navigate in virtual environments displayed in a large display using gestures detected by a depth sensor. We describe the rationale behind the development of these methods and a user study to compare their usability performed with the collaboration of 17 participants. The results suggest the users have a better performance and prefer one of them, while considering both as suitable and natural navigation methods.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Heuristic evaluation in information visualization using three sets of heuristics: An exploratory study",
        "doc_scopus_id": "84947228566",
        "doc_doi": "10.1007/978-3-319-20901-2_24",
        "doc_eid": "2-s2.0-84947228566",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Cognitive and visual heuristics",
            "Evaluation methods",
            "Exploratory studies",
            "Heuristic evaluation",
            "Information visualization",
            "InfoVis evaluation",
            "Usability",
            "Visualization application"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Evaluation in Information Visualization is inherently complex, and it is still a challenge. Whereas it is possible to adapt evaluation methods from other fields, as Human-Computer Interaction, this adaptation may not be straightforward since visualization applications are very specific interactive systems. This paper addresses issues in using heuristic evaluation to evaluate visualizations and visualization applications, and presents an exploratory study in two phases and involving 25 evaluators aimed at assessing the understandability and effectiveness of three sets of heuristics that have been used in Information Visualization.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation in visualization: Some issues and best practices",
        "doc_scopus_id": "84894530717",
        "doc_doi": "10.1117/12.2038259",
        "doc_eid": "2-s2.0-84894530717",
        "doc_date": "2014-03-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The first data and information visualization techniques and systems were developed and presented without a systematic evaluation; however, researchers have become, and are more and more, aware of the importance of evaluation (Plaisant, 2004)1. Evaluation is not only a means of improving techniques and applications, but it can also produce evidence of measurable benefits that will encourage adoption. Yet, evaluating visualization applications or techniques, is not simple. We deem visualization applications should be developed using a user-centered design approach and that evaluation should take place in several phases along the process and with different purposes. An account of what issues we consider relevant while planning an evaluation in Medical Data Visualization can be found in (Sousa Santos and Dillenseger, 2005) 2. In that work the question \"how well does a visualization represent the underlying phenomenon and help the user understand it?\" is identified as fundamental, and is decomposed in two aspects: A) the evaluation of the representation of the phenomenon (first part of the question). B) the evaluation of the users' performance in their tasks when using the visualization, which implies the understanding of the phenomenon (second part of the question). We contend that these questions transcend Medical Data Visualization and can be considered central to evaluating Data and Information Visualization applications and techniques in general. In fact, the latter part of the question is related to the question Freitas et al. (2009) 3 deem crucial to user centered visualization evaluation: \"How do we know if information visualization tools are useful and usable for real users performing real visualization tasks?\" In what follows issues and methods that we have been using to tackle this latter question, are briefly addressed. This excludes equally relevant topics as algorithm optimization, and accuracy, that can be dealt with using concepts and methods well known in other disciplines and are mainly related to how well the phenomenon is represented. A list of guidelines considered as our best practices to perform evaluations is presented and some conclusions are drawn. © 2014 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Morphing techniques for creating and representing spatiotemporal data in GIS",
        "doc_scopus_id": "84911865064",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84911865064",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Environmental Engineering",
                "area_abbreviation": "ENVI",
                "area_code": "2305"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Database research",
            "Morphing",
            "Morphing techniques",
            "Remote sensing technology",
            "Satellite images",
            "Spatio-temporal data",
            "Spatio-temporal database",
            "Spatiotemporal data model"
        ],
        "doc_abstract": "With the development of remote sensing technologies allowing capturing and transmitting geo-referenced data repeatedly along time, there are many applications demanding for efficient tools to deal with spatiotemporal data. However database research on moving objects with extent has mainly focused on spatiotemporal data models and query languages leaving several issues to be solved regarding, for example, the acquisition of spatiotemporal data. This work deals with the application of 2D polygonal morphing techniques to create spatiotemporal data representations of moving objects that may change location, size or shape continuously over time. The aim is to transform a sequence of observations representing a moving object at different time instants into a continuous movement representation suitable to be loaded into spatiotemporal databases. This work also investigates several strategies to minimize users' intervention during the processing of a sequence of observations and presents an evaluation of the reliability of movement representations using real data. The movement of icebergs in the Antarctic seas is used as case study and the data sources are sequences of satellite images capturing the position and shape of the icebergs at different dates.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Choosing a selection technique for a virtual environment",
        "doc_scopus_id": "84903639109",
        "doc_doi": "10.1007/978-3-319-07458-0_21",
        "doc_eid": "2-s2.0-84903639109",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Comparative studies",
            "Contextual information",
            "Interaction methods",
            "Interactive virtual environments",
            "Multimedia information",
            "Selection",
            "Selection techniques",
            "Virtual application"
        ],
        "doc_abstract": "Bearing in mind the difficulty required to create virtual environments, a platform for Setting-up Interactive Virtual Environments (pSIVE) was created to help non-specialists benefit from virtual applications involving virtual tours where users may interact with elements of the environment to extract contextual information. The platform allows creating virtual environments and setting up their aspects, interaction methods and hardware to be used. The construction of the world is done by loading 3D models and associating multimedia information (videos, texts or PDF documents) to them. A central interaction task in the envisioned applications of pSIVE is the selection of objects that have associated multimedia information. Thus, a comparative study between two variants of the ray-tracing selection technique was performed. The study also demonstrates the flexibility of the platform, since it was easily adapted to serve as a test environment. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Platform for setting up interactive virtual environments",
        "doc_scopus_id": "84901810220",
        "doc_doi": "10.1117/12.2038668",
        "doc_eid": "2-s2.0-84901810220",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Interaction",
            "Interactive informations",
            "Interactive virtual environments",
            "Platform development",
            "Production line",
            "Selection",
            "Testing environment",
            "Virtual worlds"
        ],
        "doc_abstract": "This paper introduces pSIVE, a platform that allows the easy setting up of Virtual Environments, with interactive information (for instance, a video or a document about a machine that is present in the virtual world) to be accessed for different 3D elements. The main goal is to create for evaluation and training on a virtual factory-but generic enough to be applied in different contexts by non-expert users (academic and touristic for instance). We show some preliminary results obtained from two different scenarios: first a production line of a factory with contextualized information associated to different elements which aimed the training of employees. Second a testing environment, to compare and assess two different selection styles that were integrated in pSIVE and to allow different users to interact with an environment created with pSIVE to collect opinions about the system. The conclusions show that the overall satisfaction was high and the comments will be considered in further platform development. © 2014 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing two input devices for virtual walkthroughs using a head mounted display (HMD)",
        "doc_scopus_id": "84901800305",
        "doc_doi": "10.1117/12.2036486",
        "doc_eid": "2-s2.0-84901800305",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Ease-of-use",
            "Head mounted displays",
            "Input and outputs",
            "Input devices",
            "User performance",
            "User study",
            "Walkthroughs"
        ],
        "doc_abstract": "Selecting input and output devices to be used in virtual walkthroughs is an important issue as it may have significant impact in usability and comfort. This paper presents a user study meant to compare the usability of two input devices used for walkthroughs in a virtual environment with a Head-Mounted Display. User performance, satisfaction, ease of use and comfort, were compared with two different input devices: a two button mouse and a joystick from a gamepad. Participants also used a desktop to perform the same tasks in order to assess if the participant groups had similar profiles. The results obtained by 45 participants suggest that both input devices have a comparable usability in the used conditions and show that participants generally performed better with the desktop; a discussion of possible causes is presented. © 2014 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DETI-interact: Interaction with large displays in public spaces using the kinect",
        "doc_scopus_id": "84901606379",
        "doc_doi": "10.1007/978-3-319-07788-8_19",
        "doc_eid": "2-s2.0-84901606379",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Attention catching",
            "Digital compass",
            "Electronic device",
            "Large displays",
            "Natural interfaces",
            "Public display",
            "Train stations",
            "Waiting room"
        ],
        "doc_abstract": "The problem of interaction with large displays in public spaces is currently of interest given the large number of displays available in such spaces (as lobbies, train stations, waiting rooms, etc.) that are only showing information with no possibility to interact with the contents. Several works have been developed in order to allow interaction with these displays using technologies such as infrared, Bluetooth, GPRS, digital compasses or touch screens. Some only intend to provide information, while others emphasize on capturing users' attention eventually leading them to some action. This paper describes DETIInteract, a system located in the entrance hall of a University department allowing users to interact with a large display without the need to carry any electronic device since a Kinect is used to capture different user's gestures. In this work, special attention was given to another issue intrinsically linked to the presentation of information on large public displays 'How to call the user's attention?' © 2014 Springer International Publishing Switzerland.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Investigating landfill contamination by visualizing geophysical data",
        "doc_scopus_id": "84898452992",
        "doc_doi": "10.1109/MCG.2014.11",
        "doc_eid": "2-s2.0-84898452992",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "3D Visualization",
            "Geophysical data",
            "Groundwater contamination",
            "Kriging",
            "Statistical anomaly detection",
            "Uncertainty representation"
        ],
        "doc_abstract": "Geophysical experts aimed to establish a method to identify contamination by landfill leakage without chemically analyzing subsoil samples, which is time-consuming and expensive. To that end, researchers developed a software package that let the experts create 3D visualizations of geophysical data acquired around the landfill and apply statistical analysis to detect anomalous values. The data used, electrical resistivity, are typically sparse. So, the application employs kriging to interpolate the data and provide a volumetric representation of the subsoil resistivity. To avoid invalid conclusions, the visualization also represents uncertainty. The application enabled the experts to better understand the phenomenon and to develop and validate their method. Their evaluation of the application indicated that it helped them throughout the method's development and significantly eased their workload. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Student projects involving novel interaction with large displays",
        "doc_scopus_id": "84897413845",
        "doc_doi": "10.1109/MCG.2014.35",
        "doc_eid": "2-s2.0-84897413845",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Computer graphics education",
            "Computer Science Education",
            "DETI-Interact",
            "Kinect",
            "Public display",
            "University of Aveiro",
            "Computer Graphics",
            "Gestures",
            "Humans",
            "Imaging, Three-Dimensional",
            "Information Science",
            "Students",
            "User-Computer Interface"
        ],
        "doc_abstract": "DETI-Interact is an interactive system that offers information relevant to students in the lobby of the University of Aveiro's Department of Electronics, Telecommunications and Informatics (DETI). The project started in 2009 with a master's thesis addressing interaction with public displays through Android smartphones. Since then, it has evolved considerably; it currently allows gesture interaction based on a Kinect sensor. Meanwhile, it has involved third-year students, master's students, and undergraduate students participating in a research initiation program. © 1981-2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extending the H-tree layout pedigree: An evaluation",
        "doc_scopus_id": "84893273871",
        "doc_doi": "10.1109/IV.2013.56",
        "doc_eid": "2-s2.0-84893273871",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Evaluation",
            "Family structure",
            "Genealogy",
            "H-tree layout",
            "Information visualization",
            "Space-filling",
            "User experience",
            "User performance"
        ],
        "doc_abstract": "Visualizing large family structures is becoming increasingly important, as more genealogical data becomes available. A space-filling h-tree layout pedigree has been recently proposed to make better use of the available space than traditional representations. In a previous paper we applauded the technique's usage of available space but remarked that it makes generation identification difficult and does not allow navigating to descendants of represented individuals. A set of extensions was proposed to help overcome these limitations and a preliminary evaluation suggested that those extensions enhance the original technique. This paper presents a more thorough evaluation carried out to assess if and how the proposed extensions improve the original h-tree layout pedigree technique. Results suggest that these extensions improve user performance on some tasks, effectively provide new functionality, and generally enhance user experience. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new approach for 3D craniometric measurements using 3d skull models",
        "doc_scopus_id": "84893271092",
        "doc_doi": "10.1109/IV.2013.61",
        "doc_eid": "2-s2.0-84893271092",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "3D manipulation",
            "3D models",
            "Acquisition process",
            "Craniometry",
            "Current problems",
            "Final objective",
            "Interactive interfaces",
            "Range acquisition"
        ],
        "doc_abstract": "This work presents an ongoing work on a new approach to perform craniometric analysis based on contactless 3D modelling of skulls. Beside the acquisition process with a 3D range sensor and initial results in the semi-automatic detection of features in the skulls, we also present some results in the development of a 3D interactive interface that eases interaction for users with little experience on digital 3D manipulation. The final objective is to provide an easy to use 3D interface to allow semi-automatic detection of features in skulls. It is our belief that this system might be the first step towards a new methodology for craniometric analysis that can solve several of the current problems such as repeatability, wide access to skull information or bone damage during measurements. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning Auditory Space: Generalization and Long-Term Effects",
        "doc_scopus_id": "84886047011",
        "doc_doi": "10.1371/journal.pone.0077900",
        "doc_eid": "2-s2.0-84886047011",
        "doc_date": "2013-10-22",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Adaptation, Physiological",
            "Adult",
            "Female",
            "Humans",
            "Learning",
            "Male",
            "Middle Aged",
            "Sound Localization",
            "Time Factors"
        ],
        "doc_abstract": "Background:Previous findings have shown that humans can learn to localize with altered auditory space cues. Here we analyze such learning processes and their effects up to one month on both localization accuracy and sound externalization. Subjects were trained and retested, focusing on the effects of stimulus type in learning, stimulus type in localization, stimulus position, previous experience, externalization levels, and time.Method:We trained listeners in azimuth and elevation discrimination in two experiments. Half participated in the azimuth experiment first and half in the elevation first. In each experiment, half were trained in speech sounds and half in white noise. Retests were performed at several time intervals: just after training and one hour, one day, one week and one month later. In a control condition, we tested the effect of systematic retesting over time with post-tests only after training and either one day, one week, or one month later.Results:With training all participants lowered their localization errors. This benefit was still present one month after training. Participants were more accurate in the second training phase, revealing an effect of previous experience on a different task. Training with white noise led to better results than training with speech sounds. Moreover, the training benefit generalized to untrained stimulus-position pairs. Throughout the post-tests externalization levels increased. In the control condition the long-term localization improvement was not lower without additional contact with the trained sounds, but externalization levels were lower.Conclusion:Our findings suggest that humans adapt easily to altered auditory space cues and that such adaptation spreads to untrained positions and sound types. We propose that such learning depends on all available cues, but each cue type might be learned and retrieved differently. The process of localization learning is global, not limited to stimulus-position pairs, and it differs from externalization processes. © 2013 Mendonça et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Reflection orders and auditory distance",
        "doc_scopus_id": "84878989549",
        "doc_doi": "10.1121/1.4800186",
        "doc_eid": "2-s2.0-84878989549",
        "doc_date": "2013-06-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Auditory distances",
            "Distant source",
            "Psychophysical",
            "Sound pressures",
            "Sound source",
            "Sound spectrum"
        ],
        "doc_abstract": "The perception of sound distance has been sparsely studied so far. It is assumed to depend on familiar loudness, reverberation, sound spectrum and parallax, but most of these factors have never been carefully addressed. Reverberation has been mostly analysed in terms of ratio between direct and indirect sound, and total duration. Here we were interested in assessing the impact of each reflection order on distance localization. We compared sound source discrimination at an intermediate and at a distant location with direct sound only, one, two, three, and four reflection orders in a 2AFC task. At the intermediate distances, normalized psychophysical curves reveal no differentiation between direct sound and up to three reflection orders, but sounds with four reflection orders have significantly lower thresholds. For the distant sources, sounds with four reflection orders yielded the best discrimination slopes, but there was also a clear benefit for sounds with three reflection orders. We conclude that at least three reflection orders are required so that reflection-related cues are accounted for in distance estimates. Also, these cues might interact differently with the direct sound pressure cues at different distances. © 2013 Acoustical Society of America.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real-time dynamic image-source implementation for auralisation",
        "doc_scopus_id": "84901780958",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84901780958",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            },
            {
                "area_name": "Music",
                "area_abbreviation": "ARTS",
                "area_code": "1210"
            }
        ],
        "doc_keywords": [
            "Computational performance",
            "Dynamic calculations",
            "Head related transfer function",
            "Image-source method",
            "Interactive virtual reality",
            "Propagation direction",
            "Real-time application",
            "Visual representations"
        ],
        "doc_abstract": "This paper describes a software package for auralisation in interactive virtual reality environments. Its purpose is to reproduce, in real time, the 3D soundfield within a virtual room where listener and sound sources can be moved freely. Output sound is presented binaurally using headphones. Auralisation is based on geometric acoustic models combined with head-related transfer functions (HRTFs): the direct sound and reflections from each source are computed dynamically by the image-source method. Directional cues are obtained by filtering these incoming sounds by the HRTFs corresponding to their propagation directions relative to the listener, computed on the basis of the information provided by a head-tracking device. Two interactive real-time applications were developed to demonstrate the operation of this software package. Both provide a visual representation of listener (position and head orientation) and sources (including image sources). One focusses on the auralisation-visualisation synchrony and the other on the dynamic calculation of reflection paths. Computational performance results of the auralisation system are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the adaptation to non-individualised HRTF auralisations: A longitudinal study",
        "doc_scopus_id": "84883364337",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84883364337",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Acoustic spaces",
            "Active Learning",
            "Localization errors",
            "Long-term effects",
            "Longitudinal study",
            "Virtual sound"
        ],
        "doc_abstract": "Auralisations with HRTFs are an innovative tool for the reproduction of acoustic space. Their broad applicability depends on the use of non-individualised models, but little is known on how humans adapt to these sounds. Previous findings have shown that simple exposure to non-individualised virtual sounds did not provide a quick adaptation, but that training and feedback would boost this process. Here, we were interested in analyzing the long-term effect of such training-based adaptation. We trained listeners in azimuth and elevation discrimination in two separate experiments and retested them immediately, one hour, one day, one week and one month after. Results revealed that, with active learning and feedback, all participants lowered their localization errors. This benefit was still found one month after training. Interestingly, participants who had trained previously with elevations were better in azimuth localization and vice-versa. Our findings suggest that humans adapt easily to new anatomically shaped spectral cues and they are able to transfer that adaptation to non-trained sounds. Copyright© (2012) by the Audio Engineering Society.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real-time auralisation system for virtual microphone positioning",
        "doc_scopus_id": "84872691863",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84872691863",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Audio processing",
            "Audio-output",
            "Output data",
            "Overlap-add method",
            "Processing time",
            "Regular grids",
            "Sound source",
            "Virtual microphone"
        ],
        "doc_abstract": "A computer application was developed to simulate the process of microphone positioning in sound recording applications. A dense, regular grid of impulse responses pre-recorded on the region of the room under study allowed the sound captured by a virtual microphone to be auralised through real-time convolution with an anechoic stream representing the sound source. Convolution was performed using a block-based variation on the overlap-add method where the summation of many small sub-convolutions produced each block of output data samples. As the applied RIR filter varied on successive audio output blocks, a short cross fade was applied to avoid glitches in the audio. The maximum possible length of impulse response applied was governed by the size of audio processing block (hence latency) employed by the program. Larger blocks allowed a lower processing time per sample. At 23.2ms latency (1024 samples at 44.1kHz), it was possible to apply 9 second impulse responses on a standard laptop computer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Implementation and evaluation of an enhanced H-tree layout pedigree visualization",
        "doc_scopus_id": "84867919811",
        "doc_doi": "10.1109/IV.2012.15",
        "doc_eid": "2-s2.0-84867919811",
        "doc_date": "2012-10-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Genealogical information",
            "Genealogy",
            "H-trees",
            "Information visualization",
            "pedigree",
            "Pedigree data",
            "Visualization method",
            "Visualization technique"
        ],
        "doc_abstract": "The constant growth of available genealogical information has encouraged the research of visualization techniques capable of representing the corresponding large amount of data. An H-Tree Layout has been recently proposed to represent pedigree data as a way to overcome some of the limitations of traditional representations. However, this new method has its own limitations which may hinder its adoption. In this paper, we propose some enhancements to the H-Tree Layout pedigree visualization method in order to overcome some of the identified limitations. An implementation of the proposed enhancements and results of a preliminary evaluation are also provided. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D visualization of geophysical resistivity data to delineate contamination anomalies in a landfill",
        "doc_scopus_id": "84867907163",
        "doc_doi": "10.1109/IV.2012.38",
        "doc_eid": "2-s2.0-84867907163",
        "doc_date": "2012-10-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "3D Visualization",
            "Contaminated areas",
            "Data sets",
            "Data type",
            "Geophysical data",
            "Geophysical resistivity",
            "Geostatistical",
            "Kriging",
            "Spatial visualization",
            "Specific areas",
            "Statistical anomaly detection",
            "Uncertainty representation",
            "Volumetric representation",
            "Volumetric visualization"
        ],
        "doc_abstract": "Geophysical data represent subsoil structure in a specific area and can be used to extract subsoil information for various purposes. In this work we used this data type to detect anomalies/contamination in the subsoil. Our case study was based on data acquired around a landfill and the main objective is identifying contaminated areas as a result of leakage in landfill. This involves the application of statistical methods to detect anomalous values taking into account the whole data set, subdividing it in sublevels in relation to the surface, instead of using a single threshold (as usual). This work combines in the same software package the anomaly statistical analysis and several 3D representations of the results to validate and also helps understanding the final results of the analysis. Given that the original data used in the analysis, resistivity sections, is normally very sparse, a kriging geostatistical process was used to interpolate data in order to provide a volumetric representation of the subsoil in the area, providing a continuous spatial visualization. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the improvement of localization accuracy with non-individualized HRTF-based sounds",
        "doc_scopus_id": "84870349596",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84870349596",
        "doc_date": "2012-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Music",
                "area_abbreviation": "ARTS",
                "area_code": "1210"
            }
        ],
        "doc_keywords": [
            "Active Learning",
            "Auralizations",
            "Head related transfer function",
            "Localization accuracy",
            "Sound perception",
            "Sound source localization",
            "Virtual sound",
            "Virtual-reality environment"
        ],
        "doc_abstract": "Auralization is a powerful tool to increase the realism and sense of immersion in Virtual Reality environments. The Head Related Transfer Function (HRTF) filters commonly used for auralization are non-individualized, as obtaining individualized HRTFs poses very serious practical difficulties. It is therefore extremely important to understand to what extent this hinders sound perception. In this paper we address this issue from a learning perspective. In a set of experiments, we observed that mere exposure to virtual sounds processed with generic HRTF did not improve the subjects' performance in sound source localization, but short training periods involving active learning and feedback led to significantly better results. We propose that using auralization with non-individualized HRTF should always be preceded by a learning period.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D-2D laser range finder calibration using a conic based geometry shape",
        "doc_scopus_id": "84864114823",
        "doc_doi": "10.1007/978-3-642-31295-3_37",
        "doc_eid": "2-s2.0-84864114823",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "2D laser scanners",
            "2d lasers",
            "3D point cloud",
            "Calibration method",
            "Inertial sensor",
            "Laser calibration",
            "Laser range finders",
            "Object geometries",
            "Sensor data",
            "Sensor fusion",
            "Stereo cameras"
        ],
        "doc_abstract": "The AtlasCar is a prototype that is being developed at the University of Aveiro to research advanced driver assistance systems. The car is equipped with several sensors: 3D and 2D laser scanners, a stereo camera, inertial sensors and GPS. The combination of all these sensor data in useful representations is essential. Therefore, calibration is one of the first problems to tackle. This paper focuses on 3D/2D laser calibration. The proposed method uses a 3D Laser Range Finder (LRF) to produce a reference 3D point cloud containing a known calibration object. Manual input from the user and knowledge of the object geometry are used to register the 3D point cloud with the 2D Lasers. Experimental results with simulated and real data demonstrate the effectiveness of the proposed calibration method. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Results from geospatial analysis of resistivity to delineate contamination anomalies: A case study of a Controlled Dump - North Portugal",
        "doc_scopus_id": "84867492970",
        "doc_doi": "10.4133/1.4721868",
        "doc_eid": "2-s2.0-84867492970",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Geophysics",
                "area_abbreviation": "EART",
                "area_code": "1908"
            },
            {
                "area_name": "Geotechnical Engineering and Engineering Geology",
                "area_abbreviation": "EART",
                "area_code": "1909"
            },
            {
                "area_name": "Environmental Engineering",
                "area_abbreviation": "ENVI",
                "area_code": "2305"
            }
        ],
        "doc_keywords": [
            "Contaminant dispersion",
            "Differential weathering",
            "Environmental consequences",
            "Geo-spatial analysis",
            "Geochemical anomaly",
            "Geostatistical method",
            "Municipal solid waste (MSW)",
            "Software applications"
        ],
        "doc_abstract": "The assessment of contaminant dispersion in Controlled Dumps (CD) of Municipal Solid Waste (MSW) is possible through the combination of geophysical, geochemical and statistical methods. The methodology applied in this study will contribute to evaluate the environmental consequences of the Matosinhos CD (N Portugal), which is set in a granitic crystalline geological context with a permeability controlled by differential weathering which, in turn, is associated with fracturing. The statistical methods that we will describe are usually used in geochemical anomaly determination and were adapted to be used with resistivity data obtained from twenty-two 2D profiles performed around the CD. The data set was inverted and the results were processed and visualized by a 3D software application that we are currently developing. Groundwater samples were also collected in piezometers, upstream and downstream of the CD, with the aim of directly confirming the presence of contaminants indirectly detected by the geophysical and geostatistical methods described before. The combination of these different approaches allows, in our opinion, an improved approach towards the detection and delineation of contaminant plumes from these deposits.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DETI-interact: Interacting with public displays through mobile phones",
        "doc_scopus_id": "80052438203",
        "doc_doi": null,
        "doc_eid": "2-s2.0-80052438203",
        "doc_date": "2011-09-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Human-computer",
            "Informatics",
            "Large screen",
            "Large screen displays",
            "Public display",
            "Public Screens",
            "Usability"
        ],
        "doc_abstract": "This work describes the necessary steps in the development of DETI interact: an information system for the exhibition of informative content from the Department of Electronics, Telecommunications, and Informatics of the University of Aveiro. This system enables users to interact with the contents displayed in a large screen in the lobby of the department through an Android mobile device. © 2011 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating user studies into computer graphics-related courses",
        "doc_scopus_id": "80052317465",
        "doc_doi": "10.1109/MCG.2011.78",
        "doc_eid": "2-s2.0-80052317465",
        "doc_date": "2011-09-07",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "computer graphics education",
            "graphics and multimedia",
            "Human-computer",
            "Information visualization",
            "Student collaboration",
            "User study"
        ],
        "doc_abstract": "The authors argue in favor of introducing user studies into computer graphics, human-computer interaction, and information visualization courses. They discuss two sets of user studies they developed and performed over several years, with student collaboration, and the different aspects of the studies they had to consider. They also discuss a user study they designed for an information visualization course. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the improvement of auditory accuracy with non-individualized HRTF-based sounds",
        "doc_scopus_id": "84866042620",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84866042620",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Active Learning",
            "Auralizations",
            "Head related transfer function",
            "Sound perception",
            "Sound source localization",
            "Virtual sound",
            "Virtual-reality environment"
        ],
        "doc_abstract": "Auralization is a powerful tool to increase the realism and sense of immersion in Virtual Reality environments. The Head Related Transfer Function (HRTF) filters commonly used for auralization are non-individualized, as obtaining individualized HRTFs poses very serious practical difficulties. It is therefore extremely important to understand to what extent this hinders sound perception. In this paper, we address this issue from a learning perspective. In a set of experiments, we observed that mere exposure to virtual sounds processed with generic HRTF did not improve the subjects' performance in sound source localization, but short training periods involving active learning and feedback led to significantly better results. We propose that using auralization with non-individualized HRTF should always be preceded by a learning period.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Finite difference room acoustic modelling on a general purpose graphics processing unit",
        "doc_scopus_id": "84866018700",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84866018700",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Acoustics and Ultrasonics",
                "area_abbreviation": "PHYS",
                "area_code": "3102"
            }
        ],
        "doc_keywords": [
            "Auralizations",
            "Computational savings",
            "Data sets",
            "Finite difference",
            "General purpose",
            "Graphics Processing Unit",
            "Listening positions",
            "Parallelisation",
            "Processing capability",
            "Processing resources",
            "Processing time",
            "Room acoustics",
            "Room impulse response",
            "Virtual rooms",
            "Walkthroughs"
        ],
        "doc_abstract": "Detailed and convincing walkthrough auralizations of virtual rooms requires much processing capability. One method of reducing this requirement is to pre-calculate a data-set of room impulse responses (RIR) at locations throughout the space. Processing resources may then focus on RIR interpolation and convolution using the dataset as the virtual listening position changes in real-time. Recent work identified the suitability of wave-based models over traditional ray-based approaches for walkthrough auralization. Despite the computational saving of wave-based methods to generate the RIR dataset, processing times are still long. This paper presents a wave-based implementation for execution on a general purpose graphics processing unit. Results validate the approach and show that parallelisation provides a notable acceleration.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring new ways of integration, visualization and interaction with geotechnical and geophysical data",
        "doc_scopus_id": "78449287527",
        "doc_doi": "10.1109/IV.2010.35",
        "doc_eid": "2-s2.0-78449287527",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Data type",
            "Geological structures",
            "Geophysical data",
            "Geotechnical",
            "Visualization technique",
            "VTK (Visualization Toolkit)"
        ],
        "doc_abstract": "The work presented in this paper aims at exploring new ways of integrating, visualizing and interacting with geotechnical and geophysical data that may be more rich and interactive than those offered by most current Geographic Information Systems (GIS). Some visualization techniques enabling simultaneous visualization of the several data types available in our case study are proposed. Moreover, methods were developed to guide experts while defining layers and other relevant geological structures. The work is still in an early stage and is main goal has been assessing the validity and adequacy of the proposed techniques to the specific geotechnical and geophysical data under consideration. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Wiimote as an input device in google earth visualization and navigation: A user study comparing two alternatives",
        "doc_scopus_id": "78449283947",
        "doc_doi": "10.1109/IV.2010.72",
        "doc_eid": "2-s2.0-78449283947",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Google earth",
            "Input devices",
            "Usability",
            "User study",
            "Wiimote"
        ],
        "doc_abstract": "This paper presents a user study performed to compare the usability of the Wiimote as an input device to visualize information and navigate in Google Earth using two different configurations. This study had the collaboration of 15 participants which performed a set of tasks using the Wiimote as an input device while the image was projected on a common projection screen, as well as a mouse on a desktop. Results show that most users clearly preferred one of the Wiimote configurations over the other, and over the mouse; moreover, they had better performances using the preferred configuration, and found it easier to use. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information visualization in facility location and vehicle routing decisions",
        "doc_scopus_id": "78449271134",
        "doc_doi": "10.1109/IV.2010.25",
        "doc_eid": "2-s2.0-78449271134",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Decision support tools",
            "Facility location",
            "Facility locations",
            "General publics",
            "Information visualization",
            "Location-routing",
            "Routing decisions",
            "Usability evaluation methods"
        ],
        "doc_abstract": "Facility location and vehicle routing are amongst the most important logistic decisions in today's organizations. These aspects are intertwined and, in some cases, should be addressed in an integrated way (giving rise to the location-routing approach). A decision support tool that can make easier the visualization (and editing) of information regarding these problems is becoming increasingly important as: it enables to further understand the problem at hand; and, at the same time, it fosters better communication of the decisions in a way easier to understand by the general public. This paper presents some concepts for information visualization on the problems arisen by the aforementioned decisions, which have been incorporated in a decision support tool and tested using usability evaluation methods. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A framework for cerebral CT perfusion imaging methods comparison",
        "doc_scopus_id": "77955401015",
        "doc_doi": "10.1007/978-3-642-13775-4_15",
        "doc_eid": "2-s2.0-77955401015",
        "doc_date": "2010-08-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Blood flow",
            "Blood volumes",
            "Brain imaging",
            "Computed Tomography",
            "Transit time"
        ],
        "doc_abstract": "Stroke is among the most frequent cause of death around the world and the decision to treat and final outcome is highly dependent on the quality of diagnosis. Recently, cerebral perfusion tomography have been used with promising results in the stroke evaluation mainly because this technique gives further information about the hemodynamic changes within the stroke area. However many different parameters are actually used to analyze the CT perfusion results, trying to integrate the temporal information it contains. Some of these parameters are Blood Volume, Blood Flow or Transit Time for example. This paper reviews the most relevant methods used to calculate perfusion related parameters and describes our framework that defines a reproducible processing pipeline that supports visual and quantified comparison between them. © 2010 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Head-mounted display versus desktop for 3D navigation in virtual reality: A user study",
        "doc_scopus_id": "58049208308",
        "doc_doi": "10.1007/s11042-008-0223-2",
        "doc_eid": "2-s2.0-58049208308",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "3d navigations",
            "Application areas",
            "Design and developments",
            "Game scenarios",
            "Head mounted display (HMD)",
            "Navigation tasks",
            "Set-ups",
            "Short periods",
            "Usability evaluations",
            "User evaluations",
            "User performances",
            "User study",
            "Virtual environments (VE)",
            "Vr systems"
        ],
        "doc_abstract": "Virtual Reality (VR) has been constantly evolving since its early days, and is now a fundamental technology in different application areas. User evaluation is a crucial step in the design and development of VR systems that do respond to users' needs, as well as for identifying applications that indeed gain from the use of such technology. Yet, there is not much work reported concerning usability evaluation and validation of VR systems, when compared with the traditional desktop setup. The paper presents a user study performed, as a first step, for the evaluation of a low-cost VR system using a Head-Mounted Display (HMD). That system was compared to a traditional desktop setup through an experiment that assessed user performance, when carrying out navigation tasks in a game scenario for a short period. The results show that, although users were generally satisfied with the VR system, and found the HMD interaction intuitive and natural, most performed better with the desktop setup. © 2008 Springer Science+Business Media, LLC.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Miaware software 3D medical image analysis with automated reporting engine and ontology-based search",
        "doc_scopus_id": "57549088553",
        "doc_doi": null,
        "doc_eid": "2-s2.0-57549088553",
        "doc_date": "2008-12-17",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "3D medical images",
            "Automatically generated",
            "Computed axial tomography",
            "Image stacks",
            "Image visualization",
            "Intelligent search engines",
            "Logical structures",
            "Ontology-based",
            "Pathological changes",
            "Radiological report",
            "Search Algorithms",
            "Teaching tools"
        ],
        "doc_abstract": "This article presents MIAWARE, a software for Medical Image Analysis With Automated Reporting Engine, which was designed and developed for doctor/radiologist assistance. It allows to analyze an image stack from computed axial tomography scan of lungs (thorax) and, at the same time, to mark all pathologies on images and report their characteristics. The reporting process is normalized - radiologists cannot describe pathological changes with their own words, but can only use some terms from a specific vocabulary set provided by the software. Consequently, a normalized radiological report is automatically generated. Furthermore, MIAWARE software is accompanied with an intelligent search engine for medical reports, based on the relations between parts of the lungs. A logical structure of the lungs is introduced to the search algorithm through the specially developed ontolgy. As a result, a deductive report search was obtained, which may be helpful for doctors while diagnosing patients' cases. Finally, the MIAWARE software can be considered also as a teaching tool for future radiologists and physicians.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D reconstruction and Auralisation of the \"painted dolmen\" of antelas",
        "doc_scopus_id": "47949110073",
        "doc_doi": "10.1117/12.766607",
        "doc_eid": "2-s2.0-47949110073",
        "doc_date": "2008-07-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D acquisition",
            "3D geometries",
            "3D reconstructions",
            "3d visual models",
            "Acoustic absorption coefficients",
            "Archaeological sites",
            "Audio sources",
            "Audio visuals",
            "Augmented reality",
            "Auralisation",
            "Dark rooms",
            "Geometric acoustics",
            "Geometric models",
            "Head Related Transfer Functions",
            "In-situ",
            "Irregular surfaces",
            "Iterative algorithms",
            "Iterative Closest points",
            "Laser range finder",
            "Orientation sensors",
            "Reconstruction softwares",
            "Reverberation times",
            "Software computes",
            "Sound waves",
            "Source localisation",
            "Stereo headphones",
            "Uniform grids",
            "Visual scenes",
            "Visualization toolkits"
        ],
        "doc_abstract": "This paper presents preliminary results on the development of a 3D audiovisual model of the Anta Pintada (painted dolmen) of Antelas, a Neolithic chamber tomb located in Oliveira de Frades and listed as Portuguese national monument. The final aim of the project is to create a highly accurate Virtual Reality (VR) model of this unique archaeological site, capable of providing not only visual but also acoustic immersion based on its actual geometry and physical properties. The project started in May 2006 with in situ data acquisition. The 3D geometry of the chamber was captured using a Laser Range Finder. In order to combine the different scans into a complete 3D visual model, reconstruction software based on the Iterative Closest Point (ICP) algorithm was developed using the Visualization Toolkit (VTK). This software computes the boundaries of the room on a 3D uniform grid and populates its interior with \"free-space nodes\", through an iterative algorithm operating like a torchlight illuminating a dark room. The envelope of the resulting set of \"free-space nodes\" is used to generate a 3D iso-surface approximating the interior shape of the chamber. Each polygon of this surface is then assigned the acoustic absorption coefficient of the corresponding boundary material. A 3D audiovisual model operating in real-time was developed for a VR Environment comprising head-mounted display (HMD) I-glasses SVGAPro, an orientation sensor (tracker) InterTrax 2 with 3 Degrees Of Freedom (3DOF) and stereo headphones. The auralisation software is based on a geometric model. This constitutes a first approach, since geometric acoustics have well-known limitations in rooms with irregular surfaces. The immediate advantage lies in their inherent computational efficiency, which allows real-time operation. The program computes the early reflections forming the initial part of the chamber's impulse response (IR), which carry the most significant cues for source localisation. These early reflections are processed through Head Related Transfer Functions (HRTF) updated in real-time according to the orientation of the user's head, so that sound waves appear to come from the correct location in space, in agreement with the visual scene. The late-reverberation tail of the IR is generated by an algorithm designed to match the reverberation time of the chamber, calculated from the actual acoustic absorption coefficients of its surfaces. The sound output to the headphones is obtained by convolving the IR with anechoic recordings of the virtual audio source. © 2008 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Camera location and aperture characterization using the transformation between a 2D plane and the image captured by the camera",
        "doc_scopus_id": "47749098979",
        "doc_doi": "10.1007/978-3-540-69812-8_38",
        "doc_eid": "2-s2.0-47749098979",
        "doc_date": "2008-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "2 D space",
            "3-D modeling",
            "3D spaces",
            "Basis vectors",
            "Camera locations",
            "Final objective",
            "Heidelberg (CO)",
            "Homography",
            "Horizontal planes",
            "International conferences",
            "Orthogonal projections",
            "Reference systems",
            "Transformation matrices"
        ],
        "doc_abstract": "This paper uses as starting point the transformation matrix defined in the homogeneous space that associates the points of a 2D plane (that represents the model) with those of another 2D space (the image one), this transformation characterizing the camera capture process. This transformation (an homography from 2D to 2D) is coming from previous work and is used within the scope of the SimulFoot project. The final objective is to reconstruct a 3D model from TV soccer scenes, making it important to characterize the transformation between a 2D plane (the soccer field) and the camera image. We suppose the transformation (from image to field) is a conic projection whose center is S and projection plane is P in the model 3D space. We formulate two additional hypotheses related to the reference system of P: its origin is the orthogonal projection of S on P, and its first basis vector is parallel to the horizontal plane xOy. In fact, these conditions are often verified in soccer scenes since the camera is fixes on a tripod. In this communication, we give the camera location and aperture expressions on the only basis of the transformation matrix values. © 2008 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Usability Evaluation in Virtual Reality: A User Study Comparing Three Different Setups",
        "doc_scopus_id": "85119835829",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85119835829",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Head-mounted-displays",
            "Low-costs",
            "Performance",
            "Usability evaluation",
            "User performance",
            "User study",
            "VR systems"
        ],
        "doc_abstract": "© The Eurographics Association 2008.We describe a user study comparing a low cost VR system using a Head-Mounted-Display (HMD) to a desktop and another setup where the image is projected on a screen. Eighteen participants played the same game in the three platforms. Results show that users generally did not like the setup using a screen and the best performances were obtained with the desktop configuration. This result could be due to the fact that most users were gamers used to the interaction through keyboard/mouse. Still, we noticed that user performance in the HMD setup was not dramatically worse and that users do not collide as often with walls.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Teaching 3D modelling and visualization using VTK",
        "doc_scopus_id": "45349097503",
        "doc_doi": "10.1016/j.cag.2008.01.005",
        "doc_eid": "2-s2.0-45349097503",
        "doc_date": "2008-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "3D modelling",
            "Computer engineering",
            "Computer graphics education",
            "Elective course",
            "Visualization application",
            "Visualization toolkits"
        ],
        "doc_abstract": "In the last two years, we have been using the Visualization Toolkit (VTK) as a tool for teaching \"3D Modelling and Visualization\", an elective course offered to Computer Engineering students. Students start by using OpenGL and, afterwards, use VTK in half of their lab classes, in order to accomplish some tasks and acquire knowledge on its features and functionalities. They are also required to develop a visualization application based on VTK. We first present the motivation for using VTK and the main features of the \"3D Modelling and Visualization\" course. Afterwards, we describe some of the most successful projects developed by our students. Then, we globally analyse the effectiveness of using VTK, and present the results of a questionnaire handed out to the students who attended the course in the last semester. © 2008 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2008-01-26 2008-01-26 2011-01-20T07:10:55 S0097-8493(08)00008-3 S0097849308000083 10.1016/j.cag.2008.01.005 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20080601 20080630 2008 2008-01-26T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst pubtype ref alllist content subj ssids 0097-8493 00978493 32 32 3 3 Volume 32, Issue 3 8 363 370 363 370 200806 June 2008 2008-06-01 2008-06-30 2008 Education article fla Copyright © 2008 Elsevier Ltd. All rights reserved. TEACHING3DMODELLINGVISUALIZATIONUSINGVTK DIAS P 1 Introduction 2 3D modelling and visualization 3 Examples of visualization projects 3.1 Visualization of brain data from different modalities 3.1.1 Volumetric MRI and SPECT data 3.1.2 EEG data 3.1.3 Sources of electrical activity (dipoles) 3.2 Visualization of water pressure and velocity around a ship's hull 3.3 Visualization of acoustic data 3.3.1 2D sound wave propagation 3.3.2 3D acoustic pressure 3.4 Visualization of Oporto underground tunnels 3.5 Visualization of a football game in a VR environment 4 Using VTK: an assessment 4.1 Evaluation of the OpenGL and VTK projects 4.2 Questionnaire handed out in 2006/2007 4.3 Our own impression 5 Conclusion Acknowledgements References HITCHNER 2000 283 288 L BOUVIER 2002 603 608 D PAQUETTE 2005 245 255 E ANGEL 2006 E INTERACTIVECOMPUTERGRAPHICSATOPDOWNAPPROACHUSINGOPENGL ZARA 2006 105 112 J BAILEY 2007 524 531 M FOLEY 1994 J INTRODUCTIONCOMPUTERGRAPHICS HEARN 2004 D COMPUTERGRAPHICSOPENGL CUNNINGHAM 2006 S COMPUTERGRAPHICSPROGRAMMINGINOPENGLFORVISUALCOMMUNICATION FROST 2004 J LEARNSVGWEBGRAPHICSSTANDARD AMES 1997 A VRML20SOURCEBOOK SHREINER 2005 D OPENGLPROGRAMMINGGUIDE SELMAN 2002 D JAVA3DPROGRAMMING LUNA 2003 F INTRODUCTION3DGAMEPROGRAMMINGDIRECTX90 2003 VTKUSERSGUIDEVERSION42 DIASX2008X363 DIASX2008X363X370 DIASX2008X363XP DIASX2008X363X370XP item S0097-8493(08)00008-3 S0097849308000083 10.1016/j.cag.2008.01.005 271576 2011-02-04T12:58:14.641468-05:00 2008-06-01 2008-06-30 true 1511805 MAIN 8 69595 849 656 IMAGE-WEB-PDF 1 si2 246 13 43 si1 244 13 43 gr1 46996 263 561 gr1 4317 59 125 gr1 272162 699 1490 gr2 17776 207 278 gr2 4533 93 125 gr2 110247 918 1233 gr3 32111 213 486 gr3 3509 55 125 gr3 328726 943 2154 gr4 41541 202 278 gr4 7373 91 125 gr4 747753 895 1233 gr5 26656 233 278 gr5 3098 93 111 gr5 178390 1033 1233 gr6 42009 253 523 gr6 5266 61 125 gr6 326391 1122 2317 gr7 16123 216 278 gr7 4762 93 120 gr7 178739 960 1233 gr8 30886 190 522 gr8 3905 45 125 gr8 140773 504 1388 CAG 1832 S0097-8493(08)00008-3 10.1016/j.cag.2008.01.005 Elsevier Ltd Fig. 1 MRI and SPECT data with different opacity values (left), and coronal plane with corresponding slice (right). Fig. 2 Mapping EEG data on a model of a patient's head. Fig. 3 Two modes of dipole representation. Fig. 4 Water velocity and pressure around a ship's hull. Fig. 5 Sound wave at a given time ( t = 39 ): visualization of 2D propagation, as well as the zero-valued iso-lines. Fig. 6 Acoustic pressure in a cubical volume at a given time ( t = 29 ): visualization using orthogonal planes (left) and iso-surfaces (right). Fig. 7 Partial representation of the tunnel data using Gaussian splatter; the cutting plane is also visible. Fig. 8 User wearing the HMD (left), and his view of the game (right). Table 1 Evaluation of the OpenGL and VTK projects in 2005/2006 and 2006/2007 2005/2006 2006/2007 OpenGL VTK OpenGL VTK Weak 4 4 2 3 Average 4 4 6 7 Good 4 – 6 4 Very good 6 10 4 4 Number of students 18 18 18 18 Table 2 Analysis of the evaluation results of the OpenGL and VTK projects 2005/2006 2006/2007 Weak evaluation in both OpenGL and VTK 4 2 Average evaluation in both OpenGL and VTK 4 4 Significantly worse evaluation in VTK than in OpenGL – 6 Significantly better evaluation in VTK than in OpenGL 4 2 Excellent evaluation in both OpenGL and VTK 6 4 Number of students 18 18 Education Teaching 3D modelling and visualization using VTK Paulo Dias Joaquim Madeira ⁎ Beatriz Sousa Santos Department of Electronics, Telecommunications and Informatics/IEETA, University of Aveiro, Campus Universitário de Santiago, P-3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351234370504; fax: +351234381128. In the last two years, we have been using the Visualization Toolkit (VTK) as a tool for teaching “3D Modelling and Visualization”, an elective course offered to Computer Engineering students. Students start by using OpenGL and, afterwards, use VTK in half of their lab classes, in order to accomplish some tasks and acquire knowledge on its features and functionalities. They are also required to develop a visualization application based on VTK. We first present the motivation for using VTK and the main features of the “3D Modelling and Visualization” course. Afterwards, we describe some of the most successful projects developed by our students. Then, we globally analyse the effectiveness of using VTK, and present the results of a questionnaire handed out to the students who attended the course in the last semester. Keywords Computer graphics education Visualization Toolkit VTK 1 Introduction In the last few years we have been witnessing a discussion on how to better teach Computer Graphics (CG) to students in different areas [1–7]. In the past, a bottom-up approach was normally used, where students had to build all necessary code (almost) from scratch. Later, many educators switched to a top-down approach, based on using a higher-level API such as OpenGL [8] or Java 3D [9], with less relevance being given to raster-level algorithms. Others developed CG courses using VRML and with some emphasis on virtual reality (VR) [10]. More recently, courses designed to take advantage of available GPUs and shading languages have also started to be offered [11,12]. Although classical CG textbooks, based on the traditional bottom-up approach (e.g., [13]), do remain useful, advances in hardware, graphics libraries and more recent API-based CG textbooks [8,14–16] offer both students and educators the possibility of exploring advanced concepts and developing useful course projects, e.g., for data visualization. Two years ago, when planning our courses in the CG area for the first semester of 2005/2006, and given the interest shown by prospective students, we decided to offer: • “3D Modelling and Visualization” (3DMV), an elective course for students already having some CG background, presenting more advanced concepts, as well as offering the possibility of working with de facto CG and scientific visualization standard libraries. • Specialization courses in CG, visualization and geometric modelling, for M.Sc. students, which had to be accompanied by an integrated “CG Laboratory” course of 4h per week, providing them with additional hands-on experience. Given the audience, M.Sc. students that are supposed to be more independent than graduation students, those lab classes were organized as to introduce a series of CG tools and libraries, progressing from the SVG [17] and VRML [18] languages to OpenGL [19], and then to VTK [16]. Since we consider the top-down approach as more adequate for our students, we decided to combine OpenGL with the well-known Visualization Toolkit (VTK). Given that we would be teaching students already possessing some basic CG knowledge and, also important, having some object-oriented programming experience, we assigned part of the lab classes to VTK and required students to develop a visualization application based on that toolkit, as final assignment. In this way, they would have to use a higher-level API, in addition to the more traditional OpenGL, and would have to acquire some knowledge on data visualization, one of the most important application areas of CG. OpenGL [19] is the current de facto CG educational standard: it is open-source, with a low-level entry barrier and appropriate documentation, as well as offering multi-platform support. VTK [16] is also an open-source, freely available toolkit not only for 3D CG, but also for image processing and data visualization. It offers a higher-level of abstraction than other rendering libraries, such as OpenGL, making it much easier for the knowledgeable user to create graphics and visualization applications. In addition, it also offers a wide variety of data visualization algorithms (including scalar, vector, tensor and volumetric methods), as well as advanced modelling techniques (e.g., implicit modelling, polygon reduction, mesh smoothing and contouring). Other CG tools were also considered as possible choices: in spite of their relative ease of coding, JOGL [20], the Java wrapper library for OpenGL, and Java 3D [21], the scene graph-based 3D API for Java, were ruled out since our students have no Java background; Microsoft's Direct3D, part of the DirectX API [22], has a higher-level entry barrier and does not offer multi-platform support. We also did not want to limit our students to the use of a modelling language such as VRML or X3D; instead we deemed important for them to design and develop software applications. The first results of using VTK as a tool for teaching and applying CG, during the first semester of 2005/2006, were presented earlier at the EUROGRAPHICS 2006 Education Programme [23]. In this paper we report on the experience of teaching the 3DMV course during the last two years. In what follows, we first present the main features of the 3DMV course, and then describe some of the most successful projects developed by our students. Afterwards, we globally analyse the effectiveness of using VTK, and present the results of the questionnaire handed out to the students who attended the course in the last semester. Finally, we present some conclusions. 2 3D modelling and visualization The 3DMV course was introduced in the first semester of 2005/2006 as a fifth year elective for Computer Engineering students, corresponding to 2h of lectures and 2h of lab classes per week. The 3DMV course was offered as an answer to the strong interest shown by prospective students; moreover, we believe that students benefit from additional exposure to advanced topics in the CG area. Also, medical imaging and data visualization are long-established research areas in our department, with relevance in many graduation and post-graduation projects and/or R&D activities. Students attending the 3DMV course mostly have an introductory background on the fundamental CG concepts and some experience using VRML, obtained in their third year Human–Computer Interaction course, but usually have no experience in using any CG API. Therefore, the main topics addressed throughout the course are: (1) Review of CG fundamentals. (2) Introduction to OpenGL (lab). (3) Geometric modelling (polygonal meshes and free-form curves and surfaces). (4) Techniques conducing to higher realism (ray-tracing, radiosity, textures). (5) Introduction to volume visualization (surface extraction and direct volume rendering). (6) Introduction to VTK (lab). Regarding lab classes, the first half of the semester is dedicated to OpenGL and the second half to VTK. OpenGL is used to illustrate CG and geometric modelling concepts addressed during the lectures, and to provide the students with their first hands-on experience using a CG API, which facilitates the later transition to VTK. The fundamentals of VTK are introduced during lab classes and consolidated using a sequence of practical exercises developed for each class. The six lab classes used to introduce VTK encompass the following topics: (1) First examples, interactors, cameras and lighting: Compiler configuration; visualization of a simple cone; using interactors and different interaction techniques; using several cameras and light sources. (2) Actor properties, multiple actors and renderers, transformations, shading and textures: Modifying actor properties (colour, opacity, etc.); managing multiple actors and multiple renderers in the same window; using transformations to change location and orientation; applying different shading techniques and textures. (3) Observers and callbacks, glyphing and picking: Using callbacks and managing events; application examples of glyphing, picking and coordinate visualization. (4) Widgets, implicit functions, contouring and probing: Using widgets; definition and visualization of quadrics using contouring; probing a quadric with a plane and visualizing the resulting iso-lines. (5) Viewing of 2D images, visualization and clipping of polygonal data: Manipulation and viewing of 2D images; importing VRML polygonal data and simple manipulation of the resulting polydata objects. (6) Visualization of non-structured grids and volumetric data: Creation and manipulation of a simple non-structured grid; association of scalar information to the data; visualization and reslicing of medical data. In addition to the work carried out during their lab classes, each group of two students is required to develop a visualization application, corresponding to approximately 20h of after-class work. Most of the projects imply that students have to visualize data from different sources using VTK, and create tools and widgets using the library to provide additional visualization capabilities (such as slicing or probing). Several data sets have been proposed to our students, ranging from electromagnetic radiation data from antennas, to medical data (brain or lung imaging) and data from physical processes (e.g., water flow around a ship's hull or temperature values within an industrial oven). These data sets were obtained through contacts with other university departments, in order to detect data visualization needs. The main idea is to give the students data sets to visualize, which are important to final users, and thus increase their motivation. For some of the applications and data sets, visualization tools already existed, but did not seem to completely satisfy their final users. In fact, our colleagues showed great interest when interviewed on the issue, since they considered the possibility of influencing the design and directly participating in the specification of the application's features as an advantage, instead of being limited to the use of existing commercial application software with limited possibilities. 3 Examples of visualization projects We will now present some of the most successful visualization applications developed by students attending the 3DMV course in 2005/2006 and 2006/2007. The examples described encompass the application areas of medical imaging, physical simulation, acoustic data visualization, reverse engineering and VR. 3.1 Visualization of brain data from different modalities One of the most interesting projects consisted in developing an application to visualize, in an integrated way, data from different brain imaging and signal modalities, namely, magnetic resonance imaging (MRI) and single photon emission computed tomography (SPECT) data, electroencephalogram (EEG) and electrical dipole data. The latter two modalities provide time-varying data sets. The work was divided into three sub-tasks, each one allocated to two students. Given the common platform (VTK), each group implemented the visualization of a different type of data. The entire work was integrated into a single application, which allows the user to easily switch among different data and visualization methods. 3.1.1 Volumetric MRI and SPECT data One group of students had to represent, in the same window, previously registered MRI and SPECT data. This simultaneous visualization provides physicians with important information concerning the location and intensity of brain activity. A surface extracted from MRI data is presented, as well as red polygonal models representing SPECT data. The interface allows the user to activate and interactively locate up to three cutting-planes (axial, coronal and sagittal) to visualize cross-sections of the registered data (Fig. 1 ). 3.1.2 EEG data A second group of students was asked to represent the location of the electrodes placed on a patient's head, as well as the EEG signal values measuring brain activity, using a mesh model of a human head. Electrodes are represented as white spheres with an associated label. The colour value assigned to each mesh vertex is defined by the signal value at the closest electrode. This results in a final representation with different colours associated to the electrical potential variations on the patient's head (Fig. 2 ). Since temporal information is available, the user can also navigate through different acquisition times and observe the evolution of the EEG data. 3.1.3 Sources of electrical activity (dipoles) A third group of students had to represent the estimated location of the sources of electrical activity (dipoles) within the brain. The dipoles are represented as arrows within a surface model of a human head. Data are read from pre-processed files describing the sampling frequency, the location and the orientation of the dipoles. In addition to these features the application can also display different groups of dipoles using different colours. As for the EEG data, the user can also navigate forward or backwards in time. Given the large range of dipole magnitudes, two visualization modes are available. In the first, the length of each arrow encodes dipole magnitude; in the second, all arrows are shown with the same length, thus simplifying the analysis and detection of clusters and patterns (Fig. 3 ). 3.2 Visualization of water pressure and velocity around a ship's hull The goal of this project was to develop visualization tools to analyse the water flow around a ship's hull. The data consisted of the coordinates of sampled points (unstructured grid), as well as pressure and velocity values. Hedgehogs were used to represent velocity data and pressure was converted to a structured grid through splatting [16]. The final application gives the user the possibility to manipulate a cutting plane where the pressure data are displayed through colour mapping. A view of the final visualization is presented in Fig. 4 . 3.3 Visualization of acoustic data Another interesting set of projects consisted in developing applications to visualize the propagation of sound waves, given time-varying data sets containing the results of sound propagation simulations. 3.3.1 2D sound wave propagation This application allows the analysis of 2D data obtained from an acoustic modelling software, which simulates the interaction among neighbouring points, and thus the propagation of sound waves, resulting from the injection of a sound pulse at an arbitrary point of a given environment, using a numerical model based on finite differences in the time domain (FDTD). Two visualization modes are offered: for a given instant in time t, the sound wave values as well as the associated zero-valued iso-lines are displayed (see Fig. 5 ); as an alternative, the maximum values recorded so far, for time t, can also be represented. As usual, the user can navigate forward or backwards in time. 3.3.2 3D acoustic pressure The second application allows the analysis of data resulting from 3D acoustic simulations, namely pressure values and the frequency response for a given 3D environment. In both cases, 3D time-varying data are simultaneously visualized through two representations, using the same colour scale: one allows slicing through the model volume using two orthogonal plane widgets, the other shows the iso-surfaces (see Fig. 6 ). 3.4 Visualization of Oporto underground tunnels The goal of this project was to allow the visualization of point clouds, acquired with a 3D laser scanner, representing some tunnels of the city of Oporto underground network, in order to assess particular tunnel surface features. A visualization application was developed allowing the representation of the scanned data in various ways: as a point cloud, a 3D Delaunay triangulated surface or a Gaussian splattered volume. In all these representations the user can interact with the data using a plane widget to define appropriate tunnel cross-sections (see Fig. 7 ). 3.5 Visualization of a football game in a VR environment A different kind of project, footVR, was also developed: the goal was to visualize the dynamics of a football game using a VR environment under development at our laboratory. The application reads the log-file from a simulation of a robotic football game and allows the user to watch the game (robots/players are represented as simple triangular prisms) either in a desktop, by controlling the viewing angle (there is an option to follow automatically the ball), or in the VR environment, which includes a head mounted display (HMD) and a tracker. In the latter, the user's head motion is registered and the camera parameters updated accordingly, as shown in Fig. 8 . 4 Using VTK: an assessment Since it is not usual to choose VTK as a tool supporting 3DMV classes, we decided to assess the effectiveness of using this toolkit taking into account three different vantage points: (1) comparing the evaluation results of the OpenGL and VTK projects developed by our students; (2) analysing the answers given to a questionnaire handed out to the students in 2006/2007; and (3) collecting our impressions on the use of VTK, given the questions and reactions from the students during the VTK lab classes. 4.1 Evaluation of the OpenGL and VTK projects In addition to developing and implementing their 3DMV projects, one using OpenGL, the other using VTK, students were required to write a report describing the main steps of the work carried out, as well as present their work to their colleagues. Their projects were then individually evaluated by the authors (see Table 1 for the global results in the last two years). Instead of directly comparing the grading of individual projects, we performed a relative comparison and identified students having similar evaluation results in both projects, or performing significantly better or worse in VTK than in OpenGL (see Table 2 ). While, for 2005/2006, the results in Table 2 seem to validate our initial idea that using VTK would not be an excessive load for most students, and would be an excellent way of further motivating interested students, this is no longer true for the results obtained in 2006/2007: less students had excellent results in both APIs or performed significantly better when using VTK; moreover, one-third of the students performed significantly worse when using VTK, than when using OpenGL. Clearly, the results in Table 2 seem to be inconclusive. Although a possible explanation for the worse results obtained in 2006/2007 with VTK might be the excessive workload, during the second half of the semester and due to other courses. Further work is needed to better identify global student shortcomings, as well as particular questions and needs regarding the use of VTK. 4.2 Questionnaire handed out in 2006/2007 A questionnaire was handed out to the students at the end of their 3DMV course: the main goal was to gather their own opinion regarding the work carried out when developing the two projects, as well as additional information on their use of OpenGL and VTK and on their experience previous to the course. The questionnaire was anonymous and we had 16 (out of 18) respondents: 13 declared having average or above average programming experience, and 13 had never used a CG programming API before. We globally analysed their grades in the programming and algorithms and data structures courses they had attended before, and concluded that less than half of them could really claim having average or above average programming experience. Regarding the OpenGL project, 14 students considered themselves satisfied with the applications they had developed, and 11 classified their work as being above average. This overall evaluation is quite similar to our own grading of their work. Regarding the VTK project, the main conclusions that can be extracted from the answers to the questionnaire are: • Thirteen students considered themselves satisfied with the applications they had developed, and 11 classified their work as being above average. • Although there is no significative difference, students globally dedicated slightly less time to the VTK project than to the OpenGL project. • Although there is no significative difference, students globally declared having had slightly less difficulty in developing their VTK project than their OpenGL project. • Students considered that, on average, the information available for VTK was mostly insufficient, and poorer than the information available for OpenGL. Regarding our own evaluation of the work carried out by the students, their answers to the questionnaire revealed that most of them had expected better grades. This was probably due to VTK allowing them to develop applications providing visually pleasing results with less work, when compared to OpenGL. We were quite surprised to find out that students seemed to have dedicated slightly less time to their VTK projects, and found it slightly less difficult, in comparison to the OpenGL projects. In our opinion, the VTK projects are more demanding and would have required more time and effort for the students to attain above than average results. Such an attitude by the 2006/2007 students might also explain why the global evaluation in Table 2 is worse than that for 2005/2006. 4.3 Our own impression In spite of the results in Table 2, our overall evaluation on the use of VTK is encouraging. We were positively surprised by the quality of the visualization applications developed by some of our students, since the time allocated to the final VTK projects was relatively reduced. Using VTK allowed us to propose challenging and motivating tasks, and students were able to develop relatively complex applications in a short time. This was certainly rewarding for most students. For the students, the object-oriented structure of VTK and its modularity were also important advantages. However, many students complained about the lack of appropriate documentation to help them use VTK. The available documentation is very often insufficient to clearly understand the features of the classes used, and examples are missing for many functions. Even with the help of the user's guide [24], it is often difficult to understand at first how VTK classes behave: this is certainly a strong limitation of the toolkit, which does not recommend its use by students with less programming experience or reduced knowledge of the object-oriented paradigm. In some way, using VTK can even be frustrating for a student, since final solutions to some programming or development difficulties are often compact (a few lines of code), but difficult to attain. A possible way to mitigate this problem might consist in providing the students with a set of additional code examples. The abovementioned shortcomings of VTK force students to a somewhat important effort, during their first contact with the toolkit, in order to overcome first difficulties. For students with low motivation this was a major drawback, and a few did not succeed in developing satisfactory work. Nevertheless, most students particularly appreciated the use of a higher-level tool as VTK, which allows developing working prototypes and provides some degree of interaction and appropriate visualization functionalities. Some students were also asked to write a short paper describing the main features of their work, to be published in the internal journal of our department. Despite the fact this additional work was asked for after the conclusion of the semester, almost all of them agreed. This exercise was, after all, a nice introduction to more challenging research projects that might be proposed to some of them later. 5 Conclusion We have presented our experience regarding the use of VTK, in the last two years, in the context of the elective 3DMV course at the University of Aveiro. In spite of the global results in 2006/2007 being somewhat poorer than expected, when compared to 2005/2006, our overall evaluation on the use of VTK is encouraging. Using OpenGL in the first lab classes provides a valuable first step for those students with no previous CG programming experience. Then, the object-oriented structure of VTK and its modularity, as well as the visualization and interaction functionalities available, are of great advantage for most students, who are able to develop complex visualization applications in a relatively short time. However, it is also clear for us that the learning-curve of VTK might delay or even prevent the progress of some students, and we still have to devise appropriate strategies to mitigate this problem. Since we intend to keep using VTK in our CG courses, both at graduate and post-graduate level, this will be our next challenge. Acknowledgements First of all, we wish to thank all the students who developed the visualization applications presented here. A word of gratitude goes to our colleagues: José Maria Fernandes, José Rocha Pereira, Filipe Teixeira-Dias, Guilherme Campos, Augusto Silva and José Silvestre Silva, who provided us with some of the data sets used in the various assignments. Thanks are also due to the 3D laser digitalization company ARTESCAN, for providing us with the Oporto underground tunnels data. We thank the anonymous referees for their comments and suggestions and Samuel Silva for his help in preparing the final version of this paper. References [1] Grissom S, Bresenham J, Kubitz B, Owen S, Schweitzer D. Approaches to teaching Computer Graphics. In: Proceedings of SIGCSE 1995, Nashville, TN, 1995. p. 382–3. [2] L.E. Hitchner H.A. Sowizral Adapting Computer Graphics curricula to changes in graphics Computers & Graphics 24 2000 283 288 [3] D.J. Bouvier From pixels to scene graphs in introductory Computer Graphics courses Computers & Graphics 26 2002 603 608 [4] E. Paquette Computer Graphics education in different curricula: analysis and proposal for courses Computers & Graphics 29 2005 245 255 [5] Angel E, Cunningham S, Shirley P, Sung K. Teaching Computer Graphics without raster-level algorithms. In: Proceedings of SIGCSE 2006, Houston, TX, 2006. p. 266–7. [6] McCracken CR. Issues in Computer Graphics education. In: Proceedings of SIGGRAPH 2006 Educators Program, Boston, MA, 2006. [7] Sung K, Shirley P, Rosenberg BR. Experiencing aspects of games programming in an introductory Computer Graphics class. In: Proceedings of SIGCSE 2007, Covington, Kent, 2007. p. 249–53. [8] E. Angel Interactive Computer Graphics: a top-down approach using OpenGL 4th ed. 2006 Addison-Wesley Reading, MA [9] Tori R, Bernardes Jr JL, Nakamura R. Teaching introductory Computer Graphics using Java 3D, games and customized software: a Brazilian experience. In: Proceedings of SIGGRAPH 2006 Educators Program, Boston, MA, 2006. [10] J. Zara Virtual reality course—a natural enrichment of Computer Graphics classes Computer Graphics Forum 25 2006 105 112 [11] M. Bailey Teaching OpenGL shaders: hands-on, interactive, and immediate feedback Computers & Graphics 31 2007 524 531 [12] Talton JO, Fitzpatrick D. Teaching graphics with the OpenGL shading language. In: Proceedings of SIGCSE 2007, Covington, Kent, 2007. p. 259–63. [13] J. Foley A. van Dam S. Feiner J. Hughes R. Phillips Introduction to Computer Graphics 1994 Addison-Wesley Reading, MA [14] D. Hearn M.P. Baker Computer Graphics with OpenGL 3rd ed. 2004 Prentice-Hall Englewood Cliffs, NJ [15] S. Cunningham Computer Graphics: programming in OpenGL for visual communication 2006 Prentice-Hall Englewood Cliffs, NJ [16] Schroeder W, Martin K, Lorensen B. The Visualization Toolkit—an object oriented approach to 3D graphics, 4th ed. Kitware; 2006. [17] J. Frost S. Goessner M. Hirtzler Learn SVG: the web graphics standard 2004 Self Publishing [18] A.L. Ames D.R. Nadeau J.L. Moreland The VRML 2.0 sourcebook 2nd ed. 1997 Wiley New York [19] D. Shreiner M. Woo J. Neider T. Davis OpenGL programming guide 5th ed. 2005 Addison-Wesley Reading, MA [20] Davis G. Learning Java bindings for OpenGL (JOGL). Author House; 2004. [21] D. Selman Java 3D programming 2002 Manning Publications [22] F.D. Luna Introduction to 3D game programming with DirectX 9.0 2003 Wordware Publishing [23] Dias P, Madeira J, Sousa Santos B. Using VTK as a tool for teaching and applying Computer Graphics. In: Proceedings of EUROGRAPHICS 2006 Education Programme, Vienna, Austria, 2006. p. 61–7. [24] Kitware Inc The VTK user's guide—version 4.2 2003 Kitware "
    },
    {
        "doc_title": "Preliminary usability evaluation of PolyMeCo: A visualization based tool for mesh analysis and comparison",
        "doc_scopus_id": "35048822035",
        "doc_doi": "10.1109/GMAI.2007.27",
        "doc_eid": "2-s2.0-35048822035",
        "doc_date": "2007-10-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Polygonal meshes",
            "Usability evaluation",
            "Visualization based tool"
        ],
        "doc_abstract": "An overall description of the methods used and the results obtained in the on-going evaluation of PolyMeCo - a mesh analysis and comparison tool - is presented. We are trying to evaluate some aspects of both the user interface and the visualization techniques implemented. Heuristic evaluation, observation and querying techniques were used and produced encouraging preliminary results, which provided new ideas, as well as information, that will inform the development of a more usable version of PolyMeCo, including new functionality. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Usability in virtual and augmented environments: A qualitative and quantitative study",
        "doc_scopus_id": "34548209132",
        "doc_doi": "10.1117/12.703878",
        "doc_eid": "2-s2.0-34548209132",
        "doc_date": "2007-08-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Augmented Reality systems",
            "Head-Mounted Display",
            "HMD visualization",
            "Usability evaluation"
        ],
        "doc_abstract": "Virtual and Augmented Reality are developing rapidly: there is a multitude of environments and experiments in several laboratories using from simple HMD (Head-Mounted Display) visualization to more complex and expensive 6-wall projection CAVEs, and other systems. Still, there is not yet a clear emerging technology in this area, nor commercial applications based on such a technology are used in large scale. In addition to the fact that this is a relatively recent technology, there is little work to validate the utility and usability of Virtual and Augmented Reality environments when compared with the traditional desktop set-up. However, usability evaluation is crucial in order to design better systems that respond to the users' needs, as well as for identifying applications that might really gain from the use of such technologies. This paper presents a preliminary usability evaluation of a low-cost Virtual and Augmented Reality environment under development at the University of Aveiro, Portugal. The objective is to assess the difference between a traditional desktop set-up and a Virtual/Augmented Reality system based on a stereo HMD. Two different studies were performed: the first one was qualitative and some feedback was obtained from domain experts who used an Augmented Reality set-up as well as a desktop in different data visualization scenarios. The second study consisted in a controlled experiment meant to compare users' performances in a gaming scenario in a Virtual Reality environment and a desktop. The overall conclusion is that these technologies still have to overcome some hardware problems. However, for short periods of time and specific applications, Virtual and Augmented Reality seems to be a valid alternative since HMD interaction is intuitive and natural. © 2007 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D reconstruction of soccer sequences using non-calibrated video cameras",
        "doc_scopus_id": "37849010353",
        "doc_doi": "10.1007/978-3-540-74260-9_111",
        "doc_eid": "2-s2.0-37849010353",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "SimulFoot project",
            "Single model information",
            "Tracking processes"
        ],
        "doc_abstract": "We present a global approach that enables the production of 3D soccer sequences from non-calibrated video cameras. Our system can produce a 3D animated model of the scene from a single non-calibrated moving camera (a TV sequence for example). The results presented here are very encouraging even with a single camera approach and will probably improve with the future introduction of multiple images that will help resolving occlusion issues and integrating into a single model information coming from various locations on the field. The key point of our approach is that it doesn't need any camera calibration and it still works when the camera parameters vary along the process. Details on the registration and tracking processes are given as well as the description of the \"Virtual Reality\" system used for displaying the resulting animated model. © Springer-Verlag Berlin Heidelberg 2007.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D reconstruction of real world scenes using a low-cost 3D range scanner",
        "doc_scopus_id": "33746696563",
        "doc_doi": "10.1111/j.1467-8667.2006.00453.x",
        "doc_eid": "2-s2.0-33746696563",
        "doc_date": "2006-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Civil and Structural Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2205"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            }
        ],
        "doc_keywords": [
            "Laser scanner",
            "Textured polygonal models",
            "Three dimensional modeling",
            "Three dimensional reconstruction"
        ],
        "doc_abstract": "This article presents a 3D reconstruction technique for real world environments based on a traditional 2D laser range finder modified to implement a 3D laser scanner. The article describes the mechanical and control issues addressed to physically achieve the 3D sensor used to acquire the data. It also presents the techniques used to process and merge range and intensity data to create textured polygonal models and illustrates the potential of such a unit. The result is a promising system for 3D modeling of real world scenes at a commercial price 10 or 20 times lower than current commercial 3D laser scanners. The use of such a system can simplify measurements of existing buildings and produce easily 3D models and ortophotos of existing structures with minimum effort and at an affordable price. © 2006 Computer-Aided Civil and Infrastructure Engineering.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Combining intensity and range images for 3D modelling",
        "doc_scopus_id": "0344666650",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0344666650",
        "doc_date": "2003-12-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Depth mapping",
            "Intensity images"
        ],
        "doc_abstract": "This paper presents a process combining range and intensity based techniques, in order to get better 3D models than those obtained using these techniques separately. The procedure needs an initial estimation for internal and external camera parameters for two or more intensity images. The technique uses passive triangulation to refine initial camera calibrations and ensure a good registration of range and video data sets. Afterwards, corresponding points from the intensity images are triangulated and introduced in the original range cloud of points. The objective is to complete the models in areas where data is missing or to increase the resolution in areas of high interest and 3D contents.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An interdisciplinary E-learning system for the K-6",
        "doc_scopus_id": "1542432731",
        "doc_doi": null,
        "doc_eid": "2-s2.0-1542432731",
        "doc_date": "2003-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Development",
                "area_abbreviation": "SOCI",
                "area_code": "3303"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            }
        ],
        "doc_keywords": [
            "Adaptive hypermedia",
            "E-learning",
            "Living knowledge",
            "Web-based education (WBE)"
        ],
        "doc_abstract": "Education as a way of being, remains besides school time, by means of a permanent learning process, making the feeling of knowledge as a whole, as knowledge itself is, through the discovery of the relationship among different matters, even those that, in a single view, are not related. This paper presents the research and results for the development of an illustrated, interactive, permanent and asynchronous learning model that allows the exploration of knowledge subjects, searching about their relationship. This is materialized via an e-learning interdisciplinary environment, to be used by K-6 students, but also involving teachers and parents, to be used within an existing model of integrated use of Information and Communication Technologies in educational contexts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Registration and fusion of intensity and range data for 3D modelling of real world scenes",
        "doc_scopus_id": "77957341858",
        "doc_doi": "10.1109/IM.2003.1240277",
        "doc_eid": "2-s2.0-77957341858",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "3D reconstruction",
            "Camera calibration",
            "Initial estimation",
            "Layout",
            "Novel techniques",
            "Real environments",
            "Spatial resolution",
            "Video sequences"
        ],
        "doc_abstract": "© 2003 IEEE.A novel technique combining intensity and range data is presented. Passive (intensity based) and active (range based) techniques used for 3D reconstruction have their limitations and separately, none of these techniques can solve all the problems inherent to the modelling of real environments. Our technique aims to demonstrate how both intensity and range data can be registered and combined into a long-range 3D system. The procedure needs an initial estimation for internal and external camera parameters for two or more intensity images. The technique uses passive triangulation of the intensity data to refine the initial camera calibrations and ensure a good registration of range and video data sets. Once a reliable calibration is achieved, corresponding points from the intensity images are triangulated and introduced in the original range data. With our technique, it is possible to complete the models in areas where data is missing or to increase the resolution in areas of high interest and 3D contents.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic registration of laser reflectance and colour intensity images for 3D reconstruction",
        "doc_scopus_id": "0037199041",
        "doc_doi": "10.1016/S0921-8890(02)00201-4",
        "doc_eid": "2-s2.0-0037199041",
        "doc_date": "2002-06-30",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The objective of the work presented in this paper is to generate complete, high-resolution models of real world scenes from passive intensity images and active range sensors. In previous work, an automatic method has been developed in order to compute 3D models of real world scenes from laser range data. The aim of this project is to improve these existing models by fusing range and intensity data. The paper presents different techniques in order to find correspondences between the different sets of data. Based on these control points, a robust camera calibration is computed with a minimal user intervention in order to avoid the fastidious point and click phase that is still necessary in many systems. The intensity images are then re-projected into the laser coordinate frame to produce an image that combines the laser reflectance and the available video intensity images into a colour texture map. © 2002 Elsevier Science B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2002-04-09 2002-04-09 2010-11-15T13:43:15 S0921-8890(02)00201-4 S0921889002002014 10.1016/S0921-8890(02)00201-4 S300 S300.2 FULL-TEXT 2015-05-15T02:57:28.012904-04:00 0 0 20020630 2002 2002-04-09T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confabbr confdate confeditor confloc contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 0921-8890 09218890 39 39 3 4 3 4 Volume 39, Issues 3–4 5 157 168 157 168 20020630 30 June 2002 2002-06-30 2002 Intelligent Robotic Systems - SIRS'2000 SIRS'2000 Reading, UK 18–20 July 2000 20000718 20000720 James M. Ferryman, Anthony D. Worrall converted-article fla Copyright © 2002 Elsevier Science B.V. All rights reserved. AUTOMATICREGISTRATIONLASERREFLECTANCECOLOURINTENSITYIMAGESFOR3DRECONSTRUCTION DIAS P 1 Introduction 2 Matching laser reflectance with intensity images 2.1 Rescaling algorithm 2.2 Point based matching 3 Camera calibration 3.1 Camera model 3.2 Robust estimation 4 Re-projection 4.1 Re-projection and Z-buffering 4.2 Colour blending 5 Current work: iterative calibration 6 Experimental results 7 Conclusions Acknowledgements References CANNY 1986 679 698 J FISCHLER 1981 381 395 M SEQUEIRA 1999 1 22 V TSAI 1987 323 344 R DIASX2002X157 DIASX2002X157X168 DIASX2002X157XP DIASX2002X157X168XP item S0921-8890(02)00201-4 S0921889002002014 10.1016/S0921-8890(02)00201-4 271599 2010-12-10T02:43:07.358792-05:00 2002-06-30 true 809028 MAIN 12 59468 849 656 IMAGE-WEB-PDF 1 si5 3127 45 593 si4 2508 48 554 si3 2681 31 646 si2 2469 45 424 si1 1894 19 450 gr1 56767 586 636 gr1 5795 164 178 gr2 14554 327 463 gr2 4852 155 219 gr3 83843 623 534 gr3 5271 163 140 gr4 52616 375 546 gr4 9146 150 219 gr5 42823 414 301 gr5 4649 164 119 gr6 48797 560 565 gr6 5461 164 165 gr7 44885 535 328 gr7 3550 163 100 gr8 105553 564 715 gr8 10392 164 208 gr9 88818 513 715 gr9 8117 157 219 fx1 9836 151 113 fx1 6136 164 123 fx2 9373 151 113 fx2 5731 164 123 fx3 8911 151 113 fx3 5665 164 123 fx4 10792 151 113 fx4 6615 164 123 ROBOT 974 S0921-8890(02)00201-4 10.1016/S0921-8890(02)00201-4 Elsevier Science B.V. Fig. 1 The original intensity (1024×1536) and reflectance (1850×444) images with the initial user selection. Fig. 2 Flowchart of the rescaling procedure. Fig. 3 Superposition of edges in a laboratory scene (a) before and (b) after the rescaling, and (c) distance between the edges. Fig. 4 Point based matching in the laboratory image. Fig. 5 Reflectance (black) and re-projected video (light grey) edges. Fig. 6 The occlusion problem, (a) direct re-projection, (b) re-projection using Z-buffer to reject occluded points. Fig. 7 The iterative camera calibration procedure. Fig. 8 Some examples of the re-projection process (a, b) and two snap-shots of the final textured model of the laboratory (c). Fig. 9 The texture map (a) and two snap-shots (b, c) of the textured 3D model of a farmhouse. Table 1 (a) Processing time of the different steps for an image of the laboratory scene; (b) blending of the 12 images in Fig. 8(b) on a Pentium III, 850MHz Time (s) (a) Rescaling 7 Matching 4 Calibration 10 Z-buffering 9 Re-projection 5 (b) Average blending 4 Rectangular blending 5 Weighted blending 88 Automatic registration of laser reflectance and colour intensity images for 3D reconstruction P. Dias a V. Sequeira a ∗ J.G.M. Gonçalves a F. Vaz b a European Commission, Joint Research Centre, Institute for the Protection and Security of the Citizen (IPSC), TP 270, 21020 Ispra (VA), Italy b University of Aveiro/IEETA, 3800 Aveiro, Portugal * Corresponding author. Tel.: +39-0332-785233; fax: +39-0332-789185 The objective of the work presented in this paper is to generate complete, high-resolution models of real world scenes from passive intensity images and active range sensors. In previous work, an automatic method has been developed in order to compute 3D models of real world scenes from laser range data. The aim of this project is to improve these existing models by fusing range and intensity data. The paper presents different techniques in order to find correspondences between the different sets of data. Based on these control points, a robust camera calibration is computed with a minimal user intervention in order to avoid the fastidious point and click phase that is still necessary in many systems. The intensity images are then re-projected into the laser coordinate frame to produce an image that combines the laser reflectance and the available video intensity images into a colour texture map. Keywords 3D reconstruction Automatic registration Camera calibration Texture map 1 Introduction With the increase in computer capabilities, 3D models are becoming more interesting as they can be displayed and used in a home PC without requiring dedicated hardware. Commercial products are already available to compute 3D models, e.g. small object for Internet application. On the other hand, real world scenes are often too complex for realistic model acquisition. This work tries to combine the information coming from the most typical 3D sensors (laser range finders and video) in order to overcome these limitations. Almost every laser returns for each measurement the distance to the point and the reflected light intensity. This leads to two different sets of data. From the distance measurement, the 3D models are computed, whereas an image can be generated from the reflectance information [13]. The choice of using the reflectance image rather than the 3D model for the registration was based on several considerations: first, the reflectance image is a 2D (normally, infrared) image such as the video intensity image, which allows the use of already existing techniques for matching intensity images [9]. In addition, the re-projection of the intensity image into the reflectance image is a very intuitive and fast way of evaluating the registration. Unfortunately, the images obtained with the laser and the video camera are very different. The camera is a high-resolution passive sensor whereas the laser is an active sensor of lower resolution. To overcome these differences a feature and area based matching algorithm was developed. Compared to other registration techniques [5,10], this one is independent of the experimental set-up (e.g. internal camera parameters), does not need any previous calibration and minimises user intervention. 2 Matching laser reflectance with intensity images The matching algorithm is divided into two main steps: rescaling and matching. Rescaling resizes the video image to match the dimensions of the reflectance image. Matching is used to locate corresponding features. 2.1 Rescaling algorithm Traditional matching techniques used in photogrammetry or stereo vision [6] use images obtained from the same sensors, i.e. same size and same resolution. In our specific case, an intermediate step is required to rescale the two images before using any matching procedure. In our system, the user has to select a rectangular area of interest in the reflectance image corresponding to the video image to be registered (see Fig. 1 ). The rescaling procedure starts with this initial approximation and evaluates the planar transformation between the images using five parameters: the planar translation coordinates, the scaling factors for both x- and y-axis, and the planar rotation between both images. The result, after a few iterations, is a coarse approximation of the transformation between the two images with enough overlapping between the main features allowing the use of matching algorithms. The flowchart in Fig. 2 presents the algorithm. In the first step, a Canny edge detector [3] and the distance transform [1,2] are applied to the video image. The reflectance edges are then iteratively translated, scaled and rotated over the distance transform image until the transformation that leads to a minimum distance between the edges is found. The results of the rescaling procedure are presented in Fig. 3 (in black the edges detected in the reflectance image and in light grey the ones extracted from the intensity image). Fig. 3(a) shows the superposition of the edges of the video and the reflectance image before running the process. Fig. 3(b) presents the results after applying the algorithm with the reflectance edges rescaled. Fig. 3(c) shows the evolution of the average distance between the edges of both images along the rescaling procedure. Despite the average error of eight pixels, the result of the rescaling is sufficient for the subsequent matching step. 2.2 Point based matching The next step of the matching procedure is to find some control points over the images. A Harris corner detector [8] is used to find points of interest in the reflectance image based on the following formula: (1) R(x,y)=A(x,y)×B(x,y)−C2(x,y)−Weight×(A(x,y)+B(x,y))2, where A and B are the smoothed direction derivatives and C is the smoothing of the products of the direction derivatives. The derivatives are calculated with a Sobel operator and smoothed with a Gaussian filter. Weight is a user-defined constant. Here, Weight=0.04, so grey value corners return positive values for R(x, y), while straight edges receive negative values. For each point of interest, correlation is computed over the gradient images (to make the matching more invariant to changes in illumination). The correlation measure used was the displaced frame difference (dfd) because for similar results as the cross correlation, it speeds up the process by a factor of 3: (2) dfd(i,j)= ∑ u,v abs|Image(i−u,j−v)−Template(l−u,c−v)| area(Template) , where (i, j) are the coordinates of the considered point of interest in the reflectance image, (l, c) are the coordinates of the point in the intensity image and (u,v) are within the interval [−TemplateSize/2; TemplateSize/2]. To optimise the procedure, the dfd is computed in the neighbourhood of each pixel. Despite the resizing algorithm and the dfd correlation, the number of outliers is still significant due to the different nature of the images. To discard bad matching pairs the following criteria are used: (i) If various matches are accumulated on a small area, only the one with the higher correlation is considered: a matching pair i is considered valid only if (3) Correlation(xi,yi)=max(correlation(xj,yj)) foreach j: (xi−xj)2+(yi−yj)2 ⩽Threshold, where (xi , yi ) are the coordinates of the video pixel of the matching ‘i’. (ii) The average distance between the matchings are computed over the images and only the pairs for which the distance is lower than a given threshold are considered: (4) (x v −x r )2+(y v −y r )2 < Dist 2 , withDist= ∑ i=1 n (x v i −x r i )2+(y v i −y r i )2 n , where (x v, y v) is the coordinate of a pixel in the video image; (x r, y r) is the coordinate of the corresponding pixel in reflectance image and n is the number of matching. The results of this matching procedure are presented in Fig. 4 for the laboratory images. In this case, 50 matching pairs are detected and considered valid. 3 Camera calibration 3.1 Camera model A Tsai camera calibration technique [14] is used to calibrate the camera. This pinhole perspective projection model is based on 11 parameters. Six extrinsic parameters characterise the position and orientation of the camera; the other five describe the camera properties (such as focal length and radial distortion). Depending on the 3D position of the control points, either the coplanar or non-coplanar calibration is used. The user has also the possibility to provide the internal parameters of the camera (if known) and calibrate only the extrinsic parameters. 3.2 Robust estimation Due to the difficulty of matching the reflectance and the video image, a relatively large number of mismatches will appear during the matching step. Nevertheless, a good calibration for the camera is computed based on a RANSAC estimation technique [7]. A calibration is computed starting with a subset of random points. For each of the candidate calibrations, the one with the largest support is selected, excluding most of the outliers. The algorithm needs three parameters: the number of trials (5); the subset of points used in each trial for the first evaluation (a third of the matching pairs); the threshold to consider a point as an outlier (15 pixels). 4 Re-projection Once a model for the camera is obtained, it is possible to associate to each 3D point its projection into the video image; this is done in the re-projection step. The aim is to obtain a texture map ready to apply to the reconstructed surfaces making the whole 3D model richer and more realistic. In most cases, different camera views overlap in parts of the models. To avoid rough transitions in these border areas, colour blending is applied. 4.1 Re-projection and Z-buffering Fig. 5 presents the edges detected on the original reflectance image (in black) and the edges detected on the re-projected image (light grey). The results are visibly better than using only the resizing algorithm (see Fig. 3). The median distance between the edges has decreased to 6.15 pixels (the error after the resizing algorithm was 8.3, see Fig. 3(c)). Projecting the video image pixels into the reflectance image can lead to errors of projection due to occluded points as shown in Fig. 6 , where the arrows point to the errors due to occlusions. To avoid this problem, a Z-buffer is filled with the Z-coordinate of the closest 3D points for each pixel of the video image. Using this Z-buffer to handle occlusions, it is possible to select the correct video points when re-projecting the data. Back-projecting the known 3D positions will limit the final texture map resolution to the range image size. To overcome this limitation, the reflectance can be “zoomed” to the size of the video image, and a linear interpolation can be used to compute the “extra” 3D positions. Using this interpolation process, it is possible to exploit the full resolution of the intensity image for the texture map. 4.2 Colour blending In most cases, the field of view of the laser image is much larger than the one from a normal camera. Nowadays it is possible to find lasers capable of acquiring 270° or even 360° images; thus, several video images are necessary to cover the whole reflectance image. To solve this problem, each image is back-projected into the laser coordinate frame followed by a colour blending technique to smooth the transitions between images. The colour blending process computes for each point the number of video images that have a pixel projected in this position and the distance to the closest contour of each image. The final value of the pixel will be a weighted combination of the intensity values in each image according to the distance to the closest contour. The formula used to compute the final intensity value in each RGB image is (5) Intensity(x,y)= ∑ i=1 n Intensity i(x,y)×dist i(x,y) Distance_total withDistance_total= ∑ i=1 n dist i(x,y), where Intensity i (x, y) is the intensity of pixel (x, y) in image i, dist i (x, y) is the distance of pixel (x, y) to the closest border in image i, and n is the number of images with a pixel projected in (x, y). 5 Current work: iterative calibration Due to the different nature of the images, a single run of the algorithm can lead to a camera calibration that is not fully satisfactory. To improve the reliability of the calibration process, an iterative procedure has been introduced (see Fig. 7 ). The point based matching (see Section 2.2) ensures the computation of a first approximation for the camera model. Based on this first calibration, the video image is re-projected into the reflectance coordinate frame. The red channel of the re-projected image (selected for its higher similarity with the infrared reflectance image) is then used as entry into a new matching algorithm. At this point, reflectance and re-projected images are already very similar (since the distortion introduced by the camera is already considered by the re-projection) and a new matching procedure is used. The matching algorithm detects corners in both the re-projected and the reflectance image, and dfd is computed between neighbouring corners. The neighbouring corner with the higher correlation is then selected as the new matching point and based on these correspondences, a new calibration is computed. 6 Experimental results Figs. 8 and 9 present the results of the re-projection and blending processes for two real scenes. The first one is a laboratory at the JRC site. The second one is a farmhouse in Laveno (Italy) on the shores of Lago Maggiore. Both the scenes were acquired with a RIEGL LMS-Z210 laser range scanner [11] and a Canon PowerShot Pro70 digital camera. Fig. 8(a) shows the re-projection of the test image, used in previous sections to illustrate the automatic matching, into the reflectance image. Fig. 8(b) is the final texture map obtained from the re-projection and blending of 12 images (for illustration, the borders of each re-projected image are also displayed). Fig. 8(c) presents two snap-shots of the model after applying the texture map. The only user intervention was the selection of a rectangle in the reflectance image corresponding approximately to the same field of view of the different video images. The rest of the procedure was fully automatic. Fig. 9 presents similar results for the model of a farmhouse. Fig. 9(a) shows one of the texture maps of the final model obtained after the re-projection of three video images into the reflectance image. In Figs. 9(b) and 9(c) two snap-shots of the final textured 3D model are shown. In this case some control points have been manually added to guide the process. To convey spatial awareness when moving through the surrounding, the 3D model is combined with a panoramic triangulated sphere providing a background image to the model [12]. Table 1 shows the processing time for the different steps of the whole calibration/re-projection procedure for an image of the laboratory scene and the blending of the 12 images in Fig. 8(b). The resolution of the images is 1850×444 for the reflectance and 1024×1536 for the video. The processing times depend on many factors, mainly the image resolution and the number of images to merge. Blending is the most time-consuming process. Some tests have been done with simpler techniques (using averaging or rectangular borders instead of a weighted blending) and these decrease significantly the processing time even if the blending results are of lower quality (see Table 1(b)). 7 Conclusions The main characteristics of the algorithm presented in this paper are: (i) Independence from the acquisition sensors. The technique has been successfully used with different experimental set-ups (lasers and cameras from different manufacturers with different resolutions and options). (ii) No need for previous calibration. (iii) Simple evaluation of the quality of the registration by superimposing the intensity and range edges. (iv) Good and reliable calibration even with images taken from different viewpoints. (v) Possibility for the user to interact with the system and, if necessary, guide the process. The algorithm performance depends on the quantity and quality of the 2D features and on the similitude between the images (better results are obtained with images taken from close viewpoints). The main innovations introduced are the adaptive rescaling of the datasets to locate reliable registration points, the colour blending of the merged video images and the occlusion handling for selecting the video points to be used in the final texture map. One of the weak points of the calibration procedure is still the difficulty to get automatically fully reliable correspondences over the reflectance and intensity images. Tests have been done using the distance between corners as an additional condition to improve the matching step. Throughout this project, the main topic of investigation was the registration of 3D sensor data taken from a range sensor providing range and intensity data with video-based colour data. Based on this registration, a texture map for the 3D models is computed. This increases significantly the quality and the realism of the models while keeping the initial geometry. A natural extension of the work presented here is the use of the registered intensity images not only for texturing but also to correct and improve the 3D geometry of the models. In effect, detecting edges in intensity images is a fast and easy process and can be used as a guide to correct geometrical entities such as lines in 3D models [4]. Acknowledgements This research was primarily funded by the EC CAMERA TMR (Training and Mobility of Researchers) network, contract number ERBFMRXCT970127 and by the Portuguese Foundation for Science and Technology through the Ph.D. grant PRAXIS XXI/BD/19555/99. References [1] G. Borgefors, A new distance transformation approximating the Euclidean distance, in: Proceedings of the Eighth International Conference on Pattern Recognition, Paris, 1986, pp. 336–338. [2] G. Borgefors, An improved version of the Chamfer matching algorithm, in: Proceedings of the Seventh International Conference on Pattern Recognition, New York, 1984, pp. 1175–1177. [3] J.F. Canny A computational approach to edge detection IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8 6 1986 679 698 [4] P. Dias, V. Sequeira, J.G.M. Gonçalves, F. Vaz, Fusion of intensity and range data for improved 3D segmentation, in: Proceedings of the IEEE ICIP’2001, Thessaloniki, Greece, Vol. III, 2001, pp. 1107–1110. [5] M.D. Elstrom, A stereo-based technique for the registration of color and ladar images, Master Degree Thesis, University of Tennessee, Knoxville, TN, August 1998. [6] O. Faugeras, Three-dimensional Computer Vision, MIT Press, Cambridge, MA, 1993. [7] M.A. Fischler R.C. Bolles Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography Communications of the ACM 24 6 1981 381 395 [8] C.G. Harris, M. Stephens, A combined corner and edge detector, in: Proceedings of the Fourth Alvey Vision Conference, 1988, pp. 147–151. [9] C. Heipke, Overview of image matching techniques, in: Proceedings of the OEEPE Workshop on the Application of Digital Photogrammetric Workstations, 1996. [10] M. Partington, C. Jaynes, A hybrid image and model based approach to photorealistic building reconstruction, in: Proceedings of the Virtual and Augmented Architecture, VAA’01, Trinity College, Dublin, June 21–22, 2001, pp. 243–254. [11] Technical Document and User’s Instruction Manual, RIEGL GmbH, Laser Mirror Scanner LMS-Z210, November 1999. [12] V. Sequeira, E. Wolfart, E. Bovisio, E. Biotti, J.G.M. Gonçalves, Hybrid 3D reconstruction and image based rendering techniques for reality modelling, in: SPIE—Videometrics and Optical Methods for 3D Shape Measurement VII, San Jose, CA, USA, January 2001. [13] V. Sequeira K. Ng E. Wolfart J.G.M. Gonçalves D. Hogg Automated reconstruction of 3D models from real environments ISPRS Journal of Photogrammetry and Remote Sensing 54 1999 1 22 [14] R.Y. Tsai A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses IEEE Journal of Robotics and Automation RA-3 4 1987 323 344 Paulo Dias is a Ph.D. student at the Surveillance and Information Retrieval Sector, within IPSC (Institute for the Protection and Security of the Citizen) of the Joint Research Centre of the European Commission, Ispra, Italy. He received his diploma in Electronics and Telecommunications Engineering from the University of Aveiro, Portugal, in 1998. His diploma thesis involved the development of tools for the control and navigation of a mobile platform and his primary research interests are now in 3D reconstruction, computer vision and robotics. Vitor Sequeira has a degree in Electronics and Telecommunications Engineering from the University of Aveiro, Portugal (1990), and the Ph.D. from Department of Electrical and Computer Engineering of the Technical University of Lisbon, Portugal (1996). He is currently a Scientific Officer of the European Commission’s Joint Research Centre (JRC), within IPSC—Surveillance and Information Retrieval Sector. His current interests are 3D reconstruction of real scenes “as-built” including the design information verification and surveillance of safeguards relevant plants, Internet-based control of remote devices including performance assessment and data security, augmented reality and tele/virtual presence applications. He has published over 30 papers in the above mentioned fields, received the “Young Authors” award from the International Society for Photogrammetry and Remote Sensing (ISPRS) in Hakodate, Japan, 1998, and the U.V. Helava Award for the best paper published in the ISPRS Journal during 1999. João G.M. Gonçalves is the Head of the Surveillance and Information Retrieval Sector within IPSC (Institute for the Protection and Security of the Citizen) of the Joint Research Centre of the European Commission. After his first degree in Electronics and Telecommunications Engineering, he got an M.Sc. in Digital Systems from Brunel University and a Ph.D. on Medical Image Processing from the University of Manchester. Former to his appointment at the JRC, he was Associate Professor at the University of Aveiro, Portugal. Professor Gonçalves has been working in the field of remote verification in nuclear safeguards and non-proliferation, involving work on mobile robotics, 3D reconstruction of real environments “as-built”, augmented reality and tele-presence. He has participated in many EU research projects and networks and contributed to over 40 scientific publications. Francisco Vaz was born in Oporto, Portugal, in 1945. He received the Electrical Engineering degree from University of Oporto in 1968 and the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 1987. His Ph.D. dissertation was on automatic EEG processing. From 1969 to 1973 he worked for the Portuguese Nuclear Committee. After working in the industry for several years he joined the staff of the Department of Electronics Engineering and Telecommunications of University of Aveiro in 1978 where he is currently Full Professor and leading the signal-processing group. Since then his research interests have always been centred on the digital processing of biological signals. Dr. Vaz is a member of the board of the Portuguese Society of Biomedical Engineering. "
    },
    {
        "doc_title": "Fusion of intensity and range data for improved 3D models",
        "doc_scopus_id": "0035163630",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0035163630",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Edge extraction",
            "Intensity data",
            "Line straightening algorithms",
            "Range data",
            "Three dimensional models"
        ],
        "doc_abstract": "In this paper, we present techniques combining intensity and range data to achieve more realistic three-dimensional models from real world scenes. The principal novelty is the use of the intensity information not only for texturing the final 3D model but also to improve the segmentation of the range data. The technique registers video and range, i.e., the camera parameters are computed in order to get both data in the same reference frame. Edge extraction techniques are then applied to select corresponding edges and a line-straightening algorithm is used to correct the 3D model. Results are presented for both indoor and outdoor environments.",
        "available": false,
        "clean_text": ""
    }
]