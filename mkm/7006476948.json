[
    {
        "doc_title": "Augmented reality situated visualization in decision-making",
        "doc_scopus_id": "85107332174",
        "doc_doi": "10.1007/s11042-021-10971-4",
        "doc_eid": "2-s2.0-85107332174",
        "doc_date": "2022-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Data representations",
            "Decision making process",
            "Decision support system (dss)",
            "Design and implementations",
            "In contexts",
            "Literature analysis",
            "Scientific fields",
            "Visual data representation"
        ],
        "doc_abstract": "© 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.Decision-making processes and decision support systems (DSS) have been improved by a variety of methods originated from several scientific fields, such as information science and artificial intelligence (AI). Situated visualization (SV) allows presenting visual data representations in context and may support better DSS. Its main characteristic is to display data representations near the data referent. As augmented reality (AR) is becoming more mature, affordable, and widespread, using it as a tool for SV becomes viable in several situations. Moreover, it may provide a positive contribution to more effective and efficient decision-making, as the users have contextual, relevant, and appropriate information that fosters more informed choices. As new challenges and opportunities arise, it is important to understand the relevance of intertwining these fields. Based on literature analysis, this paper introduces the main concepts involved, and, through practical examples, addresses and discusses current areas of application, benefits, challenges, and opportunities of using SV through AR to visualize data in context to support better decision-making processes. In the end, a set of guidelines for the design and implementation of DSS based on situated augmented reality are proposed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing Desktop vs. Mobile Interaction for the Creation of Pervasive Augmented Reality Experiences",
        "doc_scopus_id": "85127690437",
        "doc_doi": "10.3390/jimaging8030079",
        "doc_eid": "2-s2.0-85127690437",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.This paper presents an evaluation and comparison of interaction methods for the configuration and visualization of pervasive Augmented Reality (AR) experiences using two different platforms: desktop and mobile. AR experiences consist of the enhancement of real-world environments by superimposing additional layers of information, real-time interaction, and accurate 3D registration of virtual and real objects. Pervasive AR extends this concept through experiences that are continuous in space, being aware of and responsive to the user’s context and pose. Currently, the time and technical expertise required to create such applications are the main reasons preventing its widespread use. As such, authoring tools which facilitate the development and configuration of pervasive AR experiences have become progressively more relevant. Their operation often involves the navigation of the real-world scene and the use of the AR equipment itself to add the augmented information within the environment. The proposed experimental tool makes use of 3D scans from physical environments to provide a reconstructed digital replica of such spaces for a desktop-based method, and to enable positional tracking for a mobile-based one. While the desktop platform represents a non-immersive setting, the mobile one provides continuous AR in the physical environment. Both versions can be used to place virtual content and ultimately configure an AR experience. The authoring capabilities of the different platforms were compared by conducting a user study focused on evaluating their usability. Although the AR interface was generally considered more intuitive, the desktop platform shows promise in several aspects, such as remote configuration, lower required effort, and overall better scalability.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Remote collaboration in maintenance contexts using augmented reality: insights from a participatory process",
        "doc_scopus_id": "85122856560",
        "doc_doi": "10.1007/s12008-021-00798-6",
        "doc_eid": "2-s2.0-85122856560",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Adaptive capabilities",
            "Daily tasks",
            "Industry sectors",
            "Participatory process",
            "Problem-solving",
            "Remote collaboration",
            "Remote maintenance",
            "Shared understanding",
            "Team members",
            "User study"
        ],
        "doc_abstract": "© 2021, The Author(s), under exclusive licence to Springer-Verlag France SAS, part of Springer Nature.Problem solving in Industry 4.0 often requires collaboration among remote team members, which face increased complexity on their daily tasks and require mechanisms with adaptive capabilities to share and combine knowledge. Augmented Reality (AR) is one of the most promising solutions, allowing taking advantage from seamless integration of virtual and real-world objects, which can be used to provide a shared understanding of the task and context. In this regard, most research works, so far, have been devoted to explore and evolve the necessary technology. However, it is now important to revisit the subject of remote collaboration in relation with AR to understand how much of the collaborative effort can already be supported and identify gaps that should inform further research. In line with this mindset, we adopted a user-centered approach with partners from the industry sector, including participatory design and a focus group with domain experts to probe how AR could provide solutions to support their collaborative efforts. We focused on using tangible artifacts in the form of storyboards to create a shared understanding with target users in remote collaboration. Afterwards, we identify a set of requirements, which we materialize through the design and creation of a collaborative prototype based on sharing of enhanced AR annotations. Finally, we present and discuss the results from a case study on a maintenance context, which provides interesting insights that can be applied to other remote settings, thus facilitating the digitization of the industry sector.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing augmented reality visualization methods for assembly procedures",
        "doc_scopus_id": "85109364932",
        "doc_doi": "10.1007/s10055-021-00557-8",
        "doc_eid": "2-s2.0-85109364932",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Assembly process",
            "Head mounted displays",
            "Industrial scenarios",
            "Physical workloads",
            "Product demand",
            "Reality visualization",
            "User satisfaction",
            "User study"
        ],
        "doc_abstract": "© 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.Assembly processes require now more than ever a systematic way to improve efficiency complying with increasing product demand. Several industrial scenarios have been using augmented reality (AR) to enhance environments with different types of information and influence the overall user satisfaction and performance. The purpose of this work is to evaluate three different AR-based methods that can be used to support users during the execution of assembly procedures. The AR methods evaluated are handheld mobile AR, indirect AR (showing the augmented scene on a monitor) and see-through head-mounted display. A user study was performed to assess performance, mental and physical workload, as well as acceptance of the aforementioned methods. Results from a thirty participants study did not reveal a best method in terms of performance and user preference, showing that all methods are adequate to support users. However, the study highlights the strengths and weaknesses of each method, which may lead to potential advantages in specific use cases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A vision for contextualized evaluation of remote collaboration supported by AR",
        "doc_scopus_id": "85118343204",
        "doc_doi": "10.1016/j.cag.2021.10.009",
        "doc_eid": "2-s2.0-85118343204",
        "doc_date": "2022-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Characterization collaborative process",
            "Collaboration efforts",
            "Collaborative process",
            "Distributed teams",
            "Evaluation toolkit",
            "Guideline",
            "Knowledge retention",
            "Remote collaboration",
            "Team members",
            "User study"
        ],
        "doc_abstract": "© 2021 Elsevier LtdRemote collaboration using Augmented Reality (AR) has potential to support physically distributed team-members that need to achieve a common goal by increasing knowledge retention, improving understanding and awareness of the problem and its context. In this vein, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Thus, characterization and evaluation of the collaborative process is paramount, but a particularly challenging endeavor, due to the multitude of aspects that define the collaboration effort. In this context, the work presented here contributes with a critical analysis, discussing current evaluation efforts, identifying limitations and opportunities. Then, we outline a conceptual framework to support researchers in conducting evaluations in a more structured manner. To instrument this vision, an evaluation toolkit is proposed to support contextual data collection and analysis in such scenarios and obtain an additional perspective on selected dimensions of collaboration. We illustrate the usefulness and versatility of the toolkit through a case study on remote maintenance, comparing two distinct methods: sharing of video and AR-based annotations. Last, we discuss the results obtained, showing the proposed vision allows to have an additional level of insights to better understand what happened, eliciting a more complete characterization of the work effort.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2021-10-13 2021-10-13 2022-02-25 2022-02-25 2022-06-01T12:50:43 S0097-8493(21)00220-X S009784932100220X 10.1016/j.cag.2021.10.009 S300 S300.2 FULL-TEXT 2022-06-09T18:22:28.119019Z 0 0 20220201 20220228 2022 2021-10-13T16:42:28.006423Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst pubtype ref specialabst 0097-8493 00978493 true 102 102 C Volume 102 22 413 425 413 425 202202 February 2022 2022-02-01 2022-02-28 2022 Special Section on Adv Graphics+Interaction; Edited by Nuno Rodrigues, Daniel Mendes, Luís Paulo Santos, Kadi Bouatouch article fla © 2021 Elsevier Ltd. All rights reserved. AVISIONFORCONTEXTUALIZEDEVALUATIONREMOTECOLLABORATIONSUPPORTEDBYAR MARQUES B 1 Introduction 2 Background and challenges for evaluation of AR-based remote collaboration 3 A vision for contextualized collaborative AR evaluation 3.1 Evaluation purpose 3.2 Team and collaborative tasks 3.3 Experimental setup and design 3.4 Contextualized data gathering 3.5 Analysis and report 4 Toolkit for distributed evaluations using AR 5 User study on a remote maintenance scenario 5.1 Experimental setup 5.1.1 Video chat tool 5.1.2 AR-based annotation tool 5.2 Experimental design 5.3 Tasks 5.4 Measurements 5.5 Procedure 5.6 Participants 6 Results and discussion 6.1 Overall total time and task time 6.2 Overview of the collaborative process 6.3 Participants preferences and opinion 6.4 Final remarks 7 Conclusions and future work CRediT authorship contribution statement Acknowledgments References KIM 2020 529 538 K CONFERENCEVIRTUALREALITY3DUSERINTERFACES REDUCINGTASKLOADEMBODIEDINTELLIGENTVIRTUALASSISTANTFORIMPROVEDPERFORMANCEINCOLLABORATIVEDECISIONMAKING KIM 2018 2947 2962 K KIM 2018 569 607 S GERVASI 2020 841 865 R LUKOSCH 2015 515 525 S BILLINGHURST 2015 73 272 M ENS 2019 81 98 B WANG 2021 102071 P BOTTANI 2019 284 310 E WANG 2016 1 22 X LEE 2020 343 352 G IEEECONFERENCEVIRTUALREALITY3DUSERINTERFACES AUSERSTUDYVIEWSHARINGTECHNIQUESFORONETOMANYMIXEDREALITYCOLLABORATIONS LUDWIG 2021 119 167 T GUREVICH 2015 527 562 P KIM 2018 6034 6056 S KIM 2020 321 335 S NEALE 2004 112 121 D PROCEEDINGS2004ACMCONFERENCECOMPUTERSUPPORTEDCOOPERATIVEWORK EVALUATINGCOMPUTERSUPPORTEDCOOPERATIVEWORKMODELSFRAMEWORKS HAMADACHE 2009 206 221 K GROUPWAREDESIGNIMPLEMENTATIONUSE STRATEGIESTAXONOMYTAILORINGYOURCSCWEVALUATION ANTUNES 2014 146 169 P BELEN 2019 181 R MARQUES 2021 1 17 B BAI 2012 450 460 Z DEY 2018 37 A ANTON 2018 77 88 D PIUMSOMBOON 2018 2974 2982 T ARAUJO 2004 139 150 R MARQUES 2021 1 18 B MARQUES 2021 1 6 B INFORMATIONVISUALIZATIONIV VISUALLYEXPLORINGACOLLABORATIVEAUGMENTEDREALITYTAXONOMY IZARD 2007 260 280 C BARNUM 2010 C USABILITYTESTINGESSENTIALSREADYSETTEST GUTWIN 1999 243 281 C KIM 2018 6034 6056 S HUANG 2019 428 438 W PATEL 2012 1 26 H PIUMSOMBOON 2019 T MADEIRA 2021 83 89 T HUMANSYSTEMSENGINEERINGDESIGNIII EXPLORINGANNOTATIONSHANDTRACKINGINAUGMENTEDREALITYFORREMOTECOLLABORATION MARQUESX2022X413 MARQUESX2022X413X425 MARQUESX2022X413XB MARQUESX2022X413X425XB 2024-02-25T00:00:00.000Z 2024-02-25T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 0 2022-06-06T22:43:45.902Z University of Aveiro UA Universidade de Aveiro FCT SFRH/BD/143276/2019 UID/CEC/00127/2019 FCT Fundação para a Ciência e a Tecnologia European Regional Development Fund ERDF European Regional Development Fund We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [UID/CEC/00127/2019]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [ UID/CEC/00127/2019 ]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. item S0097-8493(21)00220-X S009784932100220X 10.1016/j.cag.2021.10.009 271576 2022-06-01T12:26:47.516259Z 2022-02-01 2022-02-28 true 3678202 MAIN 13 63615 849 656 IMAGE-WEB-PDF 1 gr3 43612 433 376 gr9 52635 306 376 gr1 28469 346 376 gr14 28871 413 376 gr12 42537 501 376 gr11 27866 474 376 gr7 39516 303 376 gr4 24720 434 376 gr15 11763 198 373 gr6 62725 542 376 ga1 true 16462 245 267 gr5 43801 320 376 gr2 78591 583 376 gr13 87515 441 376 gr8 45540 303 376 gr10 16187 255 369 gr3 6255 163 142 gr9 28164 164 201 gr1 4691 164 178 gr14 4530 164 149 gr12 4835 164 123 gr11 4118 164 130 gr7 20550 164 204 gr4 4480 164 142 gr15 3721 117 219 gr6 7321 163 113 ga1 true 5480 164 179 gr5 9004 164 192 gr2 9859 163 105 gr13 10706 163 139 gr8 22695 164 203 gr10 5254 151 219 gr3 341412 1919 1667 gr9 491740 1357 1667 gr1 192280 1532 1667 gr14 182688 1833 1667 gr12 307513 2220 1667 gr11 187668 2101 1667 gr7 367677 1342 1667 gr4 189805 1924 1667 gr15 78304 880 1654 gr6 482554 2402 1667 ga1 true 110211 1085 1183 gr5 291127 1420 1667 gr2 654708 2584 1667 gr13 594548 1957 1667 gr8 432457 1343 1667 gr10 110543 1129 1635 CAG 3442 S0097-8493(21)00220-X 10.1016/j.cag.2021.10.009 Elsevier Ltd Fig. 1 Conceptual framework for helping researchers evaluate of AR-remote collaboration in a more structured manner. Fig. 2 Scenario of remote collaboration using an AR-based tool instrumented with the CAPTURE toolkit: 1 — On-site technician requiring assistance; 2- Expert using AR to provide remote guidance; 3 - Researcher(s) following the evaluation process; 4- Distributed multi-user data gathering; 5- Contextual data collection based on existing dimensions of collaboration [37]; 6- Evaluation data storage; 7- Visualization dashboard for analysis of the collaborative process. Fig. 3 CAPTURE toolkit — example of pre-defined scenes associated with post-task measurements. Top — questionnaire regarding the collaboration process; Bottom — questionnaire regarding the collaborative tool. Fig. 4 CAPTURE toolkit — example of pre-defined scenes associated with selected dimensions of collaboration. Top — characteristics of the Team; Bottom — characteristics of the Task. Fig. 5 CAPTURE architecture. The toolkit can be integrated into a collaborative tool via visual editor. All data collected during collaboration is stored in a central server, which can be analyzed during post-task analysis through the visualization dashboard. Fig. 6 Overview of the CAPTURE toolkit assets: ready to use scene prefabs and editable scripts, which researchers may modify according to the aspects of collaboration being considered for the evaluation. Fig. 7 Video Chat tool for remote collaboration. Fig. 8 AR-based Annotation tool for remote collaboration. Fig. 9 Illustration of some of the completion stages associated with the maintenance tasks used in the study: 1- replace interconnected components; 2- plug and unplug some energy modules; 3- remove a specific sensor; 4- integrate new components into the equipment. Fig. 10 Total time and task time with the two conditions (in minutes). C1: video chat tool; C2: AR-based annotation tool. Fig. 11 Overview of the collaborative process outcomes for all teams during a scenario of remote maintenance, including all the selected measures collected: easy to share ideas properly, as well as communicate, level of attentional allocation, information understanding, mental effort, enjoyment, spatial presence. Top — C1: video chat tool; Bottom — C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1 — Low; 7 — High. Fig. 12 Collaborative process for the same team during remote maintenance using the two tools: Top — C1: video chat tool; Bottom — C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1 — Low; 7 — High. Fig. 13 Participants total reaction cards regarding the collaborative tools. C1: video chat tool; C2: AR-based annotation tool. A larger font size means that the word was selected by more participants (higher frequency). Red — negative meaning; gray — neutral meaning; green — positive meaning [41]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 14 Participants emotional state before (top) and after (bottom) the tasks for each condition. C1: video chat tool; C2: AR-based annotation tool. Fig. 15 Participants satisfaction towards the tools. C1: video chat tool; C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1- Low; 7- High. Special Section on Adv Graphics+Interaction A vision for contextualized evaluation of remote collaboration supported by AR Bernardo Marques Conceptualization Methodology Software Formal analysis Investigation Resources Writing – original draft Writing – review & editing Visualization Project administration Funding acquisition ⁎ Samuel Silva Conceptualization Methodology Writing – original draft Writing – review & editing Visualization António Teixeira Conceptualization Methodology Writing – original draft Writing – review & editing Visualization Paulo Dias Conceptualization Resources Writing – original draft Writing – review & editing Visualization Supervision Funding acquisition Beatriz Sousa Santos Conceptualization Writing – original draft Writing – review & editing Visualization Supervision Funding acquisition IEETA, DETI, Universidade de Aveiro, Portugal IEETA, DETI, Universidade de Aveiro Portugal DETI, IEETA, Universidade de Aveiro ⁎ Corresponding author. Remote collaboration using Augmented Reality (AR) has potential to support physically distributed team-members that need to achieve a common goal by increasing knowledge retention, improving understanding and awareness of the problem and its context. In this vein, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Thus, characterization and evaluation of the collaborative process is paramount, but a particularly challenging endeavor, due to the multitude of aspects that define the collaboration effort. In this context, the work presented here contributes with a critical analysis, discussing current evaluation efforts, identifying limitations and opportunities. Then, we outline a conceptual framework to support researchers in conducting evaluations in a more structured manner. To instrument this vision, an evaluation toolkit is proposed to support contextual data collection and analysis in such scenarios and obtain an additional perspective on selected dimensions of collaboration. We illustrate the usefulness and versatility of the toolkit through a case study on remote maintenance, comparing two distinct methods: sharing of video and AR-based annotations. Last, we discuss the results obtained, showing the proposed vision allows to have an additional level of insights to better understand what happened, eliciting a more complete characterization of the work effort. Graphical abstract Keywords Remote collaboration Augmented Reality Characterization collaborative process Guidelines Evaluation toolkit User study 1 Introduction Collaboration has the potential to achieve more effective solutions for challenging problems [1]. It has evolved from simple co-located situations to more complex remote scenarios, encompassing several team members with different experiences, expertise’s and multidisciplinary backgrounds. Remote collaboration can be described as the process of joint and interdependent activities between physically distributed collaborators performed to achieve a common goal [2–4]. This activity has become essential in many situations, as is the case of industrial, medical, and educational domains, among others [5,6]. To address such activities, remote solutions have been growing in terms of scale, complexity, and interdisciplinarity, entailing not only the mastery of multiple domains of knowledge, but also a strong level of proficiency in each [5,6]. Scenarios of remote collaboration imply that collaborators establish a joint effort to align and integrate their activities in a seamless manner. To address this, and overcome the fact team-members do not share a common space/world, there is an increasing interest in using Augmented Reality (AR) in this context [7–10]. Remote collaboration mediated by AR combines the advantages of virtual environments and the seamless integration with the real-world objects and other collaborators by overlying responsive computer-generated information on top of the real-world environment [2,11,12], allowing to establish a common ground, analogous to their understanding of the physical space, i.e., serve as a basis for situation mapping, allowing identification of issues, and making assumptions and beliefs visible [13–16]. These solutions can be used to empower workers that require knowledge from professionals unavailable on-site [17]. Remote experts can provide guidance, highlight specific areas of interest or share real-time spatial information [9,10,14,18] in the form of visual communication cues, e.g., pointers, annotations, hand gestures, among others [3,9,17,19–22]. These solutions can better support analysis, discussion and resolution of complex problems and situations, given their ability to enhance alertness, awareness, and understanding of the situation [23]. In the past decade, the community has been particularly active in this domain, concentrating efforts on creating the enabling technology to support the design and creation of an AR-based shared understanding. As the field matures and with the growing number of prototypes, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Therefore, evaluating such scenarios becomes an essential, but difficult endeavor [23–26], given the lack of methods and frameworks to guide the characterization of the collaborative process [9,27–29]. This is substantiated by Bai et al. reporting that ”it can be hard to isolate the factors that are specifically relevant to collaboration” [30]. In fact, this is further evident in remote scenarios, since the logistics associated with carrying out evaluations in these multifaceted contexts is even more demanding due to a significant number of variables that may affect the way teams collaborate [9,27]. Ratcliffe et al. report that ”remote settings introduce additional uncontrolled variables that need to be considered by researchers, such as potential unknown distractions, (...) participants and their motivation, and issues with remote environmental spaces” [31]. Also, Dey et al. suggest the existence of ”opportunities for increased user studies in collaboration” and the need for ”a wider range of evaluation methods” [32]. In this vein, Ens et al. emphasize that ” frameworks for describing groupware and MR systems are not sufficient to characterize how collaboration occurs through this new medium” [9]. Additionally, Ratcliffe et al. suggest that ”the infrastructure for collecting and storing this (mass) of XR data remotely is currently not fully implemented, and we are not aware of any end-to-end standardized framework” [31]. As such, conducting thorough evaluations is paramount to retrieve the necessary data for more comprehensive analysis that help provide a better perspective on the different factors of collaboration supported by AR. Hence, integration of proper characterization and evaluation methods, covering different contexts of use and tasks are of utmost importance. In this paper, we analyze existing evaluation efforts on remote collaboration using AR to provide a high-level overview. Motivated by the challenges reported, we present a conceptual framework for supporting researchers in obtaining an additional perspective on several dimensions of collaboration. Then, we propose the CAPTURE toolkit, a first instantiation towards the vision proposed, aiming to provide a strategy that monitors data concerning the level of collaboration, behavior and performance of each intervening party, individual and as a team, as well as contextual data. To illustrate the advantages of the framework, the toolkit usefulness and versatility are demonstrated through a case study in a remote maintenance scenario, comparing two distinct methods: sharing of video and AR-based annotations. Then, the results obtained are discussed, showing that the proposed vision allows having an additional level of insights to better understand what happened, eliciting a more complete characterization of the collaborative work effort. The remainder of this paper is organized as follows. Section 2 overviews existing evaluation efforts on remote collaboration mediated by AR. Section 3 proposes our conceptual framework for essential aspects that must be addressed. Section 4 describes the CAPTURE toolkit and Section 5 applies it through a user study on a remote maintenance scenario. Section 6 presents and discusses the main results. Finally, concluding remarks and future research opportunities are drawn in Section 7. 2 Background and challenges for evaluation of AR-based remote collaboration This section reports existing evaluation efforts addressing collaborative AR user studies. The goal is to understand how evaluation has been conducted in such scenarios, provide a high-level overview, and identify existing challenges and gaps. According to Merino et al. ”as MR/AR technologies become more mature, questions that involve human aspects will gain focus in MR/AR research. Consequently, we expect that future MR/AR papers will elaborate on human-centered evaluations that involve not only the analysis of user performance and user experience, but also the analysis of other scenarios, like understanding the role of MR/AR in working places and in communication and collaboration” [26]. However, there is no standard methodology for characterization and evaluation, specifically tailored to assess how remote collaboration occurs through AR. The literature shows that studies that evaluate their solutions rely on single-user methods, mainly focused on the comparison of technological aspects or interaction mechanisms, which are not the most adequate for multifaceted solutions that aim to support distributed team collaboration [2,9,25,27,29,32]. Also, most studies focus exclusively on the performance of one collaborator, i.e., on-site, or remote. This means evaluation usually does not consider interaction, and communication among team-members, and is not conducted in distributed scenarios, as should be the case to establish experimental conditions closer to real scenarios, Likewise, focus is given to the technological aspects of the solution being used, as well as to quantifying the effectiveness in completing the tasks, which mostly lack difficulty, diversity and ecological validity [2,3,7,29,30,32]. Moreover, the majority of studies are formal, conducted in laboratories, collecting objective and subjective data at the end of the tasks through standard practices with fixed answers like scale-based questionnaires (e.g., System Usability Scale (SUS), NASA Task Load Index (TLX), among others) or direct observation [7,9,19,27,32–34]. Adding to these data, only a reduced set of studies include measurements collected during the collaborative process (e.g., task duration and error/accuracy), as well as explicit communication (e.g., spoken messages or gestural cues), ease of collaboration and others [26,30]. While this is the case, the collection of more contextual and behavioral data is often not considered or hindered due to the complexity it entails regarding acquisition, processing and analysis, and more important, the lack of guidelines to inform researchers on what dimensions of collaboration should be collected and how. Therefore, current frameworks are not tailored to characterize how collaboration mediated by AR occurs [9,25,28,31], falling short to retrieve the necessary amount of data for more comprehensive analysis. As a consequence, without the appropriate methods, the research community does not accumulate enough experience to improve the work effort [3,9,25,26,28–30,32,35]. Thus, as the field of remote collaboration using AR matures, evaluation needs to move beyond a simple assessment of how the technology works, as it becomes essential to understand different aspects of collaboration itself, including how teams work together, how communication happens, how AR is used to create a common ground, among others. This should provide a richer output of the evaluation stage, balancing the design against requirements and leading to a more informed refinement of the context of use and system features, e.g., in line with the life-cycle for Human-Centered Design (HCD) described in the principles and activities associated with the [ISO 9241-210]. 1 1 iso.org/standard/77520.html. Given the challenges and constraints involved in evaluating the way collaboration occurs through AR, we argue it is paramount to address a set of important topics, namely: 1- conduct more collaborative-centric evaluations, i.e., move beyond usability testing, which fails to obtain a more comprehensive understanding of the work effort. Equally important, 2- develop evaluation strategies including contextual data collection and visualization, i.e., collect a richer data set to better understand how AR contributes to the collaborative process, in order to shape more effective collaboration. 3 A vision for contextualized collaborative AR evaluation The area being addressed in this work is part of a complex phenomenon. To allow answering existing problems, it is necessary to systematize knowledge and perspectives, so that it can be applied transversely. For this, it is necessary the creation of evaluation frameworks, i.e., capitalize on the hierarchies and dimensions of collaboration from ontologies and taxonomies, as well as the development of tools that allow contextualizing the use of collaborative solutions. Taking into account the challenges and needs identified in the previous section, Fig. 1 structurally presents an evaluation framework of the collaborative process when using a given tool, with a proposal of several levels of information that must be considered for contextualization, derived through a HCD methodology. In this effort, we argue that the evaluation process must be addressed by the research community, namely the definition of the evaluation purpose, as well as the team characteristics and the details of the collaborative tasks. Also, carefully establish the experimental setup and design. Equally important, explore contextualized data gathering and analysis, which requires the creation of novel tools. This last, being the aspect this work further contributes. Next, we elaborate on these with more detail. 3.1 Evaluation purpose To begin, the scope must be defined, taking into account existing dimensions of collaboration to clarify what will be evaluated, so that relevant research questions are formulated in the design phase and answered in the evaluation analysis [24]. 3.2 Team and collaborative tasks Also important, determine the team-members’ characteristics, i.e., role structure, coupling level, life-span, technology literacy and multidisciplinarity. In this context, participants with different ages, perspectives, motivations, and multidisciplinary background should be considered, which might lead to more relevant insights. Moreover, understanding of VR/AR, as well as remote tools is a benefit for the adaptation, thus removing the ’wow factor’ that makes participants feel excitement or admiration towards such technologies. Besides, participants should only perform one role, i.e., on-site or remote, so that they are only exposed to a set of tasks, concerns and responsibilities. Furthermore, the collaborative tasks goals must be clearly established including which team-members will be accountable for achieving each completion stage. It is also important to consider if the tasks are performed indoor, outdoor, or mixed between the two; A balance must be kept between task complexity and duration. Tasks must be complex and long enough to encourage interaction through AR. However, longer tasks may cause fatigue or boredom, affecting the evaluation outcomes. Equally relevant, tasks can introduce deliberated drawbacks, i.e., incorrect, contradictory, vague or missing information, to force more complex situations and elicit collaboration. 3.3 Experimental setup and design Establish the experimental setup and design are equally key. When considering prototypes, evaluation under laboratory settings should be used. Afterwards, when considering more mature solutions, evaluation should be made in the field, with real stakeholders and domain experts, moving beyond typical laboratory settings to increase the ecological validity of the evaluations. Regarding the environment, two separated rooms in the same/ different building(s) should be used. Otherwise, participants must be separated by some kind of physical barrier when in the same room. Furthermore, an adaptation period must be provided so that participants can explore the technology possibilities before the tasks, individually and as a team. Besides, a proper amount of time must be defined for other aspects, e.g., presentation of the study, pre- and post-task questionnaires, team interview, and others. 3.4 Contextualized data gathering As well observed by Merino et al. [26], future works on Mixed and Augmented Reality (MR/AR) will elaborate on human-centered evaluations involving not only the analysis of user experience and performance, but also understanding the role of such technologies in working places, in communication and in collaboration. In this scope, contextual information helps inform the conditions in which the collaboration took place. It can also be used for understanding interaction and communication changes, namely if the surroundings affected the way teams collaborate, in such a way that they needed to adapt it. Also, it helps portrait the conditions in which team-members performed a given action, received information or requested assistance, which can be used to assess uncommon situations or identify patterns that can lead to new understanding of a given artifact, as well as identify new research opportunities. Without comprehending contextual information, it becomes difficult to assess important variables related to the collaborative process, which means the findings reported may be misleading or of limited value. Hence, these aspects have an important impact on how the studies must be prepared and how they were conducted, influencing situation understanding, team-members communication, performance, and usage of AR. Literature shows that a better evaluation process can be supported by improved data collection and data visualization tools [35,36]. In particular, the following factors are crucial and must be taken in account to better understand the real impact of each aspect in the collaborative effort: team, tasks, context and AR-based tool [28]. Through these, a wide range of information is provided when performing judgment over the results and establishing conclusions. Therefore, data collection while team-members collaborate, considering different forms of measurement according to the evaluation goals is paramount and should include: • pre-task measures like demographic questionnaires (e.g., age, gender, occupation, years of experience, etc.), information on participants background: if they knew each other, previous experience with VR/AR technologies and remote tools, among other aspects; • runtime measures may comprise: – performance metrics including overall duration of specific events, number and type of errors; number and type of interactions; frequency of using each feature of the tool; screenshots of the enhanced content; – behavior metrics including conversational analysis (e.g., frequency of conversational turns, number of questions or interruptions, and dialog length, duration of overlapping speech); physical movement around the environment; number of hand gestures; physiological variables and emotions; eye gaze; – collaboration metrics including the level of effectiveness; perception; interest; engagement; awareness; togetherness; mental stress; – researchers may collect audio (or video) and register interesting events including the type (e.g., guide, request, express, propose) and frequency of communication (e.g., never, sometimes, often, continuously), if the goals were accomplished, difficulties detected, if the participants requested assistance and how many types, among other relevant aspects. • post-task measures can encompass: – register usability towards the tools(s) used; – record collaboration metrics including the level of effectiveness; perception; interest; engagement; awareness; togetherness; mental stress, etc; – collect participants reactions, opinions and preferences through semi-structured interviews. 3.5 Analysis and report The use of more contextualized approaches will provide ground to improve how research is analyzed and reported. Hence, increasing the awareness of researchers about the different dimensions of collaboration and the need to improve how the nuances associated to the collaborative effort are described. In turn, a more systematic characterization can lead to a community setting that enables easier communication, understanding, reflection, comparison and refining, building on existing research while fostering harmonization of perspectives for the field. In this context, some noticeable recommendations are: • researchers can profit from the outcomes generated to improve the level of detail provided in their reports; • the collaborative context needs to be widely described, allowing the creation of a better understanding of the surrounding conditions, including relations between individuals, their interconnection as a team, how AR was used, the characteristics of the environment, and others; • the outcomes can help identify limitations and promising functionalities regarding AR, providing opportunities for future work in a technical level; • the insights obtained may also lead to improvements in individual behavior and team collaboration in specific procedures and tasks over longer periods of time. 4 Toolkit for distributed evaluations using AR Given the challenges in evaluating the way remote collaboration occurs, the absence of frameworks and tools, this section describes CAPTURE - Contextual dAta Platform for remoTe aUgmented Reality Evaluation, a first instantiation towards addressing the vision previously described, in particular the need to include more contextual data in the evaluation of the collaborative process (Fig. 2), following the conceptual model [28]. To inform the conceptualization/development, we conducted brainstorm sessions with domain experts (academics, including faculty members and researchers) sharing several years of expertise in HCI, VR/AR, Visualization and remote collaboration, who co-authored multiple publications, and projects on these subjects. Hence, the toolkit must support: • data gathering at distributed locations in synchronous and asynchronous manner; • explicit input on different dimensions of collaboration, following a taxonomy for Collaborative AR [38,39] and an evaluation ontology for remote scenarios [37]; • data collection regarding team interaction, custom logging and registration of interesting events according to the selected scenarios of remote collaboration; • easy instrumentation into remote tools by providing ready to use scripts and prefabs for non-experts in programming, i.e., each process can be configured via visual editors; • modularity to ensure adaptation to different goals; • data storage and aggregation via a centralized server; • post-task analysis through a visualization dashboard. To elaborate, for team-members, the CAPTURE toolkit provides native off-the-shelf modules to support explicit input and data gathering regarding (Figs. 2–4): • individual and team profile: demographic data, knowledge of other collaborators, participants background, emotional state [40], experience with AR and remote tools; • collaborative context: details on the task and the environment, like the number of completion stages, resources available or the amount of persons, movement and noise in the surrounding space; • list of events: task duration, augmented content shared and received, and other relevant occurrences; • pre-defined measures: characteristics associated to the collaborative process, including, but not limited to, easy to communicate or express ideas and the level of spatial presence, enjoyment, mental effort, information understanding, attention allocation or others (Fig. 3 - top). Also, the Microsoft reaction card methodology [41] to have a grasp on team-members reaction towards the tool used for shared understanding (Fig. 3 - bottom); • interaction with the collaborative tool: duration of the collaborative process and specific events, e.g., when creation of content is started or completed, number and type of interactions, frequency of using each feature, as well as captures of the augmented instructions being shared. Regarding pre-defined measures, the aspects of collaboration proposed are the result of carefully survey existing literature to create a list of important topics facing the lack of methodologies and frameworks. This list was presented to the experts, who had an important role in selecting, analyzing and filtering said topics of collaboration by voting about the ones they considered being more relevant. To elaborate, we took inspiration from the questionnaires used by [3,42–46], as well as the works by [11,19,21,22,47–50]. Nevertheless, other aspects of collaboration can be considered according to the evaluation scope due to the inherent flexibility provided by the CAPTURE toolkit implementation, as described below. As for the researcher(s), the toolkit provides native off-the-shelf modules to support explicit input regarding (Fig. 2 - 5): • Study: area of application, research context and study type; • Time: synchronicity, duration and predictability; • Team: distribution, role structure, size, life-span, turnover, multidisciplinarity, technology usage, homogeneity of abilities, and knowledge of others (Fig. 4 - top); • Task: scope and type of task, interdependence, amount of information and movement required to fulfill the task, number of completion stages, resources necessary to achieve the goal (Fig. 4 - bottom); • User Actuation: capacity to passive-view, interact/explore, share/create, as well as level of symmetry; • Communication: structure, mode, intent, frequency and duration; • Environment: amount of noise, level of brightness, number of persons in the environment, weather conditions and resources available; • Notes: interesting events, notes, comments or difficulties, as well as if the goals were achieved and the amount of physical movement conducted by the team-members. At the system level, CAPTURE consists of a Unity Package that can easily be added to existing collaborative solutions in Unity. All data gathered from the different team-members and researcher(s) during collaboration sessions is stored in a central server for post-evaluation analysis through a visualization dashboard (Fig. 5), which allows reviewing the work effort of a particular team or set of teams, as well as compare different tools, if that is the evaluation scope [51]. The modules of the proposed toolkit can be integrated into existing remote tools via visual editors, i.e., with minimal need for programming skills (Fig. 6). It is possible to drag and drop ready to use prefabs and editable scripts into Unity 3D projects, which can be modified according to the evaluation scope in the inspector module. Fig. 6 illustrates the example of the collaborative process script, which researchers can manually edit (set the number of elements, add relevant aspects of collaboration to be assessed, etc.) according to the evaluation scope. This dynamic approach allows researchers to re-use scripts over different evaluation sessions according to the collaborative effort being considered. For development, Unity 3D was used based on C# scripts. Communication between each instance is performed over Wi-Fi through calls to a PHP server. In short, the field needs to have more contextualized evaluation strategies, allowing to learn more regarding how technology address the collaborative process. All of this can support an effort towards systematized data, which may support the proposal of guidelines in the future, resulting from the experience and knowledge accumulated through the analysis from multiple research teams and different technology approaches with contextualized information. This effort will allow to use these recommendations to jump-start the quality of current and novel solutions right from the very beginning of its conceptualization, which have already been proven useful in remote scenarios. 5 User study on a remote maintenance scenario A user study was conducted to compare the collaborative process of distributed teams using two distinct tools when instrumented with CAPTURE: Video Chat and AR-based Annotations. These were proposed following a user-centered approach with partners from the industry sector to probe how AR could provide solutions to support their collaborative needs. 5.1 Experimental setup To create a common ground between distributed team- members, two distinct methods were provided: a video chat tool and an AR-based annotation tool. Next, a brief description of the main features of each tool is provided. To clarify, the hardware used was the same for both methods, only the characteristics of the tool changed. Also, both tools were developed using the Unity 3D game engine, based on C# scripts. Communication was provided over Wi-Fi through WebRTC calls to a dedicated server. To place the augmented content in the real-world environment, we used the Vuforia library. 5.1.1 Video chat tool The first method uses video chat features to provide support (Fig. 7). On-site participants can point a handheld device to the situation context, which is shared though live video stream with the remote expert. In this context, the face of the expert is visible at all times, while the on-site participant may change between showing the task context or his face using the back and front cameras of the device. Besides, team-members can share text messages using the chat to ensure important messages are kept visible. Using these features, team-members may communicate and discuss the content being captured to express the main difficulties, identify areas of interest or the remote expert to inform where to act and what to do. 5.1.2 AR-based annotation tool The second method uses AR-based annotations as additional layers of information (Fig. 8). On-site participants can point a handheld device to capture the situation context. Using audio communication and annotation features like drawing, placing pre-defined shapes or notes, as well as sorting annotations, the participant can edit the capture to illustrate difficulties, identify specific areas of interest or indicate questions. Then, the capture is sent to the remote expert to suggest instructions accordingly i.e., inform where to act, and what to do, using similar annotation features. Afterwards, the on-site participant receives the annotations. The handheld device can be placed on top of a surface to follow the instructions in a hands-free setting. At any time, it can be picked up to perform an augmentation of the annotations, by re-aligning with the real world. 5.2 Experimental design A within-group experimental design was used. The null hypothesis (H0) considered was that the two experimental conditions are equally usable and acceptable to conduct the selected maintenance tasks. The independent variable was the information display method provided during the collaborative process, with two levels corresponding to the experimental conditions: C1 — Video Chat and C2 — AR-based Annotations. For both experimental conditions, the tools used provided a similar level of user actuation for both team-members, having identical features to view (C1 and C2), create, share and interact with augmented content (C2). Performance measures and participants’ opinion were the dependent variables. Participants’ demographic data, as well as previous experience with AR and collaborative tools were registered as secondary variables. 5.3 Tasks We focused on a case study where an on-site participant using a handheld device had to perform a maintenance procedure while being assisted from a remote expert using a computer. The tasks require accomplishing the following steps (Fig. 9): 1- replace interconnected components, 2- plug and unplug some energy modules, 3- remove a specific sensor, as well as 4- integrate new components into the equipment. For each condition, different tasks were used to minimize bias, i.e., learning effect. Nevertheless, we defined these tasks based on feedback from our industry partners regarding their usual work activities and needs, while ensuring a similar level of difficulty and resources. Each task was a defined-problem with 4 completion stages, forcing team-members to communicate in a continuous way while acting alternately (reciprocal interdependence) in an indoor environment with controlled illumination conditions and reduced noise. Besides the participants and researchers, no other individuals were present. The on-site participant needed to use different hand tools to perform the procedures, although low physical movement was required. 5.4 Measurements All data was collected through the CAPTURE toolkit for all conditions, including standard measures found in literature like task performance based on the overall total time, i.e., time needed to complete the tasks, answer to questionnaires and participation in a brief interview, as well as task time, i.e., time required for successfully fulfill the task in a collaborative manner. Besides, novel measures, taking advantage of the toolkit off-the-shelf modules, i.e., information on selected dimensions of collaboration (e.g., time, team; task; user actuation, communication, environment); the overview of the collaborative process (e.g., easy to communicate or express ideas, level of spatial presence, enjoyment, mental effort, information understanding and attention allocation) at the end of the tasks; participants emotional state, before and after the task fulfillment; participants preferences and opinion, also at the end. Hence, the toolkit was integrated into an existing video chat tool, as well as an AR-based tool [52] using stabilized annotations, following prior work with partners from the Industry sector. 5.5 Procedure Participants were instructed on the experimental setup, the tasks and gave their informed consent. Then, they were introduced to both tools and a time for adaptation was provided. Participants would act as on-site technicians with condition C1 and then C2, always in this order, while a researcher was the remote counterpart to ensure the instructions were correctly transmitted. We used this approach to facilitate collaboration, as having participants also act as the remote counterpart would add an additional level of complexity, which we believe was not necessary. Since this role was ensured by one of the researchers, we recognize that it is not the same as having a participant, but still allows to have a granular view of the work effort, since not all collaborative processes are created equal. Hence, the researcher also followed the same procedure during the evaluation. We argue that the data collected from this role convey a variability in the way collaboration occurred and in what works or not, depending on the team-members, which demonstrates the ability of the measures used to have some granularity in the evaluation of how the collaborative process took place. Participants started with a demographic questionnaire. In the next stage, they completed the maintenance tasks while observed by a researcher who assisted them if necessary, and registered any relevant event. Immediately after completing the tasks using the conditions, participants answered a post-study questionnaire regarding the collaborative process, as well as their preferences towards the tool used. Then, a small interview was conducted to understand participants’ opinion regarding their collaboration with each condition. The data collection was conducted under the guidelines of the Declaration of Helsinki. Also, all measures were followed to ensure a COVID-19 safe environment during each session of the user study. 5.6 Participants We recruited 26 participants (9 female - 34.7%), whose ages ranged from 20 to 63 years old (M = 33.1, SD = 11.7). Participants had various professions, e.g., Master and Ph.D. students, Researchers and Faculty members from different fields, as well as Software Engineers, Front-End Developers and an Assembly Line Operator. With respect to individual and team profile, 14 participants had prior experience with AR and 24 with collaborative tools. With the exception of 1 team, all collaborators had knowledge of each other prior to the study. 6 Results and discussion This section presents and discusses the main results obtained from the analysis of the data collected through CAPTURE. 6.1 Overall total time and task time As for the total duration, sessions lasted 32 min on average (SD = 3.10) using condition C1 and 28 min on average (SD = 3.03) using condition C2 (Fig. 10). Regarding task duration, it lasted 16 min on average (SD = 2.68) using condition C1 and 12 min on average (SD = 2.66) using condition C2. Therefore, participants were quicker on average to perform the tasks when using condition C2, despite having a higher data variability when compared to condition C1. 6.2 Overview of the collaborative process Regarding condition C1, participants rated the collaborative process (Likert-type scale: 1 — Low; 7 — High) as following (Fig. 11 — top): express ideas (median = 4.5), attentional allocation (median = 4), information understanding (median = 5), mental effort (median = 5), enjoyment (median = 4), communication (median = 5), spatial presence (median = 5.5). As for condition C2, participants rated the collaborative process as following (Fig. 11 - bottom): express ideas (median = 6), attentional allocation (median = 7), information understanding (median = 7), mental effort (median = 2), enjoyment (median = 6), communication (median = 6), spatial presence (median = 5). Hence, it is possible to understand that for the majority of aspects of collaboration, i.e., easy to share ideas properly, level of attention allocation, level of information understanding, level of enjoyment and easy to communicate, condition C2 was rated higher by the participants. Regarding the level of mental effort, participants rated higher condition C1, possibly due to the diminished level of attentional allocation this condition had, which lead to some communication arguing in order to understand where to perform some activities. Therefore, these results suggest that the AR-based annotation tool was better in such aspects of collaboration when compared to the video alternative. In contrast, for condition C1 the level of spatial presence was higher. This might be associated to the fact that this condition supported live video sharing between team-members, which may have an impact on participants feeling of togetherness with their collaborative counterparts, since it was possible to see the remote expert at all times during the task duration. On the other side, condition C2 provided stabilized AR-based annotations on top of captures/images of the task context. This condition did not allow to see the remote expert during the task procedures, which may have affected participants reaction towards the level of spatial presence, although not with any major difference. In this context, a smaller data variability can also be observed for easy to share ideas properly, level of information understanding, level of mental effort, easy to communicate and level of spatial presence, when analyzing the box plots of condition C1 and C2, as illustrated by Fig. 11. Through the visualization dashboard of the CAPTURE toolkit, it is possible to analyze the collaborative process at the end of an evaluation session for a specific team, or set of different teams. In particular, it is possible to analyze the aspects of collaboration obtained from the use of different tools for the elements of the same team, as explored in this study, which is illustrated in Fig. 12, through a random selection. Naturally, following the results presented above, when using condition C2, the team had a better collaborative performance when compared to the results of condition C1. Nevertheless, by analyzing the elements of each team individually, such type of visualization allows to identify aspects of collaboration that could be useful to improve over time, or that may be relevant to update in the collaborative tool being used. For example, when using condition C2, the on-site participant rated the level of spatial presence lower. This fact may suggest that in order to improve the feeling of togetherness, the AR-based annotation tool might benefit from including video sharing in its features. 6.3 Participants preferences and opinion With respect to participants experience with the tools, 44 reaction cards were selected to characterize condition C1, including 5 neutral, 9 negative and 30 with positive meaning. Likewise, 46 were selected to characterize condition C2, including 3 neutral, 1 negative and 40 with positive meaning (Fig. 13) . The following top 10 reaction cards represent participants most selected expressions to characterize each condition: C1 — accessible, collaborative, helpful, flexible, simplistic, familiar, usable, unrefined, expected and time-consuming; C2 — helpful, empowering, collaborative, appealing, easy-to-use, engaging, flexible, novel, innovative and advanced. However, when analyzing participants emotional state, collected before and after the tasks, a clearer perspective is attained. To elaborate, regarding condition C1, participants emotional state before the study varied among joy (11 out of 26), surprise (3 out of 26), excitement (8 out of 26) and contempt (4 out of 26) (Fig. 14 - top). Then, after the study, it varied among joy (7 out of 26), surprise (1 out of 26), excitement (1 out of 26) and contempt (17 out of 26) (Fig. 14 - top). As for condition C2, participants emotional state before the study varied among joy (12 out of 26), surprise (3 out of 26), excitement (7 out of 26) and contempt (4 out of 26) (Fig. 14 - bottom). Then, after the study, it varied among joy (6 out of 26), surprise (4 out of 26) and excitement (6 out of 26) (Fig. 14 - bottom). Hence, it is possible to verify that for condition C1, there was a decrease in the number of participants feeling joy, surprise and excitement at the end of the study, which lead to a significant rise associated to the emotional state of contempt. Contrarily, regarding condition C2, there were no occurrences of contempt, while joy and surprise had higher number of participants expressing those feelings. As for excitement, although the number of participants that reported such feeling is lower, it is very close to the values reported at the beginning of the study. As such, condition C2 presents significant higher values for emotions correlated with positive connotation, e.g., joy, surprise and excitement when compared to condition C1, which only presents a higher value for contempt (neutral connotation). In addition, Fig. 15 presents participants satisfaction regarding the collaborative tools used through a box plot representation, which illustrates clearly that condition C2 was preferred when compared to condition C1, following the analysis statement of participants emotional state. The interviews conducted at the end of the study also emphasize that the majority of participants preferred condition C2, since it enabled seeing non-verbal cues aligned with the task context, which they mentioned contributed to express themselves better through the augmented features, while also having a greater perception of where to perform a given action. Next, some comments by the participants are presented to provide additional context to the statement previously made: • regarding the level of attentional allocation and information understanding with condition C1, one participant emphasized the following: ”although the video tool is more familiar and quicker to start collaborate, when I needed to express myself about the equipment components or the tools I should use, that’s when I started noticing the lack of support. This lead me to repeat the same ideas in different ways to properly explain the desired goal, and the same also happened to my colleague”; • as for the level of mental effort with condition C1, another participant outlined that ”besides the use of voice, the absence of support to highlight an area of interest or express myself when using the video tool makes me prefer the use of AR-based annotations, in particular for more complex procedures, even though it was a novelty to me and I needed to learn and adapt to it”; • concerning easy to share ideas properly with condition C2, a different participant reported that “the use of AR-based annotations allowed me to interact more naturally, while also better comprehend where to perform a given action”. In regards to attentional allocation with condition C2, the same participant commented that “having the handheld device displaying the annotations near the equipment, allowed me to perform the maintenance tasks easily when compared to the video, since in this last there was no content besides the text chat I could use to remember what to do, or to confirm my actions”; • with respect to the level of mental effort and spatial presence, an additional participant mentioned the following regarding condition C2: “since I’m familiarized with remote video tools in my daily activities, I was expecting that the absence of video would affect my collaboration with the remote expert. Nevertheless, since the AR-based tool focused more on the task itself, I was engaged in such a way, that not viewing the expert did not affect me at all”. Regarding additional comments/suggestions, some participants (7 out of 26) emphasize condition C2 could help create documentation for scenarios where identical tasks may occur. Actually, the AR-based tool already supports revisiting existing annotations, a feature identified as useful by industry partners during the design of a remote maintenance support platform [52]. Nevertheless, for the case study reported, such feature was not made available, since the tasks used did not imply repeating particular activities. Another topic raised by some participants (18 out of 26) was the possible inclusion of Head Mounted Displays (HMDs), which they consider may further enhance their performance, since it supports a hands-free setting. Likewise, the AR-based tool used already supports HMD, as described in prior work [53]. Since our goal was not to compare different set-ups, we decided not to include such type of device at this moment. In addition, 4 out of 26 participants referred to possible limitations regarding the use of mobile devices as means to answer a questionnaire, since they were used to doing so on computers. They reported that for questions using drop-down menus and multiple-choice options it was easy to select the desired answer. As for the ones requiring text entry, the process could be slower and tiring. Yet, they understood the usefulness/relevance due to the fact of monitoring real-life scenarios. For example, CAPTURE is ready to be used in industry contexts, in which most technicians may find themselves without a computer. Furthermore, having these target users answering relevant questions after the tasks provides more useful insights than having them filling the questionnaires at the end of a workday on a more suitable device for writing. In this vein, we argue a compromise was required and that the solution provided takes these constraints into consideration. Nevertheless, this also opens new opportunities to propose novel forms of providing input in such scenarios. Furthermore, following the possible inclusion of HMDs in such scenarios and their similar (or even worst) capacity to answer questionnaires, this is also an open topic. Although it is possible to create text with such devices, e.g., hand interaction, literature shows it may not be the best approach. An alternative may be to use a keyboard linked to the HMD device just for answering the existing questionnaires, or perhaps, support voice/sound data collection, and later convert that into text, either via automatic or semi-automatic means. Nevertheless, more than ease of filling out questionnaires, what really matters is evaluating and monitoring collaboration in the best way possible. Therefore, as mentioned, on-the-fly feedback is essential. The choice of the most adequate input form for collecting information from the questionnaires may depend on the hardware available/being used, or on the person designing the study. Overall, the idea is that the toolkit is flexible enough to support all these options. Last, a reduced number of participants (5 out of 26) suggested viewing the remote expert, not as a basic feature, but as an option for specific cases which may help increase empathy and trust during the collaboration process. 6.4 Final remarks To summarize the added value of our proposal, and how it compares to existing approaches, the conceptual framework instantiated through the CAPTURE toolkit allows to retrieve additional amounts of contextual data, as well as selected aspects of collaboration according to the evaluation scope, (usually ignored in existing evaluations found in literature), for more comprehensive analysis using the visualization dashboard. Another aspect that must be emphasized, is the capacity to adapt to the available data collection instruments. Although self-report was used to gather the emotional response, CAPTURE can adapt to support the inclusion of external sensors (e.g., biomedical devices), if necessary for different scenarios. With all things considered, it is possible to better understand the phenomenon, i.e., recognize when selected aspects of collaboration affect the work effort. By having these insights, it is possible to more easily identify key issues that need to be tackled to ensure a proper shared understanding is attained by distributed team-members in future sections of remote collaboration. By doing so, the research community can evolve from simple evaluations on how technology works, to more complex evaluations aimed to capture a better perspective on the different factors of collaboration supported by AR, which may lead to a more effective collaborative process over time. Hence, we have shown that a better characterization of the collaborative process can be successfully used to provide an additional perspective on the nuances of remote collaboration mediated by AR, which without contextual data would not be possible. Altogether, due to the flexibility and range of the proposed conceptual model, the instrumentation through the CAPTURE toolkit establishes itself as a general-purpose evaluation approach, providing data that otherwise would be difficult to obtain and analyze. While we must be prudent with generalizing our findings, we expect our insights to be valuable for future reproduction in other domains beside maintenance context. To finish, the continuous observation of contextual data in other tools and with other users may allow, in the future, to create guidelines, supported by experimental data, which can guide the initial development of novel collaborative solutions. 7 Conclusions and future work As a contribution, a critical analysis on collaborative user studies mediated by AR is presented, showing that most studies rely on single-user methods, not adapted to collaborative scenarios and that existing frameworks are not well suited to characterize how collaboration occurs. Motivated by these, we presented a conceptual framework to support researchers in designing better evaluations based on retrieving contextualized data for more comprehensive analysis of the collaborative process. To instantiate this framework, the CAPTURE toolkit was proposed to assist with more user centric evaluations, allowing to easily analyze the collaborative process of a particular team or comparison between a set of teams or different tools. During the analysis of the results obtained, it was possible to realize that the contextual data allowed us to understand participants ease to communicate and to share ideas, and the level of attention allocation, spatial presence or others. Also, measure emotional state, and reaction towards the tools used. In this vein, participants felt AR supports more natural interaction, which contributes to increase empathy, interest and collaboration. By having a grasp on these aspects, typically not reported in the literature, but which are very informative/valuable to understand where the focus of the work, it is possible to better define how research should progress and how the tools can evolve. Hence, conduct comparative analysis of distributed teams may benefit researchers in better understanding the collaborative phenomenon, when compared to how its being currently reported, designing novel methods and improve the collaborative effort. This reinforces, once again, the need to evolve and make these experiences more contextualized and better reported, so that the research community can move into a phase of producing guidelines for remote scenarios supported by AR. Later, we intend to support data/voice collection, both during the collaborative process among the remote team members, and as an additional data input during the post-task assessment. We envision it may be relevant for researchers having metrics that can be automatically calculated and brought for analysis through an updated version of the visualization dashboard, e.g., characteristics of the dialog, during synchronous collaboration (e.g., number of questions, interruptions, occurrences of specific words). One possible way being considered is supporting some form of synchronization so that all user-related events are synchronized with video/voice streams captured in the study. Furthermore, we plan to share the toolkit with the research community, which may elicit newer data gathering/visualization requirements. Also, conduct field studies with experts from the industry sector to demonstrate the framework use in real scenarios. Last, pursue the creation of guidelines to elicit more complete evaluations in such scenarios. CRediT authorship contribution statement Bernardo Marques: Conceptualization, Methodology, Software, Formal analysis, Investigation, Resources, Writing – original draft, Writing – review & editing, Visualization, Project administration, Funding acquisition. Samuel Silva: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Visualization. António Teixeira: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Visualization. Paulo Dias: Conceptualization, Resources, Writing – original draft, Writing – review & editing, Visualization, Supervision, Funding acquisition. Beatriz Sousa Santos: Conceptualization, Writing – original draft, Writing – review & editing, Visualization, Supervision, Funding acquisition. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [UID/CEC/00127/2019]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. References [1] Kim K. de Melo C.M. Norouzi N. Bruder G. Welch G.F. Reducing task load with an embodied intelligent virtual assistant for improved performance in collaborative decision making Conference on Virtual Reality and 3D User Interfaces 2020 IEEE VR 529 538 Kim [ K], de Melo [ CM], Norouzi [ N], Bruder [ G], Welch [ GF]. Reducing task load with an embodied intelligent virtual assistant for improved performance in collaborative decision making. Conference on Virtual Reality and 3D User Interfaces, IEEE VR 2020:529–538. [2] Kim K. Billinghurst M. Bruder G. Duh H.B. Welch G.F. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008–2017) IEEE Trans Vis Comput Graphics 24 2018 2947 2962 Kim [ K], Billinghurst [ M], Bruder [ G], Duh [ HB], Welch [ GF]. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017). IEEE Transactions on Visualization and Computer Graphics 2018a;24:2947–2962. [3] Kim S. Billinghurst M. Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration Comput Support Coop Work CSCW 27 3–6 2018 569 607 Kim [ S], Billinghurst [ M], Lee [ G]. The effect of collaboration styles and view independence on video-mediated remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal 2018b;27(3-6):569–607. [4] Gervasi R. Mastrogiacomo L. Franceschini F. A conceptual framework to evaluate human-robot collaboration Int J Adv Manuf Technol 108 3 2020 841 865 Gervasi [ R], Mastrogiacomo [ L], Franceschini [ F]. A conceptual framework to evaluate human-robot collaboration. The International Journal of Advanced Manufacturing Technology 2020;108(3):841–865. [5] Lukosch S. Billinghurst M. Alem L. Kiyokawa K. Collaboration in augmented reality Comput Support Coop Work CSCW 24 2015 515 525 Lukosch [ S], Billinghurst [ M], Alem [ L], Kiyokawa [ K]. Collaboration in Augmented Reality. In: Computer Supported Cooperative Work, CSCW 2015; vol. 24. 2015, p. 515–525. [6] Schneider M, Rambach J, Stricker D. Augmented reality based on edge computing using the example of remote live support, In: 2017 IEEE International Conference on Industrial Technology, 2017, pp. 1277–1282. [7] Billinghurst M. Clark A. Lee G. A survey of augmented reality Found Trends Human Computer Interact 8 2015 73 272 Billinghurst [ M], Clark [ A], Lee [ G]. A Survey of Augmented Reality. Foundations and Trends in HumanComputer Interaction 2015;8:73–272. [8] Jalo H, Pirkkalainen H, Torro O, Kärkkäinen H, Puhto J, Kankaanpää T. How Can Collaborative Augmented Reality Support Operative Work in the Facility Management Industry? In: Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, 2018, pp. 41–51. [9] Ens B. Lanir J. Tang A. Bateman S. Lee G. Piumsomboon T. Billinghurst M. Revisiting collaboration through mixed reality: The evolution of groupware Int J Human Computer Stud 131 2019 81 98 Ens [ B], Lanir [ J], Tang [ A], Bateman [ S], Lee [ G], Piumsomboon [ T], et al. Revisiting Collaboration through Mixed Reality: The Evolution of Groupware. International Journal of Human-Computer Studies 2019;131:81–98. [10] Wang P. Bai X. Billinghurst M. Zhang S. Zhang X. Wang S. He W. Yan Y. Ji H. Ar/MR remote collaboration on physical tasks: A review Robot Comput-Integr Manuf 72 2021 102071 Wang [ P], Bai [ X], Billinghurst [ M], Zhang [ S], Zhang [ X], Wang [ S], et al. Ar/mr remote collaboration on physical tasks: A review. Robotics and Computer-Integrated Manufacturing 2021;72:102071. [11] Aschenbrenner D, Rojkov M, Leutert F, Verlinden J, Lukosch S, Latoschik M, Schilling K. Comparing Different Augmented Reality Support Applications for Cooperative Repair of an Industrial Robot, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2018, pp. 69–74. [12] Bottani E. Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade IISE Trans 51 3 2019 284 310 Bottani [ E], Vignali [ G]. Augmented reality technology in the manufacturing industry: A review of the last decade. IISE Transactions 2019;51(3):284–310. [13] Wang X. Ong S. Nee A. A comprehensive survey of augmented reality assembly research Adv Manuf 4 1 2016 1 22 Wang [ X], Ong [ SK], Nee [ AYC]. A comprehensive survey of augmented reality assembly research. Advances in Manufacturing 2016;4(1):1–22. [14] Hall M, McMahon C, Bermell-Garcia P, Johansson A, Ravindranath R. Capturing synchronous collaborative design activities: A state-of-the-art technology review, In: Proceedings of International Design Conference, DESIGN 2018, 2018, pp. 347–358. [15] Lee G. Kang H. Lee J. Han J. A user study on view-sharing techniques for one-to-many mixed reality collaborations IEEE Conference on Virtual Reality and 3D User Interfaces 2020 IEEE VR 343 352 Lee [ G], Kang [ H], Lee [ J], Han [ J]. A user study on view-sharing techniques for one-to-many mixed reality collaborations. IEEE Conference on Virtual Reality and 3D User Interfaces, IEEE VR 2020;:343–352. [16] Ludwig T. Stickel O. Tolmie P. Sellmer M. Share-IT: Ad hoc remote troubleshooting through augmented reality Comput Support Coop Work (CSCW) 30 1 2021 119 167 Ludwig [ T], Stickel [ O], Tolmie [ P], Sellmer [ M]. shARe-IT: Ad hoc Remote Troubleshooting through Augmented Reality. Computer Supported Cooperative Work (CSCW) 2021;30(1):119–167. [17] Gurevich P. Lanir J. Cohen B. Design and implementation of TeleAdvisor: a projection-based augmented reality system for remote collaboration Comput Support Coop Work (CSCW) 24 6 2015 527 562 Gurevich [ P], Lanir [ J], Cohen [ B]. Design and implementation of teleadvisor: a projection-based augmented reality system for remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal 2015;24(6):527–562. [18] Zigart T, Schlund S. Evaluation of Augmented Reality Technologies in Manufacturing - A Literature Review, In: Proceedings of the AHFE 2020 International Conference on Human Factors and Ergonomics, 2020, pp. 75–82. [19] Fakourfar O, Ta K, Tang R, Bateman S, Tang A. Stabilized Annotations for Mobile Remote Assistance, In: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 2016, pp. 1548–1560. [20] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration KSII Trans Int Inf Syst 12 12 2018 6034 6056 Kim [ S], Billinghurst [ M], Lee [ C], Lee [ G]. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet and Information Systems 2018c;12(12):6034–6056. [21] Kim S, Lee G, Huang W, Kim H, Woo W, Billinghurst M. Evaluating the Combination of Visual Communication Cues for HMD-Based Mixed Reality Remote Collaboration, In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 2019, pp. 1–13. [22] Kim S. Lee G. Billinghurst M. Huang W. The combination of visual communication cues in mixed reality remote collaboration J Multimod User Interfaces 14 4 2020 321 335 Kim [ S], Lee [ G], Billinghurst [ M], Huang [ W]. The combination of visual communication cues in mixed reality remote collaboration. Journal on Multimodal User Interfaces 2020;14(4):321–335. [23] Neale D.C. Carroll J.M. Rosson M.B. Evaluating computer-supported cooperative work: Models and frameworks Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work CSCW ’04 2004 112 121 Neale [ DC], Carroll [ JM], Rosson [ MB]. Evaluating computer-supported cooperative work: Models and frameworks. In: Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work. CSCW 04; 2004, p. 112121. [24] Hamadache K. Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation Groupware: Design, Implementation, and Use 2009 206 221 Hamadache [ K], Lancieri [ L]. Strategies and taxonomy, tailoring your CSCW evaluation. In: Groupware: Design, Implementation, and Use. 2009, p. 206–221. [25] Antunes P. Herskovic V. Ochoa S.F. Pino J.A. Reviewing the quality of awareness support in collaborative applications J Syst Softw 89 2014 146 169 Antunes [ P], Herskovic [ V], Ochoa [ SF], Pino [ JA]. Reviewing the quality of awareness support in collaborative applications. Journal of Systems and Software 2014;89:146–169. [26] Merino L, Schwarzl M, Kraus M, Sedlmair M, Schmalstieg D, Weiskopf D. Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009–2019), In: IEEE International Symposium on Mixed and Augmented Reality, ISMAR, 2020. [27] Belen R.A.J. Nguyen H. Filonik D. Favero D.D. Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018 AIMS Electron Electr Eng 3 2019 181 Belen [ RAJ], Nguyen [ H], Filonik [ D], Favero [ DD], Bednarz [ T]. A systematic review of the current state of collaborative mixed reality technologies: 20132018. AIMS Electronics and Electrical Engineering 2019;3:181. [28] Marques B, Teixeira A, Silva S, Alves Ja, Dias P, Santos BS. A Conceptual Model for Data Collection and Analysis for AR-based Remote Collaboration Evaluation, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2020, pp. 1–2. [29] Marques B. Teixeira A. Silva S. ao Alves J. Dias P. Santos B.S. A critical analysis on remote collaboration mediated by augmented reality: Making a case for improved characterization and evaluation of the collaborative process Comput Graph 2021 1 17 Marques [ B], Teixeira [ A], Silva [ S], Alves [ J], Dias [ P], Santos [ BS]. A critical analysis on remote collaboration mediated by augmented reality: Making a case for improved characterization and evaluation of the collaborative process. Computers & Graphics 2021a;:1–17. [30] Bai Z. Blackwell A.F. Analytic review of usability evaluation in ISMAR Interact Comput 24 6 2012 450 460 Bai [ Z], Blackwell [ AF]. Analytic review of usability evaluation in ISMAR. Interacting with Computers 2012;24(6):450 – 460. [31] Ratcliffe J, Soave F, Bryan-Kinns N, Tokarchuk L, Farkhatdinov I. Extended Reality (XR) Remote Research: a Survey of Drawbacks and Opportunities, In: CHI Conference on Human Factors in Computing Systems, 2021, pp. 1–13. [32] Dey A. Billinghurst M. Lindeman R.W. Swan J.E. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014 Front Robotics AI 5 2018 37 Dey [ A], Billinghurst [ M], Lindeman [ RW], Swan [ JE]. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014. Frontiers in Robotics and AI 2018;5:37. [33] Anton D. Kurillo G. Bajcsy R. User experience and interaction performance in 2D/3D telecollaboration Future Gener Comput Syst 82 2018 77 88 Anton [ D], Kurillo [ G], Bajcsy [ R]. User experience and interaction performance in 2d/3d telecollaboration. Future Generation Computer Systems 2018;82:77–88. [34] Piumsomboon T. Lee G. Ens B. Thomas B. Billinghurst M. Superman vs giant: A study on spatial perception for a multi-scale mixed reality flying telepresence interface IEEE Trans Vis Comput Graphics 24 11 2018 2974 2982 Piumsomboon [ T], Lee [ G], Ens [ B], Thomas [ B], Billinghurst [ M]. Superman vs giant: A study on spatial perception for a multi-scale mixed reality flying telepresence interface. IEEE Transactions on Visualization and Computer Graphics 2018;24(11):2974–2982. [35] Araujo R.M. Santoro F.M. Borges M.R. A conceptual framework for designing and conducting groupware evaluations Int J Comput Appl Technol 19 3 2004 139 150 Araujo [ RM], Santoro [ FM], Borges [ MRS]. A conceptual framework for designing and conducting groupware evaluations. International Journal of Computer Applications in Technology 2004;19(3):139150. [36] Araujo R, Santoro F, Borges M. The CSCW lab ontology for groupware evaluation, In: 8th International Conference on Computer Supported Cooperative Work in Design, 2, 2004, pp. 148–153. [37] Marques B, Silva S, Dias P, Santos BS. An Ontology for Evaluation of Remote Collaboration using Augmented Reality, In: European Conference on Computer-Supported Cooperative Work, ECSCW: the International Venue on Practice-Centred Computing on the Design of Cooperation Technologies - Posters & Demos, Reports of the European Society for Socially Embedded Technologies, 2021, pp. 1–8. [38] Marques B. Silva S. Alves J. Araujo T. Dias P. Santos B.S. A conceptual model and taxonomy for collaborative augmented reality IEEE Trans Vis Comput Graphics 2021 1 18 Marques [ B], Silva [ SS], Alves [ J], Araujo [ T], Dias [ P], Santos [ BS]. A conceptual model and taxonomy for collaborative augmented reality. IEEE Transactions on Visualization & Computer Graphics 2021c;:1–18. [39] Marques B. Ara T. Silva S. Dias P. Visually exploring a collaborative augmented reality taxonomy Information Visualization, IV 2021 1 6 Marques [ B], Ara [ T], Silva [ S], Dias [ P]. Visually exploring a Collaborative Augmented Reality Taxonomy. In: Information Visualization, IV. 2021d, p. 1–6. [40] Izard C.E. Basic emotions, natural kinds, emotion schemas, and a new paradigm Perspect Psychol Sci 2 3 2007 260 280 Izard [ CE]. Basic Emotions, Natural Kinds, Emotion Schemas, and a New Paradigm. Perspectives on Psychological Science 2007;2(3):260–280. [41] Barnum C.M. Usability Testing Essentials: Ready, Set...Test! first ed. 2010 Morgan Kaufmann Publishers Inc. San Francisco, CA, USA Barnum [ CM]. Usability Testing Essentials: Ready, Set...Test! 1st ed.; San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.; 2010. [42] Gutwin C. Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware ACM Trans on Computer Human Int 6 3 1999 243 281 Gutwin [ C], Greenberg [ S]. The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Transactions on Computer-Human Interaction 1999;6(3):243281. [43] Nilsson S, Johansson B, Jonsson A. Using AR to support cross-organisational collaboration in dynamic tasks, In: IEEE International Symposium on Mixed and Augmented Reality, ISMAR, 2009, pp. 3–12. [44] Kim S, Lee G, Sakata N, Billinghurst M. Improving co-presence with augmented visual communication cues for sharing experience through video conference, In: ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings, 2014, pp. 83–92. [45] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Trans Int Inf Syst 12 2018 6034 6056 Kim [ S], Billinghurst [ M], Lee [ C], Lee [ G]. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet & Information Systems 2018d;12:6034–6056. [46] Huang W. Kim S. Billinghurst M. Alem L. Sharing hand gesture and sketch cues in remote collaboration J Vis Commun Image Represent 58 2019 428 438 Huang [ W], Kim [ S], Billinghurst [ M], Alem [ L]. Sharing hand gesture and sketch cues in remote collaboration. Journal of Visual Communication and Image Representation 2019;58:428–438. [47] Patel H. Pettitt M. Wilson J.R. Factors of collaborative working: A framework for a collaboration model Applied Ergon 43 1 2012 1 26 Patel [ H], Pettitt [ M], Wilson [ JR]. Factors of collaborative working: A framework for a collaboration model. Applied ergonomics 2012;43(1):1–26. [48] Johnson S, Gibson M, Mutlu B. Handheld or Handsfree? Remote Collaboration via Lightweight Head-Mounted Displays and Handheld Devices, In: Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, 2015, pp. 1825–1836. [49] Teo T, Lee G, Billinghurst M, Adcock M. Investigating the use of different visual cues to improve social presence within a 360 mixed reality remote collaboration, In: ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry, 2019. [50] Piumsomboon T. Dey A. Ens B. Lee G. Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality Front Robotics AI 6 2019 Piumsomboon [ T], Dey [ A], Ens [ B], Lee [ G], Billinghurst [ M]. The effects of sharing awareness cues in collaborative mixed reality. Frontiers Robotics AI 2019;6. [51] Marques B, Silva S, Dias P, Santos BS. A Toolkit to Facilitate Evaluation and Characterization of the Collaborative Process in Scenarios of Remote Assistance Supported by AR, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2021, pp. 336–337. [52] Marques B, Silva S, Rocha A, Dias P, Santos BS. Remote Asynchronous Collaboration in Maintenance scenarios using Augmented Reality and Annotations, In: IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), IEEE VR 2021, 2021, pp. 567–568. [53] Madeira T. Marques B. Alves J.a. Dias P. Santos B.S. Exploring annotations and hand tracking in augmented reality for remote collaboration Human Systems Engineering and Design III 2021 Springer International Publishing 83 89 Madeira [ T], Marques [ B], Alves [ J], Dias [ P], Santos [ BS]. Exploring annotations and hand tracking in augmented reality for remote collaboration. In: Human Systems Engineering and Design III. Springer International Publishing; 2021, p. 83–89. "
    },
    {
        "doc_title": "A critical analysis on remote collaboration mediated by Augmented Reality: Making a case for improved characterization and evaluation of the collaborative process",
        "doc_scopus_id": "85113835900",
        "doc_doi": "10.1016/j.cag.2021.08.006",
        "doc_eid": "2-s2.0-85113835900",
        "doc_date": "2022-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Collaboration process",
            "Collaborative process",
            "Comprehensive evaluation",
            "Design and Development",
            "Enabling technologies",
            "In-depth knowledge",
            "Mixed and augmented realities",
            "Remote collaboration"
        ],
        "doc_abstract": "© 2021 Elsevier LtdRemote Collaboration mediated by Mixed and Augmented Reality (MR/AR) shows great potential in scenarios where physically distributed collaborators need to establish a common ground to achieve a shared goal. So far, most research efforts have been devoted to creating the enabling technology, overcoming engineering hurdles and proposing methods to support its design and development. To contribute to more in-depth knowledge on how remote collaboration occurs through these technologies, it is paramount to understand where the field stands and how characterization and evaluation have been conducted. In this vein, this work reports the results of a literature review which shows that evaluation is frequently performed in ad-hoc manners, i.e., disregarding adapting the evaluation methods to collaborative AR. Most studies rely on single-user methods, which are not suitable for collaborative solutions, falling short of retrieving the necessary amount of contextualized data for more comprehensive evaluations. This suggests minimal support of existing frameworks and a lack of theories and guidelines to guide the characterization of the collaborative process using AR. Then, a critical analysis is presented in which we discuss the maturity of the field and a roadmap of important research actions is proposed, that may help address how to improve the characterization and evaluation of the collaboration process moving forward and, in consequence, improve MR/AR based remote collaboration.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2021-08-20 2021-08-20 2022-02-25 2022-02-25 2022-06-01T12:50:43 S0097-8493(21)00170-9 S0097849321001709 10.1016/j.cag.2021.08.006 S300 S300.2 FULL-TEXT 2022-06-01T12:26:47.504333Z 0 0 20220201 20220228 2022 2021-08-20T03:46:47.260845Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor highlightsabst orcid primabst pubtype ref specialabst 0097-8493 00978493 true 102 102 C Volume 102 61 619 633 619 633 202202 February 2022 2022-02-01 2022-02-28 2022 Technical Section article fla © 2021 Elsevier Ltd. All rights reserved. ACRITICALANALYSISREMOTECOLLABORATIONMEDIATEDBYAUGMENTEDREALITYMAKINGACASEFORIMPROVEDCHARACTERIZATIONEVALUATIONCOLLABORATIVEPROCESS MARQUES B 1 Introduction 2 Related work on user evaluation in collaborative AR 2.1 Previous surveys including user evaluation information 2.2 Summary 3 Method and overview of recent literature 3.1 Augmented reality vs mixed reality 3.2 Search process 3.3 Analysis process 3.4 Validity limitations 3.5 Results 3.5.1 User studies categorization 3.5.2 Study design 3.5.3 Study type 3.5.4 Task type 3.5.5 Evaluation methods and data type 3.5.6 Participants 3.6 Summary 4 Critical analysis 4.1 Main limitations Limitation 1: partial evaluation Limitation 2: lack of contextual information Limitation 3: failure situations are not contemplated Limitation 4: lack of theories and guidelines Limitation 5: minimal support in existing frameworks Limitation 6: limited reporting of outcomes 4.2 Maturity of the field 5 Charting out a roadmap for the characterization and evaluation of the collaborative process 5.1 Definition of dimensions of collaboration 5.2 Systematization of perspectives for the field 5.3 Creation of new paradigms, architectures and frameworks 5.4 Development of tools for improved data gathering 5.5 New and better outcomes to support the assessment 6 Conclusions and future work CRediT authorship contribution statement Acknowledgments References ALEM 2011 135 148 L RECENTTRENDSMOBILECOLLABORATIVEAUGMENTEDREALITYSYSTEMS HANDSONVIDEOTOWARDSAGESTUREBASEDMOBILEARSYSTEMFORREMOTECOLLABORATION LUKOSCH 2015 515 525 S COMPUTERSUPPORTEDCOOPERATIVEWORKCSCW2015 COLLABORATIONINAUGMENTEDREALITY THOMAS 1996 P CSCWREQUIREMENTSEVALUATION KIM 2018 2947 2962 K KIM 2018 569 607 S KIM 2020 321 335 S KIM 2020 313 319 S ARIAS 2000 84 113 E GRUDIN 2013 J ENCYCLOPEDIAHUMANCOMPUTERINTERACTION BILLINGHURST 2015 73 272 M ENS 2019 81 98 B BRUNO 2019 875 887 F ONG 2008 2707 2742 S WANG 2016 1 22 X PALMARINI 2018 215 228 R BILLINGHURST 2021 1 4 M ALTUG 2016 23 41 Y CHOI 2018 51 66 S BOTTANI 2019 284 310 E VANLOPIK 2020 K BELEN 2019 181 R ELVEZIO 2017 1 2 C BAI 2012 450 460 Z DESOUZACARDOSO 2020 106159 L ROLTGEN 2020 93 100 D FERNANDEZDELAMO 2018 148 155 I JETTER 2018 18 33 J HAMADACHE 2009 206 221 K GROUPWAREDESIGNIMPLEMENTATIONUSE STRATEGIESTAXONOMYTAILORINGYOURCSCWEVALUATION ANTUNES 2014 146 169 P PATEL 2012 1 26 H DUENSER 2008 A SIGGRAPH2008 ASURVEYEVALUATIONTECHNIQUESUSEDINAUGMENTEDREALITYSTUDIES DEY 2018 37 A GUTWIN 1999 243 281 C SPEICHER 2019 1 15 M PROCEEDINGS2019CHICONFERENCEHUMANFACTORSINCOMPUTINGSYSTEMS MIXEDREALITY MILGRAM 1994 282 292 P MILGRAM 1994 1321 1329 P ROKHSARITALEMI 2020 S BAI 2020 1 13 H PIUMSOMBOON 2019 T WANG 2020 P RHEE 2020 T TEO 2019 T PIUMSOMBOON 2019 T WANG 2019 P WALDOW 2019 246 262 K TEO 2018 406 410 T KIM 2018 6034 6056 S GUNTHER 2018 339 344 S PIUMSOMBOON 2018 T RYSKELDIEV 2018 177 185 B HOPPE 2018 328 337 A AKKIL 2018 524 542 D LEE 2017 G SIGGRAPHASIA2017MOBILEGRAPHICSINTERACTIVEAPPLICATIONS MIXEDREALITYCOLLABORATIONTHROUGHSHARINGALIVEPANORAMA KOMIYAMA 2017 R GUREVICH 2015 527 562 P TAIT 2015 563 589 M KIM 2015 1669 1674 S HUANG 2013 70 77 W PECE 2013 1319 1328 F BANNAI 2006 143 154 Y REGENBRECHT 2004 338 354 H ARAUJO 2004 139 150 R RATCLIFFE 2021 1 13 J CHICONFERENCEHUMANFACTORSINCOMPUTINGSYSTEMS EXTENDEDREALITYXRREMOTERESEARCHASURVEYDRAWBACKSOPPORTUNITIES GAINES 1991 3 22 B LALANNE 2009 153 160 D PROCEEDINGS2009INTERNATIONALCONFERENCEMULTIMODALINTERFACES FUSIONENGINESFORMULTIMODALINPUTASURVEY TEIXEIRA 2014 29 A SPEECHAUTOMATAINHEALTHCAREVOICECONTROLLEDMEDICALSURGICALROBOTSCHAPTER1 ACRITICALANALYSISSPEECHBASEDINTERACTIONINHEALTHCAREROBOTSMAKINGACASEFORINCREASEDUSESPEECHINMEDICALASSISTIVEROBOTS BILLINGHURST 2003 M SERENO 2020 1 20 M COLLAZOS 2019 4789 4818 C MEYER 2019 87 97 M AUGSTEIN 2019 27 58 M NICKERSON 2013 336 359 R TERUEL 2017 e1858 M ZOLLMANN 2020 1 20 S CHANDRASEKARAN 1999 20 26 B NOY 2001 1 25 N HERSKOVIC 2007 328 336 V GROUPWAREDESIGNIMPLEMENTATIONUSE EVALUATIONMETHODSFORGROUPWARESYSTEMS PEREIRA 2015 146 157 C PEREIRA 2016 1 196 C DYNAMICEVALUATIONFORREACTIVESCENARIOS MARQUESX2022X619 MARQUESX2022X619X633 MARQUESX2022X619XB MARQUESX2022X619X633XB 2024-02-25T00:00:00.000Z 2024-02-25T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 0 item S0097-8493(21)00170-9 S0097849321001709 10.1016/j.cag.2021.08.006 271576 2022-06-01T12:26:47.504333Z 2022-02-01 2022-02-28 true 1219570 MAIN 15 65990 849 656 IMAGE-WEB-PDF 1 gr3 31653 220 376 gr1 115848 553 567 ga1 true 21692 176 301 gr2 34838 257 376 gr3 7538 128 219 gr1 9119 164 168 ga1 true 8112 128 219 gr2 9782 149 219 gr3 214178 975 1666 gr1 774364 2446 2509 ga1 true 186053 780 1333 gr2 259932 1137 1666 CAG 3400 S0097-8493(21)00170-9 10.1016/j.cag.2021.08.006 Elsevier Ltd Fig. 1 Overview of the main results from the recent literature review on evaluation and AR-supported Remote Collaboration. In the first level are the categories considered for the systematic review, raging among the participants, application areas, collaboration details, study characteristics, task details, adaptation period and evaluation methods. Then, in the outer ring, the detailed topics of interest for each category are presented, respectively. For each, the number of publications covering it is illustrated, following the literature review analysis. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Positioning of Remote Collaboration mediated by AR between the Replication and Empiricism phases of the BRETAM model. Inspired by [95]. Fig. 3 Roadmap overview of the main topics that should be addressed regarding remote collaboration mediated by AR to make the field achieve the Theory, Automation and Maturity phases of the BRETAM model. Inspired by [97]. Table 1 Summary of evaluation surveys addressing Collaborative Augmented Reality (2008–2019). Year & Authors Pub. # Pubs. analyzed Aspects of Collaboration Main outcomes 2008 - Duenser et al. [41] 10 n/a Studies that evaluated collaboration between users using AR were quite underrepresented. Only 10 papers were reported, which were divided according to the study type in formal and informal studies. 2008 - Zhou et al. [42] not specified n/a A small number of examples of collaborative AR prototypes were starting to emerge, but few had been evaluated in formal user studies. 2012 - Bai et al. [29] 9 Communication, Awareness An increase in measurements of particular interest in AR collaborative systems included explicit communication (e.g., spoken and gestural messages), ease of collaboration and information gathering (basic awareness, eye gaze). 2015 - Billinghurst et al. [12] not specified Communication Besides the standard subjective measures, process measures may be more important than quantitative outcome measures. Process measures are typically gathered by transcribing interaction between users, like speech or gestures and performing a conversational analysis. In this context, very few studies have examined communication process measures. 2018 - Kim et al. [6] not specified n/a A reduce but increasing number of publications explicitly focused on ways to improve collaboration using AR. A mixture of qualitative and quantitative experimental measures were used, such as performance time and accuracy (quantitative), and subjective questionnaires (qualitative). 2018 - Dey et al. [43] 12 n/a Need to conduct more user studies regarding collaboration using AR, more use of field studies, and the use of a wider range of evaluation methods. There is an urge to improve the reporting quality of user studies, and education of researchers on how to conduct good AR user studies. 2019 - Ens et al. [14] 110 Time, Space, Symmetry, Artificiality, Focus, Scenario Review of the history of collaborative MR systems, and investigation on how common taxonomies and frameworks in CSCW and MR research could be applied to such systems. The authors emphasize that MR systems have been facing significant engineering hurdles and have only recently started to mature to focused on the nuances of supporting collaboration. 2019 - Belen et al. [25] 259 Task, Awareness, Presence, Social factors A total of 112 papers studied how MR affects the sense of presence and the perception of social awareness, situational awareness and task awareness during collaboration. A considerable amount of research studied how collaboration reduces cognitive workload through MR environments. 55 papers were categorized under user perception and cognition studies. Table 2 Summary of User studies on Remote Collaboration using AR or MR — Part 1. Legend: S — Subjective; O — Objective; HHD — Handheld Device; HMD — Head Mounted Display. ID Pub. Year Application area Collaboration details Task type Devices used (On-site User) Devices used (Remote User) Study type Data type Study design 1 [53] 2020 Assembly Hierarchy - Synchronous Lego Brick Assembly Projector, External Camera HMD, Hand Tracker Formal S Within-subjects 2 [54] 2020 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Controllers, 360°camera HMD, Controllers Formal S Between-subjects 3 [55] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers, Hand Tracker Formal O + S – 4 [56] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Puzzle Assembly See-through HMD, 360°camera HMD, Controllers Formal O + S – 5 [57] 2019 Assembly Hierarchy - Synchronous Lego Brick Assembly See-through HMD, Depth Sensors HMD, Hand Tracker Formal O + S Within-subjects 6 [58] 2019 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation See-through HMD See-through HMD, Formal S Within-subjects 7 [59] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Lego Brick Assembly See-through HMD, 360°camera HMD, Hand Tracker Formal O + S – 8 [60] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers Formal O + S – 9 [61] 2019 Social Presence Parallel - Synchronous Puzzle Assembly See-through HMD HMD, Controllers Formal S Within-subjects 10 [62] 2019 Assembly Hierarchy - Synchronous Lego Brick Assembly Projector, Camera HMD, Hand Tracker Formal O + S Within-subjects 11 [63] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Hand Tracker Formal O + S Within-subjects 12 [52] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker HMD, Hand Tracker Formal O + S Between-subjects 13 [64] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker See-through HMD, Hand Tracker Formal O + S Within-subjects 14 [6] 2018 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Within-subjects 15 [65] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Hand Tracker Formal O + S Within-subjects 16 [66] 2018 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 17 [67] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HMD, Hand Tracker HMD, Hand Tracker Informal, Formal S Between-subjects 18 [22] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer, Mouse and Keyboard Formal O + S Between-subjects 19 [68] 2018 Assembly Hierarchy - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Hand Tracker Formal O + S Between-subjects 20 [69] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 21 [70] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD HMD, Controllers Formal O + S Within-subjects 22 [71] 2018 Assistance Parallel - Synchronous Navigation, Object Selection and Manipulation HHD HHD Formal O + S – 23 [72] 2018 Assistance Parallel - Synchronous Navigation, Object Selection and Manipulation HMD, Controllers, Hand Tracker HMD, Controllers, Hand Tracker Formal O + S – 24 [73] 2018 Assembly Parallel - Synchronous Puzzle Assembly Projector, External Camera Computer, Gaze Tracker Informal, Formal O + S Within-subjects 25 [74] 2017 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers, Hand Tracker Formal O + S – 26 [75] 2017 Assembly Hierarchy - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Gaze Tracker Formal S Between-subjects 27 [76] 2017 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera, Body Tracker Projector, Optitrack Capture Tracker Informal S Within-subjects 28 [77] 2016 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker HMD, Controllers Informal O + S Between-subjects 29 [78] 2015 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Lego Brick Assembly Projector, External Camera Computer, Mouse and Keyboard Informal, Formal O + S Within-subjects 30 [79] 2015 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD Computer, Mouse and Keyboard – O + S Between-subjects 31 [80] 2015 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 32 [81] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD Computer, Mouse and Keyboard – O + S Between-subjects 33 [82] 2014 Assembly Parallel - Synchronous Puzzle Assembly HHD or See-through HMD Computer, Mouse and Keyboard Formal O + S Between-subjects 34 [83] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer with Touch screen Informal, Field S – 35 [84] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer, Mouse and Keyboard Informal, Field O + S Within-subjects 36 [85] 2013 Assembly Hierarchy - Synchronous Puzzle Assembly Monitor, External Camera HMD Formal O + S – 37 [86] 2013 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation HHD HHD Formal O + S – 38 [87] 2012 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera See-through HMD, external camera Informal S Between-subjects 39 [88] 2012 Assistance Hierarchy - Synchronous Airplane Cockpit HHD Computer, Mouse and Keyboard Formal O + S Within-subjects 40 [89] 2007 Education Parallel - Asynchronous Navigation, Object Selection and Manipulation Computer, External Camera Computer, External Camera, Gaze Tracker Informal S Between-subjects 41 [90] 2006 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HMD, External Tracker HMD, External Tracker Formal O – 42 [91] 2004 Social Presence Hierarchy - Synchronous Navigation, Object Selection and Manipulation Computer, Mouse and Keyboard Computer, Mouse and Keyboard Formal S Within-subjects Table 3 Summary of User studies in Remote Collaboration using AR or MR — Part 2. ID Pub. Evaluation methods # Participants (# Females) Participant role Participants knew each other Previous experience with AR/VR/MR Description experimental context Adaptation period Duration (min) Recording audio and video 1 [53] Questionnaires, Interview 34 (11) On-site or Remote Yes and No – – Yes 55 – 2 [54] Questionnaires, User Preference 40 (-) On-site or Remote – Yes Yes Yes 40 – 3 [55] Task Performance, Questionnaires, User Preference 32 (8) On-site or Remote – Yes – – – – 4 [56] Task Performance, Questionnaires, User Preference 10 (1) On-site or Remote – Yes Yes Yes – – 5 [57] Task Performance, Questionnaires, User Preference 10 (4) On-site or Remote – – – – – – 6 [58] Questionnaires, User Preference 8 (4) On-site or Remote – – Yes – 30 – 7 [59] Task Performance, Questionnaires, User Preference 14 (-) On-site – – Yes – 70 Yes 8 [60] Task Performance, Questionnaires, User Preference 24 (5) On-site – Yes Yes – 90 – 9 [61] Questionnaires, Interview 48 (24) On-site Yes Yes and No Yes – – – 10 [62] Task Performance, Questionnaires, User Preference 13 (5) On-site or Remote – No – Yes – – 11 [63] Task Performance, Questionnaires, User Preference 12 (3) On-site and Remote Yes Yes Yes – – – 12 [52] Task Performance, Questionnaires, Interview 32 (9) On-site and Remote Yes Yes and No Yes Yes 120 – 13 [64] Task Performance, Questionnaires 20 (5) On-site or Remote No – Yes Yes 35 – 14 [6] Task Performance, Questionnaires, User Preference 24 (7) On-site or Remote Yes – – – – Yes 15 [65] Task Performance, Questionnaires, User Preference 8 (2) On-site – Yes Yes Yes – – 16 [66] Questionnaires, Interview 24 (4) On-site or Remote Yes – Yes Yes – – 17 [67] Questionnaires, User Preference 38 (23) On-site or Remote – – Yes Yes 30 – 18 [22] Task Performance, Questionnaires 30 (4) On-site – – – – – 19 [68] Task Performance, Questionnaires, User Preference 10 (0) On-site – – – – – – 20 [69] Task Performance, Questionnaires, User Preference 8 (4) On-site or Remote – – Yes – 60 Yes 21 [70] Task Performance, Questionnaires, User Preference 16 (5) On-site or Remote – Yes – – – – 22 [71] Task Performance, Questionnaires, User Preference 40 (-) On-site or Remote – – Yes – – – 23 [72] Task Performance, Questionnaires, User Preference 28 (-) On-site and Remote – Yes – – – – 24 [73] Task Performance, Questionnaires, User Preference 24 (16) On-site or Remote – – Yes Yes – – 25 [74] Task Performance, Questionnaires, User Preference 8 (-) On-site and Remote – – – – – – 26 [75] Questionnaires, User Preference 8 (2) On-site or Remote Yes – – – – – 27 [76] Questionnaires, User Preference 8 (-) Remote – – – – – – 28 [77] Task Performance, Questionnaires 10 (-) On-site – – – – – – 29 [78] Task Performance, Questionnaires 13 (-) + 24 (-) On-site or Remote – – – – – – 30 [79] Task Performance, Questionnaires, User Preference 36 (15) On-site or Remote – – – – – Yes 31 [80] Task Performance, Questionnaires, Interview 24 (7) Remote – – Yes Yes 70 – 32 [81] Task Performance, Questionnaires, User Preference – Remote – – – – – Yes 33 [82] Task Performance, Questionnaires, User Preference 24 (7) On-site or Remote Yes – Yes Yes 90 – 34 [83] Questionnaires, User Preference 25 (-) + 11 (5) – – – – – 50 – 35 [84] Task Performance, Questionnaires, User Preference 20 (-) + 60 (29) On-site – – – – – – 36 [85] Task Performance, Questionnaires, User Preference 14 (-) On-site and Remote – – Yes – – – 37 [86] Task Performance, Questionnaires, User Preference 36 (-) On-site or Remote – – Yes – – – 38 [87] Interview 5 (-) On-site – – – – – Yes 39 [88] Task Performance, Questionnaires 48 (21) On-site Yes and No – Yes Yes – – 40 [89] Questionnaires, Interview 9 (3) On-site – – – Yes 20 – 41 [90] Task Performance 12 (2) On-site or Remote – – – – – – 42 [91] Questionnaires, User Preference 27 (8) On-site or Remote – – Yes – – – Table 4 Overview of common approaches and what is missing regarding the evaluation process of remote collaboration mediated by AR. Common: •synchronous hierarchy collaboration •within-subjects design •formal user studies •navigation, selection, manipulation and assembly tasks •focus on technological aspects or interaction mechanisms of the collaborative AR solution •subjective and objective data collection •use of single-user questionnaires, task performance and user preferences assessment •young participants from universities •participants act as on-site or remote team-members Missing: •conduct outdoor and field studies •explore complex/adequate tasks •contemplate failure situations •provide an adaptation period •address participants relationships, knowledge and motivations •better description of the collaborative process supported by AR •reporting of study average duration •data collection on dialog turns, interaction types, main features and visual complexity •contextualized information on the team, task, environment and collaborative tool •improve existing frameworks •use of video, audio recordings, and post-task interviews ☆ This article was recommended for publication by C Sandor. Technical Section A critical analysis on remote collaboration mediated by Augmented Reality: Making a case for improved characterization and evaluation of the collaborative process Bernardo Marques Conceptualization Methodology Validation Formal analysis Investigation Data curation Writing – original draft Writing – review & editing Visualization Project administration Funding acquisition ⁎ António Teixeira Conceptualization Methodology Writing – original draft Visualization Samuel Silva Methodology Investigation Writing – original draft Writing – review & editing João Alves Validation Formal analysis Investigation Data curation Writing – original draft Paulo Dias Conceptualization Investigation Data curation Writing – original draft Writing – review & editing Supervision Funding acquisition Beatriz Sousa Santos Conceptualization Methodology Investigation Writing – original draft Writing – review & editing IEETA, DETI, University of Aveiro, Aveiro, 3810-193, Portugal IEETA, DETI, University of Aveiro Aveiro 3810-193 Portugal IEETA, DETI, University of Aveiro, Aveiro, 3810-193, Portugal ⁎ Corresponding author. Remote Collaboration mediated by Mixed and Augmented Reality (MR/AR) shows great potential in scenarios where physically distributed collaborators need to establish a common ground to achieve a shared goal. So far, most research efforts have been devoted to creating the enabling technology, overcoming engineering hurdles and proposing methods to support its design and development. To contribute to more in-depth knowledge on how remote collaboration occurs through these technologies, it is paramount to understand where the field stands and how characterization and evaluation have been conducted. In this vein, this work reports the results of a literature review which shows that evaluation is frequently performed in ad-hoc manners, i.e., disregarding adapting the evaluation methods to collaborative AR. Most studies rely on single-user methods, which are not suitable for collaborative solutions, falling short of retrieving the necessary amount of contextualized data for more comprehensive evaluations. This suggests minimal support of existing frameworks and a lack of theories and guidelines to guide the characterization of the collaborative process using AR. Then, a critical analysis is presented in which we discuss the maturity of the field and a roadmap of important research actions is proposed, that may help address how to improve the characterization and evaluation of the collaboration process moving forward and, in consequence, improve MR/AR based remote collaboration. Graphical abstract Keywords Remote collaboration Augmented Reality Mixed Reality Evaluation and characterization collaborative process Critical analysis Roadmap proposal 1 Introduction Collaboration is essential in many situations, as is the case of industrial, medical, and educational domains, among others [1–4] and can be described as the process of joint and interdependent activities between co-located or remote collaborators performed to achieve a common goal [5–9]. Collaboration scenarios have evolved from simple co-located scenarios to more complex remote collaboration, encompassing several team members with different experiences, expertise’s and multidisciplinary backgrounds distributed by different geographic locations around the world. Therefore, the methods required to address such activities have been growing in terms of scale, complexity, and interdisciplinarity, entailing not only the mastery of multiple domains of knowledge, but also a strong level of proficiency in each [3,4,10]. Remote collaboration, implies that collaborators establish a joint effort to align and integrate their activities in a seamless manner. Technological support for remote collaboration has been addressed among other fields by Computer-Supported Cooperative Work (CSCW), focusing on conceptualizing, designing, and prototyping solutions for communication, cooperation, assistance, training, learning as well as knowledge sharing between distributed collaborators. One major issue of remote collaboration is the fact that collaborators do not share a common space/world, reason for the interest in using Augmented Reality (AR) in this context [11–15]. Collaboration using AR helps distributed collaborators establish a common ground, analogous to their understanding of the physical space, allowing to inform where to act, and what to do, e.g., making assumptions and beliefs visible by providing real-time spatial information, highlighting specific areas of interest, or sharing situated information associated with relevant objects in the on-site physical environment [16–20]. Remote AR-based solutions are well suited for overlaying responsive computer-generated information on top of the real-world environment, resulting in the creation of solutions that combine the advantages of virtual environments and the possibility for seamless interaction with the real-world objects and other collaborators [6,15,17,19,21–24]. A number of studies have focused on the use of virtual annotations to augment the shared understanding, using drawings, pointers, gaze, hand gestures and others on 2D images or live video streams [3,6,9,14,22,25]. As an alternative, recent studies started to explore the use of virtual replicas [26–28], as well as reconstructions of the physical environment [29,30], although these required the existence of 3D models and additional hardware, which may limit their adoption in some scenarios of application. Using such approaches to enhance the common ground can improve efficiency and accuracy of the performed tasks by enhancing the perception of the shared understanding [6,22,24,31,32], as well as collaboration times, knowledge retention, increased problem context and awareness [16,17,33–35]. While creating the means to support collaboration clearly motivated early research, advances in AR have been limited by new technical developments, which means most of the research efforts, so far, have been focused on creating the enabling technology and propose novel methods to support its design and development [14,36,37]. On the other hand, with the growing development of CSCW, the evaluation of these solutions during the collaborative effort become an essential, but difficult endeavor [31,38,39], given the novelty of the field and the lack of methods and theories [14,25] to guide the characterization of the collaborative process, i.e., describe the contributions of AR to the collaborative work effort. In addition, scenarios of remote collaboration are multifaceted [40], which means many aspects may affect the way teams collaborate, making it difficult to identify all variables related to the collaborative process. Therefore, the integration of proper characterization and evaluation methods and guidelines is of paramount importance. In this paper, we analyze the subject of remote collaboration supported by AR through a systematic review and investigate how characterization and evaluation of the collaborative process has been conducted during user studies to better understand their specificities, rather than focusing on the development of technology itself. In this context, we analyzed existing surveys that addressed collaborative user studies and evaluation in their reviews. Plus, we performed a literature review from 2000 to 2020 to provide a high-level overview of the field, allowing identification of strengths and weaknesses of existing methods. Based on the analyzes, we describe the challenges involved with evaluating these solutions and critically analyze the state of the field. As a result, a possible roadmap is proposed to facilitate and elicit characterization of the collaboration process using AR-based solutions, so that research and development can move forward and focus on the nuances of supporting collaboration, i.e., focus squarely on the human concerns that underlie collaboration, rather than creating the enabling technology that makes remote collaboration mediated by AR possible. The rest of the paper is organized as follows: Section 2 presents an overview based on survey papers to update, complement and fill gaps on the current information on the state-of-the-art. Next, Section 3 details the methodology adopted to conduct our literature review and describes a high-level overview of the reviewed papers. Then, Section 4 provides a critical analysis in which we discuss the challenges associated to the characterization and evaluation of the collaborative process. Afterwards, Section 5 propose a roadmap to address these challenges. Finally, Section 6 concludes by summarizing the main outcomes. 2 Related work on user evaluation in collaborative AR This section identifies and analyzes existing survey papers that cover relevant evaluation details, which are summarized in Table 1. Our goal was to understand how evaluation has been conducted in collaborative scenarios, allowing to compare and contrast different methods, as well as identify opportunities and limitations associated to the characterization of the collaborative process. From the list of prior surveys, the first six entries are rather general in scope, although the review of collaborative AR papers is also mentioned, despite being only a portion of the results reported [6,12,29,41–43]. While this is the case, the two last entries of the list [14,25] focus entirely on the subject of Collaborative AR and MR, including co-located and remote examples. Although these surveys primarily focused on the development of collaborative AR technology itself, some important outcomes regarding evaluation are also reported, as described below in detail. Besides, another publication [7] was considered in this analysis, that even though not strictly a survey, includes important information regarding evaluation of collaborative work in the context being addressed. 2.1 Previous surveys including user evaluation information Duenser et al. (2008) reported on user evaluation techniques used in AR research. Then, studies that evaluate collaboration between users using AR were quite underrepresented: from a total of 161 publications included in the survey, only 10 addressed collaborative AR. Besides reporting that 8 papers were formal and 2 informal user evaluation, the survey does not present further detail on the collaborative studies [41]. In addition, Zhou et al. (2008) presented one of the first overviews of the research conducted until that moment at the ISMAR conference and its predecessors. Although the research focus was on AR technologies, it also pointed out the significance of usability evaluation. The authors reported that a small number of collaborative AR prototypes were starting to emerge, but few had been evaluated in formal user studies. The authors also highlighted how the role of different displays would affect collaboration in the future and how the location of the task affected user behaviors in terms of verbal and non-verbal communication. Since collaboration and evaluation were not one of the focus of the survey, no further detail was provided [42]. In the same way, Bai et al. (2012) conducted an analytic review on usability evaluation at ISMAR. The authors suggested that while the design of usable systems were the main focus of collaborative AR research to that point, an increase in evaluation research was emerging. They also stated that measurements of particular interest in collaborative AR systems may include explicit communication (e.g., spoken and gestural messages), ease of collaboration and information gathering (e.g., basic awareness, eye gaze). The authors also reported that subjective answers may be collected via questionnaire and that direct observation was used to extract objective results. Moreover, signs of discomfort and enjoyment during collaboration were also taken into account by researchers [29]. Billinghurst et al. (2015) published a survey on AR, in which almost 50 years of research and development in the field were summarized. The authors state that in Collaborative AR studies, besides the standard subjective measures, process measures may be more important than quantitative outcome measures. Process measures are typically gathered by transcribing interaction between users, like speech or gestures and performing a conversational analysis. Measures that have been found to be significantly relevant include: frequency of conversational turns, duration of overlapping speech, number of questions, number of interruptions, turn completions and dialog length, among others. Besides, gesture and non-verbal behaviors can also be analyzed for characteristic features. The survey acknowledges that there have been very few user studies with collaborative AR environments and almost none that examined communication process measures [12]. Then again, Kim et al. (2018) revisited the trends presented at ISMAR conferences. According to their review, user evaluation and feedback has become one of the main categories for research presented at ISMAR, with 16.4% of publications reporting evaluation being conducted, showing a significant increase when compared to Zhou et al. 5.8% [42]. The authors extended Zhou et al. list of emerging research, including interactive collaborative systems for multiple remote or co-located users. A mixture of qualitative and quantitative experimental measures were used in studies that addressed collaboration, such as performance time and accuracy (quantitative), and subjective questionnaires (qualitative) [6]. Dey et al. (2018) conducted a systematic review of AR usability studies. A total of 291 papers have been reviewed. Among other things, over the years, there were few collaborative user studies, mostly directed towards remote collaboration. The authors reported 12 papers, in a total of 15 studies associated to the collaboration application area. One noticeable feature was the fact that there were no pilot studies reported, which is an area for potential improvement. Also, a reduced number (3 out of 15) of field studies was reported and all except one were performed indoors. Furthermore, a within-subjects design was used by 14 out of 15 studies, since these require fewer participants to achieve adequate statistical significance, with only 12 participants being recruited per study. Besides, roughly one-third of the participants were females in all studies. Hence, participant populations are dominated by mostly young, educated, male participants, suggesting that the field could benefit from more diversity. A majority of the studies, 8 out of 15 collected both objective (quantitative) and subjective (qualitative) data, while 5 studies were only based on subjective data, and 2 studies were based on only objective data. Aside from subjective feedback or ratings, task completion time and error/accuracy were also extensively used. Curiously, the NASA TLX was only used by one study. This analysis suggest the need of more user studies regarding collaboration using AR, particularly more field studies, and the use of a wider range of evaluation methods [43]. Although not strictly a survey, Kim et al. (2018) proposed a questionnaire including aspects regarding overall collaboration, namely the level of enjoyment and mental stress in communication with the partner, and whether collaboration was effective or not [7]. Moreover, the questionnaire included questions about who (presence of others — users’ feeling of togetherness with the collaborating partner), what (users’ activities — effectiveness in sending and receiving messages) and where (location of activities — whether seeing work space properly and asking the level of having a same focus with a partner). The questionnaire was based on previous work by Gutwin and Greenberg (1999), which suggested three types of experimental measurements are necessary to assess collaboration: product, process, and satisfaction. Product measures focus on assessing collaboration outcomes in terms of efficiency (e.g., task completion time) or quality (e.g., accuracy). Process measures assess user communication and patterns of collaboration and can be obtained by system log data, observation, and video/audio analysis. Satisfaction measures are adequate to assess participants’ subjective opinions on the quality of their collaboration and can be collected through interviews and questionnaires [44]. More recently, Ens et al. (2019) revisited collaboration through MR. A total of 110 papers employing MR technology and motivated by challenges in collaborative scenarios was reviewed, showing a rise in the number of papers published from 2012 and onward. The authors emphasize that MR systems have been facing significant engineering hurdles, being limited by the contemporary capabilities of technology, and have only recently started to mature to the point where researchers can focus squarely on the human concerns that underlie communication and collaboration, instead of focusing on creating the enabling technology. The vast majority of papers analyzed (106, or 95%) focused on synchronous collaboration. Moreover, 30 papers (27%) worked on a co-located setting, while 75 papers (68%) worked on a remote setting, and 6 papers (5%) support both settings. In the early years (up to 2005), most research addressed co-located work. Then, the paradigm changed, and from 2006 forward most work tackled remote collaboration. In addition, 45 papers (41%) focus on symmetric collaboration, while 63 (57%) on asymmetric, and 2 (2%) supported both types. The review states that existing methods are not sufficient to characterize how collaboration occurs. Finally, it also emphasizes the need to deepen the understanding of collaborative work through more user studies [14]. Finally, Belen et al. (2019) performed a systematic review of the current state of collaborative MR technologies published from 2013 to 2018. A total of 259 papers have been classified based on their application areas, types of display devices used, collaboration setups, user interaction and experience aspects. Regarding the collaboration setups used, 129 papers (50%) report works that used a remote setup, 103 papers (40%) used a collocated setup, and 27 (10%) used both settings. The type of user interaction and user experience were categorized, resulting in 55 papers categorized under user perception and cognition studies, which aim to lessen cognitive workload for task understanding and completion time and increase users’ perceptual (e.g., situational, social, and task) awareness and presence. Besides, a total of 112 papers studied how MR affects the sense of presence and the perception of social awareness, situational awareness and task awareness during collaboration. There was also a considerable amount of research on how collaboration reduces cognitive workload through MR environments. This review also showed that user interaction in a collaborative MR environment is an essential topic that requires further investigation [25]. 2.2 Summary Research is evolving from solving technical issues using AR and MR, towards more meaningful studies on collaboration. We were able to understand that evaluation is frequently done using single-user methods, which are not always applicable to groupware collaborative solutions. To clarify, by single-user methods, we are referring to the methodologies used in the collaborative studies. For example, focusing more on technological aspects of the solution being used than in the collaborative process; including tasks with low complexity that do not elicit real collaboration among participants; using only performance measures like task completion time and error/accuracy data, while other important dimensions are ignored; collecting participant data based only on standard practices with fixed answers, applying scales, questionnaires (e.g., System Usability Scale (SUS), NASA Task Load Index (TLX), among others), which are not thought for collaborative scenarios, thus ignoring detail on crucial aspects of collaboration. The majority of papers mentioned in the surveys informed on the tasks, types of devices used (although not specific to on-site or remote users), evaluation design, evaluation methods and number of participants, but lack detail on the participants’ role, if participants knew each other previous to the study, their previous experience with Virtual Reality (VR), AR or MR solutions, description on the experimental context, among other factors of collaboration. However, our review highlights some limitations included in previous surveys, namely the absence of information regarding specific characteristics of the collaborative context. These characteristics are important since collaboration may occur at many levels and depends on several factors that may impact directly the collaborative outcomes [40]. Contextual information helps inform the conditions in which the collaborative effort took place. Without comprehending the contextual information, it becomes difficult to assess the important variables related to the collaborative process, which means the results and findings reported may be misleading or of limited value in these scenarios, thus being an important subject to improve the characterization of the collaborative process. Hence, these aspects have an important impact on how the studies must be prepared and how they were conducted, influencing situation understanding, team-members communication, task performance, and even how AR-based tools were used among team-members, among others. Therefore, it is important to conduct thorough collaborative studies, allowing to retrieve the necessary amount of data for more comprehensive analysis that helps provide a perspective on the different factors of collaboration supported by AR. To sum up, the use of AR-based multi-site solutions creates challenges to the contextualization of the actions of each user and the problems/barriers they may face. Therefore, having a grasp of those aspects is paramount to ensure characterization is genuine. By doing so, researchers may be able to better assess a wide range of information, namely individual and team personalities, motivations, performances, behaviors, who completed the tasks and who provided instructions, how was the communication process, details of the surrounding environments, as well as duration and type of interactions with the collaborative technology, among other aspects when analyzing data and establishing conclusions. 3 Method and overview of recent literature To understand to what extent user evaluation is currently being reported covering collaborative AR and Mixed Reality (MR) research, we conducted an analysis of existing works through a systematic review. This section presents the research methods employed to carry out the review process, which was divided into: the search, i.e., describing how the collection of publications was performed and the review, i.e., explaining the process employed to ensure that the papers follow our review criterion. What differentiates our review from other surveys described in the previous section is the fact that we focus exclusively on evaluation and user studies in remote scenarios mediated by AR/MR to comprehend how the collaborative process has been captured and reported, rather than addressing the technology that made collaboration possible, which was the nucleus of the two only surveys that dedicated their efforts to the subject of Collaborative AR/MR, while the remaining ones are rather general in application scenario, although also addressing more technological aspects of AR/MR. Besides, by identifying relevant aspects that are missing from existing surveys regarding evaluation and user studies, we are able to include them in our analysis, leading to a discussion in which we critically analyze the field in light of the BRETAM model, thus providing a clearer understanding of how the characterization of the collaborative process has been achieved, which lead to the proposal of a roadmap of relevant research topics, aiming to help the community move the field forward. 3.1 Augmented reality vs mixed reality While older papers used the term remote collaboration supported by AR, more recent efforts described in literature are beginning to replace the term AR by MR. Next, we elaborate on the meaning of MR, and why this sudden change has started to emerge. Many researchers see MR as a synonym for AR [45]. Some consider MR a superset of AR, i.e., a real-world object can interact with a virtual one in real-time to assist individuals in practical scenarios [46–48]. Yet, others consider MR distinct from AR in the sense that MR enables walking into, and manipulating a scene, whereas AR does not, i.e., there is a separation of the real and virtual world content, which may lead to lower user immersion [49]. Although MR is increasingly gaining in popularity and relevance, the research community is still far from a shared understanding of what MR actually constitutes. Speicher et al. (2019) highlights that currently, there is no single definition for MR, since this concept can be considered different things for different individuals. In their survey, six partially competing notions were identified based on literature analysis and experts’ responses. Nevertheless, there is no universally agreed on, one-size-fits-all definition of MR. Moreover, the authors state that it is highly unrealistic to expect one single definition may appear in the future, which means discussions about MR become increasingly difficult. Therefore, it is extremely important to be clear and consistent in terminology while communicating one’s understanding of MR in order to avoid confusion and ensure constructive discussion [45]. Among the most important applications of MR are collaborative solutions, that may be used as decision-making tools for daily life problems [14,49]. In this context, Speicher et al. (2019) suggested that MR can be considered as a type of collaboration that describes the interaction between physically separated users exploring AR and VR [45]. This definition includes mapping of the environment of an on-site AR collaborator, i.e., capturing more dimensional information about the local scene, which is reconstructed in VR for the remote collaborator [45,50] and so provides unique capabilities to achieve a common goal, e.g., improved communication cues for more efficient and easier collaboration [8,46,51,52]. Given the aforementioned panorama, we decided to include both terms in our analysis. 3.2 Search process Our review was made as inclusive as possible. We collected papers from the Scopus database (since it covers most top journals and conferences on Collaborative AR) using the search terms: (“Augmented Reality” OR “Mixed Reality”) AND (“Remote Collaboration” OR “Remote Cooperation” OR “Remote Assistance” OR “Remote Guidance” OR “Distributed Collaboration”) AND (“User Evaluation” OR “User Study” OR “User Experiment”) The search for the terms was made in the Title, Abstract, and Keywords fields. All search results published in conferences and journals between 2000 and 2020 were taken into consideration. Only publications in the English language were considered as this is the current ’lingua franca’ of the academic research. 3.3 Analysis process We obtained a total of 64 publications. The search results were analyzed individually to identify whether or not it supported evaluation of remote scenarios supported by solutions using MR or AR. Only 42 publications satisfied the defined criteria. We started by filtering the initial collection of publications to meet our objectives. We removed articles that were incorrectly selected in the search process (false positives) and identified only those articles that included user evaluation. The reviews of each paper focused on the following attributes (Tables 2 and 3): application areas and keywords; type of collaboration; type of task; types of devices used (regarding on-site and remote users); type of study; type of data collected; evaluation design; evaluation methods; number of participants (number of female participants); participant role; participants’ familiarity with each other; previous experience with AR/VR/MR; experimental context description; adaptation period provided; study average duration (min); recording of audio and video. 3.4 Validity limitations A considerable amount of effort was invested on the selection and review process. Although the Scopus bibliographic database has been used to cover a wide range of publication venues and topics, there may be limitations with the described method. The search terms used might be limiting, as other papers could have used different keywords to describe “Remote Collaboration”, “Augmented Reality”, “Mixed Reality” or “Evaluation”. Therefore, it remains likely that there are papers which may have not been included in this review. 3.5 Results Next, a high-level overview of the reviewed papers is provided (Tables 2 and 3), following a similar structure as the one used by Dey et al. (2018) in their systematic review [43], which is extended to include relevant aspects missing from the surveys analyzed in the previous section, such as collaboration details, task type, study type, data type, study design, evaluation methods, participants characteristics, experimental context, adaptation period, and duration. 3.5.1 User studies categorization The papers (Tables 2 and 3) have the following distribution by application areas: assistance (25 papers, 59.5%); assembly (11 papers, 26.2%); co-design (3 papers, 7.1%); social presence (2 paper, 4.8%); education (1 paper, 2.4%), as presented in the orange bubbles in Fig. 1. Regarding the collaboration details, 30 papers (71.4%) explored collaboration using a synchronous hierarchy approach, i.e., each member has a specific function or expertise and all team members are present and could act in real-time, while 11 papers (26.2%) studied synchronous parallel approach, where all elements have the same level of expertise and could act in real time and only 1 paper (2.4%) studied asynchronous parallel approach, i.e., all elements have the same level of expertise in which collaboration would take place at different times, as shown in the dark blue bubbles in Fig. 1. 3.5.2 Study design As shown in Table 2, 16 papers (38.7%) used a within-subjects design, while 15 papers (35.7%) used a between-subjects design. There were no mentions of a mixed-factorial design. In addition, 11 (26.2%) papers did not mention the method used, as illustrated in the green bubbles in Fig. 1. 3.5.3 Study type We found that most papers (33, 78.6%) were formal user studies. On the opposite, 7 papers (16.6%) reported conducting informal studies. Only 2 papers (4.8%) conducted user studies in the field, which shows a lack of experimentation in real-world conditions, as exhibit in the green bubbles in Fig. 1. 3.5.4 Task type As expected, most papers (26 out of 42, 61.9%) explored navigation, object selection and manipulation, forcing participants to communicate and use collaborative tools to provide indications to achieve a concrete goal. Additionally, 12 papers (28.6%) focused on assembly tasks using Lego bricks, or puzzles like tangram, pentominoes, origami, among others. Only 1 paper (2.4%) reported the use of an airplane cockpit as case study, as presented in the red bubbles in Fig. 1. This shows that there is an opportunity for conducting more user studies exploring different, more complex case studies, or even combinations of different types. Moreover, just 14 papers (33.33%) claim to have provided an adaptation period before the performance of the tasks, as shown in the purple bubbles in Fig. 1. Finally, the bulk of the user studies were conducted in an indoor environment, but only 21 papers (50%) described the experimental context, although no clear pattern emerged. 3.5.5 Evaluation methods and data type In terms of data type, 30 papers (71.4%) collected subjective and objective data, 11 papers (26.2%) collected only subjective data, and just 1 (2.4%) only objective data. Concerning the evaluation methods, we found that the most popular method is filling out questionnaires (40 papers, 95.2%), followed by assessing task performance (31 papers, 73.8%) with error/accuracy measures and task completion time. Then, user preference (28 papers, 66.7%) and finally interviews (5 papers, 11.9%), as illustrated in the light blue bubbles in Fig. 1. Note that many papers used more than one evaluation method, so the percentages sum to more than 100%. Another essential point: only 13 papers (31%) mentioned the average duration of the user study (58.5 min). Some papers mentioned the duration of the task, but no clear information on the collaboration process is provided, like dialog length, frequency of conversational turns, among others. Besides, none of the papers report to have conducted gesture or non-verbal behaviors analysis. This is supported by the lack of audio or video recording, since only 6 papers (14.3%) acknowledge to store this type of data. 3.5.6 Participants Our review of the participants shows that the number of participants involved in the analyzed studies ranged from 5 to 48, with an average of 21. Also, a total of 31 out of 42 papers (73.4%) reported involving female participants in their experiments, with the ratio of female participants to male participants being 47.6% of total participants in those 31 papers. Hence, most of the studies were run with young participants, mostly university students, rather than a more representative cross section of the population. Equally important, 23 papers (54.8%) stated that participants would perform the role of the on-site or remote user during the studies. Moreover, in 5 papers (11.9%) the participants would perform the on-site and remote role. 11 papers (26.2%) only allowed the participants to perform the on-site user, while 3 papers (7.1%) only allowed to perform the remote role. In these cases, the counterpart would be performed by a monitor, as presented in the brown bubbles in Fig. 1. Most papers, 32 out of 42 (76.2%) made no mention if participants knew each other, with only 9 clearly stating that information. Likewise, the same percentage did not mention any type of previous experience the participants might have with AR or MR systems. 3.6 Summary Our review (Table 4 and Fig. 1) shows that the dominant type of collaboration is based on the hierarchy approach focused on synchronous communication between participants. Also, that assistance and assembly are the main areas of application, exploring navigation, selection and manipulation tasks in indoor environments, during approximately one hour. On average, studies involved 21 participants, mostly young university students. Moreover, ruffly half of the papers reported that the participants would perform the role of the on-site or remote user during the studies. Besides, most papers lack information regarding if participants knew each other prior to the study and if they had previous experience with MR systems. The majority of the studies conducted are formal studies, collecting objective and subjective data using evaluation methods like questionnaires, task performance and user preferences in that order respectively. As for collaborative measures, most works focus on effectiveness, only checking if participants were able to accomplish a given task collaboratively. Moreover, the evaluation design is distributed between within-subjects and between-subjects. Besides, interviews are not used often, as is also the case of recording audio and video during the studies. In addition, half of the times the experimental context is not described and only one third of the times studies referred the existence of an adaptation period. It is important to report this last fact, as it can affect the way the collaboration process was performed between collaborators, i.e., those that had an opportunity to use, adapt and comprehend the technology that helped create a shared understanding prior to the tasks will easily interact better with their respective counterpart, when compared to the ones that have only done the adjustment process during the task itself. Another observation is that single-user evaluation methods are applied to collaborative tasks, which mainly focus on the comparison of technological aspects or interaction mechanisms based on rather simpler procedures. We argue that collaborative tasks must be difficult and long enough to encourage interaction between collaborators and for the AR-based solution being used to provide enough contribution. In general, tasks can benefit from deliberate drawbacks, and constraints, i.e., incorrect, contradictory, vague or missing information, to force more complicated situations and elicit collaboration. For example, suggest the use of an object which does not exist in the environment of the other collaborator or suggest removing a red cable, which is green in the other collaborator context. Such situations help introduce different levels of complexity, which go beyond the standard approaches used, and elicit more realistic real-life situations where the surroundings are not always perfect. Likewise, multiple procedures may be applied to an evaluation, while also exploring different levels of complexity, contextual changes in the surroundings environments, as well as stress conditions. 4 Critical analysis This section describes the main limitations hindering a better understanding regarding how AR supports collaborative work in remote scenarios. Analysis was mostly based in the results from the literature review process, complemented by meetings with domain experts, and authors’ own experience creating and conducting evaluation studies in this domain [Refs omitted for review purposes]. The contributions presented in this paper were conducted in the scope of a larger multidisciplinary research line, with a total of nine individuals with several years of expertise (minimal of 6 years, and a maximum of 40 years of experience) in the areas of Human–Computer Interaction (HCI), Virtual and Augmented Reality (VR/AR), Information Visualization (IV), Multimodal Interaction (MMI), as well as remote collaboration in several scenarios of application. To this effect, face-to-face and remote meetings were conducted, as well as focus group and brainstorm sessions (sometimes with different combinations of experts according to their availability) over several months. To conclude the section, a global assessment of the field maturity is attempted, followed by a critical analysis on how that may affect the road ahead. 4.1 Main limitations As was aforementioned, the characterization and evaluation of the collaborative process in remote scenarios using AR-based solutions have been reported mainly using single-user methods focusing on technological aspects, thus lacking information and focus on the important dimensions of collaboration. As a result the following main limitations can be identified. Limitation 1: partial evaluation According to Merino et al. “designing appropriate evaluations that examine MR/AR is challenging, and suitable guidance to design and conduct evaluations of MR/AR are largely missing” [37]. This fact is further evident in scenarios of remote collaboration, since the logistics associated with carrying out evaluations is even more demanding due to a significant number of variables that must be considered. The existence of two or more collaborators makes it more difficult to evaluate the solution as a whole, given that it requires to perform multiple evaluations at the same time and that validation from all users is required [40]. As a consequence, there is a clear lack in addressing crucial aspects of collaboration like how was the relation and communication of the collaborators during the tasks (only 10 out of 42 papers reported such information and just 6 recorded audio or video during the studies), whether they had previous experience with AR/VR/MR technologies and were able to use the available solutions to their full potential (a topic just mentioned by 11 out of 42 papers), how the available information was used to support the accomplishment of the tasks, among other aspects. In this context, trying to apply conventional evaluation techniques to collaborative settings, without adapting them can lead to an incomplete vision of the process of collaboration and in turn to dubious results, falling short to retrieve the necessary amount of data for more comprehensive evaluations and characterizations of the collaborative process which may lead to an incomplete vision of the process of collaboration. Given the complex environments and situations collaborators may encounter, such methods alone provide insufficient information and rarely are good indicators for improving distributed solutions [31,38,92,93]. Limitation 2: lack of contextual information Remote collaboration represents high levels of data by involving different types of distributed collaborators, tasks and in encompassing dynamical environments with contextual data. Dey et al. revealed that “work needs to be done towards making AR-based remote collaboration akin to the real world with not only shared understanding of the task but also shared understanding of the other collaborators emotional and physiological states” [43]. Moreover, Ratcliffe et al. suggested that “remote settings introduce additional uncontrolled variables that need to be considered by researchers, such as potential unknown distractions, trust in participants and their motivation, and issues with remote environmental spaces” [94]. However, our analysis shows that half of the papers analyzed (21 out of 42) did not described the experimental context of collaborators, and that 76.2% (32 out of 42) did not report participants knowledge of each other. The same percentage of papers did not mention previous experience with AR or MR technologies, as illustrated in Table 3. By doing this, evaluation scenarios disregard information such as contextual or user related data, obtaining only superficial results. Limitation 3: failure situations are not contemplated Bai et al. stated that: ”as deeper insight is obtained into the affordances of AR collaboration, more complex activities should be supported” [29]. This is also corroborated by Ens et al. which highlighted that “as new capabilities emerge, (...) we expect to see this trend continue, with an initial focus on perfecting the systems, followed by deeper explorations of collaboration” [14]. Furthermore, this is also supported by our analysis from the selected data set, which shows that failure situations were not taken into account by any study. For example, in the case of failure to achieve the intended goals of the collaborative process, how can we understand what went wrong? Was it caused by problems in participants communication, by too much augmented information being displayed, by the actions of a particular collaborator that did not followed correctly some indications, or was it caused by an error in the AR-based solution being used? Limitation 4: lack of theories and guidelines Literature shows an absence of rules, guidelines and theories to guide the characterization of the collaborative process using solutions mediated by AR. For example, Dey et al. suggests that “opportunities for increased user studies in collaboration, more use of field studies, and a wider range of evaluation methods” [43]. Moreover, Ens et al. reported that “MR systems faced significant engineering hurdles, and have only recently started catching up to provide new theories and lessons for collaboration” [14]. A better evaluation strategy is required by researchers and developers to obtain a comprehensive description, given the challenges involved in evaluating many aspects that may influence the way collaboration occurs, e.g., relations between individuals, their interconnection as a team and how the use of AR/MR technologies affected the accomplishment of the tasks in relation to the collaborative effort. Limitation 5: minimal support in existing frameworks The constraints and challenges identified may change according to the maturity of the solution being used, the goal of the evaluation, the participants individual and group characteristics, among other parameters. In this context, existing frameworks are not sufficiently well suited to describe how collaboration mediated by AR/MR technologies happens, thus ignoring detail on crucial aspects of collaboration [7,14,29,36,39,43]. For example, Bai et al. emphasized that “it can be hard to isolate the factors that are specifically relevant to collaboration” [29]. Likewise, Ens et al. outlined that “frameworks for describing groupware and MR systems are not sufficient to characterize how collaboration occurs through this new medium” [14]. In addition, Ratcliffe et al. communicate that “the infrastructure for collecting and storing this (mass) of XR data remotely is currently not fully implemented, and we are not aware of any end-to-end standardized framework” [94]. Therefore, integration of proper characterization and evaluation methods and guidelines, covering different contexts of use and tasks, running in its intended (real or simulated) environment are of paramount importance. Limitation 6: limited reporting of outcomes There is now an opportunity to convince researchers to better document their work, and help improve evaluations and characterizations that are, in our view, a bottleneck in this research area. Currently, researchers struggle to analyze the state of the art, since much information on existing publications lack detail on the collaborative process as previously demonstrated. This may happen since most of the research efforts have been devoted on creating the enabling technology. 4.2 Maturity of the field To put in perspective the evolution of the field, as well as consider current limitations, this section concludes with the analysis of the status of the area according to the BRETAM model (Fig. 2) [95]. This model has been considered useful for the introduction of new knowledge, technology or products and adopted in several scenarios, including for example, in a multimodal interaction review [96]. According to the current panorama reported in this publication, we argue that it is possible to situate the field of remote collaboration mediated by AR between the Replication and Empiricism phases of the BRETAM model as illustrated in Fig. 2. We argue that remote collaboration mediated by AR has already passed the Breakthrough phase, which means research institutions worldwide can replicate the basic concepts, as demonstrated by the last few decades of research [25]. The Replication and Empiricism phases on the other hand imply increased ideas to generate enough experience, leading to empirical design rules. As such, these phases seem adequate to the overall panorama described in this publication, reinforcing the need to deepen the understanding and characterization of the collaborative process through methods, frameworks, guidelines and various user studies. In our view, remote collaboration mediated by AR has still not reached the Theory phase as it requires enough empirical experience to model the basis of success and failure, which cannot be performed without proper methods for the characterization and evaluation of the collaborative process [14,25]. Likewise, the Automation phase was also rejected, which implies automation of the scientific data-gathering and analysis, since existing systems are still limited by the contextual and multi-user data they are able to collect, thus not being sufficient to characterize how collaboration occurs [14]. As such, without fulfilling the previous phases, the field cannot be positioned into the Maturity phase, i.e., turn to cost reduction and quality improvements in what describes a mature technology [95]. 5 Charting out a roadmap for the characterization and evaluation of the collaborative process According to what was said in the critical analysis, it is important to address the main limitations to make the field achieve the Theory, Automation and Maturity phases of the BRETAM model [95]. Aiming at contributing to that, in this section we propose a first roadmap, to deal with the most pressing issues (Fig. 3), composed by five key topics: • definition of dimensions of collaboration to face the partial characterization of the collaborative process; • systematization of perspectives based on the acquired knowledge of the field, facing the lack of theories and guidelines; • creation of new paradigms, architectures and frameworks to answer the limited support to development and evaluation of existing ones; • enhanced support for data gathering, leading to better design, development and evaluation with distributed users supported by AR; • new and better outcomes from the evaluation to support the assessment, leading to the creation of new theories, as well as improve the lack of contextual information. 5.1 Definition of dimensions of collaboration First, it is important to identify dimensions that need to be taken into consideration when performing the characterization of the collaborative process. In practical terms, given a concrete application context and a problem, the research community is still not able to provide an overall definition of the collaborative AR system that addresses it. Although there are works that have presented some dimensions of collaboration, existing efforts are mostly oriented towards technology. As the field matures, it is normal new proposals emerge to address new aspects related to collaboration. A comprehensive set of dimensions must be defined to more thoroughly classify and discuss the contributions of the collaborative work effort, not only addressing the technological features being used, but also encompassing the characteristics of the context. For example, Ens et al. stated the following: “While somewhat useful, the dimensions we used are fairly technical, and focus mainly on mechanical aspects of the system or properties of the underlying technologies. (...) Perhaps additional dimensions with a greater focus on user experience would better allow for capturing the essence of collaborative scenarios“ [14]. Therefore, some of the existing dimensions might still not reflect the full scope of some categories by encompassing all possibilities. Therefore, this effort cannot be intended as a closed work, but should, instead, be taken as the grounds that might enable the community to elaborate, expand, and refine the field. This may be achieved by analyzing the literature regarding collaborative work supported by AR, in particular, existing categorization efforts [13,14,25,45,98–101]. Another possibility is to adopt a conceptual-to-empirical methodology by using a participatory design process, i.e., actively involving stakeholders in focus group and brainstorming sections. This entails going beyond Collaborative AR literature, considering other domains (e.g., CSCW, Groupware, Telerehabilitation, Remote Medicine, among others) that may be relevant to characterize the collaborative effort, to identify which dimensions should be taken into account when we move from asking what existing systems can do, to understanding what they would be able to do in particular contexts, i.e., the value of AR to the collaborative process. 5.2 Systematization of perspectives for the field Ens et al. report that when considering if it is possible to clearly describe distinct categories of collaborative MR research based on the existing dimensions, the answer is “to some extent, yes, however the result is not wholly satisfying (...) these dimensions do not suffice to describe all scenarios” [14]. Therefore, another area of research that needs to be addressed given the lack of theories and guidelines [14,43], is the need to bring new dimensions forward into conceptual models, guidelines, taxonomies and ontologies, that might foster harmonization of perspectives for the field, thus creating a common ground for systematization and discussion [100,102]. Through these, it would be possible to structure the characterization of the collaboration process, which can form the basis for analysis and comparison, fostering a more detailed understanding of the field, and in turn ensure that the research adds to the body of knowledge and provides enough context and evidence to enable a transparent account [103] and transferability [104]. These can also work as a knowledge repository for evaluation, allowing researchers to observe and compare a variety of results inside the same domain and make considerations and conclusions about specific nuances of collaboration. For example, the proposal of human-centered approaches, i.e., focusing on collaboration, instead of the technology, might bring forward a perspective that is not rapidly deprecated with the advancements of technology [105]. To create conceptual models and taxonomies, it is important to ensure the dimensions of collaboration contain categories and characteristics that are mutually exclusive and collectively exhaustive [106,107]; Moreover, a detailed explanation of these objects of interest must be included, following, for example, a similar approach to the one used by Zollmann et al. [108]. It is also relevant to include discussion and refinement over several iterations with domain experts, to verify if the established dimensions, categories and characteristics are well defined, need to be merged, or if new ones can be identified [106]. Regarding ontologies, literature shows that its design is considered a creative process and no two ontologies by different individuals would be the same, since the applications of the ontology and the designer’s understanding of the domain will undoubtedly affect the ontology design choices [109,110]. As such, one possible strategy is to adapt existing ontologies when they exist, or as an alternative when this is not the case, define and populate a new ontology considering relevant dimensions of collaboration as the core classes and establish their relations with each other based on the targeted application of the ontology. 5.3 Creation of new paradigms, architectures and frameworks According to Merino et al. “as MR/AR technologies become more mature, questions that involve human aspects will gain focus in MR/AR research. Consequently, we expect that future MR/AR papers will elaborate on human-centered evaluations that involve not only the analysis of user performance and user experience, but also the analysis of other scenarios, like understanding the role of MR/AR in working places and in communication and collaboration” [37]. However, there is no standard methodology for characterization and evaluation, specifically tailored to assess how remote collaboration occurs through AR/MR technologies. In this vein, without the appropriate paradigms, methods and mechanisms, the research community does not accumulate enough experience to improve collaboration between distributed collaborators [7,14,29,36,37,39,43,92]. Currently, there is too much focus on post-task evaluation. New paradigms must also consider continuous assessment, i.e., giving proper relevance to evaluation conducted during the accomplishment of open challenges, instead of pre-defined tasks, which fail short to mimic real scenarios of remote collaboration. As such, architectures and frameworks capable of supporting the new paradigm(s) must be created, to assist researchers conducting future user studies, while eliciting more characterization of the collaboration process in remote scenarios. Such frameworks must include support for: • defining the evaluation scope for individual and collective assessment by properly identifying which dimensions of collaboration will be evaluated; • detailing collaborative challenges to be performed, including specification of the users minimum level of knowledge, definition of each collaborator activity, as well as definition of the procedures; • defining the experimental setup and design, ensuring each dimension is defined in terms of the necessary variables and how they should be measured according to specific techniques; • conducting data gathering through the use of a distributed evaluation tools focusing in the dimensions proposed specifically for remote collaboration; • performing data analysis, including inspection of what happened during the tasks, to understand how the collaboration process occurred over time. 5.4 Development of tools for improved data gathering According to Marques et al. “it is essential the existence of holistic evaluation strategies that monitor the use and performance of the proposed solutions regarding the team, its members, and the technology, allowing adequate characterization and report of collaborative processes” [36]. To achieve this, the operationalization of data gathering should also deserve its own line of work due to its importance. It is paramount to conduct thorough collaborative user studies to provide new perspectives [14,36–38,111]. A better evaluation process can be supported by improved data collection and data visualization tools [92,112]. In this context, it is necessary to collect, process and analyze a multiplicity of data, e.g., context, history, user related information like actions, emotional state, as well as the results of processing the various components of the data gathering tools, aiming at obtaining a more comprehensive understanding. To accomplish this, tools must be designed and developed to allow researchers to run multiple evaluations at different locations simultaneously, following a distributed paradigm [36]. In this process, researchers should be able define measures, custom logging and register interesting events they detect, which can be later reviewed in post-task analysis, adapting and extending, for example the works by Pereira et al. [93,113,114]. Likewise, the following factors are crucial and must be taken in account to better understand the real impact of each aspect in the collaborative effort: team, collaborative tasks, context and AR-based solution [36]. These factors can help portrait the conditions in which collaborators performed a given action, received information or requested assistance. In addition, they can be used to assert uncommon situations or identify patterns that can lead to new understanding of a given artifact, as well as identify new research questions. Therefore, such tools are essential to help researchers when performing judgment over evaluation results. 5.5 New and better outcomes to support the assessment A better characterization of the collaborative process coupled with improved and specific evaluation tools and methods will provide ground to improve how research is reported. Thus, increasing the awareness of researchers about the different dimensions of collaboration and elicit better reporting, as researchers understand the need to improve how they describe the nuances associated to the collaborative effort of their work. Currently, in most cases, data relevant to characterize the collaborative context is not reported. To elaborate, most works focus only on individual performance, on the technological aspects of the AR-based solution or in quantifying effectiveness of tasks. It is important to consider a wide range of information, namely individual and team personalities, motivations, performances, behaviors, who completed the tasks and who provided instructions, how was the communication process, as well as duration and type of interactions with the collaborative AR-based technology, among other aspects when analyzing data and establishing conclusions. The reporting process must also integrate the context in which the collaborative effort took place, thus allowing the creation of a better understanding of the surrounding conditions, while contributing to support replication of such context, if they are relevant to other researchers, in future studies. Moreover, a complete definition of the data used to substantiate the usefulness of the results reported must be included, as well as the measures used, how was the data computed, based on what criteria, etc. This is essential to move towards replication and interpretability across contributions in the field. A more systematic reporting can, in turn, lead to a community setting that enables easier communication, understanding, reflection, comparison, refining, as well as building on existing research and foster harmonization of perspectives for the field. Furthermore, researchers can also compare their outcomes, as this is also a good opportunity for reflecting and refining. It is important to use what is learned during the studies and identify aspects which did not go according to what was expected or select additional ones which may improve on existing guidelines for future user studies. 6 Conclusions and future work Collaborative AR solutions can be powerful tools for analysis, discussion and support of complex problems and situations in remote scenarios. By bringing different and sometimes opposing points of view together, such solutions can lead to new insights, innovative ideas, and interesting artifacts. However, most research efforts have been devoted to creating the enabling technology for supporting the design and development of such solutions. Hence, the characterization and evaluation of the collaborative process is an essential, but a very difficult endeavor nowadays. This paper describes a critical analysis supported by surveys that addressed evaluation and user studies in scenarios of remote collaboration mediated by AR. In addition, a literature review on works ranging from 2000 to 2020 is also presented. Based on the limitations and challenges identified, we argue that remote collaboration mediated by AR is currently between the Replication and Empiricism phases of the BRETAM model. To contribute to an advance to Theory, Automation and Maturity phases, based in the critical analysis, we propose a roadmap for important research actions that need to be addressed to facilitate and elicit more characterization of the collaboration process using AR-based solutions in the future. Work is being continued through the creation of a conceptual model and taxonomy, as well as an initial architecture and framework aligned with the proposed roadmap. These can form the basis for a common ground, as well as the development of a framework for researchers who want to follow best practices in designing their own collaborative AR user studies in remote scenarios. CRediT authorship contribution statement Bernardo Marques: Conceptualization, Methodology, Validation, Formal analysis, Investigation, Data curation, Writing – original draft, Writing – review & editing, Visualization, Project administration, Funding acquisition. António Teixeira: Conceptualization, Methodology, Writing – original draft, Writing – review, Visualization. Samuel Silva: Methodology, Investigation, Writing – original draft, Writing – review & editing. João Alves: Validation, Formal analysis, Investigation, Data curation, Writing – original draft. Paulo Dias: Conceptualization, Investigation, Data curation, Writing – original draft, Writing – review & editing, Supervision, Funding acquisition. Beatriz Sousa Santos: Conceptualization, Methodology, Investigation, Writing – original draft, Writing – review & editing, Supervision Funding acquisition. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise. This research was developed in the scope of the Ph.D. grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT , in the context of the project [UID/CEC/00127/2019]. References [1] Alem L. Tecchia F. Huang W. HandsOnVideo: Towards a gesture based mobile AR system for remote collaboration Recent Trends of Mobile Collaborative Augmented Reality Systems 2011 135 148 Alem L, Tecchia F, Huang W. Handsonvideo: Towards a gesture based mobile ar system for remote collaboration. In: Alem L, Huang W, editors. Recent Trends of Mobile Collaborative Augmented Reality Systems.2011, p. 135–148,. [2] Johnson S, Gibson M, Mutlu B. Handheld or handsfree? Remote collaboration via lightweight head-mounted displays and handheld devices. In: Proceedings of the 18th ACM conference on computer supported cooperative work & social computing; 2015, p. 1825–36. [3] Lukosch S. Billinghurst M. Alem L. Kiyokawa K. Collaboration in augmented reality Computer Supported Cooperative Work, CSCW 2015 2015 515 525 Lukosch S, Billinghurst M, Alem L, Kiyokawa K. Collaboration in Augmented Reality. In: Computer Supported Cooperative Work, CSCW 2015 vol. 24.2015, p. 515–525,. [4] Schneider M, Rambach J, Stricker D. Augmented reality based on edge computing using the example of remote live support. In: 2017 IEEE international conference on industrial technology; 2017, p. 1277–82. [5] Thomas P.J. CSCW Requirements and Evaluation 1996 Springer-Verlag Berlin, Heidelberg Thomas PJ. CSCW Requirements and Evaluation. Berlin, Heidelberg: Springer-Verlag;1996. [6] Kim K. Billinghurst M. Bruder G. Duh H.B. Welch G.F. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017) IEEE Trans Vis Comput Graphics 24 2018 2947 2962 Kim K, Billinghurst M, Bruder G, Duh HB, Welch GF. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017). IEEE Transactions on Visualization and Computer Graphics2018a;24:2947–2962,. [7] Kim S. Billinghurst M. Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration Comput Support Coop Work: CSCW: An Int J 27 3–6 2018 569 607 Kim S, Billinghurst M, Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal2018b;27(3-6):569–607,. [8] Kim S. Lee G. Billinghurst M. Huang W. The combination of visual communication cues in mixed reality remote collaboration J Multimodal User Interf 14 4 2020 321 335 Kim S, Lee G, Billinghurst M, Huang W. The combination of visual communication cues in mixed reality remote collaboration. Journal on Multimodal User Interfaces2020a;14(4):321–335,. [9] Kim S. Billinghurst M. Kim K. Multimodal interfaces and communication cues for remote collaboration J Multimodal User Interf 14 4 2020 313 319 Kim S, Billinghurst M, Kim K. Multimodal interfaces and communication cues for remote collaboration. Journal on Multimodal User Interfaces2020b;14(4):313–319,. [10] Arias E. Eden H. Fischer G. Gorman A. Scharff E. Transcending the individual human mind-creating shared understanding through collaborative design ACM Trans Comput-Human Inter 7 2000 84 113 Arias E, Eden H, Fischer G, Gorman A, Scharff E. Transcending the Individual Human Mind-Creating Shared Understanding through Collaborative Design. ACM Transactions on Computer-Human Interaction2000;7:84–113,. [11] Grudin J. Poltrock S. The Encyclopedia of Human-Computer Interaction 2013 The Interaction Design Foundation Grudin J, Poltrock S. The Encyclopedia of Human-Computer Interaction. The Interaction Design Foundation;2013. [12] Billinghurst M. Clark A. Lee G. A survey of augmented reality Found Trends Human–Comput Interact 8 2015 73 272 Billinghurst M, Clark A, Lee G. A Survey of Augmented Reality. Foundations and Trends in Human–Computer Interaction2015;8:73–272,. [13] Jalo H, Pirkkalainen H, Torro O, Kärkkäinen H, Puhto J, Kankaanpää T. How can collaborative augmented reality support operative work in the facility management industry?. In: Proceedings of the 10th international joint conference on knowledge discovery, knowledge engineering and knowledge management; 2018. p. 41–51. [14] Ens B. Lanir J. Tang A. Bateman S. Lee G. Piumsomboon T. Billinghurst M. Revisiting collaboration through mixed reality: The evolution of groupware Int J Human-Comput Stud 131 2019 81 98 Ens B, Lanir J, Tang A, Bateman S, Lee G, Piumsomboon T,, others,Revisiting Collaboration through Mixed Reality: The Evolution of Groupware. International Journal of Human-Computer Studies2019;131:81–98,. [15] Bruno F. Barbieri L. Marino E. Muzzupappa M. D’Oriano L. Colacino B. An augmented reality tool to detect and annotate design variations in an industry 4.0 approach Int J Adv Manuf Technol 105 1 2019 875 887 Bruno F, Barbieri L, Marino E, Muzzupappa M, D’Oriano L, Colacino B. An augmented reality tool to detect and annotate design variations in an Industry 4.0 approach. The International Journal of Advanced Manufacturing Technology2019;105(1):875–887,. [16] Ong S.K. Yuan M.L. Nee A.Y.C. Augmented reality applications in manufacturing: A survey Int J Prod Res 46 10 2008 2707 2742 Ong SK, Yuan ML, Nee AYC. Augmented reality applications in manufacturing: A survey. International Journal of Production Research2008;46(10):2707–2742,. [17] Wang X. Ong S.K. Nee A.Y.C. A comprehensive survey of augmented reality assembly research Adv Manuf 4 1 2016 1 22 Wang X, Ong SK, Nee AYC. A comprehensive survey of augmented reality assembly research. Advances in Manufacturing2016;4(1):1–22,. [18] Palmarini R. Erkoyuncu J.A. Roy R. Torabmostaedi H. A systematic review of augmented reality applications in maintenance Robot Comput-Integr Manuf 49 2018 215 228 Palmarini R, Erkoyuncu JA, Roy R, Torabmostaedi H. A systematic review of augmented reality applications in maintenance. Robotics and Computer-Integrated Manufacturing2018;49:215–228,. [19] Hall M, McMahon CA, Bermell-Garcia P, Johansson A, Ravindranath R. Capturing synchronous collaborative design activities: A state-of-the-art technology review. In: Proceedings of international design conference, DESIGN 2018; 2018. p. 347–58. [20] Billinghurst M. Grand challenges for augmented reality Front Virtual Reality 2 2021 1 4 Billinghurst M. Grand Challenges for Augmented Reality. Frontiers in Virtual Reality2021;2:1–4,. [21] Altug Y. Mahdy A.M. A perspective on distributed and collaborative augmented reality Int J Recent Trends Human Comput Interact (IJHCI) 7 2016 23 41 Altug Y, Mahdy AM. A Perspective on Distributed and Collaborative Augmented Reality. International Journal of Recent Trends in Human Computer Interaction (IJHCI)2016;7:23–41,. [22] Choi S. Kim M. Lee J. Situation-dependent remote AR collaborations: Image-based collaboration using a 3D perspective map and live video-based collaboration with a synchronized VR mode Comput Ind 101 2018 51 66 Choi S, Kim M, Lee J. Situation-dependent remote AR collaborations: Image-based collaboration using a 3D perspective map and live video-based collaboration with a synchronized VR mode. Computers in Industry2018;101:51–66,. [23] Bottani E. Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade IISE Trans 51 3 2019 284 310 Bottani E, Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade. IISE Transactions2019;51(3):284–310,. [24] van Lopik K. Sinclair M. Sharpe R. Conway P. West A. Developing augmented reality capabilities for industry 4.0 small enterprises: Lessons learnt from a content authoring case study Comput Ind 117 2020 van Lopik K, Sinclair M, Sharpe R, Conway P, West A. Developing augmented reality capabilities for industry 4.0 small enterprises: Lessons learnt from a content authoring case study. Computers in Industry2020;117. [25] Belen R.A.J. Nguyen H. Filonik D. Favero D.D. Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018 AIMS Electron Electr Eng 3 2019 181 Belen RAJ, Nguyen H, Filonik D, Favero DD, Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018. AIMS Electronics and Electrical Engineering2019;3:181,. [26] Oda O, Elvezio C, Sukan M, Feiner S, Tversky B. Virtual replicas for remote assistance in virtual and augmented reality. In: Proceedings of the 28th annual acm symposium on user interface software & technology - UIST ’15; 2015. p. 405–15. [27] Elvezio C. Sukan M. Oda O. Feiner S. Tversky B. Remote collaboration in AR and VR using virtual replicas ACM SIGGRAPH 2017 2017 1 2 Elvezio C, Sukan M, Oda O, Feiner S, Tversky B. Remote collaboration in AR and VR using virtual replicas. ACM SIGGRAPH 20172017;:1–2,. [28] Barroso Ja, Fonseca L, Marques B, Dias P, Sousa BS. Remote collaboration using mixed reality: Exploring a shared model approach through different interaction methods. In: Proceedings of european conference on computer-supported cooperative work, ecscw 2020 posters; 2020. p. 1–6. [29] Bai Z. Blackwell A.F. Analytic review of usability evaluation in ISMAR Interact Comput 24 6 2012 450 460 Bai Z, Blackwell AF. Analytic review of usability evaluation in ISMAR. Interacting with Computers2012;24(6):450 – 460,. [30] Zillner J, Mendez E, Wagner D. Augmented reality remote collaboration with dense reconstruction. In: Adjunct proceedings - 2018 IEEE international symposium on mixed and augmented reality, ismar-adjunct 2018; 2018. p. 38–9. [31] Neale DC, Carroll JM, Rosson MB. Evaluating computer-supported cooperative work: Models and frameworks. In: Proceedings of the 2004 ACM conference on computer supported cooperative work, CSCW ’04; 2004. p. 112–21. [32] de Souza Cardoso L.F. Mariano F.C.M.Q. Zorzal E.R. A survey of industrial augmented reality Comput Ind Eng 139 2020 106159 de Souza Cardoso LF, Mariano FCMQ, Zorzal ER. A survey of industrial augmented reality. Computers & Industrial Engineering2020;139:106159,. [33] Röltgen D. Dumitrescu R. Classification of industrial augmented reality use cases Procedia CIRP 91 2020 93 100 Röltgen D, Dumitrescu R. Classification of industrial augmented reality use cases. Procedia CIRP2020;91:93 – 100,. [34] Fernández del Amo I. Erkoyuncu J.A. Roy R. Wilding S. Augmented reality in maintenance: An information-centred design framework Proc Manuf 19 2018 148 155 Fernández del Amo I, Erkoyuncu JA, Roy R, Wilding S. Augmented reality in maintenance: An information-centred design framework. Procedia Manufacturing2018;19:148 – 155,. [35] Jetter J. Eimecke J. Rese A. Augmented reality tools for industrial applications: What are potential key performance indicators and who benefits? Comput Hum Behav 87 2018 18 33 Jetter J, Eimecke J, Rese A. Augmented reality tools for industrial applications: What are potential key performance indicators and who benefits? Computers in Human Behavior2018;87:18–33,. [36] Marques B, Teixeira A, Silva S, Alves Ja, Dias P, Santos BS. A conceptual model for data collection and analysis for AR-based remote collaboration evaluation. In: IEEE international symposium on mixed and augmented reality, ISMAR; 2020. [37] Merino L, Schwarzl M, Kraus M, Sedlmair M, Schmalstieg D, Weiskopf D. Evaluating mixed and augmented reality: A systematic literature review (2009–2019). In: IEEE international symposium on mixed and augmented reality, ISMAR; 2020. p. 438–51. [38] Hamadache K. Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation Groupware: Design, Implementation, and Use 2009 206 221 Hamadache K, Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation. In: Groupware: Design, Implementation, and Use.2009, p. 206–221,. [39] Antunes P. Herskovic V. Ochoa S.F. Pino J.A. Reviewing the quality of awareness support in collaborative applications J Syst Softw 89 2014 146 169 Antunes P, Herskovic V, Ochoa SF, Pino JA. Reviewing the quality of awareness support in collaborative applications. Journal of Systems and Software2014;89:146–169,. [40] Patel H. Pettitt M. Wilson J.R. Factors of collaborative working: A framework for a collaboration model Applied Ergon 43 1 2012 1 26 Patel H, Pettitt M, Wilson JR. Factors of collaborative working: A framework for a collaboration model. Applied ergonomics2012;43(1):1–26,. [41] Duenser A. Grasset R. Billinghurst M. A survey of evaluation techniques used in augmented reality studies SIGGRAPH 2008 2008 Duenser A, Grasset R, Billinghurst M. A survey of evaluation techniques used in augmented reality studies. In: SIGGRAPH 2008.2008,. [42] Feng Zhou, Duh HB, Billinghurst M. Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR. In: International symposium on mixed and augmented reality; 2008. p. 193–202. [43] Dey A. Billinghurst M. Lindeman R.W. Swan J.E. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014 Front Robot AI 5 2018 37 Dey A, Billinghurst M, Lindeman RW, Swan JE. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014. Frontiers in Robotics and AI2018;5:37,. [44] Gutwin C. Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware ACM Trans Comput-Hum Interact 6 3 1999 243 281 Gutwin C, Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Trans Comput-Hum Interact1999;6(3):243–281,. [45] Speicher M. Hall B.D. Nebeling M. What is mixed reality? Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems 2019 1 15 Speicher M, Hall BD, Nebeling M. What is mixed reality? In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. CHI ’19;2019, p. 1–15,. [46] Kato H, Billinghurst M. Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings 2nd IEEE and ACM international workshop on augmented reality (IWAR’99); 1999. p. 85–94. [47] Milgram P. Takemura H. Utsumi A. Kishino F. Augmented reality: A class of displays on the reality-virtuality continuum Photon Indust Appl 1994 282 292 Milgram P, Takemura H, Utsumi A, Kishino F. Augmented reality: A class of displays on the reality-virtuality continuum. Photonics for industrial applications1994;:282–292,. [48] Milgram P. Kishino F. A taxonomy of mixed reality visual displays IEICE Trans Inform Syst 0916-8532 77 12 1994 1321 1329 Milgram P, Kishino F. A taxonomy of mixed reality visual displays. IEICE TRANSACTIONS on Information and Systems1994;77(12):1321–1329,. [49] Rokhsaritalemi S. Sadeghi-Niaraki A. Choi S.-M. A review on mixed reality: Current trends, challenges and prospects Appl Sci 10 2 2020 Rokhsaritalemi S, Sadeghi-Niaraki A, Choi SM. A review on mixed reality: Current trends, challenges and prospects. Applied Sciences2020;10(2). [50] Bai H. Sasikumar P. Yang J. Billinghurst M. A user study on mixed reality remote collaboration with eye gaze and hand gesture sharing Conf Human Factors Comput Syst - Proc 2020 1 13 Bai H, Sasikumar P, Yang J, Billinghurst M. A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing. Conference on Human Factors in Computing Systems - Proceedings2020;:1–13,. [51] Masai K, Kunze K, Sugimoto M, Billinghurst M. Empathy glasses. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems, CHI EA ’16; 2016. p. 1257–63. [52] Piumsomboon T. Dey A. Ens B. Lee G. Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality Front Robot AI 6 2019 Piumsomboon T, Dey A, Ens B, Lee G, Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality. Frontiers Robotics AI2019a;6. [53] Wang P. Bai X. Billinghurst M. Zhang S. Han D. Sun M. Wang Z. Lv H. Han S. Haptic feedback helps me? A VR-SAR remote collaborative system with tangible interaction Int J Human-Comput Interact 2020 Wang P, Bai X, Billinghurst M, Zhang S, Han D, Sun M,, others,Haptic feedback helps me? a VR-SAR remote collaborative system with tangible interaction. International Journal of Human-Computer Interaction2020;. [54] Rhee T. Thompson S. Medeiros D. Dos Anjos R. Chalmers A. Augmented virtual teleportation for high-fidelity telecollaboration IEEE Trans Vis Comput Graphics 2020 Rhee T, Thompson S, Medeiros D, Dos Anjos R, Chalmers A. Augmented virtual teleportation for high-fidelity telecollaboration. IEEE Transactions on Visualization and Computer Graphics2020;. [55] Teo T, Lee G, Billinghurst M, Adcock M. Investigating the use of different visual cues to improve social presence within a 360 mixed reality remote collaboration. In: Proceedings - VRCAI 2019: 17th ACM siggraph international conference on virtual-reality continuum and its applications in industry; 2019. [56] Teo T, Hayati A, Lee G, Billinghurst M, Adcock M. A technique for mixed reality remote collaboration using 360 panoramas in 3D reconstructed scenes. In: Proceedings of the ACM symposium on virtual reality software and technology, VRST; 2019. [57] Sasikumar P, Gao L, Bai H, Billinghurst M. Wearable remotefusion: A mixed reality remote collaboration system with local eye gaze and remote hand gesture sharing. In: Adjunct Proceedings of the 2019 IEEE international symposium on mixed and augmented reality, ismar-adjunct 2019; 2019. p. 393–4. [58] Mahmood T, Fulmer W, Mungoli N, Huang J, Lu A. Improving information sharing and collaborative analysis for remote geospatial visualization using mixed reality. In: Proceedings - 2019 IEEE international symposium on mixed and augmented reality, ISMAR 2019; 2019. p. 236–47. [59] Teo T. Lawrence L. Lee G. Billinghurst M. Adcock M. Mixed reality remote collaboration combining 360 video and 3D reconstruction Conf Human Factors Comput Syst 2019 Teo T, Lawrence L, Lee G, Billinghurst M, Adcock M. Mixed reality remote collaboration combining 360 video and 3D reconstruction. Conference on Human Factors in Computing Systems2019c;. [60] Piumsomboon T. Lee G. Irlitti A. Ens B. Thomas B. Billinghurst M. On the shoulder of the giant: A multi-scale mixed reality collaboration with 360 video sharing and tangible interaction Conf Human Factors Comput Syst 2019 Piumsomboon T, Lee G, Irlitti A, Ens B, Thomas B, Billinghurst M. On the shoulder of the giant: A multi-scale mixed reality collaboration with 360 video sharing and tangible interaction. Conference on Human Factors in Computing Systems2019b;. [61] Yoon B, Kim H-I, Lee G, Billinqhurst M, Woo W. The effect of avatar appearance on social presence in an augmented reality remote collaboration. In: 26th IEEE conference on virtual reality and 3d user interfaces, vr 2019 - proceedings; 2019. p. 547–56. [62] Wang P. Zhang S. Billinghurst M. Bai X. He W. Wang S. Sun M. Zhang X. A comprehensive survey of AR/MR-based co-design in manufacturing Eng Comput 2019 Wang P, Zhang S, Billinghurst M, Bai X, He W, Wang S,, others,A comprehensive survey of AR/MR-based co-design in manufacturing. Engineering with Computers2019;. [63] Lee G, Teo T, Kim S, Billinghurst M. A user study on MR remote collaboration using live 360 video. In: Proceedings of the 2018 ieee international symposium on mixed and augmented reality, ISMAR 2018; 2019. p. 153–64. [64] Waldow K. Fuhrmann A. Grünvogel S. Investigating the effect of embodied visualization in remote collaborative augmented reality Lecture Notes in Comput Sci 11883 LNCS 2019 246 262 Waldow K, Fuhrmann A, Grünvogel S. Investigating the effect of embodied visualization in remote collaborative augmented reality. Lecture Notes in Computer Science2019;11883 LNCS:246–262,. [65] Teo T. Lee G. Billinghurst M. Adcock M. Hand gestures and visual annotation in live 360 panorama-based mixed reality remote collaboration ACM Int Conf Proc Ser 2018 406 410 Teo T, Lee G, Billinghurst M, Adcock M. Hand gestures and visual annotation in live 360 panorama-based mixed reality remote collaboration. ACM International Conference Proceeding Series2018;:406–410,. [66] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration KSII Trans Internet Inform Syst 12 12 2018 6034 6056 Kim S, Billinghurst M, Lee C, Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet and Information Systems2018c;12(12):6034–6056,. [67] Congdon B, Wang T, Steed A. Merging environments for shared spaces in mixed reality. In: Proceedings of the ACM symposium on virtual reality software and technology, VRST; 2018. [68] Yamada S, Chandrasiri N. Evaluation of hand gesture annotation in remote collaboration using augmented reality. In: 25th IEEE conference on virtual reality and 3d user interfaces, vr 2018 - proceedings; 2018. p. 727–8. [69] Günther S. Avrahami D. Kratz S. Mühlhäuser M. Exploring audio, visual, and tactile cues for synchronous remote assistance ACM Int Conf Proc Ser 2018 339 344 Günther S, Avrahami D, Kratz S, Mühlhäuser M. Exploring audio, visual, and tactile cues for synchronous remote assistance. ACM International Conference Proceeding Series2018;:339–344,. [70] Piumsomboon T. Lee G. Hart J. Ens B. Lindeman R. Thomas B. Billinghurst M. Mini-me: An adaptive avatar for mixed reality remote collaboration Conf Human Factors Comput Syst 2018-April 2018 Piumsomboon T, Lee G, Hart J, Ens B, Lindeman R, Thomas B,, others,Mini-me: An adaptive avatar for mixed reality remote collaboration. Conference on Human Factors in Computing Systems2018;2018-April. [71] Ryskeldiev B. Cohen M. Herder J. Stream space: Pervasive mixed reality telepresence for remote collaboration on mobile devices J Inform Proc 26 2018 177 185 Ryskeldiev B, Cohen M, Herder J. Stream space: Pervasive mixed reality telepresence for remote collaboration on mobile devices. Journal of Information Processing2018;26:177–185,. [72] Hoppe A. Reeb R. van de Camp F. Stiefelhagen R. Interaction of distant and local users in a collaborative virtual environment Lecture Notes in Comput Sci 10909 LNCS 2018 328 337 Hoppe A, Reeb R, van de Camp F, Stiefelhagen R. Interaction of distant and local users in a collaborative virtual environment. Lecture Notes in Computer Science2018;10909 LNCS:328–337,. [73] Akkil D. Isokoski P. Comparison of gaze and mouse pointers for video-based collaborative physical task Interact Comput 30 6 2018 524 542 Akkil D, Isokoski P. Comparison of gaze and mouse pointers for video-based collaborative physical task. Interacting with Computers2018;30(6):524–542,. [74] Lee G. Teo T. Kim S. Billinghurst M. Mixed reality collaboration through sharing a live panorama SIGGRAPH Asia 2017 Mobile Graphics and Interactive Applications 2017 Lee G, Teo T, Kim S, Billinghurst M. Mixed reality collaboration through sharing a live panorama. SIGGRAPH Asia 2017 Mobile Graphics and Interactive Applications2017a;. [75] Lee G, Kim S, Lee Y, Dey A, Piumsomboon T, Norman M, Billinghurst M. Mutually shared gaze in augmented video conference. In: Adjunct proceedings of the 2017 ieee international symposium on mixed and augmented reality, ismar-adjunct 2017; 2017. p. 79–80. [76] Komiyama R. Miyaki T. Rekimoto J. Jackin space: Designing a seamless transition between first and third person view for effective telepresence collaborations ACM Int Conf Proc Ser 2017 Komiyama R, Miyaki T, Rekimoto J. Jackin space: Designing a seamless transition between first and third person view for effective telepresence collaborations. ACM International Conference Proceeding Series2017;. [77] Chenechal M, Duval T, Gouranton V, Royan J, Arnaldi B. Vishnu: Virtual immersive support for HelpiNg users an interaction paradigm for collaborative remote guiding in mixed reality. In: 2016 IEEE 3rd vr international workshop on collaborative virtual environments, 3dcve 2016; 2016. p. 9–12. [78] Gurevich P. Lanir J. Cohen B. Design and implementation of TeleAdvisor: a projection-based augmented reality system for remote collaboration Comput Support Coop Work: CSCW: An Int J 24 6 2015 527 562 Gurevich P, Lanir J, Cohen B. Design and implementation of teleadvisor: a projection-based augmented reality system for remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal2015;24(6):527–562,. [79] Tait M. Billinghurst M. The effect of view independence in a collaborative AR system Comput Support Coop Work: CSCW: An Int J 24 6 2015 563 589 Tait M, Billinghurst M. The effect of view independence in a collaborative AR system. Computer Supported Cooperative Work: CSCW: An International Journal2015;24(6):563–589,. [80] Kim S. Lee G. Ha S. Sakata N. Billinghurst M. Automatically freezing live video for annotation during remote collaboration Conf Human Factors Comput Syst 18 2015 1669 1674 Kim S, Lee G, Ha S, Sakata N, Billinghurst M. Automatically freezing live video for annotation during remote collaboration. Conference on Human Factors in Computing Systems2015;18:1669–1674,. [81] Tait M, Billinghurst M. View independence in remote collaboration using AR. In: ISMAR 2014 - IEEE international symposium on mixed and augmented reality - science and technology 2014, proceedings; 2014. p. 309–10. [82] Kim S, Lee G, Sakata N, Billinghurst M. Improving co-presence with augmented visual communication cues for sharing experience through video conference. In: ISMAR 2014 - IEEE international symposium on mixed and augmented reality - science and technology 2014, proceedings; 2014. p. 83–92. [83] Gauglitz S, Nuernberger B, Turk M, Höllerer T. In touch with the remote world: Remote collaboration with augmented reality drawings and virtual navigation. In: Proceedings of the ACM symposium on virtual reality software and technology, vrst; 2014. p. 197–205. [84] Gauglitz S, Nuernberger B, Turk M, Höllerer T. World-stabilized annotations and virtual scene navigation for remote collaboration. In: UIST 2014 - Proceedings of the 27th annual acm symposium on user interface software and technology; 2014. p. 449–60. [85] Huang W. Alem L. Tecchia F. HandsIn3D: Supporting remote guidance with immersive virtual environments Lecture Notes in Comput Sci 8117 LNCS PART 1 2013 70 77 Huang W, Alem L, Tecchia F. Handsin3d: Supporting remote guidance with immersive virtual environments. Lecture Notes in Computer Science2013;8117 LNCS(PART 1):70–77,. [86] Pece F. Steptoe W. Wanner F. Julier S. Weyrich T. Kautz J. Steed A. PanoInserts: Mobile spatial teleconferencing Conf Human Factors Comput Syst 2013 1319 1328 Pece F, Steptoe W, Wanner F, Julier S, Weyrich T, Kautz J,, others,Panoinserts: Mobile spatial teleconferencing. Conference on Human Factors in Computing Systems2013;:1319–1328,. [87] Poppe E, Brown R, Johnson D, Recker J. Preliminary evaluation of an augmented reality collaborative process modelling system. In: Proceedings of the 2012 international conference on cyberworlds; 2012. p. 77–84. [88] Gauglitz S, Lee C, Turk M, Höllerer T. Integrating the physical environment into mobile remote collaboration. In: MobileHCI’12 - proceedings of the 14th international conference on human computer interaction with mobile devices and services; 2012. p. 241–50. [89] Barakonyi I, Prendinger H, Schmalstieg D, Ishizuka M. Cascading hand and eye movement for augmented reality videoconferencing. In: IEEE symposium on 3D user interfaces, 3dui 2007; 2007. p. 71–78. [90] Bannai Y. Tamaki H. Suzuki Y. Shigeno H. Okada K. A tangible user interface for remote collaboration system using mixed reality Lecture Notes in Comput Sci 4282 2006 143 154 Bannai Y, Tamaki H, Suzuki Y, Shigeno H, Okada K. A tangible user interface for remote collaboration system using mixed reality. Lecture Notes in Computer Science2006;4282:143–154,. [91] Regenbrecht H. Lum T. Kohler P. Ott C. Wagner M. Wilke W. Mueller E. Using augmented virtuality for remote collaboration Presence: Teleoperat Virtual Environ 13 3 2004 338 354 Regenbrecht H, Lum T, Kohler P, Ott C, Wagner M, Wilke W,, others,Using augmented virtuality for remote collaboration. Presence: Teleoperators and Virtual Environments2004;13(3):338–354,. [92] Araujo R.M. Santoro F.M. Borges M.R.S. A conceptual framework for designing and conducting groupware evaluations Int J Comput Appl Technol 19 3 2004 139 150 Araujo RM, Santoro FM, Borges MRS. A conceptual framework for designing and conducting groupware evaluations. International Journal of Computer Applications in Technology2004;19(3):139–150,. [93] Pereira C, Teixeira A, e Silva MO. Live evaluation within ambient assisted living scenarios. In: Proceedings of the 7th international conference on pervasive technologies related to assistive environments, PETRA 2014; 2014. [94] Ratcliffe J. Soave F. Bryan-Kinns N. Tokarchuk L. Farkhatdinov I. Extended reality (XR) remote research: a survey of drawbacks and opportunities CHI Conference on Human Factors in Computing Systems 2021 1 13 Ratcliffe J, Soave F, Bryan-Kinns N, Tokarchuk L, Farkhatdinov I. Extended Reality (XR) Remote Research: a Survey of Drawbacks and Opportunities. In: CHI Conference on Human Factors in Computing Systems.2021, p. 1–13,. [95] Gaines B.R. Modeling and forecasting the information sciences Inform Sci 57–58 1991 3 22 Information Sciences-Past, Present, and Future Gaines BR. Modeling and forecasting the information sciences. Information Sciences1991;57-58:3 – 22,. Information Sciences-Past, Present, and Future. [96] Lalanne D. Nigay L. Palanque p. Robinson P. Vanderdonckt J. Ladry J.-F.c. Fusion engines for multimodal input: A survey Proceedings of the 2009 International Conference on Multimodal Interfaces ICMI-MLMI ’09 2009 Association for Computing Machinery New York, NY, USA 153 160 Lalanne D, Nigay L, Palanque p, Robinson P, Vanderdonckt J, Ladry JF. Fusion engines for multimodal input: A survey. In: Proceedings of the 2009 International Conference on Multimodal Interfaces. ICMI-MLMI ’09; New York, NY, USA: Association for Computing Machinery;2009, p. 153–160,. [97] Teixeira A. A critical analysis of speech-based interaction in healthcare robots: Making a case for the increased use of speech in medical and assistive robots Speech and Automata in Healthcare : Voice-Controlled Medical and Surgical Robots - Chapter 1 2014 29 Teixeira A. A critical analysis of speech-based interaction in healthcare robots: Making a case for the increased use of speech in medical and assistive robots. In: Speech and Automata in Healthcare : Voice-Controlled Medical and Surgical Robots - Chapter 1.2014, p. 29,. [98] Billinghurst M. Kato H. Collaborative augmented reality Commun ACM 45 2003 Billinghurst M, Kato H. Collaborative augmented reality. Communications of the ACM2003;45. [99] Wang X, Dunston PS. Groupware concepts for augmented reality mediated human-to-human collaboration. In: Joint international conference on computing and decision making in civil and building engineering; 2006. p. 1836–42. [100] Brockmann T, Krueger N, Stieglitz S, Bohlsen I. A framework for collaborative augmented reality applications. In: Proceedings of the nineteenth americas conference on information systems; 2013, p. 1–10. [101] Sereno M. Wang X. Besancon L. Mcguffin M.J. Isenberg T. Collaborative work in augmented reality: A survey IEEE Trans Vis Comput Graphics 2020 1 20 Sereno M, Wang X, Besancon L, Mcguffin MJ, Isenberg T. Collaborative work in augmented reality: A survey. IEEE Transactions on Visualization and Computer Graphics2020;:1–20,. [102] Collazos C.A. Gutiérrez F.L. Gallardo J. Ortega M. Fardoun H.M. Molina A.I. Descriptive theory of awareness for groupware development J Ambient Intell Humaniz Comput 10 12 2019 4789 4818 Collazos CA, Gutiérrez FL, Gallardo J, Ortega M, Fardoun HM, Molina AI. Descriptive theory of awareness for groupware development. Journal of Ambient Intelligence and Humanized Computing2019;10(12):4789–4818,. [103] Talkad Sukumar P, Avellino I, Remy C, DeVito MA, Dillahunt TR, McGrenere J, Wilson ML. Transparency in qualitative research: Increasing fairness in the chi review process. In: Extended abstracts of the 2020 chi conference on human factors in computing systems; 2020. p. 1–6. [104] Meyer M. Dykes J. Criteria for rigor in visualization design study IEEE Trans Vis Comput Graphics 26 1 2019 87 97 Meyer M, Dykes J. Criteria for rigor in visualization design study. IEEE transactions on visualization and computer graphics2019;26(1):87–97,. [105] Augstein M. Neumayr T. A human-centered taxonomy of interaction modalities and devices Interact Comput 31 2019 27 58 Augstein M, Neumayr T. A human-centered taxonomy of interaction modalities and devices. Interacting with Computers2019;31:27–58,. [106] Nickerson R.C. Varshney U. Muntermann J. A method for taxonomy development and its application in information systems Eur J Inform Syst 22 2013 336 359 Nickerson RC, Varshney U, Muntermann J. A method for taxonomy development and its application in information systems. European Journal of Information Systems2013;22:336–359,. [107] Teruel M.A. Navarro E. López-Jaquero V. Montero F. González P. A comprehensive framework for modeling requirements of CSCW systems J Softw: Evolu Process 29 5 2017 e1858 Teruel MA, Navarro E, López-Jaquero V, Montero F, González P. A comprehensive framework for modeling requirements of cscw systems. Journal of Software: Evolution and Process2017;29(5):e1858,. [108] Zollmann S. Grasset R. Langlotz T. Lo W.H. Mori S. Regenbrecht H. Visualization techniques in augmented reality: A taxonomy, methods and patterns IEEE Trans Vis Comput Graphics 2020 1 20 Zollmann S, Grasset R, Langlotz T, Lo WH, Mori S, Regenbrecht H. Visualization techniques in augmented reality: A taxonomy, methods and patterns. IEEE Transactions on Visualization and Computer Graphics2020;:1–20,. [109] Chandrasekaran B. Josephson J.R. Benjamins V.R. What are ontologies, and why do we need them? IEEE Intell Syst Appl 14 1 1999 20 26 Chandrasekaran B, Josephson JR, Benjamins VR. What are ontologies, and why do we need them? IEEE Intelligent Systems and Their Applications1999;14(1):20–26,. [110] Noy N.F. McGuinness D.L. Ontology development 101: A guide to creating your first ontology Stanford Knowl Syst Lab Tech Rep 15 2 2001 1 25 Noy NF, McGuinness DL. Ontology Development 101: A Guide to Creating Your First Ontology. Stanford Knowledge Systems Laboratory Technical Report2001;15(2):1–25,. [111] Herskovic V. Pino J.A. Ochoa S.F. Antunes P. Evaluation methods for groupware systems Haake J.M. Ochoa S.F. Cechich A. Groupware: Design, Implementation, and Use 2007 Springer, Berlin, Heidelberg 328 336 Herskovic V, Pino JA, Ochoa SF, Antunes P. Evaluation methods for groupware systems. In: Haake JM, Ochoa SF, Cechich A, editors. Groupware: Design, Implementation, and Use. Springer, Berlin, Heidelberg;2007, p. 328–336,. [112] de Araujo RM, Santoro FM, Borges MRS. The CSCW lab ontology for groupware evaluation. In: 8th international conference on computer supported cooperative work in design, Vol. 2; 200. p. 148–53. [113] Pereira C. Almeida N. Martins A.I. Silva S. Rosa A.F. Oliveira e Silva M. Teixeira A. Evaluation of complex distributed multimodal applications: Evaluating a TeleRehabilitation system when it really matters Lecture Notes in Comput Sci (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 2015 146 157 Pereira C, Almeida N, Martins AI, Silva S, Rosa AF, Oliveira e Silva M,, others,Evaluation of complex distributed multimodal applications: Evaluating a TeleRehabilitation system when it really matters. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)2015;:146–157,. [114] Pereira C. Teixeira A. Oliveira e Silva M. Dynamic evaluation for reactive scenarios (Ph.D. thesis) 2016 University of Aveiro (MAPi) 1 196 Pereira C, Teixeira A, Oliveira e Silva M. Dynamic Evaluation for Reactive Scenarios, Ph.D. Dissertation. University of Aveiro (MAPi). Ph.D. thesis;2016. "
    },
    {
        "doc_title": "Does Remote Expert Representation really matters: A comparison of Video and AR-based Guidance",
        "doc_scopus_id": "85129657701",
        "doc_doi": "10.1109/VRW55335.2022.00208",
        "doc_eid": "2-s2.0-85129657701",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Human computer interaction",
            "Human computer interaction design and evalution method",
            "Human-centered computing",
            "Human-computer-interaction designs",
            "Interaction paradigm",
            "Mixed/augmented reality",
            "Remote experts",
            "User study"
        ],
        "doc_abstract": "© 2022 IEEE.This work describes a user study aimed at understanding how the remote expert representation affects the sense of social presence in scenarios of remote guidance. We compared a traditional video chat solution with an Augmented Reality (AR) annotation tool. These were selected due to ongoing research with partners from the industry sector, following the insights of a participatory design process. A well defined-problem was used, i.e., a synchronous maintenance task with 4 completion stages that required a remote expert using a computer to guide 26 on-site participants wielding a handheld device. The results of the study are described and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Whac-A-Mole: Exploring Virtual Reality (VR) for Upper-Limb Post-Stroke Physical Rehabilitation based on Participatory Design and Serious Games",
        "doc_scopus_id": "85129620414",
        "doc_doi": "10.1109/VRW55335.2022.00209",
        "doc_eid": "2-s2.0-85129620414",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Human computer interaction",
            "Human computer interaction design and evalution method",
            "Human-centered computing",
            "Human-computer-interaction designs",
            "Interaction paradigm",
            "Mixed/augmented reality",
            "Physical rehabilitation",
            "User study"
        ],
        "doc_abstract": "© 2022 IEEE.This paper describes a Human-Centered Design process aimed at understanding how Virtual Reality (VR) can assist in post-stroke physical rehabilitation. Based on insights from stroke survivors and healthcare professionals, a serious game prototype is proposed. We focused on upper-limb rehabilitation, which inspired the game narrative and the movements users must perform. The game supports two modes: 1-normal version*users can use any arm to pick a virtual hammer and hit objects; 2-mirror version*converts a traditional approach to VR, providing the illusion that the arm affected by the stroke is moving. These were evaluated through a user study.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using 3D Reconstruction to create Pervasive Augmented Reality Experiences: A comparison",
        "doc_scopus_id": "85129618187",
        "doc_doi": "10.1109/VRW55335.2022.00207",
        "doc_eid": "2-s2.0-85129618187",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Human computer interaction",
            "Human computer interaction design and evalution method",
            "Human-centered computing",
            "Human-computer-interaction designs",
            "Interaction paradigm",
            "Mixed/augmented reality",
            "Physical environments",
            "User study"
        ],
        "doc_abstract": "© 2022 IEEE.This paper presents a prototype for configuration and visualization of Pervasive Augmented Reality (AR) experiences using two versions: desktop and mobile. It makes use of 3D scans from physical environments to provide a reconstructed digital representation of such spaces to the desktop version and enable positional tracking for the mobile. While the desktop presents a non-immersive setting, the mobile provides continuous AR in the physical environment. Both versions can be used to place virtual content and ultimately configure an AR experience. The authoring capabilities of the proposed solution were compared by conducting a user study focused on evaluating their usability.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real-time visualization reconstruction in a real-world environment using Augmented Reality.",
        "doc_scopus_id": "85118462941",
        "doc_doi": "10.1109/IV53921.2021.00023",
        "doc_eid": "2-s2.0-85118462941",
        "doc_date": "2021-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Chart recognition",
            "Chart reconstruction",
            "Data support",
            "Data-source",
            "Digital datas",
            "Interaction",
            "Printed media",
            "Real time visualization",
            "Real world environments",
            "Real-world"
        ],
        "doc_abstract": "© 2021 IEEE.Even with the growth of digital data sources and support for creating visualizations, much information disseminated in chart format is still static, whether printed or digital. Some common problems of visualization components can be easily corrected on the chart design using a charting tool, but in the real world, mainly in the printed media, it is not a simple task. We investigate a method to reconstruct visualizations in real-time and in a real-world environment using Augmented Reality. We propose a prototype that interacts with extracted data from a bitmap chart in the real world, and we evaluated it with usability experts. The results show a high degree of satisfaction on many factors, mainly filter usage and valuable feedback about interaction.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visually exploring a Collaborative Augmented Reality Taxonomy",
        "doc_scopus_id": "85118430803",
        "doc_doi": "10.1109/IV53921.2021.00024",
        "doc_eid": "2-s2.0-85118430803",
        "doc_date": "2021-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Co-located collaboration",
            "Collaboration taxonomy",
            "Collaborative augmented realities",
            "Collaborative Work",
            "Common ground",
            "Harmonisation",
            "Remote collaboration",
            "Systematisation",
            "User study",
            "Visualization tools"
        ],
        "doc_abstract": "© 2021 IEEE.Augmented Reality (AR) has been explored with the objective to assist in scenarios of co-located or remote collaboration. To help understand how well collaborative work can be addressed with AR, it is important to foster harmonization of perspectives and create a common ground for systematization and discussion. In this vein, understand relationships among existing dimensions of collaboration, as well as identify research opportunities, is of paramount importance and thus tools that allow visually exploring information associated with Collaborative AR may be most valuable. In this paper, we present a first effort towards the creation of such an interactive visualization tool for exploration and analysis of collaborative AR research. It allows visualize data of selected papers organized according to a human-centered taxonomy on collaborative AR. In order to get insights into whether the structure was understood and if the representation was clear and efficient to use, we evaluated the proposed tool through a user study with 40 participants. Results suggest the tool has potential towards the creation of a shared understanding and identification of existing patterns, trends and opportunities within the field of collaborative AR.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using augmented reality for industrial quality assurance: a shop floor user study",
        "doc_scopus_id": "85105164742",
        "doc_doi": "10.1007/s00170-021-07049-8",
        "doc_eid": "2-s2.0-85105164742",
        "doc_date": "2021-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            }
        ],
        "doc_keywords": [
            "Industrial production",
            "Industrial scenarios",
            "Laboratory conditions",
            "Quality control tests",
            "Real-time validation",
            "Risk of human error",
            "Sustainable solution",
            "Video instructions"
        ],
        "doc_abstract": "© 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.Quality control procedures are essential in many industrial production pipelines. These repetitive and precise tasks are frequently complex, including several steps that must be performed correctly by different operators. To facilitate these, quality control tests are often documented with static media like video recordings, photos, or diagrams. However, the need for the operator to divide attention between the visual instructions and the task, and the lack of feedback lead to slow processes, with potential for improvement. By using augmented reality (AR), operators can focus on the task at hand while receiving visual feedback where it is needed. Nevertheless, existing prototypes are still at early stages, being tested only in laboratory conditions, far from mimicking real scenarios. The major contributions of this work are twofold: first, we present an AR-based quality control system capable of generating virtual content to guide operators by overlaying information in a video stream while performing real-time validation. The system evaluates the current status of the procedure to ensure the automatic progression to the next phase. Second, an evaluation was conducted in an industrial shop floor during 1 week, with seven operators to verify if the system was robust and understand possible efficiency gains when compared to the alternative, i.e., video instructions. Results showed that AR had a significant impact in procedures’ execution time (reduction of 36%), while reducing the risk of human errors, which means AR technologies may represent a profitable and sustainable solution when applied to real-world industrial scenarios, in the long run.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Remote asynchronous collaboration in maintenance scenarios using augmented reality and annotations",
        "doc_scopus_id": "85105968777",
        "doc_doi": "10.1109/VRW52623.2021.00166",
        "doc_eid": "2-s2.0-85105968777",
        "doc_date": "2021-03-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Asynchronous collaboration",
            "Collaborative approach",
            "Maintenance scenario",
            "Maintenance tasks",
            "Remote experts",
            "Shared understanding",
            "Spatial informations",
            "User study"
        ],
        "doc_abstract": "© 2021 IEEE.This paper presents an Augmented Reality (AR) remote collaborative approach making use of different stabilized annotation features, part of an ongoing research with partners from the industry. It enables a remote expert to assist an on-site technician during asynchronous maintenance tasks. To foster the creation of a shared understanding, the on-site technician uses mobile AR, allowing the identification of issues, while the remote expert uses a computer to share annotations and provide spatial information about objects, events and areas of interest. The results of a pilot user study to evaluate asynchronous collaborative aspects while using the approach are also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Toolkit to Evaluate and Characterize the Collaborative Process in Scenarios of Remote Collaboration Supported by AR",
        "doc_scopus_id": "85126382153",
        "doc_doi": "10.1109/ISMAR-Adjunct54149.2021.00074",
        "doc_eid": "2-s2.0-85126382153",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Case-studies",
            "Characterization collaborative process",
            "Collaborative process",
            "Data collection",
            "Data filtering",
            "Distributed evaluation",
            "Evaluation toolkit",
            "Remote collaboration",
            "Remote maintenance",
            "Visual editors"
        ],
        "doc_abstract": "© 2021 IEEE.Remote collaboration using Augmented Reality (AR) has enormous potential to support collaborators that need to achieve a common goal. However, there is a lack of tools for evaluating these multifaceted contexts, involving many aspects that may influence the way collaboration occurs. Therefore, it is essential to develop solutions to monitor AR-supported collaboration in a more structured manner, allowing adequate portrayal and report of such efforts. As a contribute, we describe CAPTURE, a toolkit to instrument AR-based tools via visual editors, enabling rapid data collection and filtering during distributed evaluations. We illustrate the use of the toolkit through a case study on remote maintenance and report the results obtained, which can elicit a more complete characterization of the collaborative process moving forward.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Human Factors in Industry 4.0 and Lean Information Management: Remodeling the Instructions in a Shop Floor",
        "doc_scopus_id": "85112159986",
        "doc_doi": "10.1007/978-3-030-77750-0_16",
        "doc_eid": "2-s2.0-85112159986",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Daily tasks",
            "Digital tools",
            "Digital transformation",
            "Human capitals",
            "Information flows",
            "Information sharing",
            "Organizational knowledge",
            "Shop floor"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.Industry 4.0 or also called the digital paradigm brings with it an environment where information sharing, and agile flows can improve the results of organizations but also create great challenges for operators. The digital paradigm promotes the globalization of human capital, with an impact on worker turnover, as well as on the potential increase in the loss of organizational knowledge, as workers leave companies. Thus, this work, through the combination of the concepts of digital transformation and information management, joint with the techniques of BPM and Lean, clarifies a methodology (using BPMN) for the creation of a repository of organizational knowledge (enhancing instruction modelling work routines), while promoting the introduction of digital tools in daily tasks, placing the operator in the foreground. This digital introduction promotes the elimination of waste connected with Lean Information Management, creating more fluid information flows in the company. The human factor is then valued and new points of connection between the human workforce and digital tools are established, while organizational knowledge is updated, created, and retained.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Characterization of Portuguese haemophilia patients based on the national registry data",
        "doc_scopus_id": "85105678557",
        "doc_doi": "10.1016/j.procs.2021.01.273",
        "doc_eid": "2-s2.0-85105678557",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder",
            "Characterization of PWH",
            "Clinical conditions",
            "Elsevier",
            "Hemophilia",
            "Hemophilia A",
            "National registry data",
            "Patient record",
            "Portugal",
            "Technological solution"
        ],
        "doc_abstract": "© 2021 The Authors. Published by Elsevier B.V.Haemophilia is a chronic and rare congenital bleeding disorder requiring treatment for life. An updated registry of data on this disease is of paramount importance for documenting prevalence, planning care, and evaluating effectiveness of resources in any country. The study presented in this paper is aimed at characterizing Portuguese patients' clinical condition and demography, as well as the replacement therapy used in their treatment, based on hemo@record data, a technological solution that supports the National haemophilia registry. The analysis of 110 patients records confirmed the high prevalence of haemophilia A (75.5%) compared with haemophilia B (11.8%) and a large incidence in the severe levels (or the existence of people with mild severity without diagnosis and, consequently, without proper treatment) promoting insight about the portrayal of haemophilia in Portugal. This study should be extended as the registry evolves fostering an ever more efficient management of resources and ultimately a better quality of care for people with haemophilia.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2021-02-22 2021-02-22 2021-02-22 2021-02-22 2021-03-08T15:55:40 S1877-0509(21)00322-7 S1877050921003227 10.1016/j.procs.2021.01.273 S300 S300.2 HEAD-AND-TAIL 2021-04-28T06:56:37.333898Z 0 0 20210101 20211231 2021 2021-02-22T17:04:27.749831Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 true 181 181 C Volume 181 124 995 1001 995 1001 2021 2021 2021-01-01 2021-12-31 2021 CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020 Maria Manuela Cruz-Cunha Ricardo Martinho Rui Rijo Nuno Mateus-Coelho Dulce Domingos Emanuel Peres article fla © 2021 The Author(s). Published by Elsevier B.V. CHARACTERIZATIONPORTUGUESEHAEMOPHILIAPATIENTSBASEDNATIONALREGISTRYDATA TEIXEIRA L SRIVASTAVA 2013 e1 A EVATT 2005 B COLVIN 2008 361 374 B TEITEL 2004 118 133 J HARA 2017 1 8 J REITTER 2009 13 15 S IORIO 2008 444 453 A AZNAR 2009 1327 1330 J ZDZIARSKA 2011 e189 J VONDERWEID 2012 S20 S24 N HAY 2004 21 25 C TEIXEIRA 2015 1248 1255 L TEIXEIRA 2015 65 80 L TEIXEIRA 2012 56 62 L TEIXEIRA 2017 131 137 L WINDYGA 2006 52 57 J ALI 2012 851 854 T TEIXEIRAX2021X995 TEIXEIRAX2021X995X1001 TEIXEIRAX2021X995XL TEIXEIRAX2021X995X1001XL Full 2021-01-28T00:18:19Z ElsevierWaived OA-Window This is an open access article under the CC BY-NC-ND license. © 2021 The Author(s). Published by Elsevier B.V. 2021-01-27T11:07:30.475Z FCT Fundação para a Ciência e a Tecnologia item S1877-0509(21)00322-7 S1877050921003227 10.1016/j.procs.2021.01.273 280203 2021-04-28T06:56:37.333898Z 2021-01-01 2021-12-31 true 487363 MAIN 7 46555 849 656 IMAGE-WEB-PDF 1 am 189222 ScienceDirect Available online at www.sciencedirect.com Procedia Computer Science 181 (2021) 995â€“1001 1877-0509 Â© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2020 10.1016/j.procs.2021.01.273 10.1016/j.procs.2021.01.273 1877-0509 Â© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2020 Available online at www.sciencedirect.com ScienceDirect Procedia Computer Science 00 (2019) 000â€“000 www.elsevier.com/locate/procedia 1877-0509 Â© 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN â€“ International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2020 Characterization of Portuguese Haemophilia patients based on the National Registry Data Leonor Teixeiraa,c,*, Carlos Ferreiraa,c, Beatriz Sousa Santosb,c a Department of Economics, Management, Industrial Engineering, and Tourism (DEGEIT),University of Aveiro, 3810-193 Aveiro, Portugal b Department of Electronics, Telecommunications and Informatics (DETI), University of Aveiro, 3810-193 Aveiro, Portugal cI Institute of Electronics and Informatics Engineering of Aveiro (IEETA), University of Aveiro, 3810-193 Aveiro, Portugal Abstract Haemophilia is a chronic and rare congenital bleeding disorder requiring treatment for life. An updated registry of data on this disease is of paramount importance for documenting prevalence, planning care, and evaluating effectiveness of resources in any country. The study presented in this paper is aimed at characterizing Portuguese patientsâ€™ clinical condition and demography, as well as the replacement therapy used in their treatment, based on hemo@record data, a technological solution that supports the National haemophilia registry. The analysis of 110 patients records confirmed the high prevalence of haemophilia A (75.5%) compared with haemophilia B (11.8%) and a large incidence in the severe levels (or the existence of people with mild severity without diagnosis and, consequently, without proper treatment) promoting insight about the portrayal of haemophilia in Portugal. This study should be extended as the registry evolves fostering an ever more efficient management of resources and ultimately a better quality of care for people with haemophilia. Â© 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN â€“ International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies *Corresponding author. Tel.: +351-234-370361; fax: +351-234-370215. E-mail address: lteixeira@ua.pt 996 Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 2 Author name / Procedia Computer Science 00 (2019) 000â€“000 Keywords: Haemophilia; National Registry Data; Characterization of PWH; Portugal 1. Introduction Haemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor VIII (FVIII), in haemophilia A (HA) or factor IX (FIX), in haemophilia B (HB) [1]. It has a low incidence, affecting the population in a ratio of one case per 10000 people, and thus it is classified as a rare disease [2]. This disease impairs the body's ability to make blood clots and may have very negative consequences for the patient. One of its main symptoms is joint haemorrhage, which if repeated can cause destruction of the cartilage, affecting the entire bone and soft tissue of the joint structure. Thus, people not properly treated can become disabled, with reduced quality of life. The treatment of haemophilia is complex involving highly specialized skills of multidisciplinary teams in specialized centres, known as Comprehensive Haemophilia Treatment Centres (HTCs) [3]. Usually, comprehensive care includes several health team members (e.g. haematologist, physical therapist, nurse, psychosocial expert and social worker) working in collaboration to minimize the negative effects of haemophilia and, consequently, maximizing patientsâ€™ quality of life [1]. There are different types of congenital bleeding disorder (HA, HB, von Willebrand, etc.); however, HA and HB are the most prevalent and are usually classified according their severity, namely: (i) severe - clotting factor level lower than 1% of normal; (ii) moderate - clotting factor level between 1% and 5%; and (iii) mild - clotting factor level between 6% and 40%. Clotting factor concentrate (CFC) is the drug administrated to HA and HB patients to compensate the missing clotting factor in their body. These CFC, either plasmatic (plasma-derived), or recombinant [1], contribute to life improvement of people with haemophilia (PWH), allowing the treatment at home with a high level of independence. Most people with severe haemophilia having conditions for self-infusion are, in fact, on home-therapy [4]. This provides benefits, improving not only theirs but also their familiesâ€™ life quality decreasing the economic impact of this disease likewise. Haemophilia treatment can be done either at home or in the hospital, and both types may follow different protocols: (i) on-demand treatment - at the time of clinically evident bleeding; and (ii) prophylactic treatment - to prevent blending episodes. Considering the prophylactic treatment, this can be classified as primary, secondary or tertiary prophylaxis [1]. Although prophylactic treatment can lead to better clinical outcomes, improving the quality of life, some literature reports that this type of treatment is more expensive than the â€œon-demandâ€� therapy [5]. It is important to note that the drugs used in these treatments â€“ CFC - are very expensive and fully financed by governmental entities in developed countries. According to a study involving five European countries, the total annual cost of treating severe haemophilia for 2014 was estimated at EUR 1.4 billion, or just under EUR 200,000 per patient [5]. The same study indicated that CFC represents up to 99% of those costs. Given the characteristics of disease and the impact on PHW, their families, and society in general, institutions as the World Federation of Haemophilia (WFH) and the European Haemophilia Consortium (EHC), recommend establishing National Patient Registries (NPR). This type of tool is defined as a nationwide repository system of demographic, social and clinical data for people suffering from a particular disease [2] [3]. According to Colvin et al. [3] the NPR promotes a more assertive analysis of standards of care, and can be used as a tool for characterizing a particular population, to know its conditions (identifying prevalence and incidence of the disease) and to audit clinical services, supporting the development of better quality of care and resource planning. In fact, the literature identifies a set of advantages associated with the existence of this type of tool. Some countries have already implemented their NPR [6]â€“[11], allowing comparative studies. This paper presents a characterization of Portuguese PWH based on the National Patient Registry, named Hemo@record. Hemo@record is a Web-based solution to manage the complex data associated with PWH, including clinical, social and health status monitoring data [12] [13]. Moreover, it aims to determine the prevalence and incidence of this disease comparing with the results reported in an earlier study conducted in Portugal [14], as well as the ones reported by other countries. The remaining of this paper is organized in three sections: section 2 presents the data and methods used in the study, section 3 presents its main results and finally section 4 draws conclusions and presents ideas for future work. Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 997 Author name / Procedia Computer Science 00 (2019) 000â€“000 3 2. Materials and Methods This is a first study using data from the Portuguese NPR to characterize PWH. The data collected includes, among others, socio-demographic information (e.g. age, sex, academic achievements, occupational activity), diagnosis, severity of haemophilia A and B, type of von Willebrand Disease (vWD), presence of inhibitors, virology status, treatment information, consumed CFC. More detail about the types of variables collected on the Portuguese NPR can be founded in [15]. As reported by the WFH, haemophilia affects individuals at a ratio of approximately 1 case per 10 000 people [1]. Considering that Portugal has about 10 million inhabitants, the expected number of PWH in Portugal is approximately 1000 cases. However, registries about identified cases by the Portuguese Association of Haemophilia (PAH) report about 700 cases. Thus, data considered in this study (110 patients), which were introduced by different HTC in the NPR system, represents 16% of PWH in Portugal. Hemo@record data was exported to an Excel sheet and the data analysis was based on simple descriptive statistics. Main results related to: (i) socio-demographic characteristics; (ii) clinical characteristics, and (iii) replacement therapy are presented in the next section. 3. Results and Discussion 3.1. Socio-demographic characteristics of the patients The Portuguese Haemophilia Register has 95.5% of male and 4.5% of female PWH. The age range is 5â€“82 years old; the mean value is 37 years, the median is 35 years and the first and third quartiles are 25 and 50 years old, respectively. As can be seen in table 1, thirteen patients (11.8%) are children (<15 years old); seven patients (6.4%) are young adults (from 15 to 21 years old), and the remaining patients are adults ([22 â€“ 40] 41.8%, [41 - 65] 35.5%) and seniors (> 66 years old) (4.5%). These results are similar to the obtained in a previous survey conducted in Portugal using a questionnaire [14]. Table 1: Distribution of patients by age range Age range Number Per cent <15 13 11,8% 15 - 21 7 6,4% 22 - 40 46 41,8% 41 - 65 39 35,5% >66 5 4,5% Total 110 100% Concerning academic achievements, the majority of PWH have secondary education (30.9%), 16.4% have higher education, 8.2% elementary education, and 3.6% of patients do not have elementary education. The academic achievement was not applied in 15.5% and with no information in 25.5% of the patients (see table 2). Table 2: Distribution of patients by academic achievement Academic achievement Number Per cent Without elementary education 4 3.6 % Elementary education 9 8.2 % Secondary Education 34 30.9 % Higher Education 18 16.4 % n.a 17 15.5 % unknown 28 25.5 % Total 110 100% 998 Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“10014 Author name / Procedia Computer Science 00 (2019) 000â€“000 In terms of occupational activity, 10% of PWH are students, 46.6 % employed, 11.8% retired, and 2.7% are unemployed. This condition was not applied in 6.4% and with no information in 22.75% of the patients (see table 3). Table 3: Distribution of patients by occupational activity Occupational activity Number Per cent Student 11 10.0 % Active 51 46.4 % Retired 13 11.8 % Unemployed 3 2.7 % n.a 7 6.4 % unknown 25 22.7 % Total 110 100% 3.2. Clinical characteristics of the patients Regarding the type of bleeding disorder, 75.5% suffer with haemophilia A (HA), 11.8% with haemophilia B (HB), 8.2% with vWD and 4.5% with other types of coagulopathies (see Table 4). These results were found to be similar to the previous survey conducted in Portugal through a questionnaire [14]. Table 4: Distribution of patients by inherited coagulopathies Type of coagulopathy Number Per cent Haemophilia A 83 75,5 % Haemophilia B 13 11,8 % von Willebrand 9 8,2 % Other coagulopathies 5 4.5 % Taking into account these data, the observed ratio of HA to HB is 6.2:1, very similar to the data reported in the previous study performed in Portugal (6.6:1) [14], as well as in the studies conducted in other countries, such as Spain (6.5:1) [8] and Poland (6.2:1) [16]. Considering HA severity, 66.3% of the patients present a severe level, 7.2 % a moderate one and 24.1% a mild level. 2.4 % of the patients have no reference related with severity of the disease. In HB, usually not as frequent as HA, 76.9 % present a severe level, 15.4 % a moderate one, and for 7.7% there is no information about severity. Table 5 presents details of the disease severity in both cases, showing a larger incidence of the severe levels. An additional analysis of the data base reveals the existence of patients with mild severity without diagnosis and, consequently, without proper treatment. Table 5 â€“ distribution of patients by severity of ha and HB. Type Severe (<1%) n (%) Moderate (1-5%) n (%) Mild (5-40%) n (%) HA (n=83) 55 (66.3%) 6 (7.2%) 20 (24.1%) HB (n=13) 10 (76.9%) 2 (15.4%) 0 (0%) Inhibitors were detected in 13 (11.8%) patients. The frequency of inhibitor development was 12 in HA patients (11 severe and one moderate) and one in HB patients (severe). The prevalence of inhibitors in HA and HB is in line with the results regarding another country (ex: [17]). Concerning the virology status information, twenty per cent (20%) of the PWH were infected with HCV (Hepatitis C Virus), and about eleven per cent (10.9%) with HIV (Human Immunodeficiency Virus). Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 999 Author name / Procedia Computer Science 00 (2019) 000â€“000 5 3.3. Replacement therapy of the patients Concerning the replacement of the clotting factor in deficit, the registry data indicate that 88.2% (97) of PWH do replacement treatments; of those patients 60,8% (59) are under home treatment (see table 6). Table 6: Distribution of patients with replacement treatment Treatment HA HB dVW Other C. Treatment with clotting factor (n=97) 73 ( 73.2%) 12 (12.4 %) 9 (9.3 %) 5 (5.2%) Home-treatment (n=59) 48 (81.4 %) 9 (15.3%) 2 (3.4%) -- From 97 patients that do replacement treatment, for 94 patients (69 with HA, 12 with HB, 8 with dVW, and 5 with other type of coagulopathies) the mode of substitution or the type of therapy was identified. As can be seen in table 7, among those 94 patients, 51 patients are receiving on-demand treatment, and 43 are receiving prophylactic treatment. Table 7 â€“ Distribution of patients by type of therapy Type of therapy HA HB dVW Other C. On-demand (n=51) 34 ( 66.7%) 8 (15.7 %) 6 (11.8%) 3 (5.9%) Prophylactic (n=43) 35 (81.4%) 4 (9.3%) 2 (4.7%) 2 (4.7%) Table 8 reports the number of patients on-demand and prophylactic treatments considering the type of coagulopathy (HA and HB) and disease severity. The results reveal that, under prophylactic therapy, there are more cases of severe HA and HB, than moderate or mild. Patients with mild severity, usually are under on-demand therapy. Table 8 â€“ Details of distribution of patients by type of therapy. Severe Moderate Mild On-demand (n=42) HA (n=34) 16 (47.1%) 2(5.9 %) 16 (47.1%) HB (n=8) 7 (87.5%) 1 (12.5%) -- Prophylactic (n=39) HA (n=35) 35 (100%) -- -- HB (n=4) 3 (75%) 1 (25%) -- From 97 patients who make treatment based on the replacement of clotting factor, recombinant coagulation factor was administered to 39 patients, plasmatic factor to 38, and 12 patients received both types of factor products (see table 6). For 8 patients no data are available. Table 9 - Distribution of patients by type of clotting factor used. Type of CFC HA HB dVW Other C. Recombinant (n=39) 33 (37.1%) 3 (3.4 %) -- 3 (3.4%) Plasmatic (n=38) 21 (23.6%) 7 (7.9%) 8 (9%) 2 (2.2%) Both types (n=12) 10 (10.2%) 2 (2.2%) -- -- 1000 Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 6 Author name / Procedia Computer Science 00 (2019) 000â€“000 4. Concluding remarks This study analysed 110 records of PWH registered on the Portuguese NPR, 95.5% males and 4.5% females; the results confirmed the high prevalence of haemophilia A (75.5%) compared with haemophilia B (11.8%), as well as a large incidence of severe levels. The results also revealed the presence of inhibitors in 11.8% of patients with a higher incidence in severe HA. These results are comparable to the findings of other similar studies conducted in other countries, as well as a previous study conducted in Portugal using a questionnaire to collect data. Concerning the treatment based on replacement of the CFC, the results revealed that 88.2% of PWH make this kind of treatment (54% following an on-demand protocol, and the remaining 46% on prophylactic treatment), and of those, 60.8% are under home treatment. The recombinant CFC is used in 43% of PWH, plasmatic factor in 44% and both types of CFC in 13% of the cases. This study also confirmed the effortlessness in characterizing the incidence and prevalence of a disease using data collected automatically from a national registry system. As future work, we expect to increase the number of PWH in the Portuguese NPR and, consequently, to carry out a more comprehensive characterization study including a larger and more representative sample. In fact, the results obtained from any NPR in this context promote insight about the portrayal of haemophilia in a country, which allows a more efficient management of resources and a better quality of care. Acknowledgement This work was supported by Portuguese funds through the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) and Foundation for Science and Technology, in the context of the project UIDB/00127/2020. References [1] A. Srivastava, A. K. Brewer, E. P. Mauser-Bunschoten, N. S. Key, S. Kitchen, A. Llinas, C. Ludlam, J. N. Mahlangu, K. Mulder, M. C. Poon, and A. Street, â€œGuidelines for the management of hemophiliaâ€� Haemophilia, vol. 19, no. 1, pp. e1-47, 2013. [2] B. Evatt, â€œWorld Federation of Hemophilia Guide to Developing a National Patient Registryâ€� MontrÃ©al - QuÃ©bec, 2005. [3] B. T. Colvin, J. Astermark, K. Fischer, A. Gringeri, R. Lassila, W. Schramm, A. Thomas, and J. Ingerslev, â€œEuropean principles of haemophilia careâ€� Haemophilia, vol. 14, no. 2, pp. 361â€“74, 2008. [4] J. M. Teitel, D. Barnard, S. Israels, D. Lillicrap, M. Poon, and J. Sek, â€œHome management of haemophiliaâ€� Haemophilia, vol. 10, no. 2, pp. 118â€“133, 2004. [5] J. O. Hara, D. Hughes, C. Camp, T. Burke, L. Carroll, and D. G. Diego, â€œThe cost of severe haemophilia in Europeâ€¯: the CHESS studyâ€� Orphanet J. Rare Dis., vol. 12, no. 106, pp. 1â€“8, 2017. [6] S. Reitter, R. Sturn, W. Streif, T. Schabetsberger, F. Wozak, C. Male, W. Muntean, and I. Pabinger, â€œAustrian Haemophilia Registryâ€� Hamostaseologie, vol. 29, no. 41, pp. 13â€“15, 2009. [7] A. Iorio, E. Oliovecchio, M. Morfini, and P. M. Mannucci, â€œItalian Registry of Haemophilia and Allied Disorders. Objectives, methodology and data analysisâ€� Haemophilia, vol. 14, no. 3, pp. 444â€“53, May 2008. [8] J. A. Aznar, L. Abad-Franch, V. R. Cortina, and P. Marco, â€œThe national registry of haemophilia A and B in Spain: results from a census of patientsâ€� Haemophilia, vol. 15, no. 6, pp. 1327â€“30, Nov. 2009. [9] J. Zdziarska, K. Chojnowski, A. Klukowska, M. Å�Ä™towska, A. Mital, J. MusiaÅ‚, M. Podolak-Dawidziak, J. Windyga, P. Ovesna, P. Brabec, and K. Zawilska, â€œRegistry of inherited bleeding disorders in Poland-current status and potential role of the HemoRec databaseâ€� Haemophilia, vol. 17, no. 1, pp. e189-95, 2011. [10] N. von der Weid, â€œHaemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010â€“2011â€� Hamostaseologie, vol. 32, no. 41, pp. S20â€“S24, 2012. [11] C. Hay, â€œThe UK Haemophilia database: a tool for research, audit and healthcare planning,â€� Haemophilia, vol. 10, no. S3, pp. 21â€“25, 2004. [12] L. Teixeira, V. Saavedra, J. P. SimÃµes, B. Sousa Santos, and C. Ferreira, â€œThe Portuguese National Registry for Hemophilia: Developing of a Web-based Technological Solutionâ€� Procedia Comput. Sci., vol. 64, pp. 1248â€“1255, 2015. [13] L. Teixeira, V. Saavedra, C. Ferreira, and B. Sousa Santos, â€œWeb Platform to Support the Portuguese National Registry of Haemophilia and Other Inherited Blood Disordersâ€� Int. J. Web Portals, vol. 7, no.1, pp. 65â€“80, 2015. [14] L. Teixeira, C. Ferreira, V. Saavedra, B. Sousa Santos, â€œWeb-enabled registry of inherited bleeding disorders in Portugal: conditions and perception of the patientsâ€� Haemophilia, vol. 18, no. 1, pp. 56â€“62, 2012. [15] L. Teixeira, V. Saavedra, B. Sousa Santos, C. Ferreira, â€œPortuguese Haemophilia Registry. Set of variables for a computerized solutionâ€� Hamostaseologie, vol. 37, no. 2, pp. 131â€“137, 2017. Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 1001 Author name / Procedia Computer Science 00 (2019) 000â€“000 7 [16] J. Windyga, S. Lopaciuk, E. Stefanska, A. Juszynski, D. Wozniak, and O. Strzelecki, â€œHaemophilia in Polandâ€� Haemophilia, vol. 12, no. 1, pp. 52â€“57, 2006. [17] T. Ali and J. F. Schved, â€œRegistry of hemophilia and other bleeding disorders in Syria.,â€� Haemophilia, vol. 18, no. 6, pp. 851â€“4, 2012. tive sample. In fact, the results obtained from any NPR in this context promote insight about the portrayal of haemophilia in a country, which allows a more efficient management of resources and a better quality of care. Acknowledgement This work was supported by Portuguese funds through the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) and Foundation for Science and Technology, in the context of the project UIDB/00127/2020. References [1] A. Srivastava, A. K. Brewer, E. P. Mauser-Bunschoten, N. S. Key, S. Kitchen, A. Llinas, C. Ludlam, J. N. Mahlangu, K. Mulder, M. C. Poon, and A. Street, â€œGuidelines for the management of hemophiliaâ€� Haemophilia, vol. 19, no. 1, pp. e1-47, 2013. [2] B. Evatt, â€œWorld Federation of Hemophilia Guide to Developing a National Patient Registryâ€� MontrÃ©al - QuÃ©bec, 2005. [3] B. T. Colvin, J. Astermark, K. Fischer, A. Gringeri, R. Lassila, W. Schramm, A. Thomas, and J. Ingerslev, â€œEuropean principles of haemophilia careâ€� Haemophilia, vol. 14, no. 2, pp. 361â€“74, 2008. [4] J. M. Teitel, D. Barnard, S. Israels, D. Lillicrap, M. Poon, and J. Sek, â€œHome management of haemophiliaâ€� Haemophilia, vol. 10, no. 2, pp. 118â€“133, 2004. [5] J. O. Hara, D. Hughes, C. Camp, T. Burke, L. Carroll, and D. G. Diego, â€œThe cost of severe haemophilia in Europeâ€¯: the CHESS studyâ€� Orphanet J. Rare Dis., vol. 12, no. 106, pp. 1â€“8, 2017. [6] S. Reitter, R. Sturn, W. Streif, T. Schabetsberger, F. Wozak, C. Male, W. Muntean, and I. Pabinger, â€œAustrian Haemophilia Registryâ€� Hamostaseologie, vol. 29, no. 41, pp. 13â€“15, 2009. [7] A. Iorio, E. Oliovecchio, M. Morfini, and P. M. Mannucci, â€œItalian Registry of Haemophilia PROCS 39844 S1877-0509(21)00322-7 10.1016/j.procs.2021.01.273 Characterization of Portuguese Haemophilia patients based on the National Registry Data Leonor Teixeira a c Carlos Ferreira a c Beatriz Sousa Santos b c a Department of Economics, Management, Industrial Engineering, and Tourism (DEGEIT),University of Aveiro, 3810-193 Aveiro, Portugal Department of Economics, Management, Industrial Engineering, and Tourism (DEGEIT) University of Aveiro Aveiro 3810-193 Portugal b Department of Electronics, Telecommunications and Informatics (DETI), University of Aveiro, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunications and Informatics (DETI) University of Aveiro Aveiro 3810-193 Portugal c I Institute of Electronics and Informatics Engineering of Aveiro (IEETA), University of Aveiro, 3810-193 Aveiro, Portugal I Institute of Electronics and Informatics Engineering of Aveiro (IEETA) University of Aveiro Aveiro 3810-193 Portugal Haemophilia is a chronic and rare congenital bleeding disorder requiring treatment for life. An updated registry of data on this disease is of paramount importance for documenting prevalence, planning care, and evaluating effectiveness of resources in any country. The study presented in this paper is aimed at characterizing Portuguese patients’ clinical condition and demography, as well as the replacement therapy used in their treatment, based on hemo@record data, a technological solution that supports the National haemophilia registry. The analysis of 110 patients records confirmed the high prevalence of haemophilia A (75.5%) compared with haemophilia B (11.8%) and a large incidence in the severe levels (or the existence of people with mild severity without diagnosis and, consequently, without proper treatment) promoting insight about the portrayal of haemophilia in Portugal. This study should be extended as the registry evolves fostering an ever more efficient management of resources and ultimately a better quality of care for people with haemophilia. Keywords Haemophilia National Registry Data Characterization of PWH Portugal References 1 A. Srivastava A.K. Brewer E.P. Mauser-Bunschoten N.S. Key S. Kitchen A. Llinas C. Ludlam J.N. Mahlangu K. Mulder M.C. Poon A. Street “Guidelines for the management of hemophilia” Haemophilia 19 1 2013 e1 2 B. Evatt “World Federation of Hemophilia Guide to Developing a National Patient Registry” Montréal - Québec 2005 3 B.T. Colvin J. Astermark K. Fischer A. Gringeri R. Lassila W. Schramm A. Thomas J. Ingerslev “European principles of haemophilia care” Haemophilia 14 2 2008 361 374 4 J.M. Teitel D. Barnard S. Israels D. Lillicrap M. Poon J. Sek “Home management of haemophilia” Haemophilia 10 2 2004 118 133 5 J.O. Hara D. Hughes C. Camp T. Burke L. Carroll D.G. Diego “The cost of severe haemophilia in Europe: the CHESS study” Orphanet J. Rare Dis. 12 106 2017 1 8 6 S. Reitter R. Sturn W. Streif T. Schabetsberger F. Wozak C. Male W. Muntean I. Pabinger “Austrian Haemophilia Registry” Hamostaseologie 29 41 2009 13 15 7 A. Iorio E. Oliovecchio M. Morfini P.M. Mannucci “Italian Registry of Haemophilia and Allied Disorders. Objectives, methodology and data analysis” Haemophilia 14 3 2008 444 453 May 8 J.A. Aznar L. Abad-Franch V.R. Cortina P. Marco “The national registry of haemophilia A and B in Spain: results from a census of patients” Haemophilia 15 6 2009 1327 1330 Nov. 9 J. Zdziarska K. Chojnowski A. Klukowska M. Łętowska A. Mital J. Musiał M. Podolak-Dawidziak J. Windyga P. Ovesna P. Brabec K. Zawilska “Registry of inherited bleeding disorders in Poland-current status and potential role of the HemoRec database” Haemophilia 17 1 2011 e189 10 N. von der Weid “Haemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010–2011” Hamostaseologie 32 41 2012 S20 S24 11 C. Hay “The UK Haemophilia database: a tool for research, audit and healthcare planning,” Haemophilia 10 S3 2004 21 25 12 L. Teixeira V. Saavedra J.P. Simões B. Sousa Santos C. Ferreira “The Portuguese National Registry for Hemophilia: Developing of a Web-based Technological Solution” Procedia Comput. Sci. 64 2015 1248 1255 13 L. Teixeira V. Saavedra C. Ferreira B. Sousa Santos “Web Platform to Support the Portuguese National Registry of Haemophilia and Other Inherited Blood Disorders” Int. J. Web Portals 7 1 2015 65 80 14 L. Teixeira C. Ferreira V. Saavedra B. Sousa Santos “Web-enabled registry of inherited bleeding disorders in Portugal: conditions and perception of the patients” Haemophilia 18 1 2012 56 62 15 L. Teixeira V. Saavedra B. Sousa Santos C. Ferreira “Portuguese Haemophilia Registry. Set of variables for a computerized solution” Hamostaseologie 37 2 2017 131 137 Author name / Procedia Computer Science 00 (2019) 000-000 16 J. Windyga S. Lopaciuk E. Stefanska A. Juszynski D. Wozniak O. Strzelecki “Haemophilia in Poland” Haemophilia 12 1 2006 52 57 17 T. Ali J.F. Schved “Registry of hemophilia and other bleeding disorders in Syria.,” Haemophilia 18 6 2012 851 854 "
    },
    {
        "doc_title": "Using Augmented Reality and Step by Step Verification in Industrial Quality Control",
        "doc_scopus_id": "85091118569",
        "doc_doi": "10.1007/978-3-030-58282-1_55",
        "doc_eid": "2-s2.0-85091118569",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Computer vision techniques",
            "Human demonstrations",
            "Industrial scenarios",
            "Manual intervention",
            "Quality control tests",
            "Real-time validation",
            "Step-by-step instructions",
            "Validation process"
        ],
        "doc_abstract": "© 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.Quality control procedures are extremely important among industrial applications. Generally, these tasks include many repetitive tasks that require manual intervention. Given their complexity, quality control tests are often detailed in video recordings, paper instructions, photos or diagrams to guide workers throughout the process. Augmented Reality (AR) has been making significant progress in the last decades, becoming mature enough to be used in industrial scenarios. While some AR systems have been proposed to support quality control procedures, most of them only present information to workers but do not track or validate the process in real-time being used only to guide it. Another limitation of existing systems is the generation of virtual instructions used by AR systems to guide the operator. In this work, we propose an AR-based tool to guide users by overlaying information in a video stream while performing real-time validation during the execution of quality control procedures. The main objective is to provide dynamic support and decrease the mental workload needed to complete the procedure as well as the number of errors, facilitating the procedure execution by untrained workers. Besides this, the tool allows to create virtual content that can be used to generate step-by-step instructions automatically based on human demonstrations. By making the virtual instruction creation effortlessly it is possible to eliminate the user’s need for memorizing new instructions with each change of the product lines. While presenting task relevant information the system uses computer vision techniques to keep track of the procedure stage, verifying its completion and switching automatically to the next step without requiring any interaction from the user. A comparison between the time taken to perform the procedure with and without validation was made. The results show that the validation process would confer the process a significant efficiency boost, while avoiding possible human errors.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring Annotations and Hand Tracking in Augmented Reality for Remote Collaboration",
        "doc_scopus_id": "85091104420",
        "doc_doi": "10.1007/978-3-030-58282-1_14",
        "doc_eid": "2-s2.0-85091104420",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Adaptive capabilities",
            "Head mounted displays",
            "Human-centered designs",
            "Maintenance procedures",
            "Professional experiences",
            "Real-time video streams",
            "Remote collaboration",
            "Visualization of information"
        ],
        "doc_abstract": "© 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.Collaboration among remotely distributed professionals is often required in a maintenance context. Professionals need mechanisms with adaptive capabilities to enable knowledge transfer, since the necessary experience and expertise are usually spread among different professionals. To provide a shared understanding, Augmented Reality (AR) has been explored. In this paper, which is part of ongoing research using a human-centered design approach with partners from the industry sector, we describe a framework using annotations to improve the shared perceived realities of different professionals. The framework allows manually freezing the on-site professional context and sharing it with a remote expert to create annotations. Then, the on-site professional can visualize instructions through aligned and anchored annotations, using a see-through Head Mounted Display (HMD). In addition, annotations based on real-time video stream from a remote expert are also available. Hand tracking is used to manipulate the annotations, enabling the adjustment of their position and scale in the real-world according to the context, thus enriching the on-site professional experience and improving visualization of information while conducting maintenance procedures suggested by a remote expert.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interaction with Virtual Content using Augmented Reality: A User Study in Assembly Procedures",
        "doc_scopus_id": "85095832791",
        "doc_doi": "10.1145/3427324",
        "doc_eid": "2-s2.0-85095832791",
        "doc_date": "2020-11-04",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Social Sciences (miscellaneous)",
                "area_abbreviation": "SOCI",
                "area_code": "3301"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Assembly tasks",
            "Controlled experiment",
            "Interaction design",
            "Interaction methods",
            "Lego block",
            "Limited resolution",
            "Manipulation of virtual objects",
            "See-through HMD"
        ],
        "doc_abstract": "© 2020 ACM.Assembly procedures are a common task in several domains of application. Augmented Reality (AR) has been considered as having great potential in assisting users while performing such tasks. However, poor interaction design and lack of studies, often results in complex and hard to use AR systems. This paper considers three different interaction methods for assembly procedures (Touch gestures in a mobile device; Mobile Device movements; 3D Controllers and See-through HMD). It also describes a controlled experiment aimed at comparing acceptance and usability between these methods in an assembly task using Lego blocks. The main conclusions are that participants were faster using the 3D controllers and Video see-through HMD. Participants also preferred the HMD condition, even though some reported light symptoms of nausea, sickness and/or disorientation, probably due to limited resolution of the HMD cameras used in the video see-through setting and some latency issues. In addition, although some research claims that manipulation of virtual objects with movements of the mobile device can be considered as natural, this condition was the least preferred by the participants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Conceptual Model for Data Collection and Analysis for AR-based Remote Collaboration Evaluation",
        "doc_scopus_id": "85099568572",
        "doc_doi": "10.1109/ISMAR-Adjunct51615.2020.00016",
        "doc_eid": "2-s2.0-85099568572",
        "doc_date": "2020-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Collaborative process",
            "Conceptual model",
            "Data collection",
            "Enabling technologies",
            "Holistic evaluations",
            "Novel methods",
            "Remote collaboration",
            "Team performance"
        ],
        "doc_abstract": "© 2020 IEEE.A significant effort has been devoted to the creation of the enabling technology and in the proposal of novel methods to support remote collaboration using Augmented Reality (AR), given the novelty of the field. As the field progresses to focus on the nuances of supporting collaboration and with the growing number of prototypes mediated by AR, the characterization and evaluation of the collaborative process becomes an essential, but difficult endeavor. Evaluation is particularly challenging in this multifaceted context involving many aspects that may influence the way collaboration occurs. Therefore, it is essential the existence of holistic evaluation strategies that monitor the use and performance of the proposed solutions regarding the team, its members, and the technology, allowing adequate characterization and report of collaborative processes. As a contribute, we propose a conceptual model for multi-user data collection and analysis that monitors several collaboration aspects: individual and team performance, behaviour and level of collaboration, as well as contextual data in scenarios of remote collaboration using AR-based solutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DeepRings: A Concentric-Ring Based Visualization to Understand Deep Learning Models",
        "doc_scopus_id": "85102927129",
        "doc_doi": "10.1109/IV51561.2020.00054",
        "doc_eid": "2-s2.0-85102927129",
        "doc_date": "2020-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Artificial intelligent",
            "Concentric rings",
            "Global perspective",
            "Learning models",
            "Network features",
            "Neural network predictions",
            "Potential problems",
            "Real world environments"
        ],
        "doc_abstract": "© 2020 IEEE.Artificial Intelligent (AI) techniques, such as ma-chine learning (ML), have been making significant progress over the past decade. Many systems have been applied in sensitive tasks involving critical infrastructures which affect human well-being or health. Before deploying an AI system, it is necessary to validate its behavior and guarantee that it will continue to perform as expected when deployed in a real-world environment. For this reason, it is important to comprehend specific aspects of such systems. For example, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images via feature visualization often focuses on explaining predictions for neurons of one single convolutional layer. Not presenting a global perspective over the features learned by the model leads the user to miss the bigger picture. In this work we focus on providing a representation based on the structure of deep neural networks. It presents a visualization able to give the user a global perspective over the feature maps of a convolutional neural network (CNN) in a single image, revealing potential problems of the learning representations present in the network feature maps.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "FICAvis: Data Visualization to Prevent University Dropout",
        "doc_scopus_id": "85102922172",
        "doc_doi": "10.1109/IV51561.2020.00034",
        "doc_eid": "2-s2.0-85102922172",
        "doc_date": "2020-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Creation process",
            "Failure factors",
            "Interactive exploration",
            "Participatory design",
            "Prototype development",
            "Requirements elicitation",
            "Risk indicators",
            "University students"
        ],
        "doc_abstract": "© 2020 IEEE.The FICA project-Tools for Identifying and Combating Dropout-started at the University of Aveiro in 2015 with the aim to help reduce and prevent dropouts and increase academic success among university students. Within the project a signicant amount of data is provided to different University stakeholders to monitor academic issues, however, these data are currently provided in large tables, a format difficult to analyze. In this paper, we present the main aspects of the data, the users and contexts of use. We also propose an approach to allow the visual and interactive exploration of the FICA project data to help monitor the path of the students and identify risk indicators and failure factors that can lead to critical situations such as dropout. A solution developed using the participatory design methodology is presented, detailing all stages of its creation process, from the requirements elicitation based on focus groups and interviews, design and prototype development in Power BI to its evaluation. Some suggestions for future work are also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Egocentric viewpoint in mixed reality situated visualization: Challenges and opportunities",
        "doc_scopus_id": "85102917072",
        "doc_doi": "10.1109/IV51561.2020.00012",
        "doc_eid": "2-s2.0-85102917072",
        "doc_date": "2020-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 IEEE.Mixed reality (MR) is well suited for situated visualization (SV), a method to represent data in a context, with potential in many situations. However, MR-based visualizations are commonly constrained to the users' single egocentric viewpoint reducing their ability to explore all the available information. This article discusses the main limitations and challenges of this approach based on the analysis of existing literature and identifies opportunities, as well as relevant aspects that must be considered when devising new methods aimed at overcoming those limitations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A real-world approach on the problem of chart recognition using classification, detection and perspective correction",
        "doc_scopus_id": "85089219166",
        "doc_doi": "10.3390/s20164370",
        "doc_eid": "2-s2.0-85089219166",
        "doc_date": "2020-08-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Automatic method",
            "Computer vision techniques",
            "Data extraction",
            "Intuitive understanding",
            "Perspective corrections",
            "Real-world image",
            "Recognition methods",
            "State of the art"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Data charts are widely used in our daily lives, being present in regular media, such as newspapers, magazines, web pages, books, and many others. In general, a well-constructed data chart leads to an intuitive understanding of its underlying data. In the same way, when data charts have wrong design choices, a redesign of these representations might be needed. However, in most cases, these charts are shown as a static image, which means that the original data are not usually available. Therefore, automatic methods could be applied to extract the underlying data from the chart images to allow these changes. The task of recognizing charts and extracting data from them is complex, largely due to the variety of chart types and their visual characteristics. Other features in real-world images that can make this task difficult are photo distortions, noise, alignment, etc. Two computer vision techniques that can assist this task and have been little explored in this context are perspective detection and correction. These methods transform a distorted and noisy chart in a clear chart, with its type ready for data extraction or other uses. This paper proposes a classification, detection, and perspective correction process that is suitable for real-world usage, when considering the data used for training a state-of-the-art model for the extraction of a chart in real-world photography. The results showed that, with slight changes, chart recognition methods are now ready for real-world charts, when taking time and accuracy into consideration.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Syntactic and Semantic Analysis for Extended Feedback on Computer Graphics Assignments",
        "doc_scopus_id": "85084366753",
        "doc_doi": "10.1109/MCG.2020.2981786",
        "doc_eid": "2-s2.0-85084366753",
        "doc_date": "2020-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Large groups",
            "Scalable solution",
            "Self assessment",
            "Semantic analysis",
            "Student project",
            "Test sets"
        ],
        "doc_abstract": "© 1981-2012 IEEE.Modern computer graphics courses require students to complete assignments involving computer programming. The evaluation of student programs, either by the student (self-Assessment) or by the instructors (grading) can take a considerable amount of time and does not scale well with large groups. Interactive judges giving a pass/fail verdict do constitute a scalable solution, but they only provide feedback on output correctness. In this article, we present a tool to provide extensive feedback on student submissions. The feedback is based both on checking the output against test sets, as well as on syntactic and semantic analysis of the code. These analyses are performed through a set of code features and instructor-defined rubrics. The tool is built with Python and supports shader programs written in GLSL. Our experiments demonstrate that the tool provides extensive feedback that can be useful to support self-Assessment, facilitate grading, and identify frequent programming mistakes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Serious games for stroke telerehabilitation of upper limb-a review for future research",
        "doc_scopus_id": "85097394134",
        "doc_doi": "10.5195/ijt.2020.6326",
        "doc_eid": "2-s2.0-85097394134",
        "doc_date": "2020-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Rehabilitation",
                "area_abbreviation": "MEDI",
                "area_code": "2742"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020, University Library System, University of Pittsburgh. All rights reserved.Maintaining appropriate home rehabilitation programs after stroke, with proper adherence and remote monitoring is a challenging task. Virtual reality (VR)-based serious games could be a strategy used in telerehabilitation (TR) to engage patients in an enjoyable and therapeutic approach. The aim of this review was to analyze the background and quality of clinical research on this matter to guide future research. The review was based on research material obtained from PubMed and Cochrane up to April 2020 using the PRISMA approach. The use of VR serious games has shown evidence of efficacy on upper limb TR after stroke, but the evidence strength is still low due to a limited number of randomized controlled trials (RCT), a small number of participants involved, and heterogeneous samples. Although this is a promising strategy to complement conventional rehabilitation, further investigation is needed to strengthen the evidence of effectiveness and support the dissemination of the developed solutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Storytelling with data in the context of industry 4.0: A power bi-based case study on the shop floor",
        "doc_scopus_id": "85092203609",
        "doc_doi": "10.1007/978-3-030-60152-2_48",
        "doc_eid": "2-s2.0-85092203609",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Cognitive capacity",
            "Human visual",
            "Internet of Things (IOT)",
            "Large amounts of data",
            "Manufacturing industries",
            "Manufacturing process",
            "Value chains",
            "Visualization method"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Industry 4.0 (I4.0) is characterized by cyber physical systems (CFS) and connectivity, paving the way to an end-to-end value chain, using Internet of Things (IoT) platforms supported on a decentralized intelligence in manufacturing processes. In such environments, large amounts of data are produced and there is an urgent need for organizations to take advantage of this data, otherwise its value may be lost. Data needs to be treated to produce consistent and valuable information to support decision-making. In the context of a manufacturing industry, both data analysis and visualization methods can drastically improve understanding of what is being done on the shop floor, enabling easier decision-making, ultimately reducing resources and costs. Visualization and storytelling are powerful ways to take advantage of human visual and cognitive capacities to simplify the business universe. This paper addresses the concept of “Storytelling with Data” and presents an example carried out in the shop floor of a chemical industry company meant to produce a real-time story about the data gathered from one of the manufacturing cells. The result was a streaming dashboard implemented using Microsoft Power BI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Configuration and Use of Pervasive Augmented Reality Interfaces in a Smart Home Context: A Prototype",
        "doc_scopus_id": "85071434188",
        "doc_doi": "10.1007/978-3-030-27928-8_16",
        "doc_eid": "2-s2.0-85071434188",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Context sensitive computing",
            "Dynamic environments",
            "Information display",
            "Intelligent environment",
            "Management and controls",
            "Positioning and tracking",
            "Smart homes",
            "Uninterrupted experiences"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.The Augmented Reality (AR) is believed to be a trending mechanism for intelligent environments since it provides additional layers of information on top of the physical world. Pervasive AR extends this concept through context-awareness, 3D space positioning and tracking mechanisms, resulting in experiences without interruptions. This paper proposes a pervasive AR application to create and configure such experiences, allowing management and control of appliances in a Smart Home context Based on a virtual content re-calibration process, the application is able to place and highlight what in the smart home can be interacted with a high level of accuracy and resilience to changes in dynamic environments. We also present examples of other appliances that can be integrated into the management and control features. Finally, we propose possible scenarios besides the Smart Home where the application might also be used.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Adaptive Augmented Reality User Interfaces Using Face Recognition for Smart Home Control",
        "doc_scopus_id": "85071420584",
        "doc_doi": "10.1007/978-3-030-27928-8_3",
        "doc_eid": "2-s2.0-85071420584",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Adaptive augmented realities",
            "Adaptive user interface",
            "Interaction process",
            "Smart homes",
            "User profile"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Augmented Reality (AR) offers the possibility to present information depending on the context/location: Pointing at a given appliance will automatically present relevant interfaces. In this work, we explore Pervasive AR in the context of the Smart Home: Users may select the control of a given appliance by pointing at it, simplifying the interaction process. This idea is then extended to consider user profile in the loop: face recognition is used to recognize a given user of and adapt the interface not only to location but also to specific users needs. A preliminary study was conducted to evaluate and validate the concept.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Pervasive augmented reality for indoor uninterrupted experiences: A user study",
        "doc_scopus_id": "85072884671",
        "doc_doi": "10.1145/3341162.3343759",
        "doc_eid": "2-s2.0-85072884671",
        "doc_date": "2019-09-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Context sensitive computing",
            "Dynamic environments",
            "Location based",
            "Position and orientations",
            "Real environments",
            "Uninterrupted Experiences",
            "User acceptance",
            "User study"
        ],
        "doc_abstract": "© 2019 Copyright held by the owner/author(s).Augmented Reality (AR) adds additional layers of information on top of real environments. Recently, Pervasive AR extends this concept through an AR experience that is continuous in space, being aware of and responsive to the user’s context and pose (position and orientation). This paper focus on an exploratory user study with 27 participants meant to better understand some aspects of Pervasive AR, such as how users explore, select, recognize and manipulate virtual content in uninterrupted AR experiences, as well as their preferences. The approach used to provide this sort of engaging experiences allows the creation of indoor persistent location-based experiences, with a high level of accuracy and resilience to changes in dynamic environments. Results concerning user acceptance of uninterrupted AR experiences were encouraging. In particular, users were positively impressed by the continuous display of virtual content and were willing to use this technology more often and in different contexts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Exploratory Study on the use of Virtual Reality in Balance Rehabilitation<sup>∗</sup>",
        "doc_scopus_id": "85077848963",
        "doc_doi": "10.1109/EMBC.2019.8857469",
        "doc_eid": "2-s2.0-85077848963",
        "doc_date": "2019-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Balance rehabilitations",
            "Exploratory studies",
            "Immersive virtual reality",
            "Physical rehabilitation",
            "Recovery process",
            "Spinal cord injuries (SCI)",
            "Tracking devices",
            "Exercise Therapy",
            "Humans",
            "Postural Balance",
            "Spinal Cord Injuries",
            "Video Games",
            "Virtual Reality Exposure Therapy"
        ],
        "doc_abstract": "© 2019 IEEE.Studies have shown the potential of Virtual Reality and motion tracking devices in physical rehabilitation. This paper addresses the topic of using non-immersive Virtual Reality therapeutic games with motion tracking in physical rehabilitation and describes an exploratory study performed in collaboration with a national public Rehabilitation Center about their use to motivate patients to perform exercises relevant for balance rehabilitation. The work involved developing and adapting mini-games to track patients posture; tests with patients recovering from Spinal Cord Injury suggest that this type of games can be helpful in the recovery process namely in patients' motivation for performing the therapeutic gestures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Situated Visualization in the Decision Process through Augmented Reality",
        "doc_scopus_id": "85072281162",
        "doc_doi": "10.1109/IV.2019.00012",
        "doc_eid": "2-s2.0-85072281162",
        "doc_date": "2019-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Cognitive psychology",
            "Data representations",
            "Decision making process",
            "Decision process",
            "Decision support system (dss)",
            "In contexts",
            "Literature analysis"
        ],
        "doc_abstract": "© 2019 IEEE.The decision-making process and the development of decision support systems (DSS) have been enhanced by a variety of methods originated from information science, cognitive psychology and artificial intelligence over the past years. Situated visualization (SV) is a method to present data representations in context. Its main characteristic is to display data representations near the data referent. As augmented reality (AR) is becoming more mature, affordable and widespread, using it as a tool for SV becomes feasible in several situations. In addition, it may provide a positive contribution to more effective and efficient decision-making, as the users have contextual, relevant and appropriate information to endorse their choices. As new challenges and opportunities arise, it is important to understand the relevance of intertwining these fields. Based on a literature analysis, this paper addresses and discusses current areas of application, benefits, challenges and opportunities of using SV through AR to visualize data in context and to support a decision-making process and its importance in future DSS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Monitoring system for patients with cognitive impairment using mobile devices",
        "doc_scopus_id": "85070095847",
        "doc_doi": "10.23919/CISTI.2019.8760807",
        "doc_eid": "2-s2.0-85070095847",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Cognitive impairment",
            "Healthcare institutions",
            "Identification tags",
            "Locating",
            "Monitoring system",
            "Rehabilitation hospitals",
            "Software solution",
            "Validation process"
        ],
        "doc_abstract": "© 2019 AISTI.The identification, monitoring, tracking and locating of patients with cognitive impairment in healthcare institutions has always been a serious problem for these institutions. Most solutions adopted until today rely mainly on human resources and on very simple manual techniques, like, e.g., putting stickers on the patients' clothes. This paper addresses this problem describing a software solution based on the use of mobile devices, as smartphones, smartwatches, and identification tags all linked to an information system. This work was done in close collaboration with a national public rehabilitation hospital center to really understand the underlying problems, to identify the main requirements and challenges, and to help in the testing and validation process. The final prototype demonstrated the viability of the solution and that it could be deployed in a real set-up without significant effort.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing Spatial and Mobile Augmented Reality for Guiding Assembling Procedures with Task Validation",
        "doc_scopus_id": "85068430962",
        "doc_doi": "10.1109/ICARSC.2019.8733642",
        "doc_eid": "2-s2.0-85068430962",
        "doc_date": "2019-04-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Assembly sequence",
            "Augmented reality systems",
            "Computer vision techniques",
            "Controlled experiment",
            "Mobile augmented reality",
            "Real-time validation",
            "Spatial augmented realities",
            "Validation process"
        ],
        "doc_abstract": "© 2019 IEEE.Assembly tasks are a common situation in many industrial applications. These tasks are often presented on paper or digital manuals containing instructions, photos or diagrams to guide an assembly sequence. While some Augmented Reality (AR) systems have also been proposed to support these processes, only a few track the state of the assembling procedure, validating the process in real-time. In this work, we propose two different AR-based (mobile and spatial AR) methods with real-time validation to provide assistance to users during the execution of an assembly process. The validation process uses computer vision techniques to keep track of the state of the assembly sequence, verifying the completion of each stage and providing information at the end of the assembly. A controlled experiment was used to compare the performance, ease of use, and acceptance of the two AR-based methods proposed. Participants were significantly faster and made fewer errors using the Spatial AR condition. Besides, participants also preferred this condition. In addition, Nasa TLX rating showed that the Spatial AR condition had a slightly lower cognitive load on the participants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Heuristic evaluation in visualization: An empirical study : ition paper",
        "doc_scopus_id": "85062881054",
        "doc_doi": "10.1109/BELIV.2018.8634108",
        "doc_eid": "2-s2.0-85062881054",
        "doc_date": "2019-02-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Empirical studies",
            "Heuristic evaluation",
            "heuristics sets",
            "Potential problems",
            "Usability inspection",
            "Usability problems",
            "Usability tests",
            "Visualization application"
        ],
        "doc_abstract": "© 2018 IEEE.Heuristic evaluation is a usability inspection method that has been adapted to evaluate visualization applications through the development of specific sets of heuristics. This paper presents an empirical study meant to assess the capacity of the method to anticipate the usability issues noticed by users when using a visualization application. The potential usability problems identified by 20 evaluators were compared with the issues found for the same application by 46 users through a usability test, as well as with the fixes recommended by the experimenters observing those users during the test. Results suggest that using some heuristics may have elicited potential problems that none of the users noticed while using the application; on the other hand, users encountered unpredicted usability issues.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Investigating different augmented reality approaches in circuit assembly: A user study",
        "doc_scopus_id": "85072281241",
        "doc_doi": "10.2312/egs.20191011",
        "doc_eid": "2-s2.0-85072281241",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Circuit assembly",
            "Complex task",
            "Controlled experiment",
            "Multi-devices",
            "Traditional approaches",
            "User study"
        ],
        "doc_abstract": "© 2019 The Eurographics Association.Augmented Reality (AR) has been considered as having great potential in assisting performance and training of complex tasks. Assembling electronic circuits is such a task, since many errors may occur, as wrong choice or positioning of components or incorrect wiring and thus using AR approaches may be beneficial. This paper describes a controlled experiment aimed at comparing usability and acceptance of two AR-based approaches (one based on a single device and another approach using two interconnected devices), with a traditional approach using a paper manual in the assembly of an electronic circuit. Participants were significantly faster and made fewer errors while using the AR approaches, and most preferred the multi-device approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using Virtual Reality to Increase Motivation in Poststroke Rehabilitation: VR Therapeutic Mini-Games Help in Poststroke Recovery",
        "doc_scopus_id": "85062846487",
        "doc_doi": "10.1109/MCG.2018.2875630",
        "doc_eid": "2-s2.0-85062846487",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Development process",
            "Fundamental principles",
            "Post-stroke rehabilitation",
            "Rehabilitation programs",
            "Stroke patients",
            "Task-oriented",
            "Upper limbs",
            "Adult",
            "Aged",
            "Female",
            "Humans",
            "Male",
            "Middle Aged",
            "Motivation",
            "Stroke Rehabilitation",
            "Telerehabilitation",
            "Video Games"
        ],
        "doc_abstract": "© 2019 IEEE.Virtual reality (VR) applications meet fundamental principles of rehabilitation: intensity, task oriented training, biofeedback, environments rich in stimuli, and motivation, all pivotal factors for the success of rehabilitation programs. This paper describes the development process of a set of VR minigames developed to increase the motivation of stroke patients while performing repetitive upper limb movements.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "National Patient Registry: A Web-based Technological Solution for Haemophilia in Portugal",
        "doc_scopus_id": "85055827923",
        "doc_doi": "10.1007/978-3-030-02053-8_148",
        "doc_eid": "2-s2.0-85055827923",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Data centres",
            "Haemophilia",
            "National patient registry",
            "Rare disease",
            "Technological solution",
            "WEB application",
            "Web based",
            "Web-based solutions"
        ],
        "doc_abstract": "© 2019, Springer Nature Switzerland AG.Several national and international entities have recognized the importance of national patient registry systems, as well as their benefits in the treatment of chronic and rare diseases, such as haemophilia. Despite this recognition, due to the lack of motivation to invest in systems that benefit a small proportion of the population – characteristic of rare diseases – there are several barriers to create this type of application. This paper describes a Web-based solution to support the national haemophilia registry in Portugal, the first created in this country for this type of chronic and rare disease. Currently, the technological solution is already developed, tested, and installed at the data centre of a Portuguese University, and clinicians of different Haemophilia Treatment Center around the country have access to the operational version, via web.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Information Management Framework to Industry 4.0: A Lean Thinking Approach",
        "doc_scopus_id": "85055777410",
        "doc_doi": "10.1007/978-3-030-02053-8_162",
        "doc_eid": "2-s2.0-85055777410",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Added values",
            "Information and Communication Technologies",
            "Information flows",
            "Information management framework",
            "Information process",
            "Lean thinking",
            "Production environments",
            "Unstructured data"
        ],
        "doc_abstract": "© 2019, Springer Nature Switzerland AG.Industry 4.0 is driven by modern information and communication technologies and is associated with smart factories that promote the digital industry with rapid production environments. Despite the benefits expected from the Industry 4.0, the daily volume of unstructured data in that environment may generate noise, as isolated this data do not provide any added value. In this context, Lean Thinking applied to information processes can minimize such waste, helping in the definition of information flows, as well as on the selection of relevant and value-added data. The present paper aims to analyse the relationship between Lean Thinking and Industry 4.0, and propose a framework that promotes the understanding of this relationship, as well as its impact on the information management processes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Morphological analysis of 3D skull models for ancestry estimation",
        "doc_scopus_id": "85060177532",
        "doc_doi": "10.1109/iV.2018.00104",
        "doc_eid": "2-s2.0-85060177532",
        "doc_date": "2018-12-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "3D models",
            "Computer methods",
            "Direct manipulation",
            "Feature detection",
            "Intra-observer errors",
            "Morphological analysis",
            "Overall process",
            "Semiautomatic methods"
        ],
        "doc_abstract": "© 2018 IEEE.Skull analysis is the main tool used in anthropology to identify several characteristics such as ancestry, sex, and variations between populations. Yet, skull analysis methods used by anthropologists still rely heavily on direct manipulation and measurement of the skulls producing significant inter and intra observer errors. Direct manipulation also involves risks of damaging the specimens while handling. In recent years computer methods for skull analysis that rely on 3D models of skulls acquired with a 3D scanner have been proposed. This approach gives the possibility to perform analysis otherwise not possible, simultaneously easing the overall process of skull analysis and reducing variability. This paper describes the development of automatic and semi-Automatic methods for morphological analysis of 3D skull models through the extraction and classification of structures aiming to support the estimation of ancestry. Results with fifty specimens are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluating and enhancing google tango localization in indoor environments using fiducial markers",
        "doc_scopus_id": "85048857208",
        "doc_doi": "10.1109/ICARSC.2018.8374174",
        "doc_eid": "2-s2.0-85048857208",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.Recent advances in 3D sensing technologies, as well as in inertial measurement technologies, have resulted in significant improvements in the accuracy of the localization of systems that combine all these sensors. Project Tango is one of the most successful examples of such systems. Developed by Google, it integrates in an Android mobile device a set of sensors and software required to provide accurate real-time 3D information when moving the equipment freely in hand. This is making mapping and navigation accessible to the general public, with evident applications in robotics, augmented reality, computer vision and others. The contribution of this paper is towfold: first, we present a thorough evaluation of the localization accuracy of the Tango platform in different conditions; second, we present a fiducial marker-based extension of the Tango localization system, which improves the localization estimates in certain conditions. The paper presents a set of experiments performed to evaluate the position and orientation errors in indoor environments, using Augmented Reality for visualization purposes, with and without area learning, e.g. using a priori information acquired from the environment. In addition, we propose a solution based on the use of additional visual markers, which allows the re-calibration of augmented content in specific locations, to improve tracking accuracy in dynamic environments where spatial and/or illumination changes may occur. A statistical analysis of the results shows that the Tango with area learning and the proposed solution provide a level of accuracy significantly better that the Tango without area learning. Moreover, the proposed solution can overcome some limitations of Tango with area learning when used in dynamic environments.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mobile devices for interaction in immersive virtual environments",
        "doc_scopus_id": "85048875007",
        "doc_doi": "10.1145/3206505.3206526",
        "doc_eid": "2-s2.0-85048875007",
        "doc_date": "2018-05-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "3-d interactions",
            "Flexible interfaces",
            "Head mounted displays",
            "Immersive environment",
            "Immersive virtual environments",
            "Immersive virtual reality",
            "On-board sensors",
            "Virtual representations"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery ACMGamepads and 3D controllers are the main controllers used in most Virtual Environments. Despite being simple to use, these input devices have a number of limitations as fixed layout and difficulty to remember the mapping between buttons and functions. Mobile devices present interesting characteristics that might be valuable in immersive environments: more flexible interfaces, touchscreen combined with onboard sensors that allow new interaction and easy acceptance since these devices are used daily by most users. The work described in this article proposes a solution that uses mobile devices to interact with Immersive Virtual Environments for selection and navigation tasks. The proposed solution uses the mobile device camera to track the Head-Mounted-Display position and present a virtual representation of the mobile device screen; it was tested using an Immersive Virtual Museum as use case. Based on this prototype, a study was performed to compare controller based and mobile based interaction for navigation and selection showing that using mobile devices is viable in this context and offers interesting interaction opportunities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Distinctive approaches to computer graphics education",
        "doc_scopus_id": "85031087146",
        "doc_doi": "10.1111/cgf.13305",
        "doc_eid": "2-s2.0-85031087146",
        "doc_date": "2018-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Computer and information science educations",
            "Computer graphics education",
            "Related works",
            "Undergraduate education",
            "Unsolved problems"
        ],
        "doc_abstract": "© 2017 The Authors and 2017 The Eurographics Association and John Wiley & Sons Ltd.This paper presents the latest advances and research in Computer Graphics education in a nutshell. It is concerned with topics that were presented at the Education Track of the Eurographics Conference held in Lisbon in 2016. We describe works corresponding to approaches to Computer Graphics education that are unconventional in some way and attempt to tackle unsolved problems and challenges regarding the role of arts in computer graphics education, the role of research-oriented activities in undergraduate education and the interaction among different areas of Computer Graphics, as well as their application to courses or extra-curricular activities. We present related works addressing these topics and report experiences, successes and issues in implementing the approaches.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Effect of hand-avatar in a selection task using a tablet as input device in an immersive virtual environment",
        "doc_scopus_id": "85018939288",
        "doc_doi": "10.1109/3DUI.2017.7893364",
        "doc_eid": "2-s2.0-85018939288",
        "doc_date": "2017-04-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "button selection task",
            "hand-avatar",
            "Immersive virtual environments",
            "Input devices",
            "User study"
        ],
        "doc_abstract": "© 2017 IEEE.How does the virtual representation of the user's hands influence the performance on a button selection task performed in a tablet-based interaction within an immersive virtual environment? To answer this question, we asked 55 participants to use three conditions: no-hand avatar, realistic avatar and translucent avatar. The participants were faster but made slightly more errors while using the no-avatar condition, and considered easier to perform the task with the translucent avatar.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards automatic non-metric traits analysis on 3D models of skulls",
        "doc_scopus_id": "85016055539",
        "doc_doi": "10.1109/EPCGI.2016.7851196",
        "doc_eid": "2-s2.0-85016055539",
        "doc_date": "2017-02-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "3-D scanner",
            "3D models",
            "Ancestry",
            "Feature detection",
            "Intra-observer variability",
            "Morphological analysis",
            "New approaches",
            "Non-Metric"
        ],
        "doc_abstract": "© 2016 IEEE.The morphological and metric methods used by anthropologists to assess ancestry can generate results with low repeatability besides damaging the specimens while handling. These problems have led to the development of a new approach based on skulls acquisition with a 3D scanner, using the resulting models to make measurements and morphological analyzes in the CraMs application (Craniometric Measurements). This paper focuses on the development of new methods for the morphological analysis, and the extraction and classification of structures with the objective of reducing inter and intra observer variability. The final aim is to ease the process of estimating the individual's ancestry.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "What should a virtual/augmented reality course be?",
        "doc_scopus_id": "85092203085",
        "doc_doi": "10.2312/eged.20171027",
        "doc_eid": "2-s2.0-85092203085",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Core course",
            "Introductory course",
            "New applications",
            "Virtual and augmented reality"
        ],
        "doc_abstract": "© 2017 The Author(s) Eurographics Proceedings © 2017 The Eurographics Association.Never before has Virtual and Augmented Reality hardware been so affordable allowing so many new applications of these technologies; however, developing these applications implies specific skills that are not usually acquired in core courses in Computer Science/Engineering. In this context, specific courses introducing the basics on these technologies seem to be most relevant. With this panel we intend to foster a discussion concerning what should an introductory course on Virtual/Augmented Reality be as of 2017. A review of the courses described in literature is presented as well as guidelines issued by professional/scientific associations concerning a basic Virtual Reality course identifying a set of relevant aspects to be considered when organizing such a course.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Addressing a multi-objective capacitated location-routing problem using evolutionary algorithms",
        "doc_scopus_id": "85040930529",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85040930529",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Safety, Risk, Reliability and Quality",
                "area_abbreviation": "ENGI",
                "area_code": "2213"
            }
        ],
        "doc_keywords": [
            "Capacitated location",
            "Computational experiment",
            "Distribution route",
            "Location routing problem",
            "Location-routing",
            "Multi objective",
            "Obnoxious facilities",
            "Semi-obnoxious"
        ],
        "doc_abstract": "In location-routing problems (LRP) the location of facilities and the corresponding distribution routes are addressed simultaneously, reflecting a common concern when designing logistics systems. The LRP considering capacitated routes and depots has been called capacitated location-routing problem (CLRP), where current studies focus on a single objective: Cost minimization. Although this approach is suitable for determining the location-routing of most (desirable) facilities, for (semi-)obnoxious facilities, as other objectives also gain relevance, multi-objective approaches should be used. This paper proposes two evolutionary algorithms for a multi-objective CLRP. The algorithms are based on two well-known frameworks, which are hybridized with local search procedures. Computational experiments were carried out on a set of benchmark instances adapted from the single-objective literature. Results show that local search procedures allow obtaining a better approximation of the Pareto front and reinforce the belief that multi-objective approaches should be used in LRPs when other objectives besides cost have to be considered.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Freehand gesture-based 3D manipulation methods for interaction with large displays",
        "doc_scopus_id": "85025152989",
        "doc_doi": "10.1007/978-3-319-58697-7_10",
        "doc_eid": "2-s2.0-85025152989",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D object manipulations",
            "3D user interface",
            "Hand gesture",
            "Large displays",
            "User study"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Gesture-based 3D interaction is a research topic with application in numerous scenarios which gained relevance with the recent advances in low-cost tracking systems. Yet, it poses many challenges due to its novelty and consequent lack of systematic development methodologies. Developing easy to use and learn gesture-based 3D interfaces is particularly difficult since the most adequate and intuitive gestures are not always obvious and there is often a variety of different gestures used to perform similar actions. This paper presents the development and evaluation of interaction methods to manipulate 3D virtual objects in a large display set-up using freehand gestures detected by a Kinect depth sensor. We describe the implementation of these methods and the user studies conducted to improve them and assess their usability as manipulation methods. Based on the results of these studies we also propose a method that overcomes the lack of roll movement detection by the Kinect and makes simpler the scaling and rotation in all degrees-of-freedom using hand gestures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation methods to support health information systems development: A framework supported in literature and practical experience",
        "doc_scopus_id": "85025117076",
        "doc_doi": "10.1007/978-3-319-58466-9_8",
        "doc_eid": "2-s2.0-85025117076",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Evaluation methods",
            "Formative",
            "Health information systems",
            "SDLC",
            "Summative",
            "User-centred"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Given the diversity and complexity of the Health Information Systems (HIS), and taking into account the impact of this type of systems in the clinical performance and patient outcome, a rigorous evaluation process in the system development life cycle (SDLC) is extremely important. An effective evaluation during development not only promotes the quality of the final solution, but also ensures motivated users, error-free systems, and can even establish good practices to minimize costs in future developments. However, the HIS evaluation is a difficult process due to the complex nature of the health care domain, the objects being evaluated, as well as the comprehensiveness of the concept of the evaluation itself. The present work intends to explore, based on a literature review, the main methods of HIS evaluation to support the development, identifying in which stage of the SDLC these methods can be applied. Additionally, this work discusses the reasons for the evaluation of such systems, illustrating these issues with two real case studies of HIS implementations, in which some of the methods were successfully applied.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An exploratory study on the predictive capacity of heuristic evaluation in visualization applications",
        "doc_scopus_id": "85021624109",
        "doc_doi": "10.1007/978-3-319-58071-5_28",
        "doc_eid": "2-s2.0-85021624109",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Exploratory studies",
            "Heuristic evaluation",
            "Information visualization",
            "Potential problems",
            "Predictive capacity",
            "Usability evaluation",
            "User study",
            "Visualization application"
        ],
        "doc_abstract": "© 2017, Springer International Publishing AG.Heuristic evaluation is generally considered as an adequate method to perform formative usability evaluation as it helps identify potential problems from early stages of development and may provide useful results even with a relatively low investment. In particular, the method has been adapted and used to evaluate visualization applications. This paper presents an exploratory study aimed at better understanding the capacity of heuristic evaluation to predict the issues experienced by users when using a visualization application and how to assess it. The main usability potential problems pointed out in a visualization application by 20 evaluators using heuristic evaluation are compared with the problems reported for the same application by 44 users.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Portuguese haemophilia registry: Set of variables for a computerized solution",
        "doc_scopus_id": "85019495052",
        "doc_doi": "10.5482/HAMO-15-09-0027",
        "doc_eid": "2-s2.0-85019495052",
        "doc_date": "2017-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Hematology",
                "area_abbreviation": "MEDI",
                "area_code": "2720"
            }
        ],
        "doc_keywords": [
            "Confidentiality",
            "Database Management Systems",
            "Databases, Factual",
            "Hemophilia A",
            "Humans",
            "Information Storage and Retrieval",
            "Medical Record Linkage",
            "Medical Records Systems, Computerized",
            "Portugal",
            "Registries",
            "Data Collection",
            "Databases, Factual",
            "Hemophilia A",
            "Portugal",
            "Software Design"
        ],
        "doc_abstract": "© Schattauer 2017.National Patient Registries (NPR) have an important role in the management of haemophilia and other inherited bleeding disorders, representing powerful instruments to support healthcare and research. Computer software to assist the NPR is crucial, as it facilitates the introduction of the data from a national universe that will be centralized and merged into a unique location, thus ensuring a greater reliability and accuracy of the collected data, avoiding duplication of patients. In Portugal, despite the efforts and recognition of the need of a NPR, just recently the protocol for the establishment of the computer software to support the Portuguese National Registry of Haemophilia and other Congenital Coagulopathies (PorR H&CC) was approved. This paper aims to present this newly developed computerized solution, as well as to report the main variables and information that will be available. The development of this application, which includes a set of socio-demographic, clinical and treatment data, was based on the principles of WFH, and the database that supports the NPR, with anonymized data, is operated and maintained in accordance with the Data Protection Law. Currently, the first data are available on the application. Our focus now is to ensure more registries and continuous data entry in order to have complete information on the characterization of the haemophilia patient population in Portugal.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visually supporting location and routing decisions in tourist trip planning: An exploratory approach",
        "doc_scopus_id": "84989859084",
        "doc_doi": "10.1109/IV.2016.17",
        "doc_eid": "2-s2.0-84989859084",
        "doc_date": "2016-08-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Decision making process",
            "Interactive visualizations",
            "Location-routing",
            "Solution approach",
            "Trip planning",
            "Usability evaluation",
            "Visual exploration",
            "Web map services"
        ],
        "doc_abstract": "© 2016 IEEE.This paper presents a new web-based tool which allows the visualization of real online data combined with other relevant information for supporting tourist trip planning decisions. The main decisions intended supporting concern location, namely place to stay, and routing, which sites to visit and the best sequence to visit them. As they are interconnected, location and routing should be addressed simultaneously. The integrated view has been addressed in the literature from a modeling and solution approach perspective, however, lack of comprehensive visualization solutions to help the decision-making process may hinder their real-world applicability. Although this work focuses on a specific application, the tool presented herein can support other real-world location-routing applications (e.g. in logistics). It enables interactive visualizations and navigation in web browsers, and provides functionalities considered important in the context of tourist trip planning. Focus group and usability evaluation were used throughout the development of the tool for collecting information and testing visualizations and interactions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Developing 3D freehand gesture-based interaction methods for virtual walkthroughs: Using an iterative approach",
        "doc_scopus_id": "85014148227",
        "doc_doi": "10.4018/978-1-5225-0435-1.ch003",
        "doc_eid": "2-s2.0-85014148227",
        "doc_date": "2016-06-29",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D interactions",
            "Controlled experiment",
            "Development methodology",
            "Gesture tracking",
            "Gesture-based interaction",
            "Iterative approach",
            "Navigation methods",
            "Research topics"
        ],
        "doc_abstract": "© 2016 by IGI Global. All rights reserved.Gesture-based 3D interaction has been considered a relevant research topic as it has a natural application in several scenarios. Yet, it presents several challenges due to its novelty and consequential lack of systematic development methodologies, as well as to inherent usability related problems. Moreover, it is not always obvious which are the most adequate and intuitive gestures, and users may use a variety of different gestures to perform similar actions. This chapter describes how spatial freehand gesture based navigation methods were developed to be used in virtual walkthroughs meant to be experienced in large displays using a depth sensor for gesture tracking. Several iterations of design, implementation, user tests, and controlled experiments performed as formative and summative evaluation to improve, validate, and compare the methods are presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A simple and effective evolutionary algorithm for the capacitated location-routing problem",
        "doc_scopus_id": "84955451591",
        "doc_doi": "10.1016/j.cor.2016.01.006",
        "doc_eid": "2-s2.0-84955451591",
        "doc_date": "2016-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            }
        ],
        "doc_keywords": [
            "Capacitated location",
            "Computational evaluation",
            "Computing time",
            "Hybrid genetic algorithms",
            "Local search",
            "Location-routing"
        ],
        "doc_abstract": "© 2016 Elsevier Ltd. All rights reserved.This paper proposes a hybrid genetic algorithm (GA) to solve the capacitated location-routing problem. The proposed algorithm follows the standard GA framework using local search procedures in the mutation phase. Computational evaluation was carried out on three sets of benchmark instances from the literature. Results show that, although relatively simple, the proposed algorithm is effective, providing competitive results for benchmark instances within reasonable computing time.",
        "available": true,
        "clean_text": "serial JL 271709 291210 291692 291715 291772 291813 291817 291871 31 Computers & Operations Research COMPUTERSOPERATIONSRESEARCH 2016-01-12 2016-01-12 2016-01-25 2016-01-25 2016-02-22T12:57:38 S0305-0548(16)00008-3 S0305054816000083 10.1016/j.cor.2016.01.006 S300 S300.1 FULL-TEXT 2016-09-02T17:19:40.919789-04:00 0 0 20160601 20160630 2016 2016-01-12T07:20:26.473949Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor highlightsabst primabst ref 0305-0548 03050548 true 70 70 C Volume 70 14 155 162 155 162 201606 June 2016 2016-06-01 2016-06-30 2016 article fla Copyright © 2016 Elsevier Ltd. All rights reserved. ASIMPLEEFFECTIVEEVOLUTIONARYALGORITHMFORCAPACITATEDLOCATIONROUTINGPROBLEM LOPES R 1 Introduction 2 Literature review 3 Evolutionary algorithm 3.1 Chromosome representation 3.2 Crossover operator 3.3 Mutation operators 3.3.1 Add and swap operators 3.3.2 Local search procedures 3.4 General structure 4 Computational evaluation 4.1 Implementation 4.2 Benchmark instances 4.3 Results 5 Conclusions Acknowledgments References BALDACCI 2011 1284 1296 R BARRETO 2007 968 977 S BELENGUER 2011 931 941 J BRANCO 1990 86 95 I CONTARDO 2013 263 295 C CONTARDO 2014 1 38 C CONTARDO 2014 88 102 C DREXL 2015 283 308 M DUHAMEL 2010 1912 1923 C ESCOBAR 2013 70 79 J ESCOBAR 2014 344 356 J FILHO 1998 189 209 V HANSEN 2006 802 817 P HEMMELMAYR 2012 3215 3228 V LAPORTE 1989 471 482 G LAPORTE 2000 285 300 G LIN 1973 498 516 S LOPES 2013 795 822 R MARINAKIS 2008 49 65 Y OSMAN 1993 421 451 I PRINS 2004 1985 2002 C PRINS 2006 183 194 C EVOLUTIONARYCOMPUTATIONINCOMBINATORIALOPTIMIZATIONLECTURENOTESINCOMPUTERSCIENCE AMEMETICALGORITHMPOPULATIONMANAGEMENTMAPMFORCAPACITATEDLOCATIONROUTINGPROBLEM PRINS 2006 221 238 C PRINS 2007 470 483 C PRODHON 2014 1 17 C SALHI 1999 3 19 S SAVELSBERGH 1992 146 154 M TING 2013 34 44 C TOTH 2003 333 346 P TUZUN 1999 87 99 D YU 2010 288 299 V LOPESX2016X155 LOPESX2016X155X162 LOPESX2016X155XR LOPESX2016X155X162XR 2019-01-25T00:00:00Z UnderEmbargo item S0305-0548(16)00008-3 S0305054816000083 10.1016/j.cor.2016.01.006 271709 2016-02-22T09:59:04.029387-05:00 2016-06-01 2016-06-30 true 403383 MAIN 8 57904 849 656 IMAGE-WEB-PDF 1 gr1 11275 83 219 gr2 10422 115 219 gr3 11292 107 219 gr4 10914 164 203 gr1 26364 86 226 gr2 45473 267 508 gr3 47717 247 508 gr4 51542 409 508 si0001 295 13 57 si0002 131 10 12 si0003 128 10 10 si0004 120 13 9 si0005 138 8 13 si0006 224 13 42 si0007 117 8 9 si0008 166 10 24 si0009 149 14 12 si0010 178 13 26 si0011 157 13 15 si0012 148 13 13 si0013 145 17 12 si0014 123 10 10 si0015 142 13 13 si0016 280 13 45 si0017 364 13 56 si0018 124 10 9 si0019 280 13 47 si0020 263 15 45 si0021 126 10 10 si0022 401 13 70 si0023 188 9 28 si0024 452 11 79 si0025 499 13 87 si0026 243 12 42 si0027 246 13 40 si0028 428 11 73 si0029 404 26 67 si0030 826 32 148 si0031 122 8 9 si0032 258 11 48 si0033 394 11 86 si0034 271 11 48 si0035 147 8 19 si0036 902 143 1925 si0037 165 13 25 si0038 139 10 22 si0039 165 8 23 si0040 177 13 22 CAOR 3915 S0305-0548(16)00008-3 10.1016/j.cor.2016.01.006 Elsevier Ltd Fig. 1 Chromosome representation of a solution for the CLRP. Fig. 1. Fig. 2 Example of the RCX operator. Fig. 2. Fig. 3 Examples of the add and swap mutation operators. Fig. 3. Fig. 4 Overview of the proposed hybrid GA. Fig. 4. Table 1 Results for the instances by Tuzun and Burke [31]. Table 1. Instance BKR HybridGA HybridGA+ CPU Average Best CPU Average Best Result Std dev Gap BKR Result Gap BKR Result Std dev Gap BKR Result Gap BKR 1 P111112 1467.68 19.0 1478.57 0.39 0.74 1470.05 0.16 78.3 1474.73 0.22 0.48 1469.54 0.13 2 P111122 1449.20 31.8 1460.62 0.40 0.79 1449.20 0.00 92.0 1453.42 0.19 0.29 1449.20 0.00 3 P111212 1394.80 18.1 1404.43 0.29 0.69 1396.46 0.12 70.5 1401.79 0.25 0.50 1395.80 0.07 4 P111222 1432.29 30.9 1433.09 0.04 0.06 1432.29 0.00 88.1 1432.63 0.03 0.02 1432.29 0.00 5 P112112 1167.16 21.2 1171.53 0.24 0.37 1167.53 0.03 78.5 1169.95 0.13 0.24 1167.53 0.03 6 P112122 1102.24 27.5 1106.10 0.17 0.35 1102.49 0.02 101.7 1103.64 0.06 0.13 1102.38 0.01 7 P112212 791.66 17.3 796.33 0.37 0.59 792.52 0.11 74.3 794.10 0.17 0.31 791.91 0.03 8 P112222 728.30 37.7 734.05 0.30 0.79 730.15 0.25 107.5 732.37 0.16 0.56 730.27 0.27 9 P113112 1238.24 17.1 1244.49 0.21 0.50 1239.61 0.11 72.9 1241.66 0.14 0.28 1238.49 0.02 10 P113122 1245.31 27.2 1247.15 0.04 0.15 1246.06 0.06 103.7 1246.45 0.03 0.09 1246.06 0.06 11 P113212 902.26 19.4 910.04 0.57 0.86 903.16 0.10 88.2 907.12 0.27 0.54 903.50 0.14 12 P113222 1018.29 28.1 1025.26 0.28 0.68 1019.72 0.14 99.1 1023.08 0.21 0.47 1019.39 0.11 13 P131112 1866.75 63.0 1936.10 0.34 3.71 1924.44 3.09 288.0 1930.80 0.28 3.43 1921.30 2.92 14 P131122 1823.20 81.0 1853.82 0.54 1.68 1837.67 0.79 316.7 1841.24 0.37 0.99 1830.75 0.41 15 P131212 1964.30 58.0 2007.21 0.38 2.18 1992.13 1.42 288.4 1989.59 0.20 1.29 1981.26 0.86 16 P131222 1792.80 81.2 1834.70 0.53 2.34 1818.80 1.45 322.1 1822.49 0.36 1.66 1813.72 1.17 17 P132112 1443.33 63.1 1475.33 0.50 2.22 1461.75 1.28 261.5 1465.21 0.34 1.52 1457.01 0.95 18 P132122 1434.60 78.1 1452.51 0.18 1.25 1447.88 0.93 389.9 1449.42 0.15 1.03 1446.46 0.83 19 P132212 1204.42 81.8 1219.07 0.35 1.22 1212.55 0.68 269.7 1214.56 0.17 0.84 1211.83 0.62 20 P132222 930.99 73.6 937.64 0.18 0.71 934.51 0.38 317.7 935.78 0.07 0.51 934.37 0.36 21 P133112 1694.18 53.9 1724.64 0.54 1.80 1709.67 0.91 356.8 1718.35 0.37 1.43 1708.63 0.85 22 P133122 1392.00 83.6 1409.31 0.26 1.24 1402.25 0.74 339.6 1406.15 0.22 1.02 1400.01 0.58 23 P133212 1198.20 59.5 1223.15 0.36 2.08 1214.87 1.39 234.9 1220.26 0.31 1.84 1213.93 1.31 24 P133222 1151.80 77.5 1161.55 0.20 0.85 1157.50 0.49 347.2 1159.92 0.15 0.71 1157.11 0.46 25 P121112 2243.40 121.7 2297.33 0.36 2.40 2280.97 1.67 623.0 2284.78 0.44 1.84 2270.18 1.19 26 P121122 2138.40 184.7 2213.79 0.54 3.53 2195.06 2.65 700.3 2199.78 0.63 2.87 2177.32 1.82 27 P121212 2209.30 125.8 2251.74 0.33 1.92 2237.80 1.29 615.8 2241.53 0.27 1.46 2230.37 0.95 28 P121222 2222.90 190.7 2272.27 0.48 2.22 2255.03 1.45 659.0 2261.76 0.32 1.75 2250.38 1.24 29 P122112 2073.70 137.5 2111.75 0.31 1.83 2100.21 1.28 593.3 2108.40 0.23 1.67 2101.75 1.35 30 P122122 1692.17 188.4 1723.88 0.37 1.87 1711.58 1.15 726.0 1722.06 0.31 1.77 1712.60 1.21 31 P122212 1453.18 138.8 1478.06 0.25 1.71 1472.38 1.32 689.6 1471.76 0.25 1.28 1466.64 0.93 32 P122222 1082.46 229.5 1094.81 0.11 1.14 1092.14 0.89 749.1 1090.44 0.17 0.74 1087.91 0.50 33 P123112 1954.70 137.4 1985.98 0.30 1.60 1977.48 1.17 700.6 1981.53 0.15 1.37 1976.97 1.14 34 P123122 1918.93 161.5 1976.13 0.41 2.98 1960.24 2.15 812.9 1963.24 0.35 2.31 1951.47 1.70 35 P123212 1762.00 139.6 1781.02 0.20 1.08 1774.48 0.71 630.4 1777.37 0.17 0.87 1772.91 0.62 36 P123222 1390.87 191.5 1418.24 0.59 1.97 1401.98 0.80 802.6 1406.67 0.32 1.14 1399.82 0.64 Average 86.0 0.33 1.45 0.87 363.6 0.23 1.09 0.71 Median 75.6 0.34 1.25 0.80 317.2 0.22 1.00 0.62 Table 2 Results for the instances by Barreto et al. [2]. Table 2 Instance LB BKR HybridGA HybridGA+ CPU Average Best CPU Average Best Result Std dev Gap BKR Result Gap BKR Result Std dev Gap BKR Result Gap BKR 1 Gaskell67 424.9 424.9 0.8 424.9 0.00 0.00 424.9 0.00 1.8 424.9 0.00 0.00 424.9 0.00 2 Gaskell67 585.1 585.1 0.6 585.1 0.00 0.00 585.1 0.00 1.5 585.1 0.00 0.00 585.1 0.00 3 Gaskell67 512.1 512.1 0.9 512.1 0.00 0.00 512.1 0.00 2.8 512.1 0.00 0.00 512.1 0.00 4 Gaskell67 562.2 562.2 1.1 562.2 0.00 0.00 562.2 0.00 3.5 562.2 0.00 0.00 562.2 0.00 5 Gaskell67 504.3 504.3 0.9 504.3 0.00 0.00 504.3 0.00 3.2 504.3 0.00 0.00 504.3 0.00 6 Gaskell67 460.4 460.4 1.4 460.4 0.00 0.00 460.4 0.00 4.7 460.4 0.00 0.00 460.4 0.00 7 Christofides69 565.6 565.6 4.2 565.6 0.00 0.00 565.6 0.00 16.1 565.6 0.00 0.00 565.6 0.00 8 Christofides69 810.9 844.4 23.1 853.7 0.69 1.10 846.5 0.25 62.4 849.2 0.41 0.57 844.4 0.00 9 Christofides69 833.4 833.4 17.2 839.0 0.36 0.67 833.4 0.00 82.4 835.3 0.24 0.23 833.4 0.00 10 Perl83 204.0 204.0 0.1 204.0 0.00 0.00 204.0 0.00 0.2 204.0 0.00 0.00 204.0 0.00 11 Perl83 1074.8 1112.1 20.9 1112.8 0.03 0.07 1112.1 0.00 48.5 1112.5 0.03 0.04 1112.1 0.00 12 Perl83 1568.1 1622.5 33.7 1623.6 0.04 0.07 1622.5 0.00 94.2 1622.8 0.02 0.02 1622.5 0.00 13 Perl83 557,275.2 256.7 580,397.2 0.92 4.15 572,038.3 2.65 4746.5 561,495.7 0.43 0.76 557,249.3 0.00 14 Perl83 670,118.5 215.0 685,951.5 0.55 2.36 680,246.9 1.51 1771.3 668,878.0 0.44 -0.19 663,070.0 −1.05 15 Min92 3062.0 3062.0 1.0 3062.0 0.00 0.00 3062.0 0.00 2.5 3062.0 0.00 0.00 3062.0 0.00 16 Min92 5423.0 5709.0 62.9 5794.7 0.62 1.50 5730.5 0.38 343.1 5732.8 0.25 0.42 5709.0 0.00 17 Daskin95 351.5 355.8 32.4 356.0 0.04 0.05 355.8 0.00 160.6 355.8 0.00 0.00 355.8 0.00 18 Daskin95 43,406.0 43,919.9 96.0 44,179.6 0.30 0.59 43,993.4 0.17 529.9 43,982.7 0.12 0.14 43,919.9 0.00 19 Or76 12,048.4 12,290.3 36.6 12,324.8 0.25 0.28 12,290.3 0.00 286.5 12,293.6 0.03 0.03 12,290.3 0.00 Average 42.4 0.20 0.57 0.26 429.6 0.10 0.11 −0.06 Median 17.2 0.03 0.05 0.00 48.5 0.00 0.00 0.00 Table 3 Results for the instances by Prins et al. [24]. Table 3 Instance LB BKR HybridGA HybridGA+ CPU Average Best CPU Average Best Result Std dev Gap BKR Result Gap BKR Result Std dev Gap BKR Result Gap BKR 1 20-5-1a 54,793 54,793 0.7 54,793 0.00 0.00 54,793 0.00 1.6 54,793 0.00 0.00 54,793 0.00 2 20-5-1b 39,104 39,104 0.6 39,104 0.00 0.00 39,104 0.00 1.5 39,104 0.00 0.00 39,104 0.00 3 20-5-2a 48,908 48,908 0.7 48,908 0.00 0.00 48,908 0.00 1.5 48,908 0.00 0.00 48,908 0.00 4 20-5-2b 37,542 37,542 0.5 37,542 0.00 0.00 37,542 0.00 1.4 37,542 0.00 0.00 37,542 0.00 5 50-5-1a 90,111 90,111 7.1 90,111 0.00 0.00 90,111 0.00 17.4 90,111 0.00 0.00 90,111 0.00 6 50-5-1b 63,242 63,242 4.0 63,242 0.00 0.00 63,242 0.00 14.5 63,242 0.00 0.00 63,242 0.00 7 50-5-2a 88,298 88,298 7.0 88,506 0.19 0.24 88,298 0.00 17.8 88,401 0.08 0.12 88,298 0.00 8 50-5-2b 67,308 67,308 4.1 67,402 0.12 0.14 67,308 0.00 16.8 67,315 0.01 0.01 67,308 0.00 9 50-5-2bis 84,055 84,055 6.4 84,055 0.00 0.00 84,055 0.00 15.1 84,055 0.00 0.00 84,055 0.00 10 50-5-2bbis 51,822 51,822 4.7 51,822 0.00 0.00 51,822 0.00 14.0 51,822 0.00 0.00 51,822 0.00 11 50-5-3a 86,203 86,203 7.1 86,203 0.00 0.00 86,203 0.00 16.2 86,203 0.00 0.00 86,203 0.00 12 50-5-3b 61,830 61,830 3.9 61,830 0.00 0.00 61,830 0.00 14.1 61,830 0.00 0.00 61,830 0.00 13 100-5-1a 274,814 274,814 28.4 277,376 0.23 0.93 276,490 0.61 73.2 277,320 0.20 0.91 276,467 0.60 14 100-5-1b 207,037 213,615 16.1 215,878 0.25 1.06 214,865 0.59 78.6 215,405 0.22 0.84 214,686 0.50 15 100-5-2a 193,671 193,671 29.3 195,076 0.17 0.73 194,534 0.45 73.1 194,725 0.18 0.54 194,013 0.18 16 100-5-2b 157,095 157,095 15.9 157,516 0.10 0.27 157,220 0.08 66.6 157,340 0.07 0.16 157,172 0.05 17 100-5-3a 200,079 200,079 30.7 201,145 0.20 0.53 200,354 0.14 74.8 201,062 0.18 0.49 200,327 0.12 18 100-5-3b 152,441 152,441 21.1 153,408 0.26 0.63 152,672 0.15 61.5 153,127 0.20 0.45 152,653 0.14 19 100-10-1a 258,243 287,695 39.6 295,818 0.69 2.82 291,961 1.48 95.0 292,917 0.62 1.82 289,649 0.68 20 100-10-1b 218,826 230,989 28.2 236,982 0.95 2.59 233,512 1.09 87.4 236,233 0.64 2.27 233,453 1.07 21 100-10-2a 243,590 243,590 49.3 244,959 0.25 0.56 243,855 0.11 110.3 244,712 0.10 0.46 244,075 0.20 22 100-10-2b 203,988 203,988 28.2 204,313 0.09 0.16 203,988 0.00 97.6 204,060 0.03 0.04 203,988 0.00 23 100-10-3a 222,353 250,882 52.9 254,732 0.21 1.53 253,756 1.15 109.3 254,242 0.19 1.34 253,510 1.05 24 100-10-3b 189,309 204,317 25.1 205,256 0.20 0.46 204,759 0.22 84.9 205,131 0.11 0.40 204,755 0.21 25 200-10-1a 475,294 403.7 482,532 0.22 1.52 480,593 1.11 895.5 481,511 0.13 1.31 480,435 1.08 26 200-10-1b 377,043 241.4 382,136 0.15 1.35 381,116 1.08 776.5 381,338 0.14 1.14 380,505 0.92 27 200-10-2a 449,006 336.5 452,710 0.14 0.82 451,560 0.57 835.5 451,994 0.10 0.67 451,293 0.51 28 200-10-2b 374,280 236.4 376,081 0.20 0.48 374,696 0.11 764.5 375,507 0.11 0.33 374,943 0.18 29 200-10-3a 469,433 346.3 477,577 0.19 1.73 475,855 1.37 834.8 476,462 0.17 1.50 475,276 1.24 30 200-10-3b 362,653 216.2 366,893 0.21 1.17 365,597 0.81 720.5 366,343 0.13 1.02 365,592 0.81 Average 73.1 0.17 0.66 0.37 199.1 0.12 0.53 0.32 Median 23.1 0.16 0.47 0.11 73.2 0.10 0.36 0.13 Table 4 Most recent and effective methods in the literature. Table 4 Algorithm Publication # Runs CPU GRASP+ELS [9] 5 Core2 Quad, 2.83GHz SALRP [32] 1 Core2 Quad, 2.66GHz ALNS-500K/ALNS-5000K [14] 5 Opteron 275, 2.20GHz 2-Phase HGTS [10] 1 Core2 Duo, 2.00GHz MACO [29] 10 Athlon XP 2500+, 2.00GHz GRASP+ILP [7] 10 Xeon E5462 Quad Core, 3.00GHz GVTNS [11] 1 Core2 Duo, 2.00GHz HybridGA/HybridGA+ 10 Core i7-4790, 3.60GHz Table 5 Average results for the three benchmark sets. Table 5 Algorithm Tuzun and Burke Barreto et al. Prins et al. CPU Avg Gap BKR Best Gap BKR CPU Avg Gap BKR Best Gap BKR CPU Avg Gap BKR Best Gap BKR GRASP+ELS 606.6 1.30 187.6 a 0.08 a 258.2 1.11 SALRP 826.4 1.49 464.1 0.43 422.4 0.46 ALNS-500K 829.6 0.90 0.44 177.2 a 0.25 a 0.16 a 451.1 0.74 0.44 ALNS-5000K 8103.0 0.18 1772.0 a 0.06 a 4221.0 0.27 2-Phase HGTS 392.3 1.15 105.2 a 0.78 a 176.4 0.57 MACO 201.9 1.24 191.7 0.11 191.3 0.40 GRASP+ILP 2589.5 0.54 0.18 264.3 a 0.64 a 0.14 a 1163.0 0.32 0.12 GVTNS 201.2 0.76 53.0 a 0.67 a 91.2 0.37 HybridGA 86.0 1.45 0.87 18.7 a 0.30 a 0.06 a 73.1 0.66 0.37 42.4 0.57 0.26 HybridGA+ 363.6 1.09 0.71 93.4 a 0.10 a 0.00 199.1 0.53 0.32 429.6 0.11 −0.06 a a Considering only (the same) 13 out of the 19 instances. A simple and effective evolutionary algorithm for the capacitated location–routing problem Rui Borges Lopes a ⁎ Carlos Ferreira b Beatriz Sousa Santos c a Department of Economics, Management and Industrial Engineering/CIDMA, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Economics, Management and Industrial Engineering/CIDMA, University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal b Department of Economics, Management and Industrial Engineering/IEETA, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Economics, Management and Industrial Engineering/CIO, University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal c Department of Electronics, Telecommunications and Informatics/IEETA, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunications and Informatics/IEETA, University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal ⁎ Corresponding author. This paper proposes a hybrid genetic algorithm (GA) to solve the capacitated location–routing problem. The proposed algorithm follows the standard GA framework using local search procedures in the mutation phase. Computational evaluation was carried out on three sets of benchmark instances from the literature. Results show that, although relatively simple, the proposed algorithm is effective, providing competitive results for benchmark instances within reasonable computing time. Keywords Location Location–routing Genetic algorithm 1 Introduction Location–routing problems (LRP) deal with the combination of two types of decisions that often arise in logistics: the location of facilities and the design of the distribution routes. Several LRP variants have appeared in the literature [19,8], among which the capacitated location–routing problem (CLRP) has recently emerged as one of the most addressed. The CLRP can be defined on a complete and undirected graph G = ( V , E ) with a vertex set V and an edge set E . V consists of a subset J of m potential depots and a subset I = V ∖ J of n clients. Each client i ∈ I has a non-negative demand d i , to be satisfied only once, and is to be assigned to a single depot j ∈ J with capacity w j . The shipment of clients’ demand from the assigned depot is carried out by an unlimited fleet of homogeneous vehicles with capacity Q ; each vehicle returning to the departure depot at the end of the route. The total demand of the clients assigned to each depot must not exceed its capacity and the total demand satisfied by any vehicle must not exceed Q . The following non-negative costs are incurred: fixed cost f j when depot j ∈ J has clients assigned and must be opened; fixed cost F for each vehicle used; and traveling cost c ij each time edge ( i , j ) ∈ E is in a vehicle route. The goal is to determine the set of depots to open and the tracing of the routes in order to minimize total costs. The CLRP is NP-hard and only few instances with more than 100 clients have been solved to proven optimality [1,3,5,7] making heuristic approaches often more suitable for solving real-life instances. This paper proposes a simple but effective heuristic algorithm for solving the CLRP, namely, a hybrid genetic algorithm (GA) where local search procedures are used as mutation operators. The remainder of this paper is organized as follows. Section 2 provides a brief review on the most recent heuristic algorithms for the CLRP. The proposed evolutionary algorithm is detailed in Section 3 and evaluated in Section 4. Finally, conclusions are drawn in Section 5. 2 Literature review Recent surveys on the LRP are the works by Lopes et al. [19], Prodhon and Prins [26] and Drexl and Schneider [8]. In Lopes et al. [19] heuristic approaches are classified according to the adopted framework (how location and routing phases interact) and the used method(s), compiling results of several heuristics for the CLRP. Prodhon and Prins [26] and Drexl and Schneider [8] can be seen as complementary surveys: the former emphasizing the CLRP, detailing and comparing recent algorithms, and the latter focusing on other LRP variants. Although most methods in the literature follow a hierarchical framework and use tour construction and improvement typically within metaheuristics, no clear conclusions could be drawn on the best performing frameworks and methods. The most recent and relevant approaches are mentioned hereafter. Barreto et al. [2] presents a clustering based heuristic for tackling the CLRP with no vehicle acquisition cost. Several clustering methods are used to obtain the routing data and then a facility location problem is solved with the collapsed routes. Marinakis and Marinaki [20] solved the same problem using a bilevel GA. Addressing the CLRP strictly as defined previously are the following works. Prins et al. [24] propose a constructive algorithm for the CLRP: extended savings heuristic. The heuristic is randomized and used in a greedy randomized adaptive search procedure (GRASP). The performance of this dedicated constructive algorithm is worth noting, which motivated further development, presented in Duhamel et al. [9], where a similar GRASP is hybridized with evolutionary local search. In Prins et al. [25] facility location (through Lagrangean relaxation) and vehicle routing (using a granular tabu search) are performed iteratively. The time required to obtain good solutions is remarkable, mostly due to the effectiveness of granular tabu search (GTS) in the routing phase [30]. Also using GTS are the works by Escobar et al. [10] and Escobar et al. [11]: firstly in the improvement phase of its hybrid heuristic; then within a variable neighborhood search algorithm. Other recent methods are by Yu et al. [32], with a simulated annealing (SA) based heuristic; Hemmelmayr et al. [14], using an adaptive large neighborhood search (ALNS) heuristic; Ting and Chen [29] with an ant colony optimization (ACO) algorithm; and Contardo et al. [6] slightly changing the GRASP by Prins et al. [24] and combining it with an integer-linear program. Looking at the methods’ performance, Escobar et al. [10] and Ting and Chen [29] show similar performance concerning both results and CPU times. These methods have been slightly improved by Escobar et al. [11], which provides a good trade-off of results and time to obtain them. The method by Contardo et al. [6] presents the best overall results for the benchmark instances from the literature at the expense of significantly higher computing times. Concerning genetic algorithms for the CLRP two approaches can be found in the literature: a memetic algorithm with population management [23] and a bilevel GA [20]. The memetic algorithm by Prins et al. [23] uses a fixed length chromosome composed of a sequence of genes for the depots and another for the clients, requiring a dedicated procedure for fitness evaluation. As crossover may produce unfeasible offspring a repair procedure is used. Depot configuration is only changed by crossover and local search is used for improving routing. The method works on a small population of high quality solutions using a population management procedure for ensuring diversity. The bilevel GA [20] solves the CLRP in two levels: in the first, solving the capacitated facility location problem; in the second, a vehicle routing problem (VRP) is solved for each of the individuals. Each individual in the population is a solution for the location problem and crossover and mutation only occurs at the first level. For obtaining the CLRP solution a VRP is solved in the second level using expanding neighborhood search (ENS). A large population and very few generations are used. Both methods require efficient constructive algorithms for obtaining the initial population. 3 Evolutionary algorithm The metaheuristic presented here follows the standard GA framework hybridized with local search procedures. The proposed algorithm shares some core principles with the hybrid GA by Prins [22], with good results for the VRP; the main being the use of local search as mutation operators. However, several new implementation aspects were developed or adapted to effectively address the CLRP. The most relevant are the chromosome representation and the crossover and mutation operators. The main components are detailed and the general structure of the algorithm is presented in the following sections. The similarities of this framework with memetic algorithms and scatter search are discussed in Prins [22]. Compared with other genetic algorithms in the CLRP literature, the main advantages of the proposed approach are: an intuitive chromosome representation, also allowing an easy fitness evaluation; an efficient constructive algorithm is not necessary; feasible offspring provided by crossover; and a simple framework. 3.1 Chromosome representation The chromosome in the proposed GA represents a complete solution, i.e., the collection of routes. Both the route (gene) length and the chromosome length are variable and depend on the number of clients serviced and the number of routes in the solution. For example, given a CLRP with 15 clients I={1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15} and 4 possible depot locations J={16, 17, 18, 19}, the chromosome representation of a solution is provided in Fig. 1. The solution in the figure represents installing facilities 16 and 18, and servicing the clients in the given order by four routes (vehicles). The adopted representation, assuming solution feasibility, together with the crossover operator (detailed in the following section), allows obtaining feasible children solutions. Thus, the use of repair methods to restore feasibility is avoided. Moreover, it allows a fast evaluation of its fitness value Fitness ( S ) : the total cost of solution S . Note that higher quality solutions have smaller fitness values. 3.2 Crossover operator The proposed crossover operator (inspired by an operator proposed by Hosny and Mumford [15], for a VRP) tries to copy complete routes from the parent to the child, thus will be named route copy crossover (RCX). It operates by copying to the child a random number of routes (between 1/3 and 2/3) from one of the parents, and the remaining unvisited clients are placed in a relocation pool following the original order in the other parent. The clients in the relocation pool are then inserted in the child, in new routes, and using the currently open depots (as long as capacity is obeyed, randomly opening a new one otherwise). This preempts the use of repair methods as child solutions are always feasible. An example of the RCX is illustrated in Fig. 2. In the example, assuming the first and third routes of Parent 1 are selected, both are copied to Child 1. The remaining clients not yet included in Child 1 (shown underlined in Parent 2) are copied, following their order of appearance, to the relocation pool. The clients in the relocation pool are then used to form new routes in Child 1, using the currently open depots (opening more when depot capacity constraints are violated) and following the sequence as long as vehicle (route) capacity is obeyed. The second child is created similarly, using the parents in reverse roles. The RCX allows inheriting some of the routes from one parent while at the same time randomizing the building of the child solution routes (yet still partially inheriting the structure of the route from the other parent). Moreover, the operator promotes solutions with few open depots and routes with little unused capacity, two features often found in good solutions. 3.3 Mutation operators One of the main components of genetic algorithms is mutation operators, often performing simple random moves. Although useful for providing randomization in the search for the global optimum they are usually insufficient for obtaining competitive results. Here, improvement procedures are also used. Both simple operators and local search procedures are detailed as follows. Note that the simple operators (“add” and “swap”) are important for providing additional randomness, while the local search procedures allow quickly obtaining much better results. 3.3.1 Add and swap operators The first mutation operator, named “add”, seeks to avoid a fast convergence to solutions with few depots (prone to happen due to the RCX operator). At the same time it diversifies the open depots, by opening a new one and randomly reassigning between 1 and 2/3 of the routes to it, depot capacity allowing (see example in Fig. 3, top, with changes underlined). With this operator depot configuration is randomly changed. The second operator randomly swaps the position of two clients in the tracing of the routes (possibly between two routes, depot and vehicle capacity allowing), hence named “swap”. This operator aims at providing a small randomization to the building of the routes. In Fig. 3, bottom, the swap mutation operator is applied to the underlined clients. 3.3.2 Local search procedures Two local search procedures are proposed: one for improving depot location and subsequent allocation of the routes ( LS location ), and another for route improvement ( LS routing ). LS location is a facility location algorithm, namely, the tabu search developed by Filho and Galvão [12]; chosen due to its reduced CPU time and near-optimal results. However, since solutions have routes instead of individual clients, in order to apply the heuristic, each route is collapsed into a single client and the considered distance is the smallest insertion cost of the depot in the original route, similarly to Barreto et al. [2]. LS routing is composed of a relocate algorithm [28], 1-interchange algorithm [21], and a 2-opt algorithm [24], performing intra- and inter-route moves applied sequentially and cyclically until no improvement can be found. Additionally, for the more thorough search, the HybridGA+ variant, a 3-opt algorithm [4] is used. The relocate algorithm performs in the same way as the one used in vehicle routing problems [28], where a single client is reinserted in another position inside the current route or in another route provided there is an improvement to the solution. In the latter case, an adaptation was made for the CLRP in order to account for depot capacity and vehicle constraints. The 1-interchange algorithm [21] tests exchanging each client from one route with every other customer in another route even if not sharing the same depot. If improvements are found and depot and vehicle capacities obeyed, the move is implemented. The 2-opt algorithm [24] inside the routes is equivalent to the well-known 2-opt move [18] whereas the moves between different routes have to consider depot and vehicle capacity constraints as well as depot reassignment. Similarly applying intra- and inter-route moves a 3-opt algorithm [4] is used in variant HybridGA+. All the methods in LS routing implement the first found improvement rather than the best and stop when no additional improvement is detected – for some of these procedures this has been empirically found to be better [13]. 3.4 General structure The proposed algorithm starts out by creating an initial population P made of feasible random chromosomes: Generate ( P ) . Afterwards, a main loop is performed, stopping when a maximum number α max of iterations without improving the best solution is reached. In each iteration of the main loop two parents are chosen to crossover using binary tournament selection: two individuals (chromosomes) are randomly selected from the population and the fittest becomes a parent. RCX is then applied and the two obtained children are added to the population. For maintaining the population size, the two worst solutions are removed. The mutation phase is applied afterwards, where the operators “add” ( AddMutation ) and “swap” ( SwapMutation ), followed by the local search procedures LS location and LS routing are applied to all the population. The location related operators have a mutation rate p location ; for routing related operators mutation rate p routing is used. The best solution is searched and stored (if found) at each iteration. Although increasing the computational burden, it prevents the loss of good solutions during the course of the algorithm. The algorithm ends when the main loop stops, returning the best solution ( BestSolution ). An overview of the pseudo-code of the proposed hybrid GA is given in Fig. 4. Other features, common to GA algorithms, were also tested. These will be addressed in Section 4.1. 4 Computational evaluation Implementation aspects and an evaluation of the proposed GA (using benchmark instances) are presented and discussed in the following sections. 4.1 Implementation The algorithm was coded in C# and results obtained using a 3.60GHz Intel Core i7-4790 CPU with 8GB of RAM and running Windows 7; it is embedded in the LORE tool, available at An extensive study was made testing different features, common to GA algorithms. The tested features which did not consistently improve results (in some cases incurring in additional complexity) were • half of the initial population generated with the randomized extended savings heuristic [24], • changing the order in which mutation operators are used, and • partial replacement procedures. Features typically providing worse results were • all individuals becoming parents and producing the same amount of children, sorting and removing the worst half, • using binary tournament for removing individuals in each iteration, • dynamically changing the mutation rates, and • removing “add” and “swap” mutation operators. When testing the different features one of the components that proved crucial for the method’s performance was the location related operators (“add” and LS location ), as removing them degraded results on average 12.5%. Concerning parameters, based on the testing of the algorithm, the following were found to be the most suitable. The initial population size depends directly on the number of clients and inversely on the number of depots: (1) | P | = ⌈ a n m ⌉ . As the number of depots increases less solutions are required for providing randomness. However, for better exploring the tracing of bigger routes (i.e. when more clients exist) more individuals are required in the population. Also, smaller populations tend to provide faster results, while too small fail to efficiently explore the solution space. For the maximum number of iterations without improving the best solution ( α max ) it was found that the larger the population, the less iterations were required. Thus, the following equation was used: (2) α max = ⌈ a 250 n + 500 m | P | ⌉ . The multiplier a , used in both parameters, depends on the size of the instance, taking on the value 10 for smaller instances ( n < 100 ), 5 for mid-size instances ( 100 ≤ n < 250 ), and 1 for larger instances ( n ≥ 250 ). The reasoning is: the higher the value, the more intensified is the search for good solutions (more individuals and iterations), and more time-consuming it becomes (possibly too much in larger instances). Finally, mutation rates p location and p rou ting were set respectively at 0.02 and 0.06, providing an overall 16% probability of occurring mutations. Note that the tested features as well as the proposed parameters were evaluated using the benchmark instances detailed in the following section. For newer instances, the most important parameter requiring tuning is the size of the population (using similar equations for the remaining). Results reported in this paper concern two variants of the algorithm: HybridGA and HybridGA+. The only difference between them is that in the latter variant a more thorough search is performed by running a 3-opt algorithm in LS routing and doubling α max . 4.2 Benchmark instances For evaluating the algorithm (as well as tested features and parameters) three sets of CLRP benchmark instances were used. These are the sets by Tuzun and Burke [31], Barreto et al. [2] and Prins et al. [24] with, respectively, 36, 19 and 30 instances. In the randomly generated instances by Tuzun and Burke [31] the number of clients is n ∈ {100, 150, 200} and the number m of depots is 10 and 20. Vehicle capacity Q = 150, vehicle fixed cost F = 10 and clients demand follow a uniform distribution in the range [1, 20]. These instances can be found in The benchmark instances proposed by Barreto et al. [2] are available from and were either obtained from the literature or adapted from VRP instances. The number of clients n ranges from 12 to 318, the number of depots m from 2 to 15 and there is no vehicle fixed cost. The last set by Prins et al. [24] was randomly generated (available at with n ∈ {20, 50, 100, 200}, m ∈ {5, 10}, Q ∈ {70, 150} and F = 1000. Euclidean distances are used in all the instances, multiplied by 100 and rounded up to the next integer in the third set. In the randomly generated sets clients are spatially distributed in clusters. 4.3 Results Two variants of the proposed method were tested performing ten runs on each instance, from which average solution values, best solutions and average computing times were obtained. Results for the three benchmark sets can be seen in Tables 1–3, and full solution data is available at The first column displays the name of the instance, followed by the best known lower bound (LB), when available, and currently best known result (BKR). Then, data concerning the two variants of the proposed hybrid GA is shown: average CPU time in seconds, Average and Best Results concerning cost values, and standard deviation (Std dev) of the cost over the ten runs in percentage. Finally, GapBKR concerns the gap to best known result, which is provided in percentage for both Average and Best results. Note that few lower bounds are available for the first set, and therefore only BKR is provided in Table 1. Proven optimal values are italicized and average and median values are provided (as data showed skewed distributions and/or outlying data points for CPU time and Gaps). For HybridGA, in the benchmark set by Tuzun and Burke CPU times are on average lower than 90s and, concerning best obtained solutions, gaps to best known results are on average 0.87% with highest gap at 3.09%. In the HybridGA+ variant average CPU times ascend to 6min and average gaps lower to 0.71%. In both variants median values are similar to average values. In the second set (Barreto et al.), computing times are on average less than 43 and 430s and median values are 17 and 49s (this difference is mostly due to the two outliers: instances 13 and 14). Average GapBKR of best results are for both variants respectively 0.26% and −0.06% and median values are 0% for both. Two new BKR were found, one of them reaching an improvement of over 1% to previous results. Gaps to best known results are very low for all instances pointing to low-quality lower bounds for instances 8, 11, 12 and 16. For the Prins et al. benchmark set CPU times are on average 73 and 199s and median are 23 and 73s, due to the larger instances (instances 25–30). Concerning GapBKR of best found solutions, average and median values are respectively 0.37% and 0.11% for HybridGA, 0.32% and 0.13% for HybridGA+. In this set larger gaps can be found in instances 19, 20, 23, 25, 26 and 29; in these first three instances and instance 24 results suggest corresponding low-quality lower bounds. Overall, among the two variants, the running time of HybridGA+ increases twofold for smaller instances and can go up to ten times more in larger instances. Best results are the same for smaller instances (although the second variant reaches them more consistently) and are increased in larger instances (only marginally in some cases). For all instances containing 50 or less clients, the two variants succeeded to find the optimal solution and an interesting feature is that the method seems faster when larger routes are found. This is noticeable when comparing CPU times of equal sized instances but with different vehicle capacities (e.g., instances with suffix “a” and “b” in the last benchmark set, where instances “b” have more than double the vehicle capacity of instances “a”, being the only difference between them). This behavior is not shared by most other methods in the literature. Characteristics of most recent and effective methods in the literature are presented in Table 4 where, for each algorithm, source of publication, number of runs and CPU times are provided. In Table 5 average results of the proposed approach (both HybridGA and HybridGA+) are compared with results from those other methods. CPU refers to the average computing time in seconds, Avg GapBKR and Best GapBKR to the average gaps between the obtained average or best results and the currently best known results (in percentage). In the case of the proposed variants two values are provided in the second benchmark set for both CPU and gaps for a direct comparison with algorithms only considering the same 13 out of the 19 instances. As CPU times of the remaining algorithms were acquired from the corresponding publications, therefore depending on many factors (such as computers and programming languages used), they are mostly indicative. However, the proposed method seems to be among the fastest (if not the faster for the case of HybridGA). Concerning Best GapBKR, the proposed algorithm is able to outperform all of the remaining in the benchmark set by Barreto et al. considering either the subset of 13 instances or the complete set; only MACO surpasses the faster variant (HybridGA) in the complete set. On the other two benchmark sets average results are also competitive, having the fourth best average gap for the Tuzun and Burke set and the third best for the Prins et al. set. 5 Conclusions In this paper an evolutionary algorithm (hybrid GA) is presented for solving the CLRP. It follows the standard GA framework hybridized with local search procedures in the mutation phase. Two variants of the algorithm are tested on three sets of instances from the literature. A comparative analysis is also performed with other published approaches and results suggest that the method, although relatively simple, is competitive. The algorithm provides the best results and CPU times on the Barreto et al. instances, and is among the best performing methods on the other two benchmark sets having good trade-offs of CPU times and gaps. Moreover, it was able to detect the optimal solution on all instances containing 50 or less customers. It is worth noting the ease of implementation of the proposed algorithm. As advocated by Laporte et al. [17] for the VRP, similarly extendable to the LRP, it is probably time to develop faster and simpler algorithms, even if causing a small loss in solution quality. This work is a step in that direction. The proposed hybrid GA has already been implemented in a decision-support tool available at Future work may include further fine-tuning of parameters or including other local search procedures (such as methods with destroy-and-repair operators) in the mutation phase. Concerning the problem addressed herein, it may be extended to address several time periods, making heuristic developments for the CLRP even more relevant, as such methods may be used within myopic or panoramic frameworks for tackling dynamic LRPs [16,27]. Acknowledgments This work was supported by Portuguese funds through the CIDMA – Center for Research and Development in Mathematics and Applications, and the Portuguese Foundation for Science and Technology (“FCT – Fundação para a Ciência e a Tecnologia”), within project UID/MAT/04106/2013. References [1] R. Baldacci A. Mingozzi R. Wolfler Calvo An exact method for the capacitated location–routing problem Oper Res 59 5 2011 1284 1296 [2] S. Barreto C. Ferreira J. Paixão B.S. Santos Using clustering analysis in a capacitated location–routing problem Eur J Oper Res 179 3 2007 968 977 [3] J.M. Belenguer E. Benavent C. Prins C. Prodhon R. Wolfler Calvo A branch-and-cut method for the capacitated location-routing problem Comput Oper Res 38 6 2011 931 941 [4] I.M. Branco J.D. Coelho The Hamiltonian p-median problem Eur J Oper Res 47 1 1990 86 95 [5] C. Contardo J.-F. Cordeau B. Gendron A computational comparison of flow formulations for the capacitated location–routing problem Discret Optim 10 4 2013 263 295 [6] C. Contardo J.-F. Cordeau B. Gendron A GRASP+ILP-based metaheuristic for the capacitated location–routing problem J Heuristics 20 1 2014 1 38 [7] C. Contardo J.-F. Cordeau B. Gendron An exact algorithm based on cut-and-column generation for the capacitated location–routing problem INFORMS J Comput 26 1 2014 88 102 [8] M. Drexl M. Schneider A survey of variants and extensions of the location–routing problem Eur J Oper Res 241 2 2015 283 308 [9] C. Duhamel P. Lacomme C. Prins C. Prodhon A GRASPxELS approach for the capacitated location–routing problem Comput Oper Res 37 11 2010 1912 1923 [10] J.W. Escobar R. Linfati P. Toth A two-phase hybrid heuristic algorithm for the capacitated location–routing problem Comput Oper Res 40 1 2013 70 79 [11] J.W. Escobar R. Linfati M.G. Baldoquin P. Toth A granular variable tabu neighborhood search for the capacitated location–routing problem Transp Res Part B 67 2014 344 356 [12] V.J. Filho R.D. Galvão A tabu search heuristic for the concentrator location problem Locat Sci 6 1 1998 189 209 [13] P. Hansen N. Mladenović First vs. best improvement: an empirical study Discret Appl Math 154 5 2006 802 817 [14] V.C. Hemmelmayr J.-F. Cordeau T.G. Crainic An adaptive large neighborhood search heuristic for two-echelon vehicle routing problems arising in city logistics Comput Oper Res 39 12 2012 3215 3228 [15] Hosny M, Mumford C. Investigating genetic algorithms for solving the multiple vehicle pickup and delivery problem with time windows. In: Proceedings of the VIII Metaheuristic International Conference (MIC 2009). Hamburg; 2009. [16] G. Laporte P.J. Dejax Dynamic location–routing problems J Oper Res Soc 40 5 1989 471 482 [17] G. Laporte M. Gendreau J.-Y. Potvin F. Semet Classical and modern heuristics for the vehicle routing problem Int Trans Oper Res 7 4–5 2000 285 300 [18] S. Lin B.W. Kernighan An effective heuristic algorithm for the traveling salesman problem Oper Res 21 2 1973 498 516 [19] R.B. Lopes C. Ferreira B.S. Santos S. Barreto A taxonomical analysis, current methods and objectives on location-routing problems Int Trans Oper Res 20 6 2013 795 822 [20] Y. Marinakis M. Marinaki A bilevel genetic algorithm for a real location routing problem Int J Logist Res Appl 11 1 2008 49 65 [21] I.H. Osman Metastrategy simulated annealing and tabu search algorithms for the vehicle routing problem Ann Oper Res 41 4 1993 421 451 [22] C. Prins A simple and effective evolutionary algorithm for the vehicle routing problem Comput Oper Res 31 12 2004 1985 2002 [23] C. Prins C. Prodhon R. Wolfler Calvo A memetic algorithm with population management (MA|PM) for the capacitated location–routing problem Gottlieb Jens Raidl R. Günther Evolutionary computation in combinatorial optimization – lecture notes in computer science vol. 3906 2006 Springer Berlin 183 194 [24] C. Prins C. Prodhon R. Wolfler Calvo Solving the capacitated location–routing problem by a GRASP complemented by a learning process and a path relinking 4OR 4 3 2006 221 238 [25] C. Prins C. Prodhon A. Ruiz P. Soriano R. Wolfler Calvo Solving the capacitated location–routing problem by a cooperative Lagrangean relaxation-granular tabu search heuristic Transp Sci 41 4 2007 470 483 [26] C. Prodhon C. Prins A survey of recent research on location–routing problems Eur J Oper Res 238 1 2014 1 17 [27] S. Salhi G. Nagy Consistency and robustness in location–routing Stud Locat Anal 13 1999 3 19 [28] M. Savelsbergh The vehicle routing problem with time windows: minimizing route duration INFORMS J Comput 4 2 1992 146 154 [29] C.J. Ting C.H. Chen A multiple ant colony optimization algorithm for the capacitated location routing problem Int J Prod Econ 141 1 2013 34 44 [30] P. Toth D. Vigo The granular tabu search and its application to the vehicle–routing problem INFORMS J Comput 15 4 2003 333 346 [31] D. Tuzun L.I. Burke A two-phase tabu search approach to the location routing problem Eur J Oper Res 116 1 1999 87 99 [32] V.F. Yu S.W. Lin W. Lee C.J. Ting A simulated annealing heuristic for the capacitated location routing problem Comput Ind Eng 58 2 2010 288 299 "
    },
    {
        "doc_title": "Hemo@record: The Portuguese system for registration of patients with haemophilia and other congenital coagulopathies",
        "doc_scopus_id": "85047082103",
        "doc_doi": "10.18803/capsi.v16.124-138",
        "doc_eid": "2-s2.0-85047082103",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Management Information Systems",
                "area_abbreviation": "BUSI",
                "area_code": "1404"
            },
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Congenital coagulopathies",
            "Haemophilia",
            "Hemo@record",
            "National registry",
            "WEB application"
        ],
        "doc_abstract": "© 2016 Atas da Conferencia da Associacao Portuguesa de Sistemas de Informacao. All rights reserved.This paper describes a web application designed to support the first national registry of patients with haemophilia and other congenital coagulopathies in Portugal. The importance of national patient registry systems (nPRS), in the scope of the treatment of the chronic and rare diseases such as haemophilia, has been recognized by several institutions at an international level and by national government entities. However, there are several barriers to the creation of such systems, in particular due to the lack of motivation to invest in order to benefit only a small proportion of the population, a characteristic of rare diseases. The conditions for the creation of the first Portuguese nPRS in the field of haemophilia were recently created through a joint project between a group of medical professionals belonging to the Portuguese Association of Coagulopathies Congenital (APCC) and a group of researchers from the University of Aveiro. Currently, the technological solution is already developed, tested, and installed at the data centre of the University of Aveiro. Its operational version is being used by clinicians of different Haemophilia Treatment Centres (HTCs) around the country. The data inserted accounts for about 10% of the Portuguese population with haemophilia and all entities involved are working towards achieving 100% of this population. The data inserted accounts for about 10% of the Portuguese population with haemophilia and all entities involved are working towards achieving 100% of this population.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "LORE, a decision support tool for location, routing and location-routing problems",
        "doc_scopus_id": "84984797969",
        "doc_doi": "10.1007/978-3-319-44896-1_17",
        "doc_eid": "2-s2.0-84984797969",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Data input",
            "Decision support tools",
            "Graphical user interfaces (GUI)",
            "Location routing problem",
            "Location-routing",
            "Solution-finding process"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.LOcation Routing Exploration (LORE) is a decision support tool for addressing location, routing and location-routing problems. In this paper the LORE tool will be presented, and its main characteristics addressed. Among the main features of the tool is the ability to support a variety of problems currently being studied in the location and routing literature (due to the proposed data structure), and the graphical user interface (GUI). The data structure will be presented being provided an explanation on how it can support related problems. The GUI main goal is not only to aid the solution-finding process but also to foster greater insight into the problem(s) at hand. To that extent, the GUI, developed to fit the target user’s profile and intended tasks, is presented, namely data input and visualization features.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive, multi-device visualization supported by a multimodal interaction framework: Proof of concept",
        "doc_scopus_id": "84978914860",
        "doc_doi": "10.1007/978-3-319-39943-0_27",
        "doc_eid": "2-s2.0-84978914860",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Device characteristics",
            "First impressions",
            "Future improvements",
            "Interactive visualizations",
            "Multi-devices",
            "Multi-Modal Interactions",
            "Multiple representation",
            "User satisfaction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.Nowadays, users can interact with a system using a wide variety of modalities, such as touch and speech. Nevertheless, multimodal interaction has yet to be explored for interactive visualization scenarios. Furthermore, users have access to a wide variety of devices (e.g., smartphones, tablets) that could be harnessed to provide a more versatile visualization experience, whether by providing complementary views or by enabling multiple users to jointly explore the visualization using their devices. In our effort to gather multimodal interaction and multi-device support for visualization, this paper describes our first approach to an interactive multi-device system, based on the multimodal interaction architecture proposed by the W3C, enabling interactive visualization using different devices and representations. It allows users to run the application in different types of devices, e.g., tablets or smartphones, and the visualizations can be adapted to multiple screen sizes, by selecting different representations, with different levels of detail, depending on the device characteristics. Groups of users can rely on their personal devices to synchronously visualize and interact with the same data, maintaining the ability to use a custom representation according to their personal needs. A preliminary evaluation was performed, mostly to collect users’ first impressions and guide future developments. Although the results show a moderate user satisfaction, somehow expected at this early stage of development, user feedback allowed the identification of important routes for future improvement, particularly regarding a more versatile navigation along the data and the definition of composite visualizations (e.g., by gathering multiple representations on the same screen).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gesture interactions for virtual immersive environments: Navigation, selection and manipulation",
        "doc_scopus_id": "84978891255",
        "doc_doi": "10.1007/978-3-319-39907-2_20",
        "doc_eid": "2-s2.0-84978891255",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3duis",
            "Gestural interaction",
            "Gesture interaction",
            "Gesture-based interaction",
            "Immersive environment",
            "Kinect",
            "Learning difficulties",
            "User study"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.This paper presents an extension to a Platform for Setting-Up Virtual environments with the purpose of allowing gesture interaction. The proposed solution maintains the flexibility of the original framework as well as content association (PDF, Video, Text), but allows new interactions based on gestures. An important feature is the one to one navigational input based on Kinect skeleton tracking. The framework was used to configure a virtual museum art installation using a real museum room where the user can move freely and interact with virtual contents by adding and manipulating 3D models. Two user studies were performed to compare gestures against button-controlled interactions for navigation and 3D manipulation. Most users preferred the Kinect-based navigation and gesture-based interaction despite some learning difficulties and tracking problems. Regarding manipulation, the gesture-based method was significantly faster with similar accuracy when compared to the controller. On the other hand, when dealing with rotations, the controller-based method was faster.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Living globe: Tridimensional interactive visualization of world demographic data",
        "doc_scopus_id": "84978891214",
        "doc_doi": "10.1007/978-3-319-40349-6_2",
        "doc_eid": "2-s2.0-84978891214",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D globe",
            "Demographic data",
            "Heuristic evaluation",
            "Information visualization",
            "Usability evaluation",
            "WebGL Globe"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.This paper presents Living Globe, an application for visualization of demographic data supporting the temporal comparison of data from several countries represented on a 3D globe. Living Globe allows the visual exploration of the following demographic data: total population, population density and growth, crude birth and death rates, life expectancy, net migration and population percentage of different age groups. While offering unexperienced users a default mapping of these data variables into visual variables, Living Globe allows more advanced users to select the mapping, increasing its flexibility. The main aspects of the Living Globe model and prototype are described as well as the evaluation results obtained using heuristic evaluation and usability testing. Some conclusions and ideas for future work are also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating human factors in information systems development: User centred and agile development approaches",
        "doc_scopus_id": "84978877756",
        "doc_doi": "10.1007/978-3-319-40247-5_35",
        "doc_eid": "2-s2.0-84978877756",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Agile software development",
            "Clinical information",
            "Complex environments",
            "Information system development",
            "Information systems development",
            "Interactive software",
            "Software development process",
            "Web-based applications"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.This paper presents an overview and discussion based on the literature review of recent research of some practices that incorporate human factors, emphasizing the user-centred design (UCD) and agile software development (ASD) approaches. Additionally, this article presents an experience of the development of a web-based application that aims to manage the clinical information in haemophilia care, which benefited from these practices, making use of some methods to support the collaboration and communication between designers, users, and developers. The results of our experience show that the hybrid approach, that combines the principles of UCD with values of ASD can help to integrate human factors into the software development process in a highly complex environment, characterized by missing information, shifting goals and a great deal of uncertainty, such as the healthcare field.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using Heuristic Evaluation to Foster Visualization Analysis and Design Skills",
        "doc_scopus_id": "84962627039",
        "doc_doi": "10.1109/MCG.2016.7",
        "doc_eid": "2-s2.0-84962627039",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Computer graphics education",
            "Design visualization",
            "Heuristic evaluation",
            "Information visualization",
            "Systematic analysis method",
            "Usability evaluation methods",
            "Visualization analysis",
            "Visualization application"
        ],
        "doc_abstract": "© 1981-2012 IEEE.In an effort to develop visualization analysis and design skills in master's level information visualization students, the authors use a well-known analytical usability evaluation method, heuristic evaluation, with different sets of heuristics to teach students to analyze visualization applications. The proposed approach, used for three consecutive years, has successfully stimulated critical analysis and discussion sessions as well as helped raise students' awareness concerning the benefit of using systematic analysis methods and the strategies and guidelines that should be used to design visualization applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "CraMs: Craniometric analysis application using 3d skull models",
        "doc_scopus_id": "84961700580",
        "doc_doi": "10.1109/MCG.2015.136",
        "doc_eid": "2-s2.0-84961700580",
        "doc_date": "2015-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "3D models",
            "Bone damages",
            "Initial guess",
            "Interobserver variability",
            "Point detection",
            "Points of interest",
            "Skull models",
            "Anatomic Landmarks",
            "Cephalometry",
            "Computer Graphics",
            "Humans",
            "Image Interpretation, Computer-Assisted",
            "Imaging, Three-Dimensional",
            "Models, Anatomic",
            "Reproducibility of Results",
            "Sensitivity and Specificity",
            "Skull",
            "Software"
        ],
        "doc_abstract": "© 1981-2012 IEEE.Craniometric analysis plays an important role in anthropology studies and forensics. This paper presents CraMs, an application using a new craniometric approach based on 3D models of the skull. The main objective is to obtain, through a process supervised by anthropologists, the main points of interest used to compute craniometric measurements. The application aids this process by analyzing the skull geometry and automatically providing points of interest. The application also allows for semiautomatic point detection, where the user provides an initial guess that might be refined based on the curvature of the skull, as well as the manual selection of any other points of interest. Moreover, results comparing measurements obtained with CraMs and traditional craniometry methods on eight skulls suggest that the application provides comparable craniometric measurements and lower inter-observer variability. This approach offers advantages such as an easier access to skulls with no risk of bone damage and the possibility of defining new measurements based on morphology or other skull characteristics, which are not possible using traditional methods.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "In vitro Potentiation of the Aminoglycoside Antibiotic Activity by Croton campestris A. Against Multiresistant Bacteria",
        "doc_scopus_id": "85063984678",
        "doc_doi": "10.1080/22311866.2015.1114427",
        "doc_eid": "2-s2.0-85063984678",
        "doc_date": "2015-09-03",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Toxicology",
                "area_abbreviation": "PHAR",
                "area_code": "3005"
            },
            {
                "area_name": "Agricultural and Biological Sciences (miscellaneous)",
                "area_abbreviation": "AGRI",
                "area_code": "1101"
            },
            {
                "area_name": "Pharmacology",
                "area_abbreviation": "PHAR",
                "area_code": "3004"
            },
            {
                "area_name": "Plant Science",
                "area_abbreviation": "AGRI",
                "area_code": "1110"
            },
            {
                "area_name": "Drug Discovery",
                "area_abbreviation": "PHAR",
                "area_code": "3002"
            },
            {
                "area_name": "Complementary and Alternative Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2707"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2015, © 2015 Har Krishan Bhalla & Sons.Croton campestris A., popularly known as “Velame do campo” (Euphorbiaceae), is a shrub originating from Brazil, mainly present in the Southeast and Northeast regions. The objective of this study was to evaluate the antibacterial activity of the ethanol extract (EECC), hexane (HFEECC) and dichloromethane fractions (DFEECC), obtained from the ethanol extract of Croton campestris A. leaves. Antibacterial and modulating activity (on bacterial resistance) was determined by micro dilution method to identify the MIC (Minimum Inhibitory Concentration). In the antibacterial activity tests the fractions showed a MIC of ≥ 1024 µg/mL (where ≤ 128 µg/mL for MIC is considered clinically relevant). In regards to modulation of bacterial resistance, all products showed synergism when combined with antibiotic against bacterial strains. It was observed that all products potentiated the antibiotic action of all antibiotics assayed against all bacterial strains. The results indicate that the extracts and fractions obtained from Croton campestris leaves could represent an alternative source of natural products capable of modifying and interfering with bacterial resistance to aminoglycosides.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "National Patient Registry with Hemophilia and other Congenital Coagulopathies: The Portuguese system",
        "doc_scopus_id": "84943339580",
        "doc_doi": "10.1109/CISTI.2015.7170480",
        "doc_eid": "2-s2.0-84943339580",
        "doc_date": "2015-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "congenital coagulopathies",
            "hemophilia",
            "national registry",
            "Portugal",
            "WEB application"
        ],
        "doc_abstract": "© 2015 AISTI.This work presents the recently developed Portuguese Patient Registry with Hemophilia and other Congenital Coagulopathies. It is a web-based application that aims to collect a set demographic, social and clinical data (categorized by diagnostic, treatment and follow-up data) of people with hemophilia and other congenital coagulopathies (H&oCC) in a national universe. This project was developed in collaboration with a group of clinicians belonging to the Portuguese Association of Congenital Coagulopathies (APCC) and the technological solution is currently installed in the data center of the University of Aveiro, to be used by clinicians of different Hemophilia Treatment Centers (HTC) located across the country.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A virtual and augmented reality course based on inexpensive interaction devices and displays",
        "doc_scopus_id": "85063434058",
        "doc_doi": "10.2312/eged.20151022",
        "doc_eid": "2-s2.0-85063434058",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Computer science and engineerings",
            "Course organization",
            "Graduate level course",
            "Interaction devices",
            "Practical projects",
            "Technical skills",
            "Techniques and tools",
            "Virtual and augmented reality"
        ],
        "doc_abstract": "© The Eurographics Association 2015.In the last years a plethora of affordable displays, sensors, and interaction devices has reached the market, fostering the application of Virtual and Augmented Reality to many new situations. Yet, creating such applications requires a good understanding of the field and specific technical skills typically not provided by current Computer Science and Engineering education. This paper presents a graduate level course offered to MSc. Programs in Computer and Electrical Engineering which introduces the main concepts, techniques and tools in Virtual and Augmented Reality. The aim is to provide students with enough background to understand, design, implement and test such applications. The course organization, the main issues addressed and bibliography, the sensors, interaction devices and displays used, and a sample of the practical projects are briefly described. Major issues are discussed and conclusions are drawn.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Web platform to support the Portuguese national registry of haemophilia and other inherited blood disorders",
        "doc_scopus_id": "84973569046",
        "doc_doi": "10.4018/IJWP.2015010104",
        "doc_eid": "2-s2.0-84973569046",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Haemophilia",
            "Inherited Blood Disorders",
            "National Patients Registry",
            "Portugal",
            "Rare disease",
            "WEB application"
        ],
        "doc_abstract": "Copyright © 2015, IGI Global.Patient registries are essential tools for identifying and tracking people with a particular disease and for collecting epidemiological information, having a special role in rare and chronic diseases, where haemophilia and other inherited blood disorders (HoIBD) are classified. Web-based technologies represent an excellent solution to support different types of registries, due to the benefits that they can promote in the management of disease data. This work presents the web platform developed in a joint initiative between the Portuguese Association of Congenital Coagulopathies (PACC) and the University of Aveiro (UA), with the purpose of creating the first National Patients Registry (NPR) with HoIBD in Portugal. This application is hosted in the data centre of the UA, and at this moment it is already used by clinicians of the different Haemophilia Treatment Centres (HTC) located in Portugal, with the next challenge being the increase in the number of users.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Portuguese National Registry for Hemophilia: Developing of a Web-based Technological Solution",
        "doc_scopus_id": "84962809050",
        "doc_doi": "10.1016/j.procs.2015.08.507",
        "doc_eid": "2-s2.0-84962809050",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder",
            "Coagulation factor",
            "Congenital Coagulopathies",
            "Hemophilia",
            "National Pacients Registry",
            "Technological platform",
            "Technological solution",
            "WEB application"
        ],
        "doc_abstract": "© 2015 The Authors. Published by Elsevier B.V.The crucial role that patient records have in the management of the rare and chronic diseases greatly increases the need to create mechanisms to facilitate the identification and management of the patient's data. Hemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor that affects the population on a ratio of 1 case for 10,000 people born. Currently, there are several countries with technological platforms to support the National Patients' Registries (NPR) of Hemophilia and other Congenital Coagulopathies (HoCC), due to its benefits in the management of the disease. This work presents the technological platform developed in a joint initiative between the University of Aveiro (UA) and the Portuguese Association of Congenital Coagulopathies (PACC), with the purpose of creating the first NPR with HoCC in Portugal. This web application is hosted in the data center of the University of Aveiro, and is being used by the clinicians of the different Hemophilia Treatment Centers (HTC) across the country.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2015-09-15 2015-09-15 2015-09-15 2015-09-15 2016-03-16T16:55:20 S1877-0509(15)02642-3 S1877050915026423 10.1016/j.procs.2015.08.507 S300 S300.5 HEAD-AND-TAIL 2016-03-16T13:28:00.410768-04:00 0 0 20150101 20151231 2015 2015-09-15T04:43:01.025327Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 true 64 64 C Volume 64 159 1248 1255 1248 1255 2015 2015 2015-01-01 2015-12-31 2015 Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015 Maria Manuela Cruz-Cunha João Varajão Rui Rijo Ricardo Martinho Petra Schubert Albert Boonstra Ricardo Correia Alexander Berler article fla Copyright © 2015 The Authors. Published by Elsevier B.V. PORTUGUESENATIONALREGISTRYFORHEMOPHILIADEVELOPINGAWEBBASEDTECHNOLOGICALSOLUTION TEIXEIRA L SRIVASTAVA 2013 e1 e47 A VIVIANI 2014 81 L LARA 2011 389 396 B AZNARJA 2009 1327 1330 ZDZIARSKA 2011 e189 e195 J HAY 2004 21 25 C HESSE 2013 S15 S21 J WALKER 1995 548 551 I TEIXEIRAX2015X1248 TEIXEIRAX2015X1248X1255 TEIXEIRAX2015X1248XL TEIXEIRAX2015X1248X1255XL Full 2015-08-21T00:47:13Z ElsevierWaived OA-Window item S1877-0509(15)02642-3 S1877050915026423 10.1016/j.procs.2015.08.507 280203 2016-03-16T13:28:00.410768-04:00 2015-01-01 2015-12-31 true 829119 MAIN 8 51617 849 656 IMAGE-WEB-PDF 1 Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 1877-0509 Â© 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of SciKA - Association for Promotion and Dissemination of Scientific Knowledge doi: 10.1016/j.procs.2015.08.507 ScienceDirect Available online at www.sciencedirect.com Conference on ENTERprise Information Systems / International Conference on Project MANagement / Conference on Health and Social Care Information Systems and Technologies, CENTERIS / ProjMAN / HCist 2015 October 7-9, 2015 The Portuguese National Registry for Hemophilia: Developing of a web-based technological solution Leonor Teixeira a,c,*, Vasco Saavedra a , JoÃ£o Pedro SimÃµes d, Beatriz Sousa Santos b,c, Carlos Ferreira a,c aDepartment of Economics, Management and Industrial Engineering, University of Aveiro, 3810-193 Aveiro, Portugal bDepartment of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal cIEETA - Institute of Telematics and Electronic Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal dTruphone, Lisbon, 2740-120 Oeiras, Portugal Abstract The crucial role that patient records have in the management of the rare and chronic diseases greatly increases the need to create mechanisms to facilitate the identification and management of the patientâ€™s data. Hemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor that affects the population on a ratio of 1 case for 10,000 people born. Currently, there are several countries with technological platforms to support the National Patientsâ€™ Registries (NPR) of Hemophilia and other Congenital Coagulopathies (HoCC), due to its benefits in the management of the disease. This work presents the technological platform developed in a joint initiative between the University of Aveiro (UA) and the Portuguese Association of Congenital Coagulopathies (PACC), with the purpose of creating the first NPR with HoCC in Portugal. This web application is hosted in the data center of the University of Aveiro, and is being used by the clinicians of the different Hemophilia Treatment Centers (HTC) across the country. Â© 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of SciKA - Association for Promotion and Dissemination of Scientific Knowledge. Keywords: National Pacients Registry, Web-application, Hemophilia, Congenital Coagulopathies *Corresponding author. Tel.: +351-234-370361; fax: +351-234-370215. E-mail address: lteixeira@ua.pt Â© 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of SciKA - Association for Promotion and Dissemination of Scientific Knowledge 1249 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 1. Introduction According to the report of the World Federation of Hemophilia (WFH), an important measure for improving the management of Hemophilia and other Congenital Coagulopathies (HoCC) is the existence of patientsâ€™ registries, which, in a national context, could lead to the National Patientsâ€™ Registries (NPR) with HoCC [1]. The vision of the WFH with 'Treatment for All' is that all people that suffer with HoCC must have access to medical care and appropriate treatments. For this, the identification of all patients who suffer with this disease represents the first step, being the NPR the fundamental tool for such identification [2]. Registries of patients have the invaluable potential to provide an understanding of the disorder, to provide useful information for planning health care services and to identify suitable groups of patients for clinical trials enrolment [3]. This kind of registries are particularly useful for people suffering with rare and chronic diseases, such as HoCC, where important research questions cannot be answered without a set of information about the prevalence and evolution of the disease. Registries for rare diseases are broadly accepted for their usefulness in monitoring the identification and diagnosis of people with hemophilia (PWH) and evaluating their health. The purpose of a Registry in the context of HoCC is to define the population demographics and collect observational data on specific hemophilia health concerns such as the prevalence of viral infections, factor inhibitors, implementation of prophylaxis, and track the usage of the products (drugs). In addition to the demographic characterization of the population with hemophilia, the existence of those records allows the collection of data for statistical analysis related to the specificity of the disease, such as the prevalence of viral infections, existence of inhibitor factors, implementation of prophylactic treatments, joint evaluation, among other mechanisms to assist the decision making process. Given the importance of these data for disease management, the consistent definition of this data, its correct storage and possibility of subsequent extraction of information are important factors for a more objective view of the practice of hemophilia care, with a profound impact on the health and in the quality of life of these patients. The lack of a National Patients Registry for Hemophilia and other Congenital Coagulopatias in Portugal, associated with the difficulty that clinicians of this area faced in order to manage the specific patient information, motivated a group of physicians to seek for a technological solution that facilitated and optimized the information management process. This article presents the newly developed web-based solution to support the Portuguese National Hemophilia Registry, whose project arose from a joint initiative between the hemophilia healthcare professionals, represented by the Portuguese Association of Congenital Coagulopathies (PACC) and a group of researchers from the University of Aveiro (UA) responsible for analyzing, developing and implementing the technological solution. 2. Registry and its role in chronic and rare diseases Registry is defined in epidemiology as a data file with information efficiently recoverable, related with elements that are important for health in a defined population in such a way that the registered elements can be extrapolated to a base population [4]. In the scope of congenital bleeding disorders, the registry is a database of people identified with this disease including information on personal details, diagnosis, treatment and complications [1], [5]. The registry is thus a key tool for identifying and tracking individuals with a particular disease, and consequently for quality control and quality assurance in treatment, and for studying the impact of new developments on prevention, diagnosis and treatments [6]. These registries contain demographical, social and clinical data, categorized in diagnose, treatment and follow-up data. The WFH recommends that this data be stored in a centralized unique national repository, as, among other benefits, avoids patient duplication, ensuring a greater reliability of registries in order to portray a faithful reality of the disease in a country [1]. Given the usefulness of these systems, there are several countries that have already implemented NPR in the context of Hemophilia and other Congenital Coagulopathies. Considering the results the results of a study published by Oâ€™Mahony et al. in 2012 [7], that was based in an inquiry made in 35 European countries, there are 27 countries with NPR and 8 without a NPR, a group where Portugal was included. In the countries with a NPR, not all have the information openly available, and some of them are reported in the literature, as the NPR from Austria [6], Italy [6], Spain [8], Poland [9], Switzerland [10], United Kingdom [11], 1250 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 Germany [12]. Other NPR are referred in the work of other authors as is the cases of NPR from Bulgaria, Czech Republic, Slovenia, Finland, Greece, Russia, and Slovakia [7]. Outside European continent, there are other reference NPR in this area, as is the case as the NPR from Canada [13]. 3. The Web-application to support the Portuguese National Registry for Hemophilia This work aims to present the web-application to support a National Registry of Hemophilia and other Congenital Coagulopathies in Portugal. The analysis, conception and development of the platform that would support NPR with HoCC in Portugal was a joint initiative between the Portuguese Association of Congenital Coagulopathies (PACC) and a group of researchers from the University of Aveiro (UA). The PACC is a non-profit organization represented by a group of health care providers in the area of HoCC, and the entity responsible for defining the rules, guidelines and setting goals for the NPR. The University of Aveiro was the part responsible for the technology leadership, more specifically for the analysis, conception, implementation and maintenance and support of the NPR of HoCC platform. 3.1. Disease characterization and problem contextualization Hemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor [2]. According to the WFHâ€™s annual global surveys, the number of PWH in the world is approximately 400 000, affecting the population in a ratio of 1 case for 10,000 people born [5]. Portugal is a country with 10 million inhabitants and about 1000 PWH are expected. However, the number of the other cases with Congenital Coagulopathies, including the vonWillebrand disease (vWB) and other type of deficiencies of factors, does increase this value. To treat these patients, there are different treatment centers scattered through Portugal, and despite the efforts and recognition of the need of a NPR, just recently the protocol for the creation of the system to support the Registry was approved. For this project, the establishment of an expert working group was endorsed, involving a group of physicians and a group of researchers in the field of computer science, aiming to develop the first Web-based platform to support the NPR for HoCC in Portugal. 3.2. Methodology of Development The development process of the web-platform (named hemo@record) followed an iterative and incremental process, which allowed the requirement elicitation and its validation with a group of clinicians, future users of the system [14]. Information collected in this phase was incorporated into a detailed specifications document to guide development, using an object-oriented (OO) environment. After this phase, a detailed description of the registry was submitted to the National Committee for Data Protection in March 2013 and consent for the registry was obtained two months later. The guidelines of the National Data Protection Agency on confidentiality were carefully followed along the development process of the technological solution and the web-based application that supports the NPR for HoCC in Portugal is operated and maintained in accordance with the Data Protection Law. The data will be collected in the application after consent of the patient. 3.3. Brief Description of the technological solution developed The hemo@record was developed to store and manage a set of anonymous data from patients with HoCC, and to be used by clinicians that treat people with this disease scattered over the country. There are two different profiles of users (actors) with access credentials to use the system, namely: Âƒ Physician - it includes the doctors that treat PWH and have permission to perform a set of tasks, including introduction, update, deletion of their patient's data, as well as, query and visualize information (with some critical data hidden), from all the patients at national level; Âƒ Coordinator â€“ it is also a doctor with administration permissions, having access to all the functionalities of the system. 1251 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 Each patient has his/her personal data hidden and is identified by an anonymous code, having associated a set of records that reflects his/her clinical history in terms of health through time. To provide an efficient set of functionalities to the users, the system is organized through a set of 8 modules, managing demographic, clinical and treatment data (see Fig.1). Fig.1 - Snapshot of modules present in hemo@care (user interfaces in Portuguese). Âƒ Two of those modules are statistical modules, presenting a set of statistical indicators, at national and local level; Âƒ Six modules providing data management functionalities (insert, update, delete, export): Modules for administration and audit purposes (available to the coordinator only), and modules for patient management, registry management, treatments management and death management. Modules to visualize national and local statistics The national statistics module was designed to present a set of national indicators, reflecting the current situation, encompassing the data from all the patients introduced in the system by their doctors. These indicators provide a complete analysis of the national situation, namely: (i) distribution of patients per area of residence; (ii) distribution of patients per Hemophilia Treatment Center (HTC) or Hospital; (iii) distribution of patients per type of coagulopathy; (iv) distribution of patients per type of treatment; (v) distribution of patients per severity of disease; (vi) distribution of patients per treatment regimen; (vii) evolution in the number of occurrences; and, (viii) evolution in the number of deaths. The local statistics module presents the same indicators, with the exception of those that identify the geographic location of the patient, such as: â€˜distribution of patients by place of residenceâ€™ and â€˜distribution of patients per HTC or Hospital (see Fig.2) Fig.2 - Snapshot of user interface with indicators of local statistics module (interfaces in Portuguese). 1252 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 In a nutshell, the difference between these two modules is that the former present all the national data, while the later only presents the local data associated with the user HTC. Each statistics module is presented in a specific chart (pie chart, bar chart, etc.) that is appropriate to the type of data that is provided by the module (see examples in Fig.3). Distribution of patients by type of coagulopathy Distribution of patients by HTC/ Hospital Evolution in the number of occurrences Fig.3 - Snapshots with examples of some indicators (in Portuguese with fictitious data). Modules to manage patientsâ€™ data The remaining six modules provide the information to manage all patient data. The first step to use the web-application is the introduction of the patient basic data, i.e., the anonymous and unique patient code, complemented with a set of demographical data as gender, HTC or Hospital, data of birth, place and country of birth, and place and country of residence, (Figure 4 - left side). The patient is introduced only once, the system having an automatic process that identifies possible duplications, triggering automatic mechanisms for alerting the Coordinator. After registering the patient, it is possible to introduce registries for that patient. Each registry stores the patient's health condition at a specific instant of time, more specifically the clinical condition or/and the social and life quality information. To be able to assess the evaluation through time, each time a change on a given situation or medical condition occurs; the system automatically creates a new record. As such, each patient can have several registries, and through the analysis off all the records, is possible to determine the evolution of the patient's health condition, social and quality of life evolution. The system presents patients listings in a proprietary grid view, using a color representation in order to promptly identify different situations, like patients without associated registries, patients that belong to the same HTC as the physician, patients already diseased, etc. (Figure 4 - right side). Fig.4 - Snapshot of some aspects of the user interface (in Portuguese with fictitious data). 1253 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 Due to the laws of privacy and data protection, the identifying data of patients from other HTC's (anonymous code and hospital name), are hidden. As can be seen in Fig.4 (right side), there are several functionalities that can be performed after selecting a patient, such as: create or update patient's registries, create or update patient treatment details (treatment date, drug name and consumed dosage). To facilitate the statistical analysis and detect trends, the data can be exported to a Microsoft Excel format for posterior analysis. All the grids provide powerful filter and sorting mechanisms, having each field a specific filter system adapted to the type of data hosted in that field (for example, a date field allows to filter dates within a chosen interval, etc.). The system also provides a complete solution to manage the details of the patient treatments, according to the rules imposed by the WFH such as: create or update data about Coagulation Factor Concentrate (CFC) products (drugs used in Hemophilia treatments), create or update suppliers, create or update patient consumption details, etc. Fig.5 (left side) presents the example of interface that allows to record a patient's treatment details, while Fig.5 (right side) shows all the treatments presented on the system proprietary grid, providing the same powerful filtering and sorting mechanisms. Fig.5 - Snapshot of some aspects of the user nterfaces (in Portuguese with fictitious data). 3.4 . Technological Solution From a technological standpoint, the hemo@record web-application was developed using the Spring development Framework, a Java platform that provides comprehensive infrastructure support for developing Enterprise Java applications [15]. The advantage of using the Spring framework is that it handles the infrastructure support details, allowing developers to focus on the business logic of the system. The Web frontend was developed using Sencha GXT, a Java-based framework for building feature-rich web-applications based on GWT. â€œGXT features high- performance user interfaces (UI) widgets that are interoperable with native GWT components, templates, and layout managerâ€� [16]. GWT is a development toolkit for building and optimizing complex browser-based applications. Its goal is to enable productive development of high-performance web- applications without the need to be an expert on web technologies such as HTML, JavaScript, CSS, etc. Using GWT, the cross-browser inconsistency is eliminated, as GWT compiles a specific version optimized for a specific browser. The Data Base Management System (DBMS) used was Oracle MySQL. In order to avoid the complexities of the underlying Data Base Management System (DBMS), and, if needed, to easily swap the DBMS used, an Object/Relational Mapping (ORM) framework was used. Hibernate is an ORM framework that allows the use of natural Object-oriented idioms including inheritance, polymorphism, association, composition, and the Java collections framework to manipulate database data [17]. The hemo@record runs on an Apache Tomcat Web server, an open source software implementation of the Java Servlet and JavaServer Pages technologies. In a nutshell, each use-case of the web-application generates a request that is processed by the server that runs in specific business logic, retrieves the requested data from the hemo@record database, transforming the data in Java objects by the use of the ORM framework. These objects transport the data extracted from the database, and are returned to the web-application. The browser that renders the web-application 1254 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 interprets the data, using the compiled GXT widgets to present it as useful and relevant information for the end user. 4. Conclusions The use of Information and Communication Technologies (ICT) in rare and chronic disease have shown numerous advantages in terms of data management and information retrieval, with great impact on the quality of health care provision, research and public health monitoring trends. In the context of hemophilia, several countries had already taken advantage of ICT, implementing technological solutions to support the National Registry of Patients. Portugal is a small country with about 10 million inhabitants and 1000 expected PWH. Despite the fact that the national patient registry is a desire project from several years, the protocol for the creation of an online national registry was only recently signed, involving a group of healthcare professionals, represented by the PACC and a group of researchers from the University of Aveiro. This paper described a web platform that supports the National Patientsâ€™ Registries (NPR) of Haemophilia and other Congenital Coagulopathies (HoCC) in Portugal. This recently created platform (hemo@record) allows a complete solution for managing the complexity of data associated to Haemophilia and Congenital Coagulopathies patients, including clinical, social and health status monitoring data, thus reflecting in real time the national reality in terms of HoCC data. As such, the hemo@record can give answer in real time to questions such as: 'How many?', 'Where are they?' and 'How are they?â€™ assuring a proper and informed treatment, provided by clinicians of the several HTC from all over the country. Acknowledgments We would also like to acknowledge the valuable contribution of the clinical professionals of Portuguese Association of Congenital Coagulopathies (PACC) for their contribution towards this project. This work is funded by National Funds through FCT - Foundation for Science and Technology, in the context of the project PEst-OE/EEI/UI0127/2014. References 1. Evatt B: World Federation of Hemophilia Guide to Developing a National Patient Registry. MontrÃ©al - QuÃ©bec, 2005. 2. Srivastava A, Brewer AK, Mauser-Bunschoten EP, Key NS, Kitchen S, Llinas A, et al.: Guidelines for the management of hemophilia. Haemophilia 2013; 19:e1â€“47. 3. Viviani L, Zolin A, Mehta A, Olesen HV: The European Cystic Fibrosis Society Patient Registry: valuable lessons learned on how to sustain a disease registry. Orphanet J Rare Dis 2014 ; 9:81. 4. Lara B, Morales P, Blanco I, Vendrell M, de Gracia RoldÃ¡n J, Monreal M, et al.: Respiratory Disease Registries in Spain: Fundamentals and Organization [Internet]. Arch Bronconeumol (English Version) 2011;47:389â€“396. 5. World Federation of Hemophilia: World Federation of Hemophilia Report on the ANNUAL GLOBAL SURVEY 2011. MontrÃ©al - QuÃ©bec, 2012. 6. Reitter S, Streif W, Schabetsberger T, Wozak F, Hartl H, Male C, et al.: Austrian Hemophilia Registry: design, development and set of variables. Wien Klin Wochenschr 2009; 121:196â€“201. 7. Oâ€™Mahony B, Noone D, Giangrande PLF, Prihodova L: Haemophilia care in Europe - a survey of 35 countries. Haemophilia 201; 19:e239â€“ 47. 8. Aznar Ja, Abad-Franch L, Cortina VR, Marco P: The national registry of haemophilia A and B in Spain: results from a census of patients. Haemophilia 2009; 15:1327â€“30. 9. Zdziarska J, Chojnowski K, Klukowska a, Ã ÄŠtowska M, Mital a, MusiaÃ¡ J, et al.: Registry of inherited bleeding disorders in Poland--current status and potential role of the HemoRec database. Haemophilia 2011; 17:e189â€“95. 10. Von der Weid N: Haemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010â€“2011 [Internet]. . Hamostaseologie 2012; 32:S20â€“S24. 11. Hay C: The UK Haemophilia database: a tool for research, audit and healthcare planning. Haemophilia 2004; 10:21â€“25. 12. Hesse J, Haschberger B, Heiden M, Seitz R, Schramm W: Neue Daten aus dem Deutschen HÃ¤mophilieregister [Internet]. . Hamostaseologie 2013; 33:S15â€“S21. 13. Walker I, Pai M, Akabutu J, Ritchie B, Growe G, Poon MC, et al.: The Canadian Hemophilia Registry as the basis for a national system for monitoring the use of factor concentrates. Transfusion 1995; 35:548â€“51. 1255 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 14. Teixeira L, Saavedra V, Ferreira C, SimÃµes J, Sousa Santos B: Requirements engineering using mockups and prototyping tools: developing an healthcare web-application; in Yamamoto S (ed): HCI International. Creta, Springer International Publishing Switzerland, 2014, pp 652â€“ 663. 15. Rod Johnson, Juergen Hoeller et al.: Spring Framework Reference Documentation [Internet] 2014 [cited 2015 Apr 1];Available from: 16. Sencha GXT: Create feature-rich HTML5 applications using Java [Internet] 2015 [cited 2015 Apr 1];Available from: 17. Hibernate: Hibernate ORM [Internet] [cited 2015 Apr 10];Available from: pathies (HoCC) in Portugal. This recently created platform (hemo@record) allows a complete solution for managing the complexity of data associated to Haemophilia and Congenital Coagulopathies patients, including clinical, social and health status monitoring data, thus reflecting in real time the national reality in terms of HoCC data. As such, the hemo@record can give answer in real time to questions such as: 'How many?', 'Where are they?' and 'How are they?â€™ assuring a proper and informed treatment, provided by clinicians of the several HTC from all over the country. Acknowledgments We would also like to acknowledge the valuable contribution of the clinical professionals of Portuguese Association of Congenital Coagulopathies (PACC) for their contribution towards this project. This work is funded by National Funds through FCT - Foundation for Science and Technology, in the context of the project PEst-OE/EEI/UI0127/2014. References 1. Evatt B: World Federation of Hemophilia Guide to Developing a National Patient Registry. MontrÃ©al - QuÃ©bec, 2005. 2. Srivastava A, Brewer AK, Mauser-Bunschoten EP, Key NS, Kitchen S, Llinas A, et al.: Guidelines for the management of hemophilia. Haemophilia 2013; 19:e1â€“47. 3. Viviani L, Zolin A, Mehta A, Olesen HV: The European Cystic Fibrosis Society Patient Registry: valuable lessons learned on how to sustain a disease registry. Orphanet J Rare Dis 2014 ; 9:81. 4. Lara B, Morales P, Blanco I, Vendrell M, de Gracia RoldÃ¡n J, Monreal M, et al.: Respiratory Disease Registries in Spain: Fundamentals and Organization [Internet]. Arch Bronconeumol (English Version) 2011;47:389â€“396. 5. World Federation of Hemophilia: World Federation of Hemophilia Report on the ANNUAL GLOBAL SURVEY 2011. MontrÃ©al - QuÃ©bec, 2012. 6. Reitter S, Streif W, Schabetsberger T, Wozak F, Hartl H, Male C, et al.: Austrian Hemophilia Registry: design, development and set of variables. Wien Klin Wochenschr 2009; 121:196â€“201. 7. Oâ€™Mahony B, Noone D, Giangrande PLF, Prihodova L: Haemophilia care in Europe - a survey of 35 countries. Haemophilia 201; 19:e239â€“ 47. 8. Aznar Ja, Abad-Franch L, Cortina VR, Marco P: The national registry of haemophilia A and B in Spain: results from a census of patients. Haemophilia 2009; 15:1327â€“30. 9. Zdziarska J, Chojnowski K, Klukowska a, Ã ÄŠtowska M, Mital a, MusiaÃ¡ J, et al.: Registry of inherited bleeding disorders in Poland--current status and potential role of the HemoRec database. Haemophilia 2011; 17:e189â€“95. 10. Von der Weid N: Haemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010â€“2011 [Internet]. . Hamostaseologie 2012; 32:S20â€“S24. 11. Hay C: The UK Haemophilia database: a tool for research, audit and healthcare planning. Haemophilia 2004; 10:21â€“25. 12. Hesse J, Haschberger B, Heid PROCS 6859 S1877-0509(15)02642-3 10.1016/j.procs.2015.08.507 The Authors ☆ Peer-review under responsibility of SciKA - Association for Promotion and Dissemination of Scientific Knowledge. The Portuguese National Registry for Hemophilia: Developing of a Web-based Technological Solution Leonor Teixeira a c ⁎ Vasco Saavedra a João Pedro Simões d Beatriz Sousa Santos b c Carlos Ferreira a c a Department of Economics, Management and Industrial Engineering, University of Aveiro, 3810-193 Aveiro, Portugal Department of Economics, Management and Industrial Engineering, University of Aveiro, 3810-193 Aveiro Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro Portugal c IEETA - Institute of Telematics and Electronic Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal IEETA - Institute of Telematics and Electronic Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro Portugal d Truphone, Lisbon, 2740-120 Oeiras, Portugal Truphone, Lisbon, 2740-120 Oeiras Portugal ⁎ Corresponding author. Tel.: +351-234-370361; fax: +351-234-370215. The crucial role that patient records have in the management of the rare and chronic diseases greatly increases the need to create mechanisms to facilitate the identification and management of the patient's data. Hemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor that affects the population on a ratio of 1 case for 10,000 people born. Currently, there are several countries with technological platforms to support the National Patients’ Registries (NPR) of Hemophilia and other Congenital Coagulopathies (HoCC), due to its benefits in the management of the disease. This work presents the technological platform developed in a joint initiative between the University of Aveiro (UA) and the Portuguese Association of Congenital Coagulopathies (PACC), with the purpose of creating the first NPR with HoCC in Portugal. This web application is hosted in the data center of the University of Aveiro, and is being used by the clinicians of the different Hemophilia Treatment Centers (HTC) across the country. Keywords National Pacients Registry Web-application Hemophilia Congenital Coagulopathies References [1] Evatt B: World Federation of Hemophilia Guide to Developing a National Patient Registry. Montréal - Québec, 2005. [2] A. Srivastava A.K. Brewer E.P. Mauser-Bunschoten N.S. Key S. Kitchen A. Llinas Guidelines for the management of hemophilia Haemophilia 19 2013 e1 e47 [3] L. Viviani A. Zolin A. Mehta Olesen HV: The European Cystic Fibrosis Society Patient Registry: valuable lessons learned on how to sustain a disease registry Orphanet J Rare Dis 9 2014 81 [4] B. Lara P. Morales I. Blanco M. Vendrell J. de Gracia Roldán M. Monreal Respiratory Disease Registries in Spain: Fundamentals and Organization [Internet] Arch Bronconeumol (English Version) 47 2011 389 396 [5] World Federation of Hemophilia: World Federation of Hemophilia Report on the ANNUAL GLOBAL SURVEY 2011. Montréal - Québec, 2012. [6] Reitter S, Streif W, Schabetsberger T, Wozak F, Hartl H, Male C, et al.: Austrian Hemophilia Registry: design, development and set of variables. Wien Klin Wochenschr 2009; 121:196-201. [7] O’Mahony B, Noone D, Giangrande PLF, Prihodova L: Haemophilia care in Europe - a survey of 35 countries. Haemophilia 201; 19:e239-47. [8] Aznar Ja L. Abad-Franch V.R. Cortina Marco P: The national registry of haemophilia A and B in Spain: results from a census of patients. Haemophilia 15 2009 1327 1330 [9] J. Zdziarska K. Chojnowski Klukowska a M. Łętowska a Mital J. Musiał Registry of inherited bleeding disorders in Poland--current status and potential role of the HemoRec database Haemophilia 17 2011 e189 e195 [10] Von der Weid N: Haemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010-2011 [Internet]. . Hamostaseologie 2012; 32:S20-S24. [11] C. Hay The UK Haemophilia database: a tool for research, audit and healthcare planning Haemophilia 10 2004 21 25 [12] J. Hesse B. Haschberger M. Heiden R. Seitz Schramm W: Neue Daten aus dem Deutschen Hämophilieregister [Internet] Hamostaseologie 33 2013 S15 S21 [13] I. Walker M. Pai J. Akabutu B. Ritchie G. Growe M.C. Poon The Canadian Hemophilia Registry as the basis for a national system for monitoring the use of factor concentrates Transfusion 35 1995 548 551 [14] Teixeira L, Saavedra V, Ferreira C, Simões J, Sousa Santos B: Requirements engineering using mockups and prototyping tools: developing an healthcare web-application; in Yamamoto S (ed): HCI International. Creta, Springer International Publishing Switzerland, 2014, pp 652-663. [15] Rod Johnson, Juergen Hoeller et al.: Spring Framework Reference Documentation [Internet] 2014 [cited 2015 Apr 1];Available from: [16] Sencha GXT: Create feature-rich HTML5 applications using Java [Internet] 2015 [cited 2015 Apr 1];Available from: [17] Hibernate: Hibernate ORM [Internet] [cited 2015 Apr 10];Available from: "
    },
    {
        "doc_title": "Developing and evaluating two gestural-based virtual environment navigation methods for large displays",
        "doc_scopus_id": "84947297363",
        "doc_doi": "10.1007/978-3-319-20804-6_13",
        "doc_eid": "2-s2.0-84947297363",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3DUIs",
            "Depth sensors",
            "Gestural interaction",
            "Large displays",
            "Navigation in virtual en-vironments",
            "Navigation methods",
            "User study"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.In this paper we present two methods to navigate in virtual environments displayed in a large display using gestures detected by a depth sensor. We describe the rationale behind the development of these methods and a user study to compare their usability performed with the collaboration of 17 participants. The results suggest the users have a better performance and prefer one of them, while considering both as suitable and natural navigation methods.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Heuristic evaluation in information visualization using three sets of heuristics: An exploratory study",
        "doc_scopus_id": "84947228566",
        "doc_doi": "10.1007/978-3-319-20901-2_24",
        "doc_eid": "2-s2.0-84947228566",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Cognitive and visual heuristics",
            "Evaluation methods",
            "Exploratory studies",
            "Heuristic evaluation",
            "Information visualization",
            "InfoVis evaluation",
            "Usability",
            "Visualization application"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Evaluation in Information Visualization is inherently complex, and it is still a challenge. Whereas it is possible to adapt evaluation methods from other fields, as Human-Computer Interaction, this adaptation may not be straightforward since visualization applications are very specific interactive systems. This paper addresses issues in using heuristic evaluation to evaluate visualizations and visualization applications, and presents an exploratory study in two phases and involving 25 evaluators aimed at assessing the understandability and effectiveness of three sets of heuristics that have been used in Information Visualization.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive clinical pedigree visualization using an open source pedigree drawing engine",
        "doc_scopus_id": "84947226820",
        "doc_doi": "10.1007/978-3-319-20901-2_38",
        "doc_eid": "2-s2.0-84947226820",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Electronic health record",
            "Health information systems",
            "Hemophilia care",
            "Ontofam",
            "Pedigree"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Advances in Genetics have revealed that many diseases are related to genetic factors. In this context, family health histories play an increasingly important role in healthcare, aiding practitioners in the diagnosis, risk assessment and treatment of various conditions. The clinical pedigree, a graphic representation combining family structure and clinical information, is a well-accepted tool to represent family health histories. At present this tool remains underused, possibly due to the lack of pedigree management tools in health information systems. OntoFam addresses this problem by offering a clinical pedigree information system that can be integrated with current health information systems. This paper presents the method used to create OntoFam’s interactive pedigree visualization by wrapping an existing open source pedigree drawing engine. The resulting environment allows practitioners to interactively view, create and manipulate pedigrees. This paper also describes the evaluation strategy that was developed to assess the system and includes its preliminary results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Computer-assisted myocardial perfusion assessment",
        "doc_scopus_id": "84912093309",
        "doc_doi": "10.1109/IV.2014.71",
        "doc_eid": "2-s2.0-84912093309",
        "doc_date": "2014-09-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Automatically match",
            "Computer assisted",
            "Computer-assisted analysis",
            "Interactive tool",
            "Left ventricles",
            "Myocardial perfusion"
        ],
        "doc_abstract": "© 2014 IEEE.Left ventricle functional analysis can be performed using CTA images. In recent years it has been proposed that CTA imaging might also be used for myocardial perfusion assessment, by visually looking for hypo attenuated regions that may correspond to hypo perfusion. Assigning a detected lesion to a specific myocardial segment requires significant effort while the clinician matches the Ahab standard myocardial segments to the anatomy of each patient. This article presents an interactive tool which, based on the existence of a previous left ventricle segmentation, automatically matches the lesions detected by the clinician to a myocardial segment and provides feedback over the image regarding the myocardial segments with detected lesions and their severity.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A tool for visually exploring multi-objective mixed-integer optimization models",
        "doc_scopus_id": "84912080897",
        "doc_doi": "10.1109/IV.2014.29",
        "doc_eid": "2-s2.0-84912080897",
        "doc_date": "2014-09-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Conflicting objectives",
            "Interactive visualization tool",
            "Mixed integer optimization",
            "Multi-objective optimization models",
            "Multi-objective optimization problem",
            "Optimal solutions",
            "Preferred solutions",
            "Visual exploration"
        ],
        "doc_abstract": "© 2014 IEEE.Multi-objective optimization models have been increasingly used as optimal decisions are searched in settings considering several conflicting objectives. In these cases compromises must be made and often a large number of non-dominated optimal solutions exist. From these solutions decision-makers must find the preferred one. This is a difficult task both from a computational and cognitive point of views, as it requires several solutions to be obtained and compared. An interactive visualization tool for fully understanding the best trade-offs is therefore becoming increasingly important. This paper proposes visualization solutions, implemented in a tool, for aiding decision-makers in finding the preferred solution in multi-objective optimization problems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The role of ICTs in the management of rare chronic diseases: The case of hemophilia",
        "doc_scopus_id": "84945379874",
        "doc_doi": "10.4018/978-1-4666-6339-8.ch067",
        "doc_eid": "2-s2.0-84945379874",
        "doc_date": "2014-08-31",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2015 by IGI Global. All rights reserved.This chapter outlines a study that examines the role of Information and Communication Technologies (ICTs) in management of a rare and chronic disease, hemophilia. Evidence in literature shows how the adoption of ICTs can improve the management of chronic conditions. Furthermore, these tools may also give response to rare diseases' needs, while greatly improving the quality of life of those patients. A Web-based application that was developed to facilitate communication between Healthcare Professionals (HCPs) and patients in a specific Hemophilia Treatment Center (HTC), to improve the utility and quality of clinical data and treatment information, as well as to help the management of resources involved in a specific rare chronic disease, represents a practical case presented in this chapter. This technological solution allows the management of inherited bleeding disorders, integrating, diffusing, and archiving large sets of data relating to the clinical practice of hemophilia care, more specifically the clinical practice at the Hematology Service of Coimbra Hospital Center.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation in visualization: Some issues and best practices",
        "doc_scopus_id": "84894530717",
        "doc_doi": "10.1117/12.2038259",
        "doc_eid": "2-s2.0-84894530717",
        "doc_date": "2014-03-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The first data and information visualization techniques and systems were developed and presented without a systematic evaluation; however, researchers have become, and are more and more, aware of the importance of evaluation (Plaisant, 2004)1. Evaluation is not only a means of improving techniques and applications, but it can also produce evidence of measurable benefits that will encourage adoption. Yet, evaluating visualization applications or techniques, is not simple. We deem visualization applications should be developed using a user-centered design approach and that evaluation should take place in several phases along the process and with different purposes. An account of what issues we consider relevant while planning an evaluation in Medical Data Visualization can be found in (Sousa Santos and Dillenseger, 2005) 2. In that work the question \"how well does a visualization represent the underlying phenomenon and help the user understand it?\" is identified as fundamental, and is decomposed in two aspects: A) the evaluation of the representation of the phenomenon (first part of the question). B) the evaluation of the users' performance in their tasks when using the visualization, which implies the understanding of the phenomenon (second part of the question). We contend that these questions transcend Medical Data Visualization and can be considered central to evaluating Data and Information Visualization applications and techniques in general. In fact, the latter part of the question is related to the question Freitas et al. (2009) 3 deem crucial to user centered visualization evaluation: \"How do we know if information visualization tools are useful and usable for real users performing real visualization tasks?\" In what follows issues and methods that we have been using to tackle this latter question, are briefly addressed. This excludes equally relevant topics as algorithm optimization, and accuracy, that can be dealt with using concepts and methods well known in other disciplines and are mainly related to how well the phenomenon is represented. A list of guidelines considered as our best practices to perform evaluations is presented and some conclusions are drawn. © 2014 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using ontologies and semantic web technology on a clinical pedigree information system",
        "doc_scopus_id": "84958554237",
        "doc_doi": "10.1007/978-3-319-07725-3_45",
        "doc_eid": "2-s2.0-84958554237",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "clinical pedigree",
            "Health information systems",
            "Ontology-based",
            "Ontology-based information systems",
            "Pedigree information",
            "Semantic Web technology"
        ],
        "doc_abstract": "Clinical family histories, in the form of clinical pedigrees, are recognized as valuable tools in the diagnostic, risk assessment and treatment of patients and their family members. The lack of adequate tools in present health information systems (HIS) is one of the factors that currently deter practitioners from making full use of these tools. In this paper we present OntoFam, an ontology-based clinical pedigree information system that can be integrated with existing HIS. We focus on the usage of ontologies and semantic web technology in the context of this information system and present a practical scenario of integration with hemo@care, a HIS designed for hemophilia care. © 2014 Springer International Publishing Switzerland.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Requirements engineering using mockups and prototyping tools: Developing a healthcare web-application",
        "doc_scopus_id": "84904115337",
        "doc_doi": "10.1007/978-3-319-07731-4_64",
        "doc_eid": "2-s2.0-84904115337",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Information transfers",
            "Levels of abstraction",
            "Rapid prototyping model",
            "Requirement elicitation",
            "Requirement engineering",
            "Requirements engineering process",
            "Requirements specifications",
            "Web applications"
        ],
        "doc_abstract": "Healthcare web-application development teams involve non-computer experts working (clinicians) on the requirements specification that is later processed by software engineers/analysts (conceptual model) and coded by software programmers (software project). The management of this process, which involves different levels of abstraction and professionals with different backgrounds, is often complex. As such, mediators and facilitator's mechanisms for the requirements-gathering process and information transfer are needed. The main purpose of this work is to minimize the problems associated with this complex process, supporting the requirements engineering process of a healthcare web-application in a rapid prototyping model. The results proved that a rapid and functional prototyping model can improve the effectiveness of the requirement elicitation of any software development. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Choosing a selection technique for a virtual environment",
        "doc_scopus_id": "84903639109",
        "doc_doi": "10.1007/978-3-319-07458-0_21",
        "doc_eid": "2-s2.0-84903639109",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Comparative studies",
            "Contextual information",
            "Interaction methods",
            "Interactive virtual environments",
            "Multimedia information",
            "Selection",
            "Selection techniques",
            "Virtual application"
        ],
        "doc_abstract": "Bearing in mind the difficulty required to create virtual environments, a platform for Setting-up Interactive Virtual Environments (pSIVE) was created to help non-specialists benefit from virtual applications involving virtual tours where users may interact with elements of the environment to extract contextual information. The platform allows creating virtual environments and setting up their aspects, interaction methods and hardware to be used. The construction of the world is done by loading 3D models and associating multimedia information (videos, texts or PDF documents) to them. A central interaction task in the envisioned applications of pSIVE is the selection of objects that have associated multimedia information. Thus, a comparative study between two variants of the ray-tracing selection technique was performed. The study also demonstrates the flexibility of the platform, since it was easily adapted to serve as a test environment. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of antibacterial, antifungal and modulatory activity of methanol and ethanol extracts of Padina sanctae-crucis",
        "doc_scopus_id": "84902343363",
        "doc_doi": "10.4314/ahs.v14i2.12",
        "doc_eid": "2-s2.0-84902343363",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Anti-Bacterial Agents",
            "Antifungal Agents",
            "Bacteria",
            "Ethanol",
            "Fungi",
            "Humans",
            "Methanol",
            "Microbial Sensitivity Tests",
            "Phaeophyta",
            "Plant Extracts",
            "Seaweed"
        ],
        "doc_abstract": "Background: Multi-resistant microorganisms such as Escherichia coli, Staphylococcus aureus, Pseudomonas aeruginosa, Candida tropicalis e Candida krusei are the main causes of microbial infections. Padina sanctae-crucis is a seaweed often used to check the contamination of ecosystems by materials such as heavy metals, but studies of the antimicrobial activity of the same seaweed were not found. Methods: The tests for the minimum inhibitory concentration and modulation of microbial resistance, with the use of ethanolic and methanolic extracts of Padina Sanctae-cruces combined with drugs of the class of aminoglycosides and antifungal were used to evaluate the activity against the cited microorganisms. Results: Was observed a modulation of antibiotic activity between the natural products and the E. coli and S. aureus strains, indicating a synergism and antagonism respectively. Conclusions: The results showed a moderate modulatory effect against some microorganisms studied.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Association between drugs and herbal products: In vitro enhancement of the antibiotic activity by fractions from leaves of Croton campestris A. (Euphorbiaceae)",
        "doc_scopus_id": "84901830789",
        "doc_doi": "10.1016/j.eujim.2014.03.002",
        "doc_eid": "2-s2.0-84901830789",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Complementary and Alternative Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2707"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Introduction: The objective of this study was to evaluate the antibacterial activity of methanol (MFEECC) and ethyl acetate (AFEECC) fractions, obtained from the ethanol extract of Croton campestris A. leaves. Methods: Antibacterial and modulating activity (on bacterial resistance) was determined by micro dilution method to identify the MIC (Minimum Inhibitory Concentration), performed in triplicate and statistical significance tested by ANOVA (two-way) with Bonferroni post hoc test (p<. 0.001). Results: In the antibacterial activity tests the fractions showed a MIC of ≥1024. μg/mL. In regards to modulation of bacterial resistance, MFEECC showed synergism when combined with antibiotic against bacterial strains. In the modulation tests, AFEECC potentiated the effects of amikacin against Staphylococcus aureus and Pseudomonas aeruginosa but had an antagonistic effect against Escherichia coli. AFEECC combined with gentamicin displayed antagonism against S. aureus and E. coli and an antagonistic effect against P. aeruginosa. When AFEECC was combined with Neomycin it resulted in antagonism against P. aeruginosa but did not affect S. aureus and E. coli. Conclusions: The results indicate that the extracts and fractions obtained from C. campestris leaves could represent an alternative source of natural products capable of modifying and interfering with bacterial resistance to aminoglycosides. © 2014 Elsevier GmbH.",
        "available": true,
        "clean_text": "serial JL 277409 291210 291897 291910 291928 31 European Journal of Integrative Medicine EUROPEANJOURNALINTEGRATIVEMEDICINE 2014-03-13 2014-03-13 2015-10-01T23:28:23 S1876-3820(14)00034-1 S1876382014000341 10.1016/j.eujim.2014.03.002 S300 S300.3 FULL-TEXT 2015-10-03T06:20:47.304969-04:00 0 0 20140601 20140630 2014 2014-03-13T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast orcid primabst pubtype ref 1876-3820 18763820 true 6 6 3 3 Volume 6, Issue 3 9 301 306 301 306 201406 June 2014 2014-06-01 2014-06-30 2014 Original Articles article fla Copyright © 2014 Elsevier GmbH. All rights reserved. ASSOCIATIONBETWEENDRUGSHERBALPRODUCTSINVITROENHANCEMENTANTIBIOTICACTIVITYBYFRACTIONSLEAVESCROTONCAMPESTRISAEUPHORBIACEAE LAVOR A Introduction Materials and methods Bacterial material Plant material Preparation of the extract and fractions of C. campestris Phytochemical analysis Drugs Testing antibacterial activity Modulation of bacterial resistance Statistical analysis Results Discussion Conclusion Conflict of interest Acknowledgements References COUTINHO 2010 467 H VERHOEFF 1999 461 466 J SCOTLAND 1980 90 S HUGHES 1982 247 252 C MURRAY 2004 P MICROBIOLOGIAMEDICA ARORA 2009 286 294 D GURIBFAKIM 2006 1 93 A GULCIN 2008 450 471 I COUTINHO 2008 670 675 H NASCIMENTO 2000 247 256 G COHEN 1992 1050 1055 M DAZA 1998 3 P MUSUMECI 2003 48 53 R HELUANI 2000 222 225 C DALBO 2004 S AVALIACAODAATIVIDADEANTINOCICEPTIVADASUBFRACAO63SF63OBTIDAAPARTIRDASCASCASDACROTONCELTIDIFOLIUSEUPHORBIACEAEESTUDOMECANISMODEACAO CAMURCAVASCONCELOS 2007 288 294 A RAO 2007 357 360 V BABILI 2006 384 387 F SUAREZ 2006 99 101 A LOPES 2004 437 445 E MATIAS 2010 294 298 E MATOS 1997 F INTRODUCAOAFITOQUIMICAEXPERIMENTAL NCCLS 2003 METHODSFORDILUTIONANTIMICROBIALSUSCEPTIBILITYTESTSFORBACTERIAGROWAEROBICALLYAPPROVEDSTANDARD COUTINHO 2008 328 330 H BARREIROS 2006 113 123 A OKUDA 1989 117 122 T SIMOES 2010 C FARMACOGNOSIADAPLANTAAOMEDICAMENTO MACHADO 2008 33 39 H TALEBCONTINI 2003 403 408 S COWAN 1999 564 582 M HO 2001 187 191 K DORMAN 2000 306 318 H NICOLSON 1999 233 239 K BURT 2004 223 253 S WENDAKOON 1995 280 283 C GUIMARAES 2010 667 679 D WALSH 2003 C ANTIBIOTICSACTIONSORIGINSRESISTANCE FIGUEREDO 2013 1 5 F MAGNET 2005 477 497 S LAVORX2014X301 LAVORX2014X301X306 LAVORX2014X301XA LAVORX2014X301X306XA item S1876-3820(14)00034-1 S1876382014000341 10.1016/j.eujim.2014.03.002 277409 2015-10-03T06:20:47.304969-04:00 2014-06-01 2014-06-30 true 1033493 MAIN 6 63700 849 656 IMAGE-WEB-PDF 1 gr2 570974 1446 2500 gr1 573099 1540 2500 gr2 77281 327 565 gr1 79617 348 565 gr2 20284 127 219 gr1 21039 135 219 EUJIM 316 S1876-3820(14)00034-1 10.1016/j.eujim.2014.03.002 Elsevier GmbH Fig. 1 Modulatory antibiotic activity of the methanol fraction of the ethanol extract of Croton campestris (MFEECC) in association with aminoglycosides. Fig. 2 Modulatory antibiotic activity of the ethyl-acetate fraction of ethanol extract of Croton campestris (AFEECC) in association with aminoglycosides. Table 1 Source of bacterial strains and antibiotic resistance profile. Bacteria Source Resistance profile Staphylococcus aureus SA358 Surgical wound Oxa, Gen, Tob, Ami, Can, Neo, Para, But, Sis, Net Staphylococcus aureus ATCC 25923 ATCC – Escherichia coli EC27 Surgical wound Ast, Ax, Amp, Ami, Amox, Ca, Cfc, Cf, Caz, Cip, Clo, Im, Can, Szt, Tet, Tob Escherichia coli ATCC 10536 ATCC – Pseudomonas aeruginosa PA03 Tip of catheter Cpm, Ctz, Im, Cip, Ptz, Lev, Mer, Ami Pseudomonas aeruginosa ATCC 15442 ATCC – Ast, Aztreonan; Ax, Amoxacilin; Amp, Ampicilin; Ami, Amikacin; Amox, Amoxilin; Ca, Cefadroxil; Cfc, cefaclor; Cf, Cefalotine; Caz, Ceftazinidime; Cip, Ciprofloxacin; Clo, Clorafenicol; Im, Imipenem; Can, Kanamycin; Szt, Sulfametrim, Tet, Tetraciclin; Tob, Tobramycina; Oxa, Oxacilin; Gen, Gentamicin; Neo, Neomycin; Para, Paramomycin; But, Butirosine; Sis, Sisomycin; Net, Netilmycin. Table 2 Dry mass and yield of fractions methanolic and ethyl acetate (g). Specie Solvent used (Sigla) Material Dry mass Yield Croton campestris A. Methanol (MFEECC) Extract 49 28.18 Ethyl-acetate (AFEECC) Extract 49 3 MFEECC, methanol fraction of ethanol extract of Croton campestris; AFEECC, ethyl acetate fraction of ethanol extract of Croton campestris. Table 3 Phytochemical prospection of the extract and fractions of Croton campestris. Metabolites 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 MFEECC − − + − − + + + + + + + + + + AFEECC − + − − + + + + + + + + + − 1, Phenols; 2, Hydrolizable tannins; 3, Condensed tannins; 4, Anthocianins; 5, Anthocianidines; 6, Flavones; 7, Flavonols; 8, Xanthones; 9, Chalcones; 10, Aurones; 11, Flavononols; 12, Leucoanthocianidines; 13, Catequines; 14, Flavonones; 15, Alkaloids; (+) presence; (−) absense. MFEECC, methanol fraction of ethanol extract of Croton campestris; AFEECC, ethyl-acetate fraction of ethanol extract of Croton campestris. Original article Association between drugs and herbal products: In vitro enhancement of the antibiotic activity by fractions from leaves of Croton campestris A. (Euphorbiaceae) Anne Karyzia L.S. Lavor a Edinardo F.F. Matias b Erivania F. Alves b Beatriz S. Santos b Fernando G. Figueredo a Luciene F. Lima a Nadghia F. Leite a Celestina E. Sobral-Souza a Jacqueline C. Andrade a Liscássia B.B. Alencar a Dara I.V. Brito a Rosimeire S. Albuquerque a Henrique D.M. Coutinho a ⁎ a Laboratório de Microbiologia e Biologia Molecular, Universidade Regional do Cariri, Crato, CE, Brazil Laboratório de Microbiologia e Biologia Molecular, Universidade Regional do Cariri Crato CE Brazil b Faculdade Leão Sampaio, Juazeiro do Norte, CE, Brazil Faculdade Leão Sampaio Juazeiro do Norte CE Brazil ⁎ Corresponding author at: Laboratório de Microbiologia e Biologia Molecular, Departamento de Química Biológica, Universidade Regional do Cariri – URCA, Rua Cel. Antonio Luis 1161, Pimenta, Crato 63105-000, CE, Brazil. Tel.: +55 88 31021212; fax: +55 88 31021291. Introduction The objective of this study was to evaluate the antibacterial activity of methanol (MFEECC) and ethyl acetate (AFEECC) fractions, obtained from the ethanol extract of Croton campestris A. leaves. Methods Antibacterial and modulating activity (on bacterial resistance) was determined by micro dilution method to identify the MIC (Minimum Inhibitory Concentration), performed in triplicate and statistical significance tested by ANOVA (two-way) with Bonferroni post hoc test (p <0.001). Results In the antibacterial activity tests the fractions showed a MIC of ≥1024μg/mL. In regards to modulation of bacterial resistance, MFEECC showed synergism when combined with antibiotic against bacterial strains. In the modulation tests, AFEECC potentiated the effects of amikacin against Staphylococcus aureus and Pseudomonas aeruginosa but had an antagonistic effect against Escherichia coli. AFEECC combined with gentamicin displayed antagonism against S. aureus and E. coli and an antagonistic effect against P. aeruginosa. When AFEECC was combined with Neomycin it resulted in antagonism against P. aeruginosa but did not affect S. aureus and E. coli. Conclusions The results indicate that the extracts and fractions obtained from C. campestris leaves could represent an alternative source of natural products capable of modifying and interfering with bacterial resistance to aminoglycosides. Keywords Aminoglycosides Antibacterial activity Modulating activity Croton campestris A. Microbial resistance Introduction Bacteria of the Staphylococcus genus are widely distributed, occurring on the normal skin microbiota and on the mucosa of animals and birds. Some Staphylococcus species are recognized as etiological agents of opportunist infections in animals and humans [1]. Staphylococcus aureus, S. epidermidis, S. saprophyticus and S. haemolyticus are the most frequent cause of human infections. Besides causing distinct types of illness, S. aureus represents the most common etiological agent in infections such as boils, carbuncles, abscesses, myocarditis, endocarditis, pneumonia, meningitis and bacterial arthritis [2]. Escherichia coli is a major cause of infections in humans. It produces enterotoxins whose properties and role in diarrhea have been widely investigated. The activities of cytotoxins and their role in human infection have been described [3] in urinary tract infections [4]. Pseudomonas aeruginosa is responsible for a large number of infections including those affecting the skin, urinary tract, eyes and ears. In addition, a large number of structural features such as enzymes and toxins potentiate its virulence and make it more resistant to the most common antibiotics [5]. With the emergence of antibiotic resistant bacterial strains, natural products may be a convenient alternative [6]. Different plants have been analyzed, not only for direct antimicrobial activity but also as potential agents for modifying antibiotic resistance [7]. There is growing interest in the use of biologically active compounds, isolated from plants for the treatment of infections caused by micro organisms that are resistant to antibiotics [8,9]. Even new antibiotics have been produced in the last three decades, the resistance of microorganisms against these drugs has increased [10]. In general, bacteria have a genetic capacity to transmit and gain resistance to these drugs, which are used as therapeutic agents [11]. The mechanisms of bacterial resistance to antibiotics are threefold: (1) inactivation of the antibiotics by enzymatic activity; (2) molecular modifications to avoid the antibiotic binding to its target site; (3) permeability barrier or modifications to restrict antibiotic reaching its target. It is also possible that these can occur simultaneously [12]. The association of antimicrobials is evaluated by its ability to suppress the emergence of resistant mutants and produce an in vivo synergic effect. Extending the working life of current antimicrobials could be possible if used in combination with other products, such as natural ones. These could represent a therapeutic alternative in the treatment of infections caused by pathogens [13]. Plants of the Euphorbiaceae family (290 genus and approximately 7500 species) are distributed globally, especially in tropical regions. The largest dispersion centers are found in Africa and in the Americas [14]. Numerous biological studies focus on the Croton genus. The plants from this genus are rich in secondary metabolites responsible for biological activities. Active compounds include: proanthocyanidins, alkaloids, terpenes, flavones and other phenolic compounds [15]. Among the activities attributed to the Croton genus we note the anti-helminth, cardiovascular, anti-nociceptive, moluscicidal, anti-inflammatory, anti-cancer, antioxidant and mutagenic effects [16–20]. Croton campestris A., popularly known as “velame-do-campo”, is a shrub originating in Brazil. The species is common in the Southeast and Northeast regions and popularly used as a powerful depurative as well as to combat scrofula, venereal diseases, dermatophytosis, tumors, skin diseases, rheumatisms, ulcers of the uterus, diarrhea and arthritis [21]. The objective of the present study was to assess the antibacterial properties and modulating effect on antibacterial resistance to aminoglycosides of the methanol and ethyl-acetate fractions of the ethanol extract from C. campestris A. leaves against standard and multi resistant bacterial strains. Materials and methods Bacterial material The bacterial strains used were: S. aureus (SA-ATCC25923 and SA358), E. coli (EC-ATCC 10536 and EC 27) and P. aeruginosa (PA-ATCC15442 e PA03) (see Table 1 for resistance profiles). All the strains were kept in Heart Infusion Agar slants (HIA, Difco Laboratories LTDA). Prior to experiments, the cells were cultivated for 24h in 37°C in brain heart infusion (BHI, Difco Laboratories LTDA). Plant material C. campestris leaves were collected in the city of Crato, Ceará, Brazil (May/2012 at 9am). The plant material was identified and a specimen deposited at Herbarium of the Federal University of Rio Grande do Norte (UFRN), under registry no. 7095. Preparation of the extract and fractions of C. campestris For extract and fraction preparation, leaves (which were submerged in ethanol for 72h) were filtered and concentrated in a rotary vacuum condenser (model Q – 344B – Quimis Brazil) and ultra thermal bath (model Q – 214M2 – Quimis), obtaining a mass return from the gross extract. To isolate the methanol and ethyl-acetate fractions, we used vacuum filtration with these solvents against the extract (Table 2 ). Test solutions were prepared at a concentration of 10mg/mL, dissolved in DMSO and then diluted in distilled water to a concentration of 1024μg/mL. A preliminary study (using only DMSO in various concentrations) was performed. We observed that DMSO (at the concentration used in the antibacterial and modular tests) did not show interference or toxicity. Phytochemical analysis Phytochemical tests were done in a qualitative way to detect the presence of heterosids, saponins, tannins, flavonoids, steroids, triterpenes, cumarines, quinones, organic acids and alkaloids [22]. The tests were based on colorimetric readings or formation of precipitate following the addition of specific reagents. Drugs Antibiotics used were: Amikacin, Gentamycin and Neomycin (SIGMAChemicals, St. Louis, USA) prepared according to the manufacturer's instructions. Testing antibacterial activity The MIC (Minimum Inhibitory Concentration) was calculated by broth microdilution assay [23] using a 100μL innoculation from each bacterial strain, suspended in BHI broth at a concentration of 105 UFC/mL on 96 well microtiter plates, with serial twofold dilutions. To each well, we added 100μL of test solution with final concentrations varying between 512 and 518μg/mL. Standard antibiotics were used as a control (Amikacin, Gentamycin and Neomycin) whose final concentrations varied between 2500 and 2.4μg/mL. Plates were incubated at 35°C for 24h before colorimetric readings were performed with resazurin. MICs were recorded as the lowest concentrations for inhibition of growth. Modulation of bacterial resistance Extract and fractions were tested at a sub-inhibitory concentration (MIC/8) with 100μL of a solution containing BHI 10%, bacterial suspension and test solution. 100μL of antibiotic solution was added (in a proportion of 1:1) until the penultimate well. The concentrations of aminoglycosides varied between 5000 and 2.44μg/mL. Plates were incubated at 35°C for 24h, after which, colorimetric readings were made using resazurin. Tests were performed in triplicates [24]. Statistical analysis Data are expressed as geometric means. Statistical significance was assessed with a two-way ANOVA test followed by the Bonferroni post hoc test (where p <0.001 was considered significant). Results The phytochemical analysis of the methanol (MFEECC) and ethyl acetate (AFEECC) fractions of the ethanol extract revealed the presence of several potentially bioactive compounds, such as: condensed tannins, flavones, flavonols, xanthones, chalcones, aurones, flavonoids, leucoanthocyanidins, catechins, and alkaloids (Table 3 ). Antibacterial activity of MFEECC and AFEECC displayed a MIC of ≥1024μg/mL against standard and multi resistant bacterial strains (an MIC value of ≤256μg/mL was considered clinically relevant). When combined with aminoglycosides, MFEECC showed synergistic activity against tested bacterial strains (where we verified a reduction in antibiotic concentration in the presence of a natural product). Only the combination of amikacin and natural product did not affect bacterial growth (tested against S. aureus). In addition, the combination also resulted in reduced efficacy when assayed against P. aeruginosa (as seen by an increase on MIC of amikacin in the presence of natural product). When investigating the modulation of bacterial resistance to aminoglycosides we found that when AFEECC was combined with amikacin potentiated the antibiotics effects against S. aureus and P. aeruginosa while an antagonistic effect was observed against E. coli. When AFEECC was combined with Gentamicin, it displayed antagonism against S. aureus and E. coli, an antagonistic effect against P. aeruginosa. When combined with neomycin, AFEECC showed antagonism against P. aeruginosa and no difference when tested against S. aureus and E. coli. Synergism was characterized by a reduction in the antibiotics’ MIC while antagonism was defined as the increase in antibiotic MIC. Results were analyzed statistically and significance attained when p <0.001. Sub inhibitory concentration of MFEECC and AFEECC were combined with aminoglycosides to test antibiotic resistance in multi-resistant bacterial strains (see Figs. 1 and 2 ). Discussion The metabolites identified in the phytochemical analysis displayed a variety of biological activities such as antioxidant [25], anti tumor, anti ophidic and antimicrobial [26]. By this fact, secondary metabolites frequently provide interesting biological activities [27]. Flavonoids represent one of the most important and diverse phenolic groups among the products produced by plants [27]. Recently, interest in the pharmacological properties of flavonoids has grown because they have a diverse array of activities [28], including antimicrobial [29]. This activity could be due to its ability to form complexes with extracellular soluble proteins (and cellular wall). The antimicrobial activity could also be attributed to the lipophilic character of the flavonoids, which could rupture the cellular membrane of microorganisms [30]. Tannins are described as substances with antimicrobial properties. This effect could be associated with the hydrolysis of a gallic acid ester bond, which works as a natural defense mechanism against microbial infections. The antimicrobial property of tannic acid can also be used in food processing to increase the shelf life of products. The tannin component of epicatequine and catequine (Vaccinium vitisidaea L.) had a strong antibacterial activity against fungi and bacteria [31]. Other studies that used active plant substances or natural products with antimicrobial and modulating activities have obtained satisfactory results [9]. The antibacterial activity of isolated compounds depends on its chemical structure [32]. There are numerous mechanisms by which the extracts could inhibit the development of microorganisms. This could be partly due to a hydrophobic nature of some components. These can interact with the double lipid layer of the cellular membrane and affect the respiratory chain and energy production [33]. It may also increase the permeability to antibiotics leading to the suspension of vital cellular activity [34]. Interfering with enzymatic activity could also be a potential action mechanism [35]. Aminoglycoside antibiotics exhibit a bactericidal effect by binding to 30S sub units of bacterial ribosomes. This activity prevents ribosomal activity on the mRNA, consequently blocking protein synthesis [36]. The continuous use of aminoglycoside antibiotics should be controlled due to ototoxic and nephrotoxic effects [36,37]. When used in combination with Gentamicin and Neomycin, natural products obtained from plants could be used to minimize the undesirable effects of these antibiotics when used for the treatment of tested strains. When the natural product potentiates the effects of the antibiotic, the MIC of these drugs could be lowered to achieve therapeutic success [38]. The different activities (synergistic or antagonistic) of natural products on amikacin against resistant strains could be associated with structural differences of these aminoglycosides. These are hydrophilic molecules formed by an aminociclitol ring linked to one or more amino sugars through a glycosidic connection. In most of these compounds used in the clinic, the aminococlitol group is a 2-deoxystreptamine, which can be irreplaceable in t he position 4 and 5 or 4 and 6 [39], therefore being able to interfere in the polarity, solubility and ultimately in the absorption of these drugs. Conclusion The results indicate that the methanol and ethyl acetate fractions obtained from C. campestris leaves did not present clinically relevant antibacterial activity. However, when combined with antibiotics we report that MFEECC and AFEECC significantly modulate antibiotic resistance to aminoglycosides, reducing or potentiating the pharmacological action. Thus, natural products extracted from C. campestris could represent an excellent therapeutic option in the treatment of infections, combining multiple compounds to create a greater effect. In the meantime, more in depth phytochemical studies are necessary to isolate and identify the substances responsible for modulating aminoglycosides antibiotic activitiy. Conflict of interest The authors have no conflict of interest to disclose. Acknowledgements The authors are grateful to the Brazilian Research Agencies CNPq and FUNCAP. References [1] H.D.M. Coutinho J.G.M. Costa J.P. Siqueira Jr. E.O. Lima Effect of Momordica charantia L. in the resistance to aminoglycosides in ethicilin-resistant Staphylococcus aureus Comparative Immunology, Microbiology and Infectious Disease 33 2010 467 [2] J. Verhoeff D. Beaujean H. Vlok A. Baars A. Meyler V.D.C. Werkwn A dutch approach to methicillin-resistance Staphylococcus aureus European Journal of Clinical Microbiology and Infectious Disease 18 1999 461 466 [3] S.M. Scotland N.P. Day G.A. Willshaw B. Rowe Cytotoxic enteropathogenic Escherichia coli Lancet 315 1980 90 [4] C. Hughes D. Muller J. Hacher W. Goebel Genetics and pathogenic role of Escherichia coli haemolysin Toxicon 20 1982 247 252 [5] P.R. Murray Microbiologia médica 4a ed. 2004 Guanabara Koogan Rio de Janeiro [6] D.S. Arora G.J. Kaur H. Kaur Antibacterial activity of tea and coffee: their extracts and preparations International Journal of Food Properties 12 2009 286 294 [7] A. Gurib-Fakim Medicinal plants: traditions of yesterday and drugs of tomorrow Molecular Aspects of Medicine 27 2006 1 93 [8] I. Gulcin A.Z. Tel E. Kirecci Antioxidant, antimicrobial, antifungal, and antiradical activities of cyclotrichium niveum (BOISS.) Manden and Scheng International Journal of Food Properties 11 2008 450 471 [9] H.D.M. Coutinho J.G.M. Costa J.P. Siqueira-JR E.O. Lima In vitro anti-staphylococcal activity of Hyptis martiusii Benth against methicillin-resistant Staphylococcus aureus-MRSA strains Brazilian Journal of Pharmacognosy 18 Suppl. 2008 670 675 [10] G.G.F. Nascimento J. Locatelli P.C. Freitas G.L. Silva Antibacterial activity of plant extracts and phytochemicals on antibiotic resistant bacteria Brazilian Journal of Microbiology 31 2000 247 256 [11] M.L. Cohen Epidemiology of drug resistance: implications for a postantimicrobial era Science 257 1992 1050 1055 [12] P.R.M. Daza Resistencia bacteriana a antimicrobianos: su importância en la toma de decisiones en lapráctica diária Informacion Terapeutica del Sistema Nacional de Salud 22 1998 3 [13] R. Musumeci Berberis aetnensis C. Presl. extracts: antimicrobial properties and interaction with ciprofloxacin International Journal of Antimicrobial Agents 22 2003 48 53 [14] C.S. Heluani C.A.N. Catalan L.R. Hernández E.B. Tapia P.J. Natan Three new diterpenoids based on novel sarcopetalene skeleton from Croton sarcopetalus Journal of Natural Products 63 2000 222 225 [15] S. Dalbó Avaliação da atividade antinociceptiva da subfração 63 (SF63) obtida a partir das cascas da Croton celtidifolius (Euphorbiaceae) – estudo do mecanismo de ação (MSc thesis) 2004 Universidade Federal de Santa Catarina Florianópolis [16] A.L.F. Camurça-Vasconcelos C.M.L. Bevilaqua S.M. Morais M.V. Maciel C.T. Costa I.T. Macedo Anthelmintic activity of Croton zehntneri and Lippia sidoides essential oils Revista Brasileira de Parasitologia Veterinária 148 2007 288 294 [17] V.S. Rao L.A. Gurgel R.C.P. Lima-Júnior D.T. Martins V. Cechinel-Filho F.A. Santos Dragon's blood from Croton urucurana (Baill.) attenuates visceral nociception in mice Journal of Ethnopharmacology 113 2007 357 360 [18] F.E.L. Babili N. Fabre C. Moulis I. Fouraste Molluscicidal activity against Bulinus truncatus of Croton campestris Fitoterapia 77 2006 384 387 [19] A.I. Suárez Z. Blanco R.S. Compagnone M.M. Salazar-Bookaman V. Zapata C. Alvarado Anti-inflammatory activity of Croton cuneatus aqueous extract Journal of Ethnopharmacology 105 2006 99 101 [20] E. Lopes M.I. Lopes J. Saffi S. Echeverrigaray J.A. Henriques M. Salvador Mutagenic and antioxidant activities of Croton lechleri sap in biological systems Journal of Ethnopharmacology 95 2004 437 445 [21] E.F.F. Matias K.K.A. Santos J.G.M. Costa H.D.M. Coutinho In vitro antibacterial activity of Croton campestris A., Ocimum gratissimum L. and Cordia verbenacea DC. Revista Brasileira de Biociências 8 2010 294 298 [22] F.J.A. Matos Introdução à Fitoquímica Experimental 2ª Ed. 1997 Fortaleza Edições UFC [23] NCCLS Methods for dilution antimicrobial susceptibility tests for bacteria that grow aerobically; Approved Standard 6th ed. 2003 NCCLS Document M7-A Wayne, Pennsylvania [24] H.D.M. Coutinho J.G. Costa E.O. Lima V.S. Falcão-Silva J.P. Siqueira-Júnior Enhancement of the antibiotic activity against a multiresistant Escherichia coli by Mentha arvensis L. and chlorpromazine Chemotherapy 54 2008 328 330 [25] A.L.B.S. Barreiros J.M. David Oxidative stress: relations between the formation of reactive species and the organism's defense Química Nova 29 2006 113 123 [26] T. Okuda T. Yoshiba T. Hatano Ellagitannins as active constituents of medicinal plants Planta Medica 55 1989 117 122 [27] C.M.O. Simões E.P. Schenkel G. Gosmann J.C.P. Mello L.A. Mentz P.R. Petrovick Farmacognosia: da planta ao medicamento 6ª ed. 2010 Porto Alegre/Florianópolis, Editora da UFRGS/Editora da UFSC [28] H. Machado T.J. Nagem V.M. Peters C.S. Fonseca T.T. Oliveira Flavonoids and potential therapeutic Boletim do Centro de Biologia da Reprodução 27 2008 33 39 [29] S.H. Taleb-Contini M.J. Salvador E. Watanabe I.Y. Ito C.R.O. Dionéia Antimicrobial activity of flavonoids and steroids isolated from two Chromolaena species Revista Brasileira de Ciências Farmacêuticas 30 2003 403 408 [30] M.M. Cowan Plant products as antimicrobial agents Clinical Microbiology Review 12 1999 564 582 [31] K.Y. Ho C.C. Tsai J.S. Huang C.P. Chen T.C. Lin C.C. Lin Antimicrobial activity of tannin components from Vaccinium vitisidaea L. Journal of Pharmacy and Pharmacology 53 2001 187 191 [32] H.J.D. Dorman S.G. Deans Antimicrobial agents from plants: antibacterial activity of plant volatile oils Journal of Applied Microbiology 88 2000 306 318 [33] K. Nicolson G. Evans P.W. O’toole Potentiation of methicillin activity against methicillin-resistant Staphylococcus aureus by diterpenes FEMS Microbiology Letters 179 1999 233 239 [34] S. Burt Essential oils: their antibacterial properties and potential applications in foods – a review International Journal of Food Microbiology 94 2004 223 253 [35] C. Wendakoon M. Sakaguchi Inhibition of amino acid decarboxylase activity of Enterobacter aerogenes by active components in spices International Journal of Food Properties 58 1995 280 283 [36] D.O. Guimarães L.S. Momesso M.T. Pupo Antibiotics: therapeutic importance and perspectives for the discovery and development of new agents Química Nova 33 2010 667 679 [37] C. Walsh Antibiotics: actions, origins, resistance 2003 ASM Press Washington [38] F.G. Figueredo E.O. Ferreira B.F.F. Lucena C.M.G. Torres D.L. Lucetti E.C.P. Lucetti Modulation of the antibiotic activity by extracts from Amburana cearensis A.C. Smith and Anadenanthera macrocarpa (Benth.) Brenan BioMed Research International 2013 2013 1 5 [39] S. Magnet J.S. Blanchad Molecular insights into aminiglycosides action and resistance Chemical Reviews 105 2005 477 497 "
    },
    {
        "doc_title": "Platform for setting up interactive virtual environments",
        "doc_scopus_id": "84901810220",
        "doc_doi": "10.1117/12.2038668",
        "doc_eid": "2-s2.0-84901810220",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Interaction",
            "Interactive informations",
            "Interactive virtual environments",
            "Platform development",
            "Production line",
            "Selection",
            "Testing environment",
            "Virtual worlds"
        ],
        "doc_abstract": "This paper introduces pSIVE, a platform that allows the easy setting up of Virtual Environments, with interactive information (for instance, a video or a document about a machine that is present in the virtual world) to be accessed for different 3D elements. The main goal is to create for evaluation and training on a virtual factory-but generic enough to be applied in different contexts by non-expert users (academic and touristic for instance). We show some preliminary results obtained from two different scenarios: first a production line of a factory with contextualized information associated to different elements which aimed the training of employees. Second a testing environment, to compare and assess two different selection styles that were integrated in pSIVE and to allow different users to interact with an environment created with pSIVE to collect opinions about the system. The conclusions show that the overall satisfaction was high and the comments will be considered in further platform development. © 2014 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing two input devices for virtual walkthroughs using a head mounted display (HMD)",
        "doc_scopus_id": "84901800305",
        "doc_doi": "10.1117/12.2036486",
        "doc_eid": "2-s2.0-84901800305",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Ease-of-use",
            "Head mounted displays",
            "Input and outputs",
            "Input devices",
            "User performance",
            "User study",
            "Walkthroughs"
        ],
        "doc_abstract": "Selecting input and output devices to be used in virtual walkthroughs is an important issue as it may have significant impact in usability and comfort. This paper presents a user study meant to compare the usability of two input devices used for walkthroughs in a virtual environment with a Head-Mounted Display. User performance, satisfaction, ease of use and comfort, were compared with two different input devices: a two button mouse and a joystick from a gamepad. Participants also used a desktop to perform the same tasks in order to assess if the participant groups had similar profiles. The results obtained by 45 participants suggest that both input devices have a comparable usability in the used conditions and show that participants generally performed better with the desktop; a discussion of possible causes is presented. © 2014 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "VOLUMNECT: Measuring volumes with kinect",
        "doc_scopus_id": "84901770157",
        "doc_doi": "10.1117/12.2036493",
        "doc_eid": "2-s2.0-84901770157",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Connected component",
            "Depth camera",
            "Harris corner detector",
            "Image processing - methods",
            "Logistics company",
            "Microsoft kinect",
            "Morphological operations",
            "Target application"
        ],
        "doc_abstract": "This article presents a solution to volume measurement object packing using 3D cameras (such as the Microsoft KinectTM). We target application scenarios, such as warehouses or distribution and logistics companies, where it is important to promptly compute package volumes, yet high accuracy is not pivotal. Our application auto-matically detects cuboid objects using the depth camera data and computes their volume and sorting it allowing space optimization. The proposed methodology applies to a point cloud simple computer vision and image processing methods, as connected components, morphological operations and Harris corner detector, producing encouraging results, namely an accuracy in volume measurement of 8mm. Aspects that can be further improved are identified; nevertheless, the current solution is already promising turning out to be cost effective for the envisaged scenarios. © 2014 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DETI-interact: Interaction with large displays in public spaces using the kinect",
        "doc_scopus_id": "84901606379",
        "doc_doi": "10.1007/978-3-319-07788-8_19",
        "doc_eid": "2-s2.0-84901606379",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Attention catching",
            "Digital compass",
            "Electronic device",
            "Large displays",
            "Natural interfaces",
            "Public display",
            "Train stations",
            "Waiting room"
        ],
        "doc_abstract": "The problem of interaction with large displays in public spaces is currently of interest given the large number of displays available in such spaces (as lobbies, train stations, waiting rooms, etc.) that are only showing information with no possibility to interact with the contents. Several works have been developed in order to allow interaction with these displays using technologies such as infrared, Bluetooth, GPRS, digital compasses or touch screens. Some only intend to provide information, while others emphasize on capturing users' attention eventually leading them to some action. This paper describes DETIInteract, a system located in the entrance hall of a University department allowing users to interact with a large display without the need to carry any electronic device since a Kinect is used to capture different user's gestures. In this work, special attention was given to another issue intrinsically linked to the presentation of information on large public displays 'How to call the user's attention?' © 2014 Springer International Publishing Switzerland.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Investigating landfill contamination by visualizing geophysical data",
        "doc_scopus_id": "84898452992",
        "doc_doi": "10.1109/MCG.2014.11",
        "doc_eid": "2-s2.0-84898452992",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "3D Visualization",
            "Geophysical data",
            "Groundwater contamination",
            "Kriging",
            "Statistical anomaly detection",
            "Uncertainty representation"
        ],
        "doc_abstract": "Geophysical experts aimed to establish a method to identify contamination by landfill leakage without chemically analyzing subsoil samples, which is time-consuming and expensive. To that end, researchers developed a software package that let the experts create 3D visualizations of geophysical data acquired around the landfill and apply statistical analysis to detect anomalous values. The data used, electrical resistivity, are typically sparse. So, the application employs kriging to interpolate the data and provide a volumetric representation of the subsoil resistivity. To avoid invalid conclusions, the visualization also represents uncertainty. The application enabled the experts to better understand the phenomenon and to develop and validate their method. Their evaluation of the application indicated that it helped them throughout the method's development and significantly eased their workload. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Student projects involving novel interaction with large displays",
        "doc_scopus_id": "84897413845",
        "doc_doi": "10.1109/MCG.2014.35",
        "doc_eid": "2-s2.0-84897413845",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Computer graphics education",
            "Computer Science Education",
            "DETI-Interact",
            "Kinect",
            "Public display",
            "University of Aveiro",
            "Computer Graphics",
            "Gestures",
            "Humans",
            "Imaging, Three-Dimensional",
            "Information Science",
            "Students",
            "User-Computer Interface"
        ],
        "doc_abstract": "DETI-Interact is an interactive system that offers information relevant to students in the lobby of the University of Aveiro's Department of Electronics, Telecommunications and Informatics (DETI). The project started in 2009 with a master's thesis addressing interaction with public displays through Android smartphones. Since then, it has evolved considerably; it currently allows gesture interaction based on a Kinect sensor. Meanwhile, it has involved third-year students, master's students, and undergraduate students participating in a research initiation program. © 1981-2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Location-arc routing problem: Heuristic approaches and test instances",
        "doc_scopus_id": "84887109658",
        "doc_doi": "10.1016/j.cor.2013.10.003",
        "doc_eid": "2-s2.0-84887109658",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            }
        ],
        "doc_keywords": [
            "Arc routing",
            "Capacitated arc routing problem",
            "Garbage collection",
            "Heuristic approach",
            "Heuristics",
            "Improvement methods",
            "Location-routing",
            "Test instances"
        ],
        "doc_abstract": "Location-routing is a branch of locational analysis that takes into account distribution aspects. The location-arc routing problem (LARP) considers scenarios where the demand is on the edges rather than being on the nodes of a network (usually a road network is assumed). Examples of such scenarios include locating facilities for postal delivery, garbage collection, road maintenance, winter gritting and street sweeping. This paper presents some heuristic approaches to tackle the LARP, as well as some proposals for benchmark instances (and corresponding results). New constructive and improvement methods are presented and used within different metaheuristic frameworks. Test instances were obtained from the capacitated arc routing problem (CARP) literature and adapted to address the LARP. © 2013 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 271709 291210 291692 291715 291772 291813 291817 291871 31 Computers & Operations Research COMPUTERSOPERATIONSRESEARCH 2013-10-21 2013-10-21 2013-12-03T05:28:33 S0305-0548(13)00298-0 S0305054813002980 10.1016/j.cor.2013.10.003 S300 S300.1 FULL-TEXT 2015-05-14T07:00:29.889601-04:00 0 0 20140301 20140331 2014 2013-10-21T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings suppl volfirst volissue figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0305-0548 03050548 true 43 43 C Volume 43 30 309 317 309 317 201403 March 2014 2014-03-01 2014-03-31 2014 article fla Copyright © 2013 Elsevier Ltd. All rights reserved. LOCATIONARCROUTINGPROBLEMHEURISTICAPPROACHESTESTINSTANCES LOPES R 1 Introduction 2 Problem definition 3 Constructive methods and improvement heuristics 3.1 Extended augment-merge 3.2 Extended merge 3.3 Reverse 3.4 Relocate 4 Metaheuristics 4.1 Tabu search-variable neighborhood search 4.1.1 Variable neighborhood search 4.1.2 Tabu search 4.2 Greedy randomized adaptive search procedure 4.3 Tabu search-greedy randomized adaptive search procedure 4.3.1 Greedy randomized adaptive search procedure 4.3.2 Tabu search 5 Computational results 5.1 Implementation and benchmark instances 5.2 Comparative analysis 6 Summary and conclusions References NAGY 2007 649 672 G LOPES 2013 795 822 R GHIANI 2001 151 159 G GOLDEN 1981 305 315 B PEARN 1987 285 288 W BALDACCI 2006 52 60 R LONGO 2006 1823 1837 H WOHLK 2008 29 48 S VEHICLEROUTINGPROBLEMLATESTADVANCESNEWCHALLENGES ADECADECAPACITATEDARCROUTING LEVY 1989 74 94 L LAPORTE 1988 163 198 G VEHICLEROUTINGMETHODSSTUDIES LOCATIONROUTINGPROBLEMS GHIANI 1999 291 302 G PIA 2006 125 141 A AMAYA 2007 45 53 A 1990 DISCRETELOCATIONTHEORY HERTZ 2003 247 252 A CLARKE 1964 568 581 G BELENGUER 2006 3363 3383 J PRINS 2006 221 238 C BEULLENS 2003 629 643 P LIN 1973 498 516 S SAVELSBERGH 1992 146 154 M 2003 HANDBOOKMETAHEURISTICS TALBI 2009 E METAHEURISTICSDESIGNIMPLEMENTATION MLADENOVIC 1997 1097 1100 N HANSEN 2001 449 467 P POLACEK 2008 405 423 M TAILLARD 1997 170 186 E FILHO 1998 189 209 V GLOVER 1986 533 549 F BARRETO 2007 968 977 S RESENDE 2003 219 249 M HANDBOOKMETAHEURISTICS GREEDYRANDOMIZEDADAPTIVESEARCHPROCEDURES DIJKSTRA 1959 269 271 E GOLDEN 1983 47 59 B BENAVENT 1992 669 690 E EGLESE 1994 231 244 R LOPESX2014X309 LOPESX2014X309X317 LOPESX2014X309XR LOPESX2014X309X317XR item S0305-0548(13)00298-0 S0305054813002980 10.1016/j.cor.2013.10.003 271709 2013-12-03T04:43:41.701828-05:00 2014-03-01 2014-03-31 true 1355243 MAIN 9 62354 849 656 IMAGE-WEB-PDF 1 si0183 123 10 10 si0182 148 13 13 si0181 131 15 10 si0180 138 8 13 si0179 133 10 10 si0178 128 10 10 si0177 131 10 12 si0176 151 13 14 si0175 376 11 67 si0174 167 10 17 si0173 540 14 91 si0172 167 10 17 si0171 248 10 35 si0170 425 14 67 si0169 300 11 44 si0168 352 13 53 si0167 763 17 165 si0166 248 10 35 si0165 167 10 17 si0164 358 11 72 si0163 208 11 29 si0162 132 10 10 si0161 158 13 15 si0160 246 13 38 si0159 167 10 17 si0158 503 13 105 si0157 409 12 70 si0156 166 10 18 si0155 186 11 20 si0154 300 11 44 si0153 167 10 17 si0152 316 10 69 si0151 352 11 71 si0150 332 10 69 si0149 322 11 51 si0148 405 14 65 si0147 167 10 17 si0146 120 13 9 si0145 246 13 38 si0144 167 10 17 si0143 158 13 15 si0142 132 10 10 si0141 266 14 56 si0140 208 11 29 si0139 285 14 53 si0138 144 11 12 si0137 144 11 12 si0136 144 11 12 si0135 431 16 73 si0134 409 16 63 si0133 455 14 76 si0132 317 16 47 si0131 300 11 44 si0130 261 10 40 si0129 167 10 17 si0128 272 16 38 si0127 422 16 74 si0126 167 10 17 si0125 158 13 15 si0124 132 10 10 si0123 156 12 15 si0122 193 13 36 si0121 216 13 43 si0120 122 10 10 si0119 156 12 15 si0118 156 12 15 si0117 111 11 6 si0116 158 13 15 si0115 288 8 59 si0114 274 8 46 si0113 162 13 14 si0112 246 13 38 si0111 167 10 17 si0110 319 9 57 si0109 513 15 94 si0108 132 10 11 si0107 296 15 42 si0106 513 15 94 si0105 111 11 6 si0104 128 11 9 si0103 118 13 8 si0102 111 10 6 si0101 122 10 10 si0100 111 9 7 si0099 147 12 13 si0098 124 10 9 si0097 133 10 10 si0096 112 8 7 si0095 114 8 8 si0094 154 13 14 si0093 155 13 14 si0092 1135 16 361 si0091 111 6 9 si0090 138 8 13 si0089 122 10 10 si0088 206 13 32 si0087 175 13 20 si0086 634 15 163 si0085 461 15 108 si0084 661 17 170 si0083 703 23 139 si0082 1272 27 327 si0081 877 25 213 si0080 633 27 136 si0079 1039 29 249 si0078 656 23 134 si0077 628 22 160 si0076 1516 27 334 si0075 118 13 8 si0074 202 10 30 si0073 175 13 20 si0072 118 13 8 si0071 150 13 12 si0070 206 11 31 si0069 209 14 30 si0068 163 11 19 si0067 159 12 16 si0066 156 14 17 si0065 117 8 9 si0064 156 14 17 si0063 117 8 9 si0062 124 10 9 si0061 124 10 9 si0060 124 10 9 si0059 118 10 9 si0058 124 10 9 si0057 124 10 9 si0056 147 12 17 si0055 124 10 9 si0054 156 14 17 si0053 141 13 12 si0052 124 10 9 si0051 151 13 14 si0050 202 10 30 si0049 134 10 13 si0048 146 13 13 si0047 143 14 11 si0046 267 13 49 si0045 143 14 11 si0044 133 10 10 si0043 242 16 43 si0042 142 14 11 si0041 296 16 65 si0040 120 13 9 si0039 225 12 43 si0038 133 10 10 si0037 158 12 17 si0036 141 13 12 si0035 332 17 61 si0034 130 10 11 si0033 133 10 10 si0032 120 13 9 si0031 118 13 8 si0030 111 10 6 si0029 130 10 11 si0028 151 13 14 si0027 118 13 8 si0026 111 10 6 si0025 123 10 10 si0024 148 13 13 si0023 135 10 11 si0022 133 10 10 si0021 155 13 14 si0020 134 10 13 si0019 132 10 11 si0018 267 13 49 si0017 178 13 26 si0016 152 16 12 si0015 145 17 12 si0014 205 13 34 si0013 138 8 13 si0012 120 13 9 si0011 131 10 12 si0010 132 10 11 si0009 131 10 12 si0008 300 13 61 si0007 123 10 10 si0006 148 13 13 si0005 131 15 10 si0004 138 8 13 si0003 159 13 15 si0002 157 13 15 si0001 159 13 17 gr8 49353 397 376 gr7 29076 326 263 gr6 29735 426 226 gr5 23201 241 263 gr4 32351 207 509 gr3 13456 163 378 gr2 23380 265 550 gr1 23191 165 532 gr8 5355 164 155 gr7 2991 164 132 gr6 2250 164 87 gr5 3610 164 179 gr4 2854 89 219 gr3 1716 94 219 gr2 1952 105 219 gr1 2053 68 219 CAOR 3436 S0305-0548(13)00298-0 10.1016/j.cor.2013.10.003 Elsevier Ltd © 2013 Elsevier Ltd. All rights reserved. Fig. 1 Augment (left) and merge (right) moves in augment-merge [3]. Fig. 2 Some merges in the merge phase of the EAM and EM algorithms. Fig. 3 The reverse move applied to a route. Fig. 4 Steps of the basic VNS [26]. Fig. 5 Overview of the proposed TS–VNS metaheuristic for the LARP. Fig. 6 Overview of the proposed GRASP metaheuristic for the LARP. Fig. 7 Overview of the proposed TS–GRASP metaheuristic for the LARP. Fig. 8 Graphical representation of an optimal solution for instance “gdb-1”. Table 1 Data concerning the proposed instances for the LARP. Instance | V | | E | | R | m f ¯ Q F 1 gdb-20 11 22 22 3 10 27 1 2 gdb-21 11 33 33 3 8 27 1 3 gdb-22 11 44 44 3 5 27 1 4 gdb-23 11 55 55 3 10 27 1 5 gdb-1 12 22 22 3 20 5 5 6 gdb-2 12 26 26 3 25 5 5 7 gdb-12 13 23 23 3 50 35 5 8 gdb-5 13 26 26 3 20 5 2 9 bccm-2A 24 34 34 5 20 180 5 10 bccm-2C 24 34 34 5 50 40 2 11 bccm-3A 24 35 35 5 15 80 2 12 bccm-3C 24 35 35 5 25 20 5 13 bccm-1B 24 39 39 5 15 120 2 14 bccm-1C 24 39 39 5 25 45 8 15 bccm-8A 30 63 63 5 10 200 5 16 bccm-6B 31 50 50 5 40 120 5 17 bccm-5A 34 65 65 5 20 220 5 18 bccm-5C 34 65 65 5 30 130 2 19 bccm-7A 40 66 66 5 5 200 2 20 bccm-4C 41 69 69 5 25 130 2 21 bccm-9B 50 92 92 5 30 175 5 22 bccm-10D 50 97 97 5 20 75 5 23 eglese-E1-A 77 98 51 10 250 305 20 24 eglese-E2-B 77 98 72 10 750 200 20 25 eglese-E3-C 77 98 87 10 1000 135 50 26 eglese-E4-B 77 98 98 10 650 180 75 27 eglese-S1-A 140 190 75 10 400 210 50 28 eglese-S2-B 140 190 147 10 2500 160 100 29 eglese-S3-C 140 190 159 10 1500 120 100 30 eglese-S4-A 140 190 190 10 1000 230 100 Table 2 Average and median results for the EAM and EM constructive methods with and without LS. Method CPU (s) GapLB (%) Average Median Average Median EAM 0.02 0.01 41.48 37.91 EAM+LS 0.02 0.01 38.18 35.93 EM 0.12 0.02 39.74 39.94 EM+LS 0.12 0.02 38.48 37.22 Table 3 Results for the TS–VNS, the GRASP (using EAM and EM) and the TS–GRASP metaheuristic approaches. Instance LB TS–VNS GapLB GRASP–EAM GapLB GRASP–EM GapLB TS–GRASP GapLB Best CPU Best CPU Best CPU Best CPU 1 gdb-20 135 139 0.36 2.96 135 0.03 0.00 135 0.10 0.00 135 0.04 0.00 2 gdb-21 170 176 0.77 3.53 173 0.08 1.76 173 0.28 1.76 171 0.13 0.59 3 gdb-22 210 216 1.38 2.86 215 0.29 2.38 213 0.74 1.43 213 0.29 1.43 4 gdb-23 247 264 1.23 6.88 258 0.57 4.45 254 1.22 2.83 254 0.52 2.83 5 gdb-1 353 359 0.61 1.70 353 0.02 0.00 359 0.10 1.70 353 0.05 0.00 6 gdb-2 379 400 0.47 5.54 400 0.04 5.54 390 0.17 2.90 390 0.11 2.90 7 gdb-12 532 607 0.35 14.10 607 0.16 14.10 543 0.09 2.07 537 0.05 0.94 8 gdb-5 397 419 0.43 5.54 419 0.03 5.54 407 0.16 2.52 405 0.13 2.02 9 bccm-2A 247 249 2.02 0.81 255 0.05 3.24 247 0.40 0.00 247 0.18 0.00 10 bccm-2C 358 384 0.77 7.26 384 0.04 7.26 370 0.35 3.35 370 0.17 3.35 11 bccm-3A 96 99 1.17 3.13 97 0.06 1.04 96 0.45 0.00 96 0.20 0.00 12 bccm-3C 158 174 0.98 10.13 172 0.04 8.86 166 0.44 5.06 162 0.16 2.53 13 bccm-1B 209 220 1.19 5.26 216 0.09 3.35 211 0.59 0.96 211 0.39 0.96 14 bccm-1C 310 348 1.40 12.26 340 0.07 9.68 332 0.53 7.10 325 0.24 4.84 15 bccm-8A 404 433 5.41 7.18 427 0.56 5.69 425 3.07 5.20 424 0.97 4.95 16 bccm-6B 320 335 1.62 4.69 336 0.14 5.00 329 1.38 2.81 329 0.64 2.81 17 bccm-5A 441 464 2.95 5.22 459 0.56 4.08 456 3.02 3.40 450 1.41 2.04 18 bccm-5C 448 488 2.68 8.93 475 0.53 6.03 462 2.88 3.13 463 1.04 3.35 19 bccm-7A 290 304 4.69 4.83 302 0.44 4.14 297 3.01 2.41 298 1.18 2.76 20 bccm-4C 454 467 5.51 2.86 458 0.52 0.88 458 3.35 0.88 458 1.36 0.88 21 bccm-9B 392 415 5.60 5.87 413 1.30 5.36 406 9.52 3.57 408 4.78 4.08 22 bccm-10D 521 553 6.51 6.14 549 1.63 5.37 552 11.17 5.95 544 3.83 4.41 23 eglese-E1-A 2716 3256 6.16 19.88 3231 0.43 18.96 3014 2.07 10.97 2964 2.14 9.13 24 eglese-E2-B 5070 5811 4.97 14.62 5856 0.87 15.50 5584 5.29 10.14 5469 2.20 7.87 25 eglese-E3-C 7727 9147 5.07 18.38 9123 1.42 18.07 8566 10.31 10.86 8665 4.13 12.14 26 eglese-E4-B 7148 8017 7.90 12.16 8100 1.52 13.32 7751 13.40 8.44 7669 6.52 7.29 27 eglese-S1-A 4586 8.06 4404 1.10 4378 8.16 4315 2.78 28 eglese-S2-B 15,820 33.21 15,592 6.76 15,307 55.04 15,069 22.64 29 eglese-S3-C 17,090 40.96 16,933 7.88 16,109 65.01 16,029 24.21 30 eglese-S4-A 12,722 24.29 12,491 10.76 11,977 97.13 11,879 52.35 Average 5.96 7.41 1.27 6.52 9.98 3.82 4.49 3.23 Median 2.35 5.70 0.44 5.37 1.73 2.87 0.81 2.79 Location-arc routing problem: Heuristic approaches and test instances Rui Borges Lopes a ⁎ Frank Plastria b Carlos Ferreira a Beatriz Sousa Santos c a Department of Economics, Management and Industrial Engineering – C.I.O., University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Economics, Management and Industrial Engineering – C.I.O., University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal b Department of Mathematics, Operational Research, Statistics and Information Systems for Management, Vrije Universiteit Brussel, Pleinlaan 2, B-1050 Brussel, Belgium Department of Mathematics, Operational Research, Statistics and Information Systems for Management, Vrije Universiteit Brussel Pleinlaan 2 Brussel B-1050 Belgium c Department of Electronics, Telecommunications and Informatics – I.E.E.T.A., University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunications and Informatics – I.E.E.T.A., University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal ⁎ Corresponding author. Tel.: +351 234370361. Location-routing is a branch of locational analysis that takes into account distribution aspects. The location-arc routing problem (LARP) considers scenarios where the demand is on the edges rather than being on the nodes of a network (usually a road network is assumed). Examples of such scenarios include locating facilities for postal delivery, garbage collection, road maintenance, winter gritting and street sweeping. This paper presents some heuristic approaches to tackle the LARP, as well as some proposals for benchmark instances (and corresponding results). New constructive and improvement methods are presented and used within different metaheuristic frameworks. Test instances were obtained from the capacitated arc routing problem (CARP) literature and adapted to address the LARP. Keywords Location-routing Arc routing Heuristics Test instances 1 Introduction Location-routing problems (LRP) deal with the combination of two types of decisions that often arise: the location of facilities and the design of the distribution routes. While most LRP papers address node routing (see for example [1,2]), one may consider several scenarios where the demand is on the edges rather than being on the nodes of a network (usually a road network is assumed). These problems are referred to in the literature as location-arc routing problems (LARPs), and are derived from the capacitated arc routing problem (CARP) [3]. The LARP is typically overlooked in the literature. It has been shown that node routing problems can be converted into arc routing problems (the capacitated vehicle routing problem – CVRP – can be transformed into the CARP [4]), and that the reverse is also possible, replacing each arc by three [5] or two vertices [6,7], making the two classes of problems equivalent (the same holds true for their location counterparts: the capacitated LRP and the LARP). Still, for the three transformations of the CARP into the CVRP, the resulting instance requires either fixing of variables or the use of edges with infinite cost. Moreover, the resulting CVRP graph is a complete graph of larger size. Hence, the transformation increases the problem size and the planar structure of a usual CARP graph is lost [8] dramatically changing the number of edges from linear to quadratic. The same can be extrapolated to the LARP, motivating its study using dedicated methods and algorithms. The first work on the LARP, by Levy and Bodin [9], intended to tackle a practical problem arising in the scheduling of postal carriers in the United States postal service. The developed algorithm used the location–allocation–routing (L–A–R) concept described by [10] for the LRP, which includes three steps: firstly, depots are to be located using a depot selection procedure; secondly, arcs with demand are to be allocated to depots; thirdly, an Euler tour route of minimum traverse cost is determined for each set of arcs allocated to depots. Ghiani and Laporte [11] addressed an undirected LARP, called the location rural postman problem, in which depots are to be located and routes to be drawn (serving edges with demand), at minimum cost, in an undirected graph. The authors show that the problem can be transformed into a rural postman problem if there is a single depot to open or no bounds on the number of depots. Using an exact branch-and-cut approach they solve the transformed problem. In subsequent work by Ghiani and Laporte [3] a set of common applications for the LARP is mentioned (mail delivery, garbage collection and road maintenance). Furthermore the authors define the LARP as an extension of one of the three classical arc routing problems: the Chinese postman problem, the rural postman problem, and the CARP. The authors also present some insight into heuristic approaches using the decomposition of the problem into location (L), allocation (A) and routing (R) [10]: location–allocation–routing (L–A–R) and allocation–routing–location (A–R–L). Muyldermans [12] presents a variant of the LARP: the p dead-mileage problem. In this problem, unlike the previously addressed LARPs, splitting of the demand is allowed, that is, the client can be serviced more than once. The objective is to minimize dead-mileage (deadheading) and the problem is solved exactly. Finally, the works by Pia and Filippi [13] and Amaya et al. [14] address variants of the CARP with a structure similar to the LARP, respectively, the CARP with mobile depots and the CARP with refill points. In the first, two different types of vehicles are considered: compactors and satellites. Compactors can be seen as mobile depots for the satellites. The second problem considers servicing of arcs by vehicles that must be refilled at certain nodes (to be determined) in order to complete the service. From the previously mentioned variants, the LARP addressed here is the one studied by Ghiani and Laporte [11] which can be seen as the arc routing equivalent to the capacitated LRP, and thus an extension to the CARP. In this paper some heuristic approaches are presented to tackle the LARP, as well as some proposals for benchmark instances (and corresponding results). Regarding the heuristic approaches new constructive and improvement methods are developed and used within different metaheuristic frameworks. The test instances were obtained from the CARP literature and adapted to address the LARP. The remainder of this paper is outlined as follows. In Section 2 a formal definition of the problem is given. Constructive methods and improvement heuristics are presented in Section 3, and used within different metaheuristic frameworks proposed in Section 4. The developed test instances are addressed in Section 5 as well as the corresponding computational results. Finally, conclusions and future research directions are presented in Section 6. 2 Problem definition The LARP consists of determining simultaneously depot location and routes in a graph in order to serve a specified set of required arcs under given operational constraints. Muyldermans [12] has shown that, for this problem, an optimal solution exists with the facilities located on the vertices of the graph. Formally, the LARP can be described on a weighted and directed graph G = ( V , A ) with vertex set V and set of arcs A . The vertex set V contains a non-empty subset J of m potential depot locations ( J ⊆ V ) with a fixed cost f j and an associated capacity b j ( j ∈ J ). Every arc a = ( i , j ) in the arc set A has a non-negative traversal cost c a and a non-negative demand for service d a . The arcs with positive demand form the subset R of the arcs required to be serviced, only once, by a fleet K of identical vehicles with capacity Q . Vehicles start and end their route in the same depot, and each new vehicle (or route, as it is assumed that each vehicle performs a single route) involves a fixed cost F . The movement from the end i of one required arc to the start j of another required arc without servicing the traversed arcs (either required or not) is known as “deadheading”, and has an associated cost denoted by z i j (usually the cost of the shortest path in G from i to j ). The problem aims to determine the set of depots to be opened in J and the tracing of the distribution routes assigned to each open depot in such a way that the sum of fixed and traversal costs to serve all arcs in R is minimized. Assuming G to be connected, it is possible to transform it into a complete graph G ^ = ( V ^ , A ^ ) where V ^ is composed of the set V R of vertices containing the extremities of the arcs in R ( V R ⊆ V ), and J ( V ^ = V R ∪ J ). As G ^ is a complete graph and V R ⊆ V ^ , R is a subset of A ^ . Each arc a = ( i , j ) in the arc set A ^ has a non-negative cost c ^ a which takes on the value c a if a ∈ R , z i j otherwise. For any subset S of vertices in V ^ , let δ + ( S ) ( δ − ( S )), be the set of arcs leaving (entering) S , and L ( S ) the set of arcs with both extremities in S . When S contains a single vertex v , δ + ( v ) is a simplification for δ + ( { v } ). The following binary variables are used: x a k , equal to 1 if and only if arc a ∈ A ^ is used in the route performed by vehicle k ∈ K ; y j , equal to 1 if and only if depot j is to be opened; and w a j , equal to 1 if and only if the arc a ∈ R is assigned to depot j . The LARP can be formulated as: (1) ( LARP ) min Z = ∑ j ∈ J f j y j + ∑ a ∈ A ^ ∑ k ϵ K c ^ a x a k + ∑ k ∈ K ∑ a ∈ δ + ( J ) F x a k (2) s . t . : ∑ k ∈ K x a k = 1 ∀ a ∈ R , (3) ∑ a ∈ R d a x a k ≤ Q ∀ k ∈ K , (4) ∑ a ∈ δ + ( i ) x a k − ∑ a ∈ δ − ( i ) x a k = 0 ∀ i ∈ V ^ , ∀ k ∈ K , (5) ∑ a ∈ δ + ( J ) x a k ≤ 1 ∀ k ∈ K , (6) ∑ a ∈ L ( S ) x a k ≤ | S | − 1 ∀ k ∈ K , ∀ S ⊆ V R , (7) ∑ b ∈ δ + ( j ) ∩ δ − ( V R ) x b k + x a k ≤ 1 + w a j ∀ a ∈ R , ∀ j ∈ J , ∀ k ∈ K , (8) ∑ a ∈ R d a w a j ≤ b j y j ∀ j ∈ J , (9) x a k ∈ { 0 , 1 } ∀ a ∈ A ^ , ∀ k ∈ K , (10) y j ∈ { 0 , 1 } ∀ j ∈ J , (11) w a j ∈ { 0 , 1 } ∀ a ∈ R , ∀ j ∈ J . The objective function (1) minimizes the sum of, respectively, the fixed costs of opening the depots, the costs of all traversed arcs, and the cost of acquiring vehicles. Constraints (2) ensure that each required arc is serviced once by exactly one vehicle. Capacity constraints are satisfied thanks to inequalities (3) and (8). Equalities (4) are the flow conservation constraints which, coupled with constraints (5), ensure the routes return to the departure depot. Constraints (6) are subtour elimination constraints while the set of constraints (7) specify that a required arc must be assigned to a depot in case there is a route linking them. Finally, constraints (9)–(11) define the variables. Note that integrality of w a j can be relaxed to [ 0 , 1 ] because if not pushed to 1 by (7) minimization will choose for 0 due to (8). It can be noted that the LARP considered here can be seen as an extension of the CARP, where multiple depots are considered and an additional level of decision is for locating the depots. 3 Constructive methods and improvement heuristics As the LARP results from the combination of a facility location problem and the CARP, both NP-hard problems [15,4], it is NP-hard. As a consequence, large instances can hardly be solved using exact methods; moreover, sharp bounds on the optimal value are typically hard to obtain. The best way to tackle these problems is then to use heuristic approaches [16] such as constructive methods and improvement heuristics. Constructive methods are commonly used to obtain initial solutions from which improvement heuristics seek to attain better ones. Furthermore, these approaches are often used as the first step to many metaheuristics. In this section constructive methods (extended augment-merge and extended merge) and improvement heuristics (reverse and relocate with both intra- and inter-route moves) are proposed to tackle the LARP. 3.1 Extended augment-merge The augment-merge algorithm (illustrated in Fig. 1) was proposed for the CARP by Golden and Wong [4]. It starts with a trivial solution in which each arc with demand is serviced by a separate route. Then, after sorting the obtained routes in decreasing cost order and starting with the route with highest cost, it proceeds to the augment phase testing if the route already goes through demand arcs on less costly routes. If so, and provided vehicle capacity is preserved, the latter route is augmented into the former (the route with highest cost). Afterwards the algorithm proceeds to the merge phase, where every feasible merge is evaluated for any two routes, merging the routes that provide the highest saving. This is done until no feasible saving exists. This last step is closely related to the well-known “savings” or Clarke and Wright algorithm [17]. The extended version (extended augment-merge – EAM) for the LARP obtains the initial solution by assigning, sequentially, each required arc to the closest depot in which it can fit, thus building a dedicated route. When all required arcs are assigned, the depots without demand to supply are closed. The augment phase is similar to the augment-merge algorithm, increasingly considering depot capacity constraints. In the merge phase of the EAM, the resulting route T , which may result from four different merges, is evaluated for reassignment to all depots (totaling 4 m possible merges for each pair of routes). The resulting saving σ can be calculated as follows: (12) σ = F + z r i + z j r + z s k + z l s − z j k − z t i − z l t + f r θ r + f s θ s − f t ( 1 − y t ) . θ r ( θ s ) is binary and equal to 1 if depot r ( s ), the depot of route R ( S ), supplies no more routes after the merger, and thus can be closed. y t is a binary value (defined earlier for the formulation) equal to 1 if depot t (the depot currently evaluated to be assigned to T ) is already open before the merge, and i , j , k , and l are the vertices of the arcs with demand which are connected to the depots in each route. This saving can be seen depicted in Fig. 2. The EAM ends when there is no more feasible merge with a positive saving. The time complexity of this method is O ( m | E | 2 log | E | ) due to the merge phase, where the list of ( m | E | 2 ) possible merges can be sorted using heapsort (the actual merging of the two routes can be done in linear time). 3.2 Extended merge In the augment-merge algorithm the use of the augment phase has been contested [18]. If all the arcs in A are required to be serviced, it performs well, as the arcs absorbed by the higher cost routes are often contiguous. However, if this is not the case, the deadheading distance created between absorbed arcs cannot be recovered in the merge phase, leading to degraded results. Belenguer et al. [18] further support this statement by presenting overall better results for the algorithm without the augment phase. As the EAM derives from the augment-merge, a similar situation may occur. This suggests the development of an extended merge (EM) algorithm for the LARP, similar to the EAM, but without performing the augment phase. The merge phase is used in both the EAM and the EM algorithms. The extension of the merge phase (in both algorithms) uses some concepts from the extension to the savings algorithm proposed by Prins et al. [19] for the capacitated LRP. Similarly to the EAM method, EM can be executed in O ( m | E | 2 log | E | ) time. 3.3 Reverse The reverse improvement heuristic [20] performs inside the routes and the corresponding move can be seen as the arc routing equivalent of the 2-opt move [21]. The reverse move is identical to the one used in the CARP ( Fig. 3). The algorithm replaces a subsequence of arcs by the reverse, always ensuring the required arcs are serviced. This may lead to other shortest paths (in the deadheading distance) linking the required arcs. The algorithm implements the first found feasible move that improves the solution. This is done sequentially until no more feasible moves can be found. 3.4 Relocate In the relocate improvement heuristic (well-known from the node routing context) two possible variations are considered: relocate inside the routes and relocate between two routes. In both variations the concept is to relocate a given arc (or subsequence of arcs) that requires service to another position in the route, or in another route. The relocate algorithm inside the routes is based on the CVRP algorithm by Savelsbergh [22], while the inter-route algorithm is based on the work by Beullens et al. [20] for the CARP. In the latter, not only vehicle capacity, but also depot capacity constraints have to be taken into account. In both algorithms the (subsequence of) required arc(s) or its reversal is reinserted, depending on which of the two provides the largest improvement. 4 Metaheuristics Metaheuristics are general combinatorial optimization techniques most useful and efficient in solving hard problems [23,24]. While in theory they can handle any combinatorial optimization problem, it is often the case that an important effort must be put on finding the right way to adapt the general parameters of these methods to a particular problem [16]. This is the case in this section, where three general metaheuristics (tabu search combined with a variable neighborhood search – TS–VNS; greedy randomized adaptive search procedure – GRASP; and tabu search combined with a greedy randomized adaptive search procedure – TS–GRASP) are adapted and parameters tuned in order to tackle the LARP. The metaheuristics use the previously developed constructive and improvement methods in their specific framework. 4.1 Tabu search-variable neighborhood search This approach (TS–VNS) is an iterative framework composed of a tabu search (TS) and a variable neighborhood search (VNS), respectively, for the location and (arc) routing phases. These two algorithms are performed iteratively until a stopping criterion is met, namely, a number m a x t s v n s of iterations without improvements to the solution (empirically found to be equal to 10). The TS–VNS approach starts by obtaining a solution using the VNS without constraints on the subset of depots S D to use ( S D ⊆ J ). 4.1.1 Variable neighborhood search VNS is a metaheuristic proposed by Mladenović and Hansen [25] in which the main concept is to perform a systematic change of neighborhood within the local search, exploring increasing neighborhoods of the current solution. If an improvement is found, the search proceeds to the new solution and restarts. The steps of the basic VNS can be seen in Fig. 4. In the adopted VNS: N l is a finite set of pre-selected neighborhood structures inspired in the work by Polacek et al. [27] for the CARP; the initial solution is obtained by performing the EAM (preferred to EM since it is faster to obtain solutions, as the following steps behave similarly in both methods); and the stopping condition is a given number m a x v n s of iterations reached (equal to the number of arcs in the problem, multiplied by ten: m a x v n s = 10 | A | ). The shaking step uses the CROSS-exchange operator proposed by Taillard et al. [28] for the VRP with time windows. It starts by randomly selecting two different routes, to which the CROSS-exchange operator is applied, swapping the sequence of consecutive required arcs. The number of required arcs which get swapped on each route is randomly obtained from an uniform distribution in the range [1, min( l , R T )], R T being the total number of required arcs for route T . When l = l m a x ( l m a x = 6, found empirically) the upper bound on the range is substituted by R T . The described shaking step is biased to exchange smaller sequences of required arcs, while still allowing to perform large swaps. The local search is applied on the two changed routes only, and is composed of the formerly proposed reverse and relocate improvement heuristics, performing intra-route (reverse and relocate) and inter-route (relocate) improvements, sequentially, until no additional improvement can be found. The newly obtained local optimum is only moved to when a cost reduction is obtained, thus making this a descent first improvement procedure which, according to Hansen and Mladenović [26], can be easily transformed into a descent–ascent method (although experimental trials with this variant proved unfruitful for the present approach). At each iteration i t , after obtaining the best solution S i t from the VNS using the subset of depots S D ( V N S ( S D , S i t ) ), the TS–VNS approach proceeds to the location phase, performed by the TS algorithm. 4.1.2 Tabu search The used TS algorithm is the one presented by Filho and Galvão [29], for the concentrator location problem, which provides near-optimal results in low CPU time, validating its use. TS was proposed by Glover [30] and has become one of the most widespread local search methods for combinatorial optimization. It uses a working memory called tabu list in which some attributes are stored (and forbidden to be used) for a number of moves. In the used TS algorithm some adaptations were made to handle the LARP. Each route in the current best solution is collapsed into a single client (regarding demand), and the distance to the several feasible depots is the smallest insertion cost of the depot in the route, as in Barreto et al. [31]. The problem thus becomes a facility location problem and the algorithm tries to obtain the best depot location for the current routes ( T S ( S i t ) ). Using the best obtained depot configuration (excluding the remaining depots from the problem), the TS–VNS approach proceeds to constructing the routes (routing phase) using the abovementioned VNS algorithm. If no improvement was found in the last (five) iteration(s), the approach provides some diversification by randomly opening one (two) depot(s) and closing another from S D , always observing depot capacity constraints. An overview of the pseudo code of the TS–VNS approach can be seen in Fig. 5. i t t s v n s , B e s t S o l , c o s t ( S i t ) , c o s t ( B e s t S o l ) , d e p o t s ( S i t ) respectively refers to the counter of successive iterations without improvement of the best solution, the best solution found, the cost of the current solution, the cost of the best solution found, and the depots to be opened in the current solution. 4.2 Greedy randomized adaptive search procedure The EAM and EM are randomized to provide the greedy randomized search procedure producing the GRASP–EAM and GRASP–EM. The following features are valid for both variations and apply some concepts of the GRASP developed by Prins et al. [19] for the capacitated LRP. For the randomized version of the constructive algorithms ( R C A ( S D , S i t ) ) a restricted candidate list (RCL) of size φ is created from the savings calculated during the merge evaluations. The RCL contains the pairs of routes providing the best φ savings, from which one is randomly chosen to be performed. Changing the size of the RCL during the GRASP has been shown to often give better results [32]. So, at each merge, φ is randomly selected in [ 1 , φ m a x ] , where φ m a x is the maximum RCL size allowed (found empirically: φ m a x = 7 ). The local search (LS), typically used in GRASP metaheuristics, is based on the reverse and relocate improvement heuristics previously presented, which performs intra-route (reverse and relocate) and inter-route (relocate) improvements, sequentially, until no additional improvement can be found. Moreover, a learning process was included in the GRASP which reduces the computational time and improves the final solution [19]. The constructive algorithms, at each iteration i t , provide a solution S i t which often has many open depots and, although the merges can close some, it may not be enough. In order to obtain better solutions, a subset S D of available depots is chosen to be used in the constructive algorithms ( S D ⊆ J ). In the first iteration of the GRASP all depots are used. Then, at each iteration, one of them is iteratively picked from J and the remaining depots in S D are randomly chosen (as well as their number) – c h o o s e ( S D ) – always ensuring there is enough capacity to serve all clients. This can be seen as a diversification. Adding a memorization during the diversification mode enables the GRASP to learn about the good subset to open, and to possibly find better solutions. An intensification mode using this learning process was implemented, varying the GRASP iterations (using the boolean value d i v m o d e and reaching the maximum of m a x i t = 75 ) between: • Diversification mode – applied for m a x d i v = 8 iterations, in which the solution space is explored by varying the subset of open depots (as explained previously). • Intensification mode – performed for m a x i n t = 7 iterations, where an attempt is made to improve the routing for the selected depots ( S D ), obtained from the currently best found solution. The parameters were set after a preliminary experimenting phase and allow five complete runs of the two modes returning, in the end, the best found solution ( B e s t S o l ). Fig. 6 provides an overview of the GRASP approach, where d i v and i n t are the counters of successive iterations in, respectively, diversification mode and intensification mode. 4.3 Tabu search-greedy randomized adaptive search procedure Attempting to integrate the best features of the previously described TS and GRASP, a TS–GRASP approach was developed. The TS handles the location phase while the GRASP addresses the (arc) routing phase. An iterative framework is used, in which the algorithms for both phases are performed iteratively until a stopping criterion is met (a number m a x t s g r a s p of iterations without improvements to the solution, empirically found: m a x t s g r a s p = 10 ). The required adaptations will be described below. Similarly to the TS–VNS approach, the TS–GRASP starts by obtaining a solution for the routing phase without constraints on the subset of depots S D to use ( S D ⊆ J ), however, instead of the VNS, the following GRASP is used. 4.3.1 Greedy randomized adaptive search procedure This GRASP uses the EM for the randomized constructive algorithm, as it provides bigger diversity and better results, returning a solution S i t at each iteration i t . The randomization performs similarly, using the RCL (with the same value of φ m a x ), as does the LS, with the reverse and relocate improvement heuristics (applied sequentially as long as there is improvement to the solution). Unlike the previous GRASP however, there is no diversification mode ( m a x d i v = 0 ), as the TS handles the location phase. Hence, this GRASP tries to obtain the best results (always in intensification mode) for the fixed depot configuration S D . The m a x i t parameter also differs from the previous approach, varying it at each iteration according to: (13) m a x i t = ⌈ 0.25 i t t s g r a s p ⌉ + 1 where i t t s g r a s p is the number of iterations without improvement. This further intensifies the search when fewer improvements are found (ensuring the GRASP is performed at least once). The best obtained solution ( B e s t S o l ) is used in the location phase (TS algorithm: T S ( B e s t S o l ) ) with which the TS–GRASP approach proceeds once the maximum number of iterations m a x i t is reached. 4.3.2 Tabu search The TS algorithm in this approach works exactly as in the TS–VNS. Routes are collapsed into a single client and distances to depots are updated. Using the algorithm by Filho and Galvão [29], the best subset S D of depots to open is obtained ( d e p o t s ( B e s t S o l ) ), to which a small diversification is then added: observing capacity constraints, if no improvement was found in the last (five) iteration(s), one (two) depot(s) is (are) randomly chosen to be opened and one to be closed. The TS–GRASP approach then continues to the routing phase, restarting the GRASP and using only the new subset of depots S D , as seen in Fig. 7. In the figure, n e w b e s t s o l is a boolean value indicating whether a new best solution was found in the routing phase. 5 Computational results In order to ascertain the performance of the proposed methods and approaches for the LARP, experimental results were obtained. First, we discuss implementation and propose a new set of benchmark instances. This is followed by a comparative analysis on the algorithmic proposals using the newly devised instances. 5.1 Implementation and benchmark instances All the aforementioned approaches were implemented in C♯ and the results obtained using a 3.00GHz Intel Xeon E5450 Quad Core CPU with 8 GB of RAM and Windows XP (without parallel processing). To obtain the deadheading distances ( z i j ), in both the constructive methods and improvement heuristics (and consequently in the metaheuristic approaches), the well-known Dijkstra algorithm [33] was used. A new set of instances is proposed in order to compare results and times (justified by the absence of benchmark instances in the literature). The instances were drawn from the CARP literature (the original sets can be found in and adapted to the LARP. These are the well-known and widely used instances from: Golden et al. [34], named “gdb”; Benavent et al. [35], called “bccm”; and Eglese [36], named “eglese”. The first two sets were generated on a graph, with all edges being required, while the latter originates from a real-world (winter gritting) problem for the road network of Lancashire (UK). In the latter set of instances two graphs were obtained and different instances created by changing the set of required edges and the capacities of the vehicles. From the previously mentioned CARP instances, some were chosen (promoting diversity) and adapted to support more than one depot and a cost structure in which location costs range from 30% to 70% of the total cost (with deadheading distance used as distribution costs). Table 1 displays basic data about the proposed instances named after the original CARP set followed by the instance number/name in the original set (e.g. eglese-E1-A is the instance “E1-A” from [36]). The first columns of the table display the name of the instance and the number of the vertices ( V ), edges ( E ), and required edges ( R ) (all instances use undirected graphs). Then follows the number of potential depot locations ( m ) and the average depot fixed cost ( f ¯ ). Finally, columns “ Q ” and “ F ” refer, respectively, to the vehicles capacity and fixed cost. The proposed instances are available at 5.2 Comparative analysis Results for the constructive methods (with and without the local search – LS – consisting of the improvement heuristics applied sequentially until no more improvements can be found) and for the metaheuristic approaches were obtained for the newly devised instances. For the constructive methods, a single run for each instance is performed (as there is no randomization), and the corresponding result and computing time found. For the metaheuristic approaches, twenty runs were made for each instance, from which the average and best result (and the time to obtain it) were acquired. Average and median results for the EAM and EM constructive methods (with and without LS) can be seen in Table 2: the first column shows the method name, followed by average and median values of CPU times (in seconds) and gap to lower bounds GapLB (in percentage). The lower bounds were obtained by running the optimization solver CPLEX 12 where constraints (6) (see Section 2) were relaxed, including them iteratively, as they would get violated, until running out of memory (typically running for a few days). Table 3 shows the results regarding the metaheuristic approaches (TS–VNS, GRASP–EAM, GRASP–EM, and TS–GRASP): the first columns display the instances name, followed by the lower bounds (LB). For each algorithm are also shown: the best results – Best; CPU time, in seconds; and the gap (GapLB), in percentage, between the lower bound and the algorithm best result. Average and median values are provided for CPU time and GapLB, as skewed distributions and/or outlying data points were obtained. Looking at the results for the constructive methods, it can be concluded that, overall, EM performs better than EAM. This may lead to conclude that the claim by Belenger et al. [18] for the CARP, suggesting the augment phase generally leads to poorer results, is valid for the LARP. However, as the EAM obtains faster results (and with somewhat similar final results) it may be an interesting constructive method to be used in metaheuristics. Regarding the metaheuristic approaches, computing times are, on average, less than 10s, the GRASP–EAM being the fastest, followed by the TS–GRASP, the TS–VNS, and the GRASP–EM (although the difference among them is negligible as this is a strategic problem). The TS–VNS approach presents the worst results which, looking at the results of the TS–GRASP, suggest that this is due to the VNS (used in the routing phase) not performing as well as the GRASP on the route building/improvement. Comparing the results of the two GRASP (GRASP–EAM and GRASP–EM), GRASP–EM performs better by providing an average as well as a median improvement of around 2.5%. This may be due to the approach allowing a greater diversity in the possible merges by not having an augment phase, thus easing the finding of better solutions. Looking at the TS–GRASP, when compared with the GRASP–EM results, it can be concluded that the TS, used in the location phase, performs better than the diversity mechanism used in the choosing of open depots in the GRASP (as the route building works similarly). Moreover, this difference is further stressed in the last eight instances, which have more possible depot locations. Thus, as the TS–GRASP has a better location algorithm, it does not waste so much time trying to obtain the best depot configuration, resulting in smaller computing times and allowing further intensification of the routing phase (enabling to find better results). A graphical representation of an optimal solution for instance “gdb-1” can be seen in Fig. 8, where routes are displayed using different colors and a single depot is to be installed (in node 12). 6 Summary and conclusions In this paper heuristic approaches to tackle the LARP, as well as some proposals for benchmark instances (and corresponding results) are presented. Regarding the heuristic approaches new constructive and improvement methods are proposed and used within different metaheuristic frameworks (two constructive methods, three heuristic improvements and, using these, four metaheuristic approaches). Due to the lack of benchmark instances in the literature a new set of instances was devised, adapted from the CARP literature. Concerning the proposed heuristics, the TS–GRASP outperformed the other approaches results wise, and was extremely competitive regarding computing times. A limitation of this work is that conclusions on the proposed approaches were based on the results obtained for the newly devised set of benchmark instances which, in some cases, have high gap values; yet, the computational analysis of the results allowed to validate the proposed instances as they appear to be balanced (regarding location and routing costs) and representative of several different cost configurations. Notice that the proposed approaches use the same local search methods (although within different frameworks), and thus the obtained upper bounds may be not entirely independent. On the other hand, the quality assessment of the presented results is based upon the lower bound values obtained using a commercial software package. As future work, the development of new sets of benchmark instances may prove useful, either based on real-world problems, or generated using a structure that allows to draw conclusions on which real-world situations may be tackled with a specific approach. Finally, new methods for obtaining lower and upper bounds to tackle the LARP should be developed as this is a highly applicable problem which requires dedicated methods and algorithms. References [1] G. Nagy S. Salhi Location-routing: issues, models and methods Eur J Oper Res 177 2 2007 649 672 [2] R.B. Lopes C. Ferreira B.S. Santos S. Barreto A taxonomical analysis, current methods and objectives on location-routing problems Intl Trans Oper Res 20 6 2013 795 822 10.1111/itor.12032 [3] G. Ghiani G. Laporte Location-arc routing problems Opsearch 38 2 2001 151 159 [4] B.L. Golden R.T. Wong Capacitated arc routing problems Networks 11 3 1981 305 315 [5] W.L. Pearn A. Assad B.L. Golden Transforming arc routing into node routing problems Comput Oper Res 14 4 1987 285 288 [6] R. Baldacci V. Maniezzo Exact methods based on node-routing formulations for undirected arc-routing problems Networks 47 1 2006 52 60 [7] H. Longo M.Pd. Aragão E. Uchoa Solving capacitated arc routing problems using a transformation to the CVRP Comput Oper Res 33 6 2006 1823 1837 [8] S. Wøhlk A decade of capacitated arc routing B.L. Golden S. Raghavan E. Wasil The vehicle routing problem: latest advances and new challenges 2008 Springer New York 29 48 [9] L. Levy L. Bodin The arc oriented location routing problem INFOR 27 1 1989 74 94 [10] G. Laporte Location-routing problems B.L. Golden A.A. Assad Vehicle routing: methods and studies 1988 North-Holland Amsterdam 163 198 [11] G. Ghiani G. Laporte Eulerian location problems Networks 34 4 1999 291 302 [12] Muyldermans L. Routing, districting and location for arc traversal problems [Ph.D. thesis]. Leuven: Katholieke Universiteit Leuven; 2003. [13] A.D. Pia C. Filippi A variable neighborhood descent algorithm for a real waste collection problem with mobile depots Intl Trans Oper Res 13 2 2006 125 141 [14] A. Amaya A. Langevin M. Trépanier The capacitated arc routing problem with refill points Oper Res Lett 35 1 2007 45 53 [15] P.B. Mirchandani R.L. Francis Discrete location theory 1990 Wiley New York [16] A. Hertz M. Widmer Guidelines for the use of meta-heuristics in combinatorial optimization Eur J Oper Res 151 2 2003 247 252 [17] G. Clarke J. Wright Scheduling of vehicles from a central depot to a number of delivery points Oper Res 12 4 1964 568 581 [18] J.M. Belenguer E. Benavent P. Lacomme C. Prins Lower and upper bounds for the mixed capacitated arc routing problem Comput Oper Res 33 12 2006 3363 3383 [19] C. Prins C. Prodhon R. Wolfler Calvo Solving the capacitated location-routing problem by a GRASP complemented by a learning process and a path relinking 4OR 4 3 2006 221 238 [20] P. Beullens L. Muyldermans D. Cattrysse D. Van Oudheusden A guided local search heuristic for the capacitated arc routing problem Eur J Oper Res 147 3 2003 629 643 [21] S. Lin B.W. Kernighan An effective heuristic algorithm for the traveling salesman problem Oper Res 21 2 1973 498 516 [22] M. Savelsbergh The vehicle routing problem with time windows: minimizing route duration INFORMS J Comput 4 2 1992 146 154 [23] F.W. Glover G.A. Kochenberger Handbook of metaheuristics 2003 Kluwer Academic Publishers Norwell [24] E.G. Talbi Metaheuristics: from design to implementation 2009 Wiley Hoboken [25] N. Mladenović P. Hansen Variable neighborhood search Comput Oper Res 24 11 1997 1097 1100 [26] P. Hansen N. Mladenović Variable neighborhood search: principles and applications Eur J Oper Res 130 3 2001 449 467 [27] M. Polacek K.F. Doerner R.F. Hartl V. Maniezzo A variable neighborhood search for the capacitated arc routing problem with intermediate facilities J Heuristics 14 5 2008 405 423 [28] É. Taillard P. Badeau M. Gendreau F. Guertin J.Y. Potvin A tabu search heuristic for the vehicle routing problem with soft time windows Transp Sci 31 2 1997 170 186 [29] V.J.M.F. Filho R.D. Galvão A tabu search heuristic for the concentrator location problem Locat Sci 6 1 1998 189 209 [30] F. Glover Future paths for integer programming and links to artificial intelligence Comput Oper Res 13 5 1986 533 549 [31] S. Barreto C. Ferreira J. Paixão B.S. Santos Using clustering analysis in a capacitated location-routing problem Eur J Oper Res 179 3 2007 968 977 [32] M.G.C. Resende C.C. Ribeiro Greedy randomized adaptive search procedures F.W. Glover G.A. Kochenberger Handbook of metaheuristics 2003 Kluwer Academic Publishers Norwell 219 249 [33] E.W. Dijkstra A note on two problems in connexion with graphs Numerische Math 1 1 1959 269 271 [34] B.L. Golden J.S. DeArmon E.K. Baker Computational experiments with algorithms for a class of routing problems Comput Oper Res 10 1 1983 47 59 [35] E. Benavent V. Campos Á. Corberán E. Mota The capacitated chinese postman problem: lower bounds Networks 22 7 1992 669 690 [36] R.W. Eglese Routeing winter gritting vehicles Discrete Appl Math 48 3 1994 231 244 "
    },
    {
        "doc_title": "Extending the H-tree layout pedigree: An evaluation",
        "doc_scopus_id": "84893273871",
        "doc_doi": "10.1109/IV.2013.56",
        "doc_eid": "2-s2.0-84893273871",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Evaluation",
            "Family structure",
            "Genealogy",
            "H-tree layout",
            "Information visualization",
            "Space-filling",
            "User experience",
            "User performance"
        ],
        "doc_abstract": "Visualizing large family structures is becoming increasingly important, as more genealogical data becomes available. A space-filling h-tree layout pedigree has been recently proposed to make better use of the available space than traditional representations. In a previous paper we applauded the technique's usage of available space but remarked that it makes generation identification difficult and does not allow navigating to descendants of represented individuals. A set of extensions was proposed to help overcome these limitations and a preliminary evaluation suggested that those extensions enhance the original technique. This paper presents a more thorough evaluation carried out to assess if and how the proposed extensions improve the original h-tree layout pedigree technique. Results suggest that these extensions improve user performance on some tasks, effectively provide new functionality, and generally enhance user experience. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new approach for 3D craniometric measurements using 3d skull models",
        "doc_scopus_id": "84893271092",
        "doc_doi": "10.1109/IV.2013.61",
        "doc_eid": "2-s2.0-84893271092",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "3D manipulation",
            "3D models",
            "Acquisition process",
            "Craniometry",
            "Current problems",
            "Final objective",
            "Interactive interfaces",
            "Range acquisition"
        ],
        "doc_abstract": "This work presents an ongoing work on a new approach to perform craniometric analysis based on contactless 3D modelling of skulls. Beside the acquisition process with a 3D range sensor and initial results in the semi-automatic detection of features in the skulls, we also present some results in the development of a 3D interactive interface that eases interaction for users with little experience on digital 3D manipulation. The final objective is to provide an easy to use 3D interface to allow semi-automatic detection of features in skulls. It is our belief that this system might be the first step towards a new methodology for craniometric analysis that can solve several of the current problems such as repeatability, wide access to skull information or bone damage during measurements. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A taxonomical analysis, current methods and objectives on location-routing problems",
        "doc_scopus_id": "84885471121",
        "doc_doi": "10.1111/itor.12032",
        "doc_eid": "2-s2.0-84885471121",
        "doc_date": "2013-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Business and International Management",
                "area_abbreviation": "BUSI",
                "area_code": "1403"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Strategy and Management",
                "area_abbreviation": "BUSI",
                "area_code": "1408"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            },
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            }
        ],
        "doc_keywords": [
            "Algorithmic approach",
            "Location routing problem",
            "Location-routing",
            "Multi objective",
            "Second level",
            "Solution methods",
            "Structural characteristics"
        ],
        "doc_abstract": "Location-routing is a branch of locational analysis that takes into account distribution aspects. This paper proposes a taxonomy, with two levels, for location-routing problems. The first level focuses on the structural characteristics of the problems. The second level branches into the different algorithmic approaches and objective perspectives. An introduction to the previously defined problems is presented, categorising the papers in the literature (a total of 149 references) according to the proposed classification. Moreover, an overview of the most significant aspects of the different solution methods and main objectives, with special emphasis on multi-objective approaches, is provided. Some data providing a better insight into the publication progress are also included. Finally, several promising research directions are identified. © 2013 International Federation of Operational Research Societies Published by John Wiley & Sons Ltd, 9600 Garsington Road, Oxford, OX4 2DQ, UK and 350 Main St, Malden, MA02148, USA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Usability evaluation: A systematic review of the literature",
        "doc_scopus_id": "85034736842",
        "doc_doi": "10.4304/risti.11.31-43",
        "doc_eid": "2-s2.0-85034736842",
        "doc_date": "2013-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Controlled experiment",
            "Information and Communication Technologies",
            "Inspection methods",
            "Products and services",
            "Qualitative information",
            "Systematic Review",
            "Usability evaluation",
            "Usability testing"
        ],
        "doc_abstract": "The aim of this study is to identify, analyze and classify the methods described in the literature for the evaluation of the usability of products and services based on information and communication technologies. The methodology used was a systematic review of the literature. The studies included in the analysis were classified according to the models (empirical or analytical), methods (test, inquiry, inspection or controlled experiment) and techniques used. A total of 2116 studies were included in the survey, of which 1308 were classified. The inquiry method was the most frequent in this review, followed by the test method, the inspection method and, finally, the controlled experiment method. A combination of methods is relatively common, especially the combination of test and inquiry methods, probably because the use of the two allows to collect quantitative and qualitative information contributing to a more complete assessment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The role of ICTs in the management of rare chronic diseases: The case of hemophilia",
        "doc_scopus_id": "84898365353",
        "doc_doi": "10.4018/978-1-4666-3990-4.ch033",
        "doc_eid": "2-s2.0-84898365353",
        "doc_date": "2013-04-30",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            },
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder",
            "Chronic conditions",
            "Chronic disease",
            "Clinical practices",
            "Health care professionals",
            "Information and Communication Technologies",
            "Technological solution",
            "Web-based applications"
        ],
        "doc_abstract": "© 2013 by IGI Global. All rights reserved.This chapter outlines a study that examines the role of Information and Communication Technologies (ICTs) in management of a rare and chronic disease, hemophilia. Evidence in literature shows how the adoption of ICTs can improve the management of chronic conditions. Furthermore, these tools may also give response to rare diseases' needs, while greatly improving the quality of life of those patients. A Web-based application that was developed to facilitate communication between Healthcare Professionals (HCPs) and patients in a specific Hemophilia Treatment Center (HTC), to improve the utility and quality of clinical data and treatment information, as well as to help the management of resources involved in a specific rare chronic disease, represents a practical case presented in this chapter. This technological solution allows the management of inherited bleeding disorders, integrating, diffusing, and archiving large sets of data relating to the clinical practice of hemophilia care, more specifically the clinical practice at the Hematology Service of Coimbra Hospital Center.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Biological Activities and Chemical Characterization of Cordia verbenacea DC. as Tool to Validate the Ethnobiological Usage",
        "doc_scopus_id": "84946494090",
        "doc_doi": "10.1155/2013/164215",
        "doc_eid": "2-s2.0-84946494090",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Complementary and Alternative Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2707"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Knowledge of medicinal plants is often the only therapeutic resource of many communities and ethnic groups. \"Erva-baleeira\", Cordia verbenacea DC., is one of the species of plants currently exploited for the purpose of producing a phytotherapeutic product extracted from its leaves. In Brazil, its major distribution is in the region of the Atlantic Forest and similar vegetation. The crude extract is utilized in popular cultures in the form of hydroalcoholic, decoctions and infusions, mainly as antimicrobial, antiinflammatory and analgesic agents. The aim of the present study was to establish a chemical and comparative profile of the experimental antibacterial activity and resistance modifying activity with ethnopharmacological reports. Phytochemical prospecting and HPLC analysis of the extract and fractions were in agreement with the literature with regard to the presence of secondary metabolites (tannins and flavonoids). The extract and fraction tested did not show clinically relevant antibacterial activity, but a synergistic effect was observed when combined with antibiotic, potentiating the antibacterial effect of aminoglycosides. We conclude that tests of antibacterial activity and modulating the resistance presented in this work results confirm the ethnobotanical and ethnopharmacological information, serving as a parameter in the search for new alternatives for the treatment of diseases. © 2013 Edinardo Fagner Ferreira Matias et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "How visualization courses have changed over the past 10 years",
        "doc_scopus_id": "84880781470",
        "doc_doi": "10.1109/MCG.2013.57",
        "doc_eid": "2-s2.0-84880781470",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Computer graphics education",
            "Course contents",
            "Information visualization",
            "Liberal education",
            "Visual analytics",
            "Visual representations",
            "Visualization algorithms"
        ],
        "doc_abstract": "The past 10 years have seen profound changes in visualization algorithms, techniques, methodologies, and applications. These changes are forcing alterations to visualization courses. Unfortunately, outdated course content recommendations, together with profound changes in the underlying technology and methodology, are producing an unstable ground for educators at a time when visual representations are becoming increasingly important. To address this issue, educators held meetings or workshops at Siggraph 2011 and 2012 and a panel and workshop at Eurographics 2012. This article presents the insights gathered at these events. © 1981-2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improvement of surveillance of hemophilia treatment through ICTs",
        "doc_scopus_id": "84880819568",
        "doc_doi": "10.1109/EMBC.2012.6347332",
        "doc_eid": "2-s2.0-84880819568",
        "doc_date": "2012-12-14",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Chronic disease",
            "Coordination and management",
            "Effective management",
            "National health services",
            "Rare disease",
            "Specific component",
            "Technological solution"
        ],
        "doc_abstract": "Hemophilia, in addition of being a chronic disease, is also a rare disease, and as such, quite expensive for the National Health Service (NHS) due to the cost associated with the drugs used in treatments (Clotting Factor Concentrate - CFC). On the other hand, due to the specific characteristics of this type of disorder, it is necessary to ensure that data generated during the treatments are quickly communicated to the clinicians responsible for monitoring those patients. As such, an effective management of this disease, with maximum safety for patients, involves not only an efficient information management process, but also the coordination and management of all the associated resources. This article aims to present one specific component of a technological solution that can help in coordinate actions of patients, physicians and nurses, as well as improve the surveillance of hemophilia treatment, within a specific Comprehensive Hemophilia Diagnostic and Treatment Center (HTC). © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Implementation and evaluation of an enhanced H-tree layout pedigree visualization",
        "doc_scopus_id": "84867919811",
        "doc_doi": "10.1109/IV.2012.15",
        "doc_eid": "2-s2.0-84867919811",
        "doc_date": "2012-10-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Genealogical information",
            "Genealogy",
            "H-trees",
            "Information visualization",
            "pedigree",
            "Pedigree data",
            "Visualization method",
            "Visualization technique"
        ],
        "doc_abstract": "The constant growth of available genealogical information has encouraged the research of visualization techniques capable of representing the corresponding large amount of data. An H-Tree Layout has been recently proposed to represent pedigree data as a way to overcome some of the limitations of traditional representations. However, this new method has its own limitations which may hinder its adoption. In this paper, we propose some enhancements to the H-Tree Layout pedigree visualization method in order to overcome some of the identified limitations. An implementation of the proposed enhancements and results of a preliminary evaluation are also provided. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D visualization of geophysical resistivity data to delineate contamination anomalies in a landfill",
        "doc_scopus_id": "84867907163",
        "doc_doi": "10.1109/IV.2012.38",
        "doc_eid": "2-s2.0-84867907163",
        "doc_date": "2012-10-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "3D Visualization",
            "Contaminated areas",
            "Data sets",
            "Data type",
            "Geophysical data",
            "Geophysical resistivity",
            "Geostatistical",
            "Kriging",
            "Spatial visualization",
            "Specific areas",
            "Statistical anomaly detection",
            "Uncertainty representation",
            "Volumetric representation",
            "Volumetric visualization"
        ],
        "doc_abstract": "Geophysical data represent subsoil structure in a specific area and can be used to extract subsoil information for various purposes. In this work we used this data type to detect anomalies/contamination in the subsoil. Our case study was based on data acquired around a landfill and the main objective is identifying contaminated areas as a result of leakage in landfill. This involves the application of statistical methods to detect anomalous values taking into account the whole data set, subdividing it in sublevels in relation to the surface, instead of using a single threshold (as usual). This work combines in the same software package the anomaly statistical analysis and several 3D representations of the results to validate and also helps understanding the final results of the analysis. Given that the original data used in the analysis, resistivity sections, is normally very sparse, a kriging geostatistical process was used to interpolate data in order to provide a volumetric representation of the subsoil in the area, providing a continuous spatial visualization. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "User-centered requirements engineering in health information systems: A study in the hemophilia field",
        "doc_scopus_id": "84860229517",
        "doc_doi": "10.1016/j.cmpb.2010.10.007",
        "doc_eid": "2-s2.0-84860229517",
        "doc_date": "2012-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Data collection",
            "Design process",
            "Domain model",
            "Engineering systems",
            "Evolutionary design",
            "Grounded theory",
            "Health information systems",
            "Hemophilia",
            "Implementation process",
            "Information and Communication Technologies",
            "Methodological approach",
            "Object-oriented system",
            "Requirements engineering process",
            "Task analysis",
            "User centered designs",
            "User-centered",
            "Web based information systems"
        ],
        "doc_abstract": "The use of sophisticated information and communication technologies (ICTs) in the health care domain is a way to improve the quality of services. However, there are also hazards associated with the introduction of ICTs in this domain and a great number of projects have failed due to the lack of systematic consideration of human and other non-technology issues throughout the design or implementation process, particularly in the requirements engineering process. This paper presents the methodological approach followed in the design process of a web-based information system (WbIS) for managing the clinical information in hemophilia care, which integrates the values and practices of user-centered design (UCD) activities into the principles of software engineering, particularly in the phase of requirements engineering (RE). This process followed a paradigm that combines a grounded theory for data collection with an evolutionary design based on constant development and refinement of the generic domain model using three well-known methodological approaches: (a) object-oriented system analysis; (b) task analysis; and, (c) prototyping, in a triangulation work. This approach seems to be a good solution for the requirements engineering process in this particular case of the health care domain, since the inherent weaknesses of individual methods are reduced, and emergent requirements are easier to elicit. Moreover, the requirements triangulation matrix gives the opportunity to look across the results of all used methods and decide what requirements are critical for the system success. © 2010 Elsevier Ireland Ltd.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2010-11-13 2010-11-13 2014-09-30T23:42:41 S0169-2607(10)00269-5 S0169260710002695 10.1016/j.cmpb.2010.10.007 S300 S300.2 FULL-TEXT 2015-05-14T05:25:39.534626-04:00 0 0 20120601 20120630 2012 2010-11-13T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 0169-2607 01692607 false 106 106 3 3 Volume 106, Issue 3 5 160 174 160 174 201206 June 2012 2012-06-01 2012-06-30 2012 Section I: Methodology article fla Copyright © 2010 Elsevier Ireland Ltd. All rights reserved. USERCENTEREDREQUIREMENTSENGINEERINGINHEALTHINFORMATIONSYSTEMSASTUDYINHEMOPHILIAFIELD TEIXEIRA L 1 Introduction 2 Background 2.1 Requirements engineering process 2.2 User-centered design approach 3 A web-based information system for managing clinical information in hemophilia care 3.1 Study contextualization 3.2 Methods and study design 3.2.1 Grounded theory: the research methods for data collection 3.2.2 Evolutionary design 3.2.2.1 First method: object-oriented system analysis 3.2.2.2 Second method: task analysis 3.2.2.3 Third method: prototyping 4 Results 4.1.1 High-level requirements identified 4.1.2 The general domain model 5 Discussion 6 Conclusion Conflict of interest Acknowledgements References AMMENWERTH 2005 1 3 E HORSKY 2005 264 266 J GARDE 2006 265 278 S MAGUIRE 2001 587 634 M HAN 2005 1506 1512 Y KOPPEL 2005 1197 1203 R AMMENWERTH 2003 237 248 E AARTS 2007 S1 S3 J RINKUS 2005 4 17 S ZHANG 2005 1 3 J SAMARAS 2005 61 74 G BOEHM 1991 32 41 B SEFFAH 2005 3 14 A HUMANCENTEREDSOFTWAREENGINEERINGINTEGRATINGUSABILITYINSOFTWAREDEVELOPMENTLIFECYCLE INTRODUCTIONHUMANCENTEREDSOFTWAREENGINEERINGINTEGRATINGUSABILITYINDEVELOPMENTPROCESS SOUTHON 1999 33 46 G STUMPF 2002 45 48 S HEEKS 2006 125 137 R JOHNSON 2000 394 398 C PROCEEDINGSAMIASYMPOSIUM INCREASINGPRODUCTIVITYREDUCINGERRORSTHROUGHUSABILITYANALYSISACASESTUDYRECOMMENDATIONS VIMARLUND 2002 76 80 V BEUSCARTZEPHIR 1997 19 28 M CARROLL 2002 123 135 C IRESTIG 2008 82 94 M JIYE 2005 105 109 M NUSEIBEH 2000 35 46 B PROCEEDINGSIEEEINTERNATIONALCONFERENCESOFTWAREENGINEERING REQUIREMENTSENGINEERINGAROADMAP CHENG 2007 285 303 B PROCEEDINGSIEEEINTERNATIONALCONFERENCESOFTWAREENGINEERINGFUTURESOFTWAREENGINEERING RESEARCHDIRECTIONSINREQUIREMENTSENGINEERING SAWYER 2001 30 55 P SWEBOKGUIDESOFTWAREENGINEERINGBODYKNOWLEDGE SOFTWAREREQUIREMENTS ABRAN 2004 A SWEBOKGUIDESOFTWAREENGINEERINGBODYKNOWLEDGE SOMMERVILLE 2007 I SOFTWAREENGINEERING CARROLL 1991 J TASKARTIFACTCYCLEDESIGNINGINTERACTIONPSYCHOLOGYHUMANCOMPUTERINTERFACE FAIRLEY 1997 417 432 R JOHNSON 2005 75 87 C ZHANG 2002 42 47 J BEUSCARTZEPHIR 2005 179 189 M KUSHNIRUK 2002 141 149 A ZHANG 2005 173 175 J DEROUCK 2008 589 601 S BERG 2003 297 301 M GENNARI 2005 797 807 J CHAUNCEY 2006 E KAULIO 1998 103 112 M GREENE 1985 523 545 J GLASER 1967 B DISCOVERYGROUNDEDTHEORYSTRATEGIESFORQUALITATIVERESEARCH DENZIN 2005 N SAGEHANDBOOKQUALITATIVERESEARCH STRAUSS 1998 A BASICSQUALITATIVERESEARCHTECHNIQUESPROCEDURESFORDEVELOPINGGROUNDEDTHEORY GALAL 1999 92 102 G KUZIEMSKY 2007 S141 S148 C AGGARWAL 2002 383 397 V BANHART 2000 622 626 F EGYHAZY 1998 48 65 C KUIKKA 1999 838 841 E KROL 1999 145 158 M HAKMAN 1999 1 17 M ZHU 2006 258 267 Y PARK 2007 735 746 H BOOCH 1999 G UNIFIEDMODELINGLANGUAGEUSERGUIDE WHITTEN 2004 J SYSTEMSANALYSISDESIGNMETHODS TEIXEIRA 2006 2610 2613 L PROCEEDINGS28THANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBS MODELINGAWEBBASEDINFORMATIONSYSTEMFORMANAGINGCLINICALINFORMATIONINHEMOPHILIACARE PATRICIA 1991 106 112 S PROCEEDINGS9THANNUALINTERNATIONALCONFERENCESYSTEMSDOCUMENTATION MULTIPLEMETHODSUSABILITYINTERFACEPROTOTYPESCOMPLEMENTARITYLABORATORYOBSERVATIONFOCUSGROUPS DIAPER 2004 D HANDBOOKTASKANALYSISFORHUMANCOMPUTERINTERACTION ANNETT 1971 J TASKANALYSIS STUART 2004 145 154 J PROCEEDINGS3RDANNUALCONFERENCETASKMODELSDIAGRAMS TASKARCHITECTTAKINGWORKOUTTASKANALYSIS NIELSEN 2000 J DESIGNINGWEBUSABILITYPRACTICESIMPLICITY DIX 1998 A HUMANCOMPUTERINTERACTION SHNEIDERMAN 2005 B DESIGNINGUSERINTERFACESTRATEGIESFOREFFECTIVEHUMANCOMPUTERINTERACTION TEIXEIRA 2007 3669 3672 L PROCEEDINGS29THANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBS USINGTASKANALYSISIMPROVEREQUIREMENTSELICITATIONINHEALTHINFORMATIONSYSTEM CORTES 2004 207 212 T TEIXEIRAX2012X160 TEIXEIRAX2012X160X174 TEIXEIRAX2012X160XL TEIXEIRAX2012X160X174XL item S0169-2607(10)00269-5 S0169260710002695 10.1016/j.cmpb.2010.10.007 271322 2014-10-02T02:09:58.634896-04:00 2012-06-01 2012-06-30 true 2076110 MAIN 15 48739 849 656 IMAGE-WEB-PDF 1 gr7 425114 1689 2665 gr6 1411211 2140 4333 gr5 448527 1508 2667 gr4 466177 2009 2333 gr3 465991 1272 3000 gr2 689754 2317 3167 gr1 243984 1353 1667 gr7 71867 382 602 gr6 169078 484 979 gr5 65219 340 602 gr4 62286 454 527 gr3 76106 287 678 gr2 101300 523 715 gr1 46153 305 376 gr7 7307 139 219 gr6 8969 108 219 gr5 8793 124 219 gr4 11837 164 190 gr3 7872 93 219 gr2 12621 160 219 gr1 12974 164 202 COMM 3137 S0169-2607(10)00269-5 10.1016/j.cmpb.2010.10.007 Elsevier Ireland Ltd Fig. 1 Software, system and user requirements. Fig. 2 Overview of the research approach: evolutionary process of the user-centered requirements engineering (on the left side) compared to the multiple levels of analysis of the HCC (on the right side). Fig. 3 Example of high-level use-case diagram including some insertion tasks: human actors – nurse, patient and physician; non-human actors – ClinidataXXI, IHIS and MIS [60]. Fig. 4 Part of HTA for “Register Treatment” functionality: designed with the TaskArchitect ® tool [64]. Fig. 5 Appearance of prototype interface. Fig. 6 UML class diagram giving general overview of domain model. Fig. 7 Iterative process with methods triangulation in requirements engineering. Table 1 Triangulation matrix based on three different analysis and validation methods with final list of high-level requirements of the WbIS for managing clinical information in hemophilia care. Actors/requirements identified Method OOSA HTA Prototyping Patient P1/N6: Register infusion treatment (home treatment) x + + P2/N7: Register bleeding episode x + + P3/N8: Register product administered x + + P4/D25: View treatment evolution x NA + P5/N9/D2: View treatment protocol x NA + P6: View particular stock of CFC − − x Subtotal 5 0 1 Nurse N1: Register product (CFC: Stock management) x + + N2: Assign product to patient (CFC: Stock management) x + + N3: Change product location (CFC: Stock management) − x + N4: Register safety stock (CFC: Stock management) − x + N5: View stock level (CFC: Stock management) x NA + N6/P1: Register infusion treatment (hospital treatment) x + + N7/P2: Register bleeding episode x + + N8/P3: Register product administered x + + N9/P5/D2: View treatment protocol x NA + Subtotal 7 2 0 Physician/hematologist Doctor D1: Register treatment protocol x + + D2/P5/N9: View treatment protocol x + + D3, D4: Register/view family-tree x/x +/NA +/+ D5, D6: Register/view external hospitalization x/x +/NA +/+ D7, D8: Register/view joint-exam x/x +/NA +/+ D9, D10: Register/view data surgery x/x +/NA +/+ D11, D12: Register/view bleeding diagnosis x/x +/NA +/+ D13, D14: Register/view life quality x/x +/NA +/+ D15, D16: Register/view vaccination plan −/− −/− x/x D17, D18: Register/view pathology record −/− −/− x/x D19, D20: Register/view general exam x/x +/NA +/+ D21: View medical appointment x + + D22: View inhibitor exam − − x D23: View virology exam x NA + D24: View pathological exam x NA + D25/P4: View treatment evolution x NA + D26: View different statistical data (graphs and tables) x NA + Subtotal 21 0 5 MIS M1: Register medical appointment x NA + M2: Register patient medication − NA x Clinidata XXI C1: Register inhibitor exam − NA x C2: Register virology exam x NA + C3: Register pathological exam x NA + IHIS I1: Register patient biographic data x NA + I2: Register patient contact x NA + Subtotal 5 0 2 Total identified functionalities 38 2 8 Total confirmed functionalities − 17 40 x, identify; +, confirm; −, nothing; NA, no-applicable; CFC, coagulation-factor concentrate. User-centered requirements engineering in health information systems: A study in the hemophilia field Leonor Teixeira a b e ⁎ Carlos Ferreira a c Beatriz Sousa Santos d e a Department of Economics, Management and Industrial Engineering, University of Aveiro, Portugal b Governance, Competitiveness and Public Politics (GOVCOPP), University of Aveiro, Portugal c Operational Research Centre (CIO), University of Lisbon, Lisbon, Portugal d Department of Electronics, Telecommunications and Informatics, University of Aveiro, Aveiro, Portugal e Institute of Electronics and Telematics Engineering of Aveiro (IEETA), Aveiro, Portugal ⁎ Corresponding author at: University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal. Tel.: +351 234 370 361; fax: +351 234 370 215. The use of sophisticated information and communication technologies (ICTs) in the health care domain is a way to improve the quality of services. However, there are also hazards associated with the introduction of ICTs in this domain and a great number of projects have failed due to the lack of systematic consideration of human and other non-technology issues throughout the design or implementation process, particularly in the requirements engineering process. This paper presents the methodological approach followed in the design process of a web-based information system (WbIS) for managing the clinical information in hemophilia care, which integrates the values and practices of user-centered design (UCD) activities into the principles of software engineering, particularly in the phase of requirements engineering (RE). This process followed a paradigm that combines a grounded theory for data collection with an evolutionary design based on constant development and refinement of the generic domain model using three well-known methodological approaches: (a) object-oriented system analysis; (b) task analysis; and, (c) prototyping, in a triangulation work. This approach seems to be a good solution for the requirements engineering process in this particular case of the health care domain, since the inherent weaknesses of individual methods are reduced, and emergent requirements are easier to elicit. Moreover, the requirements triangulation matrix gives the opportunity to look across the results of all used methods and decide what requirements are critical for the system success. Keywords Health information systems Requirements engineering User-centered design Human-factor engineering system Triangulation matrix Hemophilia 1 Introduction The use of sophisticated information and communication technologies (ICTs) in the health care domain is a way to improve the quality of services. The literature points to a general consensus that health information systems (HISs) are thought to have the potential to improve patient care. However, there are also hazards associated with the introduction of ICTs in health care, and some works report how difficult it is the successful introduction of ICTs in this domain [1,2]. In fact, health care is a unique and complex domain and HISs have human safety implications and profound effects on individual patient care [3]. Successful development of HISs can increase efficiency and productivity, ease of use and learning, adoption, retention and satisfaction of the users [2], and simultaneously, can help to decrease medical errors as well as to reduce support and training cost [4]. On the other hand, HISs are usually complex systems and their failures may cause negative effects on patients [5–7] and possibly, when insufficiently designed, they may result in spending more time with the computer than with the patient [8]. According to [9] ICTs have been hailed as a solution to reduce errors in health care, but there is also evidence that they can be part of the problem. There are a large number of HISs projects that have failed, and most of these failures are not due to flawed technology, but rather due to the lack of systematic consideration of human and other non-technology issues throughout the design or implementation process [10–12]. Also, several studies have shown that 80% of total maintenance costs with information systems (ISs) are related to users’ problems and not technical bugs, and among them 64% are related with usability problems [13]. A survey of over 8000 projects made by The Standish Group and undertaken by 350 US different companies revealed that one third of the projects were never completed and one half succeeded with partial functionalities. The major source of such failures resided on poor requirements, specifically: the lack of user involvement (13%), incomplete requirements (12%), changing requirements (11%), unrealistic expectations (6%), and unclear objectives (5%) [14]. These problems are mainly due to the fact that, in developing interactive software, most software engineering methodologies do not propose any mechanisms to: (i) explicitly and empirically identify and specify user needs and usability requirements; and (ii) test and validate requirements with end-users before and during the development process [15]. The health care domain has been particularly prone to such problems in recent years, and there are numerous examples of potentially useful systems that have failed or have been abandoned due to unanticipated human or organizational issues [16–18]. Since the design of systems that are used by people is a complex endeavor, the systems that cannot be used intuitively often lead to an increase in error rate and a decrease in user acceptance [19]. Involving end-users in the design process has been suggested to be a key marginal investment for being able to transform the cost related to the implementation of HISs into future benefits [20–22]. Moreover, the participation of end-users and the involvement of relevant stakeholders in early steps of the design process, not only prevents post-implementation problems, but also gives them the chance to address and resolve potential conflicts concerning the future system [23]. However, involving end-users in development of HISs is often complicated, especially when they have limited computer skills or due to the fact that users’ knowledge is tacit and consequently task description is very difficult. Particularly in healthcare, effective research of user requirements is often discussed in terms of which method to use or “which method is better”, since the dominant culture in this industry is still train people to adapt to poorly designed technology, rather than designing technology to fit to people's characteristics [11]. The principles of user-centered design (UCD) [4,24] combined with ethnographic practices [25] can improve synergies among technology, people and work environment (tasks). In order to perform a user needs analysis and to write requirements specification for integrated care in the hemophilia field, we followed a user-centered requirement engineering process involving the end-users through different techniques of requirements elicitation and validation. This work describes the application of UCD principles in the requirements engineering process to generate the requirements document of a web-based information system (WbIS) for managing the clinical information in hemophilia care, as well as some results. The research was based on three well-known methodological approaches: the first one consisted of a classical object-oriented systems analysis (OOSA) method based on Unified Modeling Language (UML) notation; the second consisted in a task analysis (TA) method based on hierarchical task analysis (HTA) notation; and the third consisted in prototyping based on the construction of an executable system model. These three methods were performed through an iterative development process combined with grounded theory (GT) in a triangulation work. 2 Background 2.1 Requirements engineering process The success of any software system depends on how well it fits the needs of its users and its environment [26]. Software requirements comprise these needs, and requirements engineering (RE) is the process by which the requirements are determined [27]. A requirement is a property that a system must exhibit in order to meet the system's motivation need; and software requirements are a property which must be exhibited by software developed to solve a particular problem within one organizational context. Therefore the software requirements are a complex combination of requirements from different people at different levels of an organization and from the environment in which the system must execute [28]. They express the needs and constraints placed on a software product that contribute to the solution of some real-world problem and normally result of an arrangement between “user requirements” and “system requirements”, Fig. 1 . User requirements denote the requisites of the people who will be the system client or end-users. System requirements add requirements of other stakeholders (such as regulatory authorities) and requirements that do not have an identifiable human source and that normally result from the intersection among technical, cultural and social environments [28,29]. The information system (IS) is never used on its own but always as part of some broader system including hardware, people and, often, organizations [30]. Successful RE involves understanding the needs of users, clients and other stakeholders, as well as understanding the context in which the software will be used [27]. Thus, identifying all the users and other stakeholders who may be impacted by the system is very important and this will help to ensure that the needs of all those involved are taken into account and, if required, the system is tested by them. In the literature, RE is presented as a knowledge area of software engineering related with elicitation, analysis, specification and validation of software requirements [29]. However, and according to [30], this process of RE is difficult for several reasons: (i) stakeholders often do not know what they want from the system, except in general terms; (ii) stakeholders express requirements in their own terms and with implicit knowledge of their work, and requirements engineers, without experience in the user domain, must understand these requirements; (iii) different stakeholders have different requirements, which they may express in different ways; (iv) political factors may influence the system requirements; (v) the business environment in which the analysis takes place is dynamic. The resulting requirements document has to be understood and usable by domain experts and other stakeholders, who may not be knowledgeable about computing [27]. Hence, requirements notation used in models that involve users’ validation must use a balanced language adequate for stakeholders having computing and non-computing backgrounds. On the other hand, it should be noted that the analyses of activities to elicit the user requirements, which normally focus on present activities, can change in time. When the new artifact was implemented, the activity also changed, thus, the analysis of the user needs might be outdated. This ‘vicious circle’ was presented by Carroll et al. [31] as the task–artifact cycle. In other words, the task–artifact cycle is an iterative process of continuous, mutually dependent development between task and artifact, a process that will never reach an optimum state. This cycle suggests a revision of the original task for which the artifact was made, and consequently the redesign of the artifact according to the new identified requirements. To avoid this trap, and to aid the communication among different stakeholders with different background, some authors proposed several techniques and methods. For example, the so-called “Concept of Operations (ConOps)” was proposed by Fairley and Thayer [32], as a way of dealing with that problematic issue. In our case, we used evolutionary design techniques combined with UCD principles to minimize the impact that may arise from the basic assumption of the task–artifact cycle. 2.2 User-centered design approach Within the field of software development, there are several approaches for designing software applications. User-centered design (UCD) is one of them concerned with incorporating the perspective of the users into the software development process in order to achieve a desirable and usable system [10,33,34]. UCD is a complement to software engineering traditional methods rather than a substitute. Maguire [4] and Ji-Ye et al. [24] describe UCD as a multidisciplinary design approach based on the active involvement of users to improve the understanding of user and task requirements, and the iteration of design and evaluation. Much has been written in the research literature about UCD and as further evidence of internationally endorsed best practice, this approach is defined in ISO documents, including ISO 13407 [35]. It states that there are four design activities that need to start at the earliest stages of a project: (i) understand and specify the context of use; (ii) specify the user and organizational requirements; (iii) produce design solutions; and, (iv) evaluate design against requirements. In the healthcare domain the application of the UCD principles is more recent, namely some UCD approaches, such as the human centered distributed information design methodology (HCDID) by Zhang et al. [34] and Rinkus et al. [10] and the user-centered framework proposed by Johnson et al. [33], which are based on four types of analyses: (i) user analysis – identifying the characteristics of existing and potential users; (ii) functional analysis – identifying a system's abstract structures of a given domain model; (iii) representational analysis – identifying an appropriate information display format for a given task performed by a specific type of users; and (iv) task analysis – identifying the procedures and actions to be carried out as well as the information to be processed to achieve task goals by using specific representations. Beuscart-Zéphir et al. [36] present another UCD approach, an user-centered assessment method, that includes three main dimensions of analysis: qualitative management, usability assessment, and performance analysis. Beuscart-Zéphir et al. [21], Carroll et al. [22] and Kushniruk [37] also advocate the importance of user involvement during early phases of the design process in the development of HISs, based on formative evaluation. Health information systems (HISs) are highly complex systems and, we believe that a proper UCD approach has the potential to improve the quality of the final solution. The lack of minimum considerations of design principles centered in human factors in many HISs makes them very difficult to learn and use, and this difficulty leads to strong resistance by users, in some cases it leads to abandoning the HIS altogether or increasing “human error” resulting from an incorrect usage [34]. The paradigm of UCD in health care domain [10–12,33,34,36,38,39] offers a new look at system design including human factors, covering more than usability engineering, and human factors. Basically, the goal of a UCD approach consists in creating systems that are modeled in conformity with the characteristics and tasks of the potential users. UCD has given rise to many forms of design practices in which various characteristics of the context of use are considered. Particularly, in the medical informatics domain, several authors emphasize the need for UCD development and argue for an early user involvement [9,12,40]. This development process paradigm can be improved using ethnographic practices. Myers [25], Sommerville [30] and Gennari et al. [41] present ethnographic method as observational techniques that can be used to understand social and organizational requirements having a strong contribution to a complex set of communication behaviors and needs. In this context, the main value of this technique is that it helps requirements engineers to discover implicit requirements that reflect the actual rather than the formal process in which people are involved. Sometimes users may have difficulties expressing their tasks, therefore we need to apply ethnographic methods to observe and analyze them in order to be able to get some evidence to help deducting the requirements. 3 A web-based information system for managing clinical information in hemophilia care In this section, we will present a study where we applied the user-centered requirements engineering methodology. Firstly, we will describe the domain problem and the critical aspects that justify the application of an UCD approach. Then we will explain the study design, namely the methods and procedures of applying the principles of UCD in RE process. After that, we present the results of the methodology process. Finally, we discuss and explain the impact of the method on the requirement engineering process in this particular problem and in health care IS, in general. 3.1 Study contextualization This study was requested by the hematology service of a regional hospital (HS_CHC) in order to evaluate the feasibility and usefulness of a WbIS to assist the hemophilia patient care at this hemophilia treatment center (HTC) located in the center region of Portugal. This HTC provides assistance to about 200 patients (≈30% of the registered patients in Portugal) in three integrated hospitals and provides clinical and laboratorial support to other hospitals all over the country. The main goal of this application is to support the management of inherited bleeding disorders, which involves the integration, distribution and archive of large sets of information coming from heterogeneous sources in the scope of hemophilia care, as well as to facilitate communication between health care professionals (HCPs) and patients at this HTC. Currently, in order to support the information flow and communication between different HCPs, this HTC uses several different computer-based and paper-based ISs. The computer-based ISs are basically: (i) IHIS (Integrated Hospital Information System) – which allows to visualize, manage and archive the administrative information while opening a clinical process; (ii) MIS (Medical Support Information System) – which allows to visualize, manage and archive clinical information during the medical appointments; (iii) NIS (Nursing Support Information System) – which allows to visualize, manage and archive nursing information; (iv) and, ClinidataXXI – to archive clinical analysis results and laboratorial information providing on-line service at the three afore mentioned hospitals. These computer-based ISs represent generic solutions, since they were developed to support generic requirements of Hospitals and Healthcare Centers, and consequently cannot respond to the specific needs of hemophilia care. In fact, HCPs of hemophilia care generate a lot of information when they see their patients; part of this information is in electronic format and is stored in computer-based ISs; but another part (i.e. home treatment records, joint exam records, life quality records) is on paper format and is stored in paper files. At present, patients record the result of home infusion treatment in paper diaries and send them to the HTC by post, fax, or bring them when they attend routine review appointments. This system has a number of weaknesses as often paper records are incomplete or not returned. Moreover, the period between individual infusions of treatments and receipt data may be long, which is undesirable, since that data is very important for clinical decision about treatments. In this pathology, the information is more than a resource, as further clinical decisions about patients’ treatment depend on it. The fact that the information concerning hemophilia patients is spread throughout different ISs is an obstacle for quality of data, fast retrieval of information and, consequently, healthcare quality. The need of an integrated IS for this specific pathology is obvious, and new ICTs have an important role in this field. Portugal, in spite of having about 700 patients with hemophilia, does not have a hemophilia national patient registry and most HTCs do not have a specific system to store and manage information concerning this pathology. This problem emerges in the scope of a highly complex environment, characterized by missing information, shifting goals and a great deal of uncertainty [37]. Healthcare decisions are complex processes strongly based on tacit knowledge that contributes for a difficult process of requirement elicitation using traditional methodologies. Therefore we believe that the combination of a traditional system development methodology with UCD actively involving users in the design process can improve the elicitation requirements process in this domain, taking into account the natural impact that may arise from the basic assumption of the task–artifact cycle, proposed by Carroll et al. [31]. 3.2 Methods and study design To conduct this study along the requirement engineering process, we followed an approach that combines a grounded theory (GT) with an evolutionary design based on constant development and refinement of the generic domain model. All users, i.e., physicians, nurses, laboratorial staff, administrative staff and patients were, directly or indirectly, involved in all steps of this study. To avoid a technical focus, which too often has been the case of similar projects, different analysis and validation methods using the UCD principles were chosen to accomplish the RE process: (i) the first one consisted of a classical OOSA method represented by UML notation; (ii) the second consisted in a model-based TA represented by HTA notation; (iii) the third consisted in the prototyping technique based on developing an executable system model. The research based on three well-known methodological approaches was developed through an iterative process under two conditions: (i) the users were active and showed about the same level of participation in all phases of the design; and, (ii) the context of use was taken into account in all phases of the design, Fig. 2 (on the left side). According to the representation in Fig. 2, the different methods of our methodology (on the left side of the Fig. 2) have a match with the levels of analysis of the human-centered approach (HCC) proposed by Zhang et al. [11,34] and Rinkus et al. [10] and used by Johnson et al. in their user-centered framework [33] (on the right side of the Fig. 2). Basically with the first method (OOSA) we are trying to capture the same information as the user and functional analysis level; our second method (TA) has the same principle of the task analysis level; and, the prototyping methods have a correspondence with the representation analysis level in the HCC methodology. Nevertheless, our methodology differs from the HCC methodology as we applied the different methods using an iterative design approach based on triangulation work, in order to validate and improve the domain model. The main objective of our methodology is to support the initial part of the process (i.e. the elicitation and transformation of users’ needs into detailed requirements specifications) in the development of a new IS. Triangulation is an approach to data collection and analysis that uses multiple methods, measures, or approaches to look for convergence on product requirements or other problem areas [42,43]. In general it refers to the application of different sources of data, methods and theories to investigate the same phenomenon [8,44]. This approach has two main objectives: (i) validation of results in order to confirm results with data from other sources or methods; and (ii) completeness of results in order to complement the data with new results, thus to find new information to get additional pieces to the overall system [8]. Therefore, these objectives were the two main reasons by which we chose and applied a triangulation approach in the present study. According to the types of triangulation mentioned by [8,44], we used (i) ‘data triangulation’ during data collection activities, where various data sources were used; and, (ii) ‘methods triangulation’ during the evolutionary design activities, where several methods for analysis and validation were applied; according to [43], a ‘between-methods triangulation’ in a ‘sequential combination of methods’. Basically, the ‘between-methods triangulation’ refers to the process of combining different methods in order to test external validity and ‘sequential combination of methods’ refers to the sequential application of the methods [43]. 3.2.1 Grounded theory: the research methods for data collection The study outlined in this article was developed along two years. During the first nine months, an exploratory study was made based on a grounded theory (GT) approach. The GT approach was published in 1967 by Glaser and Strauss as a qualitative method in social research [45] and it supports the inductive development of theories which are grounded in data to explain a phenomenon [46,47]. The theory emerges of the process from systematically recording and analyzing data related to the phenomenon, normally called ‘constant comparison’. In the present context, we used a GT approach to explain the clinical practice in the HS_CHC and consequently to support the process of RE in a complex environment on the basis of a comprehensive domain model. The use of GT to aid the requirements engineering activities is not very common, however is not novel too. For instance, the works of Galal and Paul [48], Garde and Knaup [3] and Kuziemsky et al. [49] describe the application of this qualitative approach in the RE process, and the last two were applied in the health care domain. In the present study and in order to stimulate basic research as well as to understand the information flow and the management process of clinical data, we started to analyze the clinical practice of the HS_CHC using qualitative methods. Direct observation (ethnography) of the current work practices, analysis of documentation and unstructured interviews based on open-ended questions with users were used. A preliminary specification of the new solution was drafted using information obtained from direct observation and documentation analysis. These techniques were used during the first three months, and the data were collected by the researcher using a specific report. After this phase, qualitative interviews and several focus groups with HCPs and hemophiliac patients, to explore their perceptions about the new system and to capture their needs, were conducted. Three nurses, two physicians and three patients were involved in six focus groups carried out during the following six months. The data were used to complement the preliminary specification and was converted in a preliminary domain model by the researcher. Specific unstructured interviews with patients were also conducted in an informal setting, while they waited for their appointments, as well as with the administrative staff, in their work place. Ethnographic methods were used in all exploratory study phases, since they produce results with greater accuracy when combined with other methods [46]. Basically we used a different source of data (‘data triangulation’) in order to ensure richness in the working material, reducing at the same time the inherent weaknesses of individual methods. After this exploratory study analysis we concluded that although departmental IS have sometimes been interfaced in order to transfer some data among a patient registration system, a laboratory system and a medical system, this was usually based on ad-hoc activities without the implementation of a global communication concept. 3.2.2 Evolutionary design Based on the results of the exploratory study supported by the GT approach, we outlined the preliminary requirements document and the domain model. In order to validate and consequently complement the domain model we applied the three different analysis and validation methods (‘methods triangulation’), Fig. 2. The combination of these three methods was a deliberate choice to ensure richness in the working material and a possibility to collect as many aspects as possible. The order by which they were applied was a planned choice too and will be explained in the next subsections. 3.2.2.1 First method: object-oriented system analysis First, we chose an OOSA method as an object-oriented (OO) environment is an appropriate representation method of a real-life entity or abstraction [50] and would allow the incorporation of new components based, for instance, on incremental solution and evolutionary design. Thinking about the environment in terms of objects comes naturally to people and OOSA has been also successfully applied in different HISs projects [50–57]. Among this, the UML was the notation chosen since it is an OO design technique representing a standard language for specifying, visualizing, constructing, and documenting the artifacts of systems [58,59]. One example of this phase results is the high-level use-case diagram (including insertion tasks) that is depicted in Fig. 3 . After drawing these representations, users were invited to evaluate the use-case models. For this evaluation, open-ended focus group interviews [46,61] were conducted with different groups of users (patients, nurses and physicians). The process was open-ended since the interviews were not guided by pre-defined questions; nevertheless they were supported and oriented by models to stimulate discussion. In order to simplify and help the users in this process, during the discussion, some functionalities were converted in small stories (scenarios) using natural language. We are sensible to the fact that use-case diagrams represent abstract models and consequently they could be unintelligible for people without computing science background. In order to workaround this problem, we introduced additional activities to complement a process of RE, based on TA methods. 3.2.2.2 Second method: task analysis Based on results obtained with first method, we applied another to complement the domain model, in order to validate the functionalities and consequently to identify the sequences of sub-functionality within a specific functionality. Leading the users through a small number of scenarios representing sequences of events in the application domain can help to explore the key aspects affecting the requirements. We believe that the task analysis (TA) approach has the power to help in this aspect. There are many types of TA and diverse techniques provide different types of data for analysis [62]. We applied a hierarchical task analysis (HTA), developed by Annett et al. [63], that has the benefits of being readily understandable by a wide people background that normally think about their work as a structured hierarchy of tasks. While the lack of syntax can be said to limit its expressiveness because it does not include the constructs of more formal meddling languages, HTA can also be seen as a more powerful medium for communication in that the analyst is free to express the user's task without syntactical constraints (Ormerod et al., 1998 in [64]). In our particular case, the TA was performed on user data, gathered during the first phase of the requirements engineering process and represented by use-case diagrams, Fig. 2. This analysis began with a statement of the user definition of each task. A complete list of actions needed to perform each task was subsequently produced. Users were then asked to first explain their primary goal when accomplishing a specific task in their work. After that, and based on the data resulting from the system analysis process in the first phase, a TA was performed and represented through HTA notation. We applied this technique (HTA) to tasks performed by our three human actors – nurse, patient and physician – choosing more error prone functionalities, i.e. data insertion tasks. One example of the obtained HTA model about “register treatment” is illustrated in Fig. 4 . The task models resulting from this phase were then reviewed by users: three nurses, two physicians and three patients were directly involved in this review process. They were asked to “think aloud” and verbalize how they would perform each task whilst they look at the models. The resulting information was registered and used to refine the task model, in combination with information obtained from observation and discussion with users. This cycle was repeated until we noticed that the model was not evolving anymore, which happened after three iterations. The use of this type of analysis proved to be a practical way of involving users allowing a better understanding of the human factors involved in our system and fostered an easier incorporation of the notion of usability engineering in the process of requirement engineering. Subsequently, this method assumes particular importance on the discovering process of the users’ tacit knowledge. 3.2.2.3 Third method: prototyping In order to provide a contextual framework that serves to focus the discussion, to help users verify their understanding, as well as to test the overall interaction metaphor, we decided to apply a prototyping technique in the last phase of the RE process. There are different types of prototypes and we applied a horizontal prototype (or user interface prototype) since it represents a model of the outer shell of an entire system, with many features but with very little processing behind them. This prototype, implemented in HTML and JavaScript, showed all items described in the domain model and how these changed as a result of user interaction. Besides, in prototype development, we referred to principles of Human Computer Interaction (HCI) design [65–67] and structured the information on the screen in a logical and consistent manner, corresponding to the order in which this information appear in paper-based files. Moreover, to help the users in building a mental model of the new system, the information is presented in a known and understandable manner in accordance with sequence information captured in the TA phase [68]. The appearance of one prototype interface screen is demonstrated in Fig. 5 . To evaluate the result of this phase, we discussed the prototype in an open-ended focus group [61,69]. First of all, and in order to stimulate discussion, the prototype was presented to users. Based on this, the users suggested and debated changes, and recommended other new functionalities. The resulting information was integrated in the prototype and, after that, a new discussion took place. This cycle was repeated until we noticed that the prototype was not evolving anymore, which happened after two iterations. Modeling the external features of a HIS with a horizontal prototype greatly enhances communication between the requirements engineer and the users. This opportune interaction sometimes results in more concrete outputs, and users normally find it easier to respond to a suggested approach demonstrated by a prototype, than to define their requirements with the system represented by an abstract model. With this experiment, we concluded that the prototype is the easiest tool for HCPs participation in a requirements engineering process of a HIS. 4 Results The results of the evolutionary design process of a WbIS for managing the clinical information in hemophilia care may be divided into two distinct parts: the list of high-level requirements identified and the conceptual model of domain. 4.1.1 High-level requirements identified A list of 48 high-level requirements obtained from multiple methods was presented and grouped by actor: 38 requirements were identified or validated in the first step, 2 were identified in the second step, and 8 were identified in the last step. Table 1 presents a resultant triangulation matrix that lists high-level requirements on one dimension and the three different analysis and validation methods on the other dimension. This (triangulation matrix) indicates which of the methods was responsible for the identification of the requirement (x); and after its identification, if it had confirmation by one of the subsequent methods (+) or if it was not considered (NA). For example, requirements P1, P2 and P3 were identified in the first step and were confirmed in the second and third steps. This might indicate that they are core requirements. Requirements P4 and P5 were identified in the first step; they were not considered in the second step and had confirmation in the third step. Thus, they might also be core requirements. In the second method they were not considered since we applied the TA method only to insertion tasks performed by human actors as they are more error prone functionalities. The requirement P6 was discovered only in the third step, which could indicate that it is a dependent and/or emergent requirement. This type of requirements which emerge with the users’ understanding of the system development during the design process are denominated ‘emergent requirements’ [30]. Different actors were identified: physician, nurse and patient represent the human actors; IHIS, MIS and ClinidataXXI, represent non-human actors, i.e. other existing ISs that will be integrated with WbIS. Patients have an individualized password protected home page, which can only be accessed by themselves or, in case of children, aged people or illiterates, by a caregiver. For example, when a patient receives a treatment with an infusion of coagulation-factor concentrate (CFC), he/she can access his/her personal page and enter details about that treatment (P1), associated bleeding episodes (P2) and the amount and batch number of CFC administered (P3). If the treatment takes place in HTC, these data could be registered by a nurse (N6:N8) who also has an individualized password. The patient's page shows a recommended treatment regimen for prophylaxis or immune-tolerance as well as information on bleeding episodes management (P5). These treatment protocols are entered by a physician and can be up-dated at any time (D1). The physician, normally a hematologist doctor, registers and views a lot of information specific of this pathology and related with the patient (D1:D26). The HIS, MIS and ClinidataXXI represent non-human actors and they register some data (M1, M2; C1:C3; I1, I2). 4.1.2 The general domain model The obtained general domain model was represented through a class diagram of UML notation. This diagram represents the static structure showing object classes, attributes, as well as relationships between those object classes. We identified different kinds of information that were grouped in three big components: patient information, treatment information and CFC-Stock information, Fig. 6 . In the patient information component, besides patient personal data (coming from IHIS), it is also important to collect the data obtained from the doctor appointments and prescribed medication (coming from MIS); as well as all historical information about pathological, virology, and inhibitor exams (coming from Clinidata XXI). These data will be registered in the WbIS automatically by non-human actors. Furthermore, regarding the patient, it is also necessary to store additional data as: bleeding diagnosis, life quality, family-tree, pathologic record, joint condition, and so on. Patients’ data are stored with a unique identification; and each patient can undergo many treatments. The treatments can be on-demand, prophylactic or immune-tolerance. On-demand treatments have associated bleeding episodes (in a joint, muscle or mucosa). On the other hand, treatments can be made at home or at the HTC. The registration of this data was made by the patient or his/her caregiver (in case of home treatment) or by a nurse (in case of hospital treatment). Usually, treatments consist of infusion of blood products, the coagulation-factor concentrates (CFC), which are in stock; thus, functionality concerning the management of blood products inventory should also be integrated. When a blood product arrives at the HTC, a nurse registers its data into the system (name of product, data of reception, batch number, number of units per batch, validity); on the other hand, a blood product can be in different places, as it can be given to patients for home treatment (patient stock) or stayed in the hospital (hospital stock). When a blood product is administered in the sequence of a treatment, the system should register this occurrence and automatically update the stock level. When the stock level reaches the safety stock, the system will send an alert so that a new order is placed. 5 Discussion In this study, we used an evolutionary design process in order to specify the requirements document of a WbIS for managing the clinical information in hemophilia care, and to define a more accurate and realistic domain model in this field. The user-centered requirements engineering methodology based on three different analysis and validation methods was used in an iterative design approach based on the triangulation work. As stated before, the triangulation approach has two main objectives: to confirm results with data from other sources, and to find new data to obtain a more complete solution. Validation of results is achieved when results from one method are confirmed by results from other method. Completeness of results is achieved when one method presents results which have not been found with other method. We involved the same group of users using different analysis and validation methods to confirm the previous results (validation) and consequently to complement the data with new results (completeness). Each method was applied with the same users, in the same environment but in different times. As shown in Fig. 7 , and according to the triangulation matrix presented in Table 1, the results of some parts of the study were validated by the results from other parts, and all parts combined contributed to improve and complete the results about the domain model. With this methodological approach, the requirements triangulation not only contributed for the process of elicitation and validation of the system requirements but also allowed to look across all methods used and decide what requirements are critical for the system success. For example, all requirements identified in the first step and confirmed in the second and third steps should be considered as core requirements. The requirements that were discovered only in the third step probably are dependent and/or emergent requirements. Usually, this type of requirements which emerge with the users’ understanding of the system development during the design process is associated with tacit knowledge and of difficult elicitation. With this study, we also concluded that the HCPs had severe difficulties in predicting their expectations using situations based on abstract models, as UML notation. Indeed, design representation principles emphasize the importance of using representations that are easy to understand for all the stakeholders, and UML notation is obviously not appropriate in this respect. On the other hand, the prototype proved to be the most adequate tool for HCPs participation in a requirements engineering process of a HIS, greatly enhancing communication between the requirements engineer and the users. The exploratory study supported by GT combined with requirements triangulation based on three well-known methods provides a practical framework for understanding and considering user perspectives in the initial part of the development, and transforming these needs into detailed requirements specifications, i.e., the conceptual domain model. However, this approach is time-consuming, but we believe that this disadvantage is largely balanced by the good results, since the errors or ambiguities in this initial step turn out to be costly in the future. For this reason, the application of this methodology in its complete form should be circumscribed to a complex problem characterized by unclear requirements and to support the development of new IS, and not the redesign of existing ones. Furthermore, this approach can help dealing with the basic assumption that results from the “task–artifact cycle” defended by Carroll et al. [31]. 6 Conclusion In the medical informatics literature, one of the identified reasons for information system failures and unsuccessful results is a lack of systematic consideration of human and other non-technology issues throughout design or implementation process [10–12]. Several authors emphasize the need for UCD development of health information systems and argue for an early user involvement [10,12,33,34]. The UCD approach consists in creating systems that are modeled in conformity with the characteristics and tasks of the potential users. UCD has given rise to many forms of design practices in which various characteristics of the context of use are considered. In this work, we illustrated the use of user-centered requirements engineering framework that supports the development process of a WbIS to assist the hemophilia patient care in the hematology service of a Portuguese regional hospital. This study was developed along two years, and during the first nine months, we made an exploratory study based on a GT approach, using several qualitative methods. These qualitative methods, such as ethnography and focus groups, have the ability to capture experiences, emotions, and human interaction processes through the inductive collection of one subsequent rigorous analysis of information from the individuals’ perspective [34]. In this stage the ethnographic study revealed important process details that were missed by other requirements elicitation techniques, specifically analysis of documentation and unstructured interviews. In the health care domain, ethnography is a very important method for requirements elicitation, however it is not a complete approach on its own and it should be used in complement to other approaches. After this exploratory study, we performed an evolutionary design based on constant development and refinement of the generic domain model and requirements document. Different methods based on triangulation work using the UCD principles were used. First, we chose an OOSA method since it is an appropriate method to represent the real-life entity or abstraction and use-case diagram of UML notation to identify and represent the individual interactions with systems through actor and use-case notation. As a second method we chose a model-based TA as it is a good method to capture and understand the sequences of the sub-tasks (sub-functionality) within one specific task (functionality) and thus enabled us to capture the details about the task. In a complex domain such as healthcare, it is necessary to provide a contextual framework that serves to focus the discussion and analysis. Hence, we chose in the last phase a prototyping technique in order to validate the achieved solution and consequently analyze users’ preferences concerning user interfaces. The ‘between-methods triangulation’ in a sequential combination helped us to achieve those goals and the three used different types of analysis and validation methods contributed for the domain model design that will support the development of the WbIS in hemophilia care. These three methods together proved to have great value for the design of our system allowing different views of the same problem. Although the first method based on UML notation represents the standard language for specifying, visualizing, constructing and documenting the artifacts of any system, its models must be avoided in the validation process with users without computing knowledge as it is in general the case of HCPs. In fact, we verified that the usage of abstract terminologies (like UML) in the validation process can confuse and intimidate users and little feedback is obtained, since users do not react, and sometimes just agree when they do not understand our language. The second method turned out to be a practical way in the process of understanding the human factors involved in the tasks. It is a more powerful medium for communication in that the requirement engineer is free to express users’ tasks without syntactical constraints. Little new functionalities were added in this step, yet very important information details were collected with this method. Finally, prototyping proved to be the easiest tool to involve the user in a requirement engineering process contributing to the identification of new high-level functionalities and definition of user interfaces. The result of this study, the conceptual model, was implemented using open source technologies, and followed the principles of an agile development approach, more specifically the eXtreme Programming (XP). This approach was chosen by its proximity to the UCD practices, getting a quick feedback from the user through the testing of the small software versions obtained in different iterations. The technological solution is a user-friendly web application that allows communication among hemophilia patients and HCPs, and makes information on treatment's effectiveness, and patient clinical data, as well as other disease related information available. On one hand, it offers HCPs analytical tools to transform data into information, and mine relevant information through data queries. On the other hand, using a user-friendly web application, patients can have direct access to relevant information, allowing them to view their clinical history, as well as introduce home-therapy data. Thus, the new system has improved the process of information managing associated with the clinical practice, since it allows clinicians quickly and easily access a set of meaningful data. Additionally the system provides a set of alerts to abnormal situations, or to those that require a check by a clinician. This happens, for example, when the level of safety stock is reached, or when a patient introduces a treatment registry. For the patients, the system also brought additional benefits, as it allows recording and sending their treatments data, when previously they would have to be recorded on paper and delivered at the hospital. This helped to improve the quality of life for patients, putting them in constant contact with the hospital. Finally, there is the possibility to automatically manage the stocks of CFCs used in treatments, which relieves the nurses from those manual, time consuming and error prone tasks. Based on the experience gained with this project, we conclude that requirements engineering in health care domain is a really complex task; however, we believe that using a UCD in a triangulation work in such a critical step is a good design practice. Conflict of interest None declared. Acknowledgements We would like to gratefully acknowledge the contribution of the clinical professionals of HS_CHC, specifically to Doctor Natália Martins and Doctor Ramon Salvado for providing us access to data and information systems, as well as for all the help in the requirements analysis and discussion of the proposed model. We are also gratefully to Eng. Vasco Saavedra, Enga Ana Luisa Ramos, Eng. Igor Carreira and Doctor José Moreira for the fruitful discussion about this issue. References [1] E. Ammenwerth N. Shaw Bad health informatics can kill – is evaluation the answer? Methods of Information in Medicine 44 2005 1 3 [2] J. Horsky J. Zhang V.L. Patel To err is not entirely human: complex technology and user cognition Journal of Biomedical Informatics 38 2005 264 266 [3] S. Garde P. Knaup Requirements engineering in health care: the example of chemotherapy planning in paediatric oncology Requirements Engineering 11 2006 265 278 [4] M. Maguire Methods to support human-centred design International Journal of Human–Computer Studies 55 2001 587 634 [5] M. Del-Beccaro, H. Jeffries, M. Eisenberg, E. Harry, Computerized provider order entry implementation: no association with increased mortality rates in an intensive care unit, Pediatrics 118 (2006) 290–295. [6] Y. Han J. Carcillo S. Venkataraman R. Clark S. Watson T. Nguyen H. Bayir R. Orr Unexpected increased mortality after implementation of a commercially sold computerized physician order entry system Pediatrics 116 2005 1506 1512 [7] R. Koppel J.P. Metlay A. Cohen B. Abaluck A.R. Localio S.E. Kimmel B.L. Strom Role of computerized physician order entry systems in facilitating medication errors Journal of the American Medical Association 293 2005 1197 1203 [8] E. Ammenwerth C. Iller U. Mansmann Can evaluation studies benefit from triangulation? A case study International Journal of Medical Informatics 70 2003 237 248 [9] J. Aarts P. Gorman IT in health care: sociotechnical approaches “To Err is System” International Journal of Medical Informatics 76 2007 S1 S3 [10] S. Rinkus M. Walji K.A. Johnson-Throop J.T. Malin J.P. Turley J.W. Smith J. Zhang Human-centered design of a distributed knowledge management system Journal of Biomedical Informatics 38 2005 4 17 [11] J. Zhang Human-centered computing in health information systems. Part 1. Analysis and design Journal of Biomedical Informatics 38 2005 1 3 [12] G.M. Samaras R.L. Horst A systems engineering perspective on the human-centered design of health information systems Journal of Biomedical Informatics 38 2005 61 74 [13] B.W. Boehm Software risk management: principles and practices Software, IEEE 8 1991 32 41 [14] Standish_Group, The CHAOS report. Technical report, Standish Group, 1995. [15] A. Seffah J. Gulliksen M.C. Desmarais An introduction to human-centered software engineering: integrating usability in the development process Human-Centered Software Engineering – Integrating Usability in the Software Development Lifecycle 2005 Springer Netherlands 3 14 [16] G. Southon C. Sauer K. Dampney Lessons from a failed information systems initiative: issues for complex organisations International Journal of Medical Informatics 55 1999 33 46 [17] S.H. Stumpf R.R. Zalunardo R.J. Chen Barriers to telemedicine implementation: usually it's not technology issues that undermine a project – it's everything else Healthcare Informatics 19 2002 45 48 [18] R. Heeks Health information systems: failure, success and improvisation International Journal of Medical Informatics 75 2006 125 137 [19] C. Johnson T. Johnson J. Zhang Increasing productivity and reducing errors through usability analysis: a case study and recommendations Proceedings AMIA Symposium 2000 394 398 [20] V Vimarlund T. Timpka Design participation as insurance: risk- management and end-users participation in the development of information systems in healthcare organizations Methods of Information in Medicine 41 2002 76 80 [21] M.C. Beuscart-Zéphir J. Brender R. Beuscart I. Ménager-Depriester Cognitive evaluation: how to assess the usability of information technology in healthcare Computer Methods and Programs in Biomedicine 54 1997 19 28 [22] C. Carroll P. Marsden P. Soden E. Naylor J. New T. Dornan Involving users in the design and usability evaluation of a clinical decision support system Computer Methods and Programs in Biomedicine 69 2002 123 135 [23] M. Irestig T. Timpka Politics and technology in health information systems development: a discourse analysis of conflicts addressed in a systems design group Journal of Biomedical Informatics 41 2008 82 94 [24] M. Ji-Ye V. Karel W.S. Paul C. Tom The state of user-centered design practice Communications of the ACM 48 2005 105 109 [25] M. Myers, Investigating information systems with ethnographic research, Communications of the AIS 2 (1999) 18 [26] B. Nuseibeh S. Easterbrook Requirements engineering: a roadmap Proceedings of the IEEE International Conference on Software Engineering 2000 ICM Press Limerick, Ireland 35 46 [27] B.H.C. Cheng J.M. Atlee Research directions in requirements engineering Proceedings of the IEEE International Conference on Software Engineering: Future of Software Engineering 2007 IEEE Computer Society Minneapolis (MN), USA 285 303 [28] P. Sawyer G. Kotonya Software requirements A. Abran J.W. Moore P. Bourque R. Dupuis SWEBOK – Guide to the Software Engineering Body of Knowledge 2001 IEEE Computer Society California 30 55 [29] A. Abran J.W. Moore P. Bourque R. Dupuis SWEBOK – Guide to the Software Engineering Body of Knowledge 2004 ed. 2004 IEEE Computer Society California [30] I. Sommerville Software Engineering 8th ed. 2007 Addison-Wesley Harlow [31] J.M. Carroll W.A. Kellogg M.B. Rosson The Task–Artifact Cycle, Designing Interaction: Psychology at the Human–Computer Interface 1991 Cambridge University Press 74–102 [32] R. Fairley R. Thayer The concept of operations: the bridge from operational requirements to technical specifications Annals of Software Engineering 3 1997 417 432 [33] C.M. Johnson T.R. Johnson J. Zhang A user-centered framework for redesigning health care interfaces Journal of Biomedical Informatics 38 2005 75 87 [34] J. Zhang V.L. Patel K.A. Johnson J.W. Smith Designing human-centered distributed information systems IEEE Intelligent Systems 17 2002 42 47 [35] ISO-13407, International Organization for Standardization /ISO 13407:1999, Human-centred design processes for interactive systems, 1999 [36] M.-C. Beuscart-Zéphir F. Anceaux H. Menu S. Guerlinger L. Watbled F. Evrard User-centred, multidimensional assessment method of clinical information systems: a case-study in anaesthesiology International Journal of Medical Informatics 74 2005 179 189 [37] A. Kushniruk Evaluation in the design of health information systems: application of approaches emerging from usability engineering Computers in Biology and Medicine 32 2002 141 149 [38] J. Zhang Human-centered computing in health information systems. Part 2. Evaluation Journal of Biomedical Informatics 38 2005 173 175 [39] S. De Rouck A. Jacobs M. Leys A methodology for shifting the focus of e-health support design onto user needs: a case in the homecare field International Journal of Medical Informatics 77 2008 589 601 [40] M. Berg J. Aarts J. Van Der Lei ICT in health care: sociotechnical approaches Methods of Information in Medicine 42 2003 297 301 [41] J.H. Gennari C. Weng J. Benedetti D.W. McDonald Asynchronous communication among clinical researchers: a study for systems design International Journal of Medical Informatics 74 2005 797 807 [42] E.W. Chauncey Triangulation: the explicit use of multiple methods, measures, and approaches for determining core issues in product development Interactions 13 2006 46–ff [43] M.A Kaulio I.C.M. Karlsson Triangulation strategies in user requirements investigations: a case study on the development of an IT-mediated service Behaviour & Information Technology 17 1998 103 112 [44] J. Greene C. McClintock Triangulation in evaluation: design and analysis issues Evaluation Review 9 1985 523 545 [45] B. Glaser A. Strauss The Discovery of Grounded Theory: Strategies for Qualitative Research 1967 Aldine Publishing Company Chicago, Illinois [46] N.K. Denzin Y.S. Lincoln The SAGE Handbook of Qualitative Research 3rd ed. 2005 Sage Publications Thousand Oaks, CA [47] A. Strauss J. Corbin Basics of Qualitative Research: Techniques and Procedures for Developing Grounded Theory 2nd ed. 1998 Sage Thousand Oaks, CA [48] G.H. Galal R.J. Paul A qualitative scenario approach to managing evolving requirements Requirements Engineering 4 1999 92 102 [49] C.E. Kuziemsky G.M. Downing F.M. Black F. Lau A grounded theory guided approach to palliative care systems design International Journal of Medical Informatics 76 2007 S141 S148 [50] V. Aggarwal The application of the unified modeling language in object-oriented analysis of healthcare information systems Journal of Medical Systems 26 2002 383 397 [51] F. Banhart R. Lohmann An object-oriented approach for structuring the electronic medical record Studies in Health Technology and Informatics 77 2000 622 626 [52] C. Egyhazy S. Eyestone J. Martino C. Hodgson Object-oriented analysis and design: a methodology for modeling the computer-based patient record Topics in Health Information Management 19 1998 48 65 [53] E. Kuikka A. Eerola J. Porrasmaa A. Miettinen J. Komulainen Design of the SGML-based electronic patient record system with the use of object-oriented analysis methods Studies in Health Technology and Informatics 68 1999 838 841 [54] M. Krol D.L. Reich Object-oriented analysis and design of a health care management information system Journal of Medical Systems 23 1999 145 158 [55] M. Hakman T. Groth Object-oriented biomedical system modeling – the rationale Computer Methods and Programs in Biomedicine 59 1999 1 17 [56] Y.-M. Zhu S.M. Cochoff An object-oriented framework for medical image registration, fusion, and visualization Computer Methods and Programs in Biomedicine 82 2006 258 267 [57] H.-A. Park I. Cho N. Byeun Modeling a terminology-based electronic nursing record system: an object-oriented approach International Journal of Medical Informatics 76 2007 735 746 [58] G. Booch J. Rumbaugh I. Jacobson The Unified Modeling Language User Guide 1999 Addison-Wisley Reading, MA [59] J.L. Whitten L.D. Bentley K.C. Dittman Systems Analysis and Design Methods 6th ed. 2004 McGraw-Hill/Irwin New York [60] L. Teixeira C. Ferreira B.S. Santos N. Martins Modeling a web-based information system for managing clinical information in hemophilia care Proceedings of 28th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS) 2006 IEEE CNF NY, USA 2610 2613 [61] S. Patricia Multiple methods and the usability of interface prototypes: the complementarity of laboratory observation and focus groups Proceedings of the 9th Annual International Conference on Systems Documentation Chicago 1991 106 112 [62] D. Diaper N.A. Stanton The Handbook of Task Analysis for Human–Computer Interaction 2004 Lawrence Erlbaum Associates Mahwah, New Jersey [63] J. Annett K.D. Duncan R.B. Stammers M.J. Gray Task Analysis 1971 Her Majesty's Stationery Office – HMSO London [64] J. Stuart R. Penn TaskArchitect: taking the work out of task analysis Proceedings of the 3rd Annual Conference on Task Models and Diagrams Prague, Czech Republic 2004 ACM 145 154 [65] J. Nielsen Designing Web Usability: The Practice of Simplicity 2000 New Riders Indianapolis [66] A. Dix J. Finley G. Abowd B. Russell Human–Computer Interaction 2nd ed. 1998 Prentice Hall London [67] B. Shneiderman C. Plaisant Designing the User Interface: Strategies for Effective Human–Computer Interaction 4th ed. 2005 Pearson Boston, MA [68] L. Teixeira C. Ferreira B.S. Santos Using task analysis to improve the requirements elicitation in health information system Proceedings of 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS) 2007 IEEE CNF Lyon, France 3669 3672 [69] T. Cortes A. Lee J. Boal L. Mion A. Butler Using focus groups to identify asthma care and education issues for elderly urban-dwelling minority individuals Applied Nursing Research 17 2004 207 212 "
    },
    {
        "doc_title": "Exploring different parameters to assess left ventricle global and regional functional analysis from coronary CT angiography",
        "doc_scopus_id": "84879705431",
        "doc_doi": "10.1111/j.1467-8659.2012.02090.x",
        "doc_eid": "2-s2.0-84879705431",
        "doc_date": "2012-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Cardiac cycles",
            "Cardiac phasis",
            "Clinical practices",
            "Coronary artery disease",
            "Coronary ct angiographies",
            "Left ventricles",
            "Parameters characterizing",
            "Visual exploration"
        ],
        "doc_abstract": "Coronary CT angiography is widely used in clinical practice for the assessment of coronary artery disease. Several studies have shown that the same exam can also be used to assess left ventricle (LV) function. Even though coronary CT angiography provides data concerning multiple cardiac phases, along the cardiac cycle, LV function is usually evaluated using just the end-systolic and end-diastolic phases. This unused wealth of data, mostly due to its complexity and the lack of proper tools, has still to be explored to assess if further insight is possible regarding regional LV functional analysis. Furthermore, different parameters can be computed to characterize LV function and though some are well known by clinicians others still need to be tested concerning their value in clinical scenarios. Based on these premises, we present several parameters characterizing global and regional LV function, computed for several cardiac phases over one cardiac cycle. The data provided by the computed parameters is shown using a set of visualizations allowing synchronized visual exploration of the different data. The main purpose is to provide means for clinicians to explore the data and gather insight over their meaning and their correlation with each other and with diagnosis outcomes. © 2012 The Authors.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Results from geospatial analysis of resistivity to delineate contamination anomalies: A case study of a Controlled Dump - North Portugal",
        "doc_scopus_id": "84867492970",
        "doc_doi": "10.4133/1.4721868",
        "doc_eid": "2-s2.0-84867492970",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Geophysics",
                "area_abbreviation": "EART",
                "area_code": "1908"
            },
            {
                "area_name": "Geotechnical Engineering and Engineering Geology",
                "area_abbreviation": "EART",
                "area_code": "1909"
            },
            {
                "area_name": "Environmental Engineering",
                "area_abbreviation": "ENVI",
                "area_code": "2305"
            }
        ],
        "doc_keywords": [
            "Contaminant dispersion",
            "Differential weathering",
            "Environmental consequences",
            "Geo-spatial analysis",
            "Geochemical anomaly",
            "Geostatistical method",
            "Municipal solid waste (MSW)",
            "Software applications"
        ],
        "doc_abstract": "The assessment of contaminant dispersion in Controlled Dumps (CD) of Municipal Solid Waste (MSW) is possible through the combination of geophysical, geochemical and statistical methods. The methodology applied in this study will contribute to evaluate the environmental consequences of the Matosinhos CD (N Portugal), which is set in a granitic crystalline geological context with a permeability controlled by differential weathering which, in turn, is associated with fracturing. The statistical methods that we will describe are usually used in geochemical anomaly determination and were adapted to be used with resistivity data obtained from twenty-two 2D profiles performed around the CD. The data set was inverted and the results were processed and visualized by a 3D software application that we are currently developing. Groundwater samples were also collected in piezometers, upstream and downstream of the CD, with the aim of directly confirming the presence of contaminants indirectly detected by the geophysical and geostatistical methods described before. The combination of these different approaches allows, in our opinion, an improved approach towards the detection and delineation of contaminant plumes from these deposits.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Web-enabled registry of inherited bleeding disorders in Portugal: Conditions and perception of the patients",
        "doc_scopus_id": "84355166743",
        "doc_doi": "10.1111/j.1365-2516.2011.02574.x",
        "doc_eid": "2-s2.0-84355166743",
        "doc_date": "2012-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Hematology",
                "area_abbreviation": "MEDI",
                "area_code": "2720"
            },
            {
                "area_name": "Genetics (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2716"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Local and national haemophilia registries are powerful instruments to support the healthcare and researchers and improve the communication between Comprehensive Haemophilia Diagnostic and Treatment Centres (HTCs) and patients. Hemo@care is an example of a Local Haemophilia Registry Systems (LHR-Sys) based on the Web, developed in collaboration with a HTC located in Portugal, to support the haemophilia treatments registry, collect and manage the clinical information and provide mechanisms to control the clotting factor concentrates (CFC) stock. To extend this solution (the hemo@care) to other Portuguese HTCs and consequently to meet the preconditions to create a National Haemophilia Registry Systems (NHR-Sys), a study based on a questionnaire was carried out at nationwide. This study aims to assess the conditions and motivations of people with haemophilia (PWH) geographically scattered throughout the country, to use a potential Web-enabled registry with the purpose of replacing the traditional paper-diaries, to understand their judgment about a potential NHR-Sys currently non-existent in Portugal, and at the same time, to characterize demographically and pathologically those people at the nationwide. The results based on the analysis of 168 responses (response rate of 31%) confirmed the high prevalence of the disease in haemophilia A (75%) compared with haemophilia B (11.3%) and a large incidence in the severe levels, or the existence of people with mild severity without diagnosis and treatment. Furthermore, the results also revealed the need, conditions and motivation for using a registry system by PWH; thus it is deemed to justify the extension of the hemo@care to other HTCs in Portugal and consequently to create the NHR-Sys. © 2011 Blackwell Publishing Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inter-observer variability assessment of a left ventricle segmentation tool applied to 4D MDCT images of the heart",
        "doc_scopus_id": "84862625754",
        "doc_doi": "10.1109/IEMBS.2011.6090923",
        "doc_eid": "2-s2.0-84862625754",
        "doc_date": "2011-12-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Cardiac angiography",
            "Cardiac phasis",
            "Computed Tomography",
            "Interobserver variability",
            "Left ventricles",
            "MDCT images",
            "Multiple detectors",
            "Myocardial perfusion",
            "Segmentation tool",
            "Semi-automatics"
        ],
        "doc_abstract": "Multiple detector row computed tomography (MDCT) cardiac angiography provides a large amount of data concerning multiple cardiac phases which are not often considered. Segmentation is a first step towards exploring how this additional data can be used to perform left ventricle functional analysis or myocardial perfusion assessment. We present preliminary results regarding the assessment of inter-observer variability for a semi-automatic (multi-phase) segmentation tool for the left-ventricle. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using participatory design in a health information system",
        "doc_scopus_id": "84862293238",
        "doc_doi": "10.1109/IEMBS.2011.6091321",
        "doc_eid": "2-s2.0-84862293238",
        "doc_date": "2011-12-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Health information systems",
            "Participatory design",
            "Project managers",
            "Requirement analysis",
            "User centred design"
        ],
        "doc_abstract": "This article describes the experience of developing an interactive Health Information System (iHIS) currently under test in a hospital, which benefited from the practices of the User-Centred Design (UCD), in a Participatory Design (PD) approach. Techniques from the Human-Computer Interaction (HCI) and/or Usability Engineering (UE), combined with traditional Software Engineering (SE), allowed an effective and usable solution from the user's point of view. The good results usually achieved with this approach were confirmed. Despite these good results, we deem that if there is not some control of the procedure by the project manager, it may be difficult to end the requirement analysis, since requirement reformulation is fostered. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using Participatory Design in a Health Information System.",
        "doc_scopus_id": "84055190037",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84055190037",
        "doc_date": "2011-12-01",
        "doc_type": "Article",
        "doc_areas": [],
        "doc_keywords": [],
        "doc_abstract": "This article describes the experience of developing an interactive Health Information System (iHIS) currently under test in a hospital, which benefited from the practices of the User-Centred Design (UCD), in a Participatory Design (PD) approach. Techniques from the Human-Computer Interaction (HCI) and/or Usability Engineering (UE), combined with traditional Software Engineering (SE), allowed an effective and usable solution from the user's point of view. The good results usually achieved with this approach were confirmed. Despite these good results, we deem that if there is not some control of the procedure by the project manager, it may be difficult to end the requirement analysis, since requirement reformulation is fostered. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inter-observer variability assessment of a left ventricle segmentation tool applied to 4D MDCT images of the heart.",
        "doc_scopus_id": "84055176594",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84055176594",
        "doc_date": "2011-12-01",
        "doc_type": "Article",
        "doc_areas": [],
        "doc_keywords": [],
        "doc_abstract": "This article describes the experience of developing an interactive Health Information System (iHIS) currently under test in a hospital, which benefited from the practices of the User-Centred Design (UCD), in a Participatory Design (PD) approach. Techniques from the Human-Computer Interaction (HCI) and/or Usability Engineering (UE), combined with traditional Software Engineering (SE), allowed an effective and usable solution from the user's point of view. The good results usually achieved with this approach were confirmed. Despite these good results, we deem that if there is not some control of the procedure by the project manager, it may be difficult to end the requirement analysis, since requirement reformulation is fostered. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DETI-interact: Interacting with public displays through mobile phones",
        "doc_scopus_id": "80052438203",
        "doc_doi": null,
        "doc_eid": "2-s2.0-80052438203",
        "doc_date": "2011-09-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Human-computer",
            "Informatics",
            "Large screen",
            "Large screen displays",
            "Public display",
            "Public Screens",
            "Usability"
        ],
        "doc_abstract": "This work describes the necessary steps in the development of DETI interact: an information system for the exhibition of informative content from the Department of Electronics, Telecommunications, and Informatics of the University of Aveiro. This system enables users to interact with the contents displayed in a large screen in the lobby of the department through an Android mobile device. © 2011 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating user studies into computer graphics-related courses",
        "doc_scopus_id": "80052317465",
        "doc_doi": "10.1109/MCG.2011.78",
        "doc_eid": "2-s2.0-80052317465",
        "doc_date": "2011-09-07",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "computer graphics education",
            "graphics and multimedia",
            "Human-computer",
            "Information visualization",
            "Student collaboration",
            "User study"
        ],
        "doc_abstract": "The authors argue in favor of introducing user studies into computer graphics, human-computer interaction, and information visualization courses. They discuss two sets of user studies they developed and performed over several years, with student collaboration, and the different aspects of the studies they had to consider. They also discuss a user study they designed for an information visualization course. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Myocardial perfusion analysis from adenosine-induced stress MDCT",
        "doc_scopus_id": "79959984147",
        "doc_doi": "10.1007/978-3-642-21257-4_89",
        "doc_eid": "2-s2.0-79959984147",
        "doc_date": "2011-07-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic assessment",
            "Computed Tomography",
            "Coronary artery disease",
            "Image modality",
            "Myocardial perfusion",
            "Myocardial perfusion analysis",
            "Ventricular functions",
            "Automatic assessment",
            "Coronary artery disease",
            "Image modality",
            "Induced stress",
            "Multiple detectors",
            "Myocardial perfusion",
            "Myocardial perfusion analysis",
            "Ventricular functions"
        ],
        "doc_abstract": "Myocardial perfusion assessment is of paramount importance for the diagnosis of coronary artery disease. This can be performed using different image modalities such as single-photon emission computed tomography (SPECT) or magnetic resonance imaging (MRI). Recently, cardiac multiple-detector computed tomography (MDCT) has shown promising results with the benefit of gathering data regarding coronary anatomy, ventricular function and myocardial perfusion in a single study. Preliminary results for three different methods for automatic assessment of myocardial perfusion from adenosine-induced stress MDCT are presented. © 2011 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Image denoising methods for tumor discrimination in high-resolution computed tomography",
        "doc_scopus_id": "80052901081",
        "doc_doi": "10.1007/s10278-010-9305-6",
        "doc_eid": "2-s2.0-80052901081",
        "doc_date": "2011-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computed Tomography",
            "Denoising methods",
            "Figures of merits",
            "Geometric mean",
            "High-resolution computed tomography",
            "HRCT images",
            "Image denoising methods",
            "Intensity analysis",
            "Poisson noise",
            "Reconstruction error",
            "Tomographic reconstruction",
            "Visual analysis",
            "Visual inspection",
            "Wavelet denoising",
            "Wiener filtering"
        ],
        "doc_abstract": "Pixel accuracy in images from high-resolution computed tomography (HRCT) is ultimately limited by reconstruction error and noise. While for visual analysis this may not be relevant, for computer-aided quantitative analysis in either densitometric, or shape studies aiming at accurate results, the impact of pixel uncertainty must be taken into consideration. In this work, we study several denoising methods: geometric mean filter, Wiener filtering, and wavelet denoising. The performance of each method was assessed through visual inspection, profile region intensity analysis, and global figures of merit, using images from brain and thoracic phantoms, as well as several real thoracic HRCT images. Copyright © 2010 by Society for Imaging Informatics in Medicine.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A preparatory study to choose similarity metrics for left-ventricle segmentations comparison",
        "doc_scopus_id": "79955785537",
        "doc_doi": "10.1117/12.878294",
        "doc_eid": "2-s2.0-79955785537",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Computational costs",
            "Ground truth",
            "Image processing and analysis",
            "Left ventricles",
            "preparatory studies",
            "Quantitative measures",
            "Segmentation methods",
            "Similarity metrics"
        ],
        "doc_abstract": "In medical image processing and analysis it is often required to perform segmentation for quantitative measures of extent, volume and shape. The validation of new segmentation methods and tools usually implies comparing their various outputs among themselves (or with a ground truth), using similarity metrics. Several such metrics are proposed in the literature but it is important to select those which are relevant for a particular task as opposed to using all metrics and therefore avoiding additional computational cost and redundancy. A methodology is proposed which enables the assessment of how different similarity and discrepancy metrics behave for a particular comparison and the selection of those which provide relevant data. © 2011 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using color in visualization: A survey",
        "doc_scopus_id": "79251621495",
        "doc_doi": "10.1016/j.cag.2010.11.015",
        "doc_eid": "2-s2.0-79251621495",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Color map",
            "Color scale",
            "Computer display",
            "Data and information",
            "Experimental research",
            "Guidelines",
            "Output devices",
            "Visual representations"
        ],
        "doc_abstract": "Color mapping is an important technique used in visualization to build visual representations of data and information. With output devices such as computer displays providing a large number of colors, developers sometimes tend to build their visualization to be visually appealing, while forgetting the main goal of clear depiction of the underlying data. Visualization researchers have profited from findings in adjoining areas such as human vision and psychophysics which, combined with their own experience, enabled them to establish guidelines that might help practitioners to select appropriate color scales and adjust the associated color maps, for particular applications. This survey presents an overview on the subject of color scales by focusing on important guidelines, experimental research work and tools proposed to help non-expert users. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2010-12-19 2010-12-19 2014-10-09T12:26:23 S0097-8493(10)00184-6 S0097849310001846 10.1016/j.cag.2010.11.015 S300 S300.2 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20110401 20110430 2011 2010-12-19T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb vol volfirst volissue volumelist yearnav figure body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor primabst pubtype ref 0097-8493 00978493 false 35 35 2 2 Volume 35, Issue 2 17 320 333 320 333 201104 April 2011 2011-04-01 2011-04-30 2011 Virtual Reality in Brazil Luciano Pereira Soares a Judith Kelner b a Pontifical Catholic University of Rio de Janeiro, Tecgraf Rio de Janeiro, Brazil b Universidade Federal de Pernambuco (UFPE), Centro de Informática (CIn), Virtual Reality and Multimedia Group (GRVM), Cidade Universitária , Recife, PE, Brazil Visual Computing in Biology and Medicine Bernhard Preim c Charl P. Botha d c University of Magdeburg, Department of Computer Science, Magdeburg,Germany d Department of Mediamatics, Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands Semantic 3D media and content Bianca Falcidieno e Ivan Herman f e Cons. Nazionale delle Ricerche (CNR), Ist. di Matematica Applicata e Tecn. Inform., Genova, Italy f World Wide Web Consortium (W3C) and Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands Cultural Heritage Alan Chalmers g Mark Mudge h Luís Paulo Santos i g The Digital Laboratory, WMG, The University of Warwick, UK h Cultural Heritage Imaging, San Francisco, CA 94107, USA i Departamento de Informatica, University of Minho, Campus de Gualtar, Braga, Portugal article sco Copyright © 2010 Elsevier Ltd. All rights reserved. USINGCOLORINVISUALIZATIONASURVEY SILVA S 1 Introduction 2 Color models 2.1 Device-dependent color models 2.2 Device-independent color models 3 Desired properties for color scales 4 Univariate representations 4.1 Color model components 4.2 Redundant color scales 4.3 Double-ended color scales 5 Multivariate representations 5.1 Multivariate color scales 5.2 Color blending and weaving 6 Visualization characteristics and color scale selection 6.1 Data types 6.2 Spatial frequency 6.3 Task/goals 6.4 Visualization type 6.5 Audience, cultural connotations and context 7 Learning through experimentation 7.1 Which Blair project 7.2 Face-based luminance matching 7.3 Other examples 8 Auxiliary tools and methods 8.1 Dynamic exploration of color mappings 8.2 PRAVDAColor 8.3 ColorBrewer 8.4 Self-corrected perceptual colormaps 8.5 Enhancing visual exploration by appropriate color coding 8.6 VisCheck and Daltonize 8.7 Other examples 9 Conclusion Acknowledgment References BRODLIE 1992 K SCIENTIFICVISUALIZATIONTECHNIQUESAPPLICATIONS CARD 1999 S READINGSININFORMATIONVISUALIZATIONUSINGVISIONTHINK MACDONALD 1999 20 35 L BORLAND 2007 14 17 D SILVA 2007 943 948 S PROCEEDINGS11THINTERNATIONALCONFERENCEINFORMATIONVISUALIZATIONIV07 MORECOLORSCALESMEETSEYEAREVIEWUSECOLORINVISUALIZATION GONZALEZ 2002 R DIGITALIMAGEPROCESSING BRATKOVA 2009 42 55 M FOLEY 1990 J COMPUTERGRAPHICSPRINCIPLESPRACTICE LANDA 2005 436 443 E LEVKOWITZ 1992 72 80 H TRUMBO 1981 220 226 B ROGOWITZ 1996 268 273 B ROBERTSON 1986 24 32 P ROGOWITZ 1998 52 59 B WANG 2008 1739 1754 L WARE 1988 41 49 C ROBERTSON 1988 50 63 P HAGHSHENAS 2007 1270 1277 H CHUANG 2009 1275 1282 J SPENCE 2006 R INFORMATIONVISUALIZATIONDESIGNFORINTERACTION SALOMON 1990 269 278 G ARTHUMANCOMPUTERINTERFACEDESIGN NEWUSESFORCOLOR LIGHT 2004 385 391 A TEDFORD 1977 145 150 W CLEVELAND 1983 101 105 W JULESZ 1981 91 97 B DZMURA 1991 951 966 M BAUER 1996 1439 1446 B BAUER 1998 1083 1093 B ANDRIENKO 2006 N EXPLORATORYANALYSISSPATIALTEMPORALDATA BARTRAM 2002 66 79 L MEYER 1988 28 40 G BERLIN 1991 B BASICCOLORTERMSUNIVERSALITYEVOLUTION SAUNDERS 2002 302 355 B REGIER 2009 439 446 T HEALEY 1999 145 167 C GREGORY 1998 R EYEBRAINPSYCHOLOGYSEEING CHUANG 2009 203 211 J KOSARA 2003 20 25 R RHEINGANS 1990 145 146 P SILVAX2011X320 SILVAX2011X320X333 SILVAX2011X320XS SILVAX2011X320X333XS item S0097-8493(10)00184-6 S0097849310001846 10.1016/j.cag.2010.11.015 271576 2014-10-12T07:31:45.595335-04:00 2011-04-01 2011-04-30 true 2783623 MAIN 14 58808 849 656 IMAGE-WEB-PDF 1 si0019 114 13 8 si0018 114 13 8 si0017 114 13 8 si0016 114 13 8 si0015 114 13 8 si0014 114 13 8 si0013 114 13 8 si0012 114 13 8 si0011 114 13 8 si0010 114 13 8 si0009 114 13 8 si0008 114 13 8 si0007 114 13 8 si0006 114 13 8 si0005 114 13 8 si0004 114 13 8 si0003 114 13 8 si0002 114 13 8 si0001 367 12 102 gr9 889905 2274 3583 gr8 73328 833 833 gr7 582004 2010 2250 gr6 650541 1717 2500 gr5 724889 1678 3591 gr4 31161 432 400 gr3 24106 289 400 gr2 222538 635 1300 gr15 438188 1360 2500 gr14 142664 631 1300 gr13 127438 1258 1723 gr12 230509 1022 1710 gr11 1481375 2251 3583 gr10 1849286 2720 3583 gr1 145691 1133 1027 gr9 94600 513 809 gr8 17905 188 188 gr7 69471 454 508 gr6 76116 388 565 gr5 81297 379 811 gr4 14637 162 150 gr3 12340 108 150 gr2 40192 239 489 gr15 49326 307 565 gr14 35618 237 489 gr13 21202 284 389 gr12 36005 231 386 gr11 195665 508 809 gr10 151157 614 809 gr1 40809 426 386 gr9 14764 139 219 gr8 3355 164 164 gr7 13896 163 183 gr6 17825 150 219 gr5 11426 102 219 gr4 12461 164 152 gr3 15673 158 219 gr2 10558 107 219 gr15 11251 119 219 gr14 11017 106 219 gr13 2761 160 219 gr12 13862 131 219 gr11 16406 138 219 gr10 18797 164 216 gr1 9532 163 148 CAG 2079 S0097-8493(10)00184-6 10.1016/j.cag.2010.11.015 Elsevier Ltd Fig. 1 From left to right and top to bottom: greyscale, rainbow, heated-object, and linearized optimal color scales applied to a data set. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Color blending (left) and color weaving (right). Reproduced with permission from [23]. © 2003 IEEE. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 3 Quantitative scales available in ColorBrewer 2.0 [29] suitable for nominal data representation. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 4 Sequential scales available in ColorBrewer 2.0 [29] suitable for ordinal data representation. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 Two frequency modulated gratings represented using a saturation varying color scale (center) and a luminance varying color scale (right). Reproduced with permission from [30]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 6 Segmented color scales applied to low (top row) and high (bottom row) spatial frequency data. Reproduced with permission from [13]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 7 (a) The eight color scales used in the Which Blair Project [47]; (b) The eight overlapping LAB greyscales (top) and the results of applying them to the Blair photograph and; (c) An example of a faces matrix used in the experiment. Reproduced with permission from [47]. © 2001 IEEE. Fig. 8 Double face image. Reproduced with permission from [56]. © 2002 IEEE. Fig. 9 A screen capture showing PRAVDAColor integrated in the Data Explorer environment. Reproduced with permission from [33]. © 1995 IEEE. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 10 Three-dimensional color scale construction tool. Reproduced with permission from [33]. © 1995 IEEE. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 11 ColorBrewer's interface [29]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 12 ColorBrewer's interface details [29]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 13 Measured perception function for a rainbow color scale and for a normalized rainbow color scale. Reproduced with permission from [62]. Fig. 14 Left, standard rainbow color scale applied to a topographic data; right, a normalized rainbow color scale applied to the same data. Reproduced with permission from [62]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 15 On the left, color scale adapted for a segmentation task according to boxplot quartiles; on the right, changing the mapping function allowed a proper comparison of different regions. Reproduced with permission from [63]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) ☆ This article was recommended for publication by Robert van Liere. Technical Section Using color in visualization: A survey Samuel Silva a b ⁎ B. Beatriz Sousa Santos a b Joaquim Madeira a b a Institute of Electronics and Telematics Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 234 370 500; fax: +351 234 370 545. Color mapping is an important technique used in visualization to build visual representations of data and information. With output devices such as computer displays providing a large number of colors, developers sometimes tend to build their visualization to be visually appealing, while forgetting the main goal of clear depiction of the underlying data. Visualization researchers have profited from findings in adjoining areas such as human vision and psychophysics which, combined with their own experience, enabled them to establish guidelines that might help practitioners to select appropriate color scales and adjust the associated color maps, for particular applications. This survey presents an overview on the subject of color scales by focusing on important guidelines, experimental research work and tools proposed to help non-expert users. Keywords Visualization Color scales Color maps Guidelines 1 Introduction Visualization [1] is concerned with representing, manipulating and exploring data and information graphically in such a way as to gain understanding and insight into it, i.e., mapping of data to a visual form that supports human interaction in a workspace for visual sense making [2]. Color mapping is a very important visualization technique, but the choice of the most appropriate color scale to use with a particular data set is not just a matter of choosing a colorful and visually attractive representation. Adding color which does not add additional insight to the visualization can sometimes cause confusion as users try to understand its meaning and should, therefore, be avoided [3]. So, it is particularly important to perform the right choice in order to build visualizations which depict the desired information in a clear way. Throughout the years researchers have studied such issues and, profiting from findings in other fields such as human vision and psychophysics, managed to establish some guidelines which might help users along the process of color scale selection according, for example, to the type of data and task to be performed. Nevertheless, those guidelines are still not always used by visualization builders, and some well-documented problems are still ignored by the visualization community [4]. Extending a previous paper by the authors [5], a brief overview on the subject of color use in visualization is presented, providing information on the main concerns, findings and resulting guidelines, hopefully encouraging researchers to seek new solutions, evaluate the use of color in their visualizations and share their experience, thus contributing to a deeper knowledge on the subject. After a short introduction to color models, this survey focuses on the desired properties for color scales and the use of color representations for univariate and multivariate data, and discusses other factors conditioning the use of color in visualization (e.g., data features, tasks to be accomplished and target audience), while mentioning the guidelines that should drive the choice of appropriate color scales and representations, as well as the advantages of applying such guidelines. Afterwards, experimental research work on the field and some tools proposed to help non-expert users are described. Finally, some conclusions regarding the existing guidelines and their usage are presented. 2 Color models The purpose of a color model is to allow the specification of colors in a standard way. In essence, a color model is a specification of a coordinate system, and a subspace within that system, where each color is represented by a single point [6]. Several color models are described in the literature. Each of them has its own characteristics and is more or less suited to particular tasks. Therefore, before carrying on with the use of color in visualization, it is important to present a general overview on the different ways color can be represented. In general, color models can be divided in two classes: device-dependent, when the model allows the representation of the color gamut of a particular device and the same coordinates can represent slightly different colors depending on the device features; and device-independent, when the model provides a representation of color using a coordinate system independent of any output device. A brief description of these two classes is presented in the following subsections. For extended information on the subject of color models and a wider range of references the reader is forwarded to Bratkova et al. [7], which presents a new color space for computer graphics. 2.1 Device-dependent color models In the RGB color model each color is defined by adding three primaries: red, green and blue. This is analogous to what happens in a CRT display where the phosphor has similar base chromaticities. Since there is no strict value for the chromaticity of the three primaries, the same RGB coordinates can result in slightly different colors on different output devices. The CMY color model uses cyan, magenta and yellow as primaries, which are the complementary colors of red, green and blue, respectively. Thus, while RGB is an additive color model, i.e., by representing what is added to blackness, CMY is subtractive, representing what is subtracted to white light. This color model is usually used in color printers. In many situations black is added to this model, in order to allow a better representation of darker colors, and such color model is identified as CMYK. Although the RGB color model is based on the way color is represented in a CRT monitor (and thus sometimes called, along with the CMY color model, hardware-oriented [8]), it does not relate well with the way color is intuitively perceived. Thus, as an alternative, two additional (user-oriented) color models have been proposed: HSV (hue, saturation and value) and HSL (hue, saturation and lightness). These are based on the intuitive appeal of a painter's tint, shade, and tone. 2.2 Device-independent color models The three primaries red, green and blue cannot be used to represent all visible colors (at least using only positive values). In 1931 the Commission Internationale de l’Éclairage (CIE) [9] defined a new color model, CIE XYZ to avoid this problem [8]. Three new standard primaries (X, Y and Z) were defined, thus allowing a specification of all visible colors using only positive values. Two additional color models have been defined, derived from CIE XYZ, which are perceptually uniform: CIE LUV and CIE LAB. In a perceptually uniform color model the euclidean distance between a pair of colors in the color space is directly connected with their perceptual distance, i.e., if two pairs of colors have the same euclidean distance among them, their perceptual distance is the same. The first perceptually uniform color system, which was also an influence to CIE LAB, was the Munsell color system [10] which is still in use. 3 Desired properties for color scales Given a sequence of numerical values { v 1 ≤ v 2 , … ≤ v N } represented by colors {c 1, c 2, …, c N }, respectively, it is possible to identify the following desirable properties [11,12] for a color scale: Order—The colors chosen to represent the numerical values must be perceived as having the same order as them, i.e., if the values are ordered, the colors chosen to represent them must also seem ordered. An example can be the representation of a temperature scale by using the notions of cold and warm colors and their proportional mixtures in order to obtain a scale from cold to hot temperatures. It is important to note the special case of nominal data [13]: objects should be distinguishably different but, since they are not ordered, there should be no perceptual ordering in the representation. Uniformity and representative distance—The color representation of two values should convey the distance between them, and colors representing values which equally differ from each other should also seem equally different. Beyond that, it is required that clearly separated values must be represented by distinguishable colors, and that close values must be represented by colors perceived to be closer. This is what Trumbo [12] calls the separation principle. When representing flow information, for example, complementary colors can be used to represent flows in opposite directions and similar colors (with slight differences) to represent flows in the same direction. Levkowitz et al. [11] identify analogous principles proposed by Pizer et al. [14] (associability) and Robertson et al. [15] (separation). Boundaries—If there are no boundaries on the represented numerical data the color scale should not create this effect, i.e., the color scale must be able to represent continuous scales. Rows and columns principle—This is one of the principles proposed by Trumbo [12] which applies only to bivariate information. It states that if it is important to preserve univariate information, then the display parameters must not obscure one another, i.e., rows or columns having a constant value of one variable must have constant hue, saturation, or brightness. For example, using two display primaries (e.g., red and green) goes against this principle. Diagonal principle—The second principle proposed by Trumbo which only applies to bivariate information states that if the detection of positive association of variables is a goal, the displayed colors must be easily identified as belonging to one of the three classes: the ones near the minor diagonal, the ones above it, and the ones below. This could be accomplished with the major diagonal made up of greys, elements of maximum saturation, or constant hue. A hue and lightness scheme violates the diagonal principle [13]. 4 Univariate representations When using a color scale to represent univariate data, each color represents a single scalar value. It can be a continuous color scale, if color varies along the scale in such a way that adjacent colors are similar to one another, or a discontinuous color scale if that does not happen. In what follows some examples of continuous color scales are presented (according to a survey by Rheingans [16]). Additional examples can be found in [17]. 4.1 Color model components Grey scale—This color scale (see Fig. 1 ) maps scalars to brightness. It consists in a variation from black to white, with black representing, in general, the lowest value and white the highest. While this color scale presents some advantages, such as an easy to perceive ordering (the different and increasing brightness levels), it suffers from the fact that it displays a low contrast between the different colors, which limits its use in quantitative tasks. Rainbow scale—This is one of the most popular color scales used in the visualization literature [4]. It consists in a color path along the different colors of the rainbow, built by varying hue while keeping saturation and contrast at fixed values. For example, in HSV, it consists in a complete rotation around the value (V) axis. Regardless of its popularity, this color scale presents several problems [18]. For example, to some users it might not present an intuitive ordering unless they are familiar with the color progression (light spectrum). The position occupied by some of the colors might also lead to problems. Yellow is present half way through the color scale. This means that if one is interested in depicting extreme values the middle values might interfere, since yellow has an highlighting effect being perceived as brighter than the other colors. Another important aspect whenever using yellow in a visualization is that it has the smallest number of perceived saturation steps [19]. Therefore, users find it harder to distinguish small saturation variations for yellow than, for example, for blue. Another issue might be the fact that this color scale goes from red to violet. Since these colors are quite similar, both extremes will get visually close. To avoid this problem the color scale is usually cut at blue (as seen in Fig. 1). 4.2 Redundant color scales Using multiple display parameters to represent data may have several advantages [16]. First, one can draw benefits from the characteristics of various display parameters in conveying different kinds of information: while brightness is more effective conveying shape, hue is better in providing distinguishable display levels. This kind of redundancy might also help in dealing with situations where one of the parameters becomes ambiguous (e.g., due to a visual deficiency) and is compensated by the others. Finally, using redundancy might also allow a better distinction between values by reinforcing their visual differences. Ware [20] has studied redundant color scales and proved their utility by performing several empirical studies, which led to a suggestion that a color scale varying in both luminance and hue can be used to accurately represent both metric and surface properties. Some examples of such scales are: • Redundant model components—A straightforward redundant scale can be built by mapping data values to both hue and brightness. That kind of color scale has the advantage of being suitable for use by someone with dichromatic color deficiency. • Heated-object scale—This scale represents a compromise between the grey scale and the rainbow scale. It goes from black to white, passing through orange and yellow. This color scale has a stronger perceived natural ordering than the rainbow scale, since it has a monotonic increase in brightness (see Fig. 1). • Linearized optimal color scale—This color scale was introduced by Levkowitz et al. [11] to describe a scale which maximizes the number of JNDs (just noticeable differences) while preserving a natural order (see Fig. 1). 4.3 Double-ended color scales Such color scales are created by joining two monotonically increasing scales at a common end point. For example, one can join a scale from grey to red and a scale from grey to blue, building a scale from red to grey to blue. With such color scales it is possible to visually represent high, low and middle values clearly, since they exhibit three distinct groups of colors. A recent work by Moreland [21] discusses the creation and use of double-ended color scales, for example, as an alternative to the rainbow color scale. 5 Multivariate representations It is common that a single visualization requires the depiction of multivariate data. Multivariate color scales, and color blending and weaving are presented in what follows, as alternatives for representing multivariate data. 5.1 Multivariate color scales In a multivariate color scale two or more data variables are mapped to a single color representing them all. This is the same principle as the one used with redundant scales, but now each display parameter is related with a different variable. Working with the RGB color model, it is possible to map a variable into each one of its components, thus creating a multivariate color scale. For example, Landsat “false color” images are commonly produced by representing three multispectral scanner bands with levels of red, green and blue [22]. Therefore, if the represented bands are highly correlated, the image will be composed of shades of grey, since the three components will have close values. This scheme has the advantage that the extremes of the variable range (black, red, green, blue) are easily detectable. An analogous scheme can be obtained using, for example, the HSL color model. For more details on the possible approaches see [16]. A problem occurs when one needs to decompose the shown colors in their components. How can we detect similarities between areas that have the same value for two components but differ on the third? 5.2 Color blending and weaving Another approach for multivariate representations using color can be to use different color scales for each variable and then blend the results. This approach might also pose a difficulty in identifying individual values for each variable. Urness et al. [23] introduced a technique named color weaving, applied to flow visualization, which consists in coloring the line integral convolution using side-by-side streamlines of the different colors which need to be represented in a region. Fig. 2 shows an example of this technique compared with color blending. A study by Hagh-Shenas et al. [24] compares user performance when analyzing information represented on maps using color weaving and the more traditional color blending (consisting in a linear combination of all the colors which need to overlap in a region). Color weaving is performed by representing the individual colors side-by-side, in a high frequency texture which fills the region. Results show that users perform better when color weaving is used, particularly when more than two colors are being represented. Nevertheless, performance decreases for more than four colors. A possible alternative to extend this limit might be a technique earlier presented by Shenas et al. [25] which uses color and texture. Further work regarding color weaving can be found, for example, in Luboschik et al. [26] who present color weaving applied to scatter plots. Recently, Wang et al. [19] proposed a framework which uses a set of guidelines to support choosing the proper colors for a visualization when color blending is used, e.g., to depict objects that partially overlap and where perceiving the stacking order (which object is in front/back) is important. Whenever two semi-transparent objects overlap, it is easier to perceive which one is in front if their colors have opposite hues (e.g., red and blue) than closer hues (e.g., red and green), since the overlapping region in the first case will exhibit a hue closer to the object which is in front (or at least neutral) while, in the latter case, the overlapping region will present a new (shifted) hue. In situations where more than two objects overlap, a set of hues must be chosen as to avoid any hues which can be generated by mixing other hues in the set: a general rule is to choose opposing hues for the two most important objects and then neutral colors for all others. Furthermore, for situations where multiple objects overlap, using different opacities might also help users to perceive object order by decreasing object opacity from front to back. Saturation might also play an important role in avoiding drastic hue changes in overlapping regions, or in object order perception, by increasing saturation for front objects and decreasing it for back objects. Finally, Wang et al. also found that cold colors (e.g., blue and green) tended to be perceived as being in front, even if their opacity was lower than remaining objects. The opposite was true for warm colors (e.g., red and yellow). This was an unexpected result since it is the opposite effect which is, in general, observed (due to chromostereopsis [3]). For several examples the reader is forwarded to Wang et al. [19]. Chuang et al. [27] extended Wang et al. [19] work. Their main improvement is that they manage to perform blending by preserving the hues of each object and avoiding the appearance of new (shifted) hues, with no need for an initial careful selection of opposing hues. They also introduce the concept of dominant color, i.e., the color that has more impact on the final image as opposed to Wang et al. which always consider the foremost hue to be the most important. 6 Visualization characteristics and color scale selection The color scales and methods used to build a visualization do not depend only on the number of data variables involved. Several factors such as the characteristics of the data, the tasks which need to be accomplished or the target audience are important enough to be considered. 6.1 Data types When designing a visualization (i.e., picking a color scale) care must be taken to ensure that the most striking features of the image reflect the most important features of the data. If a representation catches the user's attention with unimportant features of the data, more interesting features might be missed [16]. Bright colors, sharp boundaries, or high saturation areas will most likely catch the user's attention. Thus, it is important to consider the data that will be represented and their type, and know what is more important: for example, to call attention to middle values or to positive/negative deviations from a zero (threshold). It is possible to distinguish between four data types [28]: Nominal data—For nominal data no mathematical operations are possible, since the value assigned to a particular measurement represents a name. An example is the categorization of different lung diseases with numerical labels 1, 2, 3 and 4: no mathematical operation is meaningful on these data. Here, numbers are identifiers. As noted above, the representation used for these kinds of data should not implicitly order it. A bad choice would be to use a grey scale since users intuitively perceive an order from dark to bright or vice-versa [4]. Fig. 3 shows several segmented color scales provided by ColorBrewer [29] (part of its qualitative color scales set) which could be used to represent nominal data. The number of colors used to represent nominal data should be restricted to seven or less [3]. This guideline is based on two constraints: users ability to discriminate between colors and their ability to remember the meaning of each color while looking into the visualization. Ordinal data—With ordinal data, values are assigned to measurements (for example), but no assumption is made about the spacing in-between such measurements, i.e., there can exist a numbering of 1, 2, 3 and 4 but the distance between elements 1 and 2 cannot be assumed to be equal to the distance between 3 and 4. Ordinal data are inherently discrete [30]. The used representation should allow discrimination between objects and the perception of their relative order. The quite common rainbow scale, for example, would not be a good idea to represent ordinal data. Even though it can be said that it is an ordered scale, if we consider wavelengths, it is not perceptually ordered [31,32,4]. Fig. 4 shows several examples of suitable segmented scales provided by ColorBrewer [29] (part of the sequential color scales set). Interval data—For interval data, the numerically equal distances between values are assumed to be actually equal. These kinds of data are commonly a (experimental) measure such as temperature, etc. The used representation should account for this: equal steps in data values should correspond to equally perceived magnitude in the representation. A perceptually uniform color space can be used to help choosing the appropriate colors. Ratio data—On ratio data, ratios between values are assumed to be equal and values increase/decrease monotonically about a true zero or threshold. This feature should be preserved in the representation. An example is absolute temperature measured using the Kelvin scale. If the ratio data have a relevant zero crossing, it can be preserved by using a double-ended color scale with a transition at zero. If the data are to be represented by a segmented color scale, then it is probably a good idea to have an even number of steps (with a transition at the zero level). 6.2 Spatial frequency An important issue to consider when choosing a color scale is human spatial vision. The luminance and saturation mechanisms in human vision play an important role in spatial sensitivity, but they have different characteristics. The human visual system accurately processes high-resolution images, or data which varies rapidly over an area, if that spatial variation is represented as a variation in luminance, i.e., the luminance channels are responsible for processing high spatial frequency information. This means that, when representing data with a high spatial frequency, it is a good idea to use a color scale which provides a strong luminance variation across the data range [33]. On the other hand, the saturation mechanisms in human vision are more sensitive to low spatial frequency variations. Such kind of effects are illustrated in Fig. 5 . On the top row, a frequency modulated grating beginning at one cycle per image and increasing in spatial frequency is presented (the corresponding waveform is presented on the first column). The variation is represented using a saturation varying color scale (in the center) and a luminance varying color scale (on the right). Notice how the saturation-variation color scale makes the sinusoidal variation more visible, at the low frequency end of the spectrum, than the luminance varying color scale. On the bottom row, the opposite happens: with the saturation varying color scale (in the center) you can only observe the first few cycles of the frequency modulated grating, observing twice as many when using the luminance varying color scale (on the right). Looking, for example, at interval and ratio data, both luminance and saturation varying color scales can produce the effect of having equal steps in data to be represented by equal perceived steps on the color scale, but the first will most certainly be more adequate to high spatial frequency data variations, while the second will be more suited for low spatial frequency variations. 6.3 Task/goals The goal of a particular visualization is an important issue when choosing a color scale. Tasks which require the judgment of metric quantities in the data tend to be better accomplished with color scales which do not vary monotonically in the opponent color channels (brightness, red–green, yellow–blue). On the other hand, tasks involving qualitative judgments about value distribution shape are better served with color scales varying systematically in brightness, allowing our visual system to employ familiar shape-from-shading mechanisms [16]. In situations where the perceived size of an object is important, care must be taken in how the object is colored. An early study by Tedford et al. [34] found a significant color-size effect leading to a conclusion that warm colors such as red, orange and yellow appear larger than cool colors like green. In another study, by Cleveland et al. [35], users were asked to judge, on a map with equal colored areas in red and green, which one was the largest: the average observers considered the red areas were larger. The obtained results suggest also that the color–size effect grows stronger for very saturated colors, which indicates that these colors might not be the better choice for tasks where the user is expected to make judgments about size. Considering the kind of task to perform, the color scale can be designed accordingly [13]. For example, regarding segmentation tasks, some of the rules used to create isomorphic color scales for ratio and interval data are also useful for creating maps for segmented data. In high spacial frequency data, luminance can be used to convey monotonicity; in low spacial frequency data monotonicity can be conveyed through the saturation component. When creating a segmented color scale, it is necessary that the segments be discriminable from one another. This will limit the number of steps which can be represented. Bergman et al. [33] state that a higher number of steps can be effectively discriminated for low spatial frequency data than for high spatial frequency. In Fig. 6 , on the left side, a five-level segmented color scale is used and, on the right side, a ten-level segmented color scale is used. They are applied to low spatial frequency data (top) and high spatial frequency data (bottom). For the low spatial frequency data (top row), having additional levels provides additional information. For this particular case, additional features of the earth's magnetic field are revealed (notice the southern hemisphere). On the contrary, on the bottom row, showing high spatial frequency cloud fraction observations, additional features are not revealed by increasing the number of color scale steps. Another notable example concerns highlight tasks. The principles which should guide the selection of a color scale for highlighting particular features in the data can be found, for example, in Julesz [36]. Using those principles, it is possible to design color scales which draw attention to a particular range in the data. When dealing with high spatial frequency data, for example, a possible approach can be that of using varying saturation to represent the information (maintaining constant luminance), while modifying the hue in the regions needing to be highlighted. The work of D’Zmura [37] and later work by Bauer et al. [38,39] have treated the topic of linear separability, which can also help in highlighting tasks. When a target has a color which can be separated from all other background colors with a single straight line in color space, it will be easier to detect, i.e., its perceived difference from all others will be clearer. Another important aspect regarding hue is that similar hues are perceived as part of the same group [19] which leads to, for example, separate regions on the display having similar hues being perceived as having a connection. This is useful if the goal of the visualization is to detect groups within the global data. The different groups can be represented by different (more distant) hues. Nevertheless, it should be noted that color should probably be reserved for the most important groups (while using other graphical dimensions as shape and size for other groupings) or this advantage will be reduced [40]. Recently, Tominsky et al. [41], based on a task model proposed by Andrienko et al. [42], have presented several strategies for adapting the color mapping to specific tasks such as segmentation, localization (highlight) and comparison. For lookup tasks, for example, they use two techniques, histogram equalization and box-whisker plot adaptation, in order to build a color mapping which assigns a higher number of distinguishable colors to the value ranges where more data values are concentrated. 6.4 Visualization type It is important to consider the whole visualization during the process of color scale design for the individual elements. For example, 3D visualizations have different constraints than those imposed by 2D visualizations. A good example of the problems that can occur is related with shading: if users use shading cues to judge the 3D shape of a representation object (e.g., an isosurface); a brightness varying color scale might interfere with the brightness values resulting from the shading calculations. Nonetheless, a brightness varying scale may be used in planar objects on a 3D scene. Due to the way human vision focuses on the different colors [3] in a 2D visualization a red region is perceived as closer to the user than a blue region thus creating a depth effect. Therefore, adjacent regions of strong blue and red should be avoided to prevent this effect. Another issue might arise from the requirement of displaying multiple variables in the same visualization. The used color scales should not generally overlap, with the representation for each variable interfering with the others as little as possible. For this purpose, the weaving techniques reviewed above might provide a solution. When representing multivariate data, motion can also play an important role as it provides an additional dimension. Even though color is commonly considered irrelevant in the perception of object movement, Weiskopf [43] proposes a set of five guidelines, supported by a set of psychophysical results described in the literature, which concern the use of color to better convey motion in visualization. Those guidelines focus on improving motion perception and the discrimination of motion direction, on using moving color patterns to highlight and visually group data, and on the influence of the chromatic and achromatic channels in apparent speed perception. The author also proposes two calibration procedures, which are necessary for the proper application of some of the proposed guidelines, regarding isoluminance and speed calibration. Application examples are also presented concerning data grouping and flow visualization. It is also important to note that motion can sometimes be more effective to call users attention to a particular element of the visualization than just changing its color as shown by Bartram et al. [44,45] and therefore using both motion and color in concurrent ways, e.g., to highlight two objects, one using motion and the other using color (as opposed to Weiskopf's work where color is used to help convey motion), might lead to problems, for example, with the motion highlight diverting attention from the color highlight. 6.5 Audience, cultural connotations and context It is important to have information about the target audience of a visualization, since it can provide some clues for color scale design. For example, conventions in one application area might place blue/violet colors of a spectrum scale at the low end (in order of increasing wavelength), while in another they may be placed at the high end (in order of increasing frequency) [16]. Therefore, paying attention to area conventions may make the process of designing the color scale easier and help avoid unintentional breaks with viewer expectations. Furthermore, vision associated problems must also be considered, e.g., older people tend to be less sensitive to color. But one of the most common problems is color blindness. This is an important factor to have in mind, since it can seriously influence user performance by limiting the amount of information that can be extracted from color representations. This can be overcome by providing several parameters which users can adjust, while using hue as the only way of encoding information must be avoided. The literature [46] also describes several tests which can be used to detect color blindness problems in users. Color can also have strong cultural connotations varying from culture to culture. Following such cultural conventions it is possible, for example, to reduce the cognitive load on the viewer or use connotations that suggest natural linkings between a variable or a variable value and the color used to represent it. For example, for an USA audience the color green is connected/associated with the color of money; a natural connotation, when visualizing temperatures can be that of high temperatures represented in red and low temperatures in blue [16]. And what about the influence of culture in color cognition? Without going into much detail (given the complexity of the subject), there is active discussion regarding how culture and language might influence color cognition. On one side (among others), Berlin and Kay [48] argue that color cognition is innate and that color category perception/cognition is universal regardless of the language spoken. This is called the universalist view. On the other side, the relativist view (e.g., Saunders et al. [49]) argues that color cognition is a much more culture-related phenomenon and has pointed several flaws in Berlin and Kay's study regarding the methodology used and several a priori assumptions. Salomon et al. [31] state that cultural differences limit the number and categories of color recognized by an individual. Recently, Regier and Kay [50] argued that a purely universalist or relativist view on this subject cannot stand against recent findings and, thus, a new view gathering ideas from both sides might be a more proper approach. Regarding color categories, Kawai et al. [51] have shown that the named category in which each individual color is inserted (by people) can influence perceived color difference, i.e., if two colors belong to the same named category (e.g., blue) their perceived difference will appear smaller than the perceived difference between two colors, at the same euclidean distance from each other, but in different color categories (e.g., blue and purple). For a good illustration of these effects and a description of obtained experimental results see Healey et al. [52]. The question remains regarding how the relativist view, considering color cognition is influenced by cultural issues, would influence these findings across cultures. It is also important to understand the influence of external conditions and how adaptation mechanisms of the eye work: for example, the negative afterimage of what has been previously seen can result in visual stress from prolonged viewing; as well as the phenomenon of time dependent color perception influenced, for instance, by different environmental lightings [53] and display device properties. Providing adjustable parameters can be a good solution to deal with different output device characteristics. For instance, it is important to remember that the same RGB coordinates may result in slightly different colors in different output devices and, therefore, to render colors accurately some calibration process might be necessary. Furthermore, the display device might influence our choice of color scale to meet, for example, power consumption goals in portable devices. Researchers have been striving to propose color scales which lower display device power consumption. A recent example is the work of Chuang et al. [54] which propose color scales allowing up to a 40% save in energy. 7 Learning through experimentation In order to apply theoretical principles coming from other areas (such as psychophysics), verify the applicability of new principles, and find clues for the definition of new ones, many researchers have been conducting user studies [55]. Human color vision, a subject well studied (for more than a century now), provides strong clues for using color in visualization. However, the choice of colors for a particular task is more difficult, as it is far more complex, than the simple displays used by the experimental psychologists. So, experiments are necessary to fill the gap between theory and practice [55]. The main goal is to use well established theories to build design guidelines and then use an experiment to validate the guidelines in an applied setting. 7.1 Which Blair project The work of Rogowitz et al. [47] presents a method which uses visual judgments to perceptually evaluate color scales. Since the literature points out that color scales which monotonically increase in luminance are good candidates for representing the magnitude of continuous data, the proposed method was designed to identify scales that include a monotonic luminance component. For that purpose, an image of a human face was used (Tony Blair), since it provides a structured visual pattern with gradual variations in luminance across its surface, thus providing the identification of irregular variations in the luminance of color scales. The experiment started from eight color scales (shown in Fig. 7 a) and, for each of them, seven additional scales where created, each subtending 25% of the original scale, which gave a total of 64 color scales. Fig. 7b shows the eight overlapping CIELAB greyscales created and how they looked when applied to Tony Blair's image. During the experiment, users were shown 32 pictures randomly ordered for each observer (Fig. 7c shows an example of part of one of the image matrices used). They then had to rate each picture according to the degree to which the image appeared to be a recognizable photograph of a face. The obtained results seemed to show that the proposed method may function as a quick procedure of identifying color scales with monotonically increasing luminance with the advantage that it does not require display calibration or lengthy psychophysical procedures. 7.2 Face-based luminance matching The work of Kindlmann et al. [56] is similar to the one presented by Rogowitz et al. [47]. They address the problem of color scale luminance control by proposing a novel technique for luminance matching. Their technique, given a fixed reference color, and a test color with brightness varied by the user, allows matching the luminance of both. They use images of faces in their experiment since they want to take advantage of humans being good at recognizing faces, due to brain circuitry dedicated to this process [57]. Their method starts by a black-and-white thresholded image of a human face: shadows in black and directly lit areas in white. The test pattern is then composed by the images in Fig. 8 which also include a reverse image. Next, they replace black with a shade of grey and white with a color. According to the luminance of the colored region, in comparison with the luminance of the chosen grey, one of the faces will appear positive and the other negative. For example, in Fig. 8 the left face is positive. Therefore, to match the luminance between the two regions it is only necessary to adjust the intensity of the grey or color region until the point where none of the faces appears positive or negative. The authors claim that their method provides very good results and enables the creation of color scales, with any pre-determined pattern of luminance, in devices (e.g., monitors) which are not calibrated. 7.3 Other examples Other examples are the work of Ware [20], which leads to some rules guiding the process of color scale construction, of Healey [58], which presents a technique for effectively choosing multiple colors for use during data visualization, and of Montag [59], where the performance in judging values in univariate maps encoded using five different color scales is tested. 8 Auxiliary tools and methods During the past few years some efforts have been made in order to provide users (in particular non-experts) with tools and methods which allow them to select an appropriate color scale for their particular visualization purpose. 8.1 Dynamic exploration of color mappings Rheingans et al. [60] propose a tool which allows the exploration of data sets by interactively manipulating the color scale. On the upper left of the screen appears the image space which shows the currently selected color scale applied to the data. In the center of the screen a 3D color space appears and a curve, within it, shows the path defining the sequence of colors composing the color scale. As the path in the 3D color space is modified, the image is dynamically changed accordingly. 8.2 PRAVDAColor Bergman et al. [33] present a tool called PRAVDAColor which focuses on helping users to select color scales. With that purpose, they have built a library of color scales and defined a set of perceptual rules which allow selecting appropriate maps according to the structure of the data and visualization goal. They present a taxonomy for color scale selection which guides their work: starting from the type (ratio, interval, ordinal or nominal) and spatial frequency of the data, and according to the representation task (isomorphic, segmentation or highlight), several guidelines are proposed (see Bergman et al. [33]). Fig. 9 shows PRAVDAColor integrated in the Data Explorer environment. With PRAVDAColor the user is presented a set of color scales judged appropriate to the data set (based essentially on the data spatial frequency, but also on the presence/absence of a zero crossing in order to distinguish between ratio and interval data) and task. At first, one of those color scales is automatically applied to the data in order to produce a first representation, then the user can freely apply any other of the available color scales to the data. It is also possible to control the way the color scale is mapped onto the data by defining to which colors the data minimum, maximum and midpoint values will be mapped. To build the color scales another tool is also available (similar to the one proposed by [60]). 1 1 For recent information on 3D color pickers see Wu et al. [61]. It is a 3D color scale builder. The user can interactively build a path along a hue–lightness–saturation (HLS) double cone (Fig. 10 ). This tool provides undo and move functions for editing that path. It is then possible to use the created color scale along the Data Explorer pipeline. The output color scale can be a linear interpolation between the selected points on the HLS space, or the user may obtain a discrete color scale, with equally sized segments, for each selected point (which may be suitable for segmented representation tasks). 8.3 ColorBrewer ColorBrewer [29] (recently updated to its version 2.0), developed using Macromedia's Flash, is not exactly a tool to create color scales or directly decide the better color scale for a data set. Instead, it provides an environment and a set of color scales to help users choose the best color options for maps (see Fig. 11 ). The process starts by defining the number of data classes the user wants to represent. Then, he chooses the type of legend: sequential, diverging or qualitative. After that, some legends are presented using different color schemes and the user can choose among them. An example map is then colored according to the user's choice. It is possible to modify several map parameters: activate/deactivate a road network, city symbols, or region borders and their colors (using the interface presented in Fig. 12 ); zoom; and modify the color with which each of these parameters appear. This tool also provides an interesting feature: it informs the user about some properties of the chosen representation, namely: 1. If it is suitable for color blind people. 2. If it will withstand black and white photocopying. 3. If it is suitable to view on LCD monitors. 4. If it is color printing friendly (according to tests made on some printers). This information is provided by the symbols presented on the left of Fig. 12 (a red cross over a symbol means that the chosen color scheme does not support that property). In the interface area presented on the right of Fig. 12 it is possible to see the chosen color scheme and obtain the parameters defining each color using one of two color models (RGB and CMYK). The appearance of roads, cities and region borders can also be enable/disabled in this area and the color for each of these elements can be chosen. A new feature in ColorBrewer 2.0 is the ability to select a background for the map (a solid color or terrain data) and then choose the level of transparency applied to the overlayed data. The limitation of this tool is the fact that there is no possibility of using a different data set to perform the testing. 8.4 Self-corrected perceptual colormaps Gresh [62] presents an algorithm which, given a particular color scale, transforms it into one that is more perceptually uniform, i.e., equal steps in data values are equally perceived. The perception function for a particular user on a given monitor is experimentally measured and then, based on those results, the color scale is modified to make it as uniform as possible. Fig. 13 shows the original measured perception function and the perception function measured for a normalized rainbow color scale. Notice how the new color scale presents a flatter perception function. Fig. 14 shows, on the left, a standard rainbow color scale applied to a topographic data set and, on the right, the same data set colored with the obtained normalized rainbow color scale. It is possible to perceive more detail in the image on the right, specially in the blue region and a reduction of the green region. 8.5 Enhancing visual exploration by appropriate color coding The work of Schulze-Wollgast et al. [63] extends the methods proposed by Bergman et al. [33] by extracting statistical metadata from the data set, and then using that information to adapt the chosen color scale. The used metadata consists in the average, median, mode, minimum, maximum, skewness and quartiles of the data set. While all these values can be computed for quantitative data, only a subset can be computed for ordinal and nominal data. In the latter case, only the mode can be computed. Based on such data several automatic color scale modifications may occur which include adjusting control points for setting an appropriate mapping function. Fig. 15 shows how a color scale has been adjusted for a segmentation task by positioning control points according to boxplot quartile position. Changing the mapping function may be necessary to correctly deal with certain data distributions. Linear interpolation between control points is the most common approach, but can lead to problems if the data are not uniformly distributed (e.g., due to outliers), in which case the scale is stretched leading to unnoticeable differences between close values. This is solved by using nonlinear mapping functions. The decision is supported by analysing the skewness of the data distribution: if it is positive an exponential mapping function is used; if it is negative a logarithmic mapping function. Fig. 15, on the right, shows how changing the mapping function allows a better comparison among regions. Another feature presented by Schulze-Wollgast et al. [63] is a color legend which includes a boxplot side-by-side with the color scale, thus providing a better insight on the data distribution. This feature can be observed in Fig. 15. 8.6 VisCheck and Daltonize VisCheck [64] is a tool which shows how an image or site is seen by an user with some kind of colorblindness. It is possible to choose among three different types of vision deficiencies, deuteranope (red/green deficit), protanope (another red/green deficit) and tritanope (blue/yellow deficit), and then see how it influences the perceived colors. Daltonize [65] is a tool which allows correcting a particular image in order to allow a better perception of color differences by users with vision deficiencies. 8.7 Other examples Other examples are the work of Hyun [66], which deals with the creation and usage of non-linear color scales, and of Ventura et al. [67], which deals with the problem of ordered/unordered color scales, and its generation in a device-independent color space, through a computer-aided color coding system based on fundamental principles of human vision. 9 Conclusion Given the importance of color in data and information visualization, an overview of relevant work regarding the use of color in visualization scenarios is presented. From surveyed literature, it is clear that more detailed knowledge is of paramount importance for an informed use of color in visualization. The guidelines proposed by several authors do not provide a solution for all scenarios, but help users to understand the advantages and disadvantages of using particular color scales in specific situations, and contribute for a greater awareness to possible issues in their visualizations. The fact that these guidelines are produced with the support of experimental work is very important, as it increases their value and helps in understanding the subjacent concepts and ideas. The tools proposed to help create color scales which are suited for particular data or visualization goals, or which provide environments where color scales can be applied to data using specific taxonomies, allow that several of those guidelines be transparently (or, at least, with some support) applied by users. But, even though these guidelines and tools exist, they are not being systematically used by practitioners. In fact, one of the most problematic color scales known (the rainbow color scale) is still the most used in visualization tools [4]. Therefore, efforts must be made to motivate the community to pay more attention to the issues related with color usage in visualization, and to justify their choices with more than an aesthetic motive. The surveyed literature also reveals how important it is to learn with the work of others and will hopefully motivate researchers to share their experiences in different application areas where visualization is present. By presenting this survey, we aim to provide a contribution to such efforts. Acknowledgment The first author's work is funded by grant SFRH/BD/38073/2007 awarded by the Portuguese Science and Technology Foundation (FCT). References [1] K. Brodlie L. Carpenter R. Earnshaw J. Gallop R. Hubbold A. Mumford Scientific visualization, techniques and applications 1992 Springer [2] S. Card J. Mackinlay B. Shneiderman Readings in information visualization: using vision to think 1999 Morgan Kauffman Publishers [3] L. MacDonald Using color effectively in computer graphics IEEE Computer Graphics & Applications 19 4 1999 20 35 [4] D. Borland R. Taylor Rainbow color map (still) considered harmful IEEE Computer Graphics & Applications 27 2 2007 14 17 [5] S. Silva J. Madeira B. Sousa Santos There is more to color scales than meets the eye: a review on the use of color in visualization Proceedings of the 11th international conference on information visualization (IV07) 2007 IEEE Computer Society Zurich, Switzerland 943 948 [6] R. Gonzalez R. Woods Digital image processing 2002 Addison-Wesley [7] M. Bratkova S. Boulos P. Shirley oRGB pratical opponent color space for computer graphics IEEE Computer Graphics & Applications 29 1 2009 42 55 [8] J.D. Foley A. van Dam S.K. Feiner J.F. Hughes Computer graphics: principles and practice 1990 Addison-Wesley [9] CIE, Comission internationale de l’eclairage 〈 〉 , online March 2010. [10] E. Landa M. Fairchild Charting color for the eye of the beholder American Scientist 93 5 2005 436 443 [11] H. Levkowitz G. Herman Color scales for image data IEEE Computer Graphics & Applications 12 1 1992 72 80 [12] B. Trumbo Theory for coloring bivariate statistical maps The American Statistician 35 4 1981 220 226 [13] B. Rogowitz L. Treinish How not to lie with visualization Computers in Physics 10 3 1996 268 273 [14] Pizer SM, Zimmerman JB, Johnston RE. Contrast transmission in medical image display. In: Proceedings of the 1st international symposium on medical imaging and interpretation; 1982. p. 2–9. [15] P.K. Robertson J.F. O’Callaghan The generation of color sequences for univariate and bivariate mapping IEEE Computer Graphics & Applications 6 2 1986 24 32 [16] Rheingans P. Task-based color scale design. In: Proceedings of the SPIE—28th AIPR workshop: 3D visualization for data exploration and decision making, vol. 3905; 2000. p. 35–43. [17] Bourke P. Color ramping for data visualization 〈 〉 , online March 2010. [18] B. Rogowitz L. Treinish Data visualization: the end of the rainbow IEEE Spectrum 35 12 1998 52 59 [19] L. Wang J. Giesen K. McDonnell P. Zoliker K. Mueller Color design for illustrative visualization IEEE Transactions on Visualization and Computer Graphics 14 6 2008 1739 1754 [20] C. Ware Color sequences for univariate maps: theory, experiments, and principles IEEE Computer Graphics & Applications 8 5 1988 41 49 [21] Moreland K. Diverging colormaps for scientific visualization. In: Proceedings of the 5th international symposium on visual computing. Lecture Notes in Compuer Science, vol. 5876; 2009. p. 92–103. [22] P.K. Robertson Visualizing color gamuts: a user interface for the effective use of perceptual color spaces in data displays IEEE Computer Graphics & Applications 8 5 1988 50 63 [23] Urness T, Interrante V, Marusic I, Longmire E, Ganapathisubramani B. Effectively visualizing multi-valued flow data using color and texture. In: Proceedings of the IEEE Visualization 2003 (VIS 2003); 2003. p. 115–21 [24] H. Hagh-Shenas S. Kim V. Interrante C.G. Healey Weaving versus blending: a quantitative assessment of the information carrying capacities of two alternative methods for conveying multivariate data with color IEEE Transactions on Visualization and Computer Graphics 13 6 2007 1270 1277 [25] Shenas H, Interrante V. Compositing color with texture for multi-variate visualization. In: Proceedings of the 3rd international conference on computer graphics and interactive techniques in Australasia and South East Asia; 2005. p. 443–6 [26] Luboschik M, Radloff A, Schumann H. A new weaving technique for handling overlapping regions. In: Proceedings of the international conference on advanced visual interfaces (AVI’10); 2010. p. 25–32. [27] J. Chuang D. Weiskopf T. Möller Hue-preserving color blending IEEE Transactions on Visualization and Computer Graphics 15 6 2009 1275 1282 [28] R. Spence Information visualization: design for interaction 2nd ed 2006 Prentice Hall [29] Brewer CA. ColorBrewer 2.0 〈 〉 , online March 2010. [30] Rogowitz B, Treinish L. Why should engineers and scientists be worried about color? 〈 〉 , online March 2010. [31] G. Salomon New uses for color B. Laurel The art of human computer interface design 1990 Addison-Wesley 269 278 [32] A. Light P.J. Bartlein The end of the rainbow? Color schemes for improved data graphics EOS Transactions American Geophysical Union 85 40 2004 385 391 [33] Bergman LD, Rogowitz BE, Treimish LA. A rule-based tool for assisting color map selection. In: Proceedings of the IEEE visualization ’95, 1995. p. 118–25. [34] W.H. Tedford S.L. Berquist W.E. Flynn The size-color illusion The Journal of General Psychology 97 1977 145 150 [35] W.S. Cleveland R. McGill A color-caused optical illusion on a statistical graph The American Statistician 37 2 1983 101 105 [36] B.T. Julesz The elements of texture perception, and their interactions Nature 290 12 1981 91 97 [37] M. D’Zmura Color in visual search Vision Research 31 6 1991 951 966 [38] B. Bauer P. Jolicouer W.B. Cowan Visual search for color targets that are or are not linearly-separable from distractors Vision Research 36 1996 1439 1446 [39] B. Bauer P. Jolicouer W.B. Cowan The linear separability effect in color visual search: ruling out the additive color hypothesis Perception and Psychophysics 60 6 1998 1083 1093 [40] Color usage research lab: Using color in information display graphics 〈 〉 , online September 2010. [41] Tominski C, Fuch G, Schumann H. Task-driven color coding, In: Proceedings of the 12th international conference on information visualization (IV08); 2008. p. 373–80 [42] N. Andrienko G. Andrienko Exploratory analysis of spatial and temporal data 2006 Springer Berlin, Germany [43] Weiskopf D. On the role of color in the perception of motion in animated visualizations. In: Proceedings of the IEEE visualization 2004 (VIS 2004); 2004, p. 305–12. [44] Bartram L, Ware C, Calvert T. Moving icons: detection and distraction. In: Proceedings of the international conference on human–computer interaction (INTERACT 2001); 2001. p. 157–65. [45] L. Bartram C. Ware T. Calvert Filtering and integrating visual information with motion Information Visualization 1 1 2002 66 79 [46] G. Meyer D. Greenberg Color defective vision and computer graphics displays IEEE Computer Graphics & Applications 8 5 1988 28 40 [47] Rogowitz B, Kalvin AD. The which blair project: a quick visual method for evaluating perceptual color maps. In: Proceedings of the IEEE visualization 2001 (VIS 2001); 2001. p. 21–6. [48] B. Berlin P. Kay Basic color terms: their universality and evolution 1991 Cambridge University Press [49] B. Saunders J. Brakel The trajectory of color Perceptions on Science 10 3 2002 302 355 [50] T. Regier P. Kay Language, thought and color: Whorf was half right Trends in Cognitive Sciences 13 10 2009 439 446 [51] Kawai M, Uchikawa K, Ujike H. Influence of color category in visual search. In: Proceedings of the annual meeting association for research in vision and ophtalmology; 1995. p. 2991. [52] C.G. Healey J.T. Enns Large datasets at a glance: combining textures and colors in scientific visualization IEEE Transactions on Visualization and Computer Graphics 5 2 1999 145 167 [53] R. Gregory Eye and brain: the psychology of seeing 5th ed 1998 Oxford University Press p. 133–5 [54] J. Chuang D. Weiskopf T. Möller Energy aware color sets Computer Graphics Forum (EUROGRAPHICS 2009) 28 2 2009 203 211 [55] R. Kosara C. Healey V. Interrante D. Laidlaw C. Ware Visualization viewpoints—user studies: why, how, and when? IEEE Computer Graphics & Applications 23 4 2003 20 25 [56] Kindlmann G, Reinhard E, Creem S. Face-based luminance matching for perceptual colormap generation. In: Proceedings of the IEEE visualization 2002 (VIS 2002); 2002. [57] Biederman I, Kalocsai P. 1998. Neural and psychological analysis of object and face recognition. In: Face recognition: from theory to applications. New York: Springer-Verlag, 1998, pp. 3–25 [58] Healey C. Choosing effective colours for data visualization. In: Proceedings of the IEEE visualization ’96; 1996. p. 263–70. [59] Montag ED. The use of color in multidimensional graphical information display. In: Proceedings of the IS&T/SID 7th color imaging conference; 1999. p. 222–6. [60] P. Rheingans B. Tebbs A tool for dynamic explorations of color mappings ACM Computer Graphics 24 2 1990 145 146 [61] Wu Y, Takatsuka M. Three dimensional colour pickers. In: Proceedings of the 2005 Asia–Pacific symposium on information visualization, vol. 45; 2005. p. 107–14. [62] Gresh D. Self-corrected perceptual colormaps 〈 〉 , online March 2010. [63] Shulze-Wollgast P, Tominski C, Schumann H. Enhancing visual exploration by appropriate color coding. In: Proceedings of the international conference in central Europe on graphics, visualization and computer vision (WSCG’05); 2005. p. 203–10. [64] Dougherty B, Wade A. Vischeck 〈 〉 , online March 2010. [65] Dougherty B, Wade A. Daltonize 〈 〉 , online March 2010. [66] Hyun Y. Nonlinear color scales for interactive exploration 〈 〉 , online March 2010. [67] Ventura A, Schettini R. Computer-aided color coding for data display. In: Proceedings of the 11th IAPR international conference on pattern recognition; 1992. p. 29–32. "
    },
    {
        "doc_title": "Improving the management of chronic diseases using web-based technologies: An application in hemophilia care",
        "doc_scopus_id": "78650813584",
        "doc_doi": "10.1109/IEMBS.2010.5626026",
        "doc_eid": "2-s2.0-78650813584",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder",
            "Chronic disease",
            "Clinical practices",
            "Health care providers",
            "Information and communication",
            "Portugal",
            "Technological solution",
            "Web technologies",
            "Web-based technologies"
        ],
        "doc_abstract": "Modern methods of information and communication that use web technologies provide an opportunity to facilitate closer communication between patients and healthcare providers, allowing a joint management of chronic diseases. This paper describes a web-based technological solution to support the management of inherited bleeding disorders integrating, diffusing and archiving large sets of data relating to the clinical practice of hemophilia care, more specifically the clinical practice at the Hematology Service of Coimbra Hospital Center (a Hemophilia Treatment Center located in Portugal). © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "CardioAnalyser: A software tool for segmentation and analysis of the left ventricle from 4D MDCT images of the heart",
        "doc_scopus_id": "78449288239",
        "doc_doi": "10.1109/IV.2010.91",
        "doc_eid": "2-s2.0-78449288239",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "4D",
            "Left ventricles",
            "MDCT",
            "Segmentation",
            "Software tool"
        ],
        "doc_abstract": "Cardiac angiography using multiple detector row computerized tomography (MDCT) scanners provides 3D data concerning the heart and, in particular, the left ventricle (LV), for different cardiac phases along one cardiac cycle. Exploring this data for LV function analysis is not an easy task, given the amount of data and time involved in segmenting (or revising results provided by automatic segmentation methods) up to 12 cardiac phases. CardioAnalyser, a tool for 4D LV segmentation from MDCT data which provides a protocol to help perform LV segmentation of all cardiac phases available is presented. It uses an automatic segmentation algorithm along with a procedure which guides the user through the process. Its main goal is to reuse as much information as possible from one cardiac phase to the next in order to reduce segmentation time and the amount of user interaction. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring new ways of integration, visualization and interaction with geotechnical and geophysical data",
        "doc_scopus_id": "78449287527",
        "doc_doi": "10.1109/IV.2010.35",
        "doc_eid": "2-s2.0-78449287527",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Data type",
            "Geological structures",
            "Geophysical data",
            "Geotechnical",
            "Visualization technique",
            "VTK (Visualization Toolkit)"
        ],
        "doc_abstract": "The work presented in this paper aims at exploring new ways of integrating, visualizing and interacting with geotechnical and geophysical data that may be more rich and interactive than those offered by most current Geographic Information Systems (GIS). Some visualization techniques enabling simultaneous visualization of the several data types available in our case study are proposed. Moreover, methods were developed to guide experts while defining layers and other relevant geological structures. The work is still in an early stage and is main goal has been assessing the validity and adequacy of the proposed techniques to the specific geotechnical and geophysical data under consideration. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Wiimote as an input device in google earth visualization and navigation: A user study comparing two alternatives",
        "doc_scopus_id": "78449283947",
        "doc_doi": "10.1109/IV.2010.72",
        "doc_eid": "2-s2.0-78449283947",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Google earth",
            "Input devices",
            "Usability",
            "User study",
            "Wiimote"
        ],
        "doc_abstract": "This paper presents a user study performed to compare the usability of the Wiimote as an input device to visualize information and navigate in Google Earth using two different configurations. This study had the collaboration of 15 participants which performed a set of tasks using the Wiimote as an input device while the image was projected on a common projection screen, as well as a mouse on a desktop. Results show that most users clearly preferred one of the Wiimote configurations over the other, and over the mouse; moreover, they had better performances using the preferred configuration, and found it easier to use. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information visualization in facility location and vehicle routing decisions",
        "doc_scopus_id": "78449271134",
        "doc_doi": "10.1109/IV.2010.25",
        "doc_eid": "2-s2.0-78449271134",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Decision support tools",
            "Facility location",
            "Facility locations",
            "General publics",
            "Information visualization",
            "Location-routing",
            "Routing decisions",
            "Usability evaluation methods"
        ],
        "doc_abstract": "Facility location and vehicle routing are amongst the most important logistic decisions in today's organizations. These aspects are intertwined and, in some cases, should be addressed in an integrated way (giving rise to the location-routing approach). A decision support tool that can make easier the visualization (and editing) of information regarding these problems is becoming increasingly important as: it enables to further understand the problem at hand; and, at the same time, it fosters better communication of the decisions in a way easier to understand by the general public. This paper presents some concepts for information visualization on the problems arisen by the aforementioned decisions, which have been incorporated in a decision support tool and tested using usability evaluation methods. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A 3D tool for left ventricle segmentation editing",
        "doc_scopus_id": "77955377969",
        "doc_doi": "10.1007/978-3-642-13775-4_9",
        "doc_eid": "2-s2.0-77955377969",
        "doc_date": "2010-08-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3-D image",
            "3D segmentation",
            "Anatomical structures",
            "Application area",
            "Automatic method",
            "CT scanners",
            "Editing tools",
            "Left ventricles",
            "Quantitative measurement",
            "Robust segmentation",
            "Time instances"
        ],
        "doc_abstract": "Image segmentation has a very important role in many application areas, such as medical imaging. Even robust segmentation methods cannot deal with the wide range of variation observed, for example, in shape and orientation of an anatomical structure. Given the need to accomplish accurate segmentations in order to perform quantitative measurements or compare structures in different time instances, it is important to have tools which allow easy segmentation editing/correction by experts. In 3D images (e.g., obtained using CT scanners) performing segmentation editing of regions which span several slices might be a tiresome task if it has to be done slice-by-slice with a 2D tool. This article presents a 3D segmentation editing tool, to be applied to left ventricle segmentations, which enables radiographers to correct segmentations provided by an automatic method. © 2010 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Processing, visualization and analysis of medical images of the heart: An example of fast prototyping using MeVisLab",
        "doc_scopus_id": "70350558556",
        "doc_doi": "10.1109/VIZ.2009.40",
        "doc_eid": "2-s2.0-70350558556",
        "doc_date": "2009-11-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Fast prototyping",
            "Left ventricles",
            "MDCT images",
            "Medical images",
            "MeVisLab",
            "Prototyping platform",
            "Real problems",
            "Semi-automatic segmentation",
            "Visualization and analysis"
        ],
        "doc_abstract": "Developing and testing new algorithms for medical imaging processing can be a tiresome task as it often requires an additional set of tools to visualize the data and results and perform validation steps along the development. The integration among all these tools is also very important as it speeds all the process allowing the developer to concentrate on the real problems. This article briefly presents how MeVisLab is being used as a prototyping platform to develop a semi-automatic segmentation algorithm to extract the left ventricle from 4D MDCT images of the heart and then build a simple framework to perform a preliminary evaluation of the obtained results. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A perceptual data repository for polygonal meshes",
        "doc_scopus_id": "70350543996",
        "doc_doi": "10.1109/VIZ.2009.41",
        "doc_eid": "2-s2.0-70350543996",
        "doc_date": "2009-11-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Data repositories",
            "Experimental methodology",
            "Observer studies",
            "Perceived quality",
            "Polygonal meshes",
            "Preliminary assessment"
        ],
        "doc_abstract": "A repository containing perceived quality data for polygonal meshes, obtained through observer studies, is presented. It includes information regarding the experimental methodology, protocol and models used, with the purpose of allowing researchers to use it, e.g., for a faster preliminary assessment of their perceived quality metrics without the overhead of designing and performing an observer study. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A preliminary usability evaluation of Hemo@Care: A web-based application for managing clinical information in hemophilia care",
        "doc_scopus_id": "70350238791",
        "doc_doi": "10.1007/978-3-642-02806-9_91",
        "doc_eid": "2-s2.0-70350238791",
        "doc_date": "2009-10-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Health information system",
            "Hemo@care",
            "Nursing staff",
            "Usability evaluation",
            "WEB application",
            "Web-based applications"
        ],
        "doc_abstract": "In this work, an overall description of the methods used and the results obtained in the on-going evaluation of hemo@care is presented. To help understanding the methods and results, we first give an overview of the main functionalities of hemo@care, which is a web application to manage the clinical information in hemophilia care, developed to be used by hematologists, nursing staff and patients suffering from hemophilia. Following we described the methods used in this particular evaluation, and finally we present the main results and general conclusions of these preliminary usability evaluation. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The user's role in the development process of a clinical information system: An example in hemophilia care",
        "doc_scopus_id": "70350214967",
        "doc_doi": "10.1007/978-3-642-02806-9_106",
        "doc_eid": "2-s2.0-70350214967",
        "doc_date": "2009-10-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Clinical information system",
            "Critical phasis",
            "Development process",
            "Health information system",
            "Human-centered designs",
            "User-centered design",
            "WEB application",
            "Web based information systems"
        ],
        "doc_abstract": "This work describes the development process of a Web-based Information System for managing clinical information in hemophilia care, emphasizing the role of the users around a human-centered development. To help understanding all this process, we first present the relevant concepts concerning human-centered design; next we describe the web application for managing the clinical information in hemophilia care, as well as, the development process followed in its development; and finally we illustrate the importance of the user's involvement in critical phases through the demonstration of some results. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Left ventricle segmentation from heart MDCT",
        "doc_scopus_id": "68749088804",
        "doc_doi": "10.1007/978-3-642-02172-5_40",
        "doc_eid": "2-s2.0-68749088804",
        "doc_date": "2009-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Left ventricles",
            "Qualitative evaluations",
            "Semiautomatic methods"
        ],
        "doc_abstract": "A semi-automatic method for left ventricle segmentation from MDCT exams is presented. It was developed using ITK and custom modules integrated in the MeVisLab platform. A preliminary qualitative evaluation shows that the provided segmentation, without any tweaking or manual edition, is reasonably close to the ideal segmentation as judged by experienced radiology technicians. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Erratum to \"PolyMeCo-An integrated environment for polygonal mesh analysis and comparison\" [Comput. Graphics 33 (2009) 181-191] (DOI:10.1016/j.cag.2008.09.014)",
        "doc_scopus_id": "71849083306",
        "doc_doi": "10.1016/j.cag.2009.10.003",
        "doc_eid": "2-s2.0-71849083306",
        "doc_date": "2009-01-01",
        "doc_type": "Erratum",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2009-11-04 2009-11-04 2010-04-18T22:56:32 S0097-8493(09)00117-4 S0097849309001174 10.1016/j.cag.2009.10.003 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20091001 20091031 2009 2009-11-04T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype volfirst volissue body affil articletitle auth authfirstini authfull authlast pubtype alllist content subj ssids 0097-8493 00978493 33 33 5 5 Volume 33, Issue 5 7 624 624 200910 October 2009 2009-10-01 2009-10-31 2009 Technical Section simple-article err Copyright © 2009 Elsevier Ltd. All rights reserved. ERRATUMPOLYMECOANINTEGRATEDENVIRONMENTFORPOLYGONALMESHANALYSISCOMPARISONCOMPUTGRAPHICS332009181191 SILVA S 10.1016/j.cag.2008.09.014 S0097849308001271 SILVAX2009X624 SILVAX2009X624XS item S0097-8493(09)00117-4 S0097849309001174 10.1016/j.cag.2009.10.003 271576 2010-09-22T11:52:53.696665-04:00 2009-10-01 2009-10-31 true 97165 MAIN 1 20910 849 656 IMAGE-WEB-PDF 1 CAG 1965 S0097-8493(09)00117-4 10.1016/j.cag.2009.10.003 S0097-8493(08)00127-1 10.1016/j.cag.2008.09.014 Elsevier Ltd Erratum Erratum to “PolyMeCo—An integrated environment for polygonal mesh analysis and comparison” [Comput. Graphics 33 (2009) 181–191] Samuel Silva a b ⁎ Joaquim Madeira a b Beatriz Sousa Santos a b a Institute of Electronics and Telematics Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author at: Institute of Electronics and Telematics Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal. Tel.: +351234370500; fax: +351234370545. The publisher regrets that the above paper was incorrectly published under the Education Section. The correct section for this paper is the “Technical Section”. "
    },
    {
        "doc_title": "PolyMeCo-An integrated environment for polygonal mesh analysis and comparison",
        "doc_scopus_id": "63749095751",
        "doc_doi": "10.1016/j.cag.2008.09.014",
        "doc_eid": "2-s2.0-63749095751",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Analysis and explorations",
            "Application area",
            "Comparison result",
            "Integrated environment",
            "Mesh analysis",
            "Mesh comparison",
            "Mesh simplifications",
            "Processing method"
        ],
        "doc_abstract": "Polygonal meshes are used in several application areas to model different objects and structures. Depending on the application, mesh models sometimes have to be processed to, for instance, reduce their complexity (mesh simplification). Such operations introduce differences regarding the original mesh, whose evaluation is of paramount importance when choosing the processing methods to be applied for a particular purpose. Although some mesh analysis and comparison tools are described in the literature, little attention has been given to the way mesh features and mesh comparison results can be visualized. Moreover, particular functionalities have to be made available to enable systematic use and proper data analysis and exploration. PolyMeCo-a tool for polygonal mesh analysis and comparison-was designed and developed taking the above objectives into account. It enhances the way users analyze features and compare meshes by providing an integrated environment where various mesh quality measures and several visualization options are available and can be used in a coordinated way, thus leading to greater insight into the visualized data. © 2008 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2008-10-17 2008-10-17 2010-04-18T22:56:32 S0097-8493(08)00127-1 S0097849308001271 10.1016/j.cag.2008.09.014 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20090401 20090430 2009 2008-10-17T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst pubtype ref alllist content subj ssids 0097-8493 00978493 33 33 2 2 Volume 33, Issue 2 10 181 191 181 191 200904 April 2009 2009-04-01 2009-04-30 2009 Education article fla Copyright © 2008 Elsevier Ltd. All rights reserved. POLYMECOANINTEGRATEDENVIRONMENTFORPOLYGONALMESHANALYSISCOMPARISON SILVA S 1 Introduction 2 Mesh comparison tools 3 PolyMeCo 3.1 Computational measures 3.2 Representations 3.2.1 Colored mesh 3.2.2 Superimposed meshes 3.2.3 Statistical representations 3.3 Visualization modes 3.3.1 Original vs. processed vs. colored mesh 3.3.2 Extended results 3.3.3 Simultaneous viewing of different measures 3.3.4 Features comparison 3.4 Additional features 3.4.1 Location probe 3.4.2 Light source customization 3.4.3 Workspace saving 3.4.4 Data export 4 User evaluation 4.1 Formal evaluation 4.2 Informal evaluation 5 Application example 6 Availability and community contributions 7 Conclusions and future work Acknowledgments References CIGNONI 2005 125 133 P CIGNONI 1998 167 174 P ROY 2004 127 140 M SILVA 2005 842 847 S PROCEEDINGS9THINTERNATIONALCONFERENCEINFORMATIONVISUALIZATIONIV05LONDONUK POLYMECOAPOLYGONALMESHCOMPARISONTOOL SHEWCHUK 2002 115 126 J PROCEEDINGS11THINTERNATIONALMESHINGROUNDTABLE AGOODLINEARELEMENTINTERPOLATIONCONDITIONINGQUALITYMEASURES BHATIA 1990 309 319 R GRAF 2006 178 183 H PROCEEDINGS2006IEEEINTERNATIONALCONFERENCEVIRTUALENVIRONMENTSHUMANCOMPUTERINTERFACESMEASUREMENTSYSTEMS ADAPTIVEQUALITYMESHINGFORONTHEFLYVOLUMETRICMESHMANIPULATIONSWITHINVIRTUALENVIRONMENTS THEISEL 2004 288 297 H PROCEEDINGS12THPACIFICCONFERENCECOMPUTERGRAPHICSAPPLICATIONS NORMALBASEDESTIMATIONCURVATURETENSORFORTRIANGULARMESHES LEE 2005 659 666 C PROCEEDINGSSIGGRAPH MESHSALIENCY KARNI 2000 279 286 Z PROCEEDINGSSIGGRAPH SPECTRALCOMPRESSIONMESHGEOMETRY SORKINE 2003 42 51 O PROCEEDINGSEUROGRAPHICSSYMPOSIUMGEOMETRYPROCESSING HIGHPASSQUANTIZATIONFORMESHENCODING SOUSASANTOS 2005 15 21 B PROCEEDINGSTHIRDINTERNATIONALCONFERENCEMEDICALINFORMATIONVISUALIZATIONBIOMEDICALVISUALIZATIONMEDIVIS05 COMPARISONMETHODSFORSIMPLIFICATIONMESHMODELSLUNGSUSINGQUALITYINDICESOBSERVERSTUDY SILVA 2007 943 950 S PROCEEDINGS11THINTERNATIONALCONFERENCEINFORMATIONVISUALIZATIONIV07ZURICHSWITZERLAND MORECOLORSCALESMEETSEYEAREVIEWUSECOLORINVISUALIZATION ROGOWITZ 1998 52 59 B BORLAND 2007 14 17 D LEVKOWITZ 1992 72 80 H BOTSCH 2006 M EUROGRAPHICS2006TUTORIALSVIENNAAUSTRIA GEOMETRICMODELINGBASEDTRIANGLEMESHES SOUSASANTOS 2007 133 139 B PROCEEDINGSGEOMETRICMODELINGIMAGING2007NEWADVANCESZURICHSWITZERLAND PRELIMINARYEVALUATIONPOLYMECOAVISUALIZATIONBASEDTOOLFORMESHANALYSISCOMPARISON JACKA 2007 177 186 D PROCEEDINGSAFRIGRAPH ACOMPARISONLINEARSKINNINGTECHNIQUESFORCHARACTERANIMATION SILVA 2007 515 523 F INTERNATIONALCONFERENCECOMPUTATIONALSCIENCEAPPLICATIONSICCSA2007 NSAALGORITHMGEOMETRICALVSVISUALQUALITY PETROU 1999 M IMAGEPROCESSINGFUNDAMENTALS HORN 1984 1671 1686 B SUN 1997 164 168 C BUSTOS 2006 39 54 B REUTER 2006 342 366 M PAULY 2001 379 386 M PROCEEDINGSSIGGRAPH SPECTRALPROCESSINGPOINTSAMPLEDGEOMETRY BERGMAN 1995 118 125 L PROCEEDINGSIEEEVISUALIZATION ARULEBASEDTOOLFORASSISTINGCOLORMAPSELECTION SILVAX2009X181 SILVAX2009X181X191 SILVAX2009X181XS SILVAX2009X181X191XS item S0097-8493(08)00127-1 S0097849308001271 10.1016/j.cag.2008.09.014 271576 2010-09-22T11:48:58.726395-04:00 2009-04-01 2009-04-30 true 2994061 MAIN 11 54303 849 656 IMAGE-WEB-PDF 1 si9 106 6 8 si8 106 6 8 si7 106 6 8 si6 106 6 8 si5 106 6 8 si4 106 6 8 si3 106 6 8 si20 110 12 6 si2 173 11 35 si19 108 12 5 si18 106 6 8 si17 106 6 8 si16 106 6 8 si15 106 6 8 si14 106 6 8 si13 106 6 8 si12 106 6 8 si11 106 6 8 si10 106 6 8 si1 136 8 14 gr1 167537 628 809 gr1 6272 93 120 gr1 1328213 2782 3583 gr14 97839 456 621 gr14 5699 92 125 gr14 661505 2020 2750 gr15 122614 450 621 gr15 7650 91 125 gr15 886076 1992 2750 gr2 19377 101 809 gr2 1302 16 125 gr2 121891 447 3583 gr4 46079 251 226 gr4 5319 93 84 gr4 506954 1111 1000 gr8 47836 196 621 gr8 3167 39 125 gr8 312954 867 2750 gr10 41641 291 391 gr10 14096 163 219 gr10 294577 1288 1733 gr11 18219 134 391 gr11 5569 75 219 gr11 111085 594 1733 gr12 43854 200 809 gr12 5902 54 219 gr12 319445 886 3583 gr13 24863 368 188 gr13 6182 162 83 gr13 148470 1630 833 gr3 89607 258 809 gr3 8952 70 219 gr3 750558 1141 3583 gr5 50097 471 391 gr5 8192 164 136 gr5 371957 2086 1733 gr6 41070 349 391 gr6 11438 163 183 gr6 290957 1547 1733 gr7 64939 406 295 gr7 10330 164 119 gr7 509248 1800 1308 gr9 137937 640 809 gr9 11807 164 207 gr9 1267207 2835 3583 CAG 1889 S0097-8493(08)00127-1 10.1016/j.cag.2008.09.014 Elsevier Ltd Fig. 1 User interface of PolyMeCo: the distribution of a computed measure (mean curvature) is represented using a colored mesh. Fig. 2 PolyMeCo's pipeline. Fig. 3 Computed results are presented using a colored mesh: (a) normal deviation using a rainbow color scale and (b) a blue to cyan color scale; (c) triangle quality using a rainbow color scale and flat shading to color the faces according to the minimum angle. Fig. 4 Principal curvature direction ( κ 2 ) computed at each vertex. Fig. 5 Mesh superposition. Top: original and processed meshes; bottom: mesh superposition using solid rendering depicting a region where the original mesh overlaps the processed one, and mesh superposition with transparency applied to the original mesh, allowing volume comparison. Fig. 6 High saturation value defined to better visualize the representation of curvature over the mesh. Top, original colored mesh and colored mesh with high saturation value definition; bottom, the resulting histogram. Fig. 7 Top, the same mesh colored using three color scales depicting the same results obtained for the geometric distance. Bottom, same mesh but now with the color mapping modified in order to highlight the surface regions with larger distance to the reference mesh. Fig. 8 Comparison using boxplots, as well as numerical values, of the data obtained by computing the same measure for different meshes. Fig. 9 Some visualization modes available in PolyMeCo: (a) original vs. processed vs. colored mesh; (b) extended results; (c) feature comparison. Fig. 10 Comparison using colored meshes of the results obtained by computing the same measure for different meshes: (a) each mesh colored using an individual color map; (b) all meshes colored using a common color map allowing direct comparison among them. Fig. 11 Comparison using histograms of the data obtained with the same measure for different meshes: (a) the histograms are drawn using individual ranges; (b) all histograms are drawn using a common value range allowing direct comparison among them. Fig. 12 A vertex was selected with the Location Probe. Mesh detail showing that vertex along with its direct-neighborhood (a), and the three tabs on the Location Probe showing information about geometry (b), topology (c) and computed measure associated with it (d). Fig. 13 Window where users can customize light source properties. Fig. 14 Geometric distance mapped on the original model for six simplified versions of a model, obtained with NSA and QSlim, depicted using a common color map. Fig. 15 Triangle quality for the same six models presented in Fig. 14, mapped on the simplified models and depicted using colored models and histograms. Equilateral triangles are presented in blue and needle/flat triangles in red. Table 1 Users opinion on general and specific aspects of PolyMeCo (in a scale of 1-complete disagreement to 4-complete agreement) Features Median Is easy to learn 4 Organization is understandable 3 Response time is reasonable 4 Information layout is adequate 3 Terminology is consistent 3 Text is easy to read 4 Messages are clear 3 Feedback is adequate 3 Education PolyMeCo—An integrated environment for polygonal mesh analysis and comparison Samuel Silva a b ⁎ Joaquim Madeira a b Beatriz Sousa Santos a b a Institute of Electronics and Telematics Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351234370500; fax: +351234370545. Polygonal meshes are used in several application areas to model different objects and structures. Depending on the application, mesh models sometimes have to be processed to, for instance, reduce their complexity (mesh simplification). Such operations introduce differences regarding the original mesh, whose evaluation is of paramount importance when choosing the processing methods to be applied for a particular purpose. Although some mesh analysis and comparison tools are described in the literature, little attention has been given to the way mesh features and mesh comparison results can be visualized. Moreover, particular functionalities have to be made available to enable systematic use and proper data analysis and exploration. PolyMeCo—a tool for polygonal mesh analysis and comparison—was designed and developed taking the above objectives into account. It enhances the way users analyze features and compare meshes by providing an integrated environment where various mesh quality measures and several visualization options are available and can be used in a coordinated way, thus leading to greater insight into the visualized data. Keywords Mesh analysis Mesh comparison Data visualization 1 Introduction Models defined using polygonal meshes are commonly used in many application scenarios ranging from entertainment to medical imaging, as well as in numerous scientific visualization and engineering applications. With data complexity growing at a fast pace, it is often necessary to process an original mesh model (e.g., created from data obtained with a range scanner) and reduce the number of its faces, thus requiring less storage space and allowing easier manipulation and faster rendering. Other processing operations are usually carried out, for example, to smooth a mesh surface [1] or enhance particular mesh features [2]. Applying such operations results in meshes that are different from their originals: depending on the application it might be important to assess those differences and verify if they are acceptable. Another important issue is the analysis of particular mesh properties, as surface curvature or the quality of the elements (faces) that compose a mesh. For the purpose of mesh analysis and comparison, several measures have been proposed and there are a few tools presented in the literature [3–6] which allow comparing polygonal meshes; they will be briefly described in the following section. After a careful review of those mesh comparison tools and their usual applications, it was clear that they lacked some features deemed important. They provide numerical descriptors, along with colored meshes which depict the data obtained using some computational measures. Nevertheless, it becomes necessary to have a tool which provides an enhanced environment for systematic mesh analysis and comparison, where several meshes can be loaded during the same work session and data compared using a wide range of measures and proper visualization methods. Developers and general users can then easily analyze the results. We describe a polygonal mesh analysis and comparison tool called PolyMeCo 1 1 A version for test purposes is available at (Fig. 1 ) which offers the above mentioned features; a first prototype was presented in Ref. [7]. The main goal of the work carried out is to integrate enhanced functionalities for interactive visualization and analysis of mesh features and comparison data, in the scope of systematic mesh analysis and comparison. In the following section a short overview of mesh comparison tools that have been proposed in the literature is done. Afterwards, the architecture of PolyMeCo and its main functionalities are presented. Then, the results of some preliminary usability evaluation tests are presented and some research work in which PolyMeCo was used is briefly described (as an application example). Finally, some conclusions and ideas for future work are mentioned. 2 Mesh comparison tools Mesh comparison is usually carried out with the help of dedicated software tools providing the user with numerical data (e.g., minimum, mean and maximum difference values) and visual information (e.g., coloring a mesh according to the difference values measured at each vertex), and allowing the user to choose among several computational measures. A few such tools, which might also allow mesh feature evaluation (e.g., surface curvature), are described in the literature. Metro, developed by Cignoni et al. [3], allows both numerical and visual mesh comparisons. Among the numerical values furnished is data concerning the input mesh characteristics (number of vertices and faces, surface area, mesh volume, etc.), the minimum and maximum geometric distances between two given meshes and their difference in volume. It is also possible to view a mesh colored according to the results obtained. Also provided is the computation of the Hausdorff distance. The tool described by Zhou et al. [4] introduces some additional measures (namely, to compute surface curvature) and care was taken to offer several visualization techniques, including side-by-side viewing of the compared meshes, box-glyphs and animations. MESH, developed by Aspert et al. [5], uses the Hausdorff distance to measure the difference between two mesh models. It provides several numerical values, namely the main characteristics of the input meshes and the minimum, mean, maximum and RMS values of the computed differences. It is also possible to view a mesh colored according to such differences. Finally, Roy et al. [6] described a tool called MeshDev which allows the computation of geometric, normal and other attribute differences such as color or texture. Similarly to the other tools, it provides several numerical values characterizing the input meshes and the obtained results. It is also possible to view meshes colored according to such results. 3 PolyMeCo PolyMeCo is a tool developed using C ++ , OpenGL, OpenMesh [8] and the Fox Toolkit [9], which provides an integrated environment where it is possible to analyze and compare several polygonal meshes at once, and view the data obtained for different computational measures using various representation and visualization options. The process of analyzing and comparing meshes in PolyMeCo can be described by the pipeline depicted in Fig. 2 : 1. Meshes are loaded. 2. For some of those meshes, selected properties or difference measures are computed. 3. The obtained data are mapped to suitable representations. 4. The chosen representations are rendered. The user may change parameters along the pipeline in order to, for example, choose another mesh property or difference measure, or change the representation. 3.1 Computational measures PolyMeCo provides several computational measures which support mesh analysis and comparison. Two types of measures are provided: intrinsic properties and difference measures. Intrinsic 2 2 Here used in the sense of a property which is directly computed from mesh data, irrespective of any other reference mesh; not in the differential geometry sense as in “intrinsic curvature”. properties allow measuring a particular property of a mesh. PolyMeCo provides the following: • Smoothness: for each vertex, the distance to the centroid of its direct neighbors (one-ring vertices). • Triangle quality: the minimum angle for each mesh face, which gives an idea of how close it is to the equilateral; as well as the measures proposed by Shewchuk [10], Bhatia [11] and Graf et al. [12]. • Mean, Gaussian and principal curvatures: using the methods proposed by Theisel et al. [13]. • Saliency: using the method proposed by Lee et al. [14] which identifies mesh areas judged to be of great interest, i.e., presenting a larger number of features. Difference measures allow the comparison of properties between meshes. PolyMeCo provides the following: • Geometric distance and normal deviation: using the methods proposed by Roy et al. [6]. • Mean and Gaussian curvature deviations: using the methods proposed by Theisel et al. [13] for curvature computation and the method proposed by Roy et al. [6] for surface sampling. • Visual differences: using the method proposed by Karni et al. [15] and its enhancements proposed by Sorkine et al. [16]. • Mixed measure: based on the results presented in Sousa Santos et al. [17], mixing geometric distance and normal deviation using a blending factor which depends on the simplification ratio. Notice that, although this might be unusual, PolyMeCo does not impose any constraints on the comparison between meshes representing distinct objects. 3.2 Representations The easiest way for providing the user with information about the computed measures is by presenting some numerical values that characterize them (e.g., minimum, mean and maximum values) and/or the meshes (e.g., number of vertices and faces, bounding box diagonal and surface area) for which they were computed. Another simple approach is that of depicting a mesh and allowing the user to visually inspect it using different projections, views or rendering options (e.g., flat shading, wireframe, etc.). Apart from these, PolyMeCo provides the set of representations described next. 3.2.1 Colored mesh Through a colored mesh it is possible to show the user the distribution of the data obtained with a particular measure: each vertex/face is colored according to the corresponding value. This coloring can be done by mapping the values range to a particular color scale. A usable color scale must respect a set of principles and be properly chosen in order to enhance the insight it provides on the data it refers to [18]. It is also important to understand its disadvantages. For instance, the commonly used rainbow color scale is not the proper choice in many application scenarios according to Rogowitz et al. [19] and, more recently, Borland et al. [20]. As a first approach towards a proper use of color in PolyMeCo, several color scales described in the literature are provided, namely the rainbow, greyscale, linearized greyscale, blue-to-cyan, blue-to-yellow, linearized optimal [21] and heated-object color scales. Currently, the user is allowed to choose one color scale from this set: this is still a naive approach to a correct use of color as it fully depends on user judgment, nevertheless it provides some alternatives which can be explored. Fig. 3 (a) and (b) present the normal deviation values distribution using rainbow and blue to cyan color scales. Fig. 3(c) shows part of a mesh colored according to the computed triangle quality. For some computational measures (such as surface curvature) it may be important to display not only absolute values but also directions. This is performed by drawing a small line segment centered on the corresponding vertex and having the appropriate direction, which can be enabled or disabled at any time. Fig. 4 shows the directions of the second principal curvature superimposed on a colored mesh. 3.2.2 Superimposed meshes It is possible to visualize an original reference mesh superimposed on one of its processed versions. A first choice is to render both in a “solid” way, using different colors. This allows identifying areas where, for example, the processed mesh overlaps the original. An alternative is to render the original mesh with some degree of transparency, in order to let the user perceive any differences in volume between the two. Fig. 5 shows these two alternatives. In addition, it is also possible to visualize both meshes rendered in wireframe. 3.2.3 Statistical representations Although mesh coloring gives a good idea of the distribution of the values obtained using a computational measure, showing where they occur, a global idea of this distribution can be difficult to obtain, due to the impossibility of viewing the whole mesh simultaneously. The provided statistical representations may help to better understand and compare distributions of computed measures: • Histograms—Histograms provide complementary information: the user can, at a glance, get an idea of the global characteristics of the computed data distribution. A problem that may occur when assigning a range of values to a color scale is that of range stretching: if outliers are present in the data, they may stretch the range too much and not allow perceptible differences between close values. In order to tackle this problem, the histogram widget provides a way of choosing the range of values that will be represented by the chosen color scale. By using a slider, the user can choose a high saturation value, i.e., all values above it will be represented using the color to which the maximum value is mapped. A similar slider is available for defining a low saturation value, all values below it being represented by the color to which the minimum is mapped. Thus, the first class of the histogram will correspond to the low saturated values and the last class to the high saturated values. Fig. 6 illustrates this procedure showing a mesh colored according to its mean curvature. Due to the presence of outliers in the data the curvature representation is not perceptible. By defining a high saturation value using the slider provided on the histogram widget, it is possible to better perceive the curvature distribution. Note that the histogram bars are also colored with the chosen color mapping. This feature can also be used to highlight values in a colored mesh. Fig. 7 (top) shows the same mesh colored using different color scales, depicting the geometric distance to a reference mesh. Bottom, the same results are presented but the color mapping has been modified by adjusting the high saturation value. Notice how surface regions with larger associated geometric distance are easily identified. • Boxplots—Boxplots are very useful to compare data sets, since they allow comparing and analyzing the symmetries and ranges of the data, and detect the presence of outliers (Fig. 8 ); thus, they can help set the saturation values on the histogram widget. 3.3 Visualization modes Visualization modes use the available representations in order to provide ways of better analyzing and comparing mesh properties. The main visualization modes provided by PolyMeCo are presented next. 3.3.1 Original vs. processed vs. colored mesh In this visualization mode an original mesh, a processed mesh and a mesh colored according to the selected measure are presented. This allows the visual comparison of the two meshes, for example, to quickly identify areas where computed values are higher/lower, and then perform a more detailed analysis. It is possible to manipulate any of the meshes and change its position, orientation and scale factor. This manipulation is applied to all displayed meshes. Fig. 9 (a) shows this visualization mode. 3.3.2 Extended results This visualization mode allows the simultaneous presentation of a colored mesh, a histogram and a boxplot depicting the data obtained using the selected measure. It is possible to define high and low saturation values, using the sliders below the histogram, as previously described, in order to enhance the way results are presented using the colored mesh. Fig. 9(b) shows this visualization mode being used to analyze the normal deviation between two meshes. 3.3.3 Simultaneous viewing of different measures This mode allows the user to visualize, for one mesh, the data distributions obtained with different measures. This mode can help the user to have a clearer idea of the obtained data, and provides the possibility of comparing between them and finding, for example, similarities in their behavior. The presented meshes remain synchronized, i.e., with similar position, orientation and scale factor. 3.3.4 Features comparison This mode (Fig. 9(c)) allows the visualization of data distributions obtained with the same computational measure for several meshes. This can be useful to study different processing algorithms and compare the obtained results. Once again, the presented meshes remain synchronized. When using this visualization mode one must not forget important details. Each colored mesh uses the full range of the color scale to represent the data, i.e., for each colored mesh the “last” color of the color scale is used to represent the maximum value obtained for that particular mesh. This kind of coloring can still be useful to compare value incidence in each mesh, but it is not possible to directly compare distributions among all colored meshes. In order to allow this kind of comparison (not possible with any of the tools described earlier), PolyMeCo provides the option of using a common color mapping for all meshes. The last color of the color scale is now used to represent the maximum obtained value among all the compared meshes. The same happens with the histograms. The minimum is always considered to be zero as all representations are unsigned. Fig. 10 (a) allows comparing results using an individual color map for each mesh. Notice that it is not clear which is the one that has higher associated values. In Fig. 10(b) a common color map is used to color all the meshes, thus allowing for an easier and more accurate comparison. Side-by-side comparison can also be done using histograms: again, direct comparison among them can only be correctly performed when using a common value range. Fig. 11 shows the histograms drawn using their individual ranges, and then using a common value range. Comparison can also be done by using boxplots as shown in Fig. 8. With boxplots it is easy to compare the ranges and symmetry among all the results. 3.4 Additional features Besides different representations and visualization modes PolyMeCo provides a set of additional features in order to help inspecting meshes and results more efficiently, and allow gathering the resulting data for further analysis and presentation. These features are presented next. 3.4.1 Location probe A probe tool allows the user to obtain information regarding a particular mesh vertex. The provided information concerns geometry (vertex coordinates), topology (vertex neighborhoods and their properties) and associated data values, such as those obtained using a particular measure. Fig. 12 shows some details of the Location Probe window and its usage. When a mesh vertex is selected, a green sphere appears at its location along with a highlight of its neighbors. The user can define the number of visible neighborhood rings on the Topology tab and read information about each neighborhood (one-ring, two-ring, etc.), such as the number of vertices it includes. The information concerning vertex coordinates can be found in the Geometry tab and the value computed for it, using one of the measures, is presented in the Results tab. 3.4.2 Light source customization Scene illumination can play an important role in the analysis of a mesh surface and has been used in applications, such as the automotive industry, to inspect the quality of polygonal meshes [22]. As a first step, PolyMeCo provides the user with several functionalities to view and change the light source properties of the scene. It is possible to increase the number of light sources (up to three) and control their position as well as the ambient, diffuse and specular components. Light source orientation is always towards the origin, as can be seen in the representation appearing on the top of the window presented in Fig. 13 where cones represent the light sources and the sphere represents the illuminated object. The chosen light source properties are applied to the sphere thus allowing the user to have a better perception of the illumination results. Due to performance issues, specially when working with complex meshes, light source changes are only applied to the visualized mesh when the user presses the Apply Settings button. The light source properties set by the user are only applied when viewing original or processed meshes. In order not to interfere with the representations which use color (light source color could change perceived mesh surface colors), the light source properties for those scenes are pre-defined and cannot be changed. A desirable improvement would be to enhance this feature in order to allow surface examination using isophotes [22], in which case, the light sources ought to be modeled differently to obtain the desired effects on the surface. 3.4.3 Workspace saving A work session in PolyMeCo can involve a considerable number of meshes and the computation of many measures which, for complex meshes, can take a long time. Thus, it is important to store the contents of the workspace for future analysis or to be able to resume the work at a later time: PolyMeCo allows complete workspace loading and saving. A summary of the data contained in the workspace is present in the main workspace file which is written in XML, and it is possible to use XSLT style sheets to view it in, for example, a web browser. This allows viewing the main results on any computer, even without PolyMeCo installed. 3.4.4 Data export It may be necessary to compare the values obtained for each vertex/face using a particular measure with theoretical values (e.g., curvature) or to entail a more elaborate statistical analysis. While performing systematic analysis and comparison of mesh processing algorithms a common task is that of building tables with the data obtained with the computed measures. Yet, the usage of a large number of meshes and measures turns this into a tiresome task. To cope with this PolyMeCo provides data export. When exporting data from a particular measure the values computed for each vertex/face are written to a file; when exporting the data obtained by computing several measures for a single mesh a table is stored containing the minimum, mean, maximum and variance values for each measure; finally, when exporting the data concerning several measures computed for several meshes, a table is stored containing the mean values for each measure and for each mesh. All these files can then be opened, for example, in a spreadsheet tool. 4 User evaluation User evaluation is of paramount importance when developing an interactive tool. A formative evaluation intended to inform the development of a new version of PolyMeCo has been carried out and allowed the detection of some problems, providing clues to improve some features and gathering new ideas for future versions. Until now two types of evaluation have been performed: formal and informal evaluation. A brief description of the used methods and results is presented next. For a more detailed explanation see Sousa Santos et al. [23]. 4.1 Formal evaluation Heuristic evaluation was performed providing a list of usability problems, severity ratings and clues to their solution. Observation sessions were performed with the participation of 35 users who had to complete a list of tasks including, for example: • Load a given mesh; • Set the scene illumination parameters; • Inspect the mesh and locate a particular detail; • Obtain a colored mesh using a given measure; • Select a given color scale; • Find the value of the measure at a given vertex; • Obtain a new color mapping for the same color scale; • Compare results for meshes obtained with different processing methods using a given measure. In general all users managed to finish the tasks in a very short time (a few seconds). Only two of the tasks took longer or were not completed. They were related to the Location Probe: when trying to obtain information regarding a particular mesh vertex, the users had some difficulty in selecting it. This result clearly reveals a problem which needs further investigation. A possible solution can be that of activating the wireframe view. After completing the tasks, users gave their opinion concerning some tool features through a questionnaire. Table 1 shows the median values for each item expressed in a scale from 1 to 4. These values convey a positive opinion of PolyMeCo, even though it was the first time all users worked with such a tool. 4.2 Informal evaluation A less structured approach was used for informal evaluation performed with 20 users. In an initial 2h session the goals and main features of PolyMeCo were presented. Then, a period of questions and answers was allowed so that participants could better understand the overall purpose of the tool and the intended evaluation. After this, they installed the prototype on their computers and were given some time to use it on their own. Users were also challenged to think how they would use the tool if they had to conduct research concerning mesh processing methods. Then they were asked for suggestions on how to improve the usability of the tool and on which new useful functionalities should be included. A week later, in another session, the users were asked to further comment on the tool and to answer the same questionnaire about PolyMeCo and its features. During the evaluation sessions PolyMeCo was used in a more committed manner than we had previously anticipated, and users gave plenty of suggestions concerning interesting new functionality to include or ways to improve the usability of the tool. The results concerning their reaction and opinion about PolyMeCo, collected through the questionnaire, were very similar to the results obtained from the formal evaluation participants, conveying a positive opinion, as well. 5 Application example A task which can be performed using PolyMeCo is the comparison among different mesh simplification methods in order to identify the one which provides meshes of greater quality. As described by Silva et al. [24], PolyMeCo was used to compare between two simplification methods: QSlim and NSA. The analysis started with a visual inspection of the meshes using different light source configurations which allowed the detection of some surface artifacts. Then, several measures were computed and the results analyzed using different visualization modes, in particular the Feature Comparison visualization mode which allowed an easy comparison among results using colored meshes. For the mesh models used, QSlim obtained the smallest geometric distance for all simplified meshes but mesh quality, as expressed by the triangle quality measures (minimum angle and those proposed by Shewchuk [10], Bhatia [11] and Graf et al.[12]), was, in general, favorable to the meshes obtained using NSA. This shows the importance of analyzing mesh quality with more than one computational measure and how important it is to have systematic ways of visualizing meshes and the distribution of measure values. Fig. 14 shows the geometric distance for six models created using NSA and QSlim and three simplification levels. By using a common color map it is possible to directly compare the models and verify which ones have the largest distances to the original (NSA's). Although a widely used measure, the geometric distance did not show evidence of the problems which can be observed in Fig. 15 : the triangle quality shows that the models created with QSlim have triangles with lower quality, evidence that something may be wrong, and a visual inspection reveals strange triangulations covering the center hole. Although clearly needing further study, this analysis seems to show that QSlim might not be the proper choice for some kinds of mesh models due to the resulting artifacts and poor element (face) quality. The main contribution of PolyMeCo was clearly the way its integrated environment allowed easy computation of measures for several meshes at once, while providing interactive exploration of the obtained data using the available visualization modes. In any other tool described in the literature for mesh analysis and comparison, such comparison would have been more difficult. One must not forget that in those tools only two meshes are compared at a time and there is no possibility of comparing among the results obtained for a set of meshes, except for the numerical values, or a third party application is used to visualize the colored meshes with the already mentioned problem of independent color mappings. Having all the computational measures, meshes, and visualization options in one tool motivated data exploration, which we believe might promote a more complete and accurate analysis fostering further insight. For a more detailed description of this case study and a more complete analysis of the results see Silva et al. [24]. 6 Availability and community contributions The current version of PolyMeCo is freely available on the web at as well as some support documentation and a discussion forum. It is our purpose to open PolyMeCo to external contributions which, we believe, will result in an improved and more useful tool. PolyMeCo's main features (visualization modes, information windows, computational measures and representations) are developed as plugins. This allows users to develop and integrate their own plugins in PolyMeCo, e.g., to add a new metric. A “Plugin Kit” is freely available which provides a workspace for plugin development along with some examples of each kind of plugin supported. Also, if developers intend to improve existing functionalities or make changes in the core code of PolyMeCo, the source code can be requested. 7 Conclusions and future work PolyMeCo has several innovative features that distinguish it from similar tools presented in the literature: 1. Integrated environment where several meshes can be analyzed/compared simultaneously. 2. Wider range of computational measures. 3. Customization of light source properties. 4. Several data representations and visualization modes to provide alternative visualizations of the data. 5. Feature comparison using common color mapping. 6. Location probe to obtain specific information about the computed data. 7. Workspace saving and data export. The integrated environment allows working with several meshes simultaneously, thus speeding the analysis and comparison process. After loading all the meshes, the user can easily compute several measures and analyze the obtained data. An important feature is that no third-party application is needed to visualize the data, as happens with other tools, such as MeshDev [6]. While working with representations using color (several alternative color scales are available), the user can adjust the color mapping when in the presence of data containing outliers (in order to eliminate them from the color mapping) or to highlight specific values. Among the available visualization modes the Feature Comparison mode is of paramount importance, as it allows comparing meshes using color representations (or histograms). This is performed by allowing the definition of a common color mapping among all meshes. Without this innovative feature colored meshes can only be used to compare measures incidence among meshes, but never their relative values, as is performed in [4]. By providing a location probe, PolyMeCo allows the user to obtain information regarding vertices, namely their coordinates, neighborhood and associated values according to a measure, which is a feature not available in any of the tools described in the literature. Performing mesh analysis and comparison may require storing the computed data for future reference or to continue the analysis later. As some measures take quite some time to compute, it would be a tedious task to compute them whenever they were necessary. For this reason, PolyMeCo allows saving the current workspace (i.e., all loaded meshes and measures) and restore it later. It is also possible to export the data obtained to allow, for example, further analysis using a different tool or to build tables for data comparison and presentation. The presented application example, as well as research work carried out by other authors (see Jacka et al. [25], Silva et al. [26] and Distler [27]), show that PolyMeCo is, indeed, a helpful tool in its field. Users have expressed their appreciation towards PolyMeCo, stating that one of its advantages is the fact that it allows experimenting with the different measures and visualization modes in a very easy way. By having a plugin based architecture, PolyMeCo also provides a clean and easy way of extending its features. We believe that all the characteristics described above establish PolyMeCo as a tool for systematic mesh analysis and comparison, not only for mesh processing algorithm developers but also for those who perform mesh processing for particular purposes. Still, it can be enhanced in many ways. The Location Probe can be improved in order to provide information regarding all the loaded meshes (not only those depicting analysis/comparison data). New computational measures may be added. All the mesh comparisons performed in PolyMeCo consider that the compared meshes have their principal axes aligned and that all differences are due to surface distortion. Therefore, a mesh compared with a version of itself rotated/translated in 3D will result in significant differences, e.g., large geometric distances when the difference is only due to affine transformations. These differences should be reported to the user, but stating their nature. To deal with this, it might be a good idea to use methods such as principal component analysis [28] and the extended Gaussian image [29,30]. It would also be interesting to test if 3D shape descriptors [31,32], usually used in retrieving meshes from databases, can be used to compare polygonal meshes. Another interesting feature to add might be that of spectral analysis of mesh surfaces [33,34], in a similar approach to that provided by Fourier analysis regarding, for example, audio signals. It would be desirable that PolyMeCo somehow suggested the more appropriate color scale for each particular case based, among others, on the spectral features of the data and on the type of task the user wants to accomplish [35]. This would be an important step forward towards a more complete application of the principles described in Silva et al. [18]. Acknowledgments The authors would like to thank Michaël Roy for providing the source code for MeshDev and all the reviewers for their valuable comments and suggestions. The first author would like to thank research unit 127/94 IEETA of the University of Aveiro for the grant that allowed his work. References [1] Taubin G. Geometric signal processing on polygonal meshes. In: Eurographics 2000—state-of-the-art report, Interlaken, Switzerland, 2000. [2] P. Cignoni R. Scopigno M. Tarini A simple normal enhancement technique for interactive non-photorealistic renderings Computers & Graphics 29 1 2005 125 133 [3] P. Cignoni C. Rocchini R. Scopigno Metro: measuring error on simplified surfaces Computer Graphics Forum 17 2 1998 167 174 [4] Zhou L, Pang A. Metrics and visualization tools for surface mesh comparison. In: Proceedings of the SPIE 2001, visual data exploration and analysis VIII, vol. 4302, 2001. p. 99–110. [5] Aspert N, Santa-Cruz D, Ebrahimi T. MESH: measuring errors between surfaces using the Hausdorff distance. In: Proceedings of the IEEE international conference on multimedia and expo 2002, Lausanne, Switzerland, vol. 1, 2002. p. 705–8. [6] M. Roy S. Foufou F. Truchetet Mesh comparison using attribute deviation metric International Journal of Image and Graphics 4 1 2004 127 140 [7] S. Silva J. Madeira B. Sousa Santos PolyMeCo—a polygonal mesh comparison tool Proceedings of the 9th international conference on information visualization (IV05), London, UK 2005 IEEE Computer Society 842 847 [8] Botsch M, Steinberg S, Bischoff S, Kobbelt L. OpenMesh—a generic and efficient polygon mesh data structure. In: First OpenSG symposium, Darmstadt, Germany, 2002. [9] Fox toolkit, in 〈 〉 (online October 2008). [10] J.R. Shewchuk What is a good linear element? Interpolation, conditioning, and quality measures Proceedings of the 11th international meshing roundtable 2002 115 126 [11] R.P. Bhatia K.L. Lawrence Two-dimensional finite element mesh generation based on stripwise automatic triangulation Computers and Structures 36 1990 309 319 [12] H. Graf S.P. Serna A. Stork Adaptive quality meshing for on-the-fly volumetric mesh manipulations within virtual environments Proceedings of the 2006 IEEE international conference virtual environments, human–computer interfaces and measurement systems 2006 178 183 [13] H. Theisel C. Rössl R. Zayer H.-P. Seidel Normal based estimation of the curvature tensor for triangular meshes Proceedings of the 12th Pacific conference on computer graphics and applications 2004 288 297 [14] C. Lee A. Varshney D. Jacobs Mesh saliency Proceedings of the SIGGRAPH 2005 659 666 [15] Z. Karni C. Gotsman Spectral compression of mesh geometry Proceedings of the SIGGRAPH 2000 279 286 [16] O. Sorkine D. Cohen-Or S. Toledo High-pass quantization for mesh encoding Proceedings of the Eurographics symposium on geometry processing 2003 42 51 [17] B. Sousa Santos S. Silva C. Ferreira J. Madeira Comparison of methods for the simplification of mesh models of the lungs using quality indices and an observer study Proceedings of the third international conference on medical information visualization—biomedical visualization (MediVis05) 2005 15 21 [18] S. Silva J. Madeira B. Sousa Santos There is more to color scales than meets the eye: a review on the use of color in visualization Proceedings of the 11th international conference on information visualization (IV07), Zurich, Switzerland 2007 IEEE Computer Society 943 950 [19] B. Rogowitz L. Treinish Data visualization: the end of the rainbow IEEE Spectrum 35 12 1998 52 59 [20] D. Borland R. Taylor Rainbow color map (still) considered harmful IEEE Computer Graphics & Applications 27 2 2007 14 17 [21] H. Levkowitz G. Herman Color scales for image data IEEE Computer Graphics & Applications 12 1 1992 72 80 [22] M. Botsch M. Pauly C. Rössl S. Bischoff L. Kobbelt Geometric modeling based on triangle meshes Eurographics 2006—tutorials, Vienna, Austria 2006 [23] B. Sousa Santos S. Silva L. Teixeira C. Ferreira P. Dias J. Madeira Preliminary evaluation of PolyMeCo: a visualization based tool for mesh analysis and comparison Proceedings of geometric modeling and imaging 2007—new advances, Zurich, Switzerland 2007 IEEE Computer Society 133 139 [24] Silva S, Silva F, Madeira J, Sousa Santos B. Evaluation of mesh simplification algorithms using PolyMeCo: a case study. In: Proceedings of the SPIE. Visualization and data analysis 2007, San José, California, USA, vol. 6495, p. 54950D, 2007. [25] D. Jacka A. Reid B. Merry J. Gain A comparison of linear skinning techniques for character animation Proceedings of the AFRIGRAPH 2007 177 186 [26] F. Silva NSA algorithm: geometrical vs. visual quality International conference computational science and its applications ICCSA 2007 2007 515 523 [27] Distler P. Beschreibung und Bewertung von Algorithmen für die Tesselierung von CAD-Modellen. Diplom-Arbeit, Fachbereich Maschinenbau, TU Darmstadt, Germany, 2007. [28] M. Petrou P. Bosdogianni Image processing: the fundamentals 1999 Wiley New York [29] B.K.P. Horn Extended Gaussian images Proceedings of IEEE 72 12 1984 1671 1686 [30] C. Sun J. Sherrah 3D symmetry detection using the extended Gaussian image IEEE Transactions on Pattern Analysis and Machine Intelligence 19 2 1997 164 168 [31] B. Bustos D. Keim D. Saupe T. Schreck D. Vranic An experimental effectiveness comparison of methods for 3D similarity search International Journal on Digital Libraries 6 1 2006 39 54 [32] M. Reuter F.-E. Wolter N. Peinecke Laplace–Beltrami spectra as shape-DNA of surfaces and solids Computer-Aided Design 38 2006 342 366 [33] M. Pauly M. Gross Spectral processing of point-sampled geometry Proceedings of the SIGGRAPH 2001 379 386 [34] Sorkine O. Laplacian mesh processing. In: Eurographics 2005—state-of-the-art report, Dublin, Ireland, 2005. [35] L. Bergman B. Rogowitz L. Treinish A rule-based tool for assisting color map selection Proceedings of IEEE visualization 1995 118 125 "
    },
    {
        "doc_title": "Head-mounted display versus desktop for 3D navigation in virtual reality: A user study",
        "doc_scopus_id": "58049208308",
        "doc_doi": "10.1007/s11042-008-0223-2",
        "doc_eid": "2-s2.0-58049208308",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "3d navigations",
            "Application areas",
            "Design and developments",
            "Game scenarios",
            "Head mounted display (HMD)",
            "Navigation tasks",
            "Set-ups",
            "Short periods",
            "Usability evaluations",
            "User evaluations",
            "User performances",
            "User study",
            "Virtual environments (VE)",
            "Vr systems"
        ],
        "doc_abstract": "Virtual Reality (VR) has been constantly evolving since its early days, and is now a fundamental technology in different application areas. User evaluation is a crucial step in the design and development of VR systems that do respond to users' needs, as well as for identifying applications that indeed gain from the use of such technology. Yet, there is not much work reported concerning usability evaluation and validation of VR systems, when compared with the traditional desktop setup. The paper presents a user study performed, as a first step, for the evaluation of a low-cost VR system using a Head-Mounted Display (HMD). That system was compared to a traditional desktop setup through an experiment that assessed user performance, when carrying out navigation tasks in a game scenario for a short period. The results show that, although users were generally satisfied with the VR system, and found the HMD interaction intuitive and natural, most performed better with the desktop setup. © 2008 Springer Science+Business Media, LLC.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A decision-support tool for a capacitated location-routing problem",
        "doc_scopus_id": "56049110500",
        "doc_doi": "10.1016/j.dss.2008.07.007",
        "doc_eid": "2-s2.0-56049110500",
        "doc_date": "2008-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Management Information Systems",
                "area_abbreviation": "BUSI",
                "area_code": "1404"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Developmental and Educational Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3204"
            },
            {
                "area_name": "Arts and Humanities (miscellaneous)",
                "area_abbreviation": "ARTS",
                "area_code": "1201"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Decision-support tool",
            "Geographic datums",
            "Routing problems",
            "Support tools",
            "Vehicle fleets",
            "Web map servers",
            "Windows platforms"
        ],
        "doc_abstract": "In this paper we present a decision-support tool (DST) that implements a capacitated location-routing problem (CLRP) with two levels (depots and customers) and a capacitated and homogeneous vehicle fleet. It allows the exploration of the solution finding process in a way easily understandable by the user, and enables access to online geographic data through web map servers (WMS). This tool was developed for Windows platforms having an architecture that easily allows the integration of new functionality. © 2008 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271653 291210 291773 291813 291820 31 Decision Support Systems DECISIONSUPPORTSYSTEMS 2008-07-18 2008-07-18 2010-04-18T12:12:23 S0167-9236(08)00133-4 S0167923608001334 10.1016/j.dss.2008.07.007 S300 S300.1 FULL-TEXT 2015-05-14T05:08:13.335849-04:00 0 0 20081201 20081231 2008 2008-07-18T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 0167-9236 01679236 46 46 1 1 Volume 46, Issue 1 30 366 375 366 375 200812 December 2008 2008-12-01 2008-12-31 2008 article fla Copyright © 2008 Elsevier B.V. All rights reserved. ADECISIONSUPPORTTOOLFORACAPACITATEDLOCATIONROUTINGPROBLEM LOPES R 1 Introduction 2 A capacitated location-routing problem (CLRP) 3 Solving the capacitated location-routing problem 3.1 Cluster analysis 3.2 Travelling salesman problem 3.3 Improvement of client routes 3.4 Capacitated location-allocation problem 4 A decision-support tool for the CLRP 4.1 Graphical user interface (GUI) 4.2 Information flow 4.2.1 Data input 4.2.2 Web map server integration 4.2.3 Graphical representation 4.2.4 Visualization of the solution 4.2.5 Data output 4.2.6 Other characteristics 5 Conclusion References ALBAREDASAMBOLA 2005 407 428 M ALTER 2004 319 327 S AMBROSINO 2005 610 624 D BARRETO 2007 968 977 S BOOKBINDER 2005 461 466 J BRANCO 1990 86 95 I BRUNS 1995 1 6 A PROCEEDINGSSECONDINTERNATIONALWORKSHOPDISTRIBUTIONLOGISTICS ITERATIVEHEURISTICFORLOCATIONROUTINGPROBLEMSBASEDCLUSTERING CARLSSON 2002 105 110 C COMMISSIONOFTHEEUROPEANCOMMUNITIES 2001 EUROPEANTRANSPORTPOLICYFOR2010TIMEDECIDECOM2001370BRUSSELSWHITEPAPER DIX 2004 A HUMANCOMPUTERINTERACTION EOM 1990 333 342 H EOM 1997 18 32 S EVERITT 1993 B CLUSTERANALYSIS LAPORTE 1988 163 197 G VEHICLEROUTINGMETHODSSTUDIES LOCATIONROUTINGPROBLEMS LIN 2006 1833 1849 C MALCZEWSKI 1999 J GISMULTICRITERIADECISIONANALYSIS MELECHOVSKY 2005 375 391 J MELO 1999 471 476 A MIN 1996 259 288 H MIN 1998 1 15 H MURTY 1999 175 182 K NAGY 1996 1166 1174 G NAGY 2007 649 672 G PERL 1985 381 396 J SALHI 1989 150 156 S SHIM 2002 111 126 J SRIVASTAVA 1990 427 435 R TUZUN 1999 87 99 D LOPESX2008X366 LOPESX2008X366X375 LOPESX2008X366XR LOPESX2008X366X375XR item S0167-9236(08)00133-4 S0167923608001334 10.1016/j.dss.2008.07.007 271653 2010-10-11T17:08:56.215647-04:00 2008-12-01 2008-12-31 true 2789438 MAIN 10 90054 849 656 IMAGE-WEB-PDF 1 fx1 10806 11 11 fx1 2181 49 50 fx10 10851 11 11 fx10 2196 49 50 fx11 11175 11 11 fx11 2508 49 50 fx12 11213 11 11 fx12 2542 49 50 fx13 11257 11 11 fx13 2598 49 49 fx14 11188 11 11 fx14 2244 49 49 fx15 11100 11 11 fx15 2458 49 49 fx2 10886 11 11 fx2 2201 50 50 fx3 11063 11 11 fx3 2200 50 50 fx4 11182 11 11 fx4 2465 50 50 fx5 12649 11 24 fx5 4037 50 110 fx6 13182 11 38 fx6 3563 37 125 fx8 11280 11 11 fx8 2044 49 50 fx9 11313 11 11 fx9 2159 49 50 fx7 4383 11 24 fx7 3715 50 110 gr1 31161 425 347 gr1 1300 93 76 gr10 84385 414 493 gr10 5362 93 111 gr11 30749 351 346 gr11 1775 93 92 gr12 46073 381 329 gr12 3169 93 80 gr13 34824 388 346 gr13 1868 93 83 gr2 108611 464 444 gr2 4461 93 89 gr3 90952 451 623 gr3 5853 91 125 gr4 26887 178 274 gr4 4741 81 125 gr5 78351 270 489 gr5 5935 69 125 gr6 159012 902 623 gr6 3998 93 64 gr7 65906 388 489 gr7 4917 94 118 gr8 245962 902 623 gr8 4169 93 64 gr9 61094 450 623 gr9 3673 90 125 DECSUP 11520 S0167-9236(08)00133-4 10.1016/j.dss.2008.07.007 Elsevier B.V. Fig. 1 Input and output data concerning each phase of the sequential heuristic for the CLRP. Fig. 2 Some instances and corresponding results of the sequential heuristic for the CLRP. Fig. 3 The GUI and the four different visualization areas. Fig. 4 Clients data with the Import and Export options. Fig. 5 Graphical data input (directly into the map area). Fig. 6 Currently featured server integration (Demis® and Google Maps®). Fig. 7 Objects graphical representation according to their position on the map. Fig. 8 Changing the map scale (from 1:1098 to 1:612). Fig. 9 Visualization of clients demand. Fig. 10 Control panel displaying the results associated to each grouping method. Fig. 11 Graphical representation of the solution. Fig. 12 Data window with the numerical information of the solution. Fig. 13 Visualization of the VRP (after the third step of the heuristic) on the DST. A decision-support tool for a capacitated location-routing problem Rui Borges Lopes a ⁎ Sérgio Barreto b Carlos Ferreira a Beatriz Sousa Santos c a Department of Economics, Management and Industrial Engineering/CIO, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal b ISCA-Higher Institute of Accounting and Administration/CIO, University of Aveiro, R. Associação Humanitária dos Bombeiros de Aveiro, 3811-902 Aveiro, Portugal c Department of Electronics, Telecommunications and Informatics/IEETA, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 234370361; fax: +351 234370215. In this paper we present a decision-support tool (DST) that implements a capacitated location-routing problem (CLRP) with two levels (depots and customers) and a capacitated and homogeneous vehicle fleet. It allows the exploration of the solution finding process in a way easily understandable by the user, and enables access to online geographic data through web map servers (WMS). This tool was developed for Windows platforms having an architecture that easily allows the integration of new functionality. Keywords Decision-support tool Location-routing Web map servers 1 Introduction Location and routing have been some of the major concerns in logistics, having implications on the complete supply chain [7]. It is generally accepted that the success of many enterprises may depend on the location-distribution decisions [3,11] and that, nowadays, many managers tend to support their decisions on acquired experience [10]. This attitude may be caused by both the overall complexity of the problem and the specificities of different cases [22,25]. The development of an effective tool to support this kind of decisions is similarly complex due to the same reasons. Moreover, in the case of a tool to support decisions for location-routing problems (LRP), there are other key aspects beyond optimization issues; one such aspect is the presentation of the solution and the exploration of the process in a way easily understandable by the decision-maker, allowing better judgments. One can find in the literature a significant number of contributions concerning LRPs [16,22,25], as well as contributions regarding decision-support systems (DSS) [2,13,14,28]. On the other hand there is a lack of studies involving both areas. For that reason, an integrated approach to location and routing, and an application that can support the decision, may represent an important competitive advantage. With this paper we will try to help filling that gap, presenting a decision tool for a LRP integrating the essential features of this type of problem. 2 A capacitated location-routing problem (CLRP) The location-routing problem (LRP) appears as a combination of two (difficult) problems: the facility location problem (FLP) and the vehicle routing problem (VRP); both of them can be shown to be NP-hard. Moreover, the multiplicity of characteristics of those models also leads to a wide diversity of LRPs [22,25]. The LRP is a better model in contexts involving the simultaneous location of facilities and the design of distribution routes between the facilities and the users, given that solving separately these two aspects will most likely produce a suboptimal solution [27]. In this paper we consider a discrete capacitated location-routing problem (CLRP) with two layers (depots and customers) and a capacitated and homogeneous vehicle fleet. Each customer has a certain demand and the potential facilities (depots) have a certain capacity; the location cost of each depot is known, as well as the unitary cost of distribution (function of covered distance). Euclidean distances are considered. The model seeks to determine which depots must be opened (established) and to draw the distribution routes from these depots to the customers (vehicles start and end their routes at the same depot) minimizing the total cost (location and distribution costs) [6]. While the multiplicity of real cases and the integration of their characteristics in LRP models lead to a great diversity of problems [22,25], the above described CLRP integrates the essential LRP features and considers appropriate constraints to guarantee routes and depots with balanced capacity. 3 Solving the capacitated location-routing problem The capacitated location-routing problem (CLRP) is NP-hard; thus, most practical (large dimension) problems call for heuristic algorithms in order to obtain “good solutions” in reasonable time [1,17]; moreover, heuristic algorithms are (usually) easy to understand, modify and implement, providing several solutions allowing the user the flexibility to choose the preferred one. Heuristic methods for LRPs included sequential [23], iterative [26] and hierarchical [24] approaches. Recently, metaheuristics such as tabu search, simulated annealing and genetic algorithms were proposed [19,30]. Due to the characteristics of our model (vehicles with small capacity and significant fixed costs for the facilities), we used a sequential method as it has some advantages from a computational point of view [21,29]. More precisely we used a sequential distribution-first location-second heuristic [5], consisting of four main steps: (i) construct groups of customers with a capacity limit; (ii) determine the distribution in each customer group; (iii) improve the routes and (iv) locate the depots and assign the routes to them. Fig. 1 illustrates the schema of the heuristic and some details of the steps are presented in this section. This heuristic has been submitted to several computational tests with promising results, validating its use in a DSS (Fig. 2 ). The interested reader is referred to the work of [5] for a complete presentation and evaluation of this heuristic. The instances presented in Fig. 2 are available from [4]. 3.1 Cluster analysis Several authors have integrated cluster analysis procedures in heuristics for LRPs [9,21]. A detailed description of the clustering phase used in this heuristic can be found in [6]. The first step of the heuristic groups clients based on the capacity of the used vehicles. For this purpose, twenty different possibilities are used, combining four clustering methods (two hierarchical and two non-hierarchical) with six proximity measures (single linkage or nearest neighbour, complete linkage or farthest neighbour, group average, centroid, ward and saving) [15]. 3.2 Travelling salesman problem After the definition of the different groups of clients, an optimal distribution route for each cluster is determined, solving a travelling salesman problem (TSP). The method is based on the relaxation of the sub-routes constraints, in a two-index integer linear programming TSP formulation, whenever the group has 40 customers or less. The obtained integer problem is solved using the CPLEX® software package. Whenever sub-routes are detected, adequate constraints are introduced and the iterative process continues until the determination of the TSP optimal solution. If the group integrates more than 40 customers, a two-step heuristic is used for solving the TSP. In the first step, a feasible TSP route is obtained using the farthest-type neighbourhood criteria, to choose the next client to be included in the route, and saving criteria, to determine where to include it. In the second step, the feasible TSP solution is improved through a 3-optimal local search procedure. 3.3 Improvement of client routes This step provides an improvement of the routes found so far, using a 3-optimal local search as the one proposed by [8] for the Hamiltonian p-median problem. Besides providing a decrease of the routes cost, this step contributes to the elimination of the disagreeable biased effect, which emerges when the capacitated groups are formed. This phase provides the final routes, their capacity and the distance covered by them. 3.4 Capacitated location-allocation problem In the last step of the heuristic a capacitated location-allocation problem (CLAP) is solved. In this problem the heuristic allocates the routes to the depots (if it is advisable to install the depot). The routes are collapsed into a single client with an associated cost of allocation (to each depot) obtained by a saving function. The resulting single source CLAP is solved exactly using the CPLEX® software package. 4 A decision-support tool for the CLRP A very important issue of this tool is the target audience; we do not expect users (decision-makers) to have any background on modelling and optimization aspects concerning LRPs. Thus, the information provided to users is neither technical data regarding the heuristic nor its validation; we focused instead on providing a usable interface. To implement the application, we used an object-oriented (OO) methodology (unified modelling language—UML) and Visual Basic®, version 6.0. The developed application is organized around a main window, with all the functionalities accessible in this window, through the toolbar, or the menu (in a way easily understandable by users). In this context, the target user of this application is typically a decision-maker, with higher education and moderate computer literacy, but having much experience in the subject (professional experience in real installations of depots and designing logistic systems) and that will use the tool infrequently. This user profile suggests that the main usability goal should be ease of learning; therefore, the user interface should be extremely intuitive. The profile of the end-users, as well as the task they intend to perform using this DST, and the usability principles (e.g. consistency, compatibility, familiarity, feedback, robustness, etc.) [12] were taken into consideration during the design of the tool. The tool offers the following possibilities: • Input (or edit) new (or existing) data in order to define the problem; • Obtain solutions for the CLRP; • Visualize the results either through numeric or graphical representation; • Export the data to other applications; • Export the graphical solution. The developed tool incorporates two main parts: the solution algorithms (previously presented) and the graphical user interface (GUI). It was implemented for Windows platforms and has an open architecture which allows an easy integration of new functionality. The DST can also incorporate any other solution method for the CLRP that uses commercial software, due to its integration with the CPLEX® which allows solving the provided formulation. 4.1 Graphical user interface (GUI) The main purpose of the GUI was to allow an easy and efficient access to solutions for the CLRP. Thus, according to [18], the following characteristics are fundamental: • Easy to learn: allowing the intuitive use of the tool by any user; • Robustness: allowing the user to recover from unintended situations; • Interactivity: allowing the information to effectively flow between the user and the system; • Based on events: allowing the user to always be aware of the tasks he is performing. The GUI comprises four distinct sections as shown in Fig. 3 : • A toolbar with buttons allowing a quick access to the functionalities (Area A); • An alphanumerical component to display all the information regarding the problem (clients and depots data, vehicle capacity) (Area B); • A visualization area displaying the information regarding the maps (Area C); • A status bar showing the used hierarchic method and proximity measure (clustering phase), an icon indicating if the location of the objects in the map is provided by the application (Yes: ; No: ) and if the obtained solution is exact or approximate (Area D). Regarding the toolbar, there is a set of buttons corresponding to different functionalities, besides the standard ones (New, Open, Save, Print, etc.): Language: allows changing the language of the tool (currently supports English, Portuguese and Spanish); Import Map: allows importing an image to the map (bmp, or format files); Import Client/Depot Picture: allows importing an image (standard format: ico, bmp, etc.) to represent the client/depot; Client/Depot/Route Colour: allows modifying the client/depot/route icons colour in order to easily highlight the objects and identify the different routes on the map; Display Labels/Pictures: allows activating or deactivating the visualization of icons corresponding to clients and depots; View Demand: allows visualizing the clients based on their demand; Recalculate Scale: allows recalculating scale, hence adjusting the objects to the visualization area; Zoom In: allows zooming in on a specific area of the map. The interaction with web map servers (WMS) provides new imagery and detailed data of the selected region; Move Map: allows moving the map inside the visualization area. Through this option it is possible to adjust the map image to the remaining objects; Lock Relation: allows maintaining the relation between the map and the objects. This enables the use of different visualization scales for the map and the objects; Hide Map: allows hiding the image inserted in the map area, allowing a better view of the objects; Import Solution: allows importing a solution file obtained with other software; Save Map: allows saving to an image file all the data in the map area (map, clients, depots and routes). Most of these options are also available through the main menu, allowing a greater flexibility to the user. 4.2 Information flow In this section the information flow of the proposed DST will be presented. Firstly, the data input options, needed to insert the necessary information for characterizing and solving the problem, are presented. After the data input it is possible to obtain online geographic data from the web map servers (WMS). It is also possible to proceed to a set of changes, from a graphical point of view, namely associating maps and images, changing the visualization scale, etc., thus making the interpretation of the information easier. Further on, users can choose several ways to run the algorithm, enabling them to visualize and analyse the obtained solution. Finally, users may export the obtained solution as well as the inserted data. The following subsections describe in more detail how to work with the proposed DST, as well as its main functionalities. 4.2.1 Data input The information needed to obtain the solution using the tool is the following: • Clients location; • Demand associated with each client; • Possible locations for the depots; • Capacity associated with each depot; • Vehicles capacity. Our tool allows the required data to be inserted either globally or one client or depot at a time. In order to insert the data globally, there is an Open file option (allowing the user to recover all data regarding a problem previously saved) as well as an Import data option (allowing to import several clients and depots, even if they where originally inserted in other applications). Through this set of options the user can quickly import a large quantity of data. The input (or update) of new (or existing) data individually can be done either numerically (as shown in Fig. 4 ) or graphically (directly on the map). The option to insert the data directly on the map area makes easier the identification of the location on the map (X and Y coordinates) where the user intends to insert the client and/or depot (Fig. 5 ). In order to edit the inserted data it is possible to drag the clients and/or depots across the map area (allowing the corresponding update of its coordinates). Finally, the capacity of the fleet is inserted, thus fully characterizing the capacitated location-routing problem (CLRP). 4.2.2 Web map server integration The OpenGIS web map server (WMS) protocol defines an interface for web based mapping applications; it is based on a query syntax for posting a request for the desired layers and zoom window to the server, which returns a map as a standard picture. Our decision-support tool enables the user to obtain real online geographic information using WMS. Currently, the application supports the integration with the following servers (Fig. 6 ): • Demis®; • Google Maps®. When these options are enabled the user can zoom in on any point of the world map, thus obtaining real online geographic information on the selected area. All the options regarding the layers of both servers are fully integrated in the application. Demis® is an online server that complies with the OpenGIS WMS protocol. Google Maps® is an online server that provides the same satellite imagery than Google Earth®, although using a different projection. 4.2.3 Graphical representation Most of the graphical representation functionalities are concerned with the map and its interaction with the remaining objects (clients, depots and routes). In the map area there is a set of options that allow the introduction, editing and visualization of the inserted data. In order to ensure that the objects (clients and depots) and associated information are always visible, different icons were used according to the objects location on the map and the distance to the border (Fig. 7 ) [20]. There are different ways to define the map scale, and the user has the possibility to change the representation scale. The application allows the user to zoom in or out (by increasing or decreasing the representation scale) on a specific area of the map, in order to have a better view of the data in that area (Fig. 8 ). There is also the possibility to lock the relation between both representation scales (map and objects); moreover, combined with the “Move map” option, allows the user to easily adjust the map to the remaining objects (clients and depots). When working with the WMS the zoom option provides new imagery and geographic data regarding the selected area. It is also possible to visualize the map based on the clients demand. A circle where the radius represents the demand is drawn (clients with higher demand values correspond to circles with bigger radius). Through this option the user can easily identify clients with higher demand values, thus providing a useful view of the map (Fig. 9 ). Finally, regarding the routes, they are displayed when the user obtains or imports the solution. When one of these options is used, the application draws a line connecting the clients to the depots (or to other clients). A different colour is associated to each route, allowing a better identification of the solution proposed. Information regarding the used grouping method can be visualized in the status bar; there is also an indication about the solution: optimal or approximate. 4.2.4 Visualization of the solution In order to obtain the solution, the user can choose one of the following options: • Import the solution file; • Run the algorithm, choosing a specific grouping method; • Run the algorithm for all the grouping methods; • Run the algorithm step-by-step. By allowing to import a solution file, it becomes possible to easily interact with other applications. With this option it is also possible to get solutions from algorithms currently not included in this tool; the user only has to guarantee the solution file follows a specific structure. If the user runs the algorithm for all the grouping methods [6], the application displays a window where the user can visualize the different grouping methods. The total cost of the solution associated to each method, a bar indicating the percentage of improvement (as compared to the worst obtained result) as well as an image where the user can easily identify the best solutions found (Fig. 10 ). Afterwards, the user must choose the solution to visualize, either graphically on the map or numerically. In the former case, routes are displayed using different colours and the depots that will not be installed will have a new graphical representation, making it easier to understand the solution (Fig. 11 ). All the information regarding the solution can also be visualized numerically (Fig. 12 ). The user can see the total cost of the solution, the depots to be installed, and the data of the different routes (capacity, path and cost). Finally, it is also possible to run the algorithm step-by-step, which provides the application with a strong pedagogical component, allowing the user to have a detailed view of the development of the algorithm. This option enables the user to additionally handle the vehicle routing problem (VRP) and the capacitated location-allocation problem (CLAP) by running only a subset of the original heuristic (the first, second and third steps to obtain the VRP solution and the fourth step for the CLAP), as shown in Fig. 13 . 4.2.5 Data output Regarding the data output the following options are available: • Data export; • Save the data in a file; • Save the map image (containing all the information). In order to export data, there is the possibility of exporting the clients and the depots data to text format files, facilitating the integration with other software. There is also the possibility of saving the data in a file. Through this option the user can save all the data into a single file, with a specific format. Finally, there is the “Save Map” option, which allows the user to save the map he is currently visualizing to an image file. 4.2.6 Other characteristics Besides the previously mentioned options, there is a set of characteristics that have been added to the application in order to improve its usefulness. For instance: an option to change the language; a help file and a set of options regarding the map, the visualization scale and the algorithm used. The integration of other algorithms is made through the “Solver Options”, available in the menu. 5 Conclusion A decision-support tool that implements a capacitated location-routing problem (CLRP) with two levels (depots and customers) and a capacitated and homogeneous vehicle fleet was presented. It allows the exploration of the solution finding process in a way easily understandable by the user, and enables access to online geographic data through web map servers (WMS). A usable user interface was a great concern throughout the development of this tool, which was designed to allow decision-makers with a moderate computer literacy to be able to obtain good quality solutions without much learning effort. Although the best decision may not be the most cost efficient, this type of tool can help managers make a more scientifically supported decision, by providing the total estimated costs of a set of different solutions. From this point it is up to the manager to make the decision, taking into consideration the estimated cost, the service level or even the company strategy and motivation. A decision-support tool assists, but does not replace, the decision-maker. It does not try to provide the ‘answer’, nor does it impose a predefined sequence of analysis. It supports semi-structured decisions where parts of the analysis can be systematized by the tool, improving the decision-maker's insight and judgement. Although this decision-support tool has been developed for the CLRP, due to its modular architecture, it could also include, Network CLRP, LRP with paths and even more specific cases such as the CLAP and the VRP. Moreover, future developments could lead to further integration with other WMS or geographic information systems (GIS). References [1] M. Albareda-Sambola J.A. Díaz E. Fernández A compact model and tight bounds for a combined location-routing problem Computers & Operations Research 32 2005 407 428 [2] S. Alter A work system view of DSS in its fourth decade Decision Support Systems 38 2004 319 327 [3] D. Ambrosino M.G. Scutellà Distribution network design: new problems and related models European Journal of Operational Research 165 2005 610 624 [4] S.S. Barreto, 2003. [5] S.S. Barreto, Análise e Modelização de Problemas de Localização-Distribuição [Analysis and Modelization of Location-Routing Problems] (Ph.D. dissertation, University of Aveiro, 2004) (in Portuguese). [6] S. Barreto C. Ferreira J. Paixão B.S. Santos Using clustering analysis in a capacitated location-routing problem European Journal of Operational Research 179 2007 968 977 [7] J.H. Bookbinder Global Logistics Transportation Research E 41 2005 461 466 [8] I.M. Branco J.D. Coelho The Hamiltonian p-median problem European Journal of Operational Research 47 1990 86 95 [9] A. Bruns A. Klose An iterative heuristic for location-routing problems based on clustering Proceedings of the Second International Workshop on Distribution Logistics 1995 1 6 [10] C. Carlsson E. Turban DSS: directions for the next decade Decision Support Systems 33 2002 105 110 [11] Commission of the European Communities European transport policy for 2010: time to decide, COM (2001) 370, Brussels, White Paper 2001 [12] A. Dix J. Finlay G. Abowd R. Beale Human–Computer Interaction 3rd ed. 2004 Prentice Hall New Jersey [13] H.B. Eom S.M. Lee Decision support systems applications research: a bibliography (1971–1988) European Journal of Operational Research 46 3 1990 333 342 [14] S.B. Eom S.M. Lee C. Somarajan E.B. Kim Decision support systems applications — a bibliography (1988–1994) OR Insight 10 2 1997 18 32 [15] B.S. Everitt Cluster Analysis 3rd ed. 1993 Arnold London [16] G. Laporte Location-routing problems B.L. Golden A.A. Assad Vehicle Routing: Methods and Studies 1988 North-Holland Amsterdam 163 197 [17] C. Lin C. Kwok Multi-objective metaheuristics for a location-routing problem with multiple use of vehicles on real data and simulated data European Journal of Operational Research 175 2006 1833 1849 [18] J. Malczewski GIS and Multicriteria Decision Analysis 1999 John Wiley and Sons New York [19] J. Melechovský C. Prins C.R. Wolfler-Calvo A metaheuristic to solve a location-routing problem with non-linear costs Journal of Heuristics 11 2005 375 391 [20] A. Melo B.S. Santos C. Ferreira J.S. Pinto Software application for data visualization and interaction in a location routing problem Revista do Departamento de Electrónica e Telecomunicações da Universidade de Aveiro 2 4 1999 471 476 [21] H. Min Consolidation terminal location-allocation and consolidated routing problems Journal of Business Logistics 17 2 1996 259 288 [22] H. Min V. Jayaraman R. Srivastava Combined location-routing problems: a synthesis and future research directions European Journal of Operational Research 108 1998 1 15 [23] K.G. Murty P.A. Djang The U.S. army national guard’s mobile training simulators location and routing problem Operations Research 47 1999 175 182 [24] G. Nagy S. Salhi Nested heuristic methods for the location-routing problem Journal of the Operational Research Society 47 1996 1166 1174 [25] G. Nagy S. Salhi Location-routing: issues, models and methods European Journal of Operational Research 177 2007 649 672 [26] J. Perl M.S. Daskin A warehouse location-routing problem Transportation Research 19B 5 1985 381 396 [27] S. Salhi G.K. Rand The effect of ignoring routes when locating depots European Journal of Operational Research 39 1989 150 156 [28] J.P. Shim M. Warkentin J.F. Courtney D.J. Power R. Sharda C. Carlsson Past, present, and future of decision support technology Decision Support Systems 33 2002 111 126 [29] R. Srivastava W.C. Benton The location-routing problem: considerations in physical distribution system design Computers & Operations Research 17 5 1990 427 435 [30] D. Tuzun L.I. Burke A two-phase tabu search approach to the location routing problem European Journal of operational Research 116 1999 87 99 Rui Borges Lopes received his MSc degree in Operations Management from the University of Aveiro in 2005. He teaches at the Department of Economics, Management and Industrial Engineering of the University of Aveiro and is a researcher in the R&D unit CIO of the University of Lisbon. His current research interests lie primarily in Operations Research (location-routing models), Spatial Decision-Support Systems and Multiple Criteria Programming. Sérgio Barreto is an Associate Professor at the Accounting and Administration Institute of the University of Aveiro where he is a member of the Marketing and Data Analysis Research Center and teacher of the Mathematics group. He has a PhD in Industrial Management from the University of Aveiro. He is also a researcher of the CIO (Operations Research Center) of the University of Lisbon and author and co-author of several scientific papers presented at conferences and published in journals. His research area is related with combinatorial optimization with a special interest in location and routing problems. Carlos Ferreira received a MSc degree in Statistics and Operational Research from the University of Lisbon and a PhD in Mathematics from the University of Aveiro in 1998. He currently is an Associate Professor with the Department of Economics, Management and Industrial Engineering at the University of Aveiro, Portugal, where he is director of the Information Management MSc. His teaching and research interests are in Operations Research, Data Analysis and Information Management. Beatriz Sousa Santos received her PhD in Electrical Engineering in 1989 and is currently Associate Professor with the Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal. She lectures Human–Computer Interaction and Computer Graphics and her main research interests are in the areas of Data and Information Visualization. "
    },
    {
        "doc_title": "3D reconstruction and Auralisation of the \"painted dolmen\" of antelas",
        "doc_scopus_id": "47949110073",
        "doc_doi": "10.1117/12.766607",
        "doc_eid": "2-s2.0-47949110073",
        "doc_date": "2008-07-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "3D acquisition",
            "3D geometries",
            "3D reconstructions",
            "3d visual models",
            "Acoustic absorption coefficients",
            "Archaeological sites",
            "Audio sources",
            "Audio visuals",
            "Augmented reality",
            "Auralisation",
            "Dark rooms",
            "Geometric acoustics",
            "Geometric models",
            "Head Related Transfer Functions",
            "In-situ",
            "Irregular surfaces",
            "Iterative algorithms",
            "Iterative Closest points",
            "Laser range finder",
            "Orientation sensors",
            "Reconstruction softwares",
            "Reverberation times",
            "Software computes",
            "Sound waves",
            "Source localisation",
            "Stereo headphones",
            "Uniform grids",
            "Visual scenes",
            "Visualization toolkits"
        ],
        "doc_abstract": "This paper presents preliminary results on the development of a 3D audiovisual model of the Anta Pintada (painted dolmen) of Antelas, a Neolithic chamber tomb located in Oliveira de Frades and listed as Portuguese national monument. The final aim of the project is to create a highly accurate Virtual Reality (VR) model of this unique archaeological site, capable of providing not only visual but also acoustic immersion based on its actual geometry and physical properties. The project started in May 2006 with in situ data acquisition. The 3D geometry of the chamber was captured using a Laser Range Finder. In order to combine the different scans into a complete 3D visual model, reconstruction software based on the Iterative Closest Point (ICP) algorithm was developed using the Visualization Toolkit (VTK). This software computes the boundaries of the room on a 3D uniform grid and populates its interior with \"free-space nodes\", through an iterative algorithm operating like a torchlight illuminating a dark room. The envelope of the resulting set of \"free-space nodes\" is used to generate a 3D iso-surface approximating the interior shape of the chamber. Each polygon of this surface is then assigned the acoustic absorption coefficient of the corresponding boundary material. A 3D audiovisual model operating in real-time was developed for a VR Environment comprising head-mounted display (HMD) I-glasses SVGAPro, an orientation sensor (tracker) InterTrax 2 with 3 Degrees Of Freedom (3DOF) and stereo headphones. The auralisation software is based on a geometric model. This constitutes a first approach, since geometric acoustics have well-known limitations in rooms with irregular surfaces. The immediate advantage lies in their inherent computational efficiency, which allows real-time operation. The program computes the early reflections forming the initial part of the chamber's impulse response (IR), which carry the most significant cues for source localisation. These early reflections are processed through Head Related Transfer Functions (HRTF) updated in real-time according to the orientation of the user's head, so that sound waves appear to come from the correct location in space, in agreement with the visual scene. The late-reverberation tail of the IR is generated by an algorithm designed to match the reverberation time of the chamber, calculated from the actual acoustic absorption coefficients of its surfaces. The sound output to the headphones is obtained by convolving the IR with anechoic recordings of the virtual audio source. © 2008 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Serial volumetric registration of pulmonary CT studies",
        "doc_scopus_id": "44349121638",
        "doc_doi": "10.1117/12.769771",
        "doc_eid": "2-s2.0-44349121638",
        "doc_date": "2008-06-02",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Lung imaging",
            "Oncological pathology",
            "Pathologic changes",
            "Spatial resolution"
        ],
        "doc_abstract": "Detailed morphological analysis of pulmonary structures and tissue, provided by modern CT scanners, is of utmost importance as in the case of oncological applications both for diagnosis, treatment, and follow-up. In this case, a patient may go through several tomographic studies throughout a period of time originating volumetric sets of image data that must be appropriately registered in order to track suspicious radiological findings. The structures or regions of interest may change their position or shape in CT exams acquired at different moments, due to postural, physiologic or pathologic changes, so, the exams should be registered before any follow-up information can be extracted. Postural mismatching throughout time is practically impossible to avoid being particularly evident when imaging is performed at the limiting spatial resolution. In this paper, we propose a method for intra-patient registration of pulmonary CT studies, to assist in the management of the oncological pathology. Our method takes advantage of prior segmentation work. In the first step, the pulmonary segmentation is performed where trachea and main bronchi are identified. Then, the registration method proceeds with a longitudinal alignment based on morphological features of the lungs, such as the position of the carina, the pulmonary areas, the centers of mass and the pulmonary trans-axial principal axis. The final step corresponds to the trans-axial registration of the corresponding pulmonary masked regions. This is accomplished by a pairwise sectional registration process driven by an iterative search of the affine transformation parameters leading to optimal similarity metrics. Results with several cases of intra-patient, intra-modality registration, up to 7 time points, show that this method provides accurate registration which is needed for quantitative tracking of lesions and the development of image fusion strategies that may effectively assist the follow-up process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Teaching 3D modelling and visualization using VTK",
        "doc_scopus_id": "45349097503",
        "doc_doi": "10.1016/j.cag.2008.01.005",
        "doc_eid": "2-s2.0-45349097503",
        "doc_date": "2008-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "3D modelling",
            "Computer engineering",
            "Computer graphics education",
            "Elective course",
            "Visualization application",
            "Visualization toolkits"
        ],
        "doc_abstract": "In the last two years, we have been using the Visualization Toolkit (VTK) as a tool for teaching \"3D Modelling and Visualization\", an elective course offered to Computer Engineering students. Students start by using OpenGL and, afterwards, use VTK in half of their lab classes, in order to accomplish some tasks and acquire knowledge on its features and functionalities. They are also required to develop a visualization application based on VTK. We first present the motivation for using VTK and the main features of the \"3D Modelling and Visualization\" course. Afterwards, we describe some of the most successful projects developed by our students. Then, we globally analyse the effectiveness of using VTK, and present the results of a questionnaire handed out to the students who attended the course in the last semester. © 2008 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2008-01-26 2008-01-26 2011-01-20T07:10:55 S0097-8493(08)00008-3 S0097849308000083 10.1016/j.cag.2008.01.005 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20080601 20080630 2008 2008-01-26T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst pubtype ref alllist content subj ssids 0097-8493 00978493 32 32 3 3 Volume 32, Issue 3 8 363 370 363 370 200806 June 2008 2008-06-01 2008-06-30 2008 Education article fla Copyright © 2008 Elsevier Ltd. All rights reserved. TEACHING3DMODELLINGVISUALIZATIONUSINGVTK DIAS P 1 Introduction 2 3D modelling and visualization 3 Examples of visualization projects 3.1 Visualization of brain data from different modalities 3.1.1 Volumetric MRI and SPECT data 3.1.2 EEG data 3.1.3 Sources of electrical activity (dipoles) 3.2 Visualization of water pressure and velocity around a ship's hull 3.3 Visualization of acoustic data 3.3.1 2D sound wave propagation 3.3.2 3D acoustic pressure 3.4 Visualization of Oporto underground tunnels 3.5 Visualization of a football game in a VR environment 4 Using VTK: an assessment 4.1 Evaluation of the OpenGL and VTK projects 4.2 Questionnaire handed out in 2006/2007 4.3 Our own impression 5 Conclusion Acknowledgements References HITCHNER 2000 283 288 L BOUVIER 2002 603 608 D PAQUETTE 2005 245 255 E ANGEL 2006 E INTERACTIVECOMPUTERGRAPHICSATOPDOWNAPPROACHUSINGOPENGL ZARA 2006 105 112 J BAILEY 2007 524 531 M FOLEY 1994 J INTRODUCTIONCOMPUTERGRAPHICS HEARN 2004 D COMPUTERGRAPHICSOPENGL CUNNINGHAM 2006 S COMPUTERGRAPHICSPROGRAMMINGINOPENGLFORVISUALCOMMUNICATION FROST 2004 J LEARNSVGWEBGRAPHICSSTANDARD AMES 1997 A VRML20SOURCEBOOK SHREINER 2005 D OPENGLPROGRAMMINGGUIDE SELMAN 2002 D JAVA3DPROGRAMMING LUNA 2003 F INTRODUCTION3DGAMEPROGRAMMINGDIRECTX90 2003 VTKUSERSGUIDEVERSION42 DIASX2008X363 DIASX2008X363X370 DIASX2008X363XP DIASX2008X363X370XP item S0097-8493(08)00008-3 S0097849308000083 10.1016/j.cag.2008.01.005 271576 2011-02-04T12:58:14.641468-05:00 2008-06-01 2008-06-30 true 1511805 MAIN 8 69595 849 656 IMAGE-WEB-PDF 1 si2 246 13 43 si1 244 13 43 gr1 46996 263 561 gr1 4317 59 125 gr1 272162 699 1490 gr2 17776 207 278 gr2 4533 93 125 gr2 110247 918 1233 gr3 32111 213 486 gr3 3509 55 125 gr3 328726 943 2154 gr4 41541 202 278 gr4 7373 91 125 gr4 747753 895 1233 gr5 26656 233 278 gr5 3098 93 111 gr5 178390 1033 1233 gr6 42009 253 523 gr6 5266 61 125 gr6 326391 1122 2317 gr7 16123 216 278 gr7 4762 93 120 gr7 178739 960 1233 gr8 30886 190 522 gr8 3905 45 125 gr8 140773 504 1388 CAG 1832 S0097-8493(08)00008-3 10.1016/j.cag.2008.01.005 Elsevier Ltd Fig. 1 MRI and SPECT data with different opacity values (left), and coronal plane with corresponding slice (right). Fig. 2 Mapping EEG data on a model of a patient's head. Fig. 3 Two modes of dipole representation. Fig. 4 Water velocity and pressure around a ship's hull. Fig. 5 Sound wave at a given time ( t = 39 ): visualization of 2D propagation, as well as the zero-valued iso-lines. Fig. 6 Acoustic pressure in a cubical volume at a given time ( t = 29 ): visualization using orthogonal planes (left) and iso-surfaces (right). Fig. 7 Partial representation of the tunnel data using Gaussian splatter; the cutting plane is also visible. Fig. 8 User wearing the HMD (left), and his view of the game (right). Table 1 Evaluation of the OpenGL and VTK projects in 2005/2006 and 2006/2007 2005/2006 2006/2007 OpenGL VTK OpenGL VTK Weak 4 4 2 3 Average 4 4 6 7 Good 4 – 6 4 Very good 6 10 4 4 Number of students 18 18 18 18 Table 2 Analysis of the evaluation results of the OpenGL and VTK projects 2005/2006 2006/2007 Weak evaluation in both OpenGL and VTK 4 2 Average evaluation in both OpenGL and VTK 4 4 Significantly worse evaluation in VTK than in OpenGL – 6 Significantly better evaluation in VTK than in OpenGL 4 2 Excellent evaluation in both OpenGL and VTK 6 4 Number of students 18 18 Education Teaching 3D modelling and visualization using VTK Paulo Dias Joaquim Madeira ⁎ Beatriz Sousa Santos Department of Electronics, Telecommunications and Informatics/IEETA, University of Aveiro, Campus Universitário de Santiago, P-3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351234370504; fax: +351234381128. In the last two years, we have been using the Visualization Toolkit (VTK) as a tool for teaching “3D Modelling and Visualization”, an elective course offered to Computer Engineering students. Students start by using OpenGL and, afterwards, use VTK in half of their lab classes, in order to accomplish some tasks and acquire knowledge on its features and functionalities. They are also required to develop a visualization application based on VTK. We first present the motivation for using VTK and the main features of the “3D Modelling and Visualization” course. Afterwards, we describe some of the most successful projects developed by our students. Then, we globally analyse the effectiveness of using VTK, and present the results of a questionnaire handed out to the students who attended the course in the last semester. Keywords Computer graphics education Visualization Toolkit VTK 1 Introduction In the last few years we have been witnessing a discussion on how to better teach Computer Graphics (CG) to students in different areas [1–7]. In the past, a bottom-up approach was normally used, where students had to build all necessary code (almost) from scratch. Later, many educators switched to a top-down approach, based on using a higher-level API such as OpenGL [8] or Java 3D [9], with less relevance being given to raster-level algorithms. Others developed CG courses using VRML and with some emphasis on virtual reality (VR) [10]. More recently, courses designed to take advantage of available GPUs and shading languages have also started to be offered [11,12]. Although classical CG textbooks, based on the traditional bottom-up approach (e.g., [13]), do remain useful, advances in hardware, graphics libraries and more recent API-based CG textbooks [8,14–16] offer both students and educators the possibility of exploring advanced concepts and developing useful course projects, e.g., for data visualization. Two years ago, when planning our courses in the CG area for the first semester of 2005/2006, and given the interest shown by prospective students, we decided to offer: • “3D Modelling and Visualization” (3DMV), an elective course for students already having some CG background, presenting more advanced concepts, as well as offering the possibility of working with de facto CG and scientific visualization standard libraries. • Specialization courses in CG, visualization and geometric modelling, for M.Sc. students, which had to be accompanied by an integrated “CG Laboratory” course of 4h per week, providing them with additional hands-on experience. Given the audience, M.Sc. students that are supposed to be more independent than graduation students, those lab classes were organized as to introduce a series of CG tools and libraries, progressing from the SVG [17] and VRML [18] languages to OpenGL [19], and then to VTK [16]. Since we consider the top-down approach as more adequate for our students, we decided to combine OpenGL with the well-known Visualization Toolkit (VTK). Given that we would be teaching students already possessing some basic CG knowledge and, also important, having some object-oriented programming experience, we assigned part of the lab classes to VTK and required students to develop a visualization application based on that toolkit, as final assignment. In this way, they would have to use a higher-level API, in addition to the more traditional OpenGL, and would have to acquire some knowledge on data visualization, one of the most important application areas of CG. OpenGL [19] is the current de facto CG educational standard: it is open-source, with a low-level entry barrier and appropriate documentation, as well as offering multi-platform support. VTK [16] is also an open-source, freely available toolkit not only for 3D CG, but also for image processing and data visualization. It offers a higher-level of abstraction than other rendering libraries, such as OpenGL, making it much easier for the knowledgeable user to create graphics and visualization applications. In addition, it also offers a wide variety of data visualization algorithms (including scalar, vector, tensor and volumetric methods), as well as advanced modelling techniques (e.g., implicit modelling, polygon reduction, mesh smoothing and contouring). Other CG tools were also considered as possible choices: in spite of their relative ease of coding, JOGL [20], the Java wrapper library for OpenGL, and Java 3D [21], the scene graph-based 3D API for Java, were ruled out since our students have no Java background; Microsoft's Direct3D, part of the DirectX API [22], has a higher-level entry barrier and does not offer multi-platform support. We also did not want to limit our students to the use of a modelling language such as VRML or X3D; instead we deemed important for them to design and develop software applications. The first results of using VTK as a tool for teaching and applying CG, during the first semester of 2005/2006, were presented earlier at the EUROGRAPHICS 2006 Education Programme [23]. In this paper we report on the experience of teaching the 3DMV course during the last two years. In what follows, we first present the main features of the 3DMV course, and then describe some of the most successful projects developed by our students. Afterwards, we globally analyse the effectiveness of using VTK, and present the results of the questionnaire handed out to the students who attended the course in the last semester. Finally, we present some conclusions. 2 3D modelling and visualization The 3DMV course was introduced in the first semester of 2005/2006 as a fifth year elective for Computer Engineering students, corresponding to 2h of lectures and 2h of lab classes per week. The 3DMV course was offered as an answer to the strong interest shown by prospective students; moreover, we believe that students benefit from additional exposure to advanced topics in the CG area. Also, medical imaging and data visualization are long-established research areas in our department, with relevance in many graduation and post-graduation projects and/or R&D activities. Students attending the 3DMV course mostly have an introductory background on the fundamental CG concepts and some experience using VRML, obtained in their third year Human–Computer Interaction course, but usually have no experience in using any CG API. Therefore, the main topics addressed throughout the course are: (1) Review of CG fundamentals. (2) Introduction to OpenGL (lab). (3) Geometric modelling (polygonal meshes and free-form curves and surfaces). (4) Techniques conducing to higher realism (ray-tracing, radiosity, textures). (5) Introduction to volume visualization (surface extraction and direct volume rendering). (6) Introduction to VTK (lab). Regarding lab classes, the first half of the semester is dedicated to OpenGL and the second half to VTK. OpenGL is used to illustrate CG and geometric modelling concepts addressed during the lectures, and to provide the students with their first hands-on experience using a CG API, which facilitates the later transition to VTK. The fundamentals of VTK are introduced during lab classes and consolidated using a sequence of practical exercises developed for each class. The six lab classes used to introduce VTK encompass the following topics: (1) First examples, interactors, cameras and lighting: Compiler configuration; visualization of a simple cone; using interactors and different interaction techniques; using several cameras and light sources. (2) Actor properties, multiple actors and renderers, transformations, shading and textures: Modifying actor properties (colour, opacity, etc.); managing multiple actors and multiple renderers in the same window; using transformations to change location and orientation; applying different shading techniques and textures. (3) Observers and callbacks, glyphing and picking: Using callbacks and managing events; application examples of glyphing, picking and coordinate visualization. (4) Widgets, implicit functions, contouring and probing: Using widgets; definition and visualization of quadrics using contouring; probing a quadric with a plane and visualizing the resulting iso-lines. (5) Viewing of 2D images, visualization and clipping of polygonal data: Manipulation and viewing of 2D images; importing VRML polygonal data and simple manipulation of the resulting polydata objects. (6) Visualization of non-structured grids and volumetric data: Creation and manipulation of a simple non-structured grid; association of scalar information to the data; visualization and reslicing of medical data. In addition to the work carried out during their lab classes, each group of two students is required to develop a visualization application, corresponding to approximately 20h of after-class work. Most of the projects imply that students have to visualize data from different sources using VTK, and create tools and widgets using the library to provide additional visualization capabilities (such as slicing or probing). Several data sets have been proposed to our students, ranging from electromagnetic radiation data from antennas, to medical data (brain or lung imaging) and data from physical processes (e.g., water flow around a ship's hull or temperature values within an industrial oven). These data sets were obtained through contacts with other university departments, in order to detect data visualization needs. The main idea is to give the students data sets to visualize, which are important to final users, and thus increase their motivation. For some of the applications and data sets, visualization tools already existed, but did not seem to completely satisfy their final users. In fact, our colleagues showed great interest when interviewed on the issue, since they considered the possibility of influencing the design and directly participating in the specification of the application's features as an advantage, instead of being limited to the use of existing commercial application software with limited possibilities. 3 Examples of visualization projects We will now present some of the most successful visualization applications developed by students attending the 3DMV course in 2005/2006 and 2006/2007. The examples described encompass the application areas of medical imaging, physical simulation, acoustic data visualization, reverse engineering and VR. 3.1 Visualization of brain data from different modalities One of the most interesting projects consisted in developing an application to visualize, in an integrated way, data from different brain imaging and signal modalities, namely, magnetic resonance imaging (MRI) and single photon emission computed tomography (SPECT) data, electroencephalogram (EEG) and electrical dipole data. The latter two modalities provide time-varying data sets. The work was divided into three sub-tasks, each one allocated to two students. Given the common platform (VTK), each group implemented the visualization of a different type of data. The entire work was integrated into a single application, which allows the user to easily switch among different data and visualization methods. 3.1.1 Volumetric MRI and SPECT data One group of students had to represent, in the same window, previously registered MRI and SPECT data. This simultaneous visualization provides physicians with important information concerning the location and intensity of brain activity. A surface extracted from MRI data is presented, as well as red polygonal models representing SPECT data. The interface allows the user to activate and interactively locate up to three cutting-planes (axial, coronal and sagittal) to visualize cross-sections of the registered data (Fig. 1 ). 3.1.2 EEG data A second group of students was asked to represent the location of the electrodes placed on a patient's head, as well as the EEG signal values measuring brain activity, using a mesh model of a human head. Electrodes are represented as white spheres with an associated label. The colour value assigned to each mesh vertex is defined by the signal value at the closest electrode. This results in a final representation with different colours associated to the electrical potential variations on the patient's head (Fig. 2 ). Since temporal information is available, the user can also navigate through different acquisition times and observe the evolution of the EEG data. 3.1.3 Sources of electrical activity (dipoles) A third group of students had to represent the estimated location of the sources of electrical activity (dipoles) within the brain. The dipoles are represented as arrows within a surface model of a human head. Data are read from pre-processed files describing the sampling frequency, the location and the orientation of the dipoles. In addition to these features the application can also display different groups of dipoles using different colours. As for the EEG data, the user can also navigate forward or backwards in time. Given the large range of dipole magnitudes, two visualization modes are available. In the first, the length of each arrow encodes dipole magnitude; in the second, all arrows are shown with the same length, thus simplifying the analysis and detection of clusters and patterns (Fig. 3 ). 3.2 Visualization of water pressure and velocity around a ship's hull The goal of this project was to develop visualization tools to analyse the water flow around a ship's hull. The data consisted of the coordinates of sampled points (unstructured grid), as well as pressure and velocity values. Hedgehogs were used to represent velocity data and pressure was converted to a structured grid through splatting [16]. The final application gives the user the possibility to manipulate a cutting plane where the pressure data are displayed through colour mapping. A view of the final visualization is presented in Fig. 4 . 3.3 Visualization of acoustic data Another interesting set of projects consisted in developing applications to visualize the propagation of sound waves, given time-varying data sets containing the results of sound propagation simulations. 3.3.1 2D sound wave propagation This application allows the analysis of 2D data obtained from an acoustic modelling software, which simulates the interaction among neighbouring points, and thus the propagation of sound waves, resulting from the injection of a sound pulse at an arbitrary point of a given environment, using a numerical model based on finite differences in the time domain (FDTD). Two visualization modes are offered: for a given instant in time t, the sound wave values as well as the associated zero-valued iso-lines are displayed (see Fig. 5 ); as an alternative, the maximum values recorded so far, for time t, can also be represented. As usual, the user can navigate forward or backwards in time. 3.3.2 3D acoustic pressure The second application allows the analysis of data resulting from 3D acoustic simulations, namely pressure values and the frequency response for a given 3D environment. In both cases, 3D time-varying data are simultaneously visualized through two representations, using the same colour scale: one allows slicing through the model volume using two orthogonal plane widgets, the other shows the iso-surfaces (see Fig. 6 ). 3.4 Visualization of Oporto underground tunnels The goal of this project was to allow the visualization of point clouds, acquired with a 3D laser scanner, representing some tunnels of the city of Oporto underground network, in order to assess particular tunnel surface features. A visualization application was developed allowing the representation of the scanned data in various ways: as a point cloud, a 3D Delaunay triangulated surface or a Gaussian splattered volume. In all these representations the user can interact with the data using a plane widget to define appropriate tunnel cross-sections (see Fig. 7 ). 3.5 Visualization of a football game in a VR environment A different kind of project, footVR, was also developed: the goal was to visualize the dynamics of a football game using a VR environment under development at our laboratory. The application reads the log-file from a simulation of a robotic football game and allows the user to watch the game (robots/players are represented as simple triangular prisms) either in a desktop, by controlling the viewing angle (there is an option to follow automatically the ball), or in the VR environment, which includes a head mounted display (HMD) and a tracker. In the latter, the user's head motion is registered and the camera parameters updated accordingly, as shown in Fig. 8 . 4 Using VTK: an assessment Since it is not usual to choose VTK as a tool supporting 3DMV classes, we decided to assess the effectiveness of using this toolkit taking into account three different vantage points: (1) comparing the evaluation results of the OpenGL and VTK projects developed by our students; (2) analysing the answers given to a questionnaire handed out to the students in 2006/2007; and (3) collecting our impressions on the use of VTK, given the questions and reactions from the students during the VTK lab classes. 4.1 Evaluation of the OpenGL and VTK projects In addition to developing and implementing their 3DMV projects, one using OpenGL, the other using VTK, students were required to write a report describing the main steps of the work carried out, as well as present their work to their colleagues. Their projects were then individually evaluated by the authors (see Table 1 for the global results in the last two years). Instead of directly comparing the grading of individual projects, we performed a relative comparison and identified students having similar evaluation results in both projects, or performing significantly better or worse in VTK than in OpenGL (see Table 2 ). While, for 2005/2006, the results in Table 2 seem to validate our initial idea that using VTK would not be an excessive load for most students, and would be an excellent way of further motivating interested students, this is no longer true for the results obtained in 2006/2007: less students had excellent results in both APIs or performed significantly better when using VTK; moreover, one-third of the students performed significantly worse when using VTK, than when using OpenGL. Clearly, the results in Table 2 seem to be inconclusive. Although a possible explanation for the worse results obtained in 2006/2007 with VTK might be the excessive workload, during the second half of the semester and due to other courses. Further work is needed to better identify global student shortcomings, as well as particular questions and needs regarding the use of VTK. 4.2 Questionnaire handed out in 2006/2007 A questionnaire was handed out to the students at the end of their 3DMV course: the main goal was to gather their own opinion regarding the work carried out when developing the two projects, as well as additional information on their use of OpenGL and VTK and on their experience previous to the course. The questionnaire was anonymous and we had 16 (out of 18) respondents: 13 declared having average or above average programming experience, and 13 had never used a CG programming API before. We globally analysed their grades in the programming and algorithms and data structures courses they had attended before, and concluded that less than half of them could really claim having average or above average programming experience. Regarding the OpenGL project, 14 students considered themselves satisfied with the applications they had developed, and 11 classified their work as being above average. This overall evaluation is quite similar to our own grading of their work. Regarding the VTK project, the main conclusions that can be extracted from the answers to the questionnaire are: • Thirteen students considered themselves satisfied with the applications they had developed, and 11 classified their work as being above average. • Although there is no significative difference, students globally dedicated slightly less time to the VTK project than to the OpenGL project. • Although there is no significative difference, students globally declared having had slightly less difficulty in developing their VTK project than their OpenGL project. • Students considered that, on average, the information available for VTK was mostly insufficient, and poorer than the information available for OpenGL. Regarding our own evaluation of the work carried out by the students, their answers to the questionnaire revealed that most of them had expected better grades. This was probably due to VTK allowing them to develop applications providing visually pleasing results with less work, when compared to OpenGL. We were quite surprised to find out that students seemed to have dedicated slightly less time to their VTK projects, and found it slightly less difficult, in comparison to the OpenGL projects. In our opinion, the VTK projects are more demanding and would have required more time and effort for the students to attain above than average results. Such an attitude by the 2006/2007 students might also explain why the global evaluation in Table 2 is worse than that for 2005/2006. 4.3 Our own impression In spite of the results in Table 2, our overall evaluation on the use of VTK is encouraging. We were positively surprised by the quality of the visualization applications developed by some of our students, since the time allocated to the final VTK projects was relatively reduced. Using VTK allowed us to propose challenging and motivating tasks, and students were able to develop relatively complex applications in a short time. This was certainly rewarding for most students. For the students, the object-oriented structure of VTK and its modularity were also important advantages. However, many students complained about the lack of appropriate documentation to help them use VTK. The available documentation is very often insufficient to clearly understand the features of the classes used, and examples are missing for many functions. Even with the help of the user's guide [24], it is often difficult to understand at first how VTK classes behave: this is certainly a strong limitation of the toolkit, which does not recommend its use by students with less programming experience or reduced knowledge of the object-oriented paradigm. In some way, using VTK can even be frustrating for a student, since final solutions to some programming or development difficulties are often compact (a few lines of code), but difficult to attain. A possible way to mitigate this problem might consist in providing the students with a set of additional code examples. The abovementioned shortcomings of VTK force students to a somewhat important effort, during their first contact with the toolkit, in order to overcome first difficulties. For students with low motivation this was a major drawback, and a few did not succeed in developing satisfactory work. Nevertheless, most students particularly appreciated the use of a higher-level tool as VTK, which allows developing working prototypes and provides some degree of interaction and appropriate visualization functionalities. Some students were also asked to write a short paper describing the main features of their work, to be published in the internal journal of our department. Despite the fact this additional work was asked for after the conclusion of the semester, almost all of them agreed. This exercise was, after all, a nice introduction to more challenging research projects that might be proposed to some of them later. 5 Conclusion We have presented our experience regarding the use of VTK, in the last two years, in the context of the elective 3DMV course at the University of Aveiro. In spite of the global results in 2006/2007 being somewhat poorer than expected, when compared to 2005/2006, our overall evaluation on the use of VTK is encouraging. Using OpenGL in the first lab classes provides a valuable first step for those students with no previous CG programming experience. Then, the object-oriented structure of VTK and its modularity, as well as the visualization and interaction functionalities available, are of great advantage for most students, who are able to develop complex visualization applications in a relatively short time. However, it is also clear for us that the learning-curve of VTK might delay or even prevent the progress of some students, and we still have to devise appropriate strategies to mitigate this problem. Since we intend to keep using VTK in our CG courses, both at graduate and post-graduate level, this will be our next challenge. Acknowledgements First of all, we wish to thank all the students who developed the visualization applications presented here. A word of gratitude goes to our colleagues: José Maria Fernandes, José Rocha Pereira, Filipe Teixeira-Dias, Guilherme Campos, Augusto Silva and José Silvestre Silva, who provided us with some of the data sets used in the various assignments. Thanks are also due to the 3D laser digitalization company ARTESCAN, for providing us with the Oporto underground tunnels data. We thank the anonymous referees for their comments and suggestions and Samuel Silva for his help in preparing the final version of this paper. References [1] Grissom S, Bresenham J, Kubitz B, Owen S, Schweitzer D. Approaches to teaching Computer Graphics. In: Proceedings of SIGCSE 1995, Nashville, TN, 1995. p. 382–3. [2] L.E. Hitchner H.A. Sowizral Adapting Computer Graphics curricula to changes in graphics Computers & Graphics 24 2000 283 288 [3] D.J. Bouvier From pixels to scene graphs in introductory Computer Graphics courses Computers & Graphics 26 2002 603 608 [4] E. Paquette Computer Graphics education in different curricula: analysis and proposal for courses Computers & Graphics 29 2005 245 255 [5] Angel E, Cunningham S, Shirley P, Sung K. Teaching Computer Graphics without raster-level algorithms. In: Proceedings of SIGCSE 2006, Houston, TX, 2006. p. 266–7. [6] McCracken CR. Issues in Computer Graphics education. In: Proceedings of SIGGRAPH 2006 Educators Program, Boston, MA, 2006. [7] Sung K, Shirley P, Rosenberg BR. Experiencing aspects of games programming in an introductory Computer Graphics class. In: Proceedings of SIGCSE 2007, Covington, Kent, 2007. p. 249–53. [8] E. Angel Interactive Computer Graphics: a top-down approach using OpenGL 4th ed. 2006 Addison-Wesley Reading, MA [9] Tori R, Bernardes Jr JL, Nakamura R. Teaching introductory Computer Graphics using Java 3D, games and customized software: a Brazilian experience. In: Proceedings of SIGGRAPH 2006 Educators Program, Boston, MA, 2006. [10] J. Zara Virtual reality course—a natural enrichment of Computer Graphics classes Computer Graphics Forum 25 2006 105 112 [11] M. Bailey Teaching OpenGL shaders: hands-on, interactive, and immediate feedback Computers & Graphics 31 2007 524 531 [12] Talton JO, Fitzpatrick D. Teaching graphics with the OpenGL shading language. In: Proceedings of SIGCSE 2007, Covington, Kent, 2007. p. 259–63. [13] J. Foley A. van Dam S. Feiner J. Hughes R. Phillips Introduction to Computer Graphics 1994 Addison-Wesley Reading, MA [14] D. Hearn M.P. Baker Computer Graphics with OpenGL 3rd ed. 2004 Prentice-Hall Englewood Cliffs, NJ [15] S. Cunningham Computer Graphics: programming in OpenGL for visual communication 2006 Prentice-Hall Englewood Cliffs, NJ [16] Schroeder W, Martin K, Lorensen B. The Visualization Toolkit—an object oriented approach to 3D graphics, 4th ed. Kitware; 2006. [17] J. Frost S. Goessner M. Hirtzler Learn SVG: the web graphics standard 2004 Self Publishing [18] A.L. Ames D.R. Nadeau J.L. Moreland The VRML 2.0 sourcebook 2nd ed. 1997 Wiley New York [19] D. Shreiner M. Woo J. Neider T. Davis OpenGL programming guide 5th ed. 2005 Addison-Wesley Reading, MA [20] Davis G. Learning Java bindings for OpenGL (JOGL). Author House; 2004. [21] D. Selman Java 3D programming 2002 Manning Publications [22] F.D. Luna Introduction to 3D game programming with DirectX 9.0 2003 Wordware Publishing [23] Dias P, Madeira J, Sousa Santos B. Using VTK as a tool for teaching and applying Computer Graphics. In: Proceedings of EUROGRAPHICS 2006 Education Programme, Vienna, Austria, 2006. p. 61–7. [24] Kitware Inc The VTK user's guide—version 4.2 2003 Kitware "
    },
    {
        "doc_title": "Perceived quality assessment of polygonal meshes using observer studies: A new extended protocol",
        "doc_scopus_id": "41149133313",
        "doc_doi": "10.1117/12.766527",
        "doc_eid": "2-s2.0-41149133313",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Mesh quality",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "The complexity of a polygonal mesh is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, only a few observer studies are reported comparing the perceived quality of the simplified meshes, and it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting mesh, as perceived by the final users. Similar issues occur regarding other mesh processing methods such as smoothing. Mesh quality indices are the obvious less costly alternative to user studies, but it is also not clear how they relate to perceived quality, and which indices best describe the users behavior. This paper describes on going work concerning the evaluation of perceived quality of polygonal meshes using observer studies, while looking for a quality index which estimates user performance. In particular, given some results obtained in previous studies, a new experimental protocol was designed and a study involving 55 users was carried out, which allowed their validation, as well as further insight regarding mesh quality, as perceived by human observers. © 2008 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using task analysis to improve the requirements elicitation in health information system",
        "doc_scopus_id": "57649224764",
        "doc_doi": "10.1109/IEMBS.2007.4353127",
        "doc_eid": "2-s2.0-57649224764",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Health information system",
            "Task analysis"
        ],
        "doc_abstract": "This paper describes the application of task analysis within the design process of a Web-based information system for managing clinical information in hemophilia care, in order to improve the requirements elicitation and, consequently, to validate the domain model obtained in a previous phase of the design process (system analysis). The use of task analysis in this case proved to be a practical and efficient way to improve the requirements engineering process by involving users in the design process. ©2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "There is more to color scales than meets the eye: A review on the use of color in visualization",
        "doc_scopus_id": "35348918980",
        "doc_doi": "10.1109/IV.2007.113",
        "doc_eid": "2-s2.0-35348918980",
        "doc_date": "2007-10-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Color mapping",
            "Color scale selection"
        ],
        "doc_abstract": "The appropriate use of color in Visualization is a very important subject. The choice of the proper color scale to use with a particular data set is not just a matter of choosing the prettiest representation. Throughout the years researchers have studied this subject and managed to propose guidelines which help users along the process of color scale selection. This article presents a brief overview on the subject focusing on the desired properties for color scales, the guidelines that should drive their choice, the advantages of applying those guidelines, the experimental research work on the field, and the tools proposed to help non-expert users. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preliminary usability evaluation of PolyMeCo: A visualization based tool for mesh analysis and comparison",
        "doc_scopus_id": "35048822035",
        "doc_doi": "10.1109/GMAI.2007.27",
        "doc_eid": "2-s2.0-35048822035",
        "doc_date": "2007-10-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Polygonal meshes",
            "Usability evaluation",
            "Visualization based tool"
        ],
        "doc_abstract": "An overall description of the methods used and the results obtained in the on-going evaluation of PolyMeCo - a mesh analysis and comparison tool - is presented. We are trying to evaluate some aspects of both the user interface and the visualization techniques implemented. Heuristic evaluation, observation and querying techniques were used and produced encouraging preliminary results, which provided new ideas, as well as information, that will inform the development of a more usable version of PolyMeCo, including new functionality. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Usability in virtual and augmented environments: A qualitative and quantitative study",
        "doc_scopus_id": "34548209132",
        "doc_doi": "10.1117/12.703878",
        "doc_eid": "2-s2.0-34548209132",
        "doc_date": "2007-08-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Augmented Reality systems",
            "Head-Mounted Display",
            "HMD visualization",
            "Usability evaluation"
        ],
        "doc_abstract": "Virtual and Augmented Reality are developing rapidly: there is a multitude of environments and experiments in several laboratories using from simple HMD (Head-Mounted Display) visualization to more complex and expensive 6-wall projection CAVEs, and other systems. Still, there is not yet a clear emerging technology in this area, nor commercial applications based on such a technology are used in large scale. In addition to the fact that this is a relatively recent technology, there is little work to validate the utility and usability of Virtual and Augmented Reality environments when compared with the traditional desktop set-up. However, usability evaluation is crucial in order to design better systems that respond to the users' needs, as well as for identifying applications that might really gain from the use of such technologies. This paper presents a preliminary usability evaluation of a low-cost Virtual and Augmented Reality environment under development at the University of Aveiro, Portugal. The objective is to assess the difference between a traditional desktop set-up and a Virtual/Augmented Reality system based on a stereo HMD. Two different studies were performed: the first one was qualitative and some feedback was obtained from domain experts who used an Augmented Reality set-up as well as a desktop in different data visualization scenarios. The second study consisted in a controlled experiment meant to compare users' performances in a gaming scenario in a Virtual Reality environment and a desktop. The overall conclusion is that these technologies still have to overcome some hardware problems. However, for short periods of time and specific applications, Virtual and Augmented Reality seems to be a valid alternative since HMD interaction is intuitive and natural. © 2007 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using clustering analysis in a capacitated location-routing problem",
        "doc_scopus_id": "33845918053",
        "doc_doi": "10.1016/j.ejor.2005.06.074",
        "doc_eid": "2-s2.0-33845918053",
        "doc_date": "2007-06-16",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Cluster analysis",
            "Combinatorial optimization",
            "Location routing"
        ],
        "doc_abstract": "The location routing problem (LRP) appears as a combination of two difficult problems: the facility location problem (FLP) and the vehicle routing problem (VRP). In this work, we consider a discrete LRP with two levels: a set of potential capacitated distribution centres (DC) and a set of ordered customers. In our problem we intend to determine the set of installed DCs as well as the distribution routes (starting and ending at the DC). The problem is also constrained with capacities on the vehicles. Moreover, there is a homogeneous fleet of vehicles, carrying a single product and each customer is visited just once. As an objective we intend to minimize the routing and location costs. Several authors have integrated cluster analysis procedures in heuristics for LRPs. As a contribution to this direction, in this work several hierarchical and non-hierarchical clustering techniques (with several proximity functions) are integrated in a sequential heuristic algorithm for the above mentioned LRP model. All the versions obtained using different grouping procedures were tested on a large number of instances (adapted from data in the literature) and the results were compared so as to obtain some guidelines concerning the choice of a suitable clustering technique. © 2006.",
        "available": true,
        "clean_text": "serial JL 271700 291210 291692 291715 291813 291814 291817 291871 31 European Journal of Operational Research EUROPEANJOURNALOPERATIONALRESEARCH 2006-05-03 2011-01-13 2010-11-17T20:28:36 S0377-2217(06)00078-6 S0377221706000786 10.1016/j.ejor.2005.06.074 S300 S300.1 FULL-TEXT 2015-05-14T07:48:30.363566-04:00 0 0 20070616 2007 2006-05-03T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0377-2217 03772217 179 179 3 3 Volume 179, Issue 3 27 968 977 968 977 20070616 16 June 2007 2007-06-16 2007 Feature Cluster: Advances in Location Analysis Guest Editors: M. Dimopoulou and I. Giannikos article fla Copyright © 2006 Published by Elsevier B.V. USINGCLUSTERINGANALYSISINACAPACITATEDLOCATIONROUTINGPROBLEM BARRETO S 1 Introduction 2 A capacitated location-routing problem (CLRP) 3 A cluster analysis approach 4 Proximity measures among groups 5 A cluster analysis based heuristic 5.1 One phase hierarchical method (V1) 5.2 Two phase hierarchical method (V2) 5.2.1 Relative proximity measure 5.3 Direct assignment non-hierarchical method (V3) 5.4 Sequential assignment non-hierarchical method (V4) 6 Computational tests 7 Conclusions Acknowledgement References ANDERBERG 1973 M CLUSTERANALYSISFORAPPLICATIONS BALAKRISHNAN 1987 35 61 A BAYNE 1980 51 62 C BODIN 1975 11 29 L BRAMEL 1997 J LOGICLOGISTICS BRANCO 1990 86 95 I CHRISTOFIDES 1969 309 318 N CLARK 1964 568 581 G CULLEN 1981 125 143 F DANTZIG 1959 80 91 G DASKIN 1995 M NETWORKDISCRETELOCATIONMODELSALGORITHMSAPPLICATIONS EDELBROCK 1980 299 318 C EVERITT 1993 B CLUSTERANALYSIS FUKUNAGA 1978 176 181 K GASKELL 1967 281 295 T GOLDEN 1980 475 496 R GOWER 1985 397 405 J GOWER 1969 54 64 J JAIN 1988 A ALGORITHMSFORCLUSTERINGDATA JOHNSON 1967 241 254 S KAUFMAN 1990 L FINDINGGOUPSINDATAINTRODUCTIONCLUSTERANALYSIS KLOSE 1995 A USINGCLUSTERINGMETHODSINPROBLEMSCOMBINEDLOCATIONROUTINGOPERATIONSRESEARCHPROCEEDINGS KUEHN 1963 643 666 A MADSEN 1983 295 301 O MARANZANA 1963 129 135 F MILLIGAN 1985 97 109 G MIN 1989 377 386 H MIN 1996 235 263 H MIN 1992 259 288 H ROMESBURG 1984 H CLUSTERANALYSISFORRESEARCHERS SRIVASTAVA 1993 497 506 R SRIVASTAVA 1990 427 435 R BARRETOX2007X968 BARRETOX2007X968X977 BARRETOX2007X968XS BARRETOX2007X968X977XS item S0377-2217(06)00078-6 S0377221706000786 10.1016/j.ejor.2005.06.074 271700 2010-12-25T07:24:51.810909-05:00 2007-06-16 true 444383 MAIN 10 59864 849 656 IMAGE-WEB-PDF 1 si5 1430 41 219 si4 644 40 98 si3 999 22 184 si2 2040 41 360 si1 1032 30 227 fx1 13779 59 338 fx1 1794 22 125 gr7 22969 158 326 gr7 3432 61 125 gr4 32774 288 437 gr4 2597 82 125 gr6 24152 177 328 gr6 3098 67 125 gr5 27320 201 333 gr5 3281 75 125 gr1 19709 266 317 gr1 1994 93 111 gr2 45455 474 338 gr2 2145 93 66 gr3a 21113 275 265 gr3a 1976 93 90 gr3b 25224 275 266 gr3b 2285 93 90 gr3c 24586 274 265 gr3c 2222 93 90 gr3d 26568 274 265 gr3d 2250 93 90 EOR 7357 S0377-2217(06)00078-6 10.1016/j.ejor.2005.06.074 Fig. 1 Location of the 150 biggest European cities, where the squares represent potential DCs. Fig. 2 Graphical and analytical representation of some proximity measures. Fig. 3a Step 1. Capacity limited group construction. Fig. 3b Step 2. Route design in each group. Cost=526. Fig. 3c Step 3. Route improvement. Cost=463. Fig. 3d Step 4. DC location and route assignment to the open DCs. Cost=614. Fig. 4 Heuristic versions and proximity measures. Fig. 5 Success rates obtained for the four versions of the heuristic (V1, V2, V3, V4) and the six proximity measures (SL, CL, GA, CT, WA, SA). Fig. 6 Average results after steps 2, 3 and 4. Fig. 7 CLRP average results (after step 4). Table 1 Instances lower and upper bounds CLRP instance Vehicles capacity LB UB Gap 1 Gaskell67—21×5 6000 424.9∗ 435.9 2.59 2 Gaskell67—22×5 4500 585.1∗ 591.5 1.09 3 Gaskell67—29×5 4500 512.1∗ 512.1∗ 0.00 4 Gaskell67—32×5 8000 556.5 571.7 2.73 5 Gaskell67—32×5 11,000 504.3∗ 511.4 1.41 6 Gaskell67—36×5 250 460.4∗ 470.7 2.24 7 Christofides69—50×5 160 549.4 582.7 6.06 8 Christofides69—75×10 140 744.7 886.3 19.01 9 Christofides69—100×10 200 788.6 889.4 12.78 10 Perl83—12×2 140 204.0∗ 204.0∗ 0.00 11 Perl83—55×15 120 1074.8 1136.2 5.71 12 Perl83—85×7 160 1568.1 1656.9 5.66 13 Perl83—318×4 25,000 – 580,680.2 – 14 Perl83—318×4 8000 – 747,619.0 – 15 Min92—27×5 2500 3062.0∗ 3062.0∗ 0.00 16 Min92—134×8 850 – 6238.0 – 17 Daskin95—88×8 9,000,000 356.4 384.9 8.00 18 Daskin95—150×10 8,000,000 43,938.6 46,642.7 6.15 19 Or76—117×14 150 12,048.4 12,474.2 3.53 Average gap 4.81 Median 3.13 Using clustering analysis in a capacitated location-routing problem Sérgio Barreto a e ⁎ Carlos Ferreira b e José Paixão c e Beatriz Sousa Santos d a Higher Institute of Accounting and Administration/ISCA, University of Aveiro, Aveiro, Portugal b Department of Economics, Management and Industrial Engineering, University of Aveiro, Aveiro, Portugal c Department of Statistics and Operations Research, University of Lisbon, Lisbon, Portugal d Department of Electronics and Telecommunications/IEETA, University of Aveiro, Aveiro, Portugal e Operational Research Centre, University of Lisbon, Lisbon, Portugal ⁎ Corresponding author. Tel.: +351 234 380 110; fax: +351 234 380 111. The location routing problem (LRP) appears as a combination of two difficult problems: the facility location problem (FLP) and the vehicle routing problem (VRP). In this work, we consider a discrete LRP with two levels: a set of potential capacitated distribution centres (DC) and a set of ordered customers. In our problem we intend to determine the set of installed DCs as well as the distribution routes (starting and ending at the DC). The problem is also constrained with capacities on the vehicles. Moreover, there is a homogeneous fleet of vehicles, carrying a single product and each customer is visited just once. As an objective we intend to minimize the routing and location costs. Several authors have integrated cluster analysis procedures in heuristics for LRPs. As a contribution to this direction, in this work several hierarchical and non-hierarchical clustering techniques (with several proximity functions) are integrated in a sequential heuristic algorithm for the above mentioned LRP model. All the versions obtained using different grouping procedures were tested on a large number of instances (adapted from data in the literature) and the results were compared so as to obtain some guidelines concerning the choice of a suitable clustering technique. Keywords Combinatorial optimization Distribution Heuristics Cluster analysis Location-routing 1 Introduction In an ever more demanding society, having customers less and less willing to wait for the products they want to acquire, decisions concerning the location of distribution centres (DC) and tracing of distribution routes are a central problem, having implications on the complete supply chain (Bramel and Simchi-Levi, 1997). Nowadays, even small and medium enterprises should be aware that their future success may depend on the location–distribution decisions and recognise the need for flexible and efficient, as well as reliable, decision methods (White Paper, 2001). In the last four decades (Maranzana, 1963), the investigation on modelling and resolution of location-routing problems (LRP) has advanced and produced a large body of literature (Barreto et al., 2003b), allowing a complete and detailed view of this problem and its characteristics. However, a deficit in theoretical investigation at the simplest LRP level still exists. This deficit hinders a better understanding of its properties and the development of new approaches; furthermore, it holds back the creation of the solid foundations needed to support complex applications. As an attempt to contribute to this investigation, we present in this paper a study on a location routing problem, common in many organizations, which has two levels (customers and distribution centres) and vehicles with a limited capacity; we will call it a capacitated location-routing problem (CLRP). 2 A capacitated location-routing problem (CLRP) Let a set of customers and potential distribution centres (DCs) be represented by points on the plane. Each customer has a certain demand (units of load); the location (installation) cost of each DC is known, as well as the unitary cost of distribution (function of covered distance). The vehicles (routes) and the potential DCs have a certain capacity (units of load). The purpose of this CLRP is, then, to choose the DCs that must be opened (installed) and to draw the routes from these DCs to the customers, having as an objective the minimization of the total cost (location and distribution costs). Solving exactly a CLRP is a difficult task, since this type of problem is NP-complete (Srivastava, 1986). Thus, one way to “solve efficiently” large problems is to look for heuristics, which have some advantages such as: (i) getting “good solutions” in acceptable time; (ii) producing several “good solutions” allowing the user to choose the most suitable according to the scenario; (iii) being easy to understand, modify and implement, they allow to deal with larger problems. 3 A cluster analysis approach Cluster analysis (Anderberg, 1973) studies the division of entities (as objects or individuals) in groups based in one or several of their characteristics. An important issue is the notion of group; according to Jain and Dubes (1988): “Cluster may be described as connected regions of a multi-dimensional space containing a relatively high density of points, separated from other such regions by a region containing a relatively low density of points”. This definition of group is an excellent reason to use cluster analysis in the resolution of LRPs. Recognizing groups of customers can be a good start to obtain good LRP solutions. Let us consider, as an example, Fig. 1 showing the location of the 150 biggest European cities (Daskin, 1995), where the squares represent potential DCs. Observing this figure, it seems natural that a CLRP related to these cities will tend to construct routes encompassing high density regions (population agglomerations). The identification by visual inspection of some groups of cities that can form good bases for the distribution routes (for example, the Canaries Islands, the Iberian Peninsula, the British Islands, the north of the Europe or Italy and Greece) is not difficult. However, what is relatively easy to visualize is more difficult to carry out. Nevertheless, cluster analysis provides a vast set of grouping methodologies that can be used in heuristic approaches to the CLRP. The potential of cluster analysis for the resolution of LRPs (or problems directly related, as the facility location problem (FLP) and vehicle routing problem (VRP)) has also been recognized by other authors. Dantzig and Ramser (1959) were of the first ones to mention the identification of groups of points in the multiple travelling salesman problem (MTSP), stating that “One would look for “clusters of points” and determine by trial and error the order in which they should be traversed, taking care that no loop crosses itself”. Concerning location problems, Kuehn and Hamburger (1963), also recognized the benefits of grouping (nearby customers) when they say that “… in many cases a priori judgements can be made that customers in certain geographical regions will not be serviced from potential warehouses in other regions (…) customers can frequently be aggregated into concentrations of demand (for example, metropolitan chain grocery and wholesaler warehouses) because of geographical proximity”. This is probably one of the first explicit references on the interest of grouping customers. Fukunaga and Short (1978) used grouping algorithms in location problems as well. The well known Clarke–Wright algorithm (Clark and Wright, 1964) uses grouping, with excellent results, to implement an original saving procedure to determine the proximity matrix. Bodin (1975) mentions the construction of groups as a technique to approach Routing and Scheduling Vehicles Problems. According to Madsen (1983), also Bednar and Strohmeier solve VRPs supported by grouping procedures. Cullen et al. (1981) make use of simple grouping procedures in the development of a heuristic for VRP. Branco and Coelho (1990) also use grouping processes in algorithms for a specific LRP (Hamiltonian p-median problem). Min (1989) explicitly includes cluster analysis in algorithms for VRP with distribution and collection. He uses the group average proximity measure which, according to Romesburg (1984), has advantages over the single linkage or complete linkage measures. In another paper, Min et al. (1992) use the ward measure of proximity due to its tendency to form groups of equal dimension and the good results reported in some studies. Srivastava (1993) also considers a LRP heuristic based on grouping procedures, using the minimum spanning tree to determine a set of groups, which he refines later by means of a 1-optimal method. Bruns and Klose (1995) and Klose (1995) make explicit reference to the integration of the cluster analysis in a heuristic procedure for the LRP. They employ hierarchical grouping techniques with single and complete linkage, group average and ward proximity measures. Min (1996) considers as well a sequential method for a LRP with capacity that starts by grouping customers through a hierarchical method and uses the ward proximity measure. Actually, although one can find in the literature a significant number of attempts to integrate grouping techniques in algorithms for the LRP, the same is not true when looking for comparative studies among the several grouping techniques in order to evaluate their real capacities. 4 Proximity measures among groups Cluster analysis considers different grouping methods accordingly to the type of variables (qualitative, quantitative, binary or mixed). In this paper we have quantitative variables (co-ordinates of customers and DCs on the plane), and the classification will use hierarchical and non-hierarchical methods (or partition methods). Several measures have been proposed to determine the proximity between points on the plane (Anderberg, 1973; Gower, 1985), however the most common for quantitative data is the Euclidean metric that determines the proximity between the points I =(x i , y i ) and J =(x j , y j ) as d ( I , J ) = ( x i - x j ) 2 + ( y i - y j ) 2 . Based on this concept of proximity between two elements, some measures of proximity among groups have been proposed: single linkage (nearest neighbour), complete linkage (farthest neighbour), group average, centroid, ward and saving. With the exception of the saving measure, proposed in this study, these are some of the most popular in cluster analysis literature (Anderberg, 1973; Jain and Dubes, 1988; Kaufman and Rousseeum, 1990; Everitt, 1993). Fig. 2 shows a graphical and analytical representation of these proximity measures. According to Jain and Dubes (1988), Jardine and Sibson considered that the single linkage is the only measure satisfying all the mathematical criteria they had defined. However, they concluded that the use of this measure usually produces worse results, when compared to the other measures. An important element on the centroid and ward proximity measures is the centroid (gravity centre) of each group. In our case for two groups, A and B, we will have m A and m B defined as m A = ∑ I ∈ A x i | A | , ∑ I ∈ A y i | A | , m B = ∑ J ∈ B x j | B | , ∑ J ∈ B y j | B | . The ward proximity measure (Kaufman and Rousseeum, 1990) also includes the sum square error of a group quantified as the sum of the square deviation of the elements of the group to the centroid, SEQ ( A ) = ∑ I ∈ A [ d ( I , m A ) ] 2 . While this measure is widely used, the mean square error may prove inadequate to investigate the data structure, due to its tendency to form groups of equal size (Milligan and Schilling, 1985; Min, 1996). On the other hand, this can be an advantage to solve the LRP. Golden and Meehl (1980) claim that group average, complete linkage and ward produce better results than single linkage and centroid for a specific data set. Bayne et al. (1980) affirm that ward and complete linkage are preferable to the centroid and group average measures. Some authors have attempted to demonstrate the superiority of one or another proximity measure. Despite these efforts, we still do not have a clear idea on the general or specific potentialities of each one. The lack of agreement among the several studies reinforces the idea that there is not a measure which is adequate for all the applications and cases. In each case, the measure must be carefully chosen and, probably, only after tests with several measures, definitive conclusions can be drawn (Edelbrock and McLaughlin, 1980). 5 A cluster analysis based heuristic Heuristic methods for resolution of LRPs can be classified as sequential and iterative. The former solve sequentially the location and the vehicle routing problems. The latter solve the same problems iteratively, while it is possible to improve the solution. According to Min (1996) and Balakrishnan et al. (1987), for limited capacity of vehicles and significant fixed cost of distribution centres, the sequential methods are preferable from a computational point of view. Moreover, Srivastava and Benton (1990) conclude that the error associated to the obtained solutions is perfectly acceptable. As a consequence, we propose for the CLRP a sequential heuristic of the type distribution-first, location-second, which we present below. A sequential heuristic for the CLRP Input: Co-ordinates of N ={1,2,…, n} customers on the plane with demand e i : i ∈ N. Co-ordinates of P ={n +1, n +2,…, n + p} potential DCs with capacity u k and location cost f k : k ∈ P. w =vehicle capacity. Output: Vehicles routes based in the DCs. Step 1. Construct groups of customers with a capacity limit. Step 2. Determine the distribution route in each customer group. Step 3. Improve the routes. Step 4. Locate the DCs and assign the routes to them. Fig. 3 shows the results of each step of the heuristic applied to an instance of a CLRP with 50 customers and 5 potential DC (adapted from Christofides and Eilon (1969)). In step 2, whenever the group has 40 customers or less, the TSP routes are determined by an exact algorithm which solves the relaxation of the sub-cycles constraints with more than three customers. These constraints are introduced later if they are violated in this step. If the route integrates more than 40 customers, a two stages heuristic procedure is used. In the first stage, a feasible solution is obtained using a choice criterion of the farthest type and an insertion criterion of the saving type. In the second stage, the solution is improved through a 3-optimal local search procedure. In the improvement of the routes (step 3) a 3-optimal local search procedure is used, as the one proposed by Branco and Coelho (1990) for the Hamiltonian p-median problem. After step 3, each route collapses into one customer with a saving type DC assignment cost. Then, the single source capacitated location problem is solved, leading to a feasible solution of the CLRP. To implement step 1, four grouping methods are considered (two hierarchical and two non-hierarchical): 1. One-phase hierarchical method. 2. Two-phase hierarchical method. 3. Direct-assignment, non-hierarchical method. 4. Sequential-assignment, non-hierarchical method. For each of these methods, the six proximity measures presented in Fig. 2 are evaluated. The integration of these methods in step 1 leads to four versions of the proposed heuristic, V1, V2, V3 and V4 (Fig. 4 ). 5.1 One phase hierarchical method (V1) This is an agglomerative hierarchical method that performs an iterative merging of the nearest groups. There are several ways to implement agglomerative hierarchical methods, the most common ones being the spanning tree and the Johnson methods (Johnson, 1967). The former considers the formation of groups from the minimum spanning tree of a graph (Gower and Ross, 1969) having the disadvantage of supporting only the single linkage and complete linkage proximity measures. The later needs to store the triangular matrix of proximity between the groups; however it allows using all proximity measures showed in Fig. 2 and a similar algorithmic construction for the hierarchical and non-hierarchical cases. Beginning with groups consisting of only one customer, the hierarchical algorithm leads to the formation of one single group. In CLRPs the capacity limit avoids the consecutive joining of groups, acting as a natural stopping criterion on their final number. However, the construction of groups with limited capacity leads to a difficulty to avoid drawback related to the final part of the grouping process: the groups reaching their capacity limits prevent the merging of the near groups since this will exceed their capacity. For this reason the merging of far away groups often occurs (as it can be observed in Figs. 3a and 3b) producing the biased effect (Klose, 1995). 5.2 Two phase hierarchical method (V2) The two phase hierarchical method starts by applying the hierarchical method without capacity constraints, freely constructing a number of groups, thus preventing the undesirable biased effect. Knowing the demand of the customers (e i ) and the maximum vehicle capacity (w), the minimum number of vehicles (r) is determined as r = ∑ i ∈ N e i w . Next, the hierarchical grouping method is applied until there are only r groups. The lack of capacity constraints probably leads to the formation of groups that violate these constraints. In this case, the second phase is performed using a procedure that allows transferring customers from the groups exceeding capacity limits to other groups that can receive them without exceeding their capacity. Customer transfer is based on a relative proximity measure defined as follows. 5.2.1 Relative proximity measure Let a set G ={G 1, G 2,…, G r } include r groups on the plane. Let i ∈ G l ∈ G be a customer and Prox(i, G j ) be the proximity between customer i and group G j ∈ G. The proximity coefficient of customer i is defined as ρ prox ( i ) = min G j ∈ G ⧹ G l Prox ( i , G j ) Prox ( i , G l ) . This proximity coefficient is a relative measure, independent of the customer space distribution, which represents the degree of proximity to the next group. The numerator of the expression represents the external proximity and the denominator the internal proximity. In this second phase, customers having the least proximity coefficient are transferred, as long as the receiving group does not exceed its capacity. If, however, no customers can be transferred (and the violation of capacity still exists), the number of vehicles (r) is increased by one unit and the procedure restarts. 5.3 Direct assignment non-hierarchical method (V3) While the hierarchical methods begin with a set of groups with one element and, through a nested process, converge to one group, the non-hierarchical methods are devised to construct r groups, where r is known a priori or is determined as part of the method. In the direct assignment non-hierarchical method the minimum number of groups (r) is determined as in the two phase hierarchical method. Then, r customer sources (who will serve of first customer in each group) are established. To prevent the biased effect, the customer sources must be located on the boundary; therefore, they are chosen using a farthest neighbour proximity measure. The remaining customers are directly assigned to the group whose vertex source is next. 5.4 Sequential assignment non-hierarchical method (V4) In the previous method, capturing new customers to the group depends entirely on the customer source; the remaining elements of the group do not contribute for this gravitational action. In the sequential assignment non-hierarchical algorithm, the responsibility of capturing not yet assigned customers is shared by the entire group. That is why it is necessary to calculate, in all iterations, the proximity between the free customers and the modified groups. Customer sources are determined as in the previous method. 6 Computational tests To evaluate the four versions of the heuristic (V1, V2, V3, V4) and the six proximity measures (SL, CL, GA, CT, WA, SA) computational tests were carried out on 19 CLRP instances (see Table 1 ) obtained from the literature (Or, 1976; Perl, 1983) or adapted from data related with VRP (Gaskell, 1967; Christofides and Eilon, 1969; Min et al., 1992; Daskin, 1995). Data relative to the used instances are available in Barreto (2003) and the complete tables of results are available in Barreto (2004). To evaluate the performance of each version of the heuristic, it is essential to define “good performance” and decide which measures should be used. Naturally, a good heuristic produces good results (in this case low cost CLRP solutions). However, a heuristic may provide a very good solution for a certain instance and have a poor performance in others. In this case we cannot say that it’s a “good” heuristic. Thus, the evaluation of a heuristic must have into account its capacity to generate frequently “good” solutions. In short, a good heuristic must generate, for most of the instances, “good” solutions; still, it may not be able to find the best solution. Three types of success rates were considered: (i) number of generated solutions equal to the best known solutions (BKS); (ii) number of solutions within a 2% tolerance from the BKS; (iii) number of solutions within a 5% tolerance from the BKS. Fig. 5 shows the success rates for all the versions using all proximity measures. V2 heuristic, using the complete linkage proximity measure, yielded the best success rates. With a tolerance of 2%, the best results were obtained also by V2, but now for the group average proximity measure. V1 and V3 produced more solutions within a 5% tolerance. V3 version using SL, CL, GA, CT and SA proximity measures produced equivalent success rates. A standardization of the data (customers and DC) for the square [0,500]2 was performed in order to get comparable results for all the instances used as well as to allow the use of some statistical measures. Fig. 6 shows the average results for all the versions and proximity measures: CAR represents the average cost after a routing process (after step 2), CAI the average cost after the improvement step (step 3) and CAL the average cost after the location procedure (after step 4) or the average CLRP cost. V1 and V3 produced the best average results. V2 and V4 produced the worst results. The average standard deviation in each version is 108.2, 217.0, 142.6 and 221.7, respectively, confirming the good performance of V1. The route improvement procedure allowed a decrease of 16% in the route costs and, as shown in Fig. 6, had a significant impact in the cases of (V2,SL), (V2,SA), (V3,WA) and (V4,CL), attenuating the consequences of a poor initialization. Moreover, in Barreto et al. (2003a) we have shown that CLRP solutions obtained without the route improvement step are 25% worst. Furthermore, this step has an important role in the elimination of the biased effect as shown in Figs. 3a, 3b and 3c. Fig. 7 shows the CLRP average results and confirms the good performance of V1 version. The group average (GA) proximity measure produces the most balanced results. Generally, the analysis of the success rates and average results allows ordering the versions by decreasing performance: V1, V3, V2 and V4. Concerning the proximity measures, the best performance was obtained by GA followed by CT, SA, CL, SL and WA measures. Table 1 shows, for the 19 instances of the LRPC, a lower bound (LB) and an upper bound (UB) of the optimal solution cost. The running time was less than one second, except for instances 11 (202seconds maximum), 13 and 14 (115seconds). The CLRP instance column contains information about the author, the publication year and the number of customers and potential DCs. The lower bound was obtained with a relaxed 2-index integer linear programming formulation (Barreto, 2004) and the upper bound is the best known solution (BKS) obtained using the heuristic. The values followed by an asterisk correspond to the optimal solution. It was not possible to get LBs for instances 13, 14 and 16. For all the instances, the Gap falls between a minimum of 0% and a maximum of 19.01% with an average of 4.81% and a median of 3.13%. 7 Conclusions Due to the complexity of the capacitated location-routing problem (CLRP), the heuristic approach is a promising way to find good solutions for medium and large problems. In this paper, a cluster analysis based sequential heuristic that uses simple procedures was presented. Moreover, four grouping techniques (hierarchical and non-hierarchical) and six proximity measures (single linkage, complete linkage, group average, centroid, ward and saving) were used to obtain several versions of the heuristic. Computational tests on 19 CLRP instances adapted from the literature were performed in order to compare their performance. The obtained results seem to indicate that version V1 had a slightly better performance than the other versions; version V3 was in second place followed by version V2, while version V4 had the worst performance. Concerning proximity measures, the group average measure has produced the most balanced results, followed by the measures centroid, saving, complete linkage, single linkage and ward. However, all the measures obtained good results for some instances; thus, it seems an advisable strategy to use several versions of the proposed heuristic and several proximity measures and then choose the best solution. In absolute terms, the average Gap was 4.81% with a median of 3.13% and, in some instances, optimality was reached. Several authors used clustering techniques in algorithms to solve the CLRP but never justified its use nor evaluated its performance. In this work the good performance of the clustering procedures is demonstrated; the average gap is narrow (less than 5%) and the result improves when the median (≈3%) is considered, attenuating the previous influence of the two outliers (19.01 and 12.78) which are due to the LB solution and not to (UB) heuristic solution. Despite these encouraging results, there are yet many opportunities ahead. The work is in progress, and eventually improvements are expected. Not all the clustering methods neither the proximity measures were tested and this is an open opportunity. In this process the construction of groups (cluster) with limited capacity is a crucial moment and it is necessary to improve the methods to perform it. For instance, further improvement could be obtained in the sequential heuristic for the CLRP using the following additional steps: Step 5. Determine the solution of the TSP in each route (including the DC). Step 6. Apply a 1-optimal procedure to the customers, subjected to the capacity constraints. Return to step 5 if any route was changed. Moreover, in the non-hierarchical case, another possibility to obtain better solutions is to run the same version several times using different customer sources. All the above ideas attempt to improve the solution concerning cost; nevertheless, the choice of a final solution does not solely depend on its cost. Other objectives and aspirations of managers can also be relevant. In this context, the possibility to generate several “good” solutions in an acceptable time can be very important in order to support decisions. This study confirms the potentialities of using clustering techniques in the CLRP approach. The grouping procedures are very fast and allow obtaining good alternative solutions corresponding to different configurations. This is a good example of an investigation opportunity, with promising results, using two distinct scientific areas, cluster analysis and operational research. Acknowledgement This research was partially financially supported by the European Social Fund. References Anderberg, 1973 M.R. Anderberg Cluster Analysis for Applications 1973 Academic Press New York Balakrishnan et al., 1987 A. Balakrishnan J.E. Ward R.T. Wong Integrated facility location and vehicle routing models: Recent work and future prospects American Journal of Mathematical Sciences 7 1&2 1987 35 61 Barreto, 2003 Barreto, S.S., 2003. Available from: Barreto, 2004 Barreto, S.S., 2004. Análise e modelização de problemas de localização-distribuição [Analysis and modelling of location-routing problems]. PhD Dissertation, University of Aveiro, Aveiro, Portugal (in Portuguese). Barreto et al., 2003a Barreto, S.S., Ferreira, C.M., Paixão, J.M., 2003a. Using clustering analysis in a capacitated location-routing problem. Communication presented at XIV Meeting of the European Working Group on Locational Analysis, September 11–13, Corfu, Greece. Barreto et al., 2003b Barreto, S.S., Ferreira, C.M., Paixão, J.M., 2003b. Problemas de localização-distribuição: uma revisão bibliográfica [Location-routing problems: A bibliographical review]. In: Proceedings of the VI Congreso Galego de Estatı´stica e Investigación de Operacións, November 5–7, University of Vigo, Vigo, Spain, pp. 93–100 (in Portuguese). Bayne et al., 1980 C.K. Bayne J.J. Beauchamp C.L. Begovich V.E. Kane Monte carlo comparisons of selected clustering procedures Pattern Recognition 12 1980 51 62 Bodin, 1975 L.D. Bodin A taxonomic structure for vehicle routing and scheduling problems Computers and Urban Society 1 1975 11 29 Bramel and Simchi-Levi, 1997 J. Bramel D. Simchi-Levi The Logic of Logistics 1997 Springer-Verlag New York Branco and Coelho, 1990 I.M. Branco J.D. Coelho The hamiltonian p-median problem European Journal of Operational Research 47 1990 86 95 Bruns and Klose, 1995 Bruns, A., Klose, A., 1995. An iterative heuristic for location-routing problems based on clustering. In: Proceedings of the Second International Workshop on Distribution Logistics, The Netherlands, pp. 1–6. Christofides and Eilon, 1969 N. Christofides S. Eilon An algorithm for the vehicle-dispatching problem Operational Research Quarterly 20 3 1969 309 318 Clark and Wright, 1964 G. Clark J.W. Wright Scheduling of vehicles from a central depot to a number of delivery points Operations Research 14 1964 568 581 Cullen et al., 1981 F.H. Cullen J.J. Jarvis H.D. Ratliff Set partitioning based heuristics for interactive routing Networks 11 1981 125 143 Dantzig and Ramser, 1959 G.B. Dantzig J.H. Ramser The truck dispatching problem Management Science 6 1 1959 80 91 Daskin, 1995 M.S. Daskin Network and Discrete Location: Models, Algorithms and Applications 1995 John Wiley & Sons, Inc. New York Edelbrock and McLaughlin, 1980 C. Edelbrock B. McLaughlin Hierarchical cluster analysis of intraclass correlations: A mixture model study Multivariate Behavioral Research 15 1980 299 318 Everitt, 1993 B.S. Everitt Cluster Analysis 1993 Arnold London Fukunaga and Short, 1978 K. Fukunaga R.D. Short Generalized clustering for problem localization IEEE Transactions on Computers C 27 2 1978 176 181 Gaskell, 1967 T.J. Gaskell Bases for vehicle fleet scheduling Operational Research Quarterly 18 3 1967 281 295 Golden and Meehl, 1980 R.R. Golden P.E. Meehl Detection of biological sex – an empirical test of cluster methods Multivariate Behavioural Research 15 1980 475 496 Gower, 1985 J.C. Gower Measures of similarity, dissimilarity and distance Encyclopedia of Statistical Sciences 5 1985 397 405 Gower and Ross, 1969 J.C. Gower G.J.S. Ross Minimum spanning trees and single linkage cluster analysis Applied Statistics 18 1969 54 64 Jain and Dubes, 1988 A.K. Jain R.C. Dubes Algorithms for Clustering Data 1988 Prentice Hall Englewood Clifs, New Jersey Johnson, 1967 S.C. Johnson Hierarchical clustering schemes Psychometrika 32 3 1967 241 254 Kaufman and Rousseeum, 1990 L. Kaufman P.J. Rousseeum Finding Goups in Data: An Introduction to Cluster Analysis 1990 John Wiley & Sons Inc. New York Klose, 1995 A. Klose Using clustering methods in problems of combined location and routing. Operations Research Proceedings 1995 Springer pp. 411–416 Kuehn and Hamburger, 1963 A.A. Kuehn M.J. Hamburger A heuristic program for locating warehouses Management Science 9 4 1963 643 666 Madsen, 1983 O.B.G. Madsen Methods for solving combined two level location-routing problems of realistic dimensions European Journal of Operational Research 12 1983 295 301 Maranzana, 1963 F.E. Maranzana On the location of supply points to minimize transportation costs IBM Systems Journal 2 1963 129 135 Milligan and Schilling, 1985 G.W. Milligan D.A. Schilling Asymptotic and finite-sample characteristics of four external criterion measures Multivariate Behavioral Research 20 1985 97 109 Min, 1989 H. Min The multiple vehicle routing problem with simultaneous delivering and pick-up points Transportation Research A 23 5 1989 377 386 Min, 1996 H. Min Consolidation terminal location–allocation and consolidated routing problems Journal of Business Logistics 17 2 1996 235 263 Min et al., 1992 H. Min J. Current D. Schilling The multiple depot vehicle routing problem with backhauling Journal of Business Logistics 13 1 1992 259 288 Or, 1976 Or, I., 1976. Traveling salesman–type combinatorial problems and their relation to the logistics of regional blood banking. PhD Dissertation, Northwestern University, Evanston, Illinois, USA. Perl, 1983 Perl, J., 1983. A unified warehouse location-routing analysis. PhD Dissertation, Northwestern University, Evanston, Illinois, USA. Romesburg, 1984 H.C. Romesburg Cluster Analysis for Researchers 1984 Lifetime Learning Publications Belmont, Canada Srivastava, 1986 Srivastava, R., 1986. Algorithms for solving the location-routing problem. PhD Dissertation, The Ohio State University. Srivastava, 1993 R. Srivastava Alternate solution procedures for the location-routing problem OMEGA The International Journal of Management Sciences 21 4 1993 497 506 Srivastava and Benton, 1990 R. Srivastava W.C. Benton The location-routing problem: Considerations in physical distribution system design Computers and Operations Research 17 5 1990 427 435 White Paper, 2001 White Paper, 2001. European transport policy for 2010: Time to decide. The European Commission, COM(2001)370. "
    },
    {
        "doc_title": "Evaluation of mesh simplification algorithms using PolyMeCo: A case study",
        "doc_scopus_id": "34249048328",
        "doc_doi": "10.1117/12.704547",
        "doc_eid": "2-s2.0-34249048328",
        "doc_date": "2007-05-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Polygonal meshes",
            "Quality evaluation",
            "Simplification algorithms",
            "Systematic evaluation"
        ],
        "doc_abstract": "Polygonal meshes are used in many application scenarios. Often the generated meshes are too complex not allowing proper interaction, visualization or transmission through a network. To tackle this problem, simplification methods can be used to generate less complex versions of those meshes. For this purpose many methods have been proposed in the literature and it is of paramount importance that each new method be compared with its predecessors, thus allowing quality assessment of the solution it provides. This systematic evaluation of each new method requires tools which provide all the necessary features (ranging from quality measures to visualization methods) to help users gain greater insight into the data. This article presents the comparison of two simplification algorithms, NSA and QSlim, using PolyMeCo, a tool which enhances the way users perform mesh analysis and comparison, by providing an environment where several visualization options are available and can be used in a coordinated way. © 2007 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using task analysis to improve the requirements elicitation in health information system.",
        "doc_scopus_id": "84903811794",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84903811794",
        "doc_date": "2007-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This paper describes the application of task analysis within the design process of a Web-based information system for managing clinical information in hemophilia care, in order to improve the requirements elicitation and, consequently, to validate the domain model obtained in a previous phase of the design process (system analysis). The use of task analysis in this case proved to be a practical and efficient way to improve the requirements engineering process by involving users in the design process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of methods for the simplification of mesh models using quality indices and an observer study",
        "doc_scopus_id": "34548273966",
        "doc_doi": "10.1117/12.704098",
        "doc_eid": "2-s2.0-34548273966",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Mesh models",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "The complexity of a polygonal mesh model is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, only a few observer studies are reported comparing them regarding the perceived quality of the obtained simplified meshes, and it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting model, as perceived by the final users. Mesh quality indices are the obvious less costly alternative to user studies, but it is also not clear how they relate to perceived quality, and which indices best describe the users behavior. Following on earlier work carried out by the authors, but only for mesh models of the lungs, a comparison among the results of three simplification methods was performed through (1) quality indices and (2) a controlled experiment involving 65 observers, for a set of five reference mesh models of different kinds. These were simplified using two methods provided by the OpenMesh library - one using error quadrics, the other additionally using a normal flipping criterion - and also by the widely used QSlim method, for two simplification levels: 50% and 20% of the original number of faces. The main goal was to ascertain whether the findings previously obtained for lung models, through quality indices and a study with 32 observers, could be generalized to other types of models and confirmed for a larger number of observers. Data obtained using the quality indices and the results of the controlled experiment were compared and do confirm that some quality indices (e.g., geometric distance and normal deviation, as well as a new proposed weighted index) can be used, in specific circumstances, as reasonable estimators of the user perceived quality of mesh models. © 2007 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modeling a Web-based Information System for managing clinical information in hemophilia care",
        "doc_scopus_id": "34047122804",
        "doc_doi": "10.1109/IEMBS.2006.259679",
        "doc_eid": "2-s2.0-34047122804",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder integration",
            "Data storage",
            "Hemophiliacs"
        ],
        "doc_abstract": "Nowadays, Information Systems combined with the Internet, have a significant role in data storage, as in the efficiency and promptness of data transfer and can offer a large contribute in managing and manipulating the information resulting from treatment and attendance of chronic patients, as hemophiliacs [1, 2]. On the other hand, the Internet also created the opportunity of patients to insert data concerning home treatments. This paper briefly describes the modeling process of a Web-based information system to help the management of inherited bleeding disorders integrating, diffusing and archiving large sets of information from heterogeneous sources in scope of the hemophilia care at the Hematology Service of Coimbra Hospital Center, in Portugal. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A volumetric pulmonary CT segmentation method with applications in emphysema assessment",
        "doc_scopus_id": "33745401842",
        "doc_doi": "10.1117/12.652622",
        "doc_eid": "2-s2.0-33745401842",
        "doc_date": "2006-06-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "3D lung surfaces",
            "Emphysema Assessment",
            "Medical Image Analysis",
            "Volumetric Pulmonary Segmentation"
        ],
        "doc_abstract": "A segmentation method is a mandatory pre-processing step in many automated or semi-automated analysis tasks such as region identification and densitometric analysis, or even for 3D visualization purposes. In this work we present a fully automated volumetric pulmonary segmentation algorithm based on intensity discrimination and morphologic procedures. Our method first identifies the trachea as well as primary bronchi and then the pulmonary region is identified by applying a threshold and morphologic operations. When both lungs are in contact, additional procedures are performed to obtain two separated lung volumes. To evaluate the performance of the method, we compared contours extracted from 3D lung surfaces with reference contours, using several figures of merit. Results show that the worst case generally occurs at the middle sections of high resolution CT exams, due the presence of aerial and vascular structures. Nevertheless, the average error is inferior to the average error associated with radiologist inter-observer variability, which suggests that our method produces lung contours similar to those drawn by radiologists. The information created by our segmentation algorithm is used by an identification and representation method in pulmonary emphysema that also classifies emphysema according to its severity degree. Two clinically proved thresholds are applied which identify regions with severe emphysema, and with highly severe emphysema. Based on this thresholding strategy, an application for volumetric emphysema assessment was developed offering new display paradigms concerning the visualization of classification results. This framework is easily extendable to accommodate other classifiers namely those related with texture based segmentation as it is often the case with interstitial diseases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Perceived Quality of Simplified Polygonal Meshes: Evaluation using Observer Studies",
        "doc_scopus_id": "34249048115",
        "doc_doi": null,
        "doc_eid": "2-s2.0-34249048115",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Controlled experiment",
            "Generic modeling",
            "Lung model",
            "Mesh modeling",
            "Mesh simplifications",
            "Observer studies",
            "Perceived quality",
            "Polygonal mesh models",
            "Polygonal meshes",
            "Simplification method"
        ],
        "doc_abstract": "© The Eurographics Association 2006.The complexity of a polygonal mesh model is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting mesh, as perceived by the final users. Following on work carried out by the authors, but only for mesh models of the lungs [SSSMF05, SSFM05], a comparison among the results of three mesh simplification methods, for a few generic models and two simplification levels, was performed through a controlled experiment involving 65 observers. The goal was to ascertain whether the main findings previously obtained for lung models, through a study with 32 subjects, could be generalized to other types of models and confirmed for a larger number of observers. This was verified through the analysis of the data collected from the experiment, which shows that, regarding perceived quality, users are indeed sensitive to the mesh simplification method used and that this sensitivity varies with the simplification level.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An introductory course on Human-Computer Interaction: Programme, bibliography, practical classes and assignments",
        "doc_scopus_id": "33745607845",
        "doc_doi": "10.1016/j.cag.2006.03.007",
        "doc_eid": "2-s2.0-33745607845",
        "doc_date": "2006-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Commented bibliography",
            "Computing science and",
            "Human-Computer Interaction education",
            "Practical assignments"
        ],
        "doc_abstract": "As computing curricula have evolved, human-computer interaction has gradually become part of many of them, and the recent ACM/IEEE report on the core of computing science and engineering includes human-computer interaction as one of the fundamental sub-areas that should be addressed. However, both technology and human-computer interaction are developing rapidly, thus a continuous effort is needed to maintain a programme, bibliography, and a set of practical assignments up to date and adapted to the current technology. This paper briefly presents an introductory course on human-computer interaction offered to electrical and computer engineering students at the University of Aveiro. © 2006 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2006-05-09 2006-05-09 2010-04-18T21:57:22 S0097-8493(06)00087-2 S0097849306000872 10.1016/j.cag.2006.03.007 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20060801 20060831 2006 2006-05-09T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst pubtype ref alllist content subj ssids 0097-8493 00978493 30 30 4 4 Volume 30, Issue 4 18 658 668 658 668 200608 August 2006 2006-08-01 2006-08-31 2006 EDUCATION article fla Copyright © 2006 Elsevier Ltd. All rights reserved. INTRODUCTORYCOURSEHUMANCOMPUTERINTERACTIONPROGRAMMEBIBLIOGRAPHYPRACTICALCLASSESASSIGNMENTS SOUSASANTOS B 1 Introduction 2 Course contents 3 Bibliography 3.1 Basic bibliography 3.1.1 Alan Dix, Janet Finlay, Gregory Abowd, Russell Beale, Human–Computer Interaction, 1st, 2nd, 3rd edition, Prentice Hall, 1993, 1998, 2004 3.1.2 Jenny Preece, Yvonne Rogers, Helen Sharp, D. Benyon, S. Holland, T. Carey, Human–Computer Interaction, Addison Wesley, 1994 3.1.3 Ben Shneiderman, Designing the User Interface—Strategies for Effective Human–Computer Interaction, 3rd ed., Addison Wesley, 1998 3.1.4 Deborah Mayhew, Principles and Guidelines in Software User Interface Design, Prentice Hall, 1992 3.1.5 William M. Newman, Michael G. Lamming, Interactive System Design, Addison Wesley, 1995 3.1.6 Jenny Preece, Yvonne Rogers, Helen Sharp, Interaction Design—Beyond Human–Computer Interaction, John Wiley, 2002 3.1.7 David Redmond-Pyle, Alan Moore, Graphical User Interface Design and Evaluation—a Practical Process, Prentice Hall, 1995 3.1.8 Jakob Nielsen, Usability Engineering, Morgan Kaufmann, 1993 3.1.9 Deborah Mayhew, The Usability Engineering Lifecycle—a Practitioner's Handbook for User Interface Design, Morgan Kaufmann, 1999 3.1.10 James Foley, Andries van Dam, Steven Feiner, John Hughes, Richard Phillips, Introduction to Computer Graphics, Addison Wesley, 1993 3.1.11 David Hearn, M. Pauline Baker, Computer Graphics with Open GL, 3rd ed., Prentice Hall, 2004 3.2 Complementary bibliography 3.2.1 Ian Horrocks, Constructing the User Interface with Statecharts, Addison Wesley, 1999 3.2.2 Scott Weiss, Handheld Usability, John Wiley and Sons, 2002 3.2.3 Carolyn Snyder, Paper Prototyping—The Fast and Easy Way to Design and Refine User Interfaces, Morgan Kaufmann, 2003 3.2.4 Dan Diaper, Neville Stanton (eds.), The Handbook of Task Analysis for the Human–Computer Interaction, Laurence Erlbaum Associates Publishers, 2004 3.2.5 Jan Borchers, A Pattern Approach to Interaction Design, John Wiley, 1996 3.2.6 Alan Cooper, Robert Reimann, About Face 2.0—The Essentials of Interaction Design, John Wiley, 2003 3.2.7 Edward Tufte, Envisioning Information, Graphics Press, 1990 3.2.8 John Carroll (ed.), HCI Models, Theories and Frameworks, Toward a Multidisciplinary Science, Morgan Kaufmann, 2003 3.2.9 Julie Jacko, Andrew Sears (eds.), The Human Computer Interaction Handbook, Lawrence Erlbaum Associates, 2002 3.3 Opinion bibliography 3.3.1 Donald Norman, The Design of Everyday Things, The MIT Press, 1998 3.3.2 Donald Norman, The Invisible Computer—Why Good Products Can Fail, the Personal Computer Is So Complex and Information Appliances Are the Solution, The MIT Press, 1998 3.3.3 Jef Raskin, The Humane Interface—New Direction for Designing Interactive Systems, Addison Wesley, 2000 3.3.4 Ben Shneiderman, Leonardo's Laptop—Human Needs and the New Computing Technology, The MIT Press, 2002 4 Practical assignments 4.1 Assignments on user interface evaluation 4.2 Final assignment (on user interface design) 4.3 Other activities 4.4 Writing reports 5 Discussion and conclusions Acknowledgement References NORMAN 1999 D INVISIBLECOMPUTER SOMMERVILLE 2001 I SOFTWAREENGINEERING HU 2003 444 451 S EVANS 2003 420 428 D MCGETTRICK 2003 456 462 A DIX 1998 A HUMANCOMPUTERINTERACTION MAYHEW 1992 D PRINCIPLESGUIDELINESINSOFTWAREUSERINTERFACEDESIGN MAYHEW 1999 D USABILITYENGINEERINGLIFECYCLEAPRACTITIONERSHANDBOOKFORUSERINTERFACEDESIGN BLINN 2005 92 93 J PREECE 1994 J HUMANCOMPUTERINTERACTION SHNEIDERMAN 1998 B DESIGNINGUSERINTERFACESTRATEGIESFOREFFECTIVEHUMANCOMPUTERINTERACTION CERF 1997 33 42 V BEYONDCALCULATIONNEXTFIFTYYEARSCOMPUTING THEYREEVERYWHERE NEWMAN 1995 W INTERACTIVESYSTEMDESIGN HEARN 2004 D COMPUTERGRAPHICSOPENGL NEUMANN 2004 28 30 P TUFTE 1990 E ENVISIONINGINFORMATION HORROCKS 1999 I CONSTRUCTINGUSERINTERFACESTATECHARTS NIELSEN 1993 J USABILITYENGINEERING DIX 2004 A HUMANCOMPUTERINTERACTION SULLIVAN 2004 30 35 A FERREIRA 1996 117 159 C STRONG 1994 G SOUSASANTOS 2004 812 818 B 1992 WRITINGSPEAKINGINTECHNOLOGYPROFESSIONAPRACTICALGUIDE PAQUETTE 2005 246 255 E SOUSASANTOSX2006X658 SOUSASANTOSX2006X658X668 SOUSASANTOSX2006X658XB SOUSASANTOSX2006X658X668XB item S0097-8493(06)00087-2 S0097849306000872 10.1016/j.cag.2006.03.007 271576 2010-10-08T23:26:03.856391-04:00 2006-08-01 2006-08-31 true 197002 MAIN 11 65326 849 656 IMAGE-WEB-PDF 1 CAG 1659 S0097-8493(06)00087-2 10.1016/j.cag.2006.03.007 Elsevier Ltd Table 1 Advised readings per lecture and topic Lectures Topics Basic readings Complementary readings 1 Introduction to the problem [10] Chapter 1 [13,14] [11] Chapter 1 [15] 2 Usability principles and paradigms [10] Chapter 4 [16] 3,4 The user: human information processing system; mental models [11] Chapters 2,3 [17] 5,6 Input/output devices [18] Chapter 2 [15] [11] Chapter 12 [19] 7,8 Dialog styles [11] Chapters 4–10 [15] 9 Screen layout and colour usage; colour models [11] Chapters 14 [15] [18] Chapter 12 [20] 10,11 Models in the user interface design process [10] Chapters 6–8 [21] 12 Evaluation of user interfaces [10] Chapter 11 [14,22] 13 Response time; help and documentation; user centred design [11] Chapter 15 [15] [10] Chapter 12 [12] Chapter 1 Table 2 Practical assignments and number of hours devoted to them in class Type Practical assignments Hours in class Evaluation Observation techniques 2 Evaluation Controlled experiments 2 Evaluation Heuristic evaluation 4 Evaluation Mobile phone evaluation 2 S/W Introduction to Visual Basic 2 S/W Introduction to HTML 2 Design Design of a user interface 12 Education An introductory course on Human–Computer Interaction: Programme, bibliography, practical classes and assignments Beatriz Sousa Santos Department of Electronics and Telecommunications/IEETA, University of Aveiro, P-3810-193 Aveiro, Portugal As computing curricula have evolved, human–computer interaction has gradually become part of many of them, and the recent ACM/IEEE report on the core of computing science and engineering includes human–computer interaction as one of the fundamental sub-areas that should be addressed. However, both technology and human–computer interaction are developing rapidly, thus a continuous effort is needed to maintain a programme, bibliography, and a set of practical assignments up to date and adapted to the current technology. This paper briefly presents an introductory course on human–computer interaction offered to electrical and computer engineering students at the University of Aveiro. Keywords Human–Computer Interaction education Commented bibliography Practical assignments 1 Introduction The industry of computer systems is reaching maturity. Currently, average users are not technologically savvy; therefore, those systems have to be designed taking into consideration the needs, capacities and limitations of their target users; i.e., in this phase of the industry a human-centred development cycle is required [1] and the user interface has become increasingly important. On the other hand, a lesson learned repeatedly by engineering disciplines is that design problems have a context and that the overly narrow optimisation of one part of a design can be rendered invalid by the broader context of the problem. Thus, even from a direct computer science perspective, it is advantageous to frame the problem of human–computer interaction broadly enough so as to help students (and practitioners) avoid the classic pitfall of design divorced from the context of the problem [2]. Moreover, the design of many modern computer applications inescapably requires the design of some component of the system that interacts with a user [3]. This component typically represents more than half of a system's lines of code. It is intrinsically necessary to understand how to decide on the functionality a system will have, how to bring this out to the user, how to build the system and how to test the design. The above-mentioned reasons were recently acknowledged by several authors [4–6]; furthermore, the reports concerning computer science and engineering curricula [7,8] recognise that these curricula should reflect the ever-growing importance of the human–computer interface. Taking all this into consideration, an introductory course on human–computer interaction is offered at the University of Aveiro. This course was created in 1993/1994 and has been offered to students of two different degrees, one in the area of electrical engineering and another in the area of computer engineering. In the former this course is offered as an elective in the 5th year, and in the latter it is mandatory in the 3rd year [9]. For some years it was also offered to M.Sc. students as an elective course. The course is introductory and does not intend to train students as human–computer interaction engineers, but to expose them to the basic concepts of the field. The main objectives are: 1. to emphasise the importance of good user interface design, 2. to introduce the literature of human–computer interaction, 3. to introduce tools, techniques and ideas for user interface design, implementation and evaluation, 4. to facilitate communication between human–computer interaction experts and the soon-to-be engineers. In addition to these specific objectives, this course also intends to foster important general capabilities such as critical thinking, teamwork, and communication skills. Several books are used as core bibliography [10–12]. For each topic, one book is used as main reference according to its approach; however, other references (books, papers, Web sites) are also used. This paper briefly describes the course contents and presents a commented bibliography, as well as the practical classes and practical assignments. 2 Course contents Human–computer interaction is a rapidly evolving discipline, which implies that courses in this area must be carefully put together so that they do not become quickly out of date. This is particularly important since, although the contents of a course can be revised, people cannot generally be recalled for retraining; they must build their own future understanding upon the foundations provided by the courses taken when they were students. According to the ACM/SIGCHI report Curricula for Human–Computer Interaction [2]: “human–computer interaction is a discipline concerned with the design, evaluation and implementation of interactive computing systems for human use and the study of major problems surrounding them”. It thus has science, engineering and design aspects. These topics can be classified in two main areas: • Foundations of human–computer interaction, • Design, implementation and evaluation of user interfaces. Below all the topics addressed throughout the course are presented according to this classification. The sequence in which they are introduced has slightly changed along the years due to several reasons (e.g. adjustment to practical classes). • Foundations of human–computer interaction: ○ introduction to the problem: introduction to human–computer interaction, definition of user interface, general usability principles and paradigms; ○ user profile: Human information processing system, other relevant characteristics; ○ conceptual models: conceptual and mental models; ○ input/output devices: existing devices and usability issues. • Design, implementation and evaluation of user interfaces: ○ dialog styles: a classification, some experimental results, characteristics and applicability of each dialog style, principles and guidelines to their use; ○ screen layout and colour: principles and guidelines; most important colour models; ○ evaluation methods: analytic methods and methods that involve users, their characteristics and how to choose them; ○ response time: general observations and guidelines; ○ help and documentation: ideal characteristics and guidelines for their preparation; ○ user-centred design of user interfaces: overview of a methodology; phases of the lifecycle of interactive software and usability engineering. These topics are addressed along approximately 26h in 2h lectures; next the specific topics dealt with in each lecture, in the sequence used in the academic year of 2004/2005, are presented. Evaluation and a design methodology are for the most part tackled in practical classes; however, an overview is given in a lecture: • 1st lecture: objectives, syllabus, advised reading and evaluation method for the course (0.5h). ○ Introduction to the issue of user interfaces in interactive systems (1.5h): – historical perspective; – ergonomics and human factors; – importance and scope of human–computer interaction; – usability and usability engineering; – two complementary approaches to user interface design: principles and paradigms. • 2nd lecture: usability principles and paradigms (2h): ○ principles: compatibility, consistency, feedback, familiarity, simplicity, flexibility, robustness, protection, etc. ○ main usability objectives: ease of learning, ease of use and satisfaction. ○ paradigms: timesharing, personal computing, windows, icons, menus and pointing devices (WIMP), metaphors, direct manipulation, hypertext, multimodality, computer-supported cooperative work (CSCW), World-Wide Web (WWW) and ubiquitous computing. • 3rd and 4th lectures: the user (4h): ○ importance of knowing the user profile; ○ characteristics determining user performance (internal and external); ○ introduction to the human information processing system (HIPS): main characteristics and implications of each sub-system in user interface design: – perceptual sub-system (sensory buffer and pattern recognition); – cognitive sub-system (memories and processes); – motor sub-system (Fitts law); ○ other characteristics: (psychological and physical, task, knowledge and experience) and their implication in user interface design; ○ mental and conceptual models: – what are mental models, some characteristics and why people use them; – what are conceptual models; guidelines to their design. • 5th and 6th lectures: input/output devices (4h): ○ keyboards: existing types, main characteristics and usability issues; ○ pointing devices: existing types (mouse, track-ball, joy-stick, touch-screen, etc.), introduction to technology and usability issues; ○ other input devices (eye-trackers, data-gloves, other three-dimensional (3D) devices); ○ display devices: existing types (CRT based, LCD, plasma and others); ○ voice interaction: main usability issues and applicability, guidelines for the use of voice recognition and synthesis. • 7th and 8th lectures: dialog styles (4h): ○ a classification; ○ menus: characteristics and existing types, applicability, some studies, main guidelines; ○ direct manipulation, fill-in-forms, question-and-answer, function-keys, command languages and natural language: characteristics, applicability and main guidelines. • 9th lecture: screen layout (0.5h) and colour (1.5h). ○ screen layout: – main characteristics influencing user performance; – main guidelines and some examples; ○ colour: – importance and pitfalls of colour in user interfaces; – main guidelines and some examples; – importance of colour quantification and introduction to colour models (RGB, CMY, HSV, HLS). • 10th and 11th lectures: models to be used in the user interface design process (4h): ○ user models: – importance, main types of user models and their applicability; – cognitive models (GOMS—goals, objectives, selections and methods); – physical models (Keystroke Level Model—KLM); ○ task analysis: – importance and main types of techniques; – hierarchical task analysis—HTA; – main information sources; – using task analysis in user interface design; ○ dialog notation: – importance and main types; – diagrammatic and textual notations: advantages and disadvantages; – main graphical notations (state transition networks, petri nets, state charts), some simple examples. • 12th lecture: evaluation of user interfaces (2h): ○ importance of evaluation and when to use it; ○ evaluation styles; ○ evaluation methods: – analytic methods (cognitive walkthrough, heuristic evaluation, review-based methods and model-based methods); – methods involving users (controlled experiments, observation methods, query methods): characteristics, advantages and disadvantages; – how to select evaluation methods. • 13th lecture: response time (0.5h), help and documentation (0.5h), user-centred design (1h): ○ response time: – general observations; – main guidelines and examples; ○ help and documentation: – importance and existing types; – ideal characteristics; – examples; ○ User-centred design: – overview of a methodology; – phases of the interactive software lifecycle. 3 Bibliography A good bibliography is fundamental to support students in their individual learning effort; thus, a reading list is provided for every topic in the programme. This reading list contains book chapters that every student should read (shown in Table 1 ), as well as some advanced readings, book chapters, papers or Web sites, provided for more interested students. The book by Alan Dix et al. [10] is used as a textbook; it is, according to its authors, meant for students in the areas of computer science and engineering. In spite of the fact that the author of this paper considers that this book has a general approach adequate to her students, she feels the need to use other books. This is due to two reasons: on one hand, [10] tackles some issues at such a high abstraction level that some more specific examples are needed; on the other hand, some topics important to the general understanding of the problems addressed in the course are not addressed in enough detail. Therefore, students are advised to study the human information processing system and mental models in Ref. [11], since it addresses these subjects using a more detailed and explicit approach. The same book is also recommended for the study of response time, which is not addressed as an independent subject in Ref. [10]. The author has also used the book by Mayhew as a guide to address dialog styles and screen design, since it presents a lot more examples and guidelines than Ref. [10]. On the subject of guidelines, a word of caution is due: notwithstanding guidelines help understand the principles and concepts and thus they should be used, unlike general principles, they become obsolete as technology evolves; thus, their use needs a frequent adaptation. Another book by Mayhew [12] is used because it presents in a structured and easy to understand manner a methodology to implement the usability software lifecycle, which is a good example of how the concepts introduced along the course can be used in a user-centred design of an interactive product. The book [18], which is an up-dated computer graphics textbook, is used to support the study of input and output devices. The new (3rd) edition of the book by Alan Dix et al. [23] includes additional examples, supplementary material on subjects that were already addressed in the 2nd edition and tackles new issues; thus, it may be used to support some more topics addressed throughout the course. Besides the above-mentioned bibliographic references that have been used along the years, some other books, papers and Web sites are used (changing often) in order to present a few novel developments, discuss currently important issues or illustrate concepts in a different or more up-to-date way. In 2004/2005, the following optional readings were given to the students to support subjects that were introduced in the lectures: • a chapter from a book (published by ACM in its 50th anniversary) about ubiquitous computing that includes a scenario of its possible use in everyday life [16]; • a paper from IEEE—Computer Graphics and Applications, criticising and joking about poor user interfaces at gymnasium facilities [13]. • a paper from IEEE—Spectrum presenting two 3D displays [24]; • a set of papers from Communications of the ACM, concerning the problems of electronic voting [19], an important contemporary issue where user interfaces are critical. Below, the commented bibliography, including all the books used in this course, is presented divided into three different categories: basic, complementary and opinion bibliography. The first three books in the first category can be considered textbooks in human–computer interaction, albeit having different approaches and target audiences. The next three books address mainly user interface design, and the last five books are used to support a few specific topics. All the chapters recommended as mandatory reading belong to books in this category. The complementary bibliography includes six books (having a narrower scope) that tackle some issues in detail and which can be useful to extract examples for the lectures and practical classes or to support assignments of more advanced students. Moreover, this category comprises three other books: a handbook indispensable to the researcher, a classic reference in visualisation and a book that presents various approaches to human–computer interaction based on several different scientific areas (as cognitive psychology and sociology), which helps to understand the truly interdisciplinary nature of this scientific area. The last category includes books by well-know authors that convey personal and critical views about technology and interactive products, speculate on future developments and point out new ways to adapt technology and its products to the users’ needs and capabilities. Reading at least some of these books is essential to anyone lecturing in this area. 3.1 Basic bibliography 3.1.1 Alan Dix, Janet Finlay, Gregory Abowd, Russell Beale, Human–Computer Interaction, 1st, 2nd, 3rd edition, Prentice Hall, 1993, 1998, 2004 The 1st edition of this book was one of the first textbooks in the area; it was used to support courses that had been appearing at Universities, and simultaneously it helped to “crystallize” the content of the field that was still spread throughout the literature. Human–computer interaction is approached from a computer science perspective. This book addresses the main subjects in the area; however, the emphasis is on design methods of interactive systems. The following editions have been extensively revised and rewritten. The 3rd edition includes much new material and is organised in parts covering basic material suitable for introductory courses, including a new chapter on interaction design and more advanced material focused on different HCI models and theories, suited for more advanced courses, as well as researchers new to the field. Detailed coverage of the particular domains such as Web applications, ubiquitous computing and computer-supported collaborative work (CSCW) is also given. 3.1.2 Jenny Preece, Yvonne Rogers, Helen Sharp, D. Benyon, S. Holland, T. Carey, Human–Computer Interaction, Addison Wesley, 1994 This book covers the major subjects in human–computer interaction, using concepts from computer science, psychology, sociology and industrial design. It introduces the main issues in user interface design and development and addresses applications as CSCW, hyper- and multimedia. According to its authors, this book is meant to students in the areas of computer science, psychology and sociology. 3.1.3 Ben Shneiderman, Designing the User Interface—Strategies for Effective Human–Computer Interaction, 3rd ed., Addison Wesley, 1998 This book gives an overview of the issues central to designing, implementing, managing, maintaining and refining the user interface of interactive systems. It is meant primarily to designers and managers but also to researchers, mainly in the study of user performance, having different backgrounds. Moreover, it is useful to the educator. According to the author, the quality of user interfaces has improved greatly in the last years, and this book is meant to help researchers as well as designers keep up the momentum and encourage further progress. 3.1.4 Deborah Mayhew, Principles and Guidelines in Software User Interface Design, Prentice Hall, 1992 This book presents principles and guidelines for all aspects of user interface design, including conceptual models, dialog styles, screen layout, user documentation and response time. It also presents a literature review concerning each subject. According to its author, it is aimed at practitioners as well as computer science students; although it is still useful, it should be used with caution since in various subjects it is already dated. 3.1.5 William M. Newman, Michael G. Lamming, Interactive System Design, Addison Wesley, 1995 This book presents a set of methods fundamental to the design of interactive systems. Its authors intend to provide a framework that allows integrating methods specific to interactive systems with more general methods. The addressed issues are: methods to study the user, systems analysis, requirement specification, prototyping, as well as methods for the design and evaluation of user interfaces. Moreover, this book comprises a set of case studies very useful to the educator. Its authors consider this book adequate to students as well as practitioners. 3.1.6 Jenny Preece, Yvonne Rogers, Helen Sharp, Interaction Design—Beyond Human–Computer Interaction, John Wiley, 2002 Interaction design is approached in a more comprehensive way than what has been usual in human–computer interaction. According to its authors, interaction design currently transcends the design of interaction systems meant to be used by a user set in front of a computer; thus, subjects as ubiquitous computing using wireless and cooperative technologies are addressed. Moreover, the book presents a lot of examples and is meant to be used by practitioners and students as well. 3.1.7 David Redmond-Pyle, Alan Moore, Graphical User Interface Design and Evaluation—a Practical Process, Prentice Hall, 1995 This book introduces a set of techniques, described as a flexible, practical process for user interface design. It was written by developers for developers (software designers, analysts, programmers and managers); however, it is simple and allows non-experts to develop usable software products and it can also be useful to educators since it presents a case study project throughout the complete process. 3.1.8 Jakob Nielsen, Usability Engineering, Morgan Kaufmann, 1993 This is an essential reference in the area of usability engineering. It introduces, in a simple way adequate to non-experts, methods to improve usability as well as evaluation methods, which can be used even with very low budgets. This book is indispensable to the educator and is strongly advised to practitioners. 3.1.9 Deborah Mayhew, The Usability Engineering Lifecycle—a Practitioner's Handbook for User Interface Design, Morgan Kaufmann, 1999 This book presents a structured approach to the user interface design as an interactive software lifecycle. The proposed lifecycle encompasses a set of usability engineering tasks used in a pre-determined order during the development of a product. This approach may be used in the development of any interactive product, not only in the project of user interfaces for mainstream software. This is a pragmatic book meant for practitioners, according to its author; however, it can be useful to the educator as a source of examples or as an illustration of a specific interactive software development methodology. 3.1.10 James Foley, Andries van Dam, Steven Feiner, John Hughes, Richard Phillips, Introduction to Computer Graphics, Addison Wesley, 1993 This is an adaptation of a more complete text that has been considered as a reference in computer graphics. This smaller version is meant to be used in introductory courses; however, it is useful for practitioners who need to learn basic principles. It includes chapters on input/output devices and basic graphical interaction techniques. 3.1.11 David Hearn, M. Pauline Baker, Computer Graphics with Open GL, 3rd ed., Prentice Hall, 2004 This book presents the principles of design, use and understanding of computer graphics systems and applications, along with OpenGL programming examples. Both software and hardware components are discussed using an integrated approach of 2D and 3D topics, and no background in computer graphics is assumed. It has chapters on input and output devices and interaction techniques. 3.2 Complementary bibliography 3.2.1 Ian Horrocks, Constructing the User Interface with Statecharts, Addison Wesley, 1999 This book introduces an interactive software design technique to be used before coding. According to its author, it aims at filling a gap in the literature, which tackles mostly issues related to interaction design (addressing only the external behaviour of the software), software design (often not addressing the user interface design) or programming languages (explaining the syntax, generally disregarding the overall software structure). It seems interesting for practitioners; however, it may be useful to educators willing to prepare practical examples. 3.2.2 Scott Weiss, Handheld Usability, John Wiley and Sons, 2002 This book is a practical guide to the user interface design for handheld devices (personal digital assistants (PDA), mobile phones, pagers). It introduces the characteristics of these devices and their differences, and the main phases, as well as a set of guidelines for the user interface design. Additionally, it addresses issues related to prototyping and usability evaluation, which makes it useful for practitioners and for educators who wish to prepare practical classes on this subject. 3.2.3 Carolyn Snyder, Paper Prototyping—The Fast and Easy Way to Design and Refine User Interfaces, Morgan Kaufmann, 2003 This book describes paper prototypes and explains how they can be used to develop user interfaces and conduct usability tests. It explains in what conditions they should and should not be used, showing their advantages and disadvantages as well as other aspects that have to be considered when evaluating the possibility of using them as a tool. It has a pragmatic approach and can be useful to practitioners and educators. 3.2.4 Dan Diaper, Neville Stanton (eds.), The Handbook of Task Analysis for the Human–Computer Interaction, Laurence Erlbaum Associates Publishers, 2004 This book is a collection of papers addressing the fundaments of task analysis, as well as different perspectives from the information technology industry, the user and the computer. It also includes a summary of main issues in this field of human–computer interaction and points out promising ways. This book is meant for researchers and can be useful to support assignments of more advanced students. 3.2.5 Jan Borchers, A Pattern Approach to Interaction Design, John Wiley, 1996 This book proposes a pattern-based framework to the design of interactive systems. It also shows how this framework improves the communication among project team members and how it can help to convey accumulated experience to new members, future projects or in education. 3.2.6 Alan Cooper, Robert Reimann, About Face 2.0—The Essentials of Interaction Design, John Wiley, 2003 This book is a new up-to-date version of About Face, by Alan Cooper. It is intended to foster a better understanding of how users interact with interactive products (not only in desktop platforms), and introduces and explains the concept of goal-directed design. This method advocates significant changes to conventional methods of software development (including defining archetypal users—personas—using them in scenarios, and following principles of design for behaviour). The target audience includes programmers, designers, usability experts among other types of practitioners, but it is also interesting for academics. 3.2.7 Edward Tufte, Envisioning Information, Graphics Press, 1990 This is a classic reference in the area of information visualisation; it addresses mainly how to escape from the limitations of 2D representations revealing design strategies for enhancing the dimensionality and density of portrayals of information. This book includes a chapter presenting examples of bad and good colour usage in user interfaces. 3.2.8 John Carroll (ed.), HCI Models, Theories and Frameworks, Toward a Multidisciplinary Science, Morgan Kaufmann, 2003 This book presents several diverse approaches to human–computer interaction. Each chapter identifies the scientific field or tradition the approach rests upon, showing how particular principles of a discipline beyond HCI were specialized and developed, providing a foundation for describing in more operational detail how the approach is employed in HCI work. Moreover, each chapter comments on the state of the art suggesting issues that may be relevant during the next decade. Due to these characteristics, it is fundamental to the understanding of the multidisciplinary nature of human–computer interaction. 3.2.9 Julie Jacko, Andrew Sears (eds.), The Human Computer Interaction Handbook, Lawrence Erlbaum Associates, 2002 This is a collection of papers encompassing diverse aspects in human–computer interaction, addressing not only the fundamental principles, but also new developments in design and evaluation of interactive systems. It tackles issues related to different platforms (virtual, mobile, networked) and users (adults, the elderly, children, etc.), in the scope of specific applications (e-commerce, telecommunications, education, health, government, games, etc.). Due to its broad-spectrum nature this book is important to researchers, educators and practitioners. 3.3 Opinion bibliography 3.3.1 Donald Norman, The Design of Everyday Things, The MIT Press, 1998 Donald Norman examines several examples of bad design in everyday objects, which causes usage difficulties due to ignoring the user needs and cognitive psychology principles. It also shows examples of good design and provides a list of simple rules to improve the usability of objects as diverse as automobiles, computers, doors or phones. This book has become one of the main references in the area of human–computer interaction; it is indispensable to educators and advisable to interested students and practitioners. 3.3.2 Donald Norman, The Invisible Computer—Why Good Products Can Fail, the Personal Computer Is So Complex and Information Appliances Are the Solution, The MIT Press, 1998 This book shows why computers are difficult to use and why this complexity is intrinsic to their nature. According to Donald Norman, the only solution is to develop information appliances that adjust to people's needs. In order to attain this goal, the industry must change its way of developing products. Reading this book is advisable to educators as well as interested students and practitioners. 3.3.3 Jef Raskin, The Humane Interface—New Direction for Designing Interactive Systems, Addison Wesley, 2000 It begins by the following phrase: “We are oppressed by our electronic servants. This book is dedicated to our liberation”, which demonstrates the nature of this book. It analyses the cognitive basis of interaction between a user and a computer in order to ascertain why user interfaces succeed or fail. It describes some of the flaws of existing user interfaces and presents solutions to overcome them. Jeff Raskin demonstrates that many of the current paradigms are “dead ends” and asserts that new approaches are needed to make computers significantly more usable. Moreover, it provides many ideas for the design and development of user interfaces; thus, this book is useful for educators and interested practitioners. 3.3.4 Ben Shneiderman, Leonardo's Laptop—Human Needs and the New Computing Technology, The MIT Press, 2002 According to Ben Sheiderman, there have been significant changes in the way people look at computer technology; which resulted in a greater emphasis on the issues related to users; nowadays, the goal is not so much automating as it is helping people to pursue their activities. In this book, Ben Shneiderman wonders how an approach typical of Leonardo da Vinci, which integrates science, technology and art, would take us to new technologies, applications and designs in the areas of e-learning, e-business, e-healthcare and e-government. 4 Practical assignments Practical assignments are very important to help students understand and consolidate the concepts addressed in lectures; they have evolved along the years as a consequence of a change on the number of hours of the course, number and background of the students, progression of the area and changes in technology. For pedagogical and motivation reasons, it is advantageous that some of the assignments in courses offered to graduation and post-graduation students respond to real needs, be those in research or industry. Thus, whenever possible, the author tries to include one assignment that corresponds to the proposal of a user interface prototype of an application or the participation in usability tests or controlled experiments as subjects or evaluators for a real “client” external to the course. These clients have been post-graduation students, and some of the work developed in the scope of this course has been included mainly in national publications but also in international publications; an example of a user interface developed during one edition of the course can be found in Ref. [25]. In the first years, the course had no practical classes, it was offered simultaneously to graduation and post-graduation students, and was attended by a small number of students (between 12 and 16). During this phase, team assignments involving six to eight graduation students, coordinated by an M.Sc. student, were developed. The M.Sc. student acted as a project leader and had the responsibility to integrate the contributions of the other students. Meetings involving all the students and often an application domain expert were conducted by the teacher along the semester. The author considers that the experience was generally positive and that this approach is particularly interesting because the need of a good team-work strategy becomes more evident as teams get larger and the students usually work in teams of only two elements. However, it demands a tight control of the work each member is doing; otherwise the risk of getting a collection of modules that cannot be integrated is too high. Also, during this first phase of the course, some M.Sc. students performed individual practical assignments on subjects proposed by them, albeit subjected to the teacher's validation. All those assignments involved designing a user interface, implementing a prototype and evaluating it to some extent. These user interfaces were frequently intended to integrate an interactive system under development. Results obtained by some of those assignments were presented in internal publications or national conferences. Later, the course was no longer offered to M.Sc. students and started to have practical classes (2h/week during a term of approximately 13 weeks). The number of students increased to an average of 20 students per class and practical assignments became more guided. This was a direct consequence, not only of the increasing number of students, but also of a policy to oppose the generalised practice of giving mini-project-type practical assignments in most courses, which was having negative impact on students performance in the graduation project. In fact, at the time, students were devoting less attention to their graduation project, which should be their main concern throughout their graduation year. In this second phase of the course, the suggestion of Strong [26] concerning the organisation of practical assignments has been followed; the first assignments are devoted to user interface evaluation and the final assignment is devoted to design. In addition to the practical assignments concerning evaluation and design, currently, some time is also devoted to an introduction to two interesting tools for user interface prototyping: Visual Basic and HTML. After this introduction, students are advised to work on some examples. The author believes that, albeit very brief, this introduction is important to motivate the students to learn more by themselves. Table 2 shows the topics that have been addressed at the practical assignments in the last years, as well as the number of hours devoted to each subject during practical classes. All the assignments are performed by groups of two students. In the following sub-sections some more detail will be given. 4.1 Assignments on user interface evaluation Beginning by studying user interface evaluation methods has the advantage of allowing students to feel the difficulties users have when using user interfaces and motivates them to study the other topics addressed throughout the course. In fact, according to Nielsen [22], most professional programmers and students acquire a different attitude after witnessing tests where users “struggle” to use supposedly easy to use software. According to the same author, this effect is even greater if they have developed the tested software. Thus, in order to foster this change of attitude, the first practical classes are devoted to applying some evaluation methods such as observer techniques, controlled experiments and heuristic evaluation where students act not only as experimenters but also as users. In these classes S/W applications, Web sites and mobile phones are evaluated; however, other interactive systems as photocopiers or digital cameras have also been the subject of evaluation. Also the final assignment includes some evaluation. In the first practical assignment, about observation techniques, some simple tasks in the context of e-shopping are defined. Each student has to perform these tasks in a given e-shopping Web site, acting as user, while his/her colleague observes and registers some usability measures as well as the answers to a few questions to assess user satisfaction. At the end, they change roles and the new user has to perform the same tasks but at a different Web site. This has been performed using sites of two on-line shops (either two different ones or the same but in different countries) and allows the students to observe that different designs (sometimes only with slight differences) offering the same functionality and information can support user tasks in a very different way. It also introduces the students to the most commonly used user performance measures [22]. Given that this is the first practical assignment, a detailed guide describing the tasks to be performed, the measures to be used and the questions to be answered by the user, as well as how the observer has to behave and what he/she has to register is given to the students. In the second practical assignment, about controlled experiments, students are asked to participate as users in a controlled experiment. Usually it corresponds to a real experiment ran in the scope of the research work of the author or of a post-graduation student who had asked for collaboration. Students are not forced to participate as subjects, they may stay in the lab just watching; though, in general they participate and enjoy doing it. The procedure adopted is the same as in any other session of the experiment, with any other subjects; however, after the session is over, the author or the designer of the experiment explains its main aspects, including the motivation to perform the experiment, the hypothesis formulated, the independent and dependent variables and the experimental design. If some results of the statistical analysis are obtained before the end of the semester, some time is allocated to present them in another practical class. The third practical assignment is devoted to perform a heuristic evaluation of an application or Web site; the students are allowed to propose the subject of evaluation. Each student conducts a general appreciation of the application or site in order to understand its functionality and overall organisation and then analyse it in more detail using the 10 usability heuristics of Nielsen [22] and classifying the severity of each usability problem according to the severity rate proposed in Ref. [27]. In the end, both students confront their findings and prepare a report. On the next week, each group makes an oral presentation to the class describing their main findings. No guide is provided for this assignment; in order to prepare it, students are advised to study [27]. The fourth practical assignment consists in designing a simple mobile phone usability test. Students are advised to read about usability tests and a document concerning the Common Industry Format Usability Test Reporting (that became an ANSI-354 standard in 2001) [28] is provided. Each group proposes the type of users, the tasks and the measures to be used; a general discussion is conducted and a joint proposal is written on the blackboard. Finally, each group uses a mobile phone model unknown to one of the students, who performs part of the tasks, while the other observes and registers some usability measures. In the end, each group reports informally to the class their main findings. 4.2 Final assignment (on user interface design) The final assignment involves the requirement analysis, proposal of a conceptual model, implementation of part of a prototype and some evaluation of a user interface. All this should be performed following a user-centred design methodology. Students are allowed to present their own proposals or they may choose one of two different given problems. If they want to propose their own subject they have to submit one page stating the objectives and the reason why they are interested in working on that subject. In general, most students end up choosing one of the given problems; however, some submit proposals concerning a user interface they need to develop in the scope of their graduation projects. In 2004/2005, the final assignment consisted in proposing the user interface for either a Web application that would allow to buy and/or reserve meals at any of the University restaurants and canteens or another that would allow students to register in exams. Only a general description of the intended functionality is given so that students can propose the specific functionality based on their work on the requirements. At the first practical class devoted to this assignment, the teacher briefly describes a usability engineering lifecycle as proposed in Ref. [12] with all its phases, and advises students to follow it. The following classes are devoted to work on the problem and to discuss possible solutions with the teacher and the class. Finally, in the last class, each group must make a demonstration of their work. Students may implement the prototype using whichever tool or language they prefer, as long as it can be demonstrated in the lab. 4.3 Other activities In the scope of this course, students have also been collaborating with other people in their research or development work. Besides participating as users in controlled experiments as mentioned above, they performed heuristic evaluations of Web sites and applications and participated as observers in usability tests. Examples of such collaboration are the usability test and the controlled experiment described in Refs. [29,30]. The author considers that this collaboration has been very constructive given that it allows the students, on the one hand, to be exposed to the use of the evaluation methods in real circumstances and, on the other hand, to better understand the importance of correctly dealing with people that collaborate as users in usability tests and controlled experiments. 4.4 Writing reports Writing a report is for most students a difficult task; therefore, an explanation of how to do it is given in class before the first report is due. In order to support this explanation, a simple document is given; this document contains a straightforward description of the various parts a report should have, and some guidelines on style, format and using references. Students are also advised to browse a book published by IEEE that addresses the subject of writing and speaking in the technology professions [31]. Students are required to read each others reports before they hand them to the teacher, which corrects them and shows her corrections and comments to each group. All the groups are given the opportunity to improve and re-submit the report concerning the heuristic evaluation (the first one they have to produce during the semester). This procedure has been followed in the last years and the author has noticed a significant increase in the average quality of the reports. 5 Discussion and conclusions In this paper, an introductory human–computer interaction course offered since 1993 at the University of Aveiro, to Electrical and Computer Engineering students, is presented. Since technology and human–computer interaction are evolving rapidly, a continuous effort is needed to maintain an up-to-date programme and bibliography and practical assignments that help students to consolidate the concepts acquired along the course, and are adapted to the current technology. In fact, maintaining such a course up-to-date is a hard task. Actually, if on one hand a lot of the theory seems already reasonably stable, as usability principles that depend more on human nature than on technology, on the other hand, in order to foster the understanding of that theory, it is necessary to present instances of good practice, which of course depend heavily on current technology; however, technology evolves very rapidly and students are familiar with it. Therefore, to stay current and maintain an up-to-date “look” of the course, an educator in this area must not only be aware of the new developments in the field but also often change or adapt the examples. The same concern is valid for practical assignments. Recently, in the course described in this paper, students have been performing, in groups of two elements, two types of practical assignments: first evaluation and latter design of user interfaces. While evaluation assignments have been addressing desktop applications, Web sites, handheld devices or consumer electronics products, the design assignment has been involving only the design of a user interface and implementation of a prototype for desktop or Web applications. However, the author is considering the possibility of including handheld devices, namely PDAs, in the design assignment, given that these platforms are becoming increasingly important. The author has lectured computer graphics, visualisation and human–computer interaction as separate courses; yet she is convinced that, as these subjects are becoming ever more intertwined in their practice as well as in research, it may make sense to lecture them (at least at introductory level) together, bringing in topics traditionally addressed in different courses. Thus, in recent years the introductory course offered to the students of computer engineering at the University of Aveiro, follows this model, introducing not only the basic concepts of human–computer interaction, as described, but also topics of computer graphics; other educators have recently adopted this approach [32]. Currently, a post-graduation in computer graphics is offered at the University of Aveiro, which includes this course as an elective, since basic knowledge in the area of human–computer interaction is considered very important as a support for areas such as data and information visualisation. Acknowledgement The author is grateful to her colleague Dr. Joaquim Madeira for his suggestions and comments to this manuscript. References [1] D. Norman The invisible computer 1999 MIT Press Cambridge, MA [2] ACM SIGCHI. Curricula for human–computer interaction. ACM SIGCHI; 1996. [visited in October 2004] [3] I. Sommerville Software engineering 6th ed 2001 Addison Wesley Reading, MA [4] S. Hu A wholesome ECE education IEEE Transactions on Education 46 4 2003 444 451 [5] D. Evans S. Goodnick R. Rodel ECE curriculum in 2013 and beyond: vision for a metropolitan public research university IEEE Transactions on Education 46 4 2003 420 428 [6] A. McGettrick M. Theys D. Soldan P. Srimam Computing engineering curriculum in the new millenium IEEE Transactions on Education 46 4 2003 456 462 [7] ACM/IEEE Joint Task Force. Computing curricula 2001—computer science. ACM Journal of Educational Resources in Computing 1(3). [8] ACM/IEEE Joint Task Force. Computing curricula—computing engineering. [visited in October 2004]. [9] Sousa Santos B. An Introductory course on human computer interaction at the University of Aveiro. In: Adjunct proceedings of 10th human–computer interaction international HCII’2003, Greece, June 2003. p. 231–2. [10] A. Dix J. Finlay G. Abowd B. Russell Human computer interaction 2nd ed 1998 Prentice Hall Englewood Cliffs, NJ [11] D. Mayhew Principles and guidelines in software user interface Design 1992 Prentice Hall Englewood Cliffs, NJ [12] D. Mayhew The usability engineering lifecycle—a practitioners handbook for user interface design 1999 Prentice Hall Englewood Cliffs, NJ [13] J. Blinn User interface stories from the real world IEEE Computer Graphics and Applications 2005 92 93 [14] J. Preece Y. Rogers H. Sharp D. Benyon S. Holland T. Carey Human computer interaction 1994 Addison Wesley Reading, MA [15] B. Shneiderman Designing the user interface, strategies for effective human–computer interaction 3rd ed 1998 Addison Wesley Reading, MA [16] V. Cerf When they’re everywhere P. Denning R. MetCalfe Beyond calculation, the next fifty years of computing 1997 ACM New York 33 42 [17] W.M. Newman M.G. Lamming Interactive system design 1995 Addison Wesley Reading, MA [18] D. Hearn M.P. Baker Computer graphics with open GL 3rd ed 2004 Prentice Hall Englewood Cliffs, NJ [19] P. Neumann The problems and potentials of voting systems Communications of the ACM 47 10 2004 28 30 [20] E. Tufte Envisioning information 1990 Graphics Press [21] I. Horrocks Constructing the user interface with statecharts 1999 Addison Wesley Reading, MA [22] J. Nielsen Usability engineering 1993 Academic Press New York [23] A. Dix J. Finlay G. Abowd R. Beale Human–computer interaction 3rd ed 2004 Prentice Hall Englewood Cliffs, NJ [24] A. Sullivan 3D Deep, new displays render images you can almost reach out and touch IEEE Spectrum 2004 30 35 [25] C. Ferreira B. Sousa Santos M.E. Captivo J. Clímaco C.C. Silva Multi-objective location of unwelcome or central facilities involving environmental aspects—a prototype of a decision support system JORBEL, Belgian Journal of Operations Research, Statistics and Computer Science 36 1–2 1996 117 159 [26] G. Strong New directions in human computer interaction education Research and Practice 1994 [visited in October 2004] [27] Nielsen J. [visited in October 2004]. [28] ANSI-354, Common industry format usability test reporting. [visited in October 2004]. [29] B. Sousa Santos F. Zamfir C. Ferreira Ó. Mealha J. Nunes Visual application for the analysis of Web-based information systems usage: a preliminary usability evaluation In: Proceedings of IEEE conference on information visualization IV’04, London 2004 812 818 [30] Silva S, Sousa Santos B, Madeira J, Ferreira C. Comparing three methods for simplifying mesh models of the lungs: an observer test to assess perceived quality. In: Galloway R, Cleary K, editors. Medical imaging 2005: visualization, image guided procedures and display. Proceedings of SPIE. vol. 5744, San Diego, February 2003. p. 612–20. [31] D. Beer Writing and speaking in the technology profession: a practical guide 1992 IEEE Press New York [32] E. Paquette Computer graphics education in different curricula: analysis and proposal for courses Computers & Graphics 29 2005 246 255 "
    },
    {
        "doc_title": "POLYMECO - A polygonal mesh comparison tool",
        "doc_scopus_id": "33749052275",
        "doc_doi": "10.1109/IV.2005.98",
        "doc_eid": "2-s2.0-33749052275",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Mesh analysis",
            "Mesh processing",
            "Polygonal meshes"
        ],
        "doc_abstract": "Polygonal meshes are used in many areas to model different objects and structures. Depending on their applications, they sometimes have to be processed to, for instance, reduce their complexity (simplification). This mesh processing introduces error, whose evaluation is important when choosing the kind of processing that is to be done for a particular application. Although some mesh comparison tools are described in the literature, little attention has been given to the way results are presented. A tool is presented which enhances the way users perform mesh analysis and comparison, by providing an environment where several visualization options are available and can be used in a coordinated way. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of methods for the simplification of mesh models of the lungs using quality indices and an observer study",
        "doc_scopus_id": "33746134806",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33746134806",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Lungs",
            "Mesh models",
            "Normal flipping criterion",
            "Quality indices"
        ],
        "doc_abstract": "This paper presents a comparison among mesh simplification methods performed through quality indices and a controlled experiment involving 32 observers. The simplification methods: OpenMesh, OpenMesh with a normal flipping criterion and QSlim, were compared at two simplification levels: 50% and 20% of the original number of faces. Results obtained using the quality indices and the controlled experiment were compared and show that some quality indices can be used as reasonable estimators of the observers' performance in specific circumstances.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing three methods for simplifying mesh models of the lungs: An observer test to assess perceived quality",
        "doc_scopus_id": "24644483667",
        "doc_doi": "10.1117/12.593906",
        "doc_eid": "2-s2.0-24644483667",
        "doc_date": "2005-09-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Evaluation",
            "Interaction",
            "Lungs",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "Meshes are currently used to model objects, namely human organs and other structures. However, if they have a large number of triangles, their rendering times may not be adequate to allow interactive visualization, a mostly desirable feature in some diagnosis (or, more generally, decision) scenarios, where the choice of adequate views is important. In this case, a possible solution consists in showing a simplified version while the user interactively chooses the viewpoint and, then, a fully detailed version of the model to support its analysis. To tackle this problem, simplification methods can be used to generate less complex versions of meshes. While several simplification methods have been developed and reported in the literature, only a few studies compare them concerning the perceived quality of the obtained simplified meshes. This work describes an experiment conducted with human observers in order to compare three different simplification methods used to simplify mesh models of the lungs. We intended to study if any of these methods allows a better-perceived quality for the same simplification rate. A protocol was developed in order to measure these aspects. The results presented were obtained from 32 human observers. The comparison between the three mesh simplification methods was first performed through an Exploratory Data Analysis and the significance of this comparison was then established using other statistical methods. Moreover, the influence on the observers' performances of some other factors was also investigated.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quality evaluation in medical visualization: Some issues and a taxonomy of methods",
        "doc_scopus_id": "23244458625",
        "doc_doi": "10.1117/12.594549",
        "doc_eid": "2-s2.0-23244458625",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Medical data visualization",
            "Quantitative evaluation",
            "Taxonomy",
            "Tri-dimensional imaging"
        ],
        "doc_abstract": "Among the several medical imaging stages (acquisition, reconstruction, etc.), visualization is the latest stage on which decision is generally taken. Scientific visualization tools allow to process complex data into a graphical visible and understandable form, the goal being to provide new insight. If the evaluation of procedures is a crucial issue and a main concern in medicine, paradoxically visualization techniques, predominantly in tri-dimensional imaging, have not been the subject of many evaluation studies. This is perhaps due to the fact that the visualization process integrates the Human Visual and Cognitive Systems, which makes evaluation especially difficult. However, as in medical imaging, the question of quality evaluation of a specific visualization remains a main challenge. While a few studies concerning specific cases have already been published, there is still a great need for definition and systemization of evaluation methodologies. The goal of our study is to propose such a framework, which makes it possible to take into account all the parameters taking part in the evaluation of a visualization technique. Concerning the problem of quality evaluation in data visualization in general, and in medical data visualization in particular, three different concepts appear to be fundamental: the type and level of components used to convey to the user the information contained in the data, the type and level at which evaluation can be performed, and the methodologies used to perform such evaluation. We propose a taxonomy involving types of methods that can be used to perform evaluation at different levels.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual application for the analysis of web-based information systems usage: A preliminary usability evaluation",
        "doc_scopus_id": "4644357302",
        "doc_doi": null,
        "doc_eid": "2-s2.0-4644357302",
        "doc_date": "2004-10-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Information visualization",
            "Usability evaluation",
            "Web site structure"
        ],
        "doc_abstract": "This paper presents a general description of the methods used in the on-going evaluation of a Visualizer, which is a sub-component of the Web Log Visual Analysis System. We are trying to evaluate some aspects of the user interface and visualization techniques implemented as part of the prototype. Observation and querying techniques were used with two types of users. A general description of those users and methods is presented. Preliminary results were encouraging and provided new ideas and information that will, eventually, allow a more complete and formal evaluation of our application.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrated visualization schemes for an information and communication web log based management system",
        "doc_scopus_id": "4644325622",
        "doc_doi": null,
        "doc_eid": "2-s2.0-4644325622",
        "doc_date": "2004-10-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Information and communication management",
            "Information visualization",
            "Interface design",
            "Web logs"
        ],
        "doc_abstract": "This paper presents a set of visualization methods incorporated in a system for internal technologically mediated communication and information management. This system, which assumes an organizational context and an intranet built on World Wide Web technologies and services, was developed in order to allow a manager to evaluate the efficiency, effectiveness and adequacy of the intranet to the organization's mission. The visualization methods and the raw information that is gathered are explained. Some examples are shown and discussed alongside some preliminary research results. The system user interface, including visually correlated visualization strategies, is also briefly presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantitative evaluation of a pulmonary contour segmentation algorithm in x-ray computed tomography images",
        "doc_scopus_id": "3442887729",
        "doc_doi": "10.1016/j.acra.2004.05.004",
        "doc_eid": "2-s2.0-3442887729",
        "doc_date": "2004-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Rationale and objectives Pulmonary contour extraction from thoracic x-ray computed tomography images is a mandatory preprocessing step in many automated or semiautomated analysis tasks. This study was conducted to quantitatively assess the performance of a method for pulmonary contour extraction and region identification. Materials and methods The automatically extracted contours were statistically compared with manually drawn pulmonary contours detected by six radiologists on a set of 30 images. Exploratory data analysis, nonparametric statistical tests, and multivariate analysis were used, on the data obtained using several figures of merit, to perform a study of the interobserver variability among the six radiologists and the contour extraction method. The intraobserver variability of two human observers was also studied. Results In addition to a strong consistency among all of the quality indexes used, a wider interobserver variability was found among the radiologists than the variability of the contour extraction method when compared with each radiologist. The extraction method exhibits a similar behavior (as a pulmonary contour detector), to the six radiologists, for the used image set. Conclusion As an overall result of the application of this evaluation methodology, the consistency and accuracy of the contour extraction method was confirmed to be adequate for most of the quantitative requirements of radiologists. This evaluation methodology could be applied to other scenarios. © AUR, 2004.",
        "available": true,
        "clean_text": "serial JL 272938 291210 291703 31 Academic Radiology ACADEMICRADIOLOGY 2004-07-28 2004-07-28 2010-11-13T22:12:05 S1076-6332(04)00281-8 S1076633204002818 10.1016/j.acra.2004.05.004 S300 S300.2 FULL-TEXT 2015-05-15T05:02:15.579713-04:00 0 0 20040801 20040831 2004 2004-07-28T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast doctopic primabst pubtype ref alllist content subj ssids 1076-6332 10766332 11 11 8 8 Volume 11, Issue 8 10 868 878 868 878 200408 August 2004 2004-08-01 2004-08-31 2004 converted-article fla Copyright © 2004 AUR. Published by Elsevier Inc. All rights reserved. QUANTITATIVEEVALUATIONAPULMONARYCONTOURSEGMENTATIONALGORITHMINXRAYCOMPUTEDTOMOGRAPHYIMAGES1 SOUSASANTOS B Materials and methods Quality assessment strategies Quantitative evaluation of the performance Interobserver variability Intraobserver variability Test dataset Hand-outlined contours References contour obtained from the hand-drawn contours Comparing contours Figures of merit Statistical methods Results Study of the interobserver and intraobserver variability involving our algorithm and two expert radiologists Study of the interobserver variability through direct comparison among the algorithm and six expert radiologists Study of the interobserver variability using a reference contour Conclusions Acknowledgements References ROBB 2003 756 760 W BRINK 1994 887 893 J LI 2003 255 265 B BROWN 1997 828 839 M SONKA 1996 314 326 M DURYEA 1995 183 191 J PARKER 1980 291 295 R PROCEEDINGSPHYSICALASPECTSMEDICALIMAGING MEASUREMENTBASICCTDATA HU 2001 490 498 S HASEGAWA 1998 241 250 A SILVA 2000 583 598 J PROCEEDINGSVIBEROAMERICANSYMPOSIUMPATTERNRECOGNITIONSIARP2000 LUNGSEGMENTATIONMETHODSINXRAYCTIMAGES SILVA 2001 216 224 A SPIEMEDICALIMAGING2001 FASTPULMONARYCONTOUREXTRACTIONINXRAYCTIMAGES CHALANA 1997 642 652 V BOWYER 1999 567 606 K HANDBOOKMEDICALIMAGINGVOL2MEDICALIMAGEPROCESSINGANALYSISCAPX VALIDATIONMEDICALIMAGEANALYSISTECHNIQUES BLAKE 1998 A ACTIVECONTOURS GUNN 1997 63 68 S KASS 1988 321 331 M LIANG 1999 933 940 J INTERNATIONALCONFERENCECOMPUTERVISIONVOLUME2SEPTEMBER2025 UNITEDSNAKES YEZZI 1997 199 209 A SIVARAMAKRISHNA 2001 250 256 R FERREIRA 2003 347 358 C SPIEMEDICALIMAGING2003 COMPARISONASEGMENTATIONALGORITHMSIXEXPERTIMAGIOLOGISTSINDETECTINGPULMONARYCONTOURSXRAYCTIMAGES WAGNER 2003 213 224 R SPIEMEDICALIMAGING2003 CONTEMPORARYISSUESFOREXPERIMENTALDESIGNINASSESSMENTMEDICALIMAGINGCOMPUTERASSISTSYSTEMS ALTMAN 1999 D PRACTICALSTATISTICSFORMEDICALRESEARCH ABDOU 1979 753 763 I SACHS 1984 L APPLIEDSTATISTICSAHANDBOOKTECHNIQUES HOAGLIN 1983 D UNDERSTANDINGROBUSTEXPLORATORYDATAANALYSIS 1999 STATSOFTSTATISTICARELEASE55FORWINDOWSSTATSOFTINC GIBBONS 1997 J NONPARAMETRICMETHODSFORQUANTITATIVEANALYSIS HAIR 1995 J MULTIVARIATEDATAANALYSISREADINGS BUVAT 1999 1466 1477 I SPIEMEDICALIMAGING1999 NEEDDEVELOPGUIDELINESFOREVALUATIONMEDICALIMAGEPROCESSINGPROCEDURES SOUSASANTOSX2004X868 SOUSASANTOSX2004X868X878 SOUSASANTOSX2004X868XB SOUSASANTOSX2004X868X878XB item S1076-6332(04)00281-8 S1076633204002818 10.1016/j.acra.2004.05.004 272938 2010-12-23T00:07:40.847864-05:00 2004-08-01 2004-08-31 true 292261 MAIN 11 55545 849 656 IMAGE-WEB-PDF 1 si5 1177 40 193 si4 722 39 153 si3 793 17 188 si2 837 48 140 si1 1095 48 201 gr1 2340 93 87 gr1 68862 534 499 gr2 882 34 125 gr2 6262 137 499 gr3 1968 93 121 gr3 17616 385 499 gr4 3273 93 122 gr4 19776 288 376 gr5 1504 72 125 gr5 7590 218 376 gr6 1705 72 125 gr6 8884 218 376 gr7 1345 73 125 gr7 6598 220 376 gr8 1574 78 125 gr8 14367 310 498 XACRA 586 S1076-6332(04)00281-8 10.1016/j.acra.2004.05.004 AUR Original investigations Figure 1 Images (a, c) and corresponding contours detected by six radiologists (b, d). Figure 2 Definition of the auxiliary contour to obtain pairs of corresponding points on two contours A and B. P and Q are matching points on the contours under comparison. Figure 3 Box-plots for the comparison of the contours using (a) FPratt, (b) mean distance, and (c) angle θ, involving our algorithm and two radiologists. Figure 4 Box-plots comparing contours detected by the algorithm (a) and all the radiologists (dr) using FPratt. Figure 5 Box-plots corresponding to the comparison (to the reference) of the contours detected by each detector (dr1 to dr6 and the algorithm AlG) using the maximum distance figure of merit. Figure 6 Box-plots corresponding to the comparison (to the reference) of the contours detected by each radiologist, in the first time (dr1_T1 to dr6_T1), two radiologists in the second time (dr1_T2 and dr2_T2) and the algorithm (A) using FPratt. Figure 7 Dendogram plot (clustering analysis) showing all the radiologists in the first time (dr1_T1 to dr6_T1), two radiologists in the second time (dr1_T2 and dr2_T2), and the algorithm (A) using FPratt. Figure 8 Correspondence analysis plot showing all the radiologists in the first time (dr1_T1 to dr6_T1), two radiologists in the second time (dr1_T2 and dr2_T2), and the algorithm (alg) using FPratt. Table 1 Friedman ANOVA for the Comparison of the Contours using FPratt- H = 157.67, (P < .000001) and the Null Hypothesis is Rejected Variable Sum of Ranks DR2T1 144 DR1DR2T2 186 DR2T2 203 DR1T12 272 DR1DR2T1 284 DR1T2 293 DR1T1 377 DR2T12 401 Original investigations Quantitative evaluation of a pulmonary contour segmentation algorithm in x-ray computed tomography images1 Beatriz Sousa Santos PhD a b * Carlos Ferreira PhD c d José Silvestre Silva MSc a e Augusto Silva PhD a b Luı́sa Teixeira MD f a Departamento de Electrónica e Telecomunicações, Portugal b Instituto de Engenharia Electrónica e Telemática de Aveiro, (B.S.S., A.S.), Portugal c Departamento de Economia, Gestão e Engenharia Industrial, Universidade de Aveiro, Portugal d Centro de Investigação Operacional, Universidade de Lisboa, Portugal e Departamento de Fı́sica, Faculdade de Ciências e Tecnologia, Universidade de Coimbra, Portugal f Serviço de Imagiologia, Hospitais da Universidade de Coimbra, Portugal * Address correspondence to B.S.S. DET Departamento de Electronica e Telecomunicaçoes, Universidade de Aveiro, 3810 Aveiro, Portugal Rationale and objectives Pulmonary contour extraction from thoracic x-ray computed tomography images is a mandatory preprocessing step in many automated or semiautomated analysis tasks. This study was conducted to quantitatively assess the performance of a method for pulmonary contour extraction and region identification. Materials and methods The automatically extracted contours were statistically compared with manually drawn pulmonary contours detected by six radiologists on a set of 30 images. Exploratory data analysis, nonparametric statistical tests, and multivariate analysis were used, on the data obtained using several figures of merit, to perform a study of the interobserver variability among the six radiologists and the contour extraction method. The intraobserver variability of two human observers was also studied. Results In addition to a strong consistency among all of the quality indexes used, a wider interobserver variability was found among the radiologists than the variability of the contour extraction method when compared with each radiologist. The extraction method exhibits a similar behavior (as a pulmonary contour detector), to the six radiologists, for the used image set. Conclusion As an overall result of the application of this evaluation methodology, the consistency and accuracy of the contour extraction method was confirmed to be adequate for most of the quantitative requirements of radiologists. This evaluation methodology could be applied to other scenarios. Keywords Quantitative evaluation computed tomography (CT) pulmonary segmentation interobserver and intraobserver variability We have reached a point at which computed tomography (CT) images can be reconstructed faster than they can be read. This fact encourages software developers to design programs that will aid radiologists in the reading of CT images and in diagnosing conditions on the basis of CT findings (1). Segmentation often occurs as a preprocessing step of more global image analysis tasks, as is the case of computer-aided analysis of pulmonary x-ray tomograms (2), where many analytic procedures start by correctly identifying the pulmonary regions (3–6). Most algorithms for the segmentation of pulmonary regions are based on intensity discrimination within the Hounsfield scale (7–9); however this task may become very complex because of the presence of spurious structures within the same scale range or the visual merging of the pulmonary regions themselves. In previous works (10,11) we presented algorithms designed to cope with these difficulties, which generate contours with a variable degree of similarity to those provided by radiologists. A quantitative evaluation of the performance of these algorithms is crucial before their clinical use can be considered. Yet, the performance evaluation of segmentation algorithms in medical imaging is recognized as a difficult problem; actually, if one can find in the literature a significant number of contributions concerning the overall segmentation problem by itself, the same is not true when looking for quality and effectiveness assessments performed in some systematic way (12) and having a practical value (13). This evaluation encounters the first great obstacle: the fact that the ground truth is unknown (13) (ie, it is not possible to identify the real contour corresponding to a given image). This problem is often circumvented using the contour resulting from manually tracing the object boundary by a knowledgeable human as a surrogate of that truth. However, not only will contours drawn by two radiologists be different (interobserver variability), but there will also not be agreement between contours drawn by the same radiologist at different occasions (intraobserver variability). These two types of variability have to be taken into account in the performance evaluation of segmentation algorithms; we will have to compare this performance with the performance of several radiologists in some statistically supported manner. In an earlier work (11) we verified that a greater similarity existed between the contours produced by our algorithm and the contours drawn by two expert radiologists, than between the contours drawn by the same two radiologists. This meant that the interobserver variability between our algorithm and any of the two radiologists was less than the interobserver variability between the two radiologists. To investigate if this was specific for those two radiologists, or if it was more general, we have performed a study including six radiologists from different hospitals. To further investigate this issue, we have considered the study of the intraobserver variability relevant; in this respect our algorithm has a clear advantage because its intraobserver variability is zero. Still, the comparison of the interobserver variability between our algorithm and each radiologist to hers/his intraobserver variability could provide interesting additional information on the performance of the algorithm. While other authors have proposed pulmonary segmentation algorithms and have evaluated them (4,8), they have not compared their performance as contour detectors with as many radiologists, nor have they used such a statistically based method as we have used in this study. Materials and methods Quality assessment strategies It is common to treat the physician ground truth as unquestionable, and assume it as a relatively error-free gold standard; however, there is some level of variability in the specification of the ground truth and it is important to have an estimate of this level. This type of variability is an important concern in determining the appropriate criteria for matching a detected contour to a ground truth contour (13). Quantitative evaluation of the performance of segmentation algorithms in medical imaging has been recognized as an important problem. However, many of the evaluation studies that have been carried out did not use a large enough dataset, real images, convenient performance metrics, appropriate statistical methods, or a suitable ground truth. Thus, they cannot be considered correct or complete. Several methodologies have been proposed to perform this evaluation appropriately. The Handbook of Medical Imaging (13) presents a thorough overview of the field. Chalana and Kim (12) also present a concrete approach to segmentation performance assessment through contour comparison. Quantitative evaluation of the performance As mentioned previously, the ideal way of evaluating the performance of our segmentation algorithm would be to compare the contours detected on a valid test dataset with the “real contours” corresponding to each image. However, as we have seen, there are no such real contours. Several expert radiologists will detect different contours on the same image (see Fig 1b); also, each expert radiologist will detect on the same image, at different times, slightly different contours, unlike our algorithm, which always detects the same contours on the same image (its variability is 0 and it does not depend on any seed points introduced by a human observer, as other pulmonary segmentation algorithms (14–18)). This intraobserver variability can be used as a “variability quantum”; it gives an idea of the level of variability that has to be expected and thus can be acceptable to exist between any two contour detectors (algorithm or human). Therefore, comparing the variability between our algorithm and each radiologist with the intraobserver variability of any expert could work as an “acceptability measure” of that variability. As a consequence of the intraobserver and interobserver variability, the manually drawn contours can be considered as a collection of ground truths, all of them equally acceptable. To circumvent this problem we have performed two studies (using different methods) to compare the behavior of our algorithm, as a contour detector, with the behavior of a reasonable number of expert radiologists. These studies involve the assessment of the interobserver variability among a number of “contour detectors”: several humans and one automated (our algorithm). The rationale for this study was that, if the interobserver variability between the algorithm and any of the radiologists is similar in magnitude to the interobserver variability between any two radiologists, then the difference between the algorithm and the radiologists, as contour detectors, could be considered not significant. This rational is similar to the one behind the study by Sivaramakrishna et al (19) to validate a segmentation algorithm of mammographic images, which also seems comparable to our case. Interobserver variability In our first study concerning interobserver variability we directly compared the contours produced by all detectors (algorithm vs all radiologists and every radiologist vs all the others and algorithm). In a subsequent study we compared each detector with a reference contour (surrogate ground truth) obtained from the hand-drawn contours, as described by Ferreira et al (20). To perform these studies we asked six experienced radiologists, from three different hospitals, to draw contours of the pulmonary regions on the chosen images. This number of radiologists seemed reasonable for such a study, taking into consideration that they have been trained and work at three different hospitals. Moreover, it would be difficult to obtain the collaboration of more radiologists. Intraobserver variability To assess intraobserver variability, we asked two radiologists to hand-draw the contours on the same set of images twice, without telling them that they had already drawn contours on those images. We chose the youngest radiologist and the head of the CT department who was responsible for thoracic radiology at the University Hospital, because these radiologists have a significant difference in years of experience. This choice was made in the hope of obtaining two significantly different values of intraobserver variability (which would probably not be the case if the two radiologists had approximately the same experience). The time elapsed between the delineation of the two contours on the same image by the same radiologist was at least 1 month (which agrees with the proposal of Wagner et al (21)]) to minimize the effect of the recollection of having drawn the previous contours. Test dataset The proper choice of the used dataset is very important; a poor selection of either the number of images or the method to select these images can jeopardize the validity of the evaluation procedure. We used 30 512 × 512 images (N = 60 contours) selected using a pseudorandom generator from a set of 253 images that had not been used to develop the algorithm. These images were all the images that could be used to support diagnosis corresponding to exams of eight patients collected at the Radiology Department of the University Hospital in Coimbra, independently of their pathologies. While the used dataset contained images corresponding to different pulmonary levels, which increased variability, using images from a greater number of patients would probably increase case variability. We used the power of a hypothesis test to calculate the sample size, N, of the test dataset, specifying the smallest difference that would be worthwhile to detect. This means, according to Altman (22), trying to make “clinical” importance and statistical significance agree. As a first approach, we hoped to be able to detect a difference of 1 standard deviation. We set the power (1-β) at 90% and chose a 1% significance level (α); using the nomogram for calculating sample size (22), this gives a total sample of N = 60. Hand-outlined contours Our radiologists manually outlined all the contours on transparent sheets superimposed on quality printings of the test images working independently from each other and (as much as possible) in the same way and on the same conditions. The obtained contours were digitized and processed to identify the contours of left and right lungs. This identification is performed computing the image Radon transforms for 0° and 90°, estimating the center of each lung from the maximum values of these two transforms. Applying a morphologic filling starting from the center of one lung and a second filling starting from any point external to the lungs, we obtained an image containing the filled area of the other lung. The contour of the lung was then easily obtained. Erosion was applied to obtain a thinner version of each of the contours (20). We have chosen this method as a compromise between feasibility to the radiologists and acceptable accuracy. References contour obtained from the hand-drawn contours In the last study of interobserver variability, we compared all contours to reference contours obtained from the six contours detected by the radiologists on each image, as described by Ferreira et al (20). For most images having diagnostic value, the contours detected by all the radiologists are only slightly different and thus using a kind of “average” contour seemed an acceptable surrogate to “ground truth” (Fig 1b); however, in particular regions of a few images of the data set, affected by partial volume effect or motion artifacts, the six radiologists detected contours that seem to correspond to the use of different segmentation criteria (Fig 1d); in this case an “average” contour does not make sense as “ground truth” and a different approach should be used. This needs further investigation; however, the impact on the results of this study is not expected to be significant because of the small number of images and reduced zones where this fact was observed in the used data set. Comparing contours The comparison between any two contours was accomplished in two different ways: one based on the local distances between contours and the other exploring a similarity measure between the image masks (binary images containing the pulmonary areas defined by the contours). The computation of distances between contours implies defining pairs of matching points on both contours. To find these pairs of points we used an auxiliary contour as shown in Figure 2. Differences between the contours were quantified using the Euclidean distances measured between corresponding points (11). Figures of merit The values of the computed distances between the contours allow a localized and accurate quantification of their differences, easily assessable through simple visualization techniques. However, we consider it fundamental to use global quality figures of merit, which facilitate a comprehensive comparison. Thus, several figures of merit, based on the computed distances, were used as performance measures: the Pratt figure of merit F Pratt (23): (1) F Pratt = 1 N ∑ i=1 N 1 1+α×di 2 the Mean Distance: (2) dmean= 1 N ∑ i=1 N di the Maximum Distance: (3) d max =max(di)1≤i≤N and the number of distances greater than 5 pixels (approximately 1% error for a resolution of 512 × 512): (4) n>1%= ∑i=1 N mi N (where mi = 0 if di ≤ 5 and mi = 1 if di > 5). The Pratt figure of merit gives a general impression of the distances between contours; it is a relative measure and varies in the interval [0,1] where “1” means a complete match of the contours. In our case, α, which is a normalization parameter related to the size of the contours, was chosen to be 1/9 so that if all the distances di are equal to 3 pixels, F Pratt will have a value of 0.5. The value of 3 pixels was chosen to produce a scale that allows enough discrimination among the contours drawn by the radiologists. The mean distance also gives an integrated view of the distances between contours, while the maximum distance gives a worst case view. Finally, the number of distances greater than 1% (5 pixels in our case) provides information on the number of relevant errors and thus complements the information obtained from the previous indexes. Another figure of merit was computed based on the similarity between the two binary images, A and B, including the areas defined by the pulmonary contours. This simple measure of similarity may be defined by: (5) θ=cos −1 A·B ‖A‖·‖B‖ where “.” and “‖‖” denote the usual inner product and norm of vectors. In a Hilbert space context, θ is the angle between two binary image vectors A and B. The correct identification of the measurement scale (24) is an important issue concerning the information provided by these figures of merit and the statistical methods that can be used. In this respect, the Pratt and θ figures of merit are measured on an ordinal scale whereas the mean error and the number of distances greater than 5 pixels are measured on a ratio scale. Statistical methods As a first step in the analysis of the data obtained from the comparison among contours using all figures of merit, we performed an exploratory data analysis (25); this analysis provided an overview of the structure of the data (showing the amplitudes, asymmetries, location, possible outliers, etc) and also some clues to the type of statistical tests to be used to test our hypothesis. The software used was Statistica (Statsoft, Tulsa, OK) (26). Because the sample set did not correspond to independent experiments, nor did the data have a normal distribution, a nonparametric test was used (27). We also used multivariate data analysis (28) to assess if our algorithm is generally comparable to the six human observers as a contour detector on the used image data set. Results Study of the interobserver and intraobserver variability involving our algorithm and two expert radiologists The two radiologists were called dr1 and dr2, and the two contour drawing moments were called t1 and t2. Figure 3 shows the box-plots and corresponding median and quartile values for the comparison between the contours detected by our algorithm and the two radiologists in the two moments, using different figures of merit. The box-plots can be interpreted in the following way: drit1-comparison between the contours detected by dri at moment t1 to the contours detected by our algorithm, on the selected set of images; drit12-comparison between the contours detected by dri at moments t1 and t2, on the same images. According to this notation: dr1t12 and dr2t12 represent the intraobserver variability of experts dr1 and dr2; dr1t1, dr1t2, dr2t1 and dr2t2 represent the interobserver variability between our algorithm and each radiologist in each moment; dr1dr2t1 and dr1dr2t2 represent the interobserver variability between both radiologists at moments t1 and t2, respectively. All these can be compared in Figure 3 through several figures of merit: FPratt, mean distance, and angle θ. Observing the box-plots corresponding to FPratt we note that: 1. At moment t1, dr1 is more similar to our algorithm than to dr2, because the median value of dr1t1 (median, 0.81) is higher (ie, better) than the median value of dr1dr2t1 (median, 0.75); this was confirmed using a nonparametric test for the equality of the median, the Wilcoxon test (28), which rejected the null hypothesis (P < .00004). Also the range of the values is smaller for dr1t1 than for dr1dr2t1. Both results suggest that the interobserver variability between the two radiologists is higher than the variability between dr1 and our algorithm; 2. At moment t2, both dr1 and dr2 are more similar to our algorithm than to each other; for instance, the median value of dr1t2 (median, 0.78) is higher than the median value of dr1dr2t2 (median, 0.70), confirmed using the Wilcoxon test (P < .00009). 3. dr1 is more similar to our algorithm than to himself because the median value of dr1t1 (median, 0.81) is higher (better) than the median value of dr1t12 (median, 0.76), according to the Wilcoxon test (P < .00007). On the other hand, the median value of dr1t2 (median, 0.78) was not considered significantly different of the median value of dr1t12, according to the Wilcoxon test (P < .4). The above results suggest that the interobserver variability between dr1 and our algorithm is ≤ the intraobserver variability of dr1. These findings are not contradicted by the observation of the information obtained using the other figures of merit and were confirmed using a nonparametric method, the Friedman’s two-way analysis of variance (27). The calculated H = 157.67 (with N = 60 and k = 8); under the null hypothesis (equality of medians), H has a χ2 distribution with (k-1) degrees of freedom. In our case, for a 1% significance level (α), χ2 (7);0.01 = 18.48; thus H = 157.67⪢χ2 (7);0.01 = 18.48 (P < .000001) and the null hypothesis is rejected. Table 1 presents the sum of ranks in ascending order. This means that the medians are in fact significantly different, which reinforces the three observations presented above. Moreover, these observations can be confirmed through Table 1, where we can see, for instance, that the sum of ranks corresponding to dr1t1 and dr1t2 are both higher than the sum of ranks corresponding to dr1t12 (377, 299, and 272, respectively). Taking the observation of Table 1 further, we notice that all (except dr2t1) variabilities between our algorithm and each radiologist are less than (at least) the interobserver variability between the two radiologists at moment t2 (dr1dr2t2). These and other findings that can be extracted from these results seem to indicate that, as a detector of pulmonary contours on the used set of images, our algorithm behaves as a third human observer. Study of the interobserver variability through direct comparison among the algorithm and six expert radiologists Let us generalize the previous comparison to six radiologists. dr1 … dr6 stand for the six radiologists and A for the algorithm. Figure 4 shows the box-plots and corresponding median and quartile values for the comparison between the contours detected by our algorithm and the six radiologists in all possible combinations using FPratt (considering that, for instance, dri_drj is equal to drj_dri, we only show one). Thus, in Figure 4, the meaning is: a_dri-comparison between the contours detected by our algorithm and the contours detected by dri, on the selected set of images; it represents the interobserver variability between our algorithm and this radiologist; dri_drj-comparison between the contours detected by dri and the contours detected by drj, on the selected set of images; it represents the interobserver variability between these two radiologists. Observing Figure 4 we note that the median values corresponding to situations of the type a_dri are generally higher and more similar among them than the ones corresponding to dri_drj. Performing a correspondence analysis (28) and observing the plane defined by the first two axis (which represents approximately 46% of the total inertia), we notice that our algorithm is clearly included in the main groups formed by the comparisons among all the radiologists and the algorithm. Comparisons between dr5, dr6 and dr2 seem to be isolated. This could be because dr2 had just finished his training as a radiologist and dr5 and dr6 both work in the same hospital (different from dr2). Study of the interobserver variability using a reference contour We primarily show results obtained using the Pratt figure of merit because we have concluded in previous studies, and confirmed through this one, that the figures of merit (except for the maximum distance) produce consistent results, conveying the same type of information. As a first approach, we studied the interobserver variability among all radiologists and the algorithm in a worst-case scenario. This was performed using the maximum distance figure of merit and exploratory data analysis. Figure 5 shows the box-plots of the data resulting from the comparison of the contours obtained by each detector (humans and algorithm) to the reference contours using the maximum distance. On these plots we observe a concentration of the smaller values, some outliers for all detectors (corresponding to images that should be analyzed) and median values for all detectors between 5.4 and 9.9 pixels; these values can be considered low for images of 512 × 512 pixels. Thus, even in this case all the detectors (including our algorithm) seem to have a good performance for the used image data set. As a second approach, we studied the variability between the reference and all radiologists as well as the algorithm using the Pratt figure of merit and exploratory data analysis. In this study, we included the contours drawn by all the radiologists (dr1 to dr6) in first time, the contours drawn by dr1 and dr2 the second time (as dr1_T2 and dr2_T2), as well as the contours obtained using our algorithm (a). Observing Figure 6, which shows the box corresponding to these data, we notice that the median value obtained for our algorithm is quite similar to the value for radiologist dr4_t1, higher than the values for radiologists dr1_T2, dr3_t1, dr5_t1, dr6_T1 and lower than the values for radiologists dr1_T1, dr2_t1, dr2_T2. This indicates that our algorithm produced, for the used image set, contours more similar to the reference than a significant part of the radiologists. The above result suggested that we should further explore the relation among the performance of our algorithm as a detector to the performance of all the radiologists. Thus, we used clustering analysis (28), which closely associated our algorithm with dr1_t1 as shown by the dendogram plot of Figure 7; this means that, in this context, our algorithm is more similar to radiologist dr1 than he is to himself in different moments, namely dr1_t1 and dr1_t2. This conclusion was already obtained in the previous study through direct comparison among radiologists and algorithm. A confirmation of this result was obtained through the use of another method of multivariate data analysis. Figure 8 shows the projection on the plane defined by the first two axes (approximately 66% of the total inertia) of a correspondence analysis. Observing this figure, we notice that our algorithm is clearly included in a group of four radiologists (dr1_t1, dr1_t2, dr3_ t1, dr4_t1), radiologists dr5 and dr6 form another group and dr2 is isolated between the two groups. Note that the same conclusion could be drawn from the dendogram of Figure 7. This could be related, as observed in the previous study, to the facts that radiologists dr5 and dr6 work in the same department, (different from the others) and perhaps use different segmentation criteria, radiologist dr2 has just finished his training as a radiologist and all the others have a much larger experience. To obtain a global average view of the distance between detected contours and the reference, we used the mean-distance figure of merit and angle θ, and we obtained a confirmation of the results previously found through the Pratt figure of merit (20). Conclusions In this article we propose a methodology to the quantitative evaluation of the performance of a pulmonary contour segmentation algorithm involving the study of interobserver and intraobserver variability. Making accurate, unbiased estimates or comparisons of performance is, in general, a very difficult task. However, some guidelines are known to facilitate it (13,22,29). For our case, we considered the following guidelines useful: • Report results on common test datasets; • Use test datasets different from those used to train the segmentation method; • Use an adequate methodology to choose the test datasets and clearly state it (eg, the inclusion and exclusion criteria and the determination of the sample size); • Choose carefully and define clearly the observers and methods used to obtain the ground-truth; • Let the observers operate in the same conditions; • Clearly specify the performance metric (figures of merit) used; • Correctly identify the measurement scales, which determine the kind of statistical methods that could be used; • Choose hypothesis tests compatible with the quality indexes used and clearly justify it (as the chosen α and β and if the test is one- or two-tailed); • Use nonparametric tests if the data is categorical, the statistical distribution of the data is unknown (or known and not suitable for parametric methods) or the sample size is small; • Use paired test if possible (if all the methods can be applied to the same image). We present results concerning the interobserver variability among six radiologists and the algorithm, using two different approaches: Through the direct comparison of the contours detected by this algorithm to the contours hand-drawn by six radiologists; Through a comparison to a reference contour (obtained from the hand-drawn contours) used as a surrogate ground truth. This last approach is easier to generalize to a greater number of radiologists; however, it is necessary to further investigate what is the most correct way of computing the reference contour when radiologists use different segmentation criteria. All the comparisons were made using several figures of merit. While the Pratt figure of merit, the mean distance, and the angle θ produced consistent results conveying the same type of information, an integrated view of the distances between contours, maximum distance is useful for worst-case scenarios. We also assessed the intraobserver variability of two radiologists to have a measure of the level of interobserver variability that is expected and has to be accepted. We believe this methodology is general enough to be applicable to many other problems of segmentation on medical images, in spite of the fact that it was developed for this specific application. Concerning the performance of our segmentation algorithm, the results presented allow us to conclude that it is possibly as good a lung contour detector, in most thoracic CT images with diagnostic value, as any of the six radiologists. This assertion is mainly based on the fact that it exhibits a greater “agreement” to any of the radiologists than the radiologists among them, in the used image set. This is true, with a few exceptions, for images with complex vascular patterns crossing the interface between the mediastinic and pulmonary fields. Acknowledgements The authors express their gratitude to the following radiologists for drawing contours: Dr Pedro Agostinho from University Hospital of Coimbra, Dr Rui Pinho e Melo and Dr Jorge Pinho e Melo from CENTAC-Center of Computed Tomography, Aveiro; Dr Anabela Fidalgo and Dr Fernando Figueiredo from the Imagiology Department at the Hospital Infante D. Pedro, Aveiro. The authors are also grateful to an anonymous reviewer for his pertinent comments and suggestions. References 1 W.L. Robb Perspective on the first 10 years of the CT scanner industry Acad Radiol 10 2003 756 760 2 J. Brink J.P. Heiken G. Wang K.W. McEnery F.J. Schlueter M.W. Vannier Helical CT principles and technical considerations Radiographics 14 1994 887 893 3 B. Li G. Christensen E. Hoffmann G. McLeannan J. Reinhardt Establishing a normative atlas of the human lung intersubject warping and registration of volumetric CT images Acad Radiol 10 2003 255 265 4 M.S. Brown M.F. McNitt-Gray N.J. Mankovich Method for segmentation chest CT image data using an anatomical model preliminary results IEEE Trans Med Imaging 16 1997 828 839 5 M. Sonka W. Park E.A. Hoffman Rule-based detection of intrathoracic airways trees IEEE Trans Med Imaging 15 1996 314 326 6 J. Duryea J.M. Boone A fully automated algorithm for the segmentation of lung fields on digital chest radiographic images Med Phys 22 1995 183 191 7 R.P. Parker Measurement of basic CT data B.M. Moores R.P. Parker B.R. Pullan Proceedings of physical aspects of medical imaging 1980 Wiley & Sons Manchester, UK 291 295 8 S. Hu E.A. Hoffman J.M. Reinhardt Automatic lung segmentation for accurate quantization of volume x-ray CT images IEEE Trans Med Imaging 20 2001 490 498 9 A. Hasegawa S.-.C.B Lo J.-.S Lin M.T. Freedman S.K. Mun A shift-invariant neural network for the lung field segmentation in chest radiography J VLSI Signal Process 18 1998 241 250 10 J.S. Silva A. Silva B.S. Santos Lung segmentation methods in x-ray CT images Proceedings of V Ibero-American Symposium On Pattern Recognition-SIARP’2000 2000 APRP—Portuguese Association for Pattern Recognition Lisbon, Portugal 583 598 11 A. Silva J.S. Silva B.S. Santos C. Ferreira Fast pulmonary contour extraction in x-ray CT images a methodology and quality assessment C.-.T Chen A.V. Clough SPIE-Medical Imaging 2001 Physiology and Function from Multidimensional Images 4321 2001 SPIE Bellingham, WA 216 224 12 V. Chalana Y. Kim A methodology for evaluation of boundary detection algorithms on medical images IEEE Trans Pattern Anal Machine Intell 16 1997 642 652 13 K.W. Bowyer Validation of medical image analysis techniques J. Fitzpatrick M. Sonka Handbook of medical imaging. Vol 2. Medical image processing and analysis (cap. X) 1999 SPIE-The International Society for Optical Engineering Bellingham, WA 567 606 14 A. Blake M. Isard Active contours 1998 Springer Verlag London 15 S.R. Gunn M.S. Nixon A robust snake implementation; a dual active contour IEEE Trans Pattern Anal Machine Intell 19 1997 63 68 16 M. Kass A. Witkin D. Terzopoulos Snakes active contour models Int J Comput Vision 1 1988 321 331 17 J. Liang T. McInerney D. Terzopoulos United snakes International Conference of Computer Vision—Volume 2, September 20–25 1999 Kerkyra Greece 933 940 18 A. Yezzi S. Kichenassamy A. Kumar P. Olver A. Tannenbaum A geometric snake model for segmentation of medical imagery IEEE Trans Med Imaging 16 1997 199 209 19 R. Sivaramakrishna N. Obuchowski W. Chilcote K. Powell Automatic segmentation of mammographic density Acad Radiol 8 2001 250 256 20 C. Ferreira B.S. Santos J.S. Silva A. Silva Comparison of a segmentation algorithm to six expert imagiologists in detecting pulmonary contours on x-ray CT images SPIE Medical Imaging 2003 Image Perception, Observer Performance and Technology Assessment 2003 SPIE Bellingham, WA 347 358 21 R. Wagner S. Beiden G. Campbell C. Metz W. Sacks Contemporary issues for experimental design in assessment of medical imaging and computer-assist systems SPIE Medical Imaging 2003 Image Perception, Observer Performance and Technology Assessment 5034 2003 SPIE Bellingham, WA 213 224 22 D.G. Altman Practical statistics for medical research 1999 CRC Press London, UK 23 I.E. Abdou W.K. Pratt Quantitative design and evaluation of enhancement/thresholding edge detectors Proceedings IEEE 67 1979 753 763 24 L. Sachs Applied statistics-a handbook of techniques 1984 Springer-Verlag New York, NY 25 D. Hoaglin F. Mosteller J. Tukey Understanding robust and exploratory data analysis 1983 Wiley & Sons 26 Statsoft. Statistica-release 5.5 for Windows. Statsoft Inc 1999 27 J.D. Gibbons Nonparametric methods for quantitative analysis 1997 American Sciences Press Syracuse, NY 28 J.F. Hair R.E. Anderson R.L. Tatham W.C. Black Multivariate data analysis with readings 1995 Prentice-Hall Upper Saddle River, NJ 29 I. Buvat The need to develop guidelines for the evaluation of medical image processing procedures K.M. Hanson SPIE-Medical Imaging 1999 Image Processing 1999 SPIE Bellingham, WA 1466 1477 "
    },
    {
        "doc_title": "A Level-Set Based Volumetric CT Segmentation Technique: A Case Study with Pulmonary Air Bubbles",
        "doc_scopus_id": "35048899295",
        "doc_doi": "10.1007/978-3-540-30126-4_9",
        "doc_eid": "2-s2.0-35048899295",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Air bubbles",
            "High-resolution computed tomography",
            "Level Set",
            "Level set functions",
            "Level Set method",
            "Surface topology",
            "Volumetric CT",
            "Volumetric data"
        ],
        "doc_abstract": "The identification of pulmonary air bubbles plays a significant role for medical diagnosis of pulmonary pathologies. A method to segment these abnormal pulmonary regions on volumetric data, using a model deforming towards the objects of interest, is presented. We propose a variant to the well known level-set method that keeps the level-set function moving along desired directions, with an improved stopping function that proved to be successful, even for large time steps. A region seeking approach is used instead of the traditional edge seeking. Our method is stable, robust, and automatically handles changes in surface topology during the deformation. Experimental results, for 2D and 3D high resolution computed tomography images, demonstrate its performance. © Springer-Verlag 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual perception in computer graphics courses",
        "doc_scopus_id": "2542449298",
        "doc_doi": "10.1016/j.cag.2004.03.015",
        "doc_eid": "2-s2.0-2542449298",
        "doc_date": "2004-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "CGI",
            "Computer graphics curricula",
            "Data dimensions",
            "Human visual system"
        ],
        "doc_abstract": "Visual perception and the human visual system are important for the judgements of the results in computer graphics. So it is important to address this in computer graphics courses. We describe the experiences of including those features into graphics courses and from our experiences we give examples and proposals as to how an introduction to perception can be done. A bibliography with comments is also included in the paper. © 2004 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2004-04-12 2004-04-12 2010-04-18T18:09:26 S0097-8493(04)00038-X S009784930400038X 10.1016/j.cag.2004.03.015 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20040601 20040630 2004 2004-04-12T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue body affil articletitle auth authfirstini authfull authkeywords authlast primabst pubtype ref alllist content subj ssids 0097-8493 00978493 28 28 3 3 Volume 28, Issue 3 16 451 456 451 456 200406 June 2004 2004-06-01 2004-06-30 2004 converted-article fla Copyright © 2004 Elsevier Ltd. All rights reserved. VISUALPERCEPTIONINCOMPUTERGRAPHICSCOURSES KJELLDAHL L 1 Introduction 2 Motivations for using perception 3 Experiences from two universities 3.1 Aveiro 3.2 KTH, Stockholm 4 Bibliography 4.1 Introductory 4.2 Intermediate 4.3 Advanced 5 Conclusions References FOLEY 1990 J COMPUTERGRAPHICS HEARN 1994 D COMPUTERGRAPHICS ANGEL 2003 E INTERACTIVECOMPUTERGRAPHICSATOPDOWNAPPROACHOPENGL BROWN 1995 J VISUALIZATION SCHROEDER 1998 W VISUALIZATIONTOOLKITOBJECTORIENTEDAPPROACH3DGRAPHICS DOMIK 2000 16 19 G DIX 1998 A HUMANCOMPUTERINTERACTION PREECE 1994 J HUMANCOMPUTERINTERACTION WOLFE 1999 35 39 R WOLFE 2000 151 155 R SOUSASANTOS 2000 163 170 B PRATT 2001 W DIGITALIMAGEPROCESSING GONZALEZ 2002 R DIGITALIMAGEPROCESSING KJELLDAHLX2004X451 KJELLDAHLX2004X451X456 KJELLDAHLX2004X451XL KJELLDAHLX2004X451X456XL item S0097-8493(04)00038-X S009784930400038X 10.1016/j.cag.2004.03.015 271576 2010-10-08T23:56:46.038584-04:00 2004-06-01 2004-06-30 true 199357 MAIN 6 67515 849 656 IMAGE-WEB-PDF 1 si1 217 35 21 CAG 1349 S0097-8493(04)00038-X 10.1016/j.cag.2004.03.015 Elsevier Ltd Education Visual perception in computer graphics courses Lars Kjelldahl a * B.S. Beatriz Sousa Santos b a Numerical Analysis and Computing Science, Royal Institute of Technology (KTH), Stockholm S-10044, Sweden b Department of Electronics and Telecom., IEETA, Universidade de Aveiro, Sweden * Corresponding author. fax: +46-87900930 Visual perception and the human visual system are important for the judgements of the results in computer graphics. So it is important to address this in computer graphics courses. We describe the experiences of including those features into graphics courses and from our experiences we give examples and proposals as to how an introduction to perception can be done. A bibliography with comments is also included in the paper. Keywords Computer graphics curricula Human visual system Perception 1 Introduction This paper discusses visual perception in computer graphics education. Related areas are, however, also covered to some extent. Knowledge of the human visual system is vital for the understanding of perception and is therefore treated as being a part of visual perception in this paper. Perception and the human visual system (HVS) are also important for several areas related to computer graphics. Examples of such areas are visualization, graphics design, human–computer interaction, image processing, and computer vision. We give a few examples of how perception is treated in education within those areas. Computer graphics textbooks traditionally usually include at the most some reference to human vision mechanisms when specifically addressing topics where some knowledge of those mechanisms is essential to their understanding [1,2]. However, the more recent text by Angel [3] includes a brief section on the human visual system in the first chapter. The book [4] includes a chapter on visualization design with guidelines related to perception. Other examples from visualization are given by Healy et al. [5], which has examined pre-attentive (immediate) detection of patterns in visualization presentations. Visualization textbooks are very rare. An example of such a book, [6], does include very little on HVS. Collections with different papers that might be used for a course do sometimes include perceptual aspects such as [7]. Reports on the activities concerning the identification of a kernel curriculum in visualization do include human perception concepts as an independent topic to be addressed [8–10]. Human–computer interaction courses sometimes include perceptual aspects and HVS. Of two textbooks on human–computer interaction, one addresses the subject [11] and the other does not [12]. However, computer science is expanding its scope and has changed dramatically in the last decade, which certainly has a significant impact in computing education in general as one can evaluate from Computing Curricula 2001 [13]. This change has conceivably been responsible for a different approach that is already becoming noticeable in several documents about computer graphics education. On the one hand, computer graphics has been recognized as of interest to a wide variety of disciplines beyond computing (e.g. engineering, mathematics, physics, art) [14], on the other hand, it has been considered unlike other specialties in computer science, since it is “visual” [15,16]. If we look into the current version of the SIGGRAPH Education Committee CGI Taxonomy Project [17], we notice the reference to aspects of the HVS in several sections (human factors, colour perception, Mach bands). Most computer graphics educators agree that, due to its visual nature, computer graphics must not only consider space and time aspects, but also a user's perception of the output in the context of the user's goals [18]. In the following sections, we briefly describe how these aspects have been addressed in computer graphics (as well as in other related) courses. We include a list of commented bibliography that could support these levels. 2 Motivations for using perception There are several different reasons for taking into account the perceptual aspects in graphics. Below is a list of questions where perception may contribute to an answer. In the list we have also included aspects within visualization as this is closely related to computer graphics. • How can we adjust rendering calculations to make the images more realistic? • How can we present information and data in a way that make patterns in the data easier to detect? • How can we avoid mistakes in the human understanding of an image? • How can we emphasize the important parts of an image and how can we suppress the less important parts? • How can we adjust the presentation/image to be more pleasing to the user? • How can we avoid bad colour choices and colour combinations in images and presentations? • How can we in scientific visualization find a good mapping between data dimensions and visual dimensions? • How can we adjust our algorithms to avoid making unnecessary calculations of features that the human visual system will not perceive? • How can limitations in hardware be taken care of? 3 Experiences from two universities Below is a description of how visual perception and knowledge on HVS have been included in the computer graphics courses at two universities. Experiences are also given. 3.1 Aveiro The subject of HVS has been introduced using a structured approach since 1993, in four different introductory courses on computer graphics, visualization [19,20], human–computer interaction and digital image processing. These courses have been offered at graduate or post-graduate level in Electrical and Computer Engineering degrees. The amount of time devoted to the HVS and the importance given to each topic depends on the total duration, as well as on the type of course. Typically, in courses having 28h of lectures (2h/week×14 weeks) 1h is devoted to the subject; in courses having 42h of lectures (3h/week×14 weeks) 1 1 2 h is spent. Some of the topics are more relevant to some of the courses than to others. For instance, the cultural aspects of visual perception are particularly interesting in the scope of human–computer interaction, or lateral inhibition and Mach bands in computer graphics. The main topics addressed in the introduction to the HVS are the following: (I) Definitions. (II) Vision according to three points of view: (1) Evolution of Organic Vision, (2) Eye and Brain, (3) Perception. In part I vision is defined as the complex “process of converting sensory information into knowledge of the objects in the environment”; seeing and perceiving are also defined and the fact that they are different components of vision as well as the fact that the eye is just a sensor and extracts only a part of the total information available around us are stressed. Part II starts with a very brief overview of the evolution of organic vision so that students gain the perception that organic vision has been evolving along species following mainly two “plans” (each particular solution corresponding to different needs and having different performances). The HVS can thus be viewed as a very sophisticated system developed to solve a very complex “engineering problem”. Under the title “Eye and Brain”, one tries to answer the first part of the question “How is the pattern of light energy projected onto the light sensitive cells and transformed into a model of the world?”. This is done using low-level and high-level approaches; thus brief anatomical and physiological descriptions are given, on the one hand, of what happens when light enters the eye and electrical signals travel to the brain and on the other hand, of the high-level resulting features. The following important topics are addressed: • the anatomy of the human eye and some parameters of the HVS that can give an idea of its complexity and performance, • the projection and focus mechanism, • the structure of the retina (layers of cells) and the distribution of rods and cones on it, • the two types of vision (photopic and scotopic): bright adaptation and discrimination, • some very important high level functions: lateral inhibition, stereoscopic vision and feature detection. Finally perception is addressed; it is defined as the interpretation of what is seen, stressing the fact that this interpretation is based on past experience. The nature of the “techniques” used by the HVS is explored through the examination of its successes and failures in interpreting natural and contrived images. The following relevant topics are addressed: • perceiving the real world: recognising patterns (a complex problem that appears to be solved spontaneously, flawlessly and without surprises), • perceptual organization (Gestalt laws), • visual illusions (which provide much information on which theories of the functioning of HVS are based), • pictorial perception and culture. A hypermedia document on Human Vision [21] has been used since 1997 during the lectures and given to the students as support for self-study of this subject. It is a html document of approximately 20 pages. Its readability and interest were not formally tested, but it was made available for other computer graphics and visualization educators through the SIGGRAPH education committee project HyperVis [22]. 3.2 KTH, Stockholm Three different computer graphics courses are given. The courses correspond to 4–6 weeks of full time work each. In all courses perceptual aspects are covered to some extent. In the first introductory course perception is introduced using a collection of examples and guidelines. Some basic knowledge on colour is also given. 2h are used for this introduction. In the second course 3h of lectures give details on perception and the HVS with more examples and some theory. The assignments include a design part, where a graphics designer introduces fundamentals of design and asks the students to make sketches to be used later for their Maya modelling assignment. The third course uses a mixture of the first two courses. Below is a list of covered topics: • levels of realism, • human visual system with details of the eyes and the brain, • every day experience (such as light usually comes from above and is used to determine shape from shadow), • overview of colour, • gestalt laws, • visual variables and cue combination, • illusions, • lateral inhibition, • colour interaction, • depth perception, • contrast sensitivity function and its use in graphics rendering algorithms, • perceptually optimized 3D graphics, including things such as image metrics and tone mapping. 4 Bibliography Brief introductions to visual perception can be found for instance in textbooks of Digital Image Processing and Human–Computer Interaction [11,23,24], which are usually easy to find in technological libraries. In this section we give a bibliography of some papers and books on visual perception (and related areas). The references have been structured into three categories: introductory, intermediate and advanced. It should be observed that useful references can be found in many varying sources, such as proceedings of SPIE conferences (e.g. Human Vision Visual Processing and Digital Display and Very High Resolution and Quality Imaging) and SIGGRAPH proceedings, as well in the journals Biological Cybernetics, IEEE Transactions on Image Processing, Advanced Imaging, Vision Research, Journal of Optical Society of America A, amongst others. 4.1 Introductory Braham, R., Toward an Artificial Eye, Special Report, IEEE Spectrum, May 1996. A very interesting selection of papers that describes the fundamentals of the HVS having in perspective the efforts developed toward the construction of artificial visual systems that can be used by impaired humans. It is specially meant for engineering professionals (students), however, it seems also general enough to other professionals (students). Fishler, M., O. Firschein, Intelligence, The Eye, the Brain and the Computer, Addison Wesley, 1987. This book uses an integrated approach on human and machine intelligence, using knowledge from several areas such as computer science, cognitive science, linguistics, biology anthropology and psychology. Chapter 8 is completely devoted to the human visual system. Ferwerda, J.A., Elements of Early Vision for Computer Graphics. IEEE Computer Graphics and Applications, 2001; 21(5): 22–33. A good introduction to the HVS aiming at a reader who is working with computer graphics. An overview of the anatomy and physiology of the HVS is given. covering details about the eye as well as pathways in the brain. Different kinds of adaptation in a wide range of luminance are also discussed as well as things such as lateral inhibition and masking. Reddy, M., Perceptually Optimized 3D Graphics, IEEE Computer Graphics and Applications, 2001; 21(5): 68–75. This paper is a combined research paper and tutorial describing how limitations in the visual system can be used to optimize rendering time. Limitations in the HVS regarding contrast, spatial frequency, velocity and peripheral vision are treated. 4.2 Intermediate Gregory, R., Eye and Brain- The Psychology of Seeing, 5th ed., Oxford University Press, 1998. An introduction to the basic phenomena of visual perception is made in a highly readable text. This edition was completely revised and updated. In spite of the fact that it was not intended, in the first place, to be a textbook, it has been used for psychology, physiology, visual arts and philosophy courses. It addresses issues as seeing brightness, movement and colour. Illusions are a major theme in the book. Groß, M., Visual Computing, Springer Verlag, 1994. Addresses topics related to Visual Computing, an emergent discipline, which integrates computer graphics techniques, visual perception, digital image processing and technologies. This book presents also the fundamentals of visual perception. Hubel, D., Eye, Brain and Vision, Scientific American Library, 1995. It is mainly concerned with how the brain handles visual information and it is meant to be read by people that have scientific training but not in biology. Explores the tasks scientists face in deciphering the workings of human vision and brain. Palmer, S, Vision Science—Photons to Phenomenology, Massachusetts Institute of Technology, 1999. This textbook adopts the new approach that emerged in the last decades at the intersection of several disciplines as perceptual psychology, computer science, neurophysiology, psychophysiology, in which scientists were concerned with image understanding. Central to this approach is that vision is a kind of computation, which has dominated the cutting edge of vision research; however, it is underrepresented in most modern textbooks about perception. It should be particularly useful for someone who wants to get a global view of vision science; it covers foundations, spatial vision and visual dynamics. Rock, I., Perception, Scientific American Library, 1984. The human perception of objects in the world, in art and in visual illusions is addressed. Using ingenious experiments, it is explained, in this book, how we manage to turn the ambiguous, ever changing, two-dimensional images that fall on the eye into the rich, constant three-dimensional world as we see it. Sckuler, R., Blake, B., Perception, 3rd ed., McGraw Hill, 1994. The biological bases of vision and the perception of pattern, colour and depth are discussed in detail as well as action and the perception of events. Other senses are also discussed. It is strongly biased toward recent issues, which allow students to appreciate developments at the frontiers of perception. This can be considered a textbook for students of perception. Travis, D., Effective Color Displays: Theory and Practice, Academic Press, 1991. The aim of this book is to synthesise the knowledge needed and specify guidelines so that programmers, engineers and psychologists can use colour effectively. Tutorial material is introduced where it is necessary to grasp fundamental principles and understand the limitations of both the display device and the perceptual system. 4.3 Advanced Barten, P., Contrast Sensitivity of the Human Eye and its Effect on Image Quality, SPIE Press, 1999. This book describes models for various aspects of the contrast sensitivity of human eye based on fundamental assumptions about its functioning. Contrast sensitivity has a significant influence on the perceived quality of an image. A metric is given with which the perceived quality of an image can be calculated from the physical data on the imaging system and the psychophysical data given by the contrast sensitivity. The effects of various parameters on image quality are discussed and several examples are given. Fairchild, M D., Color Appearance Models, Addison-Wesley 1998. This books gives an overview of colour theory and an in depth description of colour appearance models. This book is written by one of the key persons within colour science and combines an educational and research flavour. Kelly, D. (Ed.), Visual Science and Engineering, Models and Applications, Marcel Dekker, 1994. This book examines, throughout a series of chapters by experts, a broad spectrum of recent topics in visual science, relating basic studies to practical applications and delineating points of intersection among various disciplines that study the mechanisms of vision. It is an interesting resource for imaging scientists and post graduate-level students. Landy, M., J. Movshon (eds.), Computational Models of Visual Processing, MIT Press, 1991. This book accounts for the enormous progress made in vision sciences in the past decades, in order to answer the question “what has the computational modelling of vision achieved?”, treating vision as an optical instrument upon whose output computations of an experimentally tested nature are performed. It is an important reference to researchers and post-graduate level students interested in using/developing models of the HVS. McNamara, A., Visual Perception in Realistic Image Synthesis, Computer Graphics Forum, 2001; 20(4): 211–224. This paper outlines the manner in which knowledge about visual perception is increasingly appearing in state-of-the-art realistic image synthesis. It describes perception-driven rendering algorithms, which focus on embedding models of the HVS directly into global illumination computations in order to improve efficiency. It also presents perception-based image quality metrics, aiming to compare images on a perceptual rather than physical basis. Finally, it discusses tone reproduction operators, which attempt to map the range of computed radiance values to the limited range of display values. Marr, D., Vision- A Computational Investigation into the Human Representation and Processing of Visual Information, W. Freeman and Company, 1982. According to the author, in order to understand vision several different explanations are needed. Each problem has to be addressed from several points of view—as a problem in representing information, as a computation capable of deriving that representation and as a problem of carrying out both things quickly and reliably. The book presents the philosophical preliminaries, the approach and the representational framework that is proposed for the overall process of visual perception, as well as the way that led to it and the results attained. Ware, C., Information Visualization, Perception for Design, Morgan Kaufmann, 2000. This book is written by a person with a background in both computer science and perception. It is intended to make human perception and its applications available to the non-specialist. It presents extensive technical information about various visual acuities, thresholds and other basic properties of the human visual system and contains specific guidelines and recommendations. It is not organized as a textbook, but includes many interesting details regarding perception and visualization. Zeki, S, A Vision of the Brain, Blackwell, 1993. This book is written by a researcher within neuroscience. It gives an advanced introduction to the brain and the visual cortex. The different parts of the brain are explained using many examples revealing the relation between what we interpret and the building blocks of the brain. Functionality discussed includes for instance motion, parallelism and colour. 5 Conclusions Aspects concerning visual perception and the human visual system are important for computer graphics courses. Computer graphics is related to design and perception, which raises the need for an interdisciplinary attitude. Students are often interested in this broader perspective. In the future, perceptual aspects will probably be even more important as the fundamental algorithms have been more fully explored. In this paper we have shown how an introduction to these aspects can be given in a computer graphics course using the experiences from Aveiro and Stockholm. According to these experiences, adopting a systematic (even if brief) approach of the subject in the beginning of computer graphics courses does not take too long and avoids the need to introduce these concepts as you need them along the course. We have also discussed some fundamental components that are of importance for such an introduction. A bibliography with books, articles and web references has been given with an orientation both to teachers and students. References [1] J. Foley A. van Dam S. Feiner J. Hughes Computer graphics principles and practice 2nd ed 1990 Addison Wesley Reading, MA [2] D. Hearn M.P. Baker Computer graphics 2nd ed 1994 Prentice Hall Eaglewood Cliffs, NJ [3] E. Angel Interactive computer graphics, a top-down approach with OpenGL 3rd ed 2003 Addison Wesley Reading, MA [4] J.R. Brown R. Earnshaw M. Jern J. Vince Visualization 1995 Wiley New York [5] Healey CG, Interrante V, Rheingans P. Fundamental Issues of Visual Perception for Effective Perception for Effective Image Generation, course #6, SIGGRAPH’99, August 8–13, 1999. [6] W. Schroeder K. Martin B. Lorensen The visualization toolkit, an object oriented approach to 3D graphics 2nd ed 1998 Prentice Hall Eaglewood Cliffs [7] Gershon N. From perception to visualization. In: Rosenblum (Ed), Scientific visualization-advances and challenges. New York: Academic Press, 1994. [8] Domik G, Owen GS. Education for visualization-activities of the ACM SIGGRAPH education committee. Proceedings of the Workshop on Graphics and Visualization Education, Eurographics’95, 1995. [9] Owen GS, Bailey M, Brodlie K, Domik G. Issues in education for visualization. Proceedings of IEEE Visualization 95, 1995. p. 427–430. [10] G. Domik Do we need a formal education in visualization? IEEE Computer Graphics & Applications 20 4 2000 16 19 [11] A. Dix J. Finlay G. Abowd B. Russell Human–computer interaction 2nd ed 1998 Prentice Hall Eaglewood Cliffs, NJ [12] J. Preece Y. Rogers H. Sharp D. Benyon S. Holland T. Carey Human–computer interaction 1994 Addison Wesley Reading, MA [13] ACM/IEEE. Computing curricula 2001-computer science. ACM Journal of Educational Resources in Computing 2001; 1(3). [14] Owen, GS, Sunderraman R, Zhang Y. The development of a digital library to support the teaching of computer graphics and visualization. Proceedings of Workshop on Graphics and Visualization Education, GVE99, 1999. p. 59–64. [15] R. Wolfe New possibilities in the introductory graphics course for computer science majors Computer Graphics 33 2 1999 35 39 [16] Wolfe R. Strengthening visual skills by recognising rendering algorithms. Proceedings of Workshop on Graphics and Visualization Education, GVE99, 1999. p. 79–84. [17] Ford Morie J. CGI Taxonomy Project. on-line January 2004. [18] R. Wolfe Bringing the introductory computer graphics course into the 21st century Computers and Graphics 24 1 2000 151 155 [19] Sousa Santos B. An introductory course on computer graphics at the University of Aveiro. Proceedings of the Workshop on Graphics and Visualization Education, Eurographics’95, 1995. [20] B. Sousa Santos An introductory course on visualization Computers and Graphics 24 1 2000 163 170 [21] Sousa Santos B.Vision, 2003, [22] Owen GS, et al. HyperVis, Acm siggraph Education Committee, on-line September 2001, 1996. [23] W. Pratt Digital image processing 3rd ed 2001 Wiley New York [24] R.C. Gonzalez R.E. Woods Digital image processing 2nd ed 2002 Addison Wesley Reading MA "
    },
    {
        "doc_title": "Detection and 3D representation of pulmonary air bubbles in HRCT volumes",
        "doc_scopus_id": "0041326302",
        "doc_doi": "10.1117/12.480283",
        "doc_eid": "2-s2.0-0041326302",
        "doc_date": "2003-09-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Bubble emphysema",
            "High resolution computer tomography",
            "Lungs",
            "Pulmonary air bubbles",
            "Pulmonary volume delimitation"
        ],
        "doc_abstract": "Bubble emphysema is a disease characterized by the presence of air bubbles with the lungs. With the purpose of identifying pulmonary air bubbles, two alternative methods are developed, using High Resolution Computer Tomography (HRCT) exams. The first detection method follows a slice by slice approach and uses selection criteria based on the Hounsfield levels, dimensions, shape and localization of the bubbles. The second detection method, after the pulmonary volume delimitation, follows a fully 3D approach. The 3D approach morphologic operators are used to remove spurious structures and to circumscribe the bubbles.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of a segmentation algorithm to six expert imagiologists in detecting pulmonary contours on X-ray CT images",
        "doc_scopus_id": "0041357174",
        "doc_doi": "10.1117/12.480072",
        "doc_eid": "2-s2.0-0041357174",
        "doc_date": "2003-09-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Quantitative evaluation of the performance of segmentation algorithms on medical images is crucial before their clinical use can be considered. We have quantitatively compared the contours obtained by a pulmonary segmentation algorithm to contours manually-drawn by six expert imaiologists on the same set of images, since the ground truth is unknown. Two types of variability (inter-observer and intra-observer) should be taken into account in the performance evaluation of segmentation algorithms and several methods to do it have been proposed. This paper describes the quantitative evaluation of the performance of our segmentation algorithm using several figures of merit, exploratory and multivariate data analysis and non parametric tests, based on the assessment of the inter-observer variability of six expert imagiologists from three different hospitals and the intra-observer variability of two expert imagiologists from the same hospital. As an overall result of this comparison we were able to claim that the consistency and accuracy of our pulmonary segmentation algorithm is adequate for most of the quantitative requirements mentioned by the imagiologists. We also believe that the methodology used to evaluate the performance of our algorithm is general enough to be applicable to many other segmentation problems on medical images.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Experimental methodology for the evaluation of the 3D visualization of quantitative information: A case study concerning SEEG information",
        "doc_scopus_id": "84943393171",
        "doc_doi": "10.2498/cit.2002.02.04",
        "doc_eid": "2-s2.0-84943393171",
        "doc_date": "2002-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Empirical evaluations",
            "Epilepsy",
            "Evaluation",
            "Experimental methodology",
            "Exploratory data analysis",
            "Quantitative information",
            "Quantitative visualization",
            "Spatio-temporal dynamics"
        ],
        "doc_abstract": "The visual analysis of Stereoeletroencephalographic (SEEG) signals in their anatomical context is aimed at understanding the spatio-temporal dynamics of epileptic processes. The magnitude of these signals may be encoded by graphical glyphs, having a direct impact on the perception of the values. This problem has motivated an evaluation of the quantitative visualization of these signals, specifically with regard to the influence of the coding scheme of the glyphs on the understanding and analysis of the signals. This work describes an experiment conducted with human observers in order to evaluate three different coding schemes used to visualize the magnitude of SEEG signals in their 3D anatomical context. Before the experiment we had no clue to which of these schemes would provide better performance to the human observers, while the literature offered theories supporting different answers. Through our experiment we intended to find out if any of these coding schemes allows better performance in two aspects: accuracy and speed. A protocol has been developed to measure these aspects. The results presented in this work were obtained from 40 human observers. Comparison between the three coding schemes was first performed through an Exploratory Data Analysis (EDA). Statistical significance of this comparison was then established using nonparametric methods. Influence of some other factors on the observers' performance was also investigated.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of the 3D visualization of quantitative stereoelectroencephalographic information. New results",
        "doc_scopus_id": "0034867503",
        "doc_doi": "10.1117/12.431194",
        "doc_eid": "2-s2.0-0034867503",
        "doc_date": "2001-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Epilepsy",
            "Exploratory data analysis",
            "Graphical glyphs",
            "Spatio temporal dynamics",
            "Stereoelectroencephalographic signals",
            "Three dimensional visualization"
        ],
        "doc_abstract": "The visual analysis of Stereoelectroencephalographic (SEEG) signals in their anatomical context is aimed at the understanding of the spatio-temporal dynamics of epileptic processes. The magnitude of these signals may be encoded by graphical glyphs, having a direct impact on the perception of the values. Our study is devoted to the evaluation of the quantitative visualization of these signals, specifically to the influence of the coding scheme of the glyphs on the understanding and the analysis of the signals. This work describes an experiment conducted with human observers in order to evaluate three different coding schemes used to visualize the magnitude of SEEG signals in their 3D anatomical context. We intended to study if any of these coding schemes allows better performances for the human observers in two aspects: accuracy and speed. A protocol has been developed in order to measure these aspects. The results that will be presented in this work were obtained from 40 human observers. The comparison between the three coding schemes has first been performed through an Exploratory Data Analysis (EDA). The statistical significance of this comparison has then been established using nonparametric methods. The influence on the observers' performance of some other factors has also been investigated.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Fast pulmonary contour extraction in X-ray CT images: A methodology and quality assessment",
        "doc_scopus_id": "0034866059",
        "doc_doi": "10.1117/12.428139",
        "doc_eid": "2-s2.0-0034866059",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Segmentation of thoracic X-Ray Computed Tomography images is a mandatory pre-processing step in many automated or semi-automated analysis tasks such us region identification, densitometric analysis, or even for 3D visualization purposes when a stack of slices has to be prepared for surface or volume rendering. In this work, we present a fully automated and fast method for pulmonary contour extraction and region identification. Our method combines adaptive intensity discrimination, geometrical feature estimation and morphological processing resulting into a fast and flexible algorithm. A complementary but not less important objective of this work consisted on a quality assessment study of the developed contour detection technique. The automatically extracted contours were statistically compared to manually drawn pulmonary outlines provided by two radiologists. Exploratory data analysis and non-parametric statistical tests were performed on the results obtained using several figures of merit. Results indicate that, besides a strong consistence among all the quality indexes, there is a wider inter-observer variability concerning both radiologists than the variability of our algorithm when compared to each one of the radiologists. As an overall conclusion we claim that the consistence and accuracy of our detection method is more than acceptable for most of the quantitative requirements mentioned by the radiologists.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of the 3D visualization of quantitative Stereoeletroencephalographic information",
        "doc_scopus_id": "0034444451",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0034444451",
        "doc_date": "2000-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Epilepsy",
            "Quantitative evaluation",
            "Stereoelectroencephalography"
        ],
        "doc_abstract": "Steoreoelectroencephalography (SEEG) has been used to define and understand the organization of epileptogenic zones of the brain. The fusion of the SEEG signals and the anatomy on a common referential is a possible method for the analysis of these signals. This work describes an experiment conducted with human observers in order to evaluate three different coding schemes used to visualize the magnitude of SEEG signal in the anatomical context.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Introductory course on visualization",
        "doc_scopus_id": "0033901189",
        "doc_doi": "10.1016/S0097-8493(99)00147-8",
        "doc_eid": "2-s2.0-0033901189",
        "doc_date": "2000-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Introductory courses",
            "Visual representation"
        ],
        "doc_abstract": "A Visualization course offered twice (1997/1998 and 1998/1999) as an elective in the M.Sc. degree on Electronics and Telecommunications at the University of Aveiro is presented. Its contents, bibliography and teaching methods are described. Some difficulties encountered during the preparation and lecturing of this course are identified.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2000-03-20 2000-03-20 2010-04-18T15:20:42 S0097-8493(99)00147-8 S0097849399001478 10.1016/S0097-8493(99)00147-8 S300 S300.1 FULL-TEXT 2015-05-14T03:37:49.188858-04:00 0 0 20000201 20000229 2000 2000-03-20T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue body acknowledge affil articletitle auth authfirstini authfull authlast footnotes primabst pubtype ref alllist content subj ssids 0097-8493 00978493 24 24 1 1 Volume 24, Issue 1 16 163 169 163 169 200002 February 2000 2000-02-01 2000-02-29 2000 converted-article sco Copyright © 2000 Elsevier Science Ltd. All rights reserved. INTRODUCTORYCOURSEVISUALIZATION SANTOS B 1 Introduction 2 General contents 3 Teaching methods 4 Bibliography 4.1 General usage of the selected bibliography 4.2 List of main bibliography 4.3 Other bibliography 5 Conclusions Acknowledgements References BEATRIZSOUSASANTOS 1997 13 16 D BANKS 1997 379 387 D SANTOSX2000X163 SANTOSX2000X163X169 SANTOSX2000X163XB SANTOSX2000X163X169XB item S0097-8493(99)00147-8 S0097849399001478 10.1016/S0097-8493(99)00147-8 271576 2010-09-30T22:33:43.403749-04:00 2000-02-01 2000-02-29 true 98631 MAIN 7 62489 849 656 IMAGE-WEB-PDF 1 CAG 790 S0097-8493(99)00147-8 10.1016/S0097-8493(99)00147-8 Elsevier Science Ltd Education An introductory course on Visualization Beatriz Sousa Santos Departamento de Electrónica e Telecomunicações, Universidade de Aveiro 3800, Aveiro, Portugal A Visualization course offered twice (1997/1998 and 1998/1999) as an elective in the M.Sc. degree on Electronics and Telecommunications at the University of Aveiro is presented. Its contents, bibliography and teaching methods are described. Some difficulties encountered during the preparation and lecturing of this course are identified. 1 Introduction Taking into consideration that Visualization is becoming very important and useful in many areas, an introductory course on Visualization seems a valid contribution to the curriculum of any post-graduation in science or technology and thus it was considered adequate as an elective course of the MSc in Electronics and Telecommunications offered at the University of Aveiro. This post-graduation program includes several courses and a thesis and aims to be a large spectrum degree encompassing mainly one of four areas of Electrical Engineering (Electronics, Signal Analysis and Processing, Telecommunications and Computers); this means that it can give formation either to future consumers or facilitators of visualization techniques and systems. This is the context where the referred introductory course was meant to exist; thus its general objective is to introduce the students to a new area which evolves rapidly, addressing the fundamental concepts, providing a basic foundation, good enough to allow and encourage them to apply or proceed with work in that area. Stating this objective more specifically, this course should introduce the students to what Visualization is and can do, as well as it should make them appreciate its benefits and how current tools can be exploited in many application areas; it intends to give a consumer's perspective as well as a facilitator's perspective. However, such a course should also serve other more general objectives related to training students to be able to perform the kind of work a post-graduate is expected to do; this involves training them on doing bibliographic research, using adequate working methods and correct technical communication (i.e. writing and speaking). In the following sections a brief description of the general contents, bibliography and teaching methods used twice (1997/1998 and 1998/1999) on the introductory course on Visualization will be presented. Some difficulties encountered by the author during the preparation and lecturing of this course are also identified. 2 General contents This course was designed to have 36h (24 lectures) spread along a semester; some of these lectures are dedicated to other activities as justified in Section 3. The list of addressed topics, presented below, indicates the duration, focus and specific bibliographic references (briefly described in Section 4) which have been used. 1. Introduction: definitions, history, goals and principles of Visualization (1 lecture) 2. Overview of visualization applications (2 lectures) 3. Human Visual System (1 lecture) 4. Visual Representation of Quantitative Information (1 lecture) 5. Framework (1 lecture) 6. Visualization techniques (4 lectures) 7. Foundations on the enabling technologies: Computer Graphics, Digital Image processing and Human–Computer Interaction (7 lectures) 8. Data characteristics (1 lecture) 9. Visualization products (1 lecture) In the first lecture, corresponding to topic no.1, a general introduction to the area is made. Besides presenting the definitions and goals of Visualization, a focused approach to visualizing data is presented as a means to select adequate techniques to allow exploration, analysis and then presentation of data. The main references used are the books by Earnshaw and Wiseman, and Keller and Keller. This first lecture is of great importance, it is intended to motivate the students and establish the focus and “philosophy” of the course. Two fundamental ideas are conveyed: (a) Visualization is concerned with exploring data and information graphically as a means of gaining understanding and insight into the data (not just presentation graphics!), and (b) how to do visualization? There is a need of a methodology for selecting visualization techniques, however, no “recipes” exist. The focused approach described by Keller and Keller in the first section of their text is presented. To conclude, a set of images produced from seismic data (also from the book by Keller and Keller) is used to illustrate the incremental process one may expect while seeking the best way of visualizing data. Topic no.2, is meant to give an idea of the large quantity of applications that visualization finds in a great diversity of scientific and engineering disciplines. This should make the students aware of the importance and usefulness of visualization, as well as allow them to better understand all the other topics of the course. This overview is given through the presentation of a large collection of visualization images obtained in a variety of disciplines and emphasizing the kinds of information revealed rather than the details of the visualization techniques. A number of examples extracted from several bibliographic references is used (such as the books by Brown et al., Keller and Keller, Brodlie et al., Earnshaw and Wiseman). The third topic is concerned with the “human part” of the visualization process, which is essential. The capabilities and limitations of the human visual and information processing systems are briefly described, stressing that they have important implications for design and that users, in spite of sharing common capabilities and limitations, are individuals with differences which should not be ignored. The human visual system is the main subject and the lecture is mainly based on a document prepared by the author which is included in HyperVis (an on-line tutorial on Visualization) and indicates specific bibliography. In Topic no.4, the fact that visualization is not new in concept is stressed; most principles that have been used to produce good maps, scientific drawings and data plots apply to computer visualization. This lecture is based on the interesting work of E. Tufte (mainly the 1983 text but also the 1990 text, to support colour usage). Topic no.5 presents a framework model describing Visualization systems in abstract terms; this framework will be used to present techniques, data characteristics, products and applications in the remaining lectures. As an introduction to this topic, models of scientific investigation and visualization process are presented. This lecture is based on the corresponding chapter of the reference by Brodlie et al. The concept of visualization technique, introduced in the previous topic as the one responsible for generating and manipulating a graphic representation from a set of data and allowing investigation through user interaction, is developed in topic no.6. The three main elements of a visualization technique are described according to the model used by Brodlie et al. A classification and a notation for visualization techniques introduced by the same authors is used to support the study of a range of techniques, generic in nature and which can be tailored to different applications. These techniques are illustrated through several examples (extracted also from the references by Brown et al., Keller and Keller, among others). Fundamental algorithms used to implement these techniques have been the subject of some of the student's presentations and can be introduced later on in the semester (as referred in Section 3). The references by Schoeder et al. and Lichenbelt et al., as well as several papers, may be used as bibliography for these presentations. Topic no.7 addresses the main aspects of Computer Graphics, Digital Image Processing and Human Computer Interactions needed to be able to understand the visualization techniques. Addressed issues are: a 2D S/W package; the 3D rendering pipeline including an overview of the problems and methods to achieve realism, geometrical transformations and projections, specifying a view in 3D, visibility, illumination and shading; colour; histogram operations; filters and image transforms and general principles for user interface design. The explanation of these subjects has always in mind its use in Visualization and examples including visualization techniques already introduced are used as much as possible. Topic no.8 is concerned with data. A classification that seems suitable for describing different types of data identified within the used reference model is presented. An overview of basic data representation as well as of data compression techniques is also presented; the first can be supported by the reference of Schroeder et al.; and the second by the reference on graphics file formats by Brown and Shepard. A Visualization system is presented, ideally, as an integrated whole providing means to support the effective exploration of complex data. Topic no.9 presents a classification of visualization software and a variety of existing software products. The adopted classification is an hybrid between the classification used by Brodlie et al. and the one proposed by Brown et al. The software buyer's guide included in Brown et al. is used to illustrate the great variety of existing commercial products and relevant characteristics which need to be considered in the choice of a S/W product. The study of this subject is complemented by demonstrations which some of the students have to make on several of these products (as part of their assignments). This is the general content of the curriculum for the presented course as it has been applied in both academic years of 1997/1998 and 1998/1999. From the initial proposal of this course [1], the main change corresponded to a substantial increase in the number of lectures devoted to the enabling disciplines. The author had initially planned to spend a lesser amount of time on these topics since elective courses on those subjects (Digital Image Processing, Computer Graphics and Human–Computer Interaction) are currently offered at the department and it seemed reasonable to expect that students taking the Visualization course would have already some background in those subjects. The fact that none of the students, in both academic years, had that background came as a surprise; in fact it seems that, most of the students had made this choice in order to get some formation in those areas. For this reason, the author decided to increase substantially the amount of time devoted to enabling technologies; this option, although controversial from the point of view of a visualization curriculum, appeared to be the most reasonable choice for those students, possibly allowing them to benefit the most from the course. The current content of the course follows, in essence, what has been done by the community of Visualization educators [2–4]. It is expected to evolve, according to the experience obtained, each time it will be taught; however, the author expects evolution to occur mostly in the specific way each topic is addressed, bibliography, sequence or duration of different topics, rather than on the overall structure. 3 Teaching methods As referred, the course is organized into 1h 30min lectures during a semester, i.e., two lectures a week; their distribution by the topics is indicated in the previous section. There are no practical classes; students are supposed to perform their assignments, under supervision of the teacher, but on their own time and with a high degree of autonomy. These students usually work full time, which makes it very difficult to organize working sessions involving all the students; that is the reason why only 19 lectures are assigned to the topics, i.e., 5 are left open and are “sacrificed” to activities that could, otherwise, be performed in extra hours. These sessions have been devoted to: • presenting some selected SIGGRAPH videos 1 From Siggraph Video Review Issue 108 — Scientific Visualization 95 which, in spite of having some years, still contains useful examples; however, a more recent issue already exists: Siggraph Video Review Issue 124 — SIGGRAPH 97: Visualization Program, that could be interesting. 1 with examples of applications (1 lecture), • visiting facilities where people use visualization regularly in their work (1 lecture), • presenting student's assignments (2 lectures), and • lectures by people that use or make visualization (1 lecture). Until now two visits were organized, both related to Earth Sciences and inside the Campus, one to a Laboratory using Remote Sensing data and another to a Laboratory of Geophysics including some field demonstration on the acquisition of Ground Penetrating Radar data. Both visits were guided by researchers from those laboratories, showing the students the kind of work they are involved in and how they use at the moment, and they would like to be able to use, visualization techniques and tools. Some other laboratories exist at the University that can be visited in years to come, as in Chemistry and Environment. The two lectures by external people that have been organized were concerned with visualization of Archaeological data and Geographical Information Systems; however, a great variety of other subjects might be interesting. As referred, general objectives of this course include training students on doing bibliographic search, using adequate working methods and correct technical communication (i.e. writing and speaking). Since these objectives are usually easier to achieve when the evaluation is based on practical assignments and the expected number of students in elective courses on this M.Sc. is low, that was the type of evaluation method used in both academic years. Each student has two different assignments, the first consists in giving an oral presentation of 40min on some subject and the second, a more demanding assignment, consists in a work which in general involves some implementation (either developing a code in some language such as C++ or using a visualization S/W as IDL). Both assignments are proposed by the teacher taking into consideration the profile of each student (background, interests, etc.); however, the students are told that they can propose their own assignment by presenting an extended abstract; these proposals may, of course, be rejected. On other courses, offered by the author of this work, some interesting proposals have been presented and accepted; however, on the Visualization course none of the students decided to do it, until now. The choice of the subjects for the oral presentations, as well as for the second assignment, is related to the current interests of both the teacher and the students and to interests of other people (from the University or outside) that ask for collaboration in some specific visualization tasks. During these two academic years the oral presentations have consisted mainly in S/W demonstrations (Matlab, IDL, Iris Explorer and OpenGL) or have addressed fundamental algorithms. Second assignments have been related to the following interest areas: • visualization of archaeological data from a XV century shipwreck existing in the Aveiro Lagoon, • visualization of geophysical data obtained from Ground Penetrating Radar, • visualization of data related to the use of Location-Routing models for obnoxious facilities, • development of an hardware coprocessor for volume rendering implemented with FPGAs, • development of evaluation methodologies for visualizations used in the study of epilepsy and involving data obtained from different modalities as surface electroencephalogram (EEG), depth electroencephalogram (SEEG), tomography (CAT) or magnetic resonance imaging (MRI), • development of evaluation methods for visualizations obtained using raycasting, and • development of evaluation methods involving models of the Human Visual System. For the supervision of many of these assignments the author had the invaluable collaboration of colleagues working in enabling technologies and application areas. These assignments consist, in general, of a bibliographic research on the subject, a proposal of what work could be interesting to develop, an implementation of part of this work, a small technical report and a document written as a scientific paper. Most students put a lot of work and enthusiasm into these assignments and supporting them has been very interesting, however, students typically take long time to finish their tasks. In spite of the fact that this course is not a research-oriented course, such as the one described by Banks [5], many of the recommendations and difficulties presented by this author are also applicable. The choice of subjects for the second assignment, its supervision and finally the preparation of the paper corresponds to a significant burden associated with the course; this solution is only viable for a very small number of students. This type of assignment has turned out to be very stimulating for the more interested students, however for less motivated students it is perhaps too demanding and may result in frustration both for the student and the teacher. For these reasons, the author is considering the possibility of maintaining this second assignment for volunteer students, giving the alternative of an examination for the ones who are not so committed to visualization. One of the main difficulties has to do with the writing of the “scientific paper”, many students just do not like to write (and do not have enough experience) and a special attention is needed to this part of the work (as well as in the preparation of the oral presentation). Difficulty in writing is a general problem of our students, so some extra effort has been devoted to it, in this course. During a lecture, the teacher usually stresses the importance of communicating correctly any work and generally instructs the students how to do it before they start preparing their oral presentations or their papers; some bibliography on the subject of writing and speaking is also recommended [6,7]. The teacher also encourages students to write in English since it is the “lingua franca” in technology. The process of preparing the papers is iterative and some of them need as many as 3 or 4 iterations. Since the teacher and, in some cases, other people also have to invest effort, not only in the supervision but also in reviewing the paper they become co-authors. When the final version of a paper is ready it can be published as an internal note (in the Department's Journal), submitted to a conference or not used at all (according to the nature and quality of the developed work and achieved results). From the 5 papers of 1997/1998, four were published as internal notes; modified versions of two were accepted at conferences organized in Portugal and one will be submitted to another conference. One of the assignments involved porting to a different platform an already existing S/W; in spite of the fact that the student has done enough work to succeed in the course, the obtained results did not justify their publication. This is always a risk of this kind of assignments and care should be taken to minimise it; however, the author feels it is impossible to completely eliminate the possibility of “something going wrong”. All the papers accepted in conferences should be presented by the students, whenever it is possible; this opportunity should be viewed by the students as a “reward” for having done a good work. From the institution's perspective this should be considered as a valuable contribution to the overall quality and good name of the degree. For this reason it was possible, until now, to find institutional financial support for this activity, with the reasonable constraint of submitting papers only to conferences which imply low travelling expenses. 4 Bibliography Knowing how to use and search for bibliography is an important part of any course, however at M.Sc. level it is fundamental. At this level, lectures are mainly meant to give the basic underlying theory and point out important issues, not to present them in detail. Thus, a good bibliography is in fact fundamental; a select bibliography is given to the students and the importance of having the capacity of obtaining new and adequate bibliography is stressed. Due to the fact that visualization is a relatively new discipline, no text books seem to be available (at least in the way in which they exist in other longer-established disciplines such as Computer Graphics or Digital Image Processing). To overcome this type of difficulty usually two alternatives exist: use several books and papers or write some notes to support the course. The second alternative, in the view of the author, is not interesting at M.Sc. level, the students ought to consult several bibliographic references; however, an experienced teacher may write some notes conveying his/her perspective on some of the topics. Since the author does not have enough experience in lecturing this course, even writing some notes seemed out of the question. Thus, the book by Brodlie et al. was used as general support for the course. As it is stated in its preface: This book proposes a framework through which scientific visualization systems may be understood and their capabilities described. It provides overviews of the techniques, data facilities and human computer interface, that are required in a scientific visualization system…. the ways in which scientific visualization has been applied to a wide range of applications are reviewed; and it seemed comprehensive enough to be used as a “reference guide”. However, since it is not a recent and detailed text, some other more up to dated or more detailed references were used as support for several topics, as already mentioned in Section 2. Attempting also to provide the students with a bibliography that can be useful in their future activities, the author selected a small set of bibliographic references that covers the subjects addressed in the lectures. In the next sections the general usage of these bibliographic references is described and a commented bibliography is provided. A list of some other references, which have been useful, for instance to support practical assignments, is also provided along with brief comments. During the lectures, but mainly during the preparation of their assignments, students are strongly advised to become familiar with other possible sources of interesting references such as journals and conference proceedings as well as the different methods of getting them (as in libraries, through databases or at the Internet). In fact, the University Library provides an information system which allows the remote bibliographic search in its databases, other Portuguese databases and in services such as INSPEC, which students are urged to use for their bibliographic research. 4.1 General usage of the selected bibliography The first bibliographic reference (of the list presented in Section 4.2) is, as already mentioned, the one that will provide general guidance for the course. The following three references of the same list can also be used as support for several topics, as definition and goals of visualization, overview of visualization applications, techniques, data characteristics, visualization products and case studies. The most interesting reference by Tufte can be used to support the study of the history and principles of visualization (independently from using computers). The reference by Schroeder et al. may be used to support the study of several subjects as basic data representation and fundamental algorithms used to implement visualization techniques as well as many topics on Computer Graphics. The reference on graphics file formats can be used for the overview on the relevant issues to file formats and data compression. The next five references can all be considered as text books of the so-called enabling technologies of Visualization: Computer Graphics, Human–Computer Interaction and digital image processing; thus they can be used by the students to obtain a background on those technologies. Finally, the last reference can be used to support the introduction to the Human Visual System. 4.2 List of main bibliography Brodlie K, Carpenter L, Earnshaw R, Gallop J, Hubbold R, Mumford A, Osland C, Quarendon P. Scientific visualization, techniques and applications. Berlin: Springer, 1992. This book was written to be a reference guide for the Visualization community on the technical aspects. A framework is described and used to present techniques, data characteristics, products and applications. In spite of not being up to dated in certain aspects it still gives a good overview of the main issues that should be addressed in an introductory course on Visualization. Brown J, Earnshaw R, Jern M, Vince J. Visualization, using computer graphics to explore data and present information. New York: Wiley, 1995. It provides background on the field of visualization giving an overview of design issues, visualization market and various visualization products. It illustrates a wide variety of real-world applications through case studies. Keller P, Keller M. Visual cues. Silver Spring: IEEE Computer Soc. Press, 1993. It is intended for people confronted with the problem of discovering the meaning of their data sets. Using practical examples from many disciplines, it illustrates visualization techniques, provides tips and rules of thumb, that help to produce informative images. Scott Owen G et al. HiperVis-Teaching Scientific Visualization Using Hypermedia, (on-line), acm siggraph Education Committee, education.siggraph.org, 1996. It is a hypermedia document under development, which addresses fundamental topics of Scientific Visualization. Tufte E. The visual display of quantitative information. Graphics Press, 1983. It is concerned with the design of statistical graphics as well as with how to communicate information through the simultaneous presentation of words, numbers and pictures. It reviews the graphical practice in the last two centuries and seeks to account for the differences in quality of graphical designs. Schroeder W, Martin K, Lorensen B. The visualization toolkit, An object oriented approach to 3D graphics, 2nd ed. Englewood Cliffs, NJ: Prentice-Hall, 1998. It offers a view of the field and the opportunity to practice since it includes a cd-rom containing the software and some sample data. It covers the fundamentals of Visualization as well as of Image and Graphics. Brown C. Graphics file formats: reference guide. Englewood Cliffs, NJ: Manning Publications, Prentice-Hall, 1995. It is a comprehensive guide to file formats used in computer graphics and related areas (including Visualization). It discusses implementation and design of file formats focusing on the basic issues for its evaluation and development (as data types, organisation and compression). Foley J, van Dam A, Feiner S, Hughes J. Computer Graphics: principles and practice, 2nd ed. Reading, MA: Addison Wesley, 1990. This is considered the standard reference in Computer Graphics. It deals with the fundamental topics of this area in adequate depth, as well as with many others. Watt A, Policarpo F. The Computer Image. Reading, MA: Addison-Wesley, 1998. It gives an updated coverage of subjects from the three fields of computer imagery which have previously only appeared in separate texts from Computer Graphics, Image Processing and Computer Vision in a coherent overview. Shneiderman B. Designing the user interface-strategies for effective human computer interaction, 3rd ed. Reading, MA: Addison-Wesley, 1998. Provides a complete and current introduction to user interface design. It offers practical techniques and guidelines taking also great care to discuss underlying issues and to support conclusion with empirical results. Dix A. Finlay J, Abowd G, Russell B. Human Computer Interaction, 2nd ed. Englewood Cliffs, NJ: Prentice-Hall, 1998. It is a text book in its area, providing a multidisciplinary perspective of the subject. It covers the basic psychology and computer technology involved and the interface between them, as well as usability and more advanced topics. Gonzalez RC, Woods RE. Digital Image Processing. Reading, MA: Addison-Wesley, 1992. It is commonly used as a text book in its area; it covers the fundamental concepts and methodologies for Digital Image Processing. Fishler M, Firschein O. Intelligence, the eye, the brain and the computer. Reading, MA: Addison-Wesley, 1987. It uses an integrated approach on human and machine intelligence, using knowledge from several areas such as computer science, cognitive science, linguistics, biology, anthropology and psychology. 4.3 Other bibliography Other bibliographic references have been used to support either specific topics of the lectures or practical assignments. These references include papers published in journals and proceedings of conferences as well as books. A list of some of the books (the ones that the author considers may be most useful to a reader of this work) is included. Tufte E. Envisioning information. Graphics Press, 1990. Presents a collection of exemplary designs representing all types of information, widely distributed in time and space, through which design excellence in complex data representation is identified and explained. Includes an interesting chapter on how to use colour. Earnshaw R, Wiseman N. An introductory guide to scientific visualization. Berlin: Springer, 1992. A book intended for readers new in the field, gives a quick summary of what Scientific Visualization is and can do. Can be indicated as an easy to read introduction to the subject. Travis D. Effective color displays: theory and practice. New York: Academic Press, 1991. Addresses colour perception, displays and models. Provides practical guidelines to the effective use of colour, without disregarding theoretical foundation. It is a useful text for designers wanting to use colour. Kaufman A. Recent trends in volume visualization, 2nd IEEE EMBS International Summer School, June 1996, Berder Island, France, 1996. Provides a survey of volume visualization and its trends with a focus on biomedical applications. Rosenblum L, Earnshaw R, Encarnação J, Hagen H, Kaufman A, Sklinenko, Nielson G, Post F, Thalman D. Scientific visualization advances and challenges. IEEE Computer Society, New York: Academic Press, 1994. Demonstrates techniques, examines diverse application areas and addresses relevant issues in Scientific Visualization. Grinstein G, Levkowitz H, editors. Perceptual issues in visualization. Berlin: Springer, 1995. Addresses issues in the field of applied perception, provides a portrayal of the problems that can be addressed towards increasing the effectiveness of information displays. Lichenbelt B, Crane R, Naqvi S. Introduction to volume rendering. Englewood Cliffs, NJ: Prentice-Hall, 1998. Provides an introduction to volume rendering concepts and presents an organised logical progression through the volume rendering pipeline; it also contains a CD-ROM with source code which is useful for practical assignments in the subject. Murray J, VanRyper W. Encyclopedia of graphics file formats. O'Reilly Associates, Inc., 1994. Provides detailed technical information on nearly 100 file formats; it also includes chapters on graphics and file format basics. 5 Conclusions Offering this course has not been an easy task for the author, who has encountered several difficulties; she hopes this work may give a positive contribution to those who may be interested in preparing a course on Visualization. Perhaps the first and main difficulty encountered in the preparation of such a course is related to the lack of text books offering comprehensive approaches to the field. This implies not only a great effort in searching for adequate bibliography but also (and perhaps more importantly) a great effort of organisation and systemisation of the topics to address. This problem can be tackled with the (direct or indirect) help of other more experienced educators and working-groups in the area which publish papers, organise workshops and maintain on-line sites containing useful information (such as siggraph, eurographics, agocg). After the initial effort of putting together a curriculum that makes sense, it is necessary to decide on the best approach for the kind of students one is expecting to encounter, to collect and choose the specific materials to the lectures and last (but not least) to choose the subjects and type of practical assignments and the evaluation method to be used. All these choices have to be re-evaluated every time the course is offered, based on the accumulated experience of the teacher and feedback from the students. The search for new up-to-dated bibliography as well as the choice and preparation of practical assignments which could turn out to be interesting have been the greatest concerns of the author of this work. These concerns are also expected to be the main concerns in the years to come since the proposed curriculum is based on the curricula of similar courses offered in several European and American Universities and it is expected to evolve mostly in the specific way each topic is addressed, bibliography, sequence or duration of different topics, rather than on its overall structure. Acknowledgements The author wishes to thank Prof. Gitta Domik, chair of the acm-siggraph Education for Visualization Committee, for her great help in the initial definition of the curriculum and bibliography of the presented course, as proposed in 1996. References [1] D. Beatriz Sousa Santos An Introductory course on Visualization Revista do Departamento de Electrónica e Telecomunicações (DETUA), Universidade de Aveiro, 2 1 1997 13 16 [2] Domik G. Education for visualization — activities of the acm-siggraph education committee. Proceedings of the Third Eurographics Workshop on Graphics and Visualization, Maastricht, August, 1995. [3] Scott Owen GS, Bailey M, Brodlie K, Domik G. Issues in education for visualization. Proceedings of IEEE Visualization 95, Atlanta, October, November, 1995. p. 427–30. [4] Scott Owen G. et al., HiperVis, ACM SIGGRAPH Education Committee (on line), siggraph.org, 1996. [5] D. Banks Including graphics and visualization research in a master's-level course Computer & Graphics 21 3 1997 379 387 [6] Beer D editor. Writing and speaking in the technology professions, a practical guide. New York: IEEE Press, 1991. [7] Referencing- (In-text-referencing) (on line), (Jan, 1999). "
    },
    {
        "doc_title": "Quantifying the objective quality of voxel based data visualizations produced by a ray caster: A proposal",
        "doc_scopus_id": "0030721073",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030721073",
        "doc_date": "1997-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Volume rendering (VR) method",
            "Voxel based data set"
        ],
        "doc_abstract": "A set of parameters to assess the objective quality of visualizations of a voxel based data set, produced using a ray caster, is proposed as a first step toward the evaluation of the overall quality of these visualizations. Results obtained using synthetic data and a simple implementation of a ray caster are presented. The final goal of this evaluation will be the computation of `confidence indices' that could offer the user a `guided visualization', i.e. allow him/her to decide what are the `best' visualizations of a data set.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Decision support system for the location of hospital facilities: a prototype",
        "doc_scopus_id": "0030314140",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030314140",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Bicriteria model",
            "Visual basic",
            "Windows platform"
        ],
        "doc_abstract": "A prototype of a Decision Support System (DSS) to the location of hospital facilities, which uses a bicriteria model is presented. This DSS is being developed for a Windows platform using Visual Basic and having an open architecture.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Three different approaches in visualization of multimodal clinical data",
        "doc_scopus_id": "0030313573",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030313573",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Problems common to the visualization in multimodal clinical environments are presented. Three different solutions for the visualization of 3D multimodal clinical data are proposed, having in mind the constrains related to the interactivity goal. Some resulting images containing this type of data in an integrated form are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On assessing the error introduced in volume data visualization by direct volume rendering methods",
        "doc_scopus_id": "0030312120",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030312120",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "A method to assess the error introduced in data visualization by Direct Volume Rendering (DVR) methods is proposed. Results obtained using synthetic data and two DVR methods are presented. The final goal will be computing `confidence indices' that help the user to decide which visualizations of a data set to use.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hypermedia authoring tools based on OLE technology",
        "doc_scopus_id": "34249761206",
        "doc_doi": "10.1007/BF01215870",
        "doc_eid": "2-s2.0-34249761206",
        "doc_date": "1995-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This paper report on recent and on going work related to the design of a set of hypermedia authoring tools intended to produce courseware, to be used for self learning or distance learning and training environments in a cooperative way. These tools have roots in a standalone hypermedia editor developed to create courseware, to which were added other tools to enhance its usability, namely a hypermedia player to view courseware in self study mode, and a hypermedia browser to create, to navigate and to display graphically the hypermedia document structure. The underlying metaphors are the overhead projector, the transparency and the layer. This system was developed for the MS-Windows environment, supporting OLE technology. We discuss the impact that such technology might have on the \"look and feel\" of hypermedia learning material and we describe the implementation of a Hypermedia Editor, Browser and Player tools. Part of this work is carried out in the context of the Co-Learn European project, which is aiming to design and to implement a third generation cooperative learning environment. © 1995 Kluwer Academic Publishers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D registration and integrated visualization of multimodal clinical data",
        "doc_scopus_id": "0028758322",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0028758322",
        "doc_date": "1994-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Computed axial tomography",
            "Inertial moments",
            "Integrated visualization",
            "Multimodal clinical data",
            "Multimodality",
            "Raycaster philosophy",
            "Single photon emission computed tomography",
            "Three dimensional registration"
        ],
        "doc_abstract": "We present in this article a registration solution and a 3D integrated representation of the fused modalities, CT and SPECT. The registration process is based on the inertial moments determination. The visualization process uses a raycaster philosophy with multiple objects in volume. Functional data is represented with pseudo colour on orthogonal clipping planes, anatomical information is displayed using a raycaster approach with a grey shade look up table. The registration process is evaluated with a phantom derived from a genuine clinical volume. A case study is presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of two models for the quantification of left ventricle function",
        "doc_scopus_id": "0028746481",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0028746481",
        "doc_date": "1994-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Contractibility",
            "Left ventricle function",
            "Principal component analysis"
        ],
        "doc_abstract": "A comparison of two models for the quantification of the Left Ventricle (LV) wall motion is presented. This comparison was made using Principal Component Analysis (PCA) as an attempt to investigate if two of the most commonly used models produce comparable results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "HyDE: A hypermedia document editor based on OLE technology",
        "doc_scopus_id": "0028138073",
        "doc_doi": "10.1109/mmcs.1994.292482",
        "doc_eid": "2-s2.0-0028138073",
        "doc_date": "1994-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Cooperative and distance education",
            "Human computer interaction",
            "Hypermedia",
            "Hypermedia document editor",
            "Multimedia"
        ],
        "doc_abstract": "This paper reports on recent and on going work related to the design of a Hypermedia Document Editor (HyDE) intended to produce courseware to be used in a cooperative and distance learning environment. This work is carried out in the context of the Co-Learn European project, which is aiming to design and to implement a third generation cooperative learning environment. We describe the impact that OLE technology might have on the `look and feel' of hypermedia learning material, the HyDE implementation and the application functionality.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An integrated courseware editor based on OLE technology",
        "doc_scopus_id": "84935422425",
        "doc_doi": "10.1007/3-540-57312-7_102",
        "doc_eid": "2-s2.0-84935422425",
        "doc_date": "1993-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Courseware",
            "Hypermedia documents",
            "Hypermedia environment",
            "Poster sessions"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 1993.We described an integrated courseware editor based on OLE technology. Our courseware editor integrates existing applications like, drawing programs, spreadsheets, sound recorders and text processors into one consistent hypermedia environment. Based on a metaphor of an “electronic” overhead projector both authoring and reading of hypermedia documents are combined in one modeless editor. Our current prototype will be demonstrated at the poster session.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Data structures for multimodality imaging: concepts and implementation",
        "doc_scopus_id": "0026963097",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0026963097",
        "doc_date": "1992-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Digital subtraction angiography",
            "Linear octree concept",
            "Single photon emission computed tomography"
        ],
        "doc_abstract": "The integration of data coming from different imaging modalities is something to take into account, due to the importance it can have in the development of a fast and reliable diagnosis by the health staff. In the medical imaging field, computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), and single photon emission computed tomography (SPECT) are examples of devices that generate 3- D data. Digital subtraction angiography (DSA) or ultrasound (US) output 2-D data can be employed for reconstructing 3-D data. An important fact is that 3-D space is common to all these devices and they are all capable of producing large amounts of data. Prior to display or even data integration, matching the various 3-D spaces has to be achieved with some specific technique, according to the anatomical region under examination. The augmented octree, an extension of the linear octree, is used for data integration; its properties can help overcome some of the constraints that occur in medical imaging. To be fully accepted by the specialist, the display and manipulation of multimodality data must be interactive and done in real-time, or at least in `nearly' real-time. Parallel architectures seem to be a solution for some computation intensive applications, and so an implementation of the linear octree encoding process was developed on a 16 Transputer machine.",
        "available": false,
        "clean_text": ""
    }
]