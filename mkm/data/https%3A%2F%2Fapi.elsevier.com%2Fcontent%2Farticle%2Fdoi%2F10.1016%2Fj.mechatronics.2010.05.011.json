{"scopus-eid": "2-s2.0-79952617090", "originalText": "serial JL 271456 291210 291718 291787 291883 31 Mechatronics MECHATRONICS 2010-07-01 2010-07-01 2011-03-08T22:21:19 1-s2.0-S0957415810001029 S0957-4158(10)00102-9 S0957415810001029 10.1016/j.mechatronics.2010.05.011 S300 S300.1 FULL-TEXT 1-s2.0-S0957415811X0003X 2015-05-15T03:43:39.596909-04:00 0 0 20110301 20110331 2011 2010-07-01T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0957-4158 09574158 21 21 2 2 Volume 21, Issue 2 7 411 422 411 422 201103 March 2011 2011-03-01 2011-03-31 2011 Special Issue on Advances in intelligent robot design for the Robocup Middle Size League M.J.G. van de Molengraft O. Zweigle Special Issue on Advances in intelligent robot design for the Robocup Middle Size League article fla Copyright \u00a9 2010 Elsevier Ltd. All rights reserved. WORLDMODELINGMSLROBOTICSOCCERTEAM SILVA J 1 Introduction 2 Related work 3 Localization 4 Ball integration 4.1 Ball position 4.2 Ball velocity 4.3 Team ball position sharing 5 Obstacle treatment 5.1 Visual obstacle detection 5.2 Obstacle selection and identification 5.3 Obstacle sharing 6 Conclusion and future work Acknowledgment References METROPOLIS 1949 335 341 N KALMAN 1960 35 45 R LUO 2002 107 119 R LEONARD 1991 376 382 J FOX 1999 391 427 D MOURIKIS 2006 666 681 A DURRANTWHYTE 2008 H SPRINGERHANDBOOKROBOTICS MULTISENSORDATAFUSION BEJCZY 2004 41 42 A ALENYA 2004 23 32 G CHROUST 2004 73 83 S THRUN 2005 W PROBABILISTICROBOTICS SICILIANO 2008 B SPRINGERHANDBOOKROBOTICS LAUER 2005 291 303 M KI2005ADVANCESINARTIFICIALINTELLIGENCE MODELINGMOVINGOBJECTSINADYNAMICALLYCHANGINGROBOTAPPLICATION FERREIN 2006 154 165 A ROBOCUP2005ROBOTSOCCERWORLDCUPIX COMPARINGSENSORFUSIONTECHNIQUESFORBALLPOSITIONESTIMATION LAUER 2006 142 153 M ROBOCUP2005ROBOTSOCCERWORLDCUPIX CALCULATINGPERFECTMATCHEFFICIENTACCURATEAPPROACHFORROBOTSELFLOCALIZATION NEVES 2007 499 507 A PROGRESSINARTIFICIALINTELLIGENCE OMNIDIRECTIONALVISIONSYSTEMFORSOCCERROBOTS CUNHA 2008 417 424 B ROBOCUP2007ROBOTSOCCERWORLDCUPXI OBTAININGINVERSEDISTANCEMAPANONSVPHYPERBOLICCATADIOPTRICROBOTICVISIONSYSTEM SILVAX2011X411 SILVAX2011X411X422 SILVAX2011X411XJ SILVAX2011X411X422XJ item S0957-4158(10)00102-9 S0957415810001029 1-s2.0-S0957415810001029 10.1016/j.mechatronics.2010.05.011 271456 2011-03-10T12:04:28.24434-05:00 2011-03-01 2011-03-31 1-s2.0-S0957415810001029-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/MAIN/application/pdf/316d3f03c10b16304bc668359b822445/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/MAIN/application/pdf/316d3f03c10b16304bc668359b822445/main.pdf main.pdf pdf true 2037530 MAIN 12 1-s2.0-S0957415810001029-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/PREVIEW/image/png/6cd8567b6f03d3eff58b3005f8f4d429/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/PREVIEW/image/png/6cd8567b6f03d3eff58b3005f8f4d429/main_1.png main_1.png png 77452 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0957415810001029-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/STRIPIN/image/gif/0c5a1a2538970ffe182ef7cd59831b2f/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/STRIPIN/image/gif/0c5a1a2538970ffe182ef7cd59831b2f/si1.gif si1 si1.gif gif 820 43 146 ALTIMG 1-s2.0-S0957415810001029-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/STRIPIN/image/gif/c05f4c2130098e8dd5d305d6d9986391/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/STRIPIN/image/gif/c05f4c2130098e8dd5d305d6d9986391/si4.gif si4 si4.gif gif 279 20 19 ALTIMG 1-s2.0-S0957415810001029-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/STRIPIN/image/gif/c05f4c2130098e8dd5d305d6d9986391/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/STRIPIN/image/gif/c05f4c2130098e8dd5d305d6d9986391/si4.gif si3 si3.gif gif 279 20 19 ALTIMG 1-s2.0-S0957415810001029-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/STRIPIN/image/gif/19a28c188b58e1df740651110fcd8980/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/STRIPIN/image/gif/19a28c188b58e1df740651110fcd8980/si2.gif si2 si2.gif gif 672 40 84 ALTIMG 1-s2.0-S0957415810001029-gr10.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr10/DOWNSAMPLED/image/jpeg/7f0b8d817b18689889ccd45426e3ead8/gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr10/DOWNSAMPLED/image/jpeg/7f0b8d817b18689889ccd45426e3ead8/gr10.jpg gr10 gr10.jpg jpg 40478 301 366 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr10.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr10/THUMBNAIL/image/gif/a31b38399ef5f785a2fc1e9a1262660b/gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr10/THUMBNAIL/image/gif/a31b38399ef5f785a2fc1e9a1262660b/gr10.sml gr10 gr10.sml sml 6601 164 200 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr11.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr11/DOWNSAMPLED/image/jpeg/ba4074f998aadcadb559633d98ae3fae/gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr11/DOWNSAMPLED/image/jpeg/ba4074f998aadcadb559633d98ae3fae/gr11.jpg gr11 gr11.jpg jpg 38816 374 635 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr11.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr11/THUMBNAIL/image/gif/cec7dc9960367ded202d0d5a06d6a2cc/gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr11/THUMBNAIL/image/gif/cec7dc9960367ded202d0d5a06d6a2cc/gr11.sml gr11 gr11.sml sml 3816 129 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr12.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr12/DOWNSAMPLED/image/jpeg/71d98416c5a46f85900640db70c79533/gr12.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr12/DOWNSAMPLED/image/jpeg/71d98416c5a46f85900640db70c79533/gr12.jpg gr12 gr12.jpg jpg 45526 461 779 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr12.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr12/THUMBNAIL/image/gif/388b71d13e2cab60e57af2571ed5edb3/gr12.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr12/THUMBNAIL/image/gif/388b71d13e2cab60e57af2571ed5edb3/gr12.sml gr12 gr12.sml sml 2642 130 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr13.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr13/DOWNSAMPLED/image/jpeg/92e0f8ac5187f8717a772e661790665f/gr13.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr13/DOWNSAMPLED/image/jpeg/92e0f8ac5187f8717a772e661790665f/gr13.jpg gr13 gr13.jpg jpg 25831 160 330 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr13.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr13/THUMBNAIL/image/gif/ff30c1eaed3092c05b9f2ce7de7b9735/gr13.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr13/THUMBNAIL/image/gif/ff30c1eaed3092c05b9f2ce7de7b9735/gr13.sml gr13 gr13.sml sml 9938 106 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr14.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr14/DOWNSAMPLED/image/jpeg/cf4287f8ec2ab74e0500530ded979e07/gr14.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr14/DOWNSAMPLED/image/jpeg/cf4287f8ec2ab74e0500530ded979e07/gr14.jpg gr14 gr14.jpg jpg 26031 374 628 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr14.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr14/THUMBNAIL/image/gif/c1a66b88fe222137004d93cdfe840efe/gr14.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr14/THUMBNAIL/image/gif/c1a66b88fe222137004d93cdfe840efe/gr14.sml gr14 gr14.sml sml 2725 130 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr15.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr15/DOWNSAMPLED/image/jpeg/4bf7aa0385289d15c947cb599ff256e4/gr15.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr15/DOWNSAMPLED/image/jpeg/4bf7aa0385289d15c947cb599ff256e4/gr15.jpg gr15 gr15.jpg jpg 25951 161 322 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr15.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr15/THUMBNAIL/image/gif/becf404f82f5f18fa7c43723b06cb6cd/gr15.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr15/THUMBNAIL/image/gif/becf404f82f5f18fa7c43723b06cb6cd/gr15.sml gr15 gr15.sml sml 10234 109 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr16.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr16/DOWNSAMPLED/image/jpeg/336ab4d3fa9c67e23365a288a13d544b/gr16.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr16/DOWNSAMPLED/image/jpeg/336ab4d3fa9c67e23365a288a13d544b/gr16.jpg gr16 gr16.jpg jpg 11424 220 243 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr16.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr16/THUMBNAIL/image/gif/0dccec18f9f7cef517d2eed1f8571c4a/gr16.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr16/THUMBNAIL/image/gif/0dccec18f9f7cef517d2eed1f8571c4a/gr16.sml gr16 gr16.sml sml 3895 164 181 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr17.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr17/DOWNSAMPLED/image/jpeg/a83ee13f6383653614a910f062ead743/gr17.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr17/DOWNSAMPLED/image/jpeg/a83ee13f6383653614a910f062ead743/gr17.jpg gr17 gr17.jpg jpg 34129 332 324 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr17.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr17/THUMBNAIL/image/gif/0a133ac93d1f123a93f90941d32ea7aa/gr17.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr17/THUMBNAIL/image/gif/0a133ac93d1f123a93f90941d32ea7aa/gr17.sml gr17 gr17.sml sml 9007 164 160 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr18.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr18/DOWNSAMPLED/image/jpeg/31d1bf028146c02f03feaf1973798092/gr18.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr18/DOWNSAMPLED/image/jpeg/31d1bf028146c02f03feaf1973798092/gr18.jpg gr18 gr18.jpg jpg 28869 336 320 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr18.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr18/THUMBNAIL/image/gif/1b2ad6b63a924d2f47d8cffdf9540913/gr18.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr18/THUMBNAIL/image/gif/1b2ad6b63a924d2f47d8cffdf9540913/gr18.sml gr18 gr18.sml sml 8228 164 156 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr19.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr19/DOWNSAMPLED/image/jpeg/acb806fff4eac0a782e752916637be0a/gr19.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr19/DOWNSAMPLED/image/jpeg/acb806fff4eac0a782e752916637be0a/gr19.jpg gr19 gr19.jpg jpg 31951 292 350 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr19.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr19/THUMBNAIL/image/gif/7678b6d7d6d233a39388adb3b06dbd99/gr19.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr19/THUMBNAIL/image/gif/7678b6d7d6d233a39388adb3b06dbd99/gr19.sml gr19 gr19.sml sml 4895 164 196 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr2/DOWNSAMPLED/image/jpeg/d729e8e05037cc76f8bb6862657217a3/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr2/DOWNSAMPLED/image/jpeg/d729e8e05037cc76f8bb6862657217a3/gr2.jpg gr2 gr2.jpg jpg 24610 181 349 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr2/THUMBNAIL/image/gif/2e173a2d3967c95ca412e12685b44f2e/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr2/THUMBNAIL/image/gif/2e173a2d3967c95ca412e12685b44f2e/gr2.sml gr2 gr2.sml sml 9025 114 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr20.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr20/DOWNSAMPLED/image/jpeg/7fe3b79a6e90cfc941dbf6add414bb76/gr20.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr20/DOWNSAMPLED/image/jpeg/7fe3b79a6e90cfc941dbf6add414bb76/gr20.jpg gr20 gr20.jpg jpg 32518 300 345 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr20.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr20/THUMBNAIL/image/gif/3d858d757c5ca71a2992b6c96e113ba6/gr20.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr20/THUMBNAIL/image/gif/3d858d757c5ca71a2992b6c96e113ba6/gr20.sml gr20 gr20.sml sml 5136 163 188 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr21.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr21/DOWNSAMPLED/image/jpeg/31a93a9dca248f66e7a344f5a08e86ea/gr21.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr21/DOWNSAMPLED/image/jpeg/31a93a9dca248f66e7a344f5a08e86ea/gr21.jpg gr21 gr21.jpg jpg 11053 222 356 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr21.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr21/THUMBNAIL/image/gif/828af8b97415e7d8bdb0ee0b5aa8cbac/gr21.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr21/THUMBNAIL/image/gif/828af8b97415e7d8bdb0ee0b5aa8cbac/gr21.sml gr21 gr21.sml sml 3093 137 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr3/DOWNSAMPLED/image/jpeg/3e97f8ecf0d9dacc67da395434b0efe2/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr3/DOWNSAMPLED/image/jpeg/3e97f8ecf0d9dacc67da395434b0efe2/gr3.jpg gr3 gr3.jpg jpg 14423 252 223 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr3/THUMBNAIL/image/gif/7a8220c5a438f4fa33c88c4baf9ebd77/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr3/THUMBNAIL/image/gif/7a8220c5a438f4fa33c88c4baf9ebd77/gr3.sml gr3 gr3.sml sml 4214 164 145 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr4/DOWNSAMPLED/image/jpeg/d2bbecdbc169e6a1f2f9af82d63f7905/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr4/DOWNSAMPLED/image/jpeg/d2bbecdbc169e6a1f2f9af82d63f7905/gr4.jpg gr4 gr4.jpg jpg 90061 420 715 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr4/THUMBNAIL/image/gif/8720c1f2d5ce5cf4572edfcbbbd7b8d8/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr4/THUMBNAIL/image/gif/8720c1f2d5ce5cf4572edfcbbbd7b8d8/gr4.sml gr4 gr4.sml sml 6962 129 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr5/DOWNSAMPLED/image/jpeg/feeef9d3d3a5ff6a540b058be712822c/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr5/DOWNSAMPLED/image/jpeg/feeef9d3d3a5ff6a540b058be712822c/gr5.jpg gr5 gr5.jpg jpg 31815 170 578 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr5/THUMBNAIL/image/gif/f221b6b477b2d34ef07fabd950c242f6/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr5/THUMBNAIL/image/gif/f221b6b477b2d34ef07fabd950c242f6/gr5.sml gr5 gr5.sml sml 3190 64 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr6/DOWNSAMPLED/image/jpeg/fdde411f7e3f35f3e373ee023ec1df79/gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr6/DOWNSAMPLED/image/jpeg/fdde411f7e3f35f3e373ee023ec1df79/gr6.jpg gr6 gr6.jpg jpg 34213 295 372 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr6/THUMBNAIL/image/gif/b7b5fdfadc26d2c19261fe4aef85d821/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr6/THUMBNAIL/image/gif/b7b5fdfadc26d2c19261fe4aef85d821/gr6.sml gr6 gr6.sml sml 5554 164 207 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr7/DOWNSAMPLED/image/jpeg/899f58adc8ecf61114dca9fa7cf06619/gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr7/DOWNSAMPLED/image/jpeg/899f58adc8ecf61114dca9fa7cf06619/gr7.jpg gr7 gr7.jpg jpg 32017 304 505 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr7/THUMBNAIL/image/gif/71eed0cf63e06fa8cb3fe1831f83dae9/gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr7/THUMBNAIL/image/gif/71eed0cf63e06fa8cb3fe1831f83dae9/gr7.sml gr7 gr7.sml sml 3661 132 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr8/DOWNSAMPLED/image/jpeg/8419bb312a122063a1720a4ccd30296c/gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr8/DOWNSAMPLED/image/jpeg/8419bb312a122063a1720a4ccd30296c/gr8.jpg gr8 gr8.jpg jpg 11017 117 266 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr8/THUMBNAIL/image/gif/908bbe17cee538caf95300efd36781fc/gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr8/THUMBNAIL/image/gif/908bbe17cee538caf95300efd36781fc/gr8.sml gr8 gr8.sml sml 2052 96 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr9.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr9/DOWNSAMPLED/image/jpeg/9007dde5edab5832ec28b0efe5bbf10c/gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr9/DOWNSAMPLED/image/jpeg/9007dde5edab5832ec28b0efe5bbf10c/gr9.jpg gr9 gr9.jpg jpg 53806 297 357 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr9/THUMBNAIL/image/gif/c18ff85fb780f4b2e6a73ff51a9e0f95/gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr9/THUMBNAIL/image/gif/c18ff85fb780f4b2e6a73ff51a9e0f95/gr9.sml gr9 gr9.sml sml 9002 164 197 IMAGE-THUMBNAIL 1-s2.0-S0957415810001029-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr1/DOWNSAMPLED/image/jpeg/2ea9db5ce576d22fa4ae92852f8fac12/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr1/DOWNSAMPLED/image/jpeg/2ea9db5ce576d22fa4ae92852f8fac12/gr1.jpg gr1 gr1.jpg jpg 35942 173 489 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810001029-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810001029/gr1/THUMBNAIL/image/gif/3b2946b79c6abfb4aee2e7881514c299/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810001029/gr1/THUMBNAIL/image/gif/3b2946b79c6abfb4aee2e7881514c299/gr1.sml gr1 gr1.sml sml 9273 77 219 IMAGE-THUMBNAIL MECH 1165 S0957-4158(10)00102-9 10.1016/j.mechatronics.2010.05.011 Elsevier Ltd Fig. 1 Picture of the team robots used to obtain the results presented on this paper. Fig. 2 Captures of an image acquired by the robot camera and processed by the vision algorithms. Left (a): The image acquired by the camera. Right (b): The same image after processing with magenta dots over the detected field lines. Fig. 3 Illustration of the compass error angle intervals. Fig. 4 Illustration of two situations where relocation was forced. Dashed line represents the angle given by the compass, solid line represents the angle estimated by the localization algorithm, red lines represent the cycles on which the error between the two angles is greater than the threshold. Left (a): The camera was covered while the robot moved. The estimated orientation error degrades progressively and after getting higher than a threshold, the cycle count starts and forces relocation. Right (b): The robot tilted. The estimated orientation error is immediately affected by more than a threshold and the cycle count starts and forces relocation. Fig. 5 Noisy position of a static ball taken from a rotating robot. Fig. 6 Plot of a robot movement around a fixed ball position. The ball positions measured by the moving robot form a cloud of points (green) in the area of the real ball position (black X). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 7 Plot of a ball movement situation. Fig. 8 Situation where a hard deviation would be detected by the filter. Positions R4,5,6, are the measured positions after the ball hits an obstacle, P4,5,6 are the predicted filtered estimations, which did not consider that something might alter the ball path. Fig. 9 Velocity representation using consecutive measures displacement. Fig. 10 Velocity representation using linear regression over Kalman filtered positions. Fig. 11 Comparison between the velocity estimated by the linear regression (blue solid line, faster convergence) and internally by the Kalman filter (red dashed line, smoother, but of slow convergence). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Diagram of the ball integration algorithm. Fig. 13 Captures of an image acquired by the robot camera and processed by the vision algorithms. The areas of interest were surrounded. (a) The image acquired by the camera. (b) The same image after processing. Obstacles are identified by their center (triangle), left and right limits (squares). It is visible that the two aligned obstacles are detected as a single larger obstacle (top right of the frames). Fig. 14 Relation between pixels and metric distances. The center of the robot is considered the origin and the metric distances are considered on the ground plane. Fig. 15 Example of an image acquired by the robot camera and processed by the vision algorithm. The areas of interest are surrounded. (a) The image acquired by the camera. (b) The same image after processing. It is visible the two possibilities of separation made: angular separation, on the bottom pair of obstacles and length separation, on the top pair of obstacles. Fig. 16 When a CAMBADA robot is on, the estimated centers of the detected obstacles are compared with the known position of the team mates and tested; the left obstacle is within the CAMBADA acceptance radius, the right one is not. Fig. 17 Illustration of single obstacles identification. (a) Image acquired from the robot camera (obstacles for identification are marked). (b) The same image after processing. (c) Image of the control station. Each robot represents itself and robot 6 (the lighter gray) draws all the five obstacles evaluated (squares with the same gray scale as itself). All team mates were correctly identified (marked by its corresponding number over the obstacle square) and the opponent is also represented with no number. Fig. 18 Illustration of multiple obstacles identification. (a) Image acquired from the robot camera (obstacle for identification marked). (b) The same image after processing. Visually, the aligned robots are only one large obstacle. (c) Image of the control station. Each robot represents itself and robot 6 (the darker gray) draws all the five obstacles (squares with the same gray scale as itself). The visual obstacle was successfully separated into the several composing obstacles, and all of them were correctly identified as the correspondent team mate (marked by its corresponding number over the obstacle square) and the opponent is also represented with no number. Fig. 19 Representation of a capture of the obstacle identification algorithm results. The path taken by the observer is represented by blue dots in the rectangular path taken. Near the center, the pivot shared position is represented by the black star and its limits by the black circle. The blob of red is the overlapping positions of the identified obstacle center, represented by a red cross. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 20 Representation of the path taken by the team mate to identify (the red dots represent each communicated position). The observer position is represented by the black star and its limits by the black circle. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 21 Image of the control station showing an obstacle of robot 2 that was not seen by itself (on the center of the field). In this case it assumes the obstacle by confirmation of both robots 5 and 6. Table 1 The mean and standard deviation of the capture perceived obstacle position. Perceived obstacle X Y Mean 0.05 2.01 Std 0.08 0.07 \u2223Real\u2212perceived\u2223=0.16. \u2223Std\u2223=0.10. Table 2 The individual ratio of successful identification of the moving team mate for the several captures performed. Total cycles Successes % Capture 1 1798 1319 73 Capture 2 1065 748 70 Capture 3 1528 1332 87 Capture 4 1162 769 66 Capture 5 1935 1278 66 Capture 6 2152 1411 66 World modeling on an MSL robotic soccer team Jo\u00e3o Silva \u204e Nuno Lau Ant\u00f3nio J.R. Neves Jo\u00e3o Rodrigues Jos\u00e9 Lu\u00eds Azevedo ATRI, IEETA/DETI, University of Aveiro, 3810-193 Aveiro, Portugal \u204e Corresponding author. When a team of robots is built with the objective of playing soccer, the coordination and control algorithms must reason, decide and actuate based on the current conditions of the robot and its surroundings. This is where sensor and information fusion techniques appear, providing the means to build an accurate model of the world around the robot, based on its own limited sensor information and the also limited information obtained through communication with the team mates. One of the most important elements of the world model is the robot self-localization, as to be able to decide what to do in an effective way, it must know its position in the field of play. In this paper, the team localization algorithm is presented focusing on the integration of visual and compass information. An important element in a soccer game, perhaps the most important, is the ball. To improve the estimations of the ball position and velocity, two different techniques have been developed. A study of the visual sensor noise is presented and, according to this analysis, the resulting noise variation is used to define the parameters of a Kalman filter for ball position estimation. Moreover, linear regression is used for velocity estimation purposes, both for the ball and the robot. This implementation of linear regression has an adaptive buffer size so that, on hard deviations from the path (detected using the Kalman filter), the regression converges faster. A team cooperation method based on sharing the ball position is presented. Other important data during the soccer game is obstacle data. This is an important challenge for cooperation purposes, allowing the improvement of team strategy with ball covering, dribble corridor estimation, pass lines, among other strategic possibilities. Thus, detecting the obstacles is ceasing to be enough and identifying which obstacles are team mates and opponents is becoming a need. An approach for this identification is presented, considering the visual information, the known characteristics of the team robots and shared localization among team members. The described work was implemented on the CAMBADA team and allowed it to achieve particularly good performances in the last two years, with a 1st and a 3rd place in the world championship RoboCup 2008 and RoboCup 2009 editions, respectively, as well as distinctively achieve 1st place in 2008 and 2009 editions of the Portuguese Robotics Open. Keywords Sensor fusion World model Kalman filter Linear regression Obstacle detection Visual matching 1 Introduction Nowadays, there are several research domains in the area of multi robot systems. One of the most popular is robotic soccer. RoboCup 1 http://www.robocup.org/. 1 is an international joint project to promote artificial intelligence, robotics and related fields. Most of the RoboCup leagues have soccer as platform for developing technology, either at software or hardware levels, with single or multiple agents, cooperative or competitive [1]. Among RoboCup leagues, the Middle Size League (MSL) is one of the most challenging. In this league, each team is composed of up to five robots with maximum size of 50\u00d750cm base, 80cm height and a maximum weight of 40kg, playing in a field of 18\u00d712m. The rules of the game are similar to the official FIFA rules, with required changes to adapt for the playing robots [2]. Each robot is autonomous and has its own sensorial means. They can communicate with each other, and with an external computer acting as a coach, through a wireless network. This coach computer cannot have any sensor, it only knows what is reported by the playing robots. The agents should be able to evaluate the state of the world and make decisions suitable to fulfill the cooperative team objective. CAMBADA, Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture, is the Middle Size League Robotic Soccer team from the University of Aveiro. The project started in 2003, coordinated by the IEETA 2 Instituto de Engenharia Electr\u00f3nica e Telem\u00e1tica de Aveiro \u2013 Aveiro\u2019s Institute of Electronic and Telematic Engineering. 2 ATRI 3 Actividade Transversal em Rob\u00f3tica Inteligente \u2013 Transverse Activity on Intelligent Robotics. 3 group and involves people working on several areas for building the mechanical structure of the robot, its hardware architecture and controllers and the software development in areas such as image analysis and processing, sensor and information fusion, reasoning and control (see Fig. 1 ). This paper provides a description of some sensor and information fusion techniques and algorithms used in the CAMBADA team. The data obtained by these techniques are necessary for building a world model of the robot environment. This paper includes the description of some of the elements of that model necessary for a team of robots to play soccer. In Section 2, a brief overview of some related topics and work in sensor and information fusion for world modeling are presented. Section 3 presents the team self-localization description, introducing it as the first necessary step for all the other information fusion. In Section 4, the ball integration process is presented in all its components, starting with the ball position, its velocity and finally its sharing among team mates. Section 5 presents an overview of obstacle treatment, with some visual detection details, the matching of positions for visual identification and the sharing of information among team mates. Finally, Section 6 concludes the paper. 2 Related work World modeling and sensor and information fusion are tightly related, as the latest provide the means to build the desired model. Sensor and information fusion is the process of combining sensory data, or data derived from sensory data, providing a resulting information that is better than would be possible when the sources were used individually [3]. One of the main areas where sensor fusion techniques are used is position tracking, both for self and object localization/tracking. The integration of information over time in order to filter sensor noise is essential to get better estimates. This type of integration may be performed using Kalman filter based approaches, Monte Carlo methods or Markov approaches. Generally, Monte Carlo [4] approaches have better performance in cases where great discontinuities of the output values are expected, as the assumption of Gaussian probability density functions of the Kalman filter [5] is usually less accurate. However, Kalman filtering is a very effective method if the assumptions of Gaussian noise can be met and the system can be linearized. Other common approaches are the use of the Extended and Unscented Kalman filters [6], which are prepared to deal with non-linear systems at the cost of more computational weight. A general overview of different methods of multi-sensor and information fusion is presented in [7], also with a brief description of application areas, such as robotics, military, biomedical and transportation. Applications in the robotics field include self-localization using either Kalman filter [8], Monte Carlo [9] or Markov [10] methods, or integration of information coming from several robots, to increase the accuracy of each of the robots position estimation [11]. A general recent overview of methods and architectures for multi-sensor data fusion can be found in [12]. Another recurrent problem nowadays is the fusion of visual and inertial sensors [13], where recent results have demonstrated that the visual tracking of objects may work at higher velocities and be more robust if combined with information coming from inertial sensors [14] and also that ego-motion estimation can be more precise and navigation more robust using these approaches [15]. Simultaneous Localization And Mapping (SLAM) is another common application of sensor fusion techniques, as in many cases, autonomous robots have to map the environment rather than simply localize themselves [16,17]. Particularly in RoboCup domain, several teams use this kind of approaches, not only for localization purposes, but also for position estimation and tracking of objects, namely the ball and other robots. Several teams have used Kalman filters for the ball position estimation [18\u201321]. In [20,21], several information fusion methods are compared for the integration of the ball position using several observers. In [21], the authors conclude that the Kalman reset filter shows the best performance. Although using well known techniques, in this paper we propose practical solutions for an efficient self-localization, ball information treatment and obstacle treatment for an MSL robotic soccer team. As far as we know, no previous work has been published focusing on these several important aspects of developing the world model of an MSL soccer team. 3 Localization Self-localization of the agent is an important issue for a soccer team, as strategic moves and positioning must be defined by positions on the field. In the MSL, the environment is partially known, as every agent knows exactly the layout of the game field but does not know the position of any other elements, either itself, other robots or the ball. Given the known map, the agent has then to locate itself. The CAMBADA team localization algorithm is based on the detected field lines, with fusion of information from the odometry sensors and an electronic compass. It is based on the approach described in [22], with some adaptations. It can be seen as an error minimization task, with a derived measure of reliability of the calculated position so that a stochastic sensor fusion process can be applied to increase the estimation accuracy [22]. From the center of the image (the center of the robot), radial sensors are created around the robot, each one represented by a line with a given angle. These are called scanlines. The image processing, in each cycle, returns a list of positions relative to the robot where the scanlines intercept the field line markings [23]. The idea is to analyze the detected line points, estimating a position, and through an error function describe the fitness of the estimation. This is done by reducing the error of the matching between the detected lines and the known field lines (Fig. 2 ). The error function must be defined considering the substantial amount of noise that affects the detected line points which would distort the representation estimation [22]. In normal operation mode, the localization is done over a limited set of base positions from which tracking is maintained. Since it is an algorithm based on optimization and since there are many local minima, the tracking only works satisfactorily if the estimations are near the solution. In situations where the robot does not possess a valid estimation, a global localization algorithm estimates the robot position on the field using a much wider set of initial estimations over which the already referred error minimization process for optimization is applied. However, this global localization algorithm is computationally heavy and time consuming. For that reason, after having an initial position, the simpler tracking localization handles the cyclic relocation. Although the odometry measurement quality quickly degrades with time, within the reduced cycle times achieved in the application, consecutive readings produce acceptable results and thus, having the visual estimation, it is fused with the odometry values to refine the estimation. This fusion is based on a Kalman filter for the robot position estimated by odometry and the robot position estimated by visual information. This approach allows the agent to estimate its position even if no visual information is available. However, it is not reliable to use only odometry values to estimate the position for more than a few cycles, as slidings and frictions on the wheels produce large errors on the estimations in short time. Due to the nature of the approach, this algorithm works acceptably with a relatively low number of points, like a few tens of points, as long as they are representative of the surroundings. Consider the case of matching a 90degrees corner. If the algorithm had access to 200 points all over the same line, it would not be capable of matching the corner. On the other hand, with only 20 or 30 points scattered over both lines, the algorithm would be capable of detecting the match. Even in situations where the points are over the same line, the merging with odometry and position tracking provide a good robustness to the algorithm [22], as long as the situation is temporary, which is usually the case. The visually estimated orientation can be ambiguous, i.e. each point on the soccer field has a symmetric position, relatively to the field center, where the robot detects exactly the same field lines. To disambiguate the symmetry problem and to detect wrong estimations, an electronic compass is used. The orientation estimated by the robot is compared to the orientation given by the compass and if the error between them is larger than a predefined threshold, actions are taken. If the error is really large (i.e. around \u00b1180degrees), it means that the robot estimated orientation is symmetric to the real one, so it should assume the mirror position. On the other hand, if the error is larger than the acceptance threshold (i.e. a 90degrees acceptable area), a counter is incremented (Fig. 3 ). This counter will be incremented every cycle in which the error is greater than the threshold. If a given number of consecutive cycles with high errors is reached (i.e. the counter reaches a given number, currently 10), the robot considers itself \u201clost\u201d, meaning that it will not continue to track its position but will instead consider the initial situation, with no a priori knowledge and thus executes the global localization algorithm. Fig. 4 shows situations where the threshold was reached and relocation was forced after some cycles. 4 Ball integration The information of the ball state (position and velocity) is, perhaps, the most important, as it is the main object of the game and it is the base over which most decisions are taken. Thus, its integration has to be as reliable as possible. To accomplish this, a Kalman filter implementation was created to filter the estimated ball position given by the visual information, and a linear regression was applied over filtered positions to estimate its velocity. 4.1 Ball position It is assumed that the ball velocity is constant between cycles. Although that is not true, due to the short time variations between cycles, around 40ms, and given the noisy environment and measurement errors, it is a quite acceptable model for the ball movement. Thus, no friction is considered to affect the ball, and the model does not include any kind of control over the ball. Therefore, given the Kalman filter formulation (described in [24]), the assumed state transition model is given by X k = 1 \u0394 T 0 1 X k - 1 where X k = Pos Vel is the state vector containing the position and velocity of the ball. Both are composed by the respective (x, y) coordinates. This velocity is only internally estimated by the filter, as the robot sensors can only take measurements on the ball position. After defining the state transition model based on the ball movement assumptions described above and the observation model, the description of the measurements and process noises are important issues to attend. The measurements noise can be statistically estimated by taking measurements of a static ball position at known distances. In practice, measurements of the static ball were taken while the robot was rotating around its vertical axis and this was done with the ball placed at several distances, measured with metric tape. Although real game conditions are probably more adverse, we lack the means to externally know the position of the elements on the field. For that reason, to know the real distance between the robot and the ball, we opted to use the described setup. Some of the results are illustrated in Fig. 5 . The standard deviation of those measurements can be used to calculate the variance and thus define the measurements noise parameter. A relation between the distance of the ball to the robot and the measurements standard deviation can be modeled by a 2nd degree polynomial best fitting the data set in a least-squares sense. Depending on the available data, a polynomial of another degree could be used, but we should always keep in mind the computational weight of increasing complexity. As for the process noise, this is not trivial to estimate, since there is no way to take independent measurements of the process to estimate its standard deviation. The process noise is represented by a matrix containing the covariances correspondent to the state variable vector. Based on the Kalman filter functioning, one can verify that forcing a near null process noise causes the filter to practically ignore the read measures, leading the filter to emphasize the model prediction. This makes it too smooth and therefore inappropriate. On the other hand, if it is too high, the read measures are taken too much into account and the filter returns the measures themselves. To face this situation, one has to find a compromise between stability and reaction. Since we assume an uniform movement for the ball, there are no frictions or other external forces considered. This means that accelerations are not considered in our model and thus, the position and velocity components are quite independent of each other. Since acceleration is the main element of relation between position and velocity, we considered that the errors associated to the process position and velocity estimations do not correlate. Because we assume an uniform movement model that we know is not the true nature of the system, we know that the speed calculation of the model is not very accurate. A process noise covariance matrix was empirically estimated, based on several tests, so that a good smoothness/reactivity relationship was kept. These empirically estimated values were made dependent on the measurement noise so that the Kalman filter predictions are also less accurate when the distance to the ball is too large. This was done so that the filter does not smooth the positions too much. In practice, this approach proved to improve the estimation of the ball position. Since we do not possess the means to externally know the positions of the elements on the field, a capture was made with the ball fixed at a known position on the field (0.0,2.0) (measured with metric tape). The robot was moving around the ball with a speed of 1.3\u00b10.5m/s and the ball position measured at each moment was recorded. The ball position measured by the robot was (\u22120.01,2.03)\u00b1(0.05,0.06)m. Fig. 6 illustrates the capture results. This experiment gives an idea of the noise associated with the ball position detection. Note that during the experiment the distance between the robot and the ball is around 2m. Comparing the ball position cloud with the one obtained at 2m in Fig. 5 one can verify that they are similar, which is consistent with the previous experiment setup to simulate robot movement by rotation on the spot. With the presented setup experiments, the existence of noise in ball measurements became clear. With that existent noise in mind, several tests were made to validate the use of the Kalman filter to reduce it. Fig. 7 represents a capture of one of those tests, a ball movement, where the black dots are the ball positions measured by the robot visual sensors and thus are unfiltered. Red stars 4 For interpretation of color in \u2018Figs. 1,2,4-7,9-11,13-15,17-20\u2019 the reader is referred to the web version of this article. 4 represent the position estimations after applying the Kalman filter. The robot position is represented by the black star in its center and its respective radius. The ball was thrown against the robot and deviated accordingly. It is easily perceptible that the unfiltered positions are affected by much noise and the path of the ball after the collision is composed of positions that do not make much physical sense. Although we lack the means to externally provide a ground truth for the ball position during its movements, the filtered positions seem to give a much better approximation to the real path taken by the ball, as they provide a path that physically makes more sense. After producing the a priori estimation of the ball position, this estimation is compared with the read measure to detect if the variation between them is too great. If the difference between them is consistently greater than a given threshold (estimated empirically), the filter can indicate that the ball suffered a hard deviation (Fig. 8 illustrates this concept). Although hard deviations are not a serious problem for the filter (as it quickly converges to the new positions), they are used for velocity convergence (as described in the next subsection). 4.2 Ball velocity The calculation of the ball velocity is a feature becoming more and more important over the time. It allows that better decisions can be implemented based on the ball speed value and direction. Assuming the same ball movement model described before, constant ball velocity between cycles and no friction considered, one could theoretically calculate the ball velocity by simple instantaneous velocity of the ball with the first order derivative of each component \u0394 D \u0394 T , being \u0394D the displacement on consecutive measures and \u0394T the time interval between consecutive measures. However, given the noisy environment, it is also predictable that this approach would be greatly affected by that noise and thus its results would not be satisfactory. Fig. 9 shows a ball movement capture where the ball was moving from left to right, as indicated by the arrow in the top of the figure, and was then deviated into a downward movement near the \u201c1st deviation\u201d tag. While moving downward, the ball was deviated again near the \u201c2nd deviation\u201d tag and started to move from right to left. Finally, in the end of the capture, a new deviation occurred near tag \u201c3rd deviation\u201d where the ball started to move upward. The estimated ball positions are represented by the blue dots. Red lines represent the velocity vectors estimated based on consecutive positions displacement. It is clear that the velocity estimates hardly give an acceptable insight of the ball movement. To keep a calculation of the object velocity consistent with its displacement, an implementation of a linear regression algorithm was chosen. This approach based on linear regression [25] is similar to the velocity estimation described in [18]. By keeping a buffer of the last m measures of the object position and sampling instant (in this case buffers of nine samples were used), one can calculate a regression line to fit the positions of the object. Since the object position is composed by two coordinates (x, y), we actually have two linear regression calculations, one for each dimension. This is made in a transparent way, so the description is presented generally, as if only one dimension was considered. When applied over the positions estimation, the linear regression velocity estimations are much more accurate than the instant velocities calculated by \u0394 D \u0394 T , and allow a better insight of the ball movement. The same ball movement capture described earlier is represented in Fig. 10 , this time with the velocity vectors estimated by the linear regression applied over the position estimations provided by the Kalman filter. In order to try to make the regression converge more quickly on deviations of the ball path, a reset feature was implemented. This allows deletion of the older values, keeping only the n most recent ones, and provides control of the buffer size. By keeping the most recent values after a hard deviation, we reduce outliers of the previous path, thus promoting faster convergence. This reset results from the interaction with the Kalman filter described earlier by querying it for the existence of a hard deviation on the ball path. The obtained values were tested to confirm if the linear regression of the ball positions was more precise and would converge faster than the internal velocity estimated by the Kalman filter. Tests showed that the velocity estimated by the Kalman filter has a slower response than the linear regression estimation when deviations occur. Given this, the linear regression was used to estimate the velocity because quickness of convergence was preferred over the slightly smoother approximation of the Kalman filter in the steady state. That is because in the game environment the ball is very dynamic, it constantly changes its direction and thus a convergence in less than half the cycles is much preferred. Fig. 11 shows the results for a theoretical velocity scenario where the ball was moving at a constant speed of 2m/s and suddenly dropped to a constant 1m/s speed. Both the speeds estimated by the Kalman filter and the ones estimated by the linear regression are presented. 4.3 Team ball position sharing Due to the highly important role that the ball has in a soccer game, when a robot cannot detect it by its own visual sensors (omni or frontal camera), it may still know the position of the ball, through sharing of that knowledge by the other team mates. The ball data structure includes a field with the number of cycles it was not visible by the robot, meaning that the ball position given by the vision sensors can be the \u201clast seen\u201d position. When the ball is not visible for more than a given number of cycles, the robot assumes that it cannot detect the ball on its own. When that is the case, it uses the information of the ball communicated by the other running team mates to know where the ball is. This can be done by getting the mean and standard deviation of the positions of the ball seen by team mates. Another approach is to simply use the ball position of the team mate that has more confidence in the detection. Independently of the chosen approach, the robot assumes that ball position as correct. When detecting the ball on its own, there is also the need to validate that information. Currently the seen ball is only considered if it is within a given margin inside the field of play as there would be no point in trying to play with a ball outside the field. For ball position sharing, an approach based on the highest confidence ball position is used. This is due to the fact that the shared positions are updated with 100ms periods, with the possibility of a few more milliseconds of unknown and unpredictable delay in packet transmission. Thus, the lifetime of the information of each team mate is different, and the use of the information of the team mate with higher confidence reduces the probability of the degradation of that information during the respective lifetime. Fig. 12 illustrates the general ball integration activity diagram. 5 Obstacle treatment While playing soccer, the robots have the need to navigate around the field effectively, which means they have to reposition themselves or dribble the ball avoiding the obstacles on the field, that can be either team or opponent robots, or eventually the referee. An increasing necessity felt by the team, to improve its performance, is a better obstacle detection and sharing of obstacle information among team mates. This is important to ensure a global idea of the field occupancy, since the team formation usually keeps the robots spread across the field. Pass lines and dribbling corridors can be estimated more easily with a good coverage of field obstacles, allowing improvements on team strategy and coordination. 5.1 Visual obstacle detection The CAMBADA robots gather their information about the surroundings by means of a robotic vision system. Currently, only the omni directional camera gathers information about obstacles, as no frontal camera is being used at this time. According to RoboCup rules, the robots are mainly black. Since during the game robots play autonomously, all obstacles in the field are the robots themselves (occasionally the referee, which is recommended to wear black/dark pants). The vision algorithm detects the obstacles by evaluating blobs of black color inside the field of play [26]. Through the mapping of image positions to real metric positions [27], obstacles are identified by their center (triangle on the processed image, Fig. 13 b) and left and right limits (squares on the processed image, Fig. 13b). This is done by searching black regions on the scanlines of the vision algorithm [23], already referred in Section 3. The detection of black color on the scanlines is analyzed both in angular intervals and length intervals, to define the limits of each black blob (considering their base points which are represented by the first black pixel in each scanline). Since the vision system is a non-SVP hyperbolic catadioptric system [27], the size of objects on the image varies with the distance to the robot. Due to an inverse distance map calculation, by exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface, the relation of distances in the image and the real world is known. Fig. 14 is an illustration of how the distance in pixels, from the center of the image, is mapped to the distance in meters, on the ground plane. Through the function represented in Fig. 14, it is possible to create a normalized relation of blobs width and length with the distance. Sometimes an obstacle is separated in several blobs, mainly due to the noise in the image and problems in color classification, which leads to failure in the detection of black regions in the scanlines. To avoid these situations, an offset is considered to decide when the angular space between blobs is considered enough to represent a real obstacle separation. The same principle is considered concerning the position of the black area in consecutive scanlines. The separation offsets of a blob close to the robot are bigger than the ones at a high distance, to maintain coherent precision. The angular separation offset is considered for situations where robots are side-by-side, at the same distance, but there is no visual contact between each blob; the length separation offset is checked for situations where, on consecutive scanlines, there are blobs with visual contact but the robots are actually at different distances. Both situations are depicted in Fig. 15 . For each detected blob, their number of pixels is calculated and an estimation of the obstacles left and right limits, as well as their centers, is made. This information is made available to the integration process for filtering and treatment. 5.2 Obstacle selection and identification With the objective of refining the information of the obstacles, and have more meaningful and human readable information, the obstacles are selected and a matching is attempted, in order to try to identify them as team mates or opponents. Due to the weak precision at long distances, a first selection of the obstacles is made by selecting only the obstacles closer than a given distance as available for identification (currently 5m). Also, obstacles that are smaller than 10cm wide or outside the field of play margin are ignored. This is done because the MSL robots are rather big, and in-game situations small obstacles are not present inside the field. Also, it would be pointless to pay attention to obstacles that are outside the field of play, since the surrounding environment is completely ignorable for the game development. To be able to distinguish obstacles, identifying which of them are team mates and which are opponent robots, a fusion between the own visual information of the obstacles and the shared team mates positions is made. By creating a circle around the team mate positions with the robot radius (considered 22cm), a matching of the estimated center of visible obstacle area is made (Fig. 16 ), and the obstacle is identified as the corresponding team mate in case of a positive matching (Figs. 17 c and 18c). This matching consists on the existence of interception points between the team mate circle and the obstacle circle or if the obstacle center is inside the team mate circle (the obstacle circle can be smaller, and thus no interception points would exist). Since the detected obstacles can be large blobs, the above described identification algorithm cannot be applied directly to the visually detected obstacles. If the detected obstacle fulfills the minimum size requisites already described, it is selected as candidate for being a robot obstacle. Its size is evaluated and classified as robot if it does not exceed the maximum size allowed for MSL robots [2] (Fig. 17a and b). If the obstacle exceeds the maximum size of an MSL robot, a division of the obstacle is made, by analyzing its total size which is used to estimate how many robots are in that obstacle. This may be a common situation, robots clashing together and thus creating a compact black blob, originating a big obstacle if they are sufficiently lined up (Fig. 18 a and b). Although the computations for obstacle identification were in use during RoboCup 2009, their results are yet to be considered in the team strategy. Currently, obstacles are always considered unfriendly and thus to be avoided. Due to this fact, there is currently no data of in-game results for this part of the work. Several captures of the obstacle identification algorithm described earlier were performed and analyzed, to further illustrate the effectiveness of the algorithm. The laboratory used for the tests receives natural light which can affect the vision processing algorithms. The presented results are not treated in any way to diminish the effects of natural light, as we are interested in understanding if the algorithms can cope with those conditions which can be found in real situations. In the first test situation, a robot was positioned on the field at (\u22120.05,1.88) while broadcasting its position. This robot will be referred to as pivot. Another robot was moving on a rectangular path around the pivot, and a capture of its data was done. This robot will be referred to as observer. This scenario is intended to give some insight about the performance of the identification when the team mates are static or nearly static (as is the case of set plays during the games. In these situations it is important to analyze passing lines). Fig. 19 is a graphic representation of the acquired data, with the pivot represented in black. The blue dots are the positions of the path taken by the observer, which covers the rectangular path for three times. In each cycle, the center of the obstacle perceived by the observer is represented by a red \u2018\u00d7\u2019. It is visible that, as expected, the obstacle position perceived by the observer is not exactly the pivot position. The capture in question is composed of 677 cycles. The identification of the obstacle as the correspondent team mate failed to succeed in only one cycle, which corresponds to a 99.85% success rate. Considering that the pivot has 22cm radius (although it is slightly bigger), the mean of the centers of the perceived obstacle is within the real area occupied by the pivot, at nearly 16cm with a standard deviation of 10cm (Table 1 ). Another test scenario was considered for evaluation of the algorithm performance for moving obstacles. Several captures were performed to evaluate the performance of the algorithm when identifying a moving team mate. This set of six captures consisted on a robot observing a team mate moving around and registering the data about the obstacles. The path taken by the moving team mate is represented in Fig. 20 . The number of failed identifications was greater when the moving robot was farther from the observer, as expected due to the noisy nature of the measurements. The captures were performed throughout the day, with different lighting conditions but with the same robot calibration. Table 2 summarizes this set of captures, which revealed a total mean identification ratio of approximately 71%. 5.3 Obstacle sharing With the purpose of improving the global perception of the team robots, the sharing of locally known information is an important feature. Obstacle sharing allows the team robots to have a more global perception of the field occupancy, allowing them to estimate, for instance, passing and dribbling corridors more effectively. However, one has to keep in mind that, mainly due to illumination conditions and eventual reflective materials, some of the detected obstacles may not be exactly robots, but dark shadowy areas. If that is the case, the simple sharing of obstacles would propagate an eventually false obstacle among the team. Thus the algorithm for sharing the obstacles makes a fusion of the several team mates information. The fusion of the information is done mate by mate. After building the worldstate by its own means, the agent checks all the available obstacle information provided by team mates, one by one. Their obstacles are matched with the own ones. If the agent does not know an obstacle shared by the team mate, it keeps it in a temporary list of unconfirmed obstacles. This is done to all the team mates obstacles. When another team mate shares a common obstacle, that same obstacle is confirmed and is transferred to the local list of obstacles. In the current cycle, the temporary obstacles that were not confirmed are not considered. A robot does not use negative information from other robots to remove obstacles it actually saw from its local world model. An outline of the algorithm is presented next. for c:=1 to total_number_of_team_mates for o:=1 to total_obstacles_of_team_mate for m:=1 to total_own_obstacles if m matches o I already know this obstacle, do nothing else if previously known by another team mate obstacle confirmed and added else obstacle considered temporarily waits for confirmation by another team mate endif endif endfor endfor endfor The matching of the team mate obstacles with the own obstacles is done in a way similar to the matching of the obstacle identification with the team mate position described earlier. The CAMBADA team mate position in Fig. 16 is replaced by the current team mate obstacle for the matching test. Fig. 21 shows a situation where robot 2, in the goal area was too far to see the obstacle on the middle of the field. Thus, it considered the obstacle in question, only because it is identified by both robots 5 and 6, as visible in the figure. 6 Conclusion and future work The techniques chosen for information and sensor fusion proved to be effective in accomplishing their objectives. The Kalman filter allows to filter the noise on the ball position and provides an important prediction feature which allows fast detection of deviations of the ball path. The linear regression used to estimate the velocity is also effective, and combined with the deviation detection based on the Kalman filter prediction error, provides a faster way to recalculate the velocity in the new trajectory. The improvement on obstacle treatment allows modifications on the overall team strategy, particularly regarding passing possibilities. It also allows the improvement of the robots movement, since team mate obstacles can have a different treatment than the opponents, because team mates have velocities and other information available. The CAMBADA team obtained the 1st place in the last years of the Portuguese robotics open (Rob\u00f3tica 2007, Rob\u00f3tica 2008, Rob\u00f3tica 2009 and Rob\u00f3tica 2010), and internationally achieved 5th place in RoboCup 2007, 1st place in RoboCup 2008, 3rd place in RoboCup 2009 and 2nd place in GermanOpen 2010. Although the described work proved to be effective and helped to achieve good results, improving is always the aim for this kind of project. Thus, improvements on the localization algorithm are desired, as well as a different way to disambiguate symmetric positions to eventually complement or replace the compass. Another path to follow would be the improving of team strategy based on obstacle identification, creating new forms of cooperation and set plays for in-game situations. Acknowledgment This work was partially supported by project ACORD Adaptive Coordination of Robotic Teams, FCT/PTDC/EIA/70695/2006. References [1] Kitano H, Asada M, Kuniyoshi Y, Noda I, Osawa E. RoboCup: the robot world cup initiative. In: Proceedings of the first international conference on autonomous agents. New York (NY, USA): ACM; 1997. p. 340\u20137. [2] MSL Technical Committee 1997\u20132009. Middle size robot league rules and regulations for 2009; 2008. [3] Elmenreich W. Sensor fusion in time-triggered systems. Ph.D. thesis. Vienna (Austria): Technische Universitat Wien, Institut fur Technische Informatik; 2002. [4] N. Metropolis S. Ulam The Monte Carlo method J Am Stat Assoc 44 247 1949 335 341 [5] R. Kalman A new approach to linear filtering and prediction problems J Basic Eng 82 1 1960 35 45 [6] Wan E, Merwe RVD. The unscented Kalman filter for nonlinear estimation. In: IEEE adaptive systems for signal processing, communications, and control symposium; 2000. p. 153\u20138. [7] R. Luo C. Yih K. Su Multisensor fusion integration: approaches, applications, and future research directions IEEE Sens J 2 2 2002 107 119 [8] J. Leonard H. Durrant-Whyte Mobile robot localization by tracking geometric beacons IEEE Trans Robotics Autom 7 3 1991 376 382 [9] Dellaert F, Fox D, Burgard W, Thrun S. Monte Carlo localization for mobile robots. In: IEEE international conference on robotics and automation; 1999. p. 1322\u20138. [10] D. Fox W. Burgard S. Thrun Markov localization for mobile robots in dynamic environments J Artif Intell Res 11 1999 391 427 [11] A. Mourikis S. Roumeliotis Performance analysis of multirobot cooperative localization IEEE Trans Robotics 22 4 2006 666 681 [12] H. Durrant-Whyte T. Henderson Multisensor data fusion B. Siciliano O. Khatib Springer handbook of robotics 2008 Springer [13] A. Bejczy J. Dias Editorial: integration of visual and inertial sensors J Robotic Syst 21 2 2004 41 42 [14] G. Aleny\u00e1 E. Mart\u00ednez C. Torras Fusing visual and inertial sensing to recover robot ego-motion J Robotic Syst 21 1 2004 23 32 [15] S. Chroust M. Vincze Fusion of vision and inertial data for motion and structure estimation J Robotic Syst 21 2 2004 73 83 [16] W.B.S. Thrun D. Fox Probabilistic robotics 2005 The MIT Press [17] B. Siciliano O. Khatib Springer handbook of robotics 2008 Springer [18] M. Lauer S. Lange M. Riedmiller Modeling moving objects in a dynamically changing robot application U. Furbach KI 2005: advances in artificial intelligence Lecture notes in computer science vol. 3698 2005 Springer 291 303 [19] Xu Y, Jiang C, Tan Y. SEU-3D 2006 soccer simulation team description. In: CD proc of RoboCup symposium 2006, Bremen, Germany; 2006. [20] Marcelino P, Nunes P, Lima P, Ribeiro MI. Improving object localization through sensor fusion applied to soccer robots. In: Proc scientific meeting of the portuguese robotics open \u2013 Rob\u00f3tica 2003, Lisbon, Portugal; 2003. [21] A. Ferrein L. Hermanns G. Lakemeyer Comparing sensor fusion techniques for ball position estimation A. Bredenfeld A. Jacoff I. Noda Y. Takahashi RoboCup 2005: robot soccer world cup IX Lecture notes in computer science vol. 4020 2006 Springer 154 165 [22] M. Lauer S. Lange M. Riedmiller Calculating the perfect match: an efficient and accurate approach for robot self-localization A. Bredenfeld A. Jacoff I. Noda Y. Takahashi RoboCup 2005: robot soccer world cup IX Lecture notes in computer science vol. 4020 2006 Springer 142 153 [23] Neves A, Martins D, Pinho A. A hybrid vision system for soccer robots using radial search lines. In: Lopes LS, Silva F, Santos V, editors. Proc of the 8th conference on autonomous robot systems and competitions, Portuguese robotics open \u2013 Rob\u00f3tica 2008, Aveiro, Portugal; 2008. p. 51\u20135. [24] Bishop G, Welch G. An introduction to the Kalman filter. In: Proc of SIGGRAPH, Course 8, No. NC 27599-3175. NC (USA): Chapel Hill; 2001. [25] Motulsky H, Christopoulos A. Fitting models to biological data using linear and nonlinear regression. GraphPad Software Inc.; 2003. [26] A. Neves G. Corrente A. Pinho An omnidirectional vision system for soccer robots J. Neves M.F. Santos J.M. Machado Progress in artificial intelligence Lecture notes in artificial intelligence vol. 4874 2007 Springer 499 507 [27] B. Cunha J. Azevedo N. Lau L. Almeida Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system U. Visser F. Ribeiro T. Ohashi F. Dellaert RoboCup 2007: robot soccer world cup XI Lecture notes in artificial intelligence vol. 5001 2008 Springer 417 424", "scopus-id": "79952617090", "coredata": {"eid": "1-s2.0-S0957415810001029", "dc:description": "When a team of robots is built with the objective of playing soccer, the coordination and control algorithms must reason, decide and actuate based on the current conditions of the robot and its surroundings. This is where sensor and information fusion techniques appear, providing the means to build an accurate model of the world around the robot, based on its own limited sensor information and the also limited information obtained through communication with the team mates. One of the most important elements of the world model is the robot self-localization, as to be able to decide what to do in an effective way, it must know its position in the field of play. In this paper, the team localization algorithm is presented focusing on the integration of visual and compass information. An important element in a soccer game, perhaps the most important, is the ball. To improve the estimations of the ball position and velocity, two different techniques have been developed. A study of the visual sensor noise is presented and, according to this analysis, the resulting noise variation is used to define the parameters of a Kalman filter for ball position estimation. Moreover, linear regression is used for velocity estimation purposes, both for the ball and the robot. This implementation of linear regression has an adaptive buffer size so that, on hard deviations from the path (detected using the Kalman filter), the regression converges faster. A team cooperation method based on sharing the ball position is presented. Other important data during the soccer game is obstacle data. This is an important challenge for cooperation purposes, allowing the improvement of team strategy with ball covering, dribble corridor estimation, pass lines, among other strategic possibilities. Thus, detecting the obstacles is ceasing to be enough and identifying which obstacles are team mates and opponents is becoming a need. An approach for this identification is presented, considering the visual information, the known characteristics of the team robots and shared localization among team members. The described work was implemented on the CAMBADA team and allowed it to achieve particularly good performances in the last two years, with a 1st and a 3rd place in the world championship RoboCup 2008 and RoboCup 2009 editions, respectively, as well as distinctively achieve 1st place in 2008 and 2009 editions of the Portuguese Robotics Open.", "openArchiveArticle": "false", "prism:coverDate": "2011-03-31", "openaccessUserLicense": null, "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0957415810001029", "dc:creator": [{"@_fa": "true", "$": "Silva, Jo\u00e3o"}, {"@_fa": "true", "$": "Lau, Nuno"}, {"@_fa": "true", "$": "Neves, Ant\u00f3nio J.R."}, {"@_fa": "true", "$": "Rodrigues, Jo\u00e3o"}, {"@_fa": "true", "$": "Azevedo, Jos\u00e9 Lu\u00eds"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0957415810001029"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0957415810001029"}], "dc:format": "application/json", "openaccessType": null, "pii": "S0957-4158(10)00102-9", "prism:volume": "21", "prism:publisher": "Elsevier Ltd.", "dc:title": "World modeling on an MSL robotic soccer team", "prism:copyright": "Copyright \u00a9 2010 Elsevier Ltd. All rights reserved.", "prism:issueName": "Special Issue on Advances in intelligent robot design for the Robocup Middle Size League", "openaccess": "0", "prism:issn": "09574158", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Sensor fusion"}, {"@_fa": "true", "$": "World model"}, {"@_fa": "true", "$": "Kalman filter"}, {"@_fa": "true", "$": "Linear regression"}, {"@_fa": "true", "$": "Obstacle detection"}, {"@_fa": "true", "$": "Visual matching"}], "openaccessArticle": "false", "prism:publicationName": "Mechatronics", "prism:number": "2", "openaccessSponsorType": null, "prism:pageRange": "411-422", "prism:endingPage": "422", "pubType": "fla", "prism:coverDisplayDate": "March 2011", "prism:doi": "10.1016/j.mechatronics.2010.05.011", "prism:startingPage": "411", "dc:identifier": "doi:10.1016/j.mechatronics.2010.05.011", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "43", "@width": "146", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "820", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "19", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "279", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "20", "@width": "19", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "279", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "40", "@width": "84", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "672", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "301", "@width": "366", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr10.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "40478", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "200", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr10.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6601", "@ref": "gr10", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "374", "@width": "635", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr11.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "38816", "@ref": "gr11", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "129", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr11.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3816", "@ref": "gr11", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "461", "@width": "779", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr12.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "45526", "@ref": "gr12", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "130", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr12.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2642", "@ref": "gr12", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "160", "@width": "330", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr13.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "25831", "@ref": "gr13", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "106", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr13.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9938", "@ref": "gr13", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "374", "@width": "628", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr14.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "26031", "@ref": "gr14", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "130", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr14.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2725", "@ref": "gr14", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "161", "@width": "322", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr15.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "25951", "@ref": "gr15", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "109", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr15.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "10234", "@ref": "gr15", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "220", "@width": "243", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr16.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "11424", "@ref": "gr16", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "181", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr16.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3895", "@ref": "gr16", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "332", "@width": "324", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr17.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "34129", "@ref": "gr17", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "160", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr17.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9007", "@ref": "gr17", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "336", "@width": "320", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr18.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "28869", "@ref": "gr18", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "156", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr18.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8228", "@ref": "gr18", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "292", "@width": "350", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr19.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "31951", "@ref": "gr19", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "196", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr19.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4895", "@ref": "gr19", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "181", "@width": "349", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "24610", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "114", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9025", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "300", "@width": "345", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr20.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "32518", "@ref": "gr20", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "163", "@width": "188", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr20.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5136", "@ref": "gr20", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "222", "@width": "356", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr21.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "11053", "@ref": "gr21", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "137", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr21.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3093", "@ref": "gr21", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "252", "@width": "223", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "14423", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "145", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4214", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "420", "@width": "715", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "90061", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "129", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6962", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "170", "@width": "578", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "31815", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "64", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3190", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "295", "@width": "372", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "34213", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "207", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5554", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "304", "@width": "505", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "32017", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "132", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3661", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "117", "@width": "266", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "11017", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "96", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2052", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "297", "@width": "357", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr9.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "53806", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "197", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9002", "@ref": "gr9", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "173", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "35942", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "77", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810001029-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "9273", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/79952617090"}}