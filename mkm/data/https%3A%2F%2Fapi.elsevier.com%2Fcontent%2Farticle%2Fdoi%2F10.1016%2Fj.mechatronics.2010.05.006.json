{"scopus-eid": "2-s2.0-79952628276", "originalText": "serial JL 271456 291210 291718 291787 291883 31 Mechatronics MECHATRONICS 2010-06-19 2010-06-19 2011-03-08T22:21:19 1-s2.0-S0957415810000863 S0957-4158(10)00086-3 S0957415810000863 10.1016/j.mechatronics.2010.05.006 S300 S300.1 FULL-TEXT 1-s2.0-S0957415811X0003X 2015-05-15T03:43:39.596909-04:00 0 0 20110301 20110331 2011 2010-06-19T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue webpdf webpdfpagecount figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0957-4158 09574158 21 21 2 2 Volume 21, Issue 2 6 399 410 399 410 201103 March 2011 2011-03-01 2011-03-31 2011 Special Issue on Advances in intelligent robot design for the Robocup Middle Size League M.J.G. van de Molengraft O. Zweigle Special Issue on Advances in intelligent robot design for the Robocup Middle Size League article fla Copyright \u00a9 2010 Elsevier Ltd. All rights reserved. EFFICIENTOMNIDIRECTIONALVISIONSYSTEMFORSOCCERROBOTSCALIBRATIONOBJECTDETECTION NEVES A 1 Introduction 2 Architecture of the vision system 3 Calibration of the vision system 3.1 Self-calibration of the digital camera parameters 3.1.1 Proposed algorithm 3.1.2 Experimental results 3.2 Distance map calibration 4 Color-based object detection 4.1 Color extraction 4.2 Object detection 4.3 Experimental results 5 Arbitrary ball detection 5.1 Related work 5.2 Proposed approach 5.3 Experimental results 6 Conclusions Acknowledgment References LIMA 2001 87 102 P ASTROM 1995 K PIDCONTROLLERSTHEORYDESIGNTUNING BAKER 1999 175 196 S BRESENHAM 1965 25 30 J TREPTOW 2004 41 48 A GRIMSON 1990 1255 1274 W NEVESX2011X399 NEVESX2011X399X410 NEVESX2011X399XA NEVESX2011X399X410XA item S0957-4158(10)00086-3 S0957415810000863 1-s2.0-S0957415810000863 10.1016/j.mechatronics.2010.05.006 271456 2011-03-10T12:04:28.24434-05:00 2011-03-01 2011-03-31 1-s2.0-S0957415810000863-main.pdf https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/MAIN/application/pdf/a746331db7f6c709bb4a90d16c702fd2/main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/MAIN/application/pdf/a746331db7f6c709bb4a90d16c702fd2/main.pdf main.pdf pdf true 1828663 MAIN 12 1-s2.0-S0957415810000863-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/PREVIEW/image/png/1f8746a255aeff500d62c010e615eee7/main_1.png https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/PREVIEW/image/png/1f8746a255aeff500d62c010e615eee7/main_1.png main_1.png png 84396 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0957415810000863-si4.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/STRIPIN/image/gif/42f865e34b3566833ce4e80a9f80463f/si4.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/STRIPIN/image/gif/42f865e34b3566833ce4e80a9f80463f/si4.gif si4 si4.gif gif 1540 53 289 ALTIMG 1-s2.0-S0957415810000863-si3.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/STRIPIN/image/gif/40697d8d0f85e64eab38d6cb26dfaa56/si3.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/STRIPIN/image/gif/40697d8d0f85e64eab38d6cb26dfaa56/si3.gif si3 si3.gif gif 1436 46 293 ALTIMG 1-s2.0-S0957415810000863-si2.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/STRIPIN/image/gif/0caadcfb18c0b6f03a011af33a124557/si2.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/STRIPIN/image/gif/0caadcfb18c0b6f03a011af33a124557/si2.gif si2 si2.gif gif 1250 46 238 ALTIMG 1-s2.0-S0957415810000863-si1.gif https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/STRIPIN/image/gif/cb8719e2fc5eb69b35edfed4bbcc0fdb/si1.gif https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/STRIPIN/image/gif/cb8719e2fc5eb69b35edfed4bbcc0fdb/si1.gif si1 si1.gif gif 1092 46 192 ALTIMG 1-s2.0-S0957415810000863-gr10.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr10/DOWNSAMPLED/image/jpeg/5b3fc002ec157bf331588ad2328649e7/gr10.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr10/DOWNSAMPLED/image/jpeg/5b3fc002ec157bf331588ad2328649e7/gr10.jpg gr10 gr10.jpg jpg 45233 182 377 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr10.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr10/THUMBNAIL/image/gif/fee130e2389814e75a9b63e9c9660f28/gr10.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr10/THUMBNAIL/image/gif/fee130e2389814e75a9b63e9c9660f28/gr10.sml gr10 gr10.sml sml 14653 106 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr11.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr11/DOWNSAMPLED/image/jpeg/b1efa2f70cdf271130d82da81718b13a/gr11.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr11/DOWNSAMPLED/image/jpeg/b1efa2f70cdf271130d82da81718b13a/gr11.jpg gr11 gr11.jpg jpg 18028 138 376 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr11.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr11/THUMBNAIL/image/gif/c0e4b808caae1290064d9e70b0b63b66/gr11.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr11/THUMBNAIL/image/gif/c0e4b808caae1290064d9e70b0b63b66/gr11.sml gr11 gr11.sml sml 2966 81 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr12.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr12/DOWNSAMPLED/image/jpeg/c8da661599f3b3ed3964b454c432f6b8/gr12.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr12/DOWNSAMPLED/image/jpeg/c8da661599f3b3ed3964b454c432f6b8/gr12.jpg gr12 gr12.jpg jpg 43502 281 757 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr12.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr12/THUMBNAIL/image/gif/9131dddccbc1199df10df0dc011f71b5/gr12.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr12/THUMBNAIL/image/gif/9131dddccbc1199df10df0dc011f71b5/gr12.sml gr12 gr12.sml sml 6024 81 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr13.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr13/DOWNSAMPLED/image/jpeg/43b121428f5d600499718a6354107741/gr13.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr13/DOWNSAMPLED/image/jpeg/43b121428f5d600499718a6354107741/gr13.jpg gr13 gr13.jpg jpg 31126 208 377 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr13.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr13/THUMBNAIL/image/gif/b66f328aa97c725c2b56d3916b7e54cc/gr13.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr13/THUMBNAIL/image/gif/b66f328aa97c725c2b56d3916b7e54cc/gr13.sml gr13 gr13.sml sml 11667 121 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr14.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr14/DOWNSAMPLED/image/jpeg/5d45d1a6939c7f8da4b992328fb3e06d/gr14.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr14/DOWNSAMPLED/image/jpeg/5d45d1a6939c7f8da4b992328fb3e06d/gr14.jpg gr14 gr14.jpg jpg 7740 60 489 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr14.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr14/THUMBNAIL/image/gif/1942fb0ea1b6497a5b2c6548b8db8019/gr14.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr14/THUMBNAIL/image/gif/1942fb0ea1b6497a5b2c6548b8db8019/gr14.sml gr14 gr14.sml sml 1994 27 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr15.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr15/DOWNSAMPLED/image/jpeg/e91e073cf248a8eb39d37ea038da85a4/gr15.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr15/DOWNSAMPLED/image/jpeg/e91e073cf248a8eb39d37ea038da85a4/gr15.jpg gr15 gr15.jpg jpg 19330 223 373 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr15.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr15/THUMBNAIL/image/gif/5c8d1c3482e1ffc5adae7f58c31648c5/gr15.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr15/THUMBNAIL/image/gif/5c8d1c3482e1ffc5adae7f58c31648c5/gr15.sml gr15 gr15.sml sml 3452 131 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr16.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr16/DOWNSAMPLED/image/jpeg/c05b5add1d5ab0d5439881ffa51bf474/gr16.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr16/DOWNSAMPLED/image/jpeg/c05b5add1d5ab0d5439881ffa51bf474/gr16.jpg gr16 gr16.jpg jpg 43365 197 765 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr16.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr16/THUMBNAIL/image/gif/8efd2679ebf2d137eda714b9cbb633f9/gr16.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr16/THUMBNAIL/image/gif/8efd2679ebf2d137eda714b9cbb633f9/gr16.sml gr16 gr16.sml sml 5658 56 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr17.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr17/DOWNSAMPLED/image/jpeg/3f5775570c8a5fc48a8b36a79b860b7a/gr17.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr17/DOWNSAMPLED/image/jpeg/3f5775570c8a5fc48a8b36a79b860b7a/gr17.jpg gr17 gr17.jpg jpg 11813 182 264 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr17.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr17/THUMBNAIL/image/gif/a2ce4e2a5450824aed79d0f8a6c44578/gr17.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr17/THUMBNAIL/image/gif/a2ce4e2a5450824aed79d0f8a6c44578/gr17.sml gr17 gr17.sml sml 4751 151 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr18.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr18/DOWNSAMPLED/image/jpeg/eec9193b12f546c26ef8a0aba3ec704e/gr18.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr18/DOWNSAMPLED/image/jpeg/eec9193b12f546c26ef8a0aba3ec704e/gr18.jpg gr18 gr18.jpg jpg 9049 172 358 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr18.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr18/THUMBNAIL/image/gif/82b402f7cd955e887d24ccfe3b3c0120/gr18.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr18/THUMBNAIL/image/gif/82b402f7cd955e887d24ccfe3b3c0120/gr18.sml gr18 gr18.sml sml 2922 105 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr19.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr19/DOWNSAMPLED/image/jpeg/4fbb522e7e44352e4275b8e002fd5f33/gr19.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr19/DOWNSAMPLED/image/jpeg/4fbb522e7e44352e4275b8e002fd5f33/gr19.jpg gr19 gr19.jpg jpg 8089 119 217 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr19.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr19/THUMBNAIL/image/gif/09017abe7196bd5383db0bd207f15635/gr19.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr19/THUMBNAIL/image/gif/09017abe7196bd5383db0bd207f15635/gr19.sml gr19 gr19.sml sml 4702 121 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr2.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr2/DOWNSAMPLED/image/jpeg/08385a40e50d5d583f7ee7ed700f2af5/gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr2/DOWNSAMPLED/image/jpeg/08385a40e50d5d583f7ee7ed700f2af5/gr2.jpg gr2 gr2.jpg jpg 27937 199 271 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr2.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr2/THUMBNAIL/image/gif/8db9242232c9e42b61cc80d32b699dbf/gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr2/THUMBNAIL/image/gif/8db9242232c9e42b61cc80d32b699dbf/gr2.sml gr2 gr2.sml sml 19723 161 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr20.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr20/DOWNSAMPLED/image/jpeg/04d8f1bf3c8cf7130ac495e4ff26fd8a/gr20.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr20/DOWNSAMPLED/image/jpeg/04d8f1bf3c8cf7130ac495e4ff26fd8a/gr20.jpg gr20 gr20.jpg jpg 23395 140 484 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr20.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr20/THUMBNAIL/image/gif/c28f0808997e475913a73e1391341cea/gr20.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr20/THUMBNAIL/image/gif/c28f0808997e475913a73e1391341cea/gr20.sml gr20 gr20.sml sml 5238 63 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr21.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr21/DOWNSAMPLED/image/jpeg/eee154ee56f74a20097d3850e09b10ec/gr21.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr21/DOWNSAMPLED/image/jpeg/eee154ee56f74a20097d3850e09b10ec/gr21.jpg gr21 gr21.jpg jpg 24408 220 361 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr21.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr21/THUMBNAIL/image/gif/8fd2cf7492ca74c863073f668108931b/gr21.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr21/THUMBNAIL/image/gif/8fd2cf7492ca74c863073f668108931b/gr21.sml gr21 gr21.sml sml 4625 133 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr3.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr3/DOWNSAMPLED/image/jpeg/c127d4f60306ff2ed3f65c730e4aa16a/gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr3/DOWNSAMPLED/image/jpeg/c127d4f60306ff2ed3f65c730e4aa16a/gr3.jpg gr3 gr3.jpg jpg 57308 455 703 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr3.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr3/THUMBNAIL/image/gif/287beeb48e4a5959ae023f9cc406d1ab/gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr3/THUMBNAIL/image/gif/287beeb48e4a5959ae023f9cc406d1ab/gr3.sml gr3 gr3.sml sml 8598 142 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr4.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr4/DOWNSAMPLED/image/jpeg/4951d591e40259572912af9f0f7a233e/gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr4/DOWNSAMPLED/image/jpeg/4951d591e40259572912af9f0f7a233e/gr4.jpg gr4 gr4.jpg jpg 25516 139 381 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr4.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr4/THUMBNAIL/image/gif/dd481bcec2d9f2df4cbad8f3d3ff6fc4/gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr4/THUMBNAIL/image/gif/dd481bcec2d9f2df4cbad8f3d3ff6fc4/gr4.sml gr4 gr4.sml sml 10182 80 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr5.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr5/DOWNSAMPLED/image/jpeg/bb2846ae8f71f9ea3176fa1e5a1a6921/gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr5/DOWNSAMPLED/image/jpeg/bb2846ae8f71f9ea3176fa1e5a1a6921/gr5.jpg gr5 gr5.jpg jpg 22239 199 575 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr5.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr5/THUMBNAIL/image/gif/b59d2bc9fda0c9955af35e1cd04cc110/gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr5/THUMBNAIL/image/gif/b59d2bc9fda0c9955af35e1cd04cc110/gr5.sml gr5 gr5.sml sml 3068 76 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr6.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr6/DOWNSAMPLED/image/jpeg/0b9e1dccca431c3a88b39da143dc26cc/gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr6/DOWNSAMPLED/image/jpeg/0b9e1dccca431c3a88b39da143dc26cc/gr6.jpg gr6 gr6.jpg jpg 26663 137 374 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr6.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr6/THUMBNAIL/image/gif/a82e407b826344dfb5d054aabdd8d778/gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr6/THUMBNAIL/image/gif/a82e407b826344dfb5d054aabdd8d778/gr6.sml gr6 gr6.sml sml 10562 80 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr7.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr7/DOWNSAMPLED/image/jpeg/37fdca03fcb302b60770b7139b85df19/gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr7/DOWNSAMPLED/image/jpeg/37fdca03fcb302b60770b7139b85df19/gr7.jpg gr7 gr7.jpg jpg 61745 405 533 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr7.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr7/THUMBNAIL/image/gif/8fe7eac0dc49c209b61b27504a812c56/gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr7/THUMBNAIL/image/gif/8fe7eac0dc49c209b61b27504a812c56/gr7.sml gr7 gr7.sml sml 15882 164 215 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr8.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr8/DOWNSAMPLED/image/jpeg/8c062458a58e584749a821850a14271a/gr8.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr8/DOWNSAMPLED/image/jpeg/8c062458a58e584749a821850a14271a/gr8.jpg gr8 gr8.jpg jpg 53286 374 533 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr8.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr8/THUMBNAIL/image/gif/e95c437863ab7a15d0dd93c998047d05/gr8.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr8/THUMBNAIL/image/gif/e95c437863ab7a15d0dd93c998047d05/gr8.sml gr8 gr8.sml sml 11776 154 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr9.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr9/DOWNSAMPLED/image/jpeg/6a7b4aee99531c4fe0ba8a6fb3a6efaa/gr9.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr9/DOWNSAMPLED/image/jpeg/6a7b4aee99531c4fe0ba8a6fb3a6efaa/gr9.jpg gr9 gr9.jpg jpg 33410 179 378 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr9.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr9/THUMBNAIL/image/gif/3b4eb10e7ee0c0fef875063cb541cbf0/gr9.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr9/THUMBNAIL/image/gif/3b4eb10e7ee0c0fef875063cb541cbf0/gr9.sml gr9 gr9.sml sml 13228 104 219 IMAGE-THUMBNAIL 1-s2.0-S0957415810000863-gr1.jpg https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr1/DOWNSAMPLED/image/jpeg/e7bed626ecc3e3418ee91fff4d2162d5/gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr1/DOWNSAMPLED/image/jpeg/e7bed626ecc3e3418ee91fff4d2162d5/gr1.jpg gr1 gr1.jpg jpg 39196 225 378 IMAGE-DOWNSAMPLED 1-s2.0-S0957415810000863-gr1.sml https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0957415810000863/gr1/THUMBNAIL/image/gif/9a5781b18eb800b10b810a544b7c4b37/gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957415810000863/gr1/THUMBNAIL/image/gif/9a5781b18eb800b10b810a544b7c4b37/gr1.sml gr1 gr1.sml sml 14502 130 219 IMAGE-THUMBNAIL MECH 1160 S0957-4158(10)00086-3 10.1016/j.mechatronics.2010.05.006 Elsevier Ltd Fig. 1 The CAMBADA team playing at RoboCup 2009, Graz, Austria. Fig. 2 On the left, a detailed view of the CAMBADA vision system. On the right, one of the robots. Fig. 3 Some experiments using the automated calibration procedure. At the top, results obtained starting with all the parameters of the camera set to zero. At the bottom, results obtained with all the parameters set to the maximum value. On the left, the initial image acquired. In the middle, the image obtained after applying the automated calibration procedure. On the right, the graphics showing the evolution of the parameters along the time. Fig. 4 On the left, an example of an image acquired with the camera parameters in auto-mode. On the right, an image acquired after applying the automated calibration algorithm. Fig. 5 The histogram of the intensities of the two images presented in Fig. 4. In (a) it is shown the histogram of the image obtained with the camera parameters set to the maximum value. (b) Shows the histogram of the image obtained after applying the automated calibration procedure. Fig. 6 On the left, an image acquired outdoors using the camera in auto-mode. As it is possible to observe, the colors are washed out. This happens because the camera\u2019s auto-exposure algorithm tries to compensate the black region around the mirror. On the right, the same image with the camera calibrated using our algorithm. As can be seen, the colors and the contours of the objects are much more defined. Fig. 7 A screenshot of the tool developed to calibrate some important parameters of the vision system, namely the inverse distance map, the mirror and robot center and the regions of the image to be processed. Fig. 8 A screenshot of the interface to calibrate some important parameters need to obtain the inverse distance map (these parameters are described in [17]). Fig. 9 Acquired image after reverse-mapping into the distance map. On the left, the map was obtained with all misalignment parameters set to zero. On the right, after automatic correction. Fig. 10 A 0.5m grid, superimposed on the original image. On the left, with all correction parameters set to zero. On the right, the same grid after geometrical parameter extraction. Fig. 11 On the left, the position of the radial search lines used in the omnidirectional vision system, after detecting the center of the robot in the image using the tool described in this section. On the right, an example of a robot mask used to select the pixels to be processed, obtained with the same tool. White points represent the area that will be processed. Fig. 12 The software architecture of the omnidirectional vision system. Fig. 13 A screenshot of the application used to calibrate the color ranges for each color class using the HSV color space. Fig. 14 An example of a transition. \u201cG\u201d means green pixel, \u201cW\u201d means white pixel and \u201cX\u201d means pixel with a color different from green or white, for example resulting due to some noise or a not perfect color calibration. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 15 Relation between pixels and metric distances. The center of the robot is considered the origin and the metric distances are considered on the ground plane. Fig. 16 On the left, an example of an original image acquired by the omnidirectional vision system. In the center, the corresponding image of labels. On the right, the color blobs detected in the images. Marks over the ball point to the mass center. The several marks near the white lines (magenta) are the position of the white lines. The cyan marks are the position of the obstacles. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 17 Experimental results obtained by the omnidirectional system using the color ball detection. In this experiment, the ball was positioned in the center of the field, position (0,0). The robot performed a predefined trajectory while the position of the ball and the robot was recorded. Both axes in the graphics are in meters. Fig. 18 The circular Hough transform. a and b represent the parameter space that in this application are the radius of the ball and the distance to robot, respectively. Fig. 19 Example of a circle detection through the use of the circular Hough transform. Fig. 20 Example of a captured image using the proposed approach. The cross over the ball points out the detected position. In (b) the image (a), with the Canny edge detector applied. In (c), the image (b) after applying the circular Hough transform. Fig. 21 Experimental results obtained by the omnidirectional system using the morphological ball detection. In this experience, the ball was positioned in the penalty mark of the field. The robot performed a predefined trajectory while the position of the ball was recorded. Both axes in the graphics are in meters. Table 1 Statistical measures obtained for the images presented in Figs. 3 and 4. The initial values refer to the images obtained with the camera before applying the proposed automated calibration procedure. The final values refer to the images acquired with the cameras configured with the proposed algorithm. Experiment \u2013 ACM \u03bc E MSV Parameters set to zero Initial 111.00 16.00 0.00 1.00 Final 39.18 101.95 6.88 2.56 Parameters set to maximum Initial 92.29 219.03 2.35 4.74 Final 42.19 98.59 6.85 2.47 Camera in auto-mode Initial 68.22 173.73 6.87 3.88 Final 40.00 101.14 6.85 2.54 An efficient omnidirectional vision system for soccer robots: From calibration to object detection Ant\u00f3nio J.R. Neves \u204e Armando J. Pinho Daniel A. Martins Bernardo Cunha ATRI, IEETA/DETI, University of Aveiro, 3810-193 Aveiro, Portugal \u204e Corresponding author. Robotic soccer is nowadays a popular research domain in the area of multi-robot systems. In the context of RoboCup, the Middle Size League is one of the most challenging. This paper presents an efficient omnidirectional vision system for real-time object detection, developed for the robotic soccer team of the University of Aveiro, CAMBADA. The vision system is used to find the ball and white lines, which are used for self-localization, as well as to find the presence of obstacles. Algorithms for detecting these objects and also for calibrating most of the parameters of the vision system are presented in this paper. We also propose an efficient approach for detecting arbitrary FIFA balls, which is an important topic of research in the Middle Size League. The experimental results that we present show the effectiveness of our algorithms, both in terms of accuracy and processing time, as well as the results that the team has been achieving: 1st place in RoboCup 2008, 3rd place in 2009 and 1st place in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. Keywords Robotic vision Omnidirectional vision systems Color-based object detection Shape-based object detection Vision system calibration 1 Introduction The Middle Size League (MSL) of RoboCup is a forum where several research areas have been challenged for proposing solutions to well-defined practical problems. The robotic vision is one of those areas and, for most of the MSL teams, it has become the only way of sensing the surrounding world. From the point of view of a robot, the playing field during a game provides a fast-changing scenery, where the teammates, the opponents and the ball move quickly and often in an unpredictable way. The robots have to capture these scenes through their cameras and have to discover where the objects of interest are located. There is no time for running complex algorithms. Everything has to be computed and decided in a small fraction of a second, for allowing real-time operation; otherwise, it becomes useless. Real-time is not the only challenge that needs to be addressed. Year after year, the initially well controlled and robot friendly environment where the competition takes place has become increasingly more hostile. Conditions that previously have been taken for granted, such as controlled lighting or easy to recognize color coded objects, have been relaxed or even completely suppressed. Therefore, the vision system of the robots needs to be prepared for adapting to strong lighting changes during a game, as well as, for example, for ball-type changes across games. In this paper, we provide a comprehensive description of the vision system of the MSL CAMBADA team (Fig. 1 ). Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture (CAMBADA) is the RoboCup MSL soccer team of the Institute of Electronics and Telematics Engineering of Aveiro (IEETA) research institute, University of Aveiro, Portugal. The team, which started officially in October 2003, won the 2008 MSL RoboCup World Championship and ranked 3rd in the 2009 edition. We start by presenting and explaining the hardware architecture of the vision system used by the robots of the CAMBADA team, which relies on an omnidirectional vision system (Section 2). Then, we proceed with the description of the approach that we have adopted regarding the calibration of a number of crucial parameters and in the construction of auxiliary data structures (Section 3). Concerning the calibration of the intrinsic parameters of the digital camera, we propose an automated calibration algorithm that is used to configure the most important features of the camera, namely, the saturation, exposure, white-balance, gain and brightness. The proposed algorithm uses the histogram of intensities of the acquired images and a black and a white area, known in advance, to estimate the referred parameters. We also describe a general solution to calculate the robot centered distances map, exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface. The soccer robots need to locate several objects of interest, such as the ball, the opponent robots and the teammates. Moreover, they also need to collect information for self-localization, namely, the position of the field white lines. For these tasks, we have developed fast and efficient algorithms that rely on color information. The color extraction algorithms are based on lookup tables and use a radial model for color object detection. Due to the severe restrictions imposed by the real-time constraint, some of the image processing tasks are implemented using a multi-threading approach and use special data structures to reduce the processing time. Section 4 provides a detailed description of these algorithms. As previously mentioned, the color codes assigned to the objects of interest tend to disappear as the competition evolves. For example, the usual orange ball used in the MSL will soon be replaced by an arbitrary FIFA ball, increasing the difficulty in locating one of the most important objects in the game. Anticipating this scenario, we developed a fast method for detecting soccer balls independently of their colors. In Section 5, we describe a solution based on the morphological analysis of the image. The algorithm relies on edge detection and on the circular Hough transform, attaining a processing time almost constant and complying with the real-time constraint. Its appropriateness has been clearly demonstrated by the results obtained in the mandatory technical challenge of the RoboCup MSL: 2nd place in 2008 and 1st place in 2009. 2 Architecture of the vision system The CAMBADA robots [1] use a catadioptric vision system, often named omnidirectional vision system, based on a digital video camera pointing at a hyperbolic mirror, as presented in Fig. 2 . We are using a digital camera Point Grey Flea 2, 1 http://www.ptgrey.com/products/flea2/, Last accessed: 18/02/2010. 1 FL2-08S2C with a 1/3\u201d CCD Sony ICX204 that can deliver images up to 1024\u00d7768 pixels in several image formats, namely RGB, YUV 4:1:1, YUV 4:2:2 or YUV 4:4:4. The hyperbolic mirror was developed by IAIS Fraunhofer Gesellschaft 2 http://www.iais.fraunhofer.de/, Last accessed: 18/02/2010. 2 (FhG-AiS). Although the mirror was designed for the vision system of the FhG Volksbot 3 http://www.volksbot.de/, Last accessed: 18/02/2010. 3 we are achieving also an excellent result with it in our vision system. The use of omnidirectional vision systems have captured much interest in the last years, because it allows a robot to attain a 360\u00b0 field of view around its central vertical rotation axis, without having to move itself or its camera. In fact, it has been a common solution for the main sensorial element in a significant number of autonomous mobile robot applications, as is the case of the MSL, where most of the teams have adopted this approach [2\u20139]. A catadioptric vision system ensures an integrated perception of all major target objects in the surrounding area of the robot, allowing a higher degree of maneuverability. However, this also implies higher degradation in the resolution with growing distances away from the robot, when compared to non-isotropic setups. 3 Calibration of the vision system An important task in the MSL is the calibration of the vision system. This includes the calibration of intrinsic parameters of the digital camera, the computation of the inverse distance map, the detection of the mirror and robot center and the definition of the regions of the image that have to be processed. Calibration has to be performed when environmental conditions change, such as playing in a different soccer field or when the lighting conditions vary over time. Therefore, there are adjustments that have to be made almost continuously, for example if the playing field is unevenly illuminated, or less frequently, when the playing field changes. Moreover, a number of adjustments have also to be performed when some of the vision hardware of the robot is replaced, such as the camera or the mirror. All these calibrations and adjustments should be robust, i.e., they should be as much as possible insensitive to small environmental variations, they should be fast to perform and they should be simple to execute, so that no special calibration expert is required to operate them. 3.1 Self-calibration of the digital camera parameters In a near future, it is expected that the MSL robots will have to play under natural lighting conditions and in outdoor fields. This introduces new challenges. In outdoor fields, the illumination may change slowly during the day, due to the movement of the sun, but also may change quickly in short periods of time due to a partial and temporally varying covering of the sun by clouds. In this case, the robots have to adjust, in real-time, both the color segmentation values as well as some of the camera parameters, in order to adapt to new lighting conditions [10]. The common approach regarding the calibration of the robot cameras in the MSL has been based on manual adjustments, that are performed prior to the games, or through some automatic process that runs offline using a pre-acquired video sequence. However, most (or even all) of the parameters remain fixed during the game. We propose an algorithm that does not require human interaction to configure the most important parameters of the camera, namely the exposure, the white-balance, the gain and the brightness. Moreover, this algorithm runs continuously, even during the game, allowing coping with environmental changes that often occur when playing. We use the histogram of intensities of the acquired images and a black and a white area, which location is known in advance, to estimate the referred parameters of the camera. Note that this approach differs from the well known problem of photometric camera calibration (a survey can be found in [11]), since we are not interested in obtaining the camera response values, but only to configure its parameters according to some measures obtained from the acquired images. The self-calibration process for a single robot requires a few seconds, including the time necessary to start the application. This is significantly faster than the usual manual calibration by an expert user, for which several minutes are needed. 3.1.1 Proposed algorithm The proposed calibration algorithm processes the image acquired by the camera and analyzes a white area in the image (a white area in a fixed place on the robot body, near the camera in the center of the image), in order to calibrate the white-balance. A black area (we use a part of the image that represents the robot itself, actually a rectangle in the upper left side of the image) is used to calibrate the brightness of the image. Finally, the histogram of the image intensities is used to calibrate the exposure and gain. The histogram of the intensities of an image is a representation of the number of times that each intensity value appears in the image. For an image represented using 8 bits per pixel, the possible values are between 0 and 255. Image histograms can indicate some aspects of the lighting conditions, particularly the exposure of the image and whether if it is underexposed or overexposed. The assumptions used by the proposed algorithm are the following: (i) The white area should appear white in the acquired image. In the YUV color space, this means that the average value of U and V should be close to 127, that is to say, the chrominance components of the white section should be as close to zero as possible. If the white-balance is not correctly configured, these values are different from 127 and the image does not have the correct colors. The white-balance parameter is composed by two values, WB_BLUE and WB_RED, directly related to the values of U and V, respectively. (ii) The black area should be black. In the RGB color space, this means that the average values of R, G and B should be close to zero. If the brightness parameter is too high, it is observed that the black region becomes blue, resulting in a degradation of the image. (iii) The histogram of intensities should be centered around 127 and should span all intensity values. Dividing the histogram into regions, the left regions represent dark colors, while the right regions represent light colors. An underexposed image will be leaning to the left, while an overexposed image will be leaning to the right in the histogram (for an example, see Fig. 5a). The values of the gain and exposure parameters are adjusted according to the characteristic of the histogram. Statistical measures can be extracted from the images to quantify the image quality [12,13]. A number of typical measures used in the literature can be computed from the image gray level histogram, namely, the mean (1) \u03bc = \u2211 i = 0 N - 1 iP i , \u03bc \u2208 [ 0 , 255 ] , the entropy (2) E = - \u2211 i = 0 N - 1 P i log ( P i ) , E \u2208 [ 0 , 8 ] , the absolute central moment (3) ACM = \u2211 i = 0 N - 1 | i - \u03bc | P i , ACM \u2208 [ 0 - 127 ] and the mean sample value (4) MSV = \u2211 j = 0 4 ( j + 1 ) x j \u2211 j = 0 4 x j , MSV \u2208 [ 0 - 5 ] , where N is the number of possible gray values in the histogram (typically, 256), P i is the relative frequency of each gray value and x j is the sum of the gray values in region j of the histogram (in the proposed approach we divided the histogram into five regions). When the histogram values of an image are uniformly distributed in the possible values, then \u03bc \u2248127, E \u22488, ACM \u224860 and MSV \u22482.5. In the experimental results we use these measures to analyze the performance of the proposed calibration algorithm. Moreover, we use the information of MSV to calibrate the exposure and the gain of the camera. The algorithm is depicted next. do do acquire image calculate the histogram of intensities calculate the MSV value if MSV<2.0 OR MSV>3.0 apply the PI controller to adjust exposure else apply the PI controller to adjust gain set the camera with new exposure and gain values while exposure or gain parameters change do acquire image calculate average U and V values of the white area apply the PI controller to adjust WB_BLUE apply the PI controller to adjust WB_RED set the camera with new white-balance parameters while white-balance parameters change do acquire image calculate average R, G and B values of the black area apply the PI controller to adjust brightness set the camera with new brightness value while brightness parameter change while any parameter changed The calibration algorithm configures one parameter at a time, proceeding to the next one when the current one has converged. For each of these parameters, a PI controller was implemented. PI controllers are used instead of proportional controllers as they result in better control, having no stationary error. The coefficients of the controller were obtained experimentally: first, the proportional gain was increased until the camera parameter started to oscillate. Then, it was reduced to about 70% of that value and the integral gain was increased until an acceptable time to reach the desired reference was obtained [14]. The algorithm stops when all the parameters have converged. More details regarding this algorithm can be found in [15]. 3.1.2 Experimental results To measure the performance of this calibration algorithm, tests have been conducted using the camera with different initial configurations. In Fig. 3 , results are presented both when the algorithm starts with the parameters of the camera set to zero, as well as when set to the maximum value. As can be seen, the configuration obtained after running the proposed algorithm is approximately the same, independently of the initial configuration of the camera. Moreover, the algorithm is fast to converge (it takes between 60 and 70 frames). In Fig. 4 , it is presented an image acquired with the camera in auto-mode. As can be seen, the image obtained using the camera with the parameters in auto-mode is overexposed and the white balance is not configured correctly. This is due to the fact that the camera analyzes the entire image and, as can be observed in Fig. 3, there are large black regions corresponding to the robot itself. Our approach uses a mask to select the region of interest, in order to calibrate the camera using exclusively the valid pixels. Moreover, and due to the changes in the environment when the robot is moving, leaving the camera in auto-mode leads to undesirable changes in the parameters of the camera, causing color classification problems. Table 1 presents the values of the statistical measures described in (1)\u2013(4), regarding the experimental results presented in Fig. 3. These results confirm that the camera is correctly configured after applying the automated calibration procedure, since the results obtained are close to the optimal. Moreover, the algorithm converges always to the same set of parameters, independently of the initial configuration. According to the experimental results presented in Table 1, we conclude that the MSV measure is the best one for classifying the quality of an image. This is due to the fact that it is closer to the optimal values when the camera is correctly calibrated. Moreover, this measure can distinguish between two images that have close characteristics, as is the case when the camera is used in auto-mode. The good results of the automated calibration procedure can also be confirmed in the histograms presented in Fig. 5 . The histogram of the image obtained after applying the proposed automated calibration procedure (Fig. 5b) is centered near the intensity 127, which is a desirable property, as shown in Fig. 3 in the middle images. The histogram of the image acquired using the camera with all the parameters set to the maximum value (Fig. 5a) shows that the image is overexposed, leading that the majority of the pixels have bright colors. This algorithm has also been tested outdoors, under natural light. Fig. 6 shows that it works well even when the robot is under very different lighting conditions, showing its robustness. 3.2 Distance map calibration For most practical applications, the setup of the vision system requires the translation of the planar field of view at the camera sensor plane, into real world coordinates at the ground plane, using the robot as the center of this system. In order to simplify this non-linear transformation, most practical solutions adopted in real robots choose to create a mechanical geometric setup that ensures a symmetrical solution for the problem by means of a single viewpoint (SVP) approach. This, on the other hand, calls for a precise alignment of the four major points comprising the vision setup: the mirror focus, the mirror apex, the lens focus and the center of the image sensor. Furthermore, it also demands the sensor plane to be both parallel to the ground field and normal to the mirror axis of revolution, and the mirror foci to be coincident with the effective viewpoint and the camera pinhole respectively [16]. Although tempting, this approach requires a precision mechanical setup. In this section, we briefly present a general solution to calculate the robot centered distances map on non-SVP catadioptric setups, exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface. A detailed description of the algorithms can be found in [17] and a screenshot of the application is presented in Figs. 7 and 8 . This solution effectively compensates for the misalignment that may result either from a simple mechanical setup or from the use of low cost video cameras. The method can also extract most of the required parameters from the acquired image itself, allowing it to be used for self-calibration purposes. In order to allow further trimming of these parameters, two simple image feedback tools have been developed. The first one creates a reverse mapping of the acquired image into the real world distance map. A fill-in algorithm is used to integrate image data in areas outside pixel mapping on the ground plane. This produces a plane vision from above, allowing visual check of line parallelism and circular asymmetries (Fig. 9 ). The second generates a visual grid with 0.5m distances between both lines and columns, which is superimposed on the original image. This provides an immediate visual clue for the need of possible further distance correction (Fig. 10 ). With this tool, it is also possible to determine some other important parameters, namely the mirror center and the area of the image that will be processed by the object detection algorithms (Fig. 11 ). 4 Color-based object detection The algorithms that we propose for object detection can be split into three main modules, namely the Utility Sub-System, the Color Processing Sub-System and the Morphological Processing Sub-System, as shown in Fig. 12 . In the Color Processing Sub-System, proper color classification and extraction processes were developed, along with an object detection process to extract information from the acquired image, through color analysis. The Morphological Processing Sub-System presented in Section 5, is used to detect arbitrary FIFA balls independently of their colors. In order to satisfy the real-time constrains in the proposed image processing system, we implemented efficient data structures to process the image data [18,19]. Moreover, we use a two-thread approach to perform the most time consuming operations in parallel, namely the color classification and the color extraction, taking advantage of the dual core processor used by the laptop computers of our robots. 4.1 Color extraction Image analysis in the MSL is simplified, since objects are color coded. Black robots play with an orange ball on a green field that has white lines. Thus, the color of a pixel is a strong hint for object segmentation. We exploit this fact by defining color classes, using a look-up table (LUT) for fast color classification. The table consists of 16,777,216 entries (224, 8 bits for red, 8 bits for green and 8 bits for blue), each 8 bits wide, occupying a total of 16 MByte. Note that for other color spaces the table size would be the same, changing only the meaning of each component. Each bit expresses whether the color is within the corresponding class or not. This means that a certain color can be assigned to several classes at the same time. To classify a pixel, we first read the pixel\u2019s color and then use the color as an index into the table. The 8-bit value read from the table is called the \u201ccolor mask\u201d of that pixel. The color calibration is performed in the HSV (Hue, Saturation and Value) color space, since it provides a single, independent, color spectrum variable. In the current setup, the image is acquired in RGB or YUV format and then is converted to an image of labels using the appropriate LUT. Fig. 13 presents a screenshot of the application used to calibrate the color ranges for each color class, using the HSV color space and a histogram based analysis. Certain regions of the image are excluded from analysis. One of them is the part in the image that reflects the robot itself. Other regions are the sticks that hold the mirror and the areas outside the mirror. These regions are found using the algorithm described in Section 3.2. An example is presented on the right of Fig. 11, where the white pixels indicate the area that will be processed. With this approach, we can reduce the time spent in the conversion and searching phases and we also eliminate the problem of finding erroneous objects in those areas. To extract color information from the image we use radial search lines, instead of processing the whole image. A radial search line is a line that starts at the center of the robot, with some angle, and ends at the limits of the image. In an omnidirectional system, the center of the robot is approximately the center of the image (see left of Fig. 11). The search lines are constructed based on the Bresenham line algorithm [20]. They are constructed once, when the application starts, and saved in a structure in order to improve the access to these pixels in the color extraction module. For each search line, we iterate through its pixels to search for transitions between two colors and areas with specific colors. The use of radial search lines accelerates the process of object detection, due to the fact that we only process part of the valid pixels. This approach has a processing time almost constant, independently of the information that is captured by the camera. Moreover, the polar coordinates, inherent to the radial search lines, facilitate the definition of the bounding boxes of the objects in omnidirectional vision systems. We developed an algorithm for detecting areas of a specific color which eliminates the possible noise that could appear in the image. For each radial scanline, it is performed a median filtering operation. Each time a pixel is found with a color of interest, the algorithm analyzes the pixels that follow (a predefined number). If it does not find more pixels of that color, it discards the pixel found and continues. When a predefined number of pixels with that color is found, it considers that the search line has that color. Regarding the ball detection, we created an algorithm to recover lost orange pixels due to the ball shadow cast over itself. As soon as we find a valid orange pixel in the radial sensor, the shadow recovery algorithm tries to search for darker orange pixels previously discarded in the color segmentation analysis. The search is conducted in each radial sensor, starting at the first orange pixel found when searching towards the center of the robot, limited to a maximum number of pixels. For each pixel analyzed, a comparison is performed using a wider region of the color space, in order to being able to accept darker orange pixels. Once a different color is found or the maximum number of pixels is reached, the search along the current sensor is completed and the next sensor is processed. In Fig. 16, we can see the pixels recovered by this algorithm (the orange blobs contain pixels that were not originally classified as orange). To accelerate the process of calculating the position of the objects, we put the color information that was found in each of the search lines into a list of colors. We are interested in the first pixel (in the corresponding search line) where the color was found and with the number of pixels with that color that have been found in the search line. Then, using the previous information, we separate the information of each color into blobs (Fig. 16 shows an example). After this, it is calculated the blob descriptor that will be used for the object detection module, which contains the following information: \u2013 Distance to the robot. \u2013 Closest pixel to the robot. \u2013 Position of the mass center. \u2013 Angular width. \u2013 Number of pixels. \u2013 Number of green and white pixels in the neighborhood of the blob. 4.2 Object detection The objects of interest that are present in a MSL game are: a ball, obstacles and the green field with white lines. Currently, our system detects efficiently all these objects with a set of simple algorithms that, using the color information collected by the radial search lines, calculate the object position and/or its limits in a polar representation (distance and angle). The algorithm that searches for the transitions between green pixels and white pixels is described next. If a non-green pixel is found in a radial scanline, we search for the next green pixel, counting the number of non-green pixels and the number of white pixels that meanwhile appeared. If these values are greater than a predefined threshold, the center of this region is considered a transition point corresponding to a position of a soccer field line. The algorithm is illustrated in Fig. 14 with an example. A similar approach has been described in [21]. The ball is detected using the following algorithm: (i) Separate the orange information into blobs. (ii) For each blob, calculate the information described previously. (iii) Perform a first validation of the orange blobs using the information about the green and white pixels in the neighborhood of the blob, to guarantee that only balls inside the field are detected. (iv) Validate the remaining orange blobs according to the number of pixels. As illustrated in Fig. 15 , it is known the relation between the pixel size at the ground plane and the distance to the center of the robot. Using this knowledge, we estimate the number of pixels that a ball should have according to the distance. (v) Following the same approach, the angular width is also used to validate the blobs. (vi) The ball candidate is the valid blob closest to the robot. The position of the ball is the mass center of the blob. To calculate the position of the obstacles around the robot, we use the following algorithm: (i) Separate the black information into blobs. (ii) Calculate the information for each blob. (iii) Perform a simple validation of the black blobs using the information about the green and white pixels in the neighborhood of the blob, to guarantee that only obstacles inside the field are detected. (iv) The position of the obstacle is given by the distance of the blob relatively to the robot. The limits of the obstacle are obtained using the angular width of the blob. More details regarding the detection and identification of obstacles can be found in [22]. Fig. 16 presents an example of an acquired image, the corresponding segmented image and the detected color blobs. As can be seen, the objects are correctly detected. The position of the white lines, the position of the ball and the information about the obstacles are then sent to the Real-time Database [1,23] and used, afterward, by the high level process responsible for the behaviors of the robots [24,25,22,26]. 4.3 Experimental results To experimentally measure the efficiency of the proposed algorithms, the robot was moved along a predefined path through the robotic soccer field, leaving the ball in a known location. The ball position given by the robot is then compared with the real position of the ball. Note that the results in this test may be affected by errors in the localization algorithm and by some bumps while the robot is moving. The separate study of these sources of error has being left outside this experimental evaluation. However, they should be performed, for better understanding the several factors that influence the correct localization of the ball. The robot path across the field may be seen in Fig. 17 , along with the measured ball position. According to that data, it is possible to notice that the average of the measured positions of the ball is almost centered in the real ball position, showing the effectiveness of the proposed algorithms. Our measures show a very high detection ratio (near 95%), and a good accuracy, with the average measures very close to the real ball position. In our experiments, we verified that the robots are able to detect the ball up to 6m with regular light conditions and a good color calibration, easy to obtain after applying the proposed automated calibration algorithm described in Section 3. The proposed algorithm has an almost constant processing time, independently of the environment around the robot, typically around 6ms. It needs approximately 35MBytes of memory. The experimental results were obtained using a camera resolution of 640\u00d7480 pixels and a laptop with an Intel Core 2 duo at 2.0GHz and 1GB of memory. 5 Arbitrary ball detection The color codes tend to disappear as the competition evolves, increasing the difficulty posed to the vision algorithms. The color of the ball, currently orange, is the next color scheduled to become arbitrary. In this section, we propose a solution for overcoming this new challenge, i.e., a method for detecting balls independently of their colors. This solution is based on a morphological analysis of the image, being strictly directed to detect round objects in the field with specific characteristics, in this case the ball. Morphological object recognition through image analysis has became more robust and accurate in the past years, whereas still very time consuming even to modern personal computers. Because RoboCup is a real-time environment, available processing time can become a serious constraint when analyzing large amounts of data or executing complex algorithms. This section presents an arbitrary FIFA ball recognition algorithm, based on the use of image segmentation and the circular Hough transform. The processing time is almost constant and allows real-time processing. As far as we know, this approach has never been proposed. The experimental results obtained, as well as the classifications obtained by the CAMBADA team, seem to be very promising. Regarding the vision system described in Fig. 12, it is possible to specify whether to use the Morphological sub-system to detect the ball or the current color-based approach. Currently, in the MSL, the shape-based detection is only necessary in the mandatory challenge of the competition, although it will be incorporated in the rules in the next years. 5.1 Related work Many of the algorithms proposed during previous research work showed their effectiveness but, unfortunately, their processing time is in some cases over one second per video frame [27]. In [28], the circular Hough transform was presented in the context of colored ball detection as a validation step. However, no details about the implementation and experimental results have been presented. Hanek et al. [29] proposed a Contracting Curve Density algorithm to recognize the ball without color labeling. This algorithm fits parametric curve models to the image data by using local criteria based on local image statistics to separate adjacent regions. This method can extract the contour of the ball even in cluttered environments under different illumination, but the vague position of the ball should be known in advance. The global detection cannot be realized by this method. Treptow et al. [30] proposed a method to detect and track a ball without color information in real-time, by integrating the Adaboost Feature Learning algorithm into a condensation tracking framework. Mitri et al. [31] presented a scheme for color invariant ball detection, in which the edged filtered images serve as the input of an Adaboost learning procedure that constructs a cascade of classification and regression trees. This method can detect different soccer balls in different environments, but the false positive rate is high when there are other round objects in the environment. Coath et al. [32] proposed an edge-based arc fitting algorithm to detect the ball for soccer robots. However, the algorithm is used in a perspective camera vision system in which the field of view is far smaller and the image is also far less complex than that of the omnidirectional vision system used by most of the robotic soccer teams. More recently, Lu et al. [33] considered that the ball on the field can be approximated by an ellipse. They scan the color variation to search for the possible major and minor axes of the ellipse, using radial and rotary scanning, respectively. A ball is considered if the middle points of a possible major axis and a possible minor axis are very close to each other in the image. However, this method has a processing time that can achieve 150ms if the tracking algorithm fails, which might cause problems in real-time applications. 5.2 Proposed approach The proposed approach is presented in the top layer of Fig. 12. The search for potential ball candidates is conducted taking advantage of morphological characteristics of the ball (round shape), using a feature extraction technique known as the Hough transform. This is a technique for identifying the locations and orientations of certain types of features in a digital image [34]. The Hough transform algorithm uses an accumulator and can be described as a transformation of a point in the x, y-plane to the parameter space. The parameter space is defined according to the shape of the object of interest, in this case, the ball presents a rounded shape. First used to identify lines in images, the Hough transform has been generalized through the years to identify positions of arbitrary shapes by a voting procedure [35\u201337]. Fig. 18 shows an example of a circular Hough transform, for a constant radius, from the x, y-space to the parameter space. In Fig. 19 , we show an example of circle detection through the circular Hough transform. We can see the original image of a dark circle (known radius r) on a bright background (see Fig. 19a). For each dark pixel, a potential circle-center locus is defined by a circle with radius r and center at that pixel (see Fig. 19b). The frequency with which image pixels occur in the circle-center loci is determined (see Fig. 19c). Finally, the highest-frequency pixel represents the center of the circle with radius r. To feed the Hough transform process, it is necessary a binary image with the edge information of the objects. This image, Edges Image, is obtained using an edge detector operator. In the following, we present an explanation of this process and its implementation. To be possible to use this image processing system in real-time, and increase time efficiency, a set of data structures to process the image data has been implemented [18,19]. The proposed algorithm is based on three main operations: (i) Edge detection: this is the first image processing step in the morphological detection. It must be as efficient and accurate as possible in order not to compromise the efficiency of the whole system. Besides being fast to calculate, the intended resulting image must be absent of noise as much as possible, with well defined contours, and be tolerant to the motion blur introduced by the movement of the ball and the robots. Some popular edge detectors were tested, namely Sobel [38,39], Laplace [40,41] and Canny [42]. The tests were conducted under two distinct situations: with the ball standing still and with the ball moving fast through the field. The test with the ball moving fast was performed in order to study the motion blur effect in the edge detectors, on high speed objects captured with a frame rate of 30 frames per second. For choosing the best edge detector for this purpose, the results from the tests were compared taking into account the image of edges and processing time needed by each edge detector. On one hand, the real-time capability must be assured. On the other hand, the algorithm must be able to detect the edges of the ball independently of its motion blur effect. According to our experiments, the Canny edge detector was the most demanding in terms of processing time. Even so, it was fast enough for real-time operation and, because it provided the most effective contours, it was chosen. The parameters of the edge detector were obtained experimentally. (ii) Circular Hough transform: this is the next step in the proposed approach to find points of interest containing eventual circular objects. After finding these points, a validation procedure is used for choosing points containing a ball, according to our characterization. The voting procedure of the Hough transform is carried out in a parameter space. Object candidates are obtained as local maxima of a denoted Intensity Image (Fig. 20 c), that is constructed by the Hough Transform block (Fig. 12). Due to the special features of the Hough circular transform, a circular object in the Edges Image would produce an intense peak in Intensity Image corresponding to the center of the object (as can be seen in Fig. 20c). On the contrary, a non-circular object would produce areas of low intensity in the Intensity Image. However, as the ball moves away, its edge circle size decreases. To solve this problem, information about the distance between the robot center and the ball is used to adjust the Hough transform. We use the inverse mapping of our vision system [17] to estimate the radius of the ball as a function of distance. (iii) Validation: in some situations, particularly when the ball is not present in the field, false positives might be produced. To solve this problem and improve the ball information reliability, we propose a validation algorithm that discards false positives based on information from the Intensity Image and the Acquired Image. This validation algorithm is based on two tests against which each ball candidate is put through. In the first test performed by the validation algorithm, the points with local maximum values in the Intensity Image are considered if they are above a distance-dependent threshold. This threshold depends on the distance of the ball candidate to the robot center, decreasing as this distance increases. This first test removes some false ball candidates, leaving a reduced group of points of interest. Then, a test is made in the Acquired Image over each point of interest selected by the previous test. This test is used to eliminate false balls that usually appear in the intersection of the lines of the field and other robots (regions with several contours). To remove these false balls, we analyze a square region of the image centered in the point of interest. We discard this point of interest if the sum of all green pixels is over a certain percentage of the square area. Note that the area of this square depends on the distance of the point of interest to the robot center, decreasing as this distance increases. Choosing a square where the ball fits tightly makes this test very effective, considering that the ball fills over 90% of the square. In both tests, we use threshold values that were obtained experimentally. Besides the color validation, it is also performed a validation of the morphology of the candidate, more precisely a circularity validation. Here, from the candidate point to the center of the ball, it is performed a search of pixels at a distance r from the center. For each edge found between the expected radius, the number of edges at that distance are determined. By the size of the square which covers the possible ball and the number of edge pixels, it is calculated the edges percentage. If the edges percentage is greater than 70, then the circularity of the candidate is verified. The position of the detect ball is then sent to the Real-time Database, together with the information of the white lines and the information about the obstacles to be used, afterward, by the high level process responsible for the behaviors of the robots. 5.3 Experimental results Fig. 20 presents an example of the Morphological Processing Sub-System. As can be observed, the balls in the Edges Image (Fig. 20b) have almost circular contours. Fig. 20c) shows the resulting image after applying the circular Hough transform. Notice that the center of the balls present a very high peak when compared to the rest of the image. The ball considered was the closest to the robot, due to the fact that it has the high peak in the image. To ensure good results in the RoboCup competition, the system was tested with the algorithms described above. For that purpose, the robot was moved along a predefined path through the robotic soccer field, leaving the ball in a known location. The ball position given by the robot is then compared with the real position of the ball. The results in this test may be affected by the errors in the localization algorithm and by the robot bumps while moving. These external errors are out of the scope of this study. The robot path in the field may be seen in Fig. 21 , along with the measured ball position. It is possible to notice that the average of the measured positions of the ball is almost centered in the real ball position, showing the accuracy of the proposed algorithms. We obtained a very high detection ratio (near 90%) and a false positive rate around 0%, which is a very significant result. With the proposed approach, the omnidirectional vision system can detect the ball within this precision until distances up to 4 meters. The average processing time of the proposed approach was approximately 16ms. It needs approximately 40MBytes of memory. The experimental results have been obtained using a camera resolution of 640\u00d7480 pixels and a laptop with an Intel Core 2 duo at 2.0GHz. 6 Conclusions This paper presents the omnidirectional vision system developed for the CAMBADA MSL robotic soccer team, from the calibration to the object detection. We presented several algorithms for the calibration of the most important parameters of the vision system and we proposed efficient color-based algorithms for object detection. Moreover, we proposed a solution for the detection of arbitrary FIFA balls, one of the current challenges in the MSL. The CAMBADA team won the last three editions of the Portuguese Robotics Festival, ranked 5th in RoboCup 2007, won the RoboCup 2008 and ranked 3rd in RoboCup 2009, demonstrating the effectiveness of our vision algorithms in a competition environment. As far as we know, no previous work has been published describing all the steps of the design of an omnidirectional vision system. Moreover, some of the algorithms presented in this paper are state-of-the-art, as demonstrated by the first place obtained in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. We are currently working in the automatic calibration of the inverse distance mapping and in efficient algorithms for autonomous color calibration, based on region growing. Regarding the object detection algorithms, as we have reduced the processing time to a few milliseconds, we are working on the acquisition of higher resolution images, capturing only a region of interest. The idea of work with higher image resolutions is to improve the object detection at higher distances. Moreover, we continue the development of algorithms for shape-based object detection, also to incorporate as a validation of the color-based algorithms. Acknowledgment This work was supported in part by the FCT (Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia). References [1] Neves A, Azevedo J, Cunha NLB, Silva J, Santos F, Corrente G, et al. CAMBADA soccer team: from robot architecture to multiagent coordination. In: Vladan Papic editor. Robot soccer. Vienna, Austria: I-Tech Education and Publishing; 2010 [chapter 2]. [2] Zivkovic Z, Booij O. How did we built our hyperbolic mirror omni-directional camera-practical issues and basic geometry. Tech. rep. Intelligent Systems Laboratory, University of Amsterdam; 2006. [3] Wolf J. Omnidirectional vision system for mobile robot localization in the robocup environment. Master\u2019s thesis. Graz University of Technology; 2003. [4] Menegatti E, Nori F, Pagello E, Pellizzari C, Spagnoli D. Designing an omnidirectional vision system for a goalkeeper robot. In: Proc of RoboCup 2001. Lecture notes in computer science, vol. 2377. Springer; 2001. p. 78\u201387. [5] Menegatti E, Pretto A, Pagello E. Testing omnidirectional vision-based monte carlo localization under occlusion. In: Proc of the IEEE intelligent robots and systems, IROS 2004; 2004. p. 2487\u201393. [6] P. Lima A. Bonarini C. Machado F. Marchese C. Marques F. Ribeiro Omni-directional catadioptric vision for soccer robots Robot Auton Syst 36 2\u20133 2001 87 102 [7] Liu F, Lu H, Zheng Z. A robust approach of field features extraction for robot soccer. In: Proc of the 4th IEEE Latin America robotic symposium, Monterry, Mexico; 2007. [8] Lu H, Zheng Z, Liu F, Wang X. A robust object recognition method for soccer robots. In: Proc of the 7th world congress on intelligent control and automation, Chongqing, China; 2008. [9] Voigtlrande A, Lange S, Lauer M, Riedmiller M. Real-time 3D ball recognition using perspective and catadioptric cameras. In: Proc of the 3rd European conference on mobile robots, Freiburg, Germany; 2007. [10] Mayer G, Utz H, Kraetzschmar G. Playing robot soccer under natural light: a case study. In: Proc of the RoboCup 2003. Lecture notes on artificial intelligence, vol. 3020. Springer; 2003. [11] Krawczyk G, Goesele M, Seidel H. Photometric calibration of high dynamic range cameras. Research Report MPI-I-2005-4-005. Max-Planck-Institut f\u00fcr Informatik, Stuhlsatzenhausweg 85, 66123 Saarbr\u00fccken, Germany; April 2005. [12] Shirvaikar MV. An optimal measure for camera focus and exposure. In: Proc of the IEEE southeastern symposium on system theory, Atlanta (USA); 2004. [13] Nourani-Vatani N, Roberts J. Automatic camera exposure control. In: Proc of the 2007 Australasian conference on robotics and automation, Brisbane, Australia; 2007. [14] K. \u00c5str\u00f6m T. H\u00e5gglund PID controllers: theory, design, and tuning 2nd ed. 1995 Instruments Society of America [15] Neves AJR, Cunha AJPB, Pinheiro I. Autonomous configuration of parameters in robotic digital cameras. In: Proc of the 4th Iberian conference on pattern recognition and image analysis, IbPRIA-2009. Lecture notes in computer science, vol. 5524. P\u00f3voa do Varzim, Portugal: Springer; 2009. p. 80\u20137. [16] S. Baker S.K. Nayar A theory of single-viewpoint catadioptric image formation Int J Comput Vis 2 1999 175 196 [17] Cunha B, Azevedo JL, Lau N, Almeida L. Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system. In: Proc of the RoboCup 2007. Lecture notes in computer science, vol. 5001. Atlanta (USA): Springer; 2007. p. 417\u201324. [18] Neves AJR, Martins DA, Pinho AJ. A hybrid vision system for soccer robots using radial search lines. In: Proc of the 8th conference on autonomous robot systems and competitions. Portuguese robotics open \u2013 ROBOTICA\u20192008, Aveiro, Portugal; 2008. p. 51\u20135. [19] Neves AJR. Corrente G, Pinho AJ. An omnidirectional vision system for soccer robots. In: Proc of the 2nd international workshop on intelligent robotics, IROBOT 2007. Lecture notes in artificial intelligence, vol. 4874. Springer; 2007. p. 499\u2013507. [20] J.E. Bresenham Algorithm for computer control of a digital plotter IBM Syst J 4 1 1965 25 30 [21] Merke A, Welker S, Riedmiller M. Line base robot localisation under natural light conditions. In: Proc of the ECAI workshop on agents in dynamic and real-time environments, Valencia, Spain; 2002. [22] Silva J, Lau N, Rodrigues J, Azevedo JL, Neves AJR. Sensor and information fusion applied to a robotic soccer team. In: RoboCup 2009: robot soccer world cup XIII. Lecture notes in artificial intelligence. Springer; 2009. [23] Almeida L, Santos F, Facchinetti T, Pedreira P, Silva V, Lopes LS. Coordinating distributed autonomous agents with a real-time database: the CAMBADA project. In: Proc of the 19th international symposium on computer and information sciences, ISCIS 2004. Lecture notes in computer science, vol. 3280. Springer; 2004. p. 878\u201386. [24] Lau N, Lopes LS, Corrente G, Filipe N. Roles, positionings and set plays to coordinate a msl robot team. In: Proc of the 4th international workshop on intelligent robotics, IROBOT\u201909. Lecture notes in computer science, vol. 5816. Aveiro, Portugal: Springer; 2009. p. 323\u201337. [25] Lau N, Lopes LS, Corrente G, Filipe N. Multi-robot team coordination through roles, positioning and coordinated procedures. In: Proc of the IEEE/RSJ international conference on intelligent robots and systems, MO, USA: St. Louis; 2009. p. 5841\u201348. [26] Silva J, Lau N, Neves AJR, Rodrigues J, Azevedo JL. Obstacle detection, identification and sharing on a robotic soccer team. In: Proc of the 4th international workshop on intelligent robotics, IROBOT\u201909. Lecture notes in computer science, LNAI 5816. Aveiro, Portugal: Springer; 2009. p. 350\u201360. [27] Mitri S, Frintrop S, Pervolz K, Surmann H, Nuchter A. Robust object detection at regions of interest with an application in ball recognition. In: Proc of the 2005 IEEE international conference on robotics and automation, ICRA 2005, Barcelona, Spain; 2005. p. 125\u201330. [28] Jonker P, Caarls J, Bokhove W. Fast and accurate robot vision for vision based motion. In: RoboCup 2000: robot soccer world cup IV. Lecture notes in computer science. Springer; 2000. p. 149\u201358. [29] Hanek R, Schmitt T, Buck S. Fast image-based object localization in natural scenes. In: Proc of the 2002 IEEE/RSJ international conference on intelligent robotics and systems, Lausanne, Switzerland; 2002. p. 116\u201322. [30] A. Treptow A. Zell Real-time object tracking for soccer-robots without color information Robot Auton Syst 48 1 2004 41 48 [31] Mitri S, Pervolz K, Surmann H, Nuchter A. Fast color independent ball detection for mobile robots. In: Proc of the 2004 IEEE international conference on mechatronics and robotics, Aechen, Germany; 2004. p. 900\u20135. [32] Coath G, Musumeci P. Adaptive arc fitting for ball detection in RoboCup. In: Proc of the APRS workshop on digital image computing, WDIC 2003, Brisbane, Australia; 2003. p. 63\u20138. [33] Lu H, Zhang H, Zheng Z. Arbitrary ball recognition based on omni-directional vision for soccer robots. In: Proc of RoboCup 2008; 2008. [34] Nixon M, Aguado A. Feature extraction and image processing. 1st ed. Linacre House, Jordan Hill, Oxford OX2 8DP 225 Wildwood Avenue, Woburn, MA 01801-2041: Reed Educational and Professional Publishing Ltd.; 2002. [35] Ser PK, Siu WC. Invariant hough transform with matching technique for the recognition of non-analytic objects. In: IEEE international conference on acoustics, speech, and signal processing, ICASSP 1993, vol. 5; 1993. p. 9\u201312. [36] Zhang YJ, Liu ZQ. Curve detection using a new clustering approach in the hough space. In: IEEE international conference on systems, man, and cybernetics, 2000, vol. 4; 2000. p. 2746\u201351. [37] W.E.L. Grimson D.P. Huttenlocher On the sensitivity of the hough transform for object recognition IEEE Trans Pattern Anal Mach Intell 12 1990 1255 1274 [38] Zou J, Li H, Liu B, Zhang R. Color edge detection based on morphology. In: First international conference on communications and electronics, ICCE 2006; 2006. p. 291\u20133. [39] Zin TT, Takahashi H, Hama H. Robust person detection using far infrared camera for image fusion. In: Second international conference on innovative computing, information and control, ICICIC 2007; 2007. p. 310. [40] Zou Y, Dunsmuir W. Edge detection using generalized root signals of 2-d median filtering. In: Proc of the international conference on image processing, 1997, vol. 1; 1997. p. 417\u20139. [41] Blaffert T, Dippel S, Stahl M, Wiemker R. The laplace integral for a watershed segmentation. In: Proc of the international conference on image processing, 2000, vol. 3; 2000. p. 444\u20137. [42] Canny JF. A computational approach to edge detection. IEEE Trans Pattern Anal Mach Intell 8 (6).", "scopus-id": "79952628276", "coredata": {"eid": "1-s2.0-S0957415810000863", "dc:description": "Robotic soccer is nowadays a popular research domain in the area of multi-robot systems. In the context of RoboCup, the Middle Size League is one of the most challenging. This paper presents an efficient omnidirectional vision system for real-time object detection, developed for the robotic soccer team of the University of Aveiro, CAMBADA. The vision system is used to find the ball and white lines, which are used for self-localization, as well as to find the presence of obstacles. Algorithms for detecting these objects and also for calibrating most of the parameters of the vision system are presented in this paper. We also propose an efficient approach for detecting arbitrary FIFA balls, which is an important topic of research in the Middle Size League. The experimental results that we present show the effectiveness of our algorithms, both in terms of accuracy and processing time, as well as the results that the team has been achieving: 1st place in RoboCup 2008, 3rd place in 2009 and 1st place in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball.", "openArchiveArticle": "false", "prism:coverDate": "2011-03-31", "openaccessUserLicense": null, "prism:aggregationType": "Journal", "prism:url": "https://api.elsevier.com/content/article/pii/S0957415810000863", "dc:creator": [{"@_fa": "true", "$": "Neves, Ant\u00f3nio J.R."}, {"@_fa": "true", "$": "Pinho, Armando J."}, {"@_fa": "true", "$": "Martins, Daniel A."}, {"@_fa": "true", "$": "Cunha, Bernardo"}], "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/article/pii/S0957415810000863"}, {"@_fa": "true", "@rel": "scidir", "@href": "https://www.sciencedirect.com/science/article/pii/S0957415810000863"}], "dc:format": "application/json", "openaccessType": null, "pii": "S0957-4158(10)00086-3", "prism:volume": "21", "prism:publisher": "Elsevier Ltd.", "dc:title": "An efficient omnidirectional vision system for soccer robots: From calibration to object detection", "prism:copyright": "Copyright \u00a9 2010 Elsevier Ltd. All rights reserved.", "prism:issueName": "Special Issue on Advances in intelligent robot design for the Robocup Middle Size League", "openaccess": "0", "prism:issn": "09574158", "prism:issueIdentifier": "2", "dcterms:subject": [{"@_fa": "true", "$": "Robotic vision"}, {"@_fa": "true", "$": "Omnidirectional vision systems"}, {"@_fa": "true", "$": "Color-based object detection"}, {"@_fa": "true", "$": "Shape-based object detection"}, {"@_fa": "true", "$": "Vision system calibration"}], "openaccessArticle": "false", "prism:publicationName": "Mechatronics", "prism:number": "2", "openaccessSponsorType": null, "prism:pageRange": "399-410", "prism:endingPage": "410", "pubType": "fla", "prism:coverDisplayDate": "March 2011", "prism:doi": "10.1016/j.mechatronics.2010.05.006", "prism:startingPage": "399", "dc:identifier": "doi:10.1016/j.mechatronics.2010.05.006", "openaccessSponsorName": null}, "objects": {"object": [{"@category": "thumbnail", "@height": "53", "@width": "289", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-si4.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1540", "@ref": "si4", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "46", "@width": "293", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-si3.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1436", "@ref": "si3", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "46", "@width": "238", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-si2.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1250", "@ref": "si2", "@mimetype": "image/gif"}, {"@category": "thumbnail", "@height": "46", "@width": "192", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-si1.gif?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "ALTIMG", "@size": "1092", "@ref": "si1", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "182", "@width": "377", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr10.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "45233", "@ref": "gr10", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "106", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr10.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14653", "@ref": "gr10", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "138", "@width": "376", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr11.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "18028", "@ref": "gr11", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "81", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr11.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2966", "@ref": "gr11", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "281", "@width": "757", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr12.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "43502", "@ref": "gr12", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "81", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr12.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "6024", "@ref": "gr12", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "208", "@width": "377", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr13.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "31126", "@ref": "gr13", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "121", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr13.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "11667", "@ref": "gr13", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "60", "@width": "489", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr14.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "7740", "@ref": "gr14", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "27", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr14.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "1994", "@ref": "gr14", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "223", "@width": "373", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr15.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "19330", "@ref": "gr15", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "131", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr15.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3452", "@ref": "gr15", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "197", "@width": "765", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr16.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "43365", "@ref": "gr16", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "56", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr16.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5658", "@ref": "gr16", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "182", "@width": "264", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr17.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "11813", "@ref": "gr17", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "151", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr17.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4751", "@ref": "gr17", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "172", "@width": "358", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr18.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "9049", "@ref": "gr18", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "105", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr18.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "2922", "@ref": "gr18", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "119", "@width": "217", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr19.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "8089", "@ref": "gr19", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "121", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr19.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4702", "@ref": "gr19", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "199", "@width": "271", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr2.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "27937", "@ref": "gr2", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "161", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr2.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "19723", "@ref": "gr2", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "140", "@width": "484", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr20.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "23395", "@ref": "gr20", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "63", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr20.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "5238", "@ref": "gr20", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "220", "@width": "361", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr21.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "24408", "@ref": "gr21", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "133", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr21.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "4625", "@ref": "gr21", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "455", "@width": "703", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr3.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "57308", "@ref": "gr3", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "142", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr3.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "8598", "@ref": "gr3", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "139", "@width": "381", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr4.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "25516", "@ref": "gr4", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "80", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr4.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "10182", "@ref": "gr4", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "199", "@width": "575", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr5.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "22239", "@ref": "gr5", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "76", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr5.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "3068", "@ref": "gr5", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "137", "@width": "374", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr6.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "26663", "@ref": "gr6", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "80", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr6.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "10562", "@ref": "gr6", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "405", "@width": "533", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr7.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "61745", "@ref": "gr7", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "164", "@width": "215", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr7.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "15882", "@ref": "gr7", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "374", "@width": "533", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr8.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "53286", "@ref": "gr8", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "154", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr8.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "11776", "@ref": "gr8", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "179", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr9.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "33410", "@ref": "gr9", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "104", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr9.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "13228", "@ref": "gr9", "@mimetype": "image/gif"}, {"@category": "standard", "@height": "225", "@width": "378", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr1.jpg?httpAccept=%2A%2F%2A", "@multimediatype": "JPEG image file", "@type": "IMAGE-DOWNSAMPLED", "@size": "39196", "@ref": "gr1", "@mimetype": "image/jpeg"}, {"@category": "thumbnail", "@height": "130", "@width": "219", "@_fa": "true", "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957415810000863-gr1.sml?httpAccept=%2A%2F%2A", "@multimediatype": "GIF image file", "@type": "IMAGE-THUMBNAIL", "@size": "14502", "@ref": "gr1", "@mimetype": "image/gif"}]}, "link": {"@rel": "abstract", "@href": "https://api.elsevier.com/content/abstract/scopus_id/79952628276"}}