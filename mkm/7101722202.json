[
    {
        "doc_title": "Segmentation and Manipulation of Cork Strips in Bulk",
        "doc_scopus_id": "85107151302",
        "doc_doi": "10.1109/ICARSC52212.2021.9429769",
        "doc_eid": "2-s2.0-85107151302",
        "doc_date": "2021-04-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [
            "Computer vision techniques",
            "Conveyor belts",
            "Cork stoppers",
            "Industry sectors",
            "Motion planners",
            "Natural cork",
            "Punching machine",
            "Rgb-d cameras"
        ],
        "doc_abstract": "© 2021 IEEE.The production of cork stoppers is the largest application of natural cork, which is an ever-growing industry sector. Many attempts have been made to increase the automation of this process, such as the use of automated cork punching machines, but not all steps of this process are fully efficient such as the manipulation of cork strips prior to perforation, which is still a hand labor. This paper presents a system based on an RGBD camera and a 6 DoF robotic arm that manipulates cork strips which are disposed in bulk, either in a container or in a conveyor belt. It uses computer vision techniques to segment a single cork strip from the bunch and motion planners to control the robotic arm in order to grab the selected cork strip. On the experiments made, the system was able to correctly grab a cork strip with 92% success rate and with a frequency of 6 strips per minute.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Neural Network Classifier and Robotic Manipulation for an Autonomous Industrial Cork Feeder",
        "doc_scopus_id": "85115444254",
        "doc_doi": "10.1007/978-3-030-86230-5_34",
        "doc_eid": "2-s2.0-85115444254",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Convolutional neural network",
            "Cork",
            "Deep learning",
            "High quality",
            "Image processing technique",
            "Network robotics",
            "Neural networks classifiers",
            "Robotic manipulation",
            "Specific orientation",
            "Universal robot"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.This paper presents a solution for an autonomous cork puncher feeder with a robotic arm using image processing techniques and a convolutional neural network. Due to the need for cork strips to be inserted into the puncher with a specific orientation, to produce high quality cork stoppers, the identification of the orientation of each cork strip on the conveyor belt is a necessity. In response to this problem a convolutional neural network is used to analyse images processed with subtracted background, to create a robust solution for cork strips classification. In the tests carried out, a classification accuracy of 100% was obtained in a test data set with 12 different cork strips.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "TIMAIRIS: Autonomous Blank Feeding for Packaging Machines",
        "doc_scopus_id": "85087543982",
        "doc_doi": "10.1007/978-3-030-34507-5_7",
        "doc_eid": "2-s2.0-85087543982",
        "doc_date": "2020-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Alternative solutions",
            "Computer vision system",
            "Current packaging",
            "Different shapes",
            "Industrial environments",
            "Mobile manipulator",
            "Modes of operation",
            "Multi-Modal Interactions"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Current packaging machine vendors do not provide any automated mechanism for blank feeding and the state of the art is to have a human operator dedicated to feed the blank piles to the packaging machine. This is a tedious, repetitive and tiring task. This also results in problems with unintentional errors, such as using the wrong pile of blanks. An alternative solution is the use of a fixed robotic arm surrounded by a protective cage. However, this solution is restricted to a single packaging machine, a unique type of blank shapes and does not cooperate with humans. TIMAIRIS is a joint effort between IMA S.p.A., Italy, (IMA) and the Universidade de Aveiro, Portugal, (UAVR), promoted by the European Robotics Challenges (EuRoC) project. Together, we propose a system based on a mobile manipulator for flexible, autonomous and collaborative blank feeding of packaging machines on industrial shop floor. The system provides a software architecture that allows a mobile robot to take high level decisions on how the task should be executed, which can depend on variables such as the number of packaging machines to feed and the rate of blank consumption at each one. Through a computer vision system, blanks of different shapes and sizes are correctly identified for adequate manipulation. The manipulation of the piles of blanks is performed using a single arm using compliant modes of operation to increase manipulation safety and robustness. Additionally, it has a safe navigation system that allows the robot to be integrated in an industrial environment where humans are present. Finally, it provides an enhanced multimodal interaction between human and robot that can be adapted to the environment and operator characteristics to make communication intuitive, redundant and safe.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Skill-based anytime agent architecture for European Robotics Challenges in realistic environments: EuRoC Challenge 2, Stage II — realistic labs",
        "doc_scopus_id": "85070213536",
        "doc_doi": "10.1016/j.robot.2019.06.006",
        "doc_eid": "2-s2.0-85070213536",
        "doc_date": "2019-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Agent architectures",
            "European robotics challenges (EuRoC)",
            "Manufacturing industries",
            "Mobile manipulation",
            "Realistic environments",
            "Robotics technology",
            "Scientific competition",
            "Skill-based"
        ],
        "doc_abstract": "© 2019As demands on pragmatic solutions of robotics technology increase in the manufacturing industry, deep affinities between research experts and industry users are required. The European Robotics Challenges (EuRoC) research project has proposed a scientific competition and matched up research labs with industrial end users to establish challenger teams to develop and test solutions that will be applied in the real context of the industrial end-users. The paper reports the result of TIMAIRIS who is one of 6 challenger teams to advance to the final stage out of 103 teams and technical details used in the Challenge 2 - Shop Floor Logistics and Manipulation. To address the requirements and achieve the objectives of the challenge, a skill-based anytime agent architecture has been developed and extended to make the team focus on the challenging research that addresses real issues in the user environments. Finally, shop floor logistics and manipulation scenarios have been developed and demonstrated in a realistic environment for autonomous packaging.",
        "available": true,
        "clean_text": "serial JL 271599 291210 291866 291870 291882 291883 31 Robotics and Autonomous Systems ROBOTICSAUTONOMOUSSYSTEMS 2019-08-02 2019-08-02 2019-08-08 2019-08-08 2019-09-13T06:57:28 S0921-8890(17)30794-7 S0921889017307947 10.1016/j.robot.2019.06.006 S300 S300.1 FULL-TEXT 2020-01-13T11:13:04.440713Z 0 0 20191001 20191031 2019 2019-08-02T15:14:58.064687Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref vitae 0921-8890 09218890 true 120 120 C Volume 120 9 103227 103227 103227 201910 October 2019 2019-10-01 2019-10-31 2019 article fla © 2019 Published by Elsevier B.V. SKILLBASEDANYTIMEAGENTARCHITECTUREFOREUROPEANROBOTICSCHALLENGESINREALISTICENVIRONMENTSEUROCCHALLENGE2STAGEIIREALISTICLABS LIM G 1 Introduction 1.1 Motivation 1.2 Related work 2 European robotics challenges (EuRoC) 2.1 Challenge 2: shop floor logistics and manipulation 2.1.1 Benchmarking 2.1.2 Showcase: autonomous packaging 2.2 EuRoC platform 2.3 TIMAIRIS software architecture 3 Agent architecture 3.1 Skill-based anytime agent architecture 3.2 Resource management scheme 4 Solving benchmarking tasks 4.1 Task 1: production logistics 4.1.1 SLC detection 4.1.2 Manipulation and planning strategy 4.2 Task 2: product assembly 4.2.1 Fixture detection 4.2.2 Bolt, nut and washer detection 4.3 Manipulation and planning strategy 5 Solving showcase tasks 5.1 Manipulation of stacked non rigid objects 5.2 Manipulator for packaging 5.3 Showcase perception 5.4 Motion planing 5.5 Task planning 5.6 Safe human–robot collaboration 6 Challenge evaluation 6.1 Benchmark evaluation 6.2 Showcase evaluation 7 Conclusion Acknowledgments References PRATT 2013 10 12 G KITANO 1997 340 347 H PROCEEDINGSFIRSTINTERNATIONALCONFERENCEAUTONOMOUSAGENTS ROBOCUPROBOTWORLDCUPINITIATIVE SICILIANO 2014 1 7 B ISRROBOTIK201441STINTERNATIONALSYMPOSIUMROBOTICSPROCEEDINGS EUROCTHECHALLENGEINITIATIVEFOREUROPEANROBOTICS BISCHOFF 2010 15 16 R CULLY 2015 503 A HILDEBRANDT 2016 21 27 A AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2016INTERNATIONALCONFERENCE AFLEXIBLEROBOTICFRAMEWORKFORAUTONOMOUSMANUFACTURINGPROCESSESREPORTEUROPEANROBOTICSCHALLENGESTAGE1 ZADEH 2017 81 103 S BAKLOUTI 2017 9 14 E NILSSON 1999 205 226 K CHATZILYGEROUDIS 2018 236 250 K INSAURRALDE 2015 87 104 C DEAN 1988 49 54 T AAAIVOL88 ANALYSISTIMEDEPENDENTPLANNING GREFENSTETTE 1992 189 195 J MACHINELEARNINGPROCEEDINGS1992 APPROACHANYTIMELEARNING PEDROSA 2015 457 468 E PROGRESSINARTIFICIALINTELLIGENCE17THPORTUGUESECONFERENCEARTIFICIALINTELLIGENCEEPIA2015 ASKILLBASEDARCHITECTUREFORPICKPLACEMANIPULATIONTASKS AMARAL 2017 198 203 F AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2017IEEEINTERNATIONALCONFERENCE SKILLBASEDANYTIMEAGENTARCHITECTUREFORLOGISTICSMANIPULATIONTASKSEUROCCHALLENGE2STAGEIIREALISTICLABSBENCHMARKING LIM 2017 159 164 G AUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC2017IEEEINTERNATIONALCONFERENCE RICHROBUSTHUMANROBOTINTERACTIONGESTURERECOGNITIONFORASSEMBLYTASKS LIM 2017 15 27 G IBERIANROBOTICSCONFERENCE HUMANROBOTCOLLABORATIONSAFETYMANAGEMENTFORLOGISTICSMANIPULATIONTASKS MOKHTARI 2016 993 1005 V INTELLIGENTAUTONOMOUSSYSTEMS13 GATHERINGCONCEPTUALIZINGPLANBASEDROBOTACTIVITYEXPERIENCES LIM 2019 G BALOGH 2005 17 R THRUN 2006 661 692 S BUEHLER 2009 M DARPAURBANCHALLENGEAUTONOMOUSVEHICLESINCITYTRAFFICVOL56 LIM 2017 336 341 G 2017IEEEINTERNATIONALCONFERENCEMULTISENSORFUSIONINTEGRATIONFORINTELLIGENTSYSTEMSMFI NEURALREGULARIZATIONJOINTLYINVOLVINGNEURONSCONNECTIONSFORROBUSTIMAGECLASSIFICATION RUSSELL 2010 S ARTIFICIALINTELLIGENCEAMODERNAPPROACH LIM 2018 231 236 G 2018IEEEINTERNATIONALCONFERENCEAUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC MOBILEMANIPULATIONFORAUTONOMOUSPACKAGINGINREALISTICENVIRONMENTSEUROCCHALLENGE2STAGEIISHOWCASE LIM 2019 1 11 G LIM 2013 387 395 G INTELLIGENTAUTONOMOUSSYSTEMSVOL12 ONTOLOGYREPRESENTATIONINSTANTIATIONFORSEMANTICMAPBUILDINGBYAMOBILEROBOT LIM 2011 492 509 G RUSU 2008 927 941 R RUSU 2009 R SEMANTIC3DOBJECTMAPSFOREVERYDAYMANIPULATIONINHUMANLIVINGENVIRONMENTS OLIVEIRA 2016 614 626 M PEDROSA 2016 35 40 E 2016INTERNATIONALCONFERENCEAUTONOMOUSROBOTSYSTEMSCOMPETITIONSICARSC ASCANMATCHINGAPPROACHSLAMADYNAMICLIKELIHOODFIELD COHENOR 1995 453 461 D RUSU 2011 1 4 R ROBOTICSAUTOMATIONICRA2011IEEEINTERNATIONALCONFERENCE 3DPOINTCLOUDLIBRARYPCL LOWE 1999 1150 1157 D COMPUTERVISION1999PROCEEDINGSSEVENTHIEEEINTERNATIONALCONFERENCEVOL2 OBJECTRECOGNITIONLOCALSCALEINVARIANTFEATURES TUDICO 2017 498 509 A PORTUGUESECONFERENCEARTIFICIALINTELLIGENCE IMPROVINGBENCHMARKINGMOTIONPLANNINGFORAMOBILEMANIPULATOROPERATINGINUNSTRUCTUREDENVIRONMENTS LIMX2019X103227 LIMX2019X103227XG 2021-08-08T00:00:00.000Z 2021-08-08T00:00:00.000Z © 2019 Published by Elsevier B.V. 2019-08-10T21:40:24.934Z S0921889017307947 National Funds China National Funds for Distinguished Young Scientists FCT - Foundation for Science and Technology UID/CEC/00127/2013 This work was supported by the EuRoC Project under Grant no. 608849 and by National Funds through the FCT - Foundation for Science and Technology , in the context of the project UID/CEC/00127/2013 . Dr. Gi Hyun Lim received the B.S. degree in metallurgical engineering and the M.S. and Ph.D. degrees in electronics and computer engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Marie Curie individual fellow in the School of Computer Science at University of Manchester, UK. His research interests lie in the area of artificial intelligence and machine learning for autonomous robots, including perception, semantics, cognition and spatiotemporal representations on neuromorphic architectures. Eurico Pedrosa is a Post-Doc Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his Informatics Engineering degree from University of Aveiro in 2010 and a Computer Science Ph.D. degree from Aveiro University in 2018. His research interest are focused on intelligent robotics, robotic navigation including localization and mapping (SLAM), space representation using volumetric grids and most recently the application of radar sensors in indoor robotics. Filipe Amaral is a Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his MSc degree in Computer and Telematics Engineering from University of Aveiro in 2014. His current research interests are in the area of autonomous mobile robotics. Prof. Dr. Artur Pereira was born in Vila Nova de Famalicão, Portugal, in April 1960. He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 2003. He is currently an Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Instituto de Engenharia Electrónica e Informática de Aveiro. The main focus of his research is robotics at the architectural and software levels, with emphasis on simulation, navigation, localization, mapping, and machine learning. Nuno Lau is Assistant Professor at Aveiro University, Portugal and Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), where he leads the Intelligent Robotics and Systems group (IRIS). He got is Electrical Engineering Degree from Oporto University in 1993, a DEA degree in Biomedical Engineering from Claude Bernard University, France, in 1994 and the Ph.D. from Aveiro University in 2003. His research interests are focused on Intelligent Robotics, Artificial Intelligence, Multi-Agent Systems and Simulation. Nuno Lau participated in more than 15 international and national research projects, having the tasks of general or local coordinator in about half of them. Nuno Lau won more than 50 scientific awards in robotic competitions, conferences (best papers) and education. He has lectured courses at Phd and MSc levels on Intelligent Robotics, Distributed Artificial Intelligence, Computer Architecture, Programming, etc. Nuno Lau is the author of more than 150 publications in international conferences and journals. He was President of the Portuguese Robotics Society from 2015 to 2017, and is currently the Vice-President of this Society. Prof. Dr. José Luís Azevedo is currently Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Institute of Electronics and Informatics Engineering of Aveiro (IEETA). He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 1998. His current research interests are in the area of cooperative autonomous mobile robotics. Prof. Dr. Bernardo Cunha was born in 1959 in Porto, Portugal. He earned his doctoral degree in electrical engineering at the University of Aveiro, Portugal, in 1999. He is a full time teacher at Universidade de Aveiro in the computer architecture area and an investigator at the Instituto de Engenharia Electrónica e Informática de Aveiro. Current research interests are centered in the area of cooperative autonomous mobile robotics. Simone Badini is a Mechanical Designer in the Research and Development department of IMA Spa since 2013. IMA Spa is a world leader company in the design and manufacture of automatic machines for the processing and packaging of pharmaceuticals, cosmetics, food, tea and coffee and tobacco. He got is M.Sc. degree in Mechanical Engineering from University of Bologna, Italy in 2012. He is currently project manager for the integration of cobot and autonomous mobile robot in the production lines for the IMA group. item S0921-8890(17)30794-7 S0921889017307947 10.1016/j.robot.2019.06.006 271599 2020-01-13T11:13:04.440713Z 2019-10-01 2019-10-31 true 3280747 MAIN 14 53587 849 656 IMAGE-WEB-PDF 1 gr11 88037 82 219 gr6 76125 134 219 gr10 99763 151 219 gr1 84425 55 219 gr12 78236 62 219 pic3 95379 163 140 gr13 84239 60 219 gr17 92453 164 193 gr2 91612 129 219 gr19 18466 78 219 fx1002 5080 40 219 gr9 80285 162 219 pic5 90620 164 140 pic8 92385 164 140 gr4 86357 129 219 gr7 83216 163 155 gr8 78279 144 219 pic4 91410 163 140 gr5 84112 164 138 gr3 101514 163 219 gr18 77492 164 204 pic1 90433 163 140 gr16 82949 47 219 fx1001 5682 68 219 gr14 84362 115 219 gr20 34694 98 219 pic2 92082 164 140 pic6 88104 163 140 gr21 22647 162 219 gr15 93327 108 219 pic7 96532 163 140 gr11 115147 142 378 gr6 102249 210 342 gr10 127663 261 378 gr1 119793 128 506 gr12 100014 106 376 pic3 101319 132 113 gr13 105403 103 376 gr17 149053 321 378 gr2 129409 223 378 gr19 42698 110 309 fx1002 27829 150 816 gr9 109290 277 375 pic5 98434 132 113 pic8 101766 132 113 gr4 113348 190 323 gr7 120919 318 302 gr8 117887 327 496 pic4 97712 132 113 gr5 117350 225 189 gr3 125523 242 325 gr18 101664 243 302 pic1 99455 132 113 gr16 120354 112 525 fx1001 36645 252 816 gr14 114768 194 370 gr20 52488 146 325 pic2 99552 131 112 pic6 98208 132 113 gr21 59417 279 378 gr15 119748 187 378 pic7 107374 132 113 gr11 364806 629 1674 gr6 175979 929 1514 gr10 495261 1154 1674 gr1 351578 568 2243 gr12 185445 470 1668 pic3 169551 583 500 gr13 214287 455 1666 gr17 622945 1422 1674 gr2 342433 988 1675 gr19 90504 487 1369 fx1002 103050 398 2169 gr9 245828 1230 1663 pic5 158735 584 500 pic8 168594 584 500 gr4 258891 842 1433 gr7 296978 1410 1339 gr8 281811 1449 2197 pic4 140145 583 500 gr5 232097 999 840 gr3 443350 1074 1440 gr18 160189 1077 1340 pic1 155377 583 500 gr16 409379 497 2326 fx1001 144071 670 2169 gr14 245424 859 1639 gr20 178264 647 1440 pic2 165958 583 499 pic6 138608 583 500 gr21 195782 1237 1675 gr15 306022 828 1676 pic7 174577 583 500 si27 26913 si33 1898 si34 1618 si31 7355 si3 2005 si14 1133 si6 2114 si38 3009 si21 1832 si37 4191 si18 3593 si36 1780 si16 8122 si2 1418 si8 1661 si1 1607 si9 4591 si26 2321 si19 1613 si32 6363 am 19107071 ROBOT 3227 103227 S0921-8890(17)30794-7 10.1016/j.robot.2019.06.006 Fig. 1 Human operators in a packaging industry. Fig. 2 KUKA KMR mobile manipulator. Fig. 3 Initial setup of the table for task 2. Fig. 4 Showcase environment in the gazebo simulator. Fig. 5 EuRoC software framework for C2 [23]. Fig. 6 Use case diagram of skill-based anytime agent architecture (SAAA). Fig. 7 A higher level overview of SAAA. Fig. 8 Sequence diagram of SAAA. Fig. 9 A skill-based architecture for safe human–robot collaboration. Fig. 10 Example of detection of two SLCs on a shelf. The top image is what is perceived by our detection system (plus depth). The bottom image contains a superimposed 3D model of the SLC for each detection, including the center of the SLC (red dot) and a possible pick point (green dot) . (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 11 Example of nut and bolt detection. The top image contains the detection of a nut and the bottom image contains the detection of a bolt. The same detection algorithm is used for nuts and bolts. Fig. 12 Washer detection in the fixture. The top image is what is acquired by the camera. The bottom image is the result of the segmentation by intensity. Fig. 13 Snapshots of bent blank piles. Fig. 14 Gripper and blank magazine in simulator. Fig. 15 Grpeer and blank magazine in the realistic environment. Fig. 16 Two pallets and blank piles and their detection results. The red lines in center and right images indicate the pose of pallets, while the green circles indicate the positions of blank piles. Fig. 17 A sequential result of the pose estimation of a blank pile. Fig. 18 Drawing pin filter to detect outlines. Fig. 19 A task plan for 9 blank piles on two pallets. Fig. 20 Human tracking and HRI during the EuRoC evaluation. Fig. 21 Interaction tree for the showcase task. Table 1 Benckmark task 1. Team Metric 1 Metric 2 Bonus Time AutoMAP 5 5 6.22 4:56 MTC-LU-UoB-Airbus 5 4 0 18:13 RSAII 5 5 3.32 9:14 NimbRo Logistics 5 5 3.45 8:54 TIMAIRIS 5 5 10 3:04 Table 2 Benckmark task 2. Team Metric 1 Metric 2 Bonus Time AutoMAP 5 5 4.16 15.01 MTC-LU-UoB-Airbus 0 1 0 8:54 RSAII 5 5 3.78 16:31 NimbRo Logistics 4 4 0 41:27 TIMAIRIS 5 5 10 6:15 Table 3 Quantifiable evaluation. Objectives Metrics Targets Events Percent (%) O1 O1M1 6+5+3 6+5+3 100 O1 O1M2 6 6 100 O1 O1M3 5 5 100 O1 O1M4 2 2 100 O2 O2M1 6 6 100 O2 O2M2 6 6 100 O2 O2M3 6 6 100 O2 O2M4 2 2 100 O3 O3M1 3 3 100 O3 O3M2 12 12 100 O4 O4M1 4 4 100 O4 O4M2 2 2 100 Skill-based anytime agent architecture for European Robotics Challenges in realistic environments: EuRoC Challenge 2, Stage II — realistic labs Gi Hyun Lim a 1 Eurico Pedrosa a 1 Filipe Amaral a Artur Pereira a ⁎ Nuno Lau a ⁎ José Luís Azevedo a Bernardo Cunha a Simone Badini b a IEETA, Universidade de Aveiro, Aveiro, Portugal IEETA, Universidade de Aveiro Aveiro Portugal IEETA,Universidade de Aveiro, Aveiro, Portugal b IMA, Via Emilia 428-442, 40064 Ozzano dell’Emilia, Italy IMA Via Emilia 428-442 Ozzano dell’Emilia 40064 Italy IMA,Via Emilia 428-442, 40064 Ozzano dellEmilia, Italy ⁎ Corresponding authors. 1 These authors contributed equally to this work. As demands on pragmatic solutions of robotics technology increase in the manufacturing industry, deep affinities between research experts and industry users are required. The European Robotics Challenges (EuRoC) research project has proposed a scientific competition and matched up research labs with industrial end users to establish challenger teams to develop and test solutions that will be applied in the real context of the industrial end-users. The paper reports the result of TIMAIRIS who is one of 6 challenger teams to advance to the final stage out of 103 teams and technical details used in the Challenge 2 - Shop Floor Logistics and Manipulation. To address the requirements and achieve the objectives of the challenge, a skill-based anytime agent architecture has been developed and extended to make the team focus on the challenging research that addresses real issues in the user environments. Finally, shop floor logistics and manipulation scenarios have been developed and demonstrated in a realistic environment for autonomous packaging. Keywords Skill-based Anytime agent architecture Mobile manipulation Autonomous packaging European robotics challenges (EuRoC) 1 Introduction 1.1 Motivation Several robotics challenges and competitions have been launched for the exchange of research results and for comparative evaluations including manipulation, such as the DARPA Robotics Challenge [1], the RoboCup leagues [2] and the European Robotics challenges (EuRoC) project [3], since neither resources such as robots’ platforms, source codes and datasets are available to the public nor are simulators sufficiently mature to present real environments. To obtain a good result in a robotics challenge or competition, challengers need to develop a robotic system that completes given challenge tasks within a limited time. EuRoC is a research project where a robotic competition is conducted with the aim to develop and present solutions to the European manufacturing industry [3]. The EuRoC consortium has launched three industry-relevant challenges: C1 - Reconfigurable Interactive Manufacturing Cell, C2 - Shop Floor Logistics and Manipulation, and C3 - Plant Servicing and Inspection. In particular, the Challenge 2 (C2) addresses the SRA2009 [4] scenarios: Logistics and Robotic Co-Workers. Mobile manipulators are provided by the EuRoC host as a suggested solution to utilize as logistic carriers and dexterous manipulators. Each challenge team should consist of both a research group and an industry partner to show the use case in a realistic environment. Our work is being developed within the Challenge 2 context. The Challenge 2 host especially set the working time regulations to share the resources among all challenger teams and to make sure that they have the same amount of effective lab time in the realistic environment where the challengers access robotics platforms and benchmark infrastructures. As the use case scenario of TIMAIRIS, which is a collaborative challenge team between the Intelligent Robotics and Intelligent Systems (IRIS) research group from the University of Aveiro (UAVR) in Portugal and IMA S.p.A industry from Italy, an autonomous blank feeding task is investigated because it is not easily solved by a robot. Proof of such assumption is that the industrial state-of-the-art solution for this problem is to use human operators to perform it, as shown in Fig. 1. This is a tedious, repetitive and tiring task and human operators may occasionally refrain from collecting blank piles from more distance pallets. This paper presents a skill-based anytime agent architecture (SAAA) and the result of TIMAIRIS team in the second stage of C2. TIMAIRIS is one of 6 challenger teams advancing to the final stage out of 103 teams in the simulation contest, The second stage consists of three phases to be performed in a real environment: Benchmarking, Freestyle and Showcase. In the realistic labs of the EuRoC challenge tasks, the main constraint is time, not only the running time (online efficiency) in an evaluation matrix but also development time (offline efficiency) to complete the tasks in a limited time. Each challenger team is allowed to take a limited fixed time to access an experimental environment in which EuRoC hosts offer support and the mandatory robot platform is available. Especially, for the three phases of the stage II, each team has evenly-distributed eleven weeks (54 working days) to exclusively access the environment. Challenger teams are required to find efficient and robust skills and complete evaluations in the given time. SAAA provides opportunities to increase efficiency not only in autonomous execution for shop floor logistics and manipulation but also in its use of development during the challenge competition. 1.2 Related work To meet the requirements and constraints on a robotic challenge, challenger teams need an adaptive, flexible, robust and efficient robotics architecture [5,6]. Several reactive architectures are proposed to find a solution that makes a robot complete tasks on-time. By managing mission time, Zadeh et al. [7] developed an autonomous reactive architecture of unmanned vehicles in realistic ocean environment. Synchronization between high level mission and low level path planning is configured to control mission time and to guarantee termination of the mission with the best sequence of tasks fitted to available time. Baklouti et al. [8] proposed a reactive control architecture for wheelchair robot navigation without human intervention and prior knowledge of the world. Nilsson and Johansson [9] introduced a layered Open Robot Control (ORC) architecture to meet industrial demands such as computing efficiency and simple factory-floor operation with integration of online and offline robot programming. Offline programming typically done by robot programmers requires abstract modeling, whereas online programming typically done by production engineers or robot operators utilizes physical robots in real environments. Recently several trial and error methods are proposed to adapt for use by damaged robots [5,10]. Those focus on the online time challenge by reducing the number of trials to recover from damage to complete tasks. Cully et al. [5] introduced an intelligent trial and error algorithm which enables a robot to discover a compensatory behavior from damage without requiring pre-programmed contingency plans. The algorithm conducts experiments based on high-performing policies for the intact robot work on the damaged robot. While traditional reinforcement learning (RC) methods for robots need to reset their learning environments and robots to an initial state, Chatzilygeroudis et al. [10] proposed a reset-free trial-and-error learning for robot damage recovery. Insaurralde and Petillot [11] proposed a capability-oriented robot architecture that enables multiple unmanned vehicles to collaborate to autonomously carry out underwater intervention missions in a fault-tolerant manner. On the other hand, anytime approaches use iterative improvement techniques in problem solving, planning, scheduling [12] and learning [13]. Those address the online time challenge by returning results at any time. To adapt to realistic industrial environments, this paper proposes a skill-based anytime agent architecture (SAAA) by integrating previous work [14–17]. The architecture consists of a solver, modular skills and task representations. The separation of task s that organize orders of objects to assemble into graphs from an agent processing algorithm [18,19] allows a robot to start at any task state. It is not necessary to reset the robot and its environment to run from an initial state. During development, a robotics challenger team needs to repeat a subtask or to execute a skill from a specific state to refine the subtask or the skill until the time of evaluation. On the other hand a human co-worker will continue or restart a task after a task failure or a task completion at the runtime. To the best of our knowledge, robotic system architectures and anytime algorithms consider only online efficiency while the proposed architecture also takes offline efficiency into account. 2 European robotics challenges (EuRoC) Many robotic challenges have been so far launched to find competitive solutions for their applications [20]. The DARPA Grand Challenge [21] and its successive events, DARPA Urban Challenge [22] and DARPA Robotics Challenge [1], have drawn worldwide attention as robotic competitions for autonomous ground vehicles on an off-road course, for autonomous operation in a urban environment and for autonomous emergency maintenance robots, respectively. The main objective of the DARPA challenges is bridging the gap between fundamental discoveries of academia and military use. Since 1997, RoboCup [2] has been held annually to foster AI and intelligent robotics research. Initially the target of competition was a world cup with real robots, nowadays there are also other leagues such as RoboCup Industrial, RoboCup Rescue and RoboCup@Home. Especially, the RoboCup Industrial league defines two tasks: logistics and manipulation; but robots in this league have size constraints. To match the Strategic Research Agenda (SRA) for robotics [4], the EuRoC project was launched in 2014. It aims at exploiting synergies among all the actors of robotics and manufacturing to accelerate the transference of state-of-the-art technologies from academia to industry. After qualification, the members of each team are required to organize from both communities. The main motivation of the European Robotics Challenges (EuRoC) is to make use of robotics products and services by strengthening collaboration between the industrial and the research community [3]. The EuRoC is a research project based on robotics competitions that drives innovation in robotics and manufacturing through a number of application experiments. The Challenge Advisory Board of EuRoC is in charge of the evaluation of the competition project empowered on robotic platforms and benchmark infrastructures to get rid of contestants’ burden on platform-related low-level problems and maintenance. As a realization in this context, the EuRoC initiative has launched and run three challenges in parallel with different motivations and objectives. The Shop Floor Logistics and Manipulation or Challenge 2 (C2) covers application scenarios about robotic co-workers and logistics robots in industrial, professional service and domestic service sectors, which is matched to the Strategic Research Agenda (SRA) for robotics in Europe [4]. In the co-worker scenario, a robot becomes an assistant or collaborator that works literally hand-in-hand with its human counterparts in unstructured environments. Here, the robot needs to communicate with a human to take an order, to ask confirmation, and to reply to a question. Each EuRoC challenge is structured in three successive stages, over the period of 4 years: Stage I QUALIFYING: Simulation Contest, Stage II REALISTIC LABS: Benchmarking, Freestyle and Showcase and Stage III FIELD TESTS: Pilot Experiments. 2.1 Challenge 2: shop floor logistics and manipulation The Shop Floor Logistics and Manipulation challenge or Challenge 2 of EuRoC project consists of a assembly of tasks designed to be solved by a mobile robotic platform operating in industrial environment. This challenge is divided in three main stages where three different scenarios were designed and considered. Stages I and II were already completed and the Stage III will be done at the end of the EuRoC project. Briefly, the first stage was based on a stationary platform and the achieved solutions were integrated in a simulation environment. The Stage II consists in a scenario where a real Light-Weight-Robot (LBR iiwa) equipped with a two-finger jaw-gripper mounted on a mobile base (KUKA omniRob) was controlled through an intranet-based framework (similar to the one that was used in Stage I), as shown in Fig. 2. The final stage, Stage III, will be an application of all the achieved solutions in a real scenario of industry with real work pieces provided by an end-user, showing the real capabilities of this innovative solution and the contribution to the manufacturing dynamics improvement. 2.1.1 Benchmarking For the Stage II in the Challenge 2, the EuRoC host has provided a benchmark environment at DLR in a realistic factory set-up with elements from real end users. It consists of two tasks in simplified manufacturing environment: logistics and assembly. The first task of the Benchmarking phase addresses production logistics. In this task, the mobile manipulator has to bring 5 Small Load Carriers (SLCs) which are distributed in the room to a fixed target area on a table. One SLC is placed on the goal table out of the target area, two are placed on another table and two are located in a shelf. The room has enough space for the robot to move between the tables and the shelf. The start position is in the middle of the room. The setup of the room will be static over the development and evaluation. The SLCs are filled with a various number of nuts and washers from the Benchmarking task 2. For each correctly delivered SLC (its footprint has to be inside the target area) one point is awarded (max 5 points). For each SLC, if all parts in it remain within it during the transport to the goal area, another point is awarded (max 5 points). If the task is completed successfully extra points could be awarded for the execution time. The amount of points is inversely related to the best teams time (max 10 points). The second task of Benchmarking is about mobile manipulation for basic pre-assembly of products. In this task, the robot has to assemble five bolts. The layout of the room and the start position of the robot are the same as in the previous benchmark task. The assembly table is identified as “Pickup table”. On that table, there are 5 nuts, 5 bolts and 5 washers within a fixed area. Next to the parts are two fixtures attached to the table. One of the fixtures holds the washers and the other has a gap with the bolt’s head shape to allow the nut to be screwed. The parts are arranged to minimize occlusions (see Fig. 3). The bolts are upright, the nuts are flat and the washers are placed in the fixture to give a good orientation for grasping. The translation between the different parts may vary. The nuts and bolts are touching a virtual line parallel to the fixtures 10 cm and 20 cm behind, respectively. For each nut correctly screwed, one point is awarded (max 5 points). Applying a washer onto the bolt before screwing a nut gives another point (max 5 points). As in the previous task, if the task is completed successfully extra points are awarded for the execution time. Again, the amount of points is inversely related to the best teams time (max 10 points). 2.1.2 Showcase: autonomous packaging As the third and final challenge of the Stage II, the showcase runs in an simplified environment that, although not located in an industrial plant, includes the most important elements of the real environment where the task will be executed as a final product, as shown in Fig. 4. It uses real pallets with real piles of blanks and a prototype of the blank feeding mechanism of a packaging machine that includes all relevant features for the task to be performed. In this task the following issues are going to be covered: the platform will be able to recognize empty pallets, plan and replan its actions; an initial version of the multimodal interface will be used, integrating the possibility of gesture commands in the interaction with the platform (gestures will be used mostly for safe navigation); the robot will be able to navigate in an environment that includes a few humans and interact with them; the manipulation of the selected type of blanks will be demonstrated. To solve the blank feeding task using a mobile robot, several technical issues are identified. • Manipulation of stacked objects: The blank pile is not a unique rigid solid object. Not only it can bend under its own weight, but blanks can also easily break free from the pile due to the fact that a considerable amount of low friction blanks are stacked on top of each other. • Single arm manipulation: The manipulation of a blank pile is performed using a single arm with a gripper. In order not to miss any blanks out of a stack on a pallet, picking up the blank pile requires several manipulation steps and a specialized gripper. • Smooth placing of the blank pile: Blank piles have to be placed in the feeding mechanism avoiding hard collisions with the blanks that are already there and with the four blank guiding rods that prevent blanks from slipping out of the inclined magazine surface. • Shared workspace with humans: The robot has to be able to navigate in an environment that is shared with humans. This introduces safety concerns but, at the same time, it also rises the opportunity to take advantage of the human–robot proximity to explore human–robot interactions (HRI) to control the robot [16]. • Robust detection of blanks: Blanks are provided in untied piles, piled up on pallets close to each other, which can cause erroneous detection. • Handling global localization errors: The feeding of the blank magazine is an example of a manipulation action that requires a high level of precision for a proper feeding while preventing collisions between the blank magazine and the manipulator. 2.2 EuRoC platform As an objective of the EuRoC, robotics platforms and benchmark infrastructures have been developed to make challengers focus on the challenging research without efforts on platform-related low-level problems and maintenance. Especially for C2, two mobile manipulators are suggested to address logistics and robotic co-workers scenarios, as shown in Fig. 2. EuRoC hosts and robot manufacturers also provide programming and simulation frameworks, and open interfaces on lowest levels. Available EuRoC SW tools 2 2 are also provided for use in the EuRoC. Fig. 5 shows the EuRoC software framework for C2. Challengers are required to develop high-level software components with the similar functionalities but different scenarios to test and validate in meaningful contexts typically for shop floor logistics and manipulation in C2. 2.3 TIMAIRIS software architecture TIMAIRIS software is composed of three main components: a skill-based agent architecture (SAAA) [15], a human tracker and safety manager subsystem [17] and a realistic simulator [14]. TIMAIRIS software uses a skill-based anytime agent architecture [15] that has been evolving from the one used for Stage I simulation tasks [14]. The separation of task-dependent representations and a generic agent processing algorithm allow the robot to start at any task state. Human collaborators or robot operators need to repeat a task after recovering from failure or to test the skill from a given starting point. Having the same architecture performing successfully for such a different set of tasks demonstrates that this team solution is remarkably flexible and well adapted for EuRoC C2 platform and its capabilities. Ensuring safety, industrial robots need to share an environment with humans and to work hand in hand. To realize safe human–robot collaboration, a human tracker and safety manager subsystem has been integrated into the SAAA [17]. The human tracker keeps on tracking humans in a workspace. The safety manager infers whether it is in a safe state or not based on system states and human tracking information. Then, human–robot interaction module takes an order from the operator to resume or stop the paused task using gestures [16,24]. A realistic simulator has been developed for the Challenge 2 (see Fig. 4), using, as starting point, the Stage II simulator provided by the EuRoC host. The simulator allows the execution of the complete Challenge 2 tasks and was an essential tool in the development of TIMAIRIS’ Challenge 2 software. 3 Agent architecture 3.1 Skill-based anytime agent architecture To address the requirements and achieve the objectives of C2, a skill-based anytime agent architecture (SAAA) has been developed to solve the logistics and manipulation tasks [15]. This paper extends the previous architecture in the way that a plan manages more than one object and a robot explores workspace that cannot be covered by its cameras without moving the platform. In particular, the new architecture has been used to solve the Production Logistics and Product Assembly tasks previously described. These tasks have four objectives: perception, manipulation, planning and human robot interaction. Fig. 6 shows the use case diagram of SAAA at development time and/or runtime for human–robot collaboration. A robot and two types of humans are involved in the use case: a developer and a co-worker in a scenario for autonomous packaging. A robot developer sets a robot state to refine a skill which is executed at the state. It is possible that the developer repeat to execute the skill to improve performance and reduce runtime without reseting the robot and its environment. For example, a placing skill will be executed after picking and transport skills in pick-and-place tasks. To refine the placing skill, it is not efficient to run whole pick-and-place task by reseting to an initial state. A human co-worker may provide a command to start and stop a task or to change a sequence of objects to be delivered. A mobile manipulator (see Fig. 2) sequentially executes skills to complete the Production Logistics and Product Assembly task. In a higher level view of the skill-based framework, as shown in Fig. 7, the functional components are represented by boxes. A Perception module collects sensory data and processes them to extract information used by high-level modules. A Skill is the capacity of doing a particular task, such as, picking and placing an object or moving the end-effector of the manipulator to a desired pose. Perception modules and Skills collect and transmit sensory-motor data via Sensor interface and Effector interface, respectively. The Action Planner provides a plan for the target object. A plan contains a sequence of skills and their corresponding objects. The Solver is responsible for taking decisions on how to solve the current task based on the current sensory data and available Skills. To solve these tasks, an agent is required to perceive the environment through sensors and act upon that environment using actuators [25]. A skill-based agent architecture is implemented to develop a generic solution capable of handling shop floor logistics and assembly tasks by analyzing the properties of the environment. Henceforth, the proposed method has been utilized for several packing scenarios by increasing the realization during the EuRoC challenges [16,17,24,26,27]. Fig. 8 shows the sequence diagram of SAAA for autonomous packaging. To complete a task, a robot developer can set the state of a robot to start a new task from the initial state or to repeat the previous skill to continue the previously uncompleted task. When the robot start a task, the Solver [15] reads the Order graph [14] of the task and also tries to get a command from Interaction which is a module that tracks humans [17] and gets a command from a co-worker via gesture recognition [16]. The Solver requests a plan from Planner with the states of the robot and its environment, and executes a sequence of skills by following the plan. It continues until completing the task or terminated by the robot developer. when the robot stops, the developer repeats to set the state and to start the robot. The proposed algorithm is summarized in 1. In each task, the set O of objects to be manipulated, including their properties and place zones, is known in advance. The algorithm starts by building an Order Graph which represents the order of objects. The order is restricted by a direct acyclic graph (DAG), which represents a dependency graph between objects in terms of order of manipulation [14]. Leafs represent objects that need to be handled first. The dummy object λ is added to represent the graph root. To ensure that all object are eventually detected [28], a set of search poses S is estimated by the procedure buildSearchSpace with the insurances that all combined poses cover the whole working space. The set S is encoded as a circular list so that the search for poses never ends. Additionally, to improve the search [29], the vision system on the pan tilt unit is used to detect objects in the environment and the obtained poses are put in the head of S in the order defined by G . It may not detect any object, but, if it does, the system can gain in execution times due to good initial search poses. The algorithm then executes nested loops that, making the robot move around the search poses, finishing when only the root node ( λ ) remains in the Order Graph. Each search pose is explored to see if a leaf object is there. A plan is a tuple [19] consisting of a sequence of skills and their corresponding objects that allows to properly pick objects, move the end-effector of the manipulator and place the objects in the target position [30]. Those skills depend on a priori calculation of the pick and place pose. The specific plan depends on the task being solved. The Action Planner can estimate the state of task based on the input object. The plan could be related with several objects. For instance, in the assembly process several parts are added in sequence until the final assembly is produced. If the execution of the skill succeeds, the leaf corresponding to the processed object is removed from the graph. Because Order Graph and plan are separated from the agent processing algorithm, only three procedures are task dependent, buildOrderGraph, buildSearchSpace and makePlan. This formulation has two significant advantages. First, the agent can start execution at any task state, even if the agent meets a failure while execution, as it can continue the task after recovering from the failure. Second, to solve a task the developer only has to focus on the creation of a plan supported by a set of available skills. 3.2 Resource management scheme Fig. 9 shows an extension of the skill-based architecture for safe human–robot collaboration. The Human tracker keeps on tracking humans in the workspace by using laser scanners. The Human–Robot Interaction (HRI) module communicates with a human by recognizing gestures [24] and by providing information via multi-modal interfaces [16]. The Safety manager infers whether it is in a safe state or not based on system states and human tracking information. When the Safety manager decides to pause an executing task, it requests Solver to gaze at the nearest human operator, and to trigger HRI to interact with him. Then, HRI takes an order from the operator to resume or stop the paused task using gestures. To continuously monitor humans, the Human tracker and Safety manager need to be continuously active, while all other modules including skills and perceptions modules just run on request. That should cause conflicts over resources. For example, when the robot wants to recognize the pose of a blank magazine to feed a blank pile, a perception module tries to rotate the pan–tilt camera system to the blank magazine on the table. If, at the moment, a human operator approaches, the Safety manager also tries to rotate the same camera system to gaze the operator. Based on the skill-based agent architecture [14,15], a resource management scheme is added to the system architecture, as shown in Algorithm 2. When a robot starts, each module which needs to run continuously is launched and becomes a daemon. The Solver builds a resource map which lists all necessary resources for each daemon module. At every spin, which means a wake-up for all subscriptions, services, timers and so on in ROS (Robot Operating System), 3 3 the daemon checks the availability of its resources. If available, it runs normal procedures and release all resources at the end of the procedures. 4 Solving benchmarking tasks The objective of our team is to solve the tasks proposed for the EuRoC Benchmarking phase with high precision, accuracy and robustness, as fast as possible. Time is a key factor in solving the tasks at hand: not only it is an evaluation metric, but, for industrial applications, it a matter of productivity by reducing the time for development. In this section, we present our approach to solve two tasks of the Benchmarking phase -Production Logistics and Product Assembly- using the presented architecture. 4.1 Task 1: production logistics The goal of this task is to find all SLCs present in the room such as tables, shelves and workbenchs and place them in the designated target area in a workbench. Their approximate locations are known, on top of two tables and in a shelf, but their exact positions on those locations are unknown. To solve this task we start by designating several search poses that guarantees that eventually all SLCs are detected with the stereo camera mounted in the pan and tilt unit. Once an SLC is in sight, its pose should be detected, in order to pick it up. Then the SLC is placed somewhere else, in an intermediate or target area. This last step has several variations that affect the overall execution time because of the locations of SLCs. 4.1.1 SLC detection The detection of SLCs has two stages: first, color segmentation is used to define candidate regions in the image, that may contain at least an SLC; second, a 3D point cloud for each region is generated and matched against 3D template of an SLC to find its position and orientation, i.e. its pose. Color segmentation, in the HSV color space, is initially used to detect an SLC due to the SLC’s color homogeneity and good contrast with the environment. The output of the color segmentation is then used to generates blobs that represent SLC candidates. The resulting blobs must have a minimum size to be valid. The overlapping of SLCs in the image may generate a single blob for multiple SLCs, but that is not a concern as the disambiguation is deferred to the next stage. Once color segmentation is completed, the resulting blobs are used as masks to generate a point cloud for each candidate. While 3D point cloud generation and matching processes are heavy, the color segmentation is not. Thus, the SLC detection is time-optimized by reducing many candidates from the color segmentation. Let P be a point cloud and { p i } the set of points of P . The calculation of SLC’s pose relies on the processing of the point cloud P . To reduce the computational complexity of processing a point cloud with a high number of points, P is decimated by applying a voxelization filter. Then, to remove possible outliers in the point cloud a statistical outlier removal [31] is used. To address the possibility of SLCs overlap, the point cloud is divided into clusters using a euclidean cluster extractor [32], the cluster with the highest number of points being assigned to P and the remaining points being discarded – thanks to our agent architecture, discarded SLCs will be detected in the next cycle. The position of the SLC is approximately calculated from the centroid c of P , given by c = 1 n ∑ i = 1 n p i , while its orientation is initially provided by the Principal Component Analysis (PCA) of the projection of P in the X O Y plane [33]. However, this approach is not enough, as the view of the SLC may provide a partial point cloud that skews the centroid from the real center, and the orientation of the PCA has an ambiguity of π radians. The final pose of the SLC is calculated by matching P against a 3D template of the SLC that has its centroid in the origin and its bearing defined by the X axis. Before matching the point cloud, P is transformed to its origin, i.e. the inverse pose of P is applied to itself, then, the transformation that results from the matching between the template and P is the correction of the initial pose calculation. Note that because the orientation given by PCA is ambiguous, we do the matching with the initial orientation and with another rotated by π radians. To correct the pose, we use the matching transformation that provided the best matching. The algorithm used for matching is the adaptation of the scan matching algorithm proposed by Pedrosa et al. [34] to three dimensions. An example of SLC detection is shown in Fig. 10. 4.1.2 Manipulation and planning strategy To solve this task three manipulation skills were used. The pick_object and place_object are based on the skills trained in a simulation environment [14] with some minors adaptations. The pick_object_shelf skill is also derived from the pick_object skill but taking in account the space between the shelves of the shelf. So in this skill instead of approaching the object using a vertical movement it is approached at an angle of 45 degrees. This allows the arm to reach the objects on the shelf without hitting the upper level of the self. Our initial approach was to pick an SLC, hold it and deliver it to the goal area. With this approach the robot has to move across the room 4 times, 2 for the SLCs on the pickup table and 2 for the SLCs in the shelf. It takes around 15 min to complete the job, being most of this time spent on navigation between the goal table and the pickup table and shelf. In order to optimize this process a second approach was developed. Because a considerable amount of time was being spent on navigation, we manage to transport the SLCs on top of the robot platform. This way, navigation was reduced to one trip to each side: first, the two SLCs in the table are picked up and placed in the top of the robot; then, the robot moves to the shelf, pick up another SLC and also put it in its top surface; finally, the fourth SLC is picked up and held in the gripper, while the robot navigates to the target table. On the target table, the four SLCs are put in the target area, after which the fifth SLC, the one in the target table, is detected, picked and placed. With this improvement the time dropped to around 10 min. After this pick and place strategy was implemented, some improvements were accomplished, by parallelizing some steps. For example, after picking an SLC, while placing it on the top of the robot, movement to the next observation and picking position can be performed. Task time was now reduced to around 8 min. A significant amount of time was still being spent in picking the SLCs from the top of the robot and placing them on the goal area (3 SLCs from the top of the robot plus the fourth that was transported on the gripper). So our final solution was to rearrange the SLCs on top of the robot so that the manipulator can pick two at the same time. This way one of the picking from the robot’s body was eliminated. Also, by placing the fourth SLC on top of the robot aligned with the third and by picking both at once, the number of places in the target area was reduced to 3 – 2 places holding two SLCs and one holding only one SLC. The 4th SLC is placed on top of the robot while it is navigating to the target table. Also, during navigation the arm could pick the 3rd and 4th SLCs so that when the robot reaches the target table the SLCs are already grasped. These improvements dropped the time to 5 min. Furthermore, speed increasing was previously prepared through parameters in a configuration file. During the evaluation, 6 out of 7 attempts were successful and with decreasing times, which lead to the final task time of 3 min and 4 s just by increasing the speed of the movements. 4 4 4.2 Task 2: product assembly The goal of this task is to pre-assemble a set of bolts, nuts and washers using a single manipulator. Since the single arm manipulation is not able to assemble two parts, two fixtures are provided to help in the assembly: one contains the washers in an approximate upright position to facilitate picking; the other has a well with the shape of the bolt head shallow enough to hold the bolt in place when a torque is applied, i.e. it secures the bolt when screwing the nut. The specifications of the task includes the approximate locations of the assembly pieces and fixtures, but not their exact positions. To solve the task we start by defining an observation point for the camera that is mounted in the arm, that provides a complete view of all necessary elements for the task and allows the manipulation of all parts without changing the position of the platform. This is important as, if the platform does not move, the relative positions of all objects are maintained with high precision after the first detection (see Fig. 3). The robot has to detect the bolt, pick it and place it in the bolt fixture, then it has to detect the washer, pick it and place it in the bolt, and finally it has to detect a nut, pick it and screw it in the bolt. Positions are calculated using the pinhole model instead of the point cloud provided by the stereo rig. this strategy can be pursued because the dimensions of all elements in the task and a precise distance from the camera to the working table, that is inferred from the pose of the arm are known in advance. This is done to speed up detection, because the generation of depth information is computationally expensive. 4.2.1 Fixture detection The detection of the fixtures, although executed only once, is an important step. The robot does not start from the working area but has to navigate there. Thus, location errors are inevitable. The assembly area is well defined, therefore, the fixtures are used as reference points to the rest of the elements. The detection of the fixtures is done using HSV color segmentation. We start by detecting both fixtures from the observation point. Once the resulting blobs are obtained, for each blob the rotated rectangle that best encloses it is calculated. The detection from the observation point is not very precise, therefore using the information from the rotated rectangle, the camera is approximated to each fixture and the detection is repeated individually. The information about the fixture that contains the washers is used in the detection of the washers, as their relative position to the fixture is known and so the search space in the image can be reduced. The center of the rotated rectangle that derives from the bolt’s fixture coincides with screwing place. 4.2.2 Bolt, nut and washer detection The detection algorithm for the bolts and nuts is the same. We explore their shape similarity when viewed from the top, i.e. when the object is located at the image center. The detection of a bolt/nut starts by performing color segmentation, then blobs are created from the resulting segmentation. Only the blob closer to the image center and in the vicinity of the bolts virtual line is considered, the rest being discarded. We perform a shape analysis to find the vertices of its convex shape and to find afterwards two consecutive edges that resemble the bolt/nut hexagonal top in length and angle (Fig. 11). Those edges are then used to calculate the center of the bolt/nut, and its orientation. The orientation is required for the nut so that it can be picked up by its edges. The described approach assumes that the object to be detected is as much as possible in the center of the image. This is achieved by generating a hint list for bolts and nuts that gives a rough approximation of their positions. The list is populated by running the color segmentation once for the bolts and nuts from the observation position. The resulting blobs are then used as hints. When it is time to detect a bolt/nut the hint is used to approximate the camera to the object in focus. The detection of the washers uses a different strategy. Instead of color segmentation, we do an intensity filtering that keeps the pixels with higher intensity (Fig. 12). Then, from the resulting segmentation we extract several blobs validated by their size. The centers of these blobs are then used to calculate the positions of the washers. 4.3 Manipulation and planning strategy To place the bolt in the screwing fixture, the place_bolt skill takes advantage of the compliant movement of the arm. To insert the bolt head into the fixture, after aligning the bolt with the center of the fixture, the bolt is pushed against the fixture while rotating it. As soon as a drop in the bolt’s height is detected, the action finishes, as it means the bolt’s head entered the fixture. For the place_nut skill the strategy is similar. While using compliant movement of the arm, the robot starts rotating the nut while at the same time pushes it against the bolt tip. Based on how much the height of the nut dropped during the first rotation, the number of rotations is adjusted in order to be fully screwed on the bolt. In the pick_washer skill, since they are in an upright position slightly tilted, the gripper approaches the washers just like in a normal pick but with the fingers adjusted for the washer diameter. Then the gripper moves down slowly in order to push the adjacent washer and create space for the picking. Because they are tilted the robot pushes them in the opposite direction of the tilt in order to became upright and aligned with the fingers of the gripper. To place the washers on the bolt, the place_washer skill aligns the inner bottom edge of the washer with the top of the bolt, just touching it on the side. Then the washer is rotated by 60 degrees while pushing it against the bolt. This tilts the bolt a little bit making it push the washer to the correct position when it is released. The planning strategy for this task is straightforward. Not considering the perception parts, it corresponds to the following sequence of actions: first, a bolt is picked up and placed in the fixture; then, a washer is picked up and placed in the bolt; next, a nut is picked up and screwed in the bolt; finally, the assembly is picked up and placed in a target region. The same procedure applies to the other bolts, washers and nuts. The speed of the movements and the movements connecting the different skills were parameterized and optimized during evaluation, starting with safer speeds and proceeding to faster ones after the task had been completed with success. In 7 attempts to solve the task, only one of them failed to complete all the objectives. With this strategy the best achieved task time was 6 min and 15 s. 5 5 5 Solving showcase tasks 5.1 Manipulation of stacked non rigid objects Solving the showcase tasks introduced a number of technological issues. The blank pile is not a unique rigid solid object, as shown in Fig. 13. It is a stack of cardboards, that can bend under its own weight and can also be disrupted, since some blanks can break free from the pile, due to low friction between them. To prevent the breakdown of a pile, any manipulation procedure has to be aware of this fact. Blanks are provided in untied piles, piled up on pallets close to each other, which can cause erroneous detections. Blank piles have to be placed in a feeding mechanism avoiding hard collisions with blanks that can already be there and with the four blank guiding rods that prevent blanks from slipping out of the inclined magazine surface. This feeding task requires a high level of precision for a proper feeding, while preventing collisions between the blank magazine and the manipulator. The manipulation has to be performed using a single arm. Since the gripper provided by the host is not appropriate, a solution has to be revised, taking into account the aforementioned issues. Aside from the gripper design, a proper sequence of actions has to be planned to accomplish the showcase tasks. 5.2 Manipulator for packaging Regarding the gripper design, the TIMAIRIS team decided to used the gripper provided with the robotic arm and adapt it to the required manipulations. Two fingers were designed, able to grasp a pile of blanks with a maximum height of 80 mm. The closing force of 80N (or at least 50N) should be enough at least for manipulating the smaller piles. In order to grasp a pile, it should be partially dragged out of the table, as shown in Fig. 13. To do so, two thin shafts were attached to the fingers, as shown in Fig. 14(a) and Fig. 15(a). These thin shafts can slide along their own axes thanks to the presence of a spring, ensuring the existence of contact between the shafts and the interlayer and thus preventing the loss of any blank during the dragging of the pile. Warehouse or blank magazine is composed of a plate inclined by 55 ∘ and four aluminum legs that sustain the plate. The plate is ad hoc built for the industrial partner’s blanks or cardboards. On the plate, four thin rods are mounted that guide the blanks once the robot delivers the whole stack, as shown in Fig. 14(b) and Fig. 15(b). Moreover, a proximity sensor is added and connected to a led yellow light. The proximity sensor checks if the number of cardboards is below a given threshold, in which case the light is turned on, as shown in Fig. 15(b). 5.3 Showcase perception To accomplish the showcase tasks a number of perception activities must be implemented. The piles of blanks are put over two pallets, thus these pallets must be detected and located relative to the robot. The poses of the piles themselves must be precisely estimated, so the picking up could be well performed. Finally, the blank magazine must be detected and located in order to perform the feeding procedure. The detection of the pallets is done by processing the stereo images captured using the pan–tilt camera. It unfolds into three steps: first, voxel grid filtering [35] based on the height of the pallets is used to remove irrelevant points from the point cloud [36]; second, the filtered points are projected on a horizontal plane and clustering is applied to define candidate regions; finally, these regions are matched against a 3D template to find the poses of the pallets. The same approach is applied for a first rough estimation of the poses of the piles in the pallets, by using the average height of the piles and an appropriate template. Fig. 16 illustrates the detection of both pallets and piles. However, to reliably pick a blank pile up from a pallet, a robust pose estimation of piles is necessary. On the one hand, in real industry scenarios, the piles on a pallet are tightly-aligned and put close to each other. On the other hand, the patterns printed on the blanks, even for the same blank shape, are variable and can change with end users’ demands. Therefore, detection and pose estimation of blanks cannot rely on conventional approaches such as color segmentation or local feature matching such as SIFT [37]. The approximate piles’ poses estimated so far can be used to position the TCP camera, in order to obtain more reliable ones. The camera is positioned over a pile and an image is captured, as shown in Fig. 17(a). Then, a Canny edge detector is applied to that image, as shown in Fig. 17(b). The detected edges come from the blanks’ outline but also from the printed patterns on the top blank. A drawing pin filter which classifies each point with various number of neighbors is applied to detect the outline of a blank pile [26]. The kernel is defined as K D P = [ x i ] = p + h n , if x i is in the center h n , otherwise p > h × | [ x ] | , n = ∑ x i , where p and h are the values of pin and head, respectively. The p is larger than the sum of heads and n is the normalization factor of the kernel. In this work, a drawing pin kernel of size 5 × 5 with p = 50 and h = 1 is used, as shown in Fig. 18. The basic assumption is that the edges of outlines are straight lines and their points have few neighbors except those in the line, while the points in printed patterns have many neighbors. In the filtering algorithm, points with more than the kernel size are discarded, as shown in Fig. 17(c). Finally, to find the precise pose of the pile, it is matched against a template, starting from the roughly estimated pose in a spiral direction. Fig. 17(d) shows the result of the match. 5.4 Motion planing After grasping the blank pile, the arm is put in a transport pose. This pose is defined considering two requirements. In one side, the arm should be completely inside the robot footprint. This way, navigation to the blank magazine table will be much safer. In the other side, the arm pose should minimize the necessary motions to place the pile in the magazine. All of these manipulations have to be done while maintaining the blank pile in a pose that prevents it from breaking apart. In this challenge, motions have been generated in the simulator in advance and then the results are applied to the real robot. In order to achieve the best results specific filters had to be used to enhance and complete the trajectories that were derived from existing motion planners. Several different motion planners have also been tested and benchmarked for tasks of different complexities [38]. 5.5 Task planning The manipulation of the blank pile, aside from the gripper, also raises planning issues. The first relates to the order by which blank piles must be manipulated. Due to their dispositions in the pallet, certain blank piles are blocked by others and cannot be dragged before these are removed. Therefore, the order by which blank piles are manipulated has to be planned. The plan considers the blank piles available in all the pallets and gives preference to the pallet with the lowest number of blank piles, since an empty pallet can be replaced with a fully packed one. Fig. 19 shows a task plan with the sequence and dragging directions to pick up all blank piles on two pallets. The dragging itself is also a challenge. The gripper and the arm have to maintain pressure against the pallet while dragging the blank pile. Too much pressure and the pallet could move, not enough pressure and the pile can be disrupted. Therefore, the robotic arm has to be used in compliant mode and the pressure should be properly calculated. 5.6 Safe human–robot collaboration For small batch production, changing or adding new features, such as continuously changing printed patterns of blanks, can be a burden on both human operators and autonomous robots. Since industrial environments are noisy, where machines produce a continuous whirring sound, verbal communication is difficult for humans and impractical for robots. Thus, gestures have been considered as a practical alternative way of communication. Until now, gesture recognition systems in HRI have focused on small number of implicit interactions such as pointing and handing over. With respect to pointing, the target object must be in the field of view of both human and robot. Fig. 20 shows an example of human tracking and human–robot interaction during the EuRoC evaluation. When the robot detects an operator entering a region for safety handling, it stops the executing task and points the pan–tilt to the operator. To ask to fetch objects in a cluttered and unstructured environment, HRI systems need to have a rich vocabulary to designate an object [16]. Fig. 21 shows an interaction tree for the showcase task. That represents commands by sequentially encoding 8 gestures, which consist of 6 numbers 0 to 5 and thumb up/down. Gesture recognition module recognizes a one-hand gesture provided by the operator. Then HRI module tries to build a command with a sequence of gestures. Following the command, the robot continues or stops the paused task. 6 Challenge evaluation 6.1 Benchmark evaluation During the benchmark phase of the Stages II, all the Challenge 2 participants have been evaluated by developing same two tasks under the same conditions: the robot platform, EuRoC software architecture and working time. As the benchmark task 1, the robot has to pick up and place 5 SLCs into the target area to show the logistics capabilities with different payload from different location to a fixed place. The task has two metrics: a number of delivered SLCs regardless of whether parts are lost (metric 1) and a number of delivered SLCs without losing parts (metric 2). Table 1 shows the results, which are their best results out of three trials. The bonus is calculated as inverse proportion to the time difference from the best on that between the best and worst. As the benchmark task 2, the robot is required to screw nuts on 5 bolts placed in a fixed area on a table using a fixture. The task also has two metrics: a number of washers successfully put onto bolts (metric 1) and a number of bolts with a successfully screwed on nut (metric 2) Table 2 shows the results of the benchmark task 2. The bonus is calculated as same as that of the task 1 among the records of teams, who finish the assembly task. TIMAIRIS has completed the both tasks without failures in the shortest times. 6.2 Showcase evaluation TIMARIS’ showcase addresses all of these issues (and others) in the form of three challenges and two extra demos, with quantifiable metrics. The objectives and metrics are organized so that the highlights described above can be evaluated in a comprehensive and robust way. The first objective (O1) is focused on perception and includes 4 metrics: detecting the pose of blank piles, pallets and blank magazine (O1M1); recognizing the need for blank feeding (O1M2); identifying the number of piles in each pallet (O1M3); identifying the presence of humans in the vicinity of the robot (O1M4). The second objective is focused on manipulation and navigation and includes 4 metrics: correctly picking a blank pile (O2M1); transporting the piles to the blank magazine (O2M2); placing the blank piles in the blank magazine (O2M3); stopping manipulation and navigation if a human is close to the robot (O2M4). The third objective evaluates the planning capabilities and includes 2 metrics: providing a plan to pick the blank piles (O3M1); adapting navigation paths (O3M2). The last objective evaluates human robot interaction and includes 2 metrics: recognizing gesture commands provided by the human (O4M1); tracking a human for interaction (O4M2). The achieved metrics are presented in Table 3. TIMAIRIS completed the showcase with every objective and metric being accomplished as can be seen from Table 3 where targets is the number of trials in predefined task scenarios and events is the number of successes during the showcase evaluation. The metrics of O1M1 are composed of three metrics for detecting the pose of blank piles, pallets and blank magazine. In what regards Perception, Manipulation/Navigation and Planning, i.e. Objectives 1, 2 and 3 previously specified, all metrics have been achieved during the execution of the first part of the showcase evaluation that consisted of three challenges. Tracking humans with the pan–tilt camera, Metric 2 of Objective 4, was also achieved during this first part of the showcase evaluation. The gesture recognition was demonstrated during the first part of the showcase evaluation, where gesture commands have been used to start challenge execution. Safety has been demonstrated in manipulation and navigation several times. During the showcase, the gesture recognition, which includes automatic correction methods, has been presented independently and the results showed that it is very robust and adequate for this kind of interaction (see the linked video 6 6 ). The results obtained in the showcase phase provide an excellent base for the development of the pilot experiment. The pilot experiment environment is a real industrial setting but, as already referred, all the developments of showcase are directly applicable in this final environment. Some new features will have to be addressed that also depend on the speedup of the final prototype, such as considering several packaging machines and different types/shapes of blanks, enhanced safety and more challenging navigation issues. Still, the showcase results are a complete and very solid basis for the work that needs to be done in the pilot experiments. 7 Conclusion To enable the paradigm shift in the packaging industry, many developed technologies must be considered as the integration cannot be performed and only the complete set allows the execution of the task. The developed system has gradually evolved from the qualification stage in a limited time. During the Stage 2 of the EuRoC project, the proposed architecture has demonstrated the feasibility of practical use of for Shop Floor Logistics and Manipulation. It is believed that SAAA is efficient not only in autonomous execution for autonomous packaging tasks but also in its use of development during the competitive challenges and that the simple architecture and distributed skills make the system efficient. The results of benchmarking tasks in which TIMAIRIS took first place among 5 Stage 2 challenger teams was possible that our team just focused on improving individual skills to complete all tasks and to reduce running time. One of important development during the showcase is showing that autonomous blank feeding is possible in a realistic industrial environment. That is a pending task of the industry partner IMA S.p.A which is a world leading company in the design and manufacture of automatic machines for the processing and packaging. As the result of the Stage II evaluations, TIMAIRIS has been selected to advance to the final stage of the EuRoC project. Some new features will have to be addressed that also depend on the speedup of the final prototype to complete the pilot experiments. Still, the results of the Stage II are a complete and very solid basis for the work that needs to be done in the final stage of the EuRoC project. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments This work was supported by the EuRoC Project under Grant no. 608849 and by National Funds through the FCT - Foundation for Science and Technology , in the context of the project UID/CEC/00127/2013. References [1] Pratt G. Manzo J. The DARPA robotics challenge [competitions] IEEE Robot. Autom. Mag. 20 2 2013 10 12 G. Pratt, J. Manzo, The DARPA robotics challenge [competitions], IEEE Robotics & Automation Magazine 20 (2) (2013) 10–12 [2] Kitano H. Asada M. Kuniyoshi Y. Noda I. Osawa E. Robocup: the robot world cup initiative Proceedings of the First International Conference on Autonomous Agents 1997 ACM 340 347 H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, E. Osawa, Robocup: The robot world cup initiative, in: Proceedings of the first international conference on Autonomous agents, ACM, 1997, pp. 340–347 [3] Siciliano B. Caccavale F. Zwicker E. Achtelik M. Mansard N. Borst C. Achtelik M. Jepsen N.O. Awad R. Bischoff R. EuRoC-The challenge initiative for european robotics ISR/Robotik 2014; 41st International Symposium on Robotics; Proceedings of 2014 VDE 1 7 B. Siciliano, F. Caccavale, E. Zwicker, M. Achtelik, N. Mansard, C. Borst, M. Achtelik, N. O. Jepsen, R. Awad, R. Bischoff, EuRoC-the challenge initiative for european robotics, in: ISR/Robotik 2014 41st International Symposium on Robotics; Proceedings of, VDE, 2014, pp. 1–7 [4] Bischoff R. Guhl T. The strategic research agenda for robotics in europe [industrial activities] IEEE Robot. Autom. Mag. 1 17 2010 15 16 R. Bischoff, T. Guhl, The strategic research agenda for robotics in europe [industrial activities], IEEE Robotics & Automation Magazine 1 (17) (2010) 15–16 [5] Cully A. Clune J. Tarapore D. Mouret J.-B. Robots that can adapt like animals Nature 521 7553 2015 503 A. Cully, J. Clune, D. Tarapore, J.-B. Mouret, Robots that can adapt like animals, Nature 521 (7553) (2015) 503 [6] Hildebrandt A.-C. Schuetz C. Wahrmann D. Wittmann R. Rixen D. A flexible robotic framework for autonomous manufacturing processes: report from the european robotics challenge stage 1 Autonomous Robot Systems and Competitions (ICARSC), 2016 International Conference on 2016 IEEE 21 27 A.-C. Hildebrandt, C. Schuetz, D. Wahrmann, R. Wittmann, D. Rixen, A flexible robotic framework for autonomous manufacturing processes: Report from the European Robotics Challenge Stage 1, in: Autonomous Robot Systems and Competitions (ICARSC), 2016 International Conference on, IEEE, 2016, pp. 21–27 [7] Zadeh S.M. Powers D.M. Sammut K. An autonomous reactive architecture for efficient AUV mission time management in realistic dynamic ocean environment Robot. Auton. Syst. 87 2017 81 103 S. M. Zadeh, D. M. Powers, K. Sammut, An autonomous reactive architecture for efficient auv mission time management in realistic dynamic ocean environment, Robotics and Autonomous Systems 87 (2017) 81–103 [8] Baklouti E. Amor N.B. Jallouli M. Reactive control architecture for mobile robot autonomous navigation Robot. Auton. Syst. 89 2017 9 14 E. Baklouti, N. B. Amor, M. Jallouli, Reactive control architecture for mobile robot autonomous navigation, Robotics and Autonomous Systems 89 (2017) 9–14 [9] Nilsson K. Johansson R. Integrated architecture for industrial robot programming and control Robot. Auton. Syst. 29 4 1999 205 226 K. Nilsson, R. Johansson, Integrated architecture for industrial robot programming and control, Robotics and Autonomous Systems 29 (4) (1999) 205–226 [10] Chatzilygeroudis K. Vassiliades V. Mouret J.-B. Reset-free trial-and-error learning for robot damage recovery Robot. Auton. Syst. 100 2018 236 250 K. Chatzilygeroudis, V. Vassiliades, J.-B. Mouret, Reset-free trial-and-error learning for robot damage recovery, Robotics and Autonomous Systems 100 (2018) 236–250 [11] Insaurralde C.C. Petillot Y.R. Capability-oriented robot architecture for maritime autonomy Robot. Auton. Syst. 67 2015 87 104 C. C. Insaurralde, Y. R. Petillot, Capability-oriented robot architecture for maritime autonomy, Robotics and Autonomous Systems 67 (2015) 87–104 [12] Dean T.L. Boddy M.S. An analysis of time-dependent planning. AAAI, vol. 88 1988 49 54 T. L. Dean, M. S. Boddy, An analysis of time-dependent planning., in: AAAI, Vol. 88, 1988, pp. 49–54 [13] Grefenstette J.J. Ramsey C.L. An approach to anytime learning Machine Learning Proceedings 1992 1992 Elsevier 189 195 J. J. Grefenstette, C. L. Ramsey, An approach to anytime learning, in: Machine Learning Proceedings 1992, Elsevier, 1992, pp. 189–195 [14] Pedrosa E. Lau N. Pereira A. Cunha B. A skill-based architecture for pick and place manipulation tasks Progress in Artificial Intelligence: 17th Portuguese Conference on Artificial Intelligence, EPIA 2015 2015 Springer 457 468 E. Pedrosa, N. Lau, A. Pereira, B. Cunha, A skill-based architecture for pick and place manipulation tasks, in: Progress in Artificial Intelligence: 17th Portuguese Conference on Artificial Intelligence, EPIA 2015, Springer, 2015, pp. 457–468 [15] Amaral F. Pedrosa E. Lim G.H. Shafii N. Pereira A. Azevedo J.L. Cunha B. Reis L.P. Badini S. Lau N. Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC challenge 2, stage II-realistic labs: benchmarking Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on 2017 IEEE 198 203 F. Amaral, E. Pedrosa, G. H. Lim, N. Shafii, A. Pereira, J. L. Azevedo, B. Cunha, L. P. Reis, S. Badini, N. Lau, Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC Challenge 2, Stage II-Realistic Labs: Benchmarking, in: Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on, IEEE, 2017, pp. 198–203 [16] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Dias P. Azevedo J.L. Cunha B. Reis L.P. Rich and robust human-robot interaction on gesture recognition for assembly tasks Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on 2017 IEEE 159 164 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, P. Dias, J. L. Azevedo, B. Cunha, L. P. Reis, Rich and robust human-robot interaction on gesture recognition for assembly tasks, in: Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on, IEEE, 2017, pp. 159–164 [17] Lim G.H. Pedrosa E. Amaral F. Dias R. Pereira A. Lau N. Azevedo J.L. Cunha B. Reis L.P. Human-robot collaboration and safety management for logistics and manipulation tasks Iberian Robotics Conference 2017 Springer 15 27 G. H. Lim, E. Pedrosa, F. Amaral, R. Dias, A. Pereira, N. Lau, J. L. Azevedo, B. Cunha, L. P. Reis, Human-robot collaboration and safety management for logistics and manipulation tasks, in: Iberian Robotics conference, Springer, 2017, pp. 15–27 [18] Mokhtari V. Lim G.H. Lopes L.S. Pinho A.J. Gathering and conceptualizing plan-based robot activity experiences Intelligent Autonomous Systems 13 2016 Springer 993 1005 V. Mokhtari, G. H. Lim, L. S. Lopes, A. J. Pinho, Gathering and conceptualizing plan-based robot activity experiences, in: Intelligent Autonomous Systems 13, Springer, 2016, pp. 993–1005 [19] Lim G.H. Shared representations of actions for alternative suggestion with incomplete information Robot. Auton. Syst. 2019 G. H. Lim, Shared representations of actions for alternative suggestion with incomplete information, Robotics and Autonomous Systems. [20] Balogh R. I am a robot–competitor: a survey of robotic competitions Int. J. Adv. Robot. Syst. 2 2005 17 R. Balogh, I am a robot–competitor: A survey of robotic competitions, International Journal of Advanced Robotic Systems 2 (2005) 17 [21] Thrun S. Montemerlo M. Dahlkamp H. Stavens D. Aron A. Diebel J. Fong P. Gale J. Halpenny M. Hoffmann G. Stanley: the robot that won the DARPA grand challenge J. Field Robot. 23 9 2006 661 692 S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel, P. Fong, J. Gale, M. Halpenny, G. Hoffmann, et al., Stanley: The robot that won the darpa grand challenge, Journal of field Robotics 23 (9) (2006) 661–692 [22] Buehler M. Iagnemma K. Singh S. The DARPA Urban Challenge: Autonomous Vehicles in City Traffic, vol. 56 2009 Springer M. Buehler, K. Iagnemma, S. Singh, The DARPA urban challenge: autonomous vehicles in city traffic, Vol. 56, springer, 2009 [23] A. Dömel, S. Kriegel, M. Brucker, M. Suppa, Autonomous pick and place operations in industrial production, in: Ubiquitous Robots and Ambient Intelligence (URAI), 2015 12th International Conference on, 2015, pp. 356–356, [24] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Azevedo J.L. Cunha B. Neural regularization jointly involving neurons and connections for robust image classification 2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI) 2017 IEEE 336 341 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, J. L. Azevedo, B. Cunha, Neural regularization jointly involving neurons and connections for robust image classification, in: 2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI), IEEE, 2017, pp. 336–341 [25] Russell S. Norvig P. Artificial Intelligence: A Modern Approach third ed. 2010 Prentice Hall S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, 3rd Edition, Prentice Hall, 2010 [26] Lim G.H. Pedrosa E. Amaral F. Lau N. Pereira A. Azevedo J.L. Cunha B. Badini S. Mobile manipulation for autonomous packaging in realistic environments: euroc challenge 2, stage ii, showcase 2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC) 2018 IEEE 231 236 G. H. Lim, E. Pedrosa, F. Amaral, N. Lau, A. Pereira, J. L. Azevedo, B. Cunha, S. Badini, Mobile manipulation for autonomous packaging in realistic environments: Euroc challenge 2, stage ii, showcase, in: 2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC), IEEE, 2018, pp. 231–236 [27] Lim G.H. Lau N. Pedrosa E. Amaral F. Pereira A. Luís Azevedo J. Cunha B. Precise and efficient pose estimation of stacked objects for mobile manipulation in industrial robotics challenges Adv. Robot. 2019 1 11 G. H. Lim, N. Lau, E. Pedrosa, F. Amaral, A. Pereira, J. Luís Azevedo, B. Cunha, Precise and efficient pose estimation of stacked objects for mobile manipulation in industrial robotics challenges, Advanced Robotics (2019) 1–11 [28] Lim G.H. Yi C. Suh I.H. Ko D.W. Hong S.W. Ontology representation and instantiation for semantic map building by a mobile robot Intelligent Autonomous Systems, vol. 12 2013 Springer 387 395 G. H. Lim, C. Yi, I. H. Suh, D. W. Ko, S. W. Hong, Ontology representation and instantiation for semantic map building by a mobile robot, in: Intelligent Autonomous Systems 12, Springer, 2013, pp. 387–395 [29] G.H. Lim, M. Oliveira, S.H. Kasaei, L.S. Lopes, Hierarchical nearest neighbor graphs for building perceptual hierarchies, in: 22nd International Conference on Neural Information Processing, ICONIP2015, 2015. [30] Lim G.H. Suh I.H. Suh H. Ontology-based unified robot knowledge for service robots in indoor environments IEEE Trans. Syst., Man, Cybern. A 41 3 2011 492 509 10.1109/TSMCA.2010.2076404 G. H. Lim, I. H. Suh, H. Suh, Ontology-based unified robot knowledge for service robots in indoor environments, Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on 41 (3) (2011) 492 –509 doi:101109/TSMCA.20102076404 [31] Rusu R. Marton Z. Blodow N. Dolha M. Towards 3D point cloud based object maps for household environments Robot. Auton. Syst. 56 11 2008 927 941 R. B. Rusu, Z. C. Marton, N. Blodow, M. Dolha, Towards 3D point cloud based object maps for household environments, Robotics and Autonomous Systems 56 (11) (2008) 927–941 [32] Rusu R.B. Semantic 3D object maps for everyday manipulation in human living environments (Ph.D. thesis) 2009 Computer Science department, Technische Universitaet Muenchen, Germany R. B. Rusu, Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments, Ph.D. thesis, Computer Science department, Technische Universitaet Muenchen, Germany (October 2009) [33] Oliveira M. Lopes L.S. Lim G.H. Kasaei S.H. Tomé A.M. Chauhan A. 3D object perception and perceptual learning in the RACE project Robot. Auton. Syst. 75 2016 614 626 M. Oliveira, L. S. Lopes, G. H. Lim, S. H. Kasaei, A. M. Tomé, A. Chauhan, 3d object perception and perceptual learning in the race project, Robotics and Autonomous Systems 75 (2016) 614–626 [34] Pedrosa E. Pereira A. Lau N. A scan matching approach to slam with a dynamic likelihood field 2016 International Conference on Autonomous Robot Systems and Competitions, ICARSC 2016 IEEE Portugal, Bragança 35 40 10.1109/ICARSC.2016.23 E. Pedrosa, A. Pereira, N. Lau, A Scan Matching Approach to SLAM with a Dynamic Likelihood Field, in: 2016 International Conference on Autonomous Robot Systems and Competitions (ICARSC), IEEE, Portugal, Bragança, 2016, pp. 35–40 doi:101109/ICARSC.201623 [35] Cohen-Or D. Kaufman A. Fundamentals of surface voxelization Graph. Models Image Process. 57 6 1995 453 461 D. Cohen-Or, A. Kaufman, Fundamentals of surface voxelization, Graphical models and image processing 57 (6) (1995) 453–461 [36] Rusu R.B. Cousins S. 3D is here: point cloud library (PCL) Robotics and Automation (ICRA), 2011 IEEE International Conference on 2011 IEEE 1 4 R. B. Rusu, S. Cousins, 3D is here: Point cloud library (PCL), in: Robotics and Automation (ICRA), 2011 IEEE International Conference on, IEEE, 2011, pp. 1–4 [37] Lowe D.G. Object recognition from local scale-invariant features Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, vol. 2 1999 Ieee 1150 1157 D. G. Lowe, Object recognition from local scale-invariant features, in: Computer vision, 1999 The proceedings of the seventh IEEE international conference on, Vol. 2, Ieee, 1999, pp. 1150–1157 [38] Tudico A. Lau N. Pedrosa E. Amaral F. Mazzotti C. Carricato M. Improving and benchmarking motion planning for a mobile manipulator operating in unstructured environments Portuguese Conference on Artificial Intelligence 2017 Springer 498 509 A. Tudico, N. Lau, E. Pedrosa, F. Amaral, C. Mazzotti, M. Carricato, Improving and benchmarking motion planning for a mobile manipulator operating in unstructured environments, in: Portuguese Conference on Artificial Intelligence, Springer, 2017, pp. 498–509 Dr. Gi Hyun Lim received the B.S. degree in metallurgical engineering and the M.S. and Ph.D. degrees in electronics and computer engineering from Hanyang University, Seoul, Korea, in 1997, 2007 and 2010, respectively. He is currently a Marie Curie individual fellow in the School of Computer Science at University of Manchester, UK. His research interests lie in the area of artificial intelligence and machine learning for autonomous robots, including perception, semantics, cognition and spatiotemporal representations on neuromorphic architectures. Eurico Pedrosa is a Post-Doc Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his Informatics Engineering degree from University of Aveiro in 2010 and a Computer Science Ph.D. degree from Aveiro University in 2018. His research interest are focused on intelligent robotics, robotic navigation including localization and mapping (SLAM), space representation using volumetric grids and most recently the application of radar sensors in indoor robotics. Filipe Amaral is a Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) integrated in Intelligent Robotics and Systems group (IRIS). He got his MSc degree in Computer and Telematics Engineering from University of Aveiro in 2014. His current research interests are in the area of autonomous mobile robotics. Prof. Dr. Artur Pereira was born in Vila Nova de Famalicão, Portugal, in April 1960. He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 2003. He is currently an Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Instituto de Engenharia Electrónica e Informática de Aveiro. The main focus of his research is robotics at the architectural and software levels, with emphasis on simulation, navigation, localization, mapping, and machine learning. Nuno Lau is Assistant Professor at Aveiro University, Portugal and Researcher at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), where he leads the Intelligent Robotics and Systems group (IRIS). He got is Electrical Engineering Degree from Oporto University in 1993, a DEA degree in Biomedical Engineering from Claude Bernard University, France, in 1994 and the Ph.D. from Aveiro University in 2003. His research interests are focused on Intelligent Robotics, Artificial Intelligence, Multi-Agent Systems and Simulation. Nuno Lau participated in more than 15 international and national research projects, having the tasks of general or local coordinator in about half of them. Nuno Lau won more than 50 scientific awards in robotic competitions, conferences (best papers) and education. He has lectured courses at Phd and MSc levels on Intelligent Robotics, Distributed Artificial Intelligence, Computer Architecture, Programming, etc. Nuno Lau is the author of more than 150 publications in international conferences and journals. He was President of the Portuguese Robotics Society from 2015 to 2017, and is currently the Vice-President of this Society. Prof. Dr. José Luís Azevedo is currently Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro and a researcher at the Intelligent Robotics and Systems group (IRIS Lab) of the Institute of Electronics and Informatics Engineering of Aveiro (IEETA). He received the Ph.D. degree in Electrical Engineering from the University of Aveiro, Portugal, in 1998. His current research interests are in the area of cooperative autonomous mobile robotics. Prof. Dr. Bernardo Cunha was born in 1959 in Porto, Portugal. He earned his doctoral degree in electrical engineering at the University of Aveiro, Portugal, in 1999. He is a full time teacher at Universidade de Aveiro in the computer architecture area and an investigator at the Instituto de Engenharia Electrónica e Informática de Aveiro. Current research interests are centered in the area of cooperative autonomous mobile robotics. Simone Badini is a Mechanical Designer in the Research and Development department of IMA Spa since 2013. IMA Spa is a world leader company in the design and manufacture of automatic machines for the processing and packaging of pharmaceuticals, cosmetics, food, tea and coffee and tobacco. He got is M.Sc. degree in Mechanical Engineering from University of Bologna, Italy in 2012. He is currently project manager for the integration of cobot and autonomous mobile robot in the production lines for the IMA group. "
    },
    {
        "doc_title": "Multi-Robot Fast-Paced Coordination with Leader Election",
        "doc_scopus_id": "85070697946",
        "doc_doi": "10.1007/978-3-030-27544-0_2",
        "doc_eid": "2-s2.0-85070697946",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Global objective",
            "Leader election",
            "Multi-agent coordinations",
            "Multi-robot systems",
            "Real time constraints",
            "Soccer-playing robots",
            "Stochastic environment",
            "Task assignment"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.Coordination in Multi-Robot Systems is an active research line in Artificial Intelligence applied to Robotics. Through coordination, a team of robots can efficiently achieve their pre-defined global objective. From a wide range of multi-agent coordination sub-topics, one of the current open issues is task assignment and role selection in fast-paced environments. In homogeneous teams, where robots have the ability to dynamically change roles, working in highly dynamic and stochastic environments, it is important that any solution is able to perform and achieve results while complying with realtime constraints. In this paper, we balance the advantages and disadvantages of completely decentralised solutions and centralised ones, and then present our solution for leader election among a team, which is based on the Raft algorithm and tackles two of its limitations. The proposed solution was implemented in a real team of soccer-playing robots and the experimental results are thoroughly presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mobile manipulation for autonomous packaging in realistic environments: EuRoC challenge 2, stage II, showcase",
        "doc_scopus_id": "85048892736",
        "doc_doi": "10.1109/ICARSC.2018.8374188",
        "doc_eid": "2-s2.0-85048892736",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.European Robotics Challenges (EuRoC) project has been launched to find competitive solutions by exploiting synergies across research institutes to industrial end-users. This paper reports the research conducted by the TIMAIRIS team to fulfill EuRoC C2 Stage II tasks. TIMAIRIS is one of the 6 EuRoC finalists (Stage III) from an initial group of 102 teams. The packaging industry is very interested in recent advances in robotics but is still quite conservative in the way it uses automation, since shapes and printed patterns of blanks vary a lot to comply with end users' demands. The use of programmable logic controllers (PLCs) is widely common but the use of more sophisticated decision mechanisms is not so common. A shop floor logistics and manipulation system has been developed and demonstrated in a realistic environment for autonomous packaging.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Human-Robot Collaboration and Safety Management for Logistics and Manipulation Tasks",
        "doc_scopus_id": "85042220407",
        "doc_doi": "10.1007/978-3-319-70836-2_2",
        "doc_eid": "2-s2.0-85042220407",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Human-robot collaboration",
            "Industrial context",
            "Manipulation task",
            "Mobile manipulator",
            "Reasoning methods",
            "Region-based filtering",
            "Safety concerns",
            "Safety management"
        ],
        "doc_abstract": "© Springer International Publishing AG 2018.To realize human-robot collaboration in manufacturing, industrial robots need to share an environment with humans and to work hand in hand. This introduces safety concerns but also provides the opportunity to take advantage of human-robot interactions to control the robot. The main objective of this work is to provide HRI without compromising safety issues in a realistic industrial context. In the paper, a region-based filtering and reasoning method for safety has been developed and integrated into a human-robot collaboration system. The proposed method has been successfully demonstrated keeping safety during the showcase evaluation of the European robotics challenges with a real mobile manipulator.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Neural regularization jointly involving neurons and connections for robust image classification",
        "doc_scopus_id": "85042368378",
        "doc_doi": "10.1109/MFI.2017.8170451",
        "doc_eid": "2-s2.0-85042368378",
        "doc_date": "2017-12-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Classification performance",
            "Classification results",
            "Experimental analysis",
            "Fully connected neural network",
            "Fully-connected layers",
            "Prediction performance",
            "Regularization methods",
            "Regularization technique"
        ],
        "doc_abstract": "© 2017 IEEE.This paper presents an integrated neural regularization method in fully-connected neural networks that jointly combines the cutting edge of regularization techniques; Dropout [1] and DropConnect [2]. With a small number of data set, trained feed-forward networks tend to show poor prediction performance on test data which has never been introduced while training. In order to reduce the overfitting, regularization methods commonly use only a sparse subset of their inputs. While a fully-connected layer with Dropout takes account of a randomly selected subset of hidden neurons with some probability, a layer with DropConnect only keeps a randomly selected subset of connections between neurons. It has been reported that their performances are dependent on domains. Image classification results show that the integrated method provides more degrees of freedom to achieve robust image recognition in the test phase. The experimental analyses on CIFAR-10 and one-hand gesture dataset show that the method provides the opportunity to improve classification performance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real-time multi-object tracking on highly dynamic environments",
        "doc_scopus_id": "85026875635",
        "doc_doi": "10.1109/ICARSC.2017.7964072",
        "doc_eid": "2-s2.0-85026875635",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Dynamic environments",
            "Multi-object tracking",
            "Multiple moving objects",
            "Real time constraints",
            "Robotics research",
            "Soccer-playing robots",
            "Tracking objects",
            "Trade off"
        ],
        "doc_abstract": "© 2017 IEEE.An accurate representation of the environment is important to plan tasks and efficiently control an autonomous robot. Multi-object tracking has long been an important task in robotics research whenever an autonomous robot needs to plan its tasks on an environment with multiple moving objects. Specially when the application is defined by real-time constraints in a fast-paced environment, a trade-off between accuracy and performance is automatically imposed. This paper presents a solution for real-time multi-object tracking on stochastic and highly dynamic environments. Although not limited to this specific application domain, the implementation and evaluation of this solution was performed on a team of autonomous soccer-playing robots. The proposed solution is detailed from the detection and feature extraction phase to the general problem of efficiently tracking objects in the environment. Results on the real testbed are also presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Skill-based anytime agent architecture for logistics and manipulation tasks: EuRoC Challenge 2, Stage II - Realistic Labs: Benchmarking",
        "doc_scopus_id": "85026869156",
        "doc_doi": "10.1109/ICARSC.2017.7964075",
        "doc_eid": "2-s2.0-85026869156",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Agent architectures",
            "Continuous development",
            "Effective solution",
            "Manipulation task",
            "Planning strategies",
            "Production quality",
            "Robotic technologies",
            "Scientific competition"
        ],
        "doc_abstract": "© 2017 IEEE.Nowadays, the increase of robotic technology application to industry scenarios is notorious. Proposals for new effective solutions are in continuous development once industry needs a constantly improvement in time as well as in production quality and efficiency. The EuRoC research project proposes a scientific competition in which research and industry manufacturers joint teams are encouraged to develop and test solutions that can solve several issues as well as be useful in manufacturing improvement. This paper presents the TIMAIRIS architecture and approach used in the Challenge 2 - Stage II - Benchmarking phase, namely regarding the perception, manipulation and planning strategy that was applied to achieve the tasks objectives. The used approach proved to be quite robust and efficient, which allowed us to rank first in the Benchmarking phase.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Rich and robust human-robot interaction on gesture recognition for assembly tasks",
        "doc_scopus_id": "85026864168",
        "doc_doi": "10.1109/ICARSC.2017.7964069",
        "doc_eid": "2-s2.0-85026864168",
        "doc_date": "2017-06-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Assembly tasks",
            "Human robot Interaction (HRI)",
            "Manufacturing enterprise",
            "Mobile manipulator",
            "Multiple feature fusion",
            "Puzzle games",
            "Robotics technology",
            "Small and medium sized enterprise"
        ],
        "doc_abstract": "© 2017 IEEE.The adoption of robotics technology has the potential to advance quality, efficiency and safety for manufacturing enterprises, in particular small and medium-sized enterprises. This paper presents a human-robot interaction (HRI) system that enables a robot to receive commands, provide information to a human teammate and ask them a favor. In order to build a robust HRI system based on gesture recognition, three key issues are addressed: richness, multiple feature fusion and failure verification. The developed system has been tested and validated in a realistic lab with a real mobile manipulator and a human teammate to solve a puzzle game.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Time-constrained detection of colored objects on raw Bayer data",
        "doc_scopus_id": "84959311753",
        "doc_doi": "10.1201/b19241-51",
        "doc_eid": "2-s2.0-84959311753",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Autonomous robotics",
            "Color object detection",
            "Demosaicing algorithm",
            "Full color images",
            "Image processing applications",
            "Object detection algorithms",
            "Perception and actions",
            "Real-time application"
        ],
        "doc_abstract": "© 2016 Taylor & Francis Group, London.In most image processing applications, the raw data acquired by an image sensor passes through a demosaicing algorithm in order to obtain a full color image. The demosaicing step can take time and resources that are of great importance when developing real-time applications. Most of the times, this is done with single purpose of rendering the images into a viewable format for humans. However, most of nowadays sensors allow the retrieval of images in raw format and processing the data as is. In this paper we present a study on the direct usage of raw Bayer data for real-time color object detection, in the scenario of autonomous robot soccer. The experimental results that we provide prove that the efficiency of the chosen object detection algorithm is not lowered and that several improvements of the autonomous robotic vision system used in this study can be noted: the bandwidth needed to transmit the data between the digital camera and the PC is reduced, allowing, in some models, to increase the maximum frame rate. Moreover, a decrease in the delay between the perception and action of the autonomous robot was measured, when using raw image data for processing the environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of aerial balls in robotic soccer using a mixture of color and depth information",
        "doc_scopus_id": "84933059765",
        "doc_doi": "10.1109/ICARSC.2015.13",
        "doc_eid": "2-s2.0-84933059765",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Ball detection",
            "Color classification",
            "League competition",
            "Omni-directional vision",
            "Omnidirectional cameras",
            "Real-time application",
            "Robotic vision",
            "Trajectory calculations"
        ],
        "doc_abstract": "© 2015 IEEE.Detection of aerial objects is a difficult problem to tackle given the dynamics and speed of a flying object. The problem is even more difficult when considering a noncontrolled environment, where the predominance of a given color is not guaranteed, and/or when the vision system is located on a moving platform. Taking as an example the game of robotic soccer promoted by the RoboCup Federation, most of the teams participating in the soccer competitions detect the objects in the environment using an omni directional camera. Omni directional vision systems only detect the ball when it is on the ground, and thus precise information on the ball position when in the air is lost. In this paper we present a novel approach for 3D ball detection in which we use the color information to identify ball candidates and the 3D data for filtering the relevant color information. The main advantage of our approach is the low processing time, being thus suitable for real-time applications. We present experimental results showing the effectiveness of the proposed algorithm. Moreover, this approach was already used in the last official RoboCup Middle Size League competition. The goalkeeper was able to move to a right position in order to defend a goal, in situations where the ball was flying towards the goal.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving the kicking accuracy in a soccer robot",
        "doc_scopus_id": "84955480270",
        "doc_doi": "10.1145/2695664.2695862",
        "doc_eid": "2-s2.0-84955480270",
        "doc_date": "2015-04-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Accuracy",
            "Friction parameters",
            "Heuristic approach",
            "Kicking device",
            "Middle-Size League",
            "Physical characteristics",
            "RoboCu",
            "Target direction"
        ],
        "doc_abstract": "Copyright 2015 ACM.Most of the soccer robots in the Middle Size League of Robo-Cup use electromagnetic kicking devices that allow to kick the ball with adjustable strength. In order to be efficient to score, the kicking strength should be calculated according to several parameters, namely the physical characteristics of the kicking device, the distance to the target, the velocity of the robot and the floor friction parameters. Moreover, the kicking decision should be taken at a moment in which the actual movement of the robot would result in the ball being released to the target direction, even if it is not physically aligned with it. This paper proposes an algorithm to improve the kicking accuracy, taking into account the described parameters, in a heuristic approach. The experimental results presented in this paper show the effectiveness of the proposed solution to improve the efficiency of the kicking device.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UAVision: A modular time-constrained vision library for soccer robots",
        "doc_scopus_id": "84958540813",
        "doc_doi": "10.1007/978-3-319-18615-3_40",
        "doc_eid": "2-s2.0-84958540813",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.The game of soccer is one of the main focuses of the RoboCup competitions, being a fun and entertaining research environment for the development of autonomous multi-agent cooperative systems. For an autonomous robot to be able to play soccer, first it has to perceive the surrounding world and extract only the relevant information in the game context. Therefore, the vision system of a robotic soccer player is probably the most important sensorial element, on which the acting of the robot is fully based. In this paper we present a new modular time-constrained vision library, named UAVision, that allows the use of video sensors up to a frame rate of 50 fps in full resolution and provides accurate results in terms of detection of the objects of interest for a robot playing soccer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From an autonomous soccer robot to a robotic platform for elderly care",
        "doc_scopus_id": "84861984252",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861984252",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Developed countries",
            "Elderly care",
            "Hardware and software",
            "Home care",
            "Independent living",
            "Innovative solutions",
            "Number of peoples",
            "Nursing homes",
            "Robotic platforms",
            "Robotic soccer",
            "Soccer robot"
        ],
        "doc_abstract": "Current societies in developed countries face a serious problem of aged population. The growing number of people with reduced health and capabilities, allied with the fact that elders are reluctant to leave their own homes to move to nursing homes, requires innovative solutions since continuous home care can be very expensive and dedicated 24/7 care can only be accomplished by more than one care-giver. This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An heuristic for trajectory generation in mobile robotics",
        "doc_scopus_id": "80655142776",
        "doc_doi": "10.1109/ETFA.2011.6059220",
        "doc_eid": "2-s2.0-80655142776",
        "doc_date": "2011-11-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "2-D space",
            "Formal proofs",
            "Holonomic robots",
            "Initial position",
            "Mobile robotic",
            "Numerical experiments",
            "RoboCup",
            "Trajectory generation",
            "Trapezoidal velocity profile",
            "Viable solutions"
        ],
        "doc_abstract": "We present an heuristic to compute the points in a trajectory in a 2D space based on the trapezoidal velocity profile, where the computed trajectory is subject to the constraints of initial position and velocity, final position and velocity (defined as vectors) and the values for acceleration and plateau speed (defined as scalars). The proposed heuristics are directed at omnidirectional holonomic robots, i.e., robots that are capable of, amongst others, manoeuvring without affecting the orientation. These algorithms are currently being applied to the CAMBADA team RoboCup MSL robots. Although without a formal proof, numerical experiments have shown that the algorithms converged to a viable solution when the data fulfils the necessary conditions. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A mobile robotic platform for elderly care",
        "doc_scopus_id": "79960496492",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960496492",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Elderly care",
            "Hardware and software",
            "Independent living",
            "Mobile robotic",
            "Next generation network",
            "Robotic platforms",
            "Robotic soccer"
        ],
        "doc_abstract": "This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "World modeling on an MSL robotic soccer team",
        "doc_scopus_id": "79952617090",
        "doc_doi": "10.1016/j.mechatronics.2010.05.011",
        "doc_eid": "2-s2.0-79952617090",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Coordination and Control",
            "Information fusion techniques",
            "Limited information",
            "Localization algorithm",
            "Obstacle detection",
            "Sensor fusion",
            "Visual matching",
            "World model"
        ],
        "doc_abstract": "When a team of robots is built with the objective of playing soccer, the coordination and control algorithms must reason, decide and actuate based on the current conditions of the robot and its surroundings. This is where sensor and information fusion techniques appear, providing the means to build an accurate model of the world around the robot, based on its own limited sensor information and the also limited information obtained through communication with the team mates. One of the most important elements of the world model is the robot self-localization, as to be able to decide what to do in an effective way, it must know its position in the field of play. In this paper, the team localization algorithm is presented focusing on the integration of visual and compass information. An important element in a soccer game, perhaps the most important, is the ball. To improve the estimations of the ball position and velocity, two different techniques have been developed. A study of the visual sensor noise is presented and, according to this analysis, the resulting noise variation is used to define the parameters of a Kalman filter for ball position estimation. Moreover, linear regression is used for velocity estimation purposes, both for the ball and the robot. This implementation of linear regression has an adaptive buffer size so that, on hard deviations from the path (detected using the Kalman filter), the regression converges faster. A team cooperation method based on sharing the ball position is presented. Other important data during the soccer game is obstacle data. This is an important challenge for cooperation purposes, allowing the improvement of team strategy with ball covering, dribble corridor estimation, pass lines, among other strategic possibilities. Thus, detecting the obstacles is ceasing to be enough and identifying which obstacles are team mates and opponents is becoming a need. An approach for this identification is presented, considering the visual information, the known characteristics of the team robots and shared localization among team members. The described work was implemented on the CAMBADA team and allowed it to achieve particularly good performances in the last two years, with a 1st and a 3rd place in the world championship RoboCup 2008 and RoboCup 2009 editions, respectively, as well as distinctively achieve 1st place in 2008 and 2009 editions of the Portuguese Robotics Open. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271456 291210 291718 291787 291883 31 Mechatronics MECHATRONICS 2010-07-01 2010-07-01 2011-03-08T22:21:19 S0957-4158(10)00102-9 S0957415810001029 10.1016/j.mechatronics.2010.05.011 S300 S300.1 FULL-TEXT 2015-05-15T03:43:39.596909-04:00 0 0 20110301 20110331 2011 2010-07-01T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0957-4158 09574158 21 21 2 2 Volume 21, Issue 2 7 411 422 411 422 201103 March 2011 2011-03-01 2011-03-31 2011 Special Issue on Advances in intelligent robot design for the Robocup Middle Size League M.J.G. van de Molengraft O. Zweigle Special Issue on Advances in intelligent robot design for the Robocup Middle Size League article fla Copyright © 2010 Elsevier Ltd. All rights reserved. WORLDMODELINGMSLROBOTICSOCCERTEAM SILVA J 1 Introduction 2 Related work 3 Localization 4 Ball integration 4.1 Ball position 4.2 Ball velocity 4.3 Team ball position sharing 5 Obstacle treatment 5.1 Visual obstacle detection 5.2 Obstacle selection and identification 5.3 Obstacle sharing 6 Conclusion and future work Acknowledgment References METROPOLIS 1949 335 341 N KALMAN 1960 35 45 R LUO 2002 107 119 R LEONARD 1991 376 382 J FOX 1999 391 427 D MOURIKIS 2006 666 681 A DURRANTWHYTE 2008 H SPRINGERHANDBOOKROBOTICS MULTISENSORDATAFUSION BEJCZY 2004 41 42 A ALENYA 2004 23 32 G CHROUST 2004 73 83 S THRUN 2005 W PROBABILISTICROBOTICS SICILIANO 2008 B SPRINGERHANDBOOKROBOTICS LAUER 2005 291 303 M KI2005ADVANCESINARTIFICIALINTELLIGENCE MODELINGMOVINGOBJECTSINADYNAMICALLYCHANGINGROBOTAPPLICATION FERREIN 2006 154 165 A ROBOCUP2005ROBOTSOCCERWORLDCUPIX COMPARINGSENSORFUSIONTECHNIQUESFORBALLPOSITIONESTIMATION LAUER 2006 142 153 M ROBOCUP2005ROBOTSOCCERWORLDCUPIX CALCULATINGPERFECTMATCHEFFICIENTACCURATEAPPROACHFORROBOTSELFLOCALIZATION NEVES 2007 499 507 A PROGRESSINARTIFICIALINTELLIGENCE OMNIDIRECTIONALVISIONSYSTEMFORSOCCERROBOTS CUNHA 2008 417 424 B ROBOCUP2007ROBOTSOCCERWORLDCUPXI OBTAININGINVERSEDISTANCEMAPANONSVPHYPERBOLICCATADIOPTRICROBOTICVISIONSYSTEM SILVAX2011X411 SILVAX2011X411X422 SILVAX2011X411XJ SILVAX2011X411X422XJ item S0957-4158(10)00102-9 S0957415810001029 10.1016/j.mechatronics.2010.05.011 271456 2011-03-10T12:04:28.24434-05:00 2011-03-01 2011-03-31 true 2037530 MAIN 12 77452 849 656 IMAGE-WEB-PDF 1 si1 820 43 146 si4 279 20 19 si3 279 20 19 si2 672 40 84 gr10 40478 301 366 gr10 6601 164 200 gr11 38816 374 635 gr11 3816 129 219 gr12 45526 461 779 gr12 2642 130 219 gr13 25831 160 330 gr13 9938 106 219 gr14 26031 374 628 gr14 2725 130 219 gr15 25951 161 322 gr15 10234 109 219 gr16 11424 220 243 gr16 3895 164 181 gr17 34129 332 324 gr17 9007 164 160 gr18 28869 336 320 gr18 8228 164 156 gr19 31951 292 350 gr19 4895 164 196 gr2 24610 181 349 gr2 9025 114 219 gr20 32518 300 345 gr20 5136 163 188 gr21 11053 222 356 gr21 3093 137 219 gr3 14423 252 223 gr3 4214 164 145 gr4 90061 420 715 gr4 6962 129 219 gr5 31815 170 578 gr5 3190 64 219 gr6 34213 295 372 gr6 5554 164 207 gr7 32017 304 505 gr7 3661 132 219 gr8 11017 117 266 gr8 2052 96 219 gr9 53806 297 357 gr9 9002 164 197 gr1 35942 173 489 gr1 9273 77 219 MECH 1165 S0957-4158(10)00102-9 10.1016/j.mechatronics.2010.05.011 Elsevier Ltd Fig. 1 Picture of the team robots used to obtain the results presented on this paper. Fig. 2 Captures of an image acquired by the robot camera and processed by the vision algorithms. Left (a): The image acquired by the camera. Right (b): The same image after processing with magenta dots over the detected field lines. Fig. 3 Illustration of the compass error angle intervals. Fig. 4 Illustration of two situations where relocation was forced. Dashed line represents the angle given by the compass, solid line represents the angle estimated by the localization algorithm, red lines represent the cycles on which the error between the two angles is greater than the threshold. Left (a): The camera was covered while the robot moved. The estimated orientation error degrades progressively and after getting higher than a threshold, the cycle count starts and forces relocation. Right (b): The robot tilted. The estimated orientation error is immediately affected by more than a threshold and the cycle count starts and forces relocation. Fig. 5 Noisy position of a static ball taken from a rotating robot. Fig. 6 Plot of a robot movement around a fixed ball position. The ball positions measured by the moving robot form a cloud of points (green) in the area of the real ball position (black X). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 7 Plot of a ball movement situation. Fig. 8 Situation where a hard deviation would be detected by the filter. Positions R4,5,6, are the measured positions after the ball hits an obstacle, P4,5,6 are the predicted filtered estimations, which did not consider that something might alter the ball path. Fig. 9 Velocity representation using consecutive measures displacement. Fig. 10 Velocity representation using linear regression over Kalman filtered positions. Fig. 11 Comparison between the velocity estimated by the linear regression (blue solid line, faster convergence) and internally by the Kalman filter (red dashed line, smoother, but of slow convergence). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Diagram of the ball integration algorithm. Fig. 13 Captures of an image acquired by the robot camera and processed by the vision algorithms. The areas of interest were surrounded. (a) The image acquired by the camera. (b) The same image after processing. Obstacles are identified by their center (triangle), left and right limits (squares). It is visible that the two aligned obstacles are detected as a single larger obstacle (top right of the frames). Fig. 14 Relation between pixels and metric distances. The center of the robot is considered the origin and the metric distances are considered on the ground plane. Fig. 15 Example of an image acquired by the robot camera and processed by the vision algorithm. The areas of interest are surrounded. (a) The image acquired by the camera. (b) The same image after processing. It is visible the two possibilities of separation made: angular separation, on the bottom pair of obstacles and length separation, on the top pair of obstacles. Fig. 16 When a CAMBADA robot is on, the estimated centers of the detected obstacles are compared with the known position of the team mates and tested; the left obstacle is within the CAMBADA acceptance radius, the right one is not. Fig. 17 Illustration of single obstacles identification. (a) Image acquired from the robot camera (obstacles for identification are marked). (b) The same image after processing. (c) Image of the control station. Each robot represents itself and robot 6 (the lighter gray) draws all the five obstacles evaluated (squares with the same gray scale as itself). All team mates were correctly identified (marked by its corresponding number over the obstacle square) and the opponent is also represented with no number. Fig. 18 Illustration of multiple obstacles identification. (a) Image acquired from the robot camera (obstacle for identification marked). (b) The same image after processing. Visually, the aligned robots are only one large obstacle. (c) Image of the control station. Each robot represents itself and robot 6 (the darker gray) draws all the five obstacles (squares with the same gray scale as itself). The visual obstacle was successfully separated into the several composing obstacles, and all of them were correctly identified as the correspondent team mate (marked by its corresponding number over the obstacle square) and the opponent is also represented with no number. Fig. 19 Representation of a capture of the obstacle identification algorithm results. The path taken by the observer is represented by blue dots in the rectangular path taken. Near the center, the pivot shared position is represented by the black star and its limits by the black circle. The blob of red is the overlapping positions of the identified obstacle center, represented by a red cross. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 20 Representation of the path taken by the team mate to identify (the red dots represent each communicated position). The observer position is represented by the black star and its limits by the black circle. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 21 Image of the control station showing an obstacle of robot 2 that was not seen by itself (on the center of the field). In this case it assumes the obstacle by confirmation of both robots 5 and 6. Table 1 The mean and standard deviation of the capture perceived obstacle position. Perceived obstacle X Y Mean 0.05 2.01 Std 0.08 0.07 ∣Real−perceived∣=0.16. ∣Std∣=0.10. Table 2 The individual ratio of successful identification of the moving team mate for the several captures performed. Total cycles Successes % Capture 1 1798 1319 73 Capture 2 1065 748 70 Capture 3 1528 1332 87 Capture 4 1162 769 66 Capture 5 1935 1278 66 Capture 6 2152 1411 66 World modeling on an MSL robotic soccer team João Silva ⁎ Nuno Lau António J.R. Neves João Rodrigues José Luís Azevedo ATRI, IEETA/DETI, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. When a team of robots is built with the objective of playing soccer, the coordination and control algorithms must reason, decide and actuate based on the current conditions of the robot and its surroundings. This is where sensor and information fusion techniques appear, providing the means to build an accurate model of the world around the robot, based on its own limited sensor information and the also limited information obtained through communication with the team mates. One of the most important elements of the world model is the robot self-localization, as to be able to decide what to do in an effective way, it must know its position in the field of play. In this paper, the team localization algorithm is presented focusing on the integration of visual and compass information. An important element in a soccer game, perhaps the most important, is the ball. To improve the estimations of the ball position and velocity, two different techniques have been developed. A study of the visual sensor noise is presented and, according to this analysis, the resulting noise variation is used to define the parameters of a Kalman filter for ball position estimation. Moreover, linear regression is used for velocity estimation purposes, both for the ball and the robot. This implementation of linear regression has an adaptive buffer size so that, on hard deviations from the path (detected using the Kalman filter), the regression converges faster. A team cooperation method based on sharing the ball position is presented. Other important data during the soccer game is obstacle data. This is an important challenge for cooperation purposes, allowing the improvement of team strategy with ball covering, dribble corridor estimation, pass lines, among other strategic possibilities. Thus, detecting the obstacles is ceasing to be enough and identifying which obstacles are team mates and opponents is becoming a need. An approach for this identification is presented, considering the visual information, the known characteristics of the team robots and shared localization among team members. The described work was implemented on the CAMBADA team and allowed it to achieve particularly good performances in the last two years, with a 1st and a 3rd place in the world championship RoboCup 2008 and RoboCup 2009 editions, respectively, as well as distinctively achieve 1st place in 2008 and 2009 editions of the Portuguese Robotics Open. Keywords Sensor fusion World model Kalman filter Linear regression Obstacle detection Visual matching 1 Introduction Nowadays, there are several research domains in the area of multi robot systems. One of the most popular is robotic soccer. RoboCup 1 1 is an international joint project to promote artificial intelligence, robotics and related fields. Most of the RoboCup leagues have soccer as platform for developing technology, either at software or hardware levels, with single or multiple agents, cooperative or competitive [1]. Among RoboCup leagues, the Middle Size League (MSL) is one of the most challenging. In this league, each team is composed of up to five robots with maximum size of 50×50cm base, 80cm height and a maximum weight of 40kg, playing in a field of 18×12m. The rules of the game are similar to the official FIFA rules, with required changes to adapt for the playing robots [2]. Each robot is autonomous and has its own sensorial means. They can communicate with each other, and with an external computer acting as a coach, through a wireless network. This coach computer cannot have any sensor, it only knows what is reported by the playing robots. The agents should be able to evaluate the state of the world and make decisions suitable to fulfill the cooperative team objective. CAMBADA, Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture, is the Middle Size League Robotic Soccer team from the University of Aveiro. The project started in 2003, coordinated by the IEETA 2 Instituto de Engenharia Electrónica e Telemática de Aveiro – Aveiro’s Institute of Electronic and Telematic Engineering. 2 ATRI 3 Actividade Transversal em Robótica Inteligente – Transverse Activity on Intelligent Robotics. 3 group and involves people working on several areas for building the mechanical structure of the robot, its hardware architecture and controllers and the software development in areas such as image analysis and processing, sensor and information fusion, reasoning and control (see Fig. 1 ). This paper provides a description of some sensor and information fusion techniques and algorithms used in the CAMBADA team. The data obtained by these techniques are necessary for building a world model of the robot environment. This paper includes the description of some of the elements of that model necessary for a team of robots to play soccer. In Section 2, a brief overview of some related topics and work in sensor and information fusion for world modeling are presented. Section 3 presents the team self-localization description, introducing it as the first necessary step for all the other information fusion. In Section 4, the ball integration process is presented in all its components, starting with the ball position, its velocity and finally its sharing among team mates. Section 5 presents an overview of obstacle treatment, with some visual detection details, the matching of positions for visual identification and the sharing of information among team mates. Finally, Section 6 concludes the paper. 2 Related work World modeling and sensor and information fusion are tightly related, as the latest provide the means to build the desired model. Sensor and information fusion is the process of combining sensory data, or data derived from sensory data, providing a resulting information that is better than would be possible when the sources were used individually [3]. One of the main areas where sensor fusion techniques are used is position tracking, both for self and object localization/tracking. The integration of information over time in order to filter sensor noise is essential to get better estimates. This type of integration may be performed using Kalman filter based approaches, Monte Carlo methods or Markov approaches. Generally, Monte Carlo [4] approaches have better performance in cases where great discontinuities of the output values are expected, as the assumption of Gaussian probability density functions of the Kalman filter [5] is usually less accurate. However, Kalman filtering is a very effective method if the assumptions of Gaussian noise can be met and the system can be linearized. Other common approaches are the use of the Extended and Unscented Kalman filters [6], which are prepared to deal with non-linear systems at the cost of more computational weight. A general overview of different methods of multi-sensor and information fusion is presented in [7], also with a brief description of application areas, such as robotics, military, biomedical and transportation. Applications in the robotics field include self-localization using either Kalman filter [8], Monte Carlo [9] or Markov [10] methods, or integration of information coming from several robots, to increase the accuracy of each of the robots position estimation [11]. A general recent overview of methods and architectures for multi-sensor data fusion can be found in [12]. Another recurrent problem nowadays is the fusion of visual and inertial sensors [13], where recent results have demonstrated that the visual tracking of objects may work at higher velocities and be more robust if combined with information coming from inertial sensors [14] and also that ego-motion estimation can be more precise and navigation more robust using these approaches [15]. Simultaneous Localization And Mapping (SLAM) is another common application of sensor fusion techniques, as in many cases, autonomous robots have to map the environment rather than simply localize themselves [16,17]. Particularly in RoboCup domain, several teams use this kind of approaches, not only for localization purposes, but also for position estimation and tracking of objects, namely the ball and other robots. Several teams have used Kalman filters for the ball position estimation [18–21]. In [20,21], several information fusion methods are compared for the integration of the ball position using several observers. In [21], the authors conclude that the Kalman reset filter shows the best performance. Although using well known techniques, in this paper we propose practical solutions for an efficient self-localization, ball information treatment and obstacle treatment for an MSL robotic soccer team. As far as we know, no previous work has been published focusing on these several important aspects of developing the world model of an MSL soccer team. 3 Localization Self-localization of the agent is an important issue for a soccer team, as strategic moves and positioning must be defined by positions on the field. In the MSL, the environment is partially known, as every agent knows exactly the layout of the game field but does not know the position of any other elements, either itself, other robots or the ball. Given the known map, the agent has then to locate itself. The CAMBADA team localization algorithm is based on the detected field lines, with fusion of information from the odometry sensors and an electronic compass. It is based on the approach described in [22], with some adaptations. It can be seen as an error minimization task, with a derived measure of reliability of the calculated position so that a stochastic sensor fusion process can be applied to increase the estimation accuracy [22]. From the center of the image (the center of the robot), radial sensors are created around the robot, each one represented by a line with a given angle. These are called scanlines. The image processing, in each cycle, returns a list of positions relative to the robot where the scanlines intercept the field line markings [23]. The idea is to analyze the detected line points, estimating a position, and through an error function describe the fitness of the estimation. This is done by reducing the error of the matching between the detected lines and the known field lines (Fig. 2 ). The error function must be defined considering the substantial amount of noise that affects the detected line points which would distort the representation estimation [22]. In normal operation mode, the localization is done over a limited set of base positions from which tracking is maintained. Since it is an algorithm based on optimization and since there are many local minima, the tracking only works satisfactorily if the estimations are near the solution. In situations where the robot does not possess a valid estimation, a global localization algorithm estimates the robot position on the field using a much wider set of initial estimations over which the already referred error minimization process for optimization is applied. However, this global localization algorithm is computationally heavy and time consuming. For that reason, after having an initial position, the simpler tracking localization handles the cyclic relocation. Although the odometry measurement quality quickly degrades with time, within the reduced cycle times achieved in the application, consecutive readings produce acceptable results and thus, having the visual estimation, it is fused with the odometry values to refine the estimation. This fusion is based on a Kalman filter for the robot position estimated by odometry and the robot position estimated by visual information. This approach allows the agent to estimate its position even if no visual information is available. However, it is not reliable to use only odometry values to estimate the position for more than a few cycles, as slidings and frictions on the wheels produce large errors on the estimations in short time. Due to the nature of the approach, this algorithm works acceptably with a relatively low number of points, like a few tens of points, as long as they are representative of the surroundings. Consider the case of matching a 90degrees corner. If the algorithm had access to 200 points all over the same line, it would not be capable of matching the corner. On the other hand, with only 20 or 30 points scattered over both lines, the algorithm would be capable of detecting the match. Even in situations where the points are over the same line, the merging with odometry and position tracking provide a good robustness to the algorithm [22], as long as the situation is temporary, which is usually the case. The visually estimated orientation can be ambiguous, i.e. each point on the soccer field has a symmetric position, relatively to the field center, where the robot detects exactly the same field lines. To disambiguate the symmetry problem and to detect wrong estimations, an electronic compass is used. The orientation estimated by the robot is compared to the orientation given by the compass and if the error between them is larger than a predefined threshold, actions are taken. If the error is really large (i.e. around ±180degrees), it means that the robot estimated orientation is symmetric to the real one, so it should assume the mirror position. On the other hand, if the error is larger than the acceptance threshold (i.e. a 90degrees acceptable area), a counter is incremented (Fig. 3 ). This counter will be incremented every cycle in which the error is greater than the threshold. If a given number of consecutive cycles with high errors is reached (i.e. the counter reaches a given number, currently 10), the robot considers itself “lost”, meaning that it will not continue to track its position but will instead consider the initial situation, with no a priori knowledge and thus executes the global localization algorithm. Fig. 4 shows situations where the threshold was reached and relocation was forced after some cycles. 4 Ball integration The information of the ball state (position and velocity) is, perhaps, the most important, as it is the main object of the game and it is the base over which most decisions are taken. Thus, its integration has to be as reliable as possible. To accomplish this, a Kalman filter implementation was created to filter the estimated ball position given by the visual information, and a linear regression was applied over filtered positions to estimate its velocity. 4.1 Ball position It is assumed that the ball velocity is constant between cycles. Although that is not true, due to the short time variations between cycles, around 40ms, and given the noisy environment and measurement errors, it is a quite acceptable model for the ball movement. Thus, no friction is considered to affect the ball, and the model does not include any kind of control over the ball. Therefore, given the Kalman filter formulation (described in [24]), the assumed state transition model is given by X k = 1 Δ T 0 1 X k - 1 where X k = Pos Vel is the state vector containing the position and velocity of the ball. Both are composed by the respective (x, y) coordinates. This velocity is only internally estimated by the filter, as the robot sensors can only take measurements on the ball position. After defining the state transition model based on the ball movement assumptions described above and the observation model, the description of the measurements and process noises are important issues to attend. The measurements noise can be statistically estimated by taking measurements of a static ball position at known distances. In practice, measurements of the static ball were taken while the robot was rotating around its vertical axis and this was done with the ball placed at several distances, measured with metric tape. Although real game conditions are probably more adverse, we lack the means to externally know the position of the elements on the field. For that reason, to know the real distance between the robot and the ball, we opted to use the described setup. Some of the results are illustrated in Fig. 5 . The standard deviation of those measurements can be used to calculate the variance and thus define the measurements noise parameter. A relation between the distance of the ball to the robot and the measurements standard deviation can be modeled by a 2nd degree polynomial best fitting the data set in a least-squares sense. Depending on the available data, a polynomial of another degree could be used, but we should always keep in mind the computational weight of increasing complexity. As for the process noise, this is not trivial to estimate, since there is no way to take independent measurements of the process to estimate its standard deviation. The process noise is represented by a matrix containing the covariances correspondent to the state variable vector. Based on the Kalman filter functioning, one can verify that forcing a near null process noise causes the filter to practically ignore the read measures, leading the filter to emphasize the model prediction. This makes it too smooth and therefore inappropriate. On the other hand, if it is too high, the read measures are taken too much into account and the filter returns the measures themselves. To face this situation, one has to find a compromise between stability and reaction. Since we assume an uniform movement for the ball, there are no frictions or other external forces considered. This means that accelerations are not considered in our model and thus, the position and velocity components are quite independent of each other. Since acceleration is the main element of relation between position and velocity, we considered that the errors associated to the process position and velocity estimations do not correlate. Because we assume an uniform movement model that we know is not the true nature of the system, we know that the speed calculation of the model is not very accurate. A process noise covariance matrix was empirically estimated, based on several tests, so that a good smoothness/reactivity relationship was kept. These empirically estimated values were made dependent on the measurement noise so that the Kalman filter predictions are also less accurate when the distance to the ball is too large. This was done so that the filter does not smooth the positions too much. In practice, this approach proved to improve the estimation of the ball position. Since we do not possess the means to externally know the positions of the elements on the field, a capture was made with the ball fixed at a known position on the field (0.0,2.0) (measured with metric tape). The robot was moving around the ball with a speed of 1.3±0.5m/s and the ball position measured at each moment was recorded. The ball position measured by the robot was (−0.01,2.03)±(0.05,0.06)m. Fig. 6 illustrates the capture results. This experiment gives an idea of the noise associated with the ball position detection. Note that during the experiment the distance between the robot and the ball is around 2m. Comparing the ball position cloud with the one obtained at 2m in Fig. 5 one can verify that they are similar, which is consistent with the previous experiment setup to simulate robot movement by rotation on the spot. With the presented setup experiments, the existence of noise in ball measurements became clear. With that existent noise in mind, several tests were made to validate the use of the Kalman filter to reduce it. Fig. 7 represents a capture of one of those tests, a ball movement, where the black dots are the ball positions measured by the robot visual sensors and thus are unfiltered. Red stars 4 For interpretation of color in ‘Figs. 1,2,4-7,9-11,13-15,17-20’ the reader is referred to the web version of this article. 4 represent the position estimations after applying the Kalman filter. The robot position is represented by the black star in its center and its respective radius. The ball was thrown against the robot and deviated accordingly. It is easily perceptible that the unfiltered positions are affected by much noise and the path of the ball after the collision is composed of positions that do not make much physical sense. Although we lack the means to externally provide a ground truth for the ball position during its movements, the filtered positions seem to give a much better approximation to the real path taken by the ball, as they provide a path that physically makes more sense. After producing the a priori estimation of the ball position, this estimation is compared with the read measure to detect if the variation between them is too great. If the difference between them is consistently greater than a given threshold (estimated empirically), the filter can indicate that the ball suffered a hard deviation (Fig. 8 illustrates this concept). Although hard deviations are not a serious problem for the filter (as it quickly converges to the new positions), they are used for velocity convergence (as described in the next subsection). 4.2 Ball velocity The calculation of the ball velocity is a feature becoming more and more important over the time. It allows that better decisions can be implemented based on the ball speed value and direction. Assuming the same ball movement model described before, constant ball velocity between cycles and no friction considered, one could theoretically calculate the ball velocity by simple instantaneous velocity of the ball with the first order derivative of each component Δ D Δ T , being ΔD the displacement on consecutive measures and ΔT the time interval between consecutive measures. However, given the noisy environment, it is also predictable that this approach would be greatly affected by that noise and thus its results would not be satisfactory. Fig. 9 shows a ball movement capture where the ball was moving from left to right, as indicated by the arrow in the top of the figure, and was then deviated into a downward movement near the “1st deviation” tag. While moving downward, the ball was deviated again near the “2nd deviation” tag and started to move from right to left. Finally, in the end of the capture, a new deviation occurred near tag “3rd deviation” where the ball started to move upward. The estimated ball positions are represented by the blue dots. Red lines represent the velocity vectors estimated based on consecutive positions displacement. It is clear that the velocity estimates hardly give an acceptable insight of the ball movement. To keep a calculation of the object velocity consistent with its displacement, an implementation of a linear regression algorithm was chosen. This approach based on linear regression [25] is similar to the velocity estimation described in [18]. By keeping a buffer of the last m measures of the object position and sampling instant (in this case buffers of nine samples were used), one can calculate a regression line to fit the positions of the object. Since the object position is composed by two coordinates (x, y), we actually have two linear regression calculations, one for each dimension. This is made in a transparent way, so the description is presented generally, as if only one dimension was considered. When applied over the positions estimation, the linear regression velocity estimations are much more accurate than the instant velocities calculated by Δ D Δ T , and allow a better insight of the ball movement. The same ball movement capture described earlier is represented in Fig. 10 , this time with the velocity vectors estimated by the linear regression applied over the position estimations provided by the Kalman filter. In order to try to make the regression converge more quickly on deviations of the ball path, a reset feature was implemented. This allows deletion of the older values, keeping only the n most recent ones, and provides control of the buffer size. By keeping the most recent values after a hard deviation, we reduce outliers of the previous path, thus promoting faster convergence. This reset results from the interaction with the Kalman filter described earlier by querying it for the existence of a hard deviation on the ball path. The obtained values were tested to confirm if the linear regression of the ball positions was more precise and would converge faster than the internal velocity estimated by the Kalman filter. Tests showed that the velocity estimated by the Kalman filter has a slower response than the linear regression estimation when deviations occur. Given this, the linear regression was used to estimate the velocity because quickness of convergence was preferred over the slightly smoother approximation of the Kalman filter in the steady state. That is because in the game environment the ball is very dynamic, it constantly changes its direction and thus a convergence in less than half the cycles is much preferred. Fig. 11 shows the results for a theoretical velocity scenario where the ball was moving at a constant speed of 2m/s and suddenly dropped to a constant 1m/s speed. Both the speeds estimated by the Kalman filter and the ones estimated by the linear regression are presented. 4.3 Team ball position sharing Due to the highly important role that the ball has in a soccer game, when a robot cannot detect it by its own visual sensors (omni or frontal camera), it may still know the position of the ball, through sharing of that knowledge by the other team mates. The ball data structure includes a field with the number of cycles it was not visible by the robot, meaning that the ball position given by the vision sensors can be the “last seen” position. When the ball is not visible for more than a given number of cycles, the robot assumes that it cannot detect the ball on its own. When that is the case, it uses the information of the ball communicated by the other running team mates to know where the ball is. This can be done by getting the mean and standard deviation of the positions of the ball seen by team mates. Another approach is to simply use the ball position of the team mate that has more confidence in the detection. Independently of the chosen approach, the robot assumes that ball position as correct. When detecting the ball on its own, there is also the need to validate that information. Currently the seen ball is only considered if it is within a given margin inside the field of play as there would be no point in trying to play with a ball outside the field. For ball position sharing, an approach based on the highest confidence ball position is used. This is due to the fact that the shared positions are updated with 100ms periods, with the possibility of a few more milliseconds of unknown and unpredictable delay in packet transmission. Thus, the lifetime of the information of each team mate is different, and the use of the information of the team mate with higher confidence reduces the probability of the degradation of that information during the respective lifetime. Fig. 12 illustrates the general ball integration activity diagram. 5 Obstacle treatment While playing soccer, the robots have the need to navigate around the field effectively, which means they have to reposition themselves or dribble the ball avoiding the obstacles on the field, that can be either team or opponent robots, or eventually the referee. An increasing necessity felt by the team, to improve its performance, is a better obstacle detection and sharing of obstacle information among team mates. This is important to ensure a global idea of the field occupancy, since the team formation usually keeps the robots spread across the field. Pass lines and dribbling corridors can be estimated more easily with a good coverage of field obstacles, allowing improvements on team strategy and coordination. 5.1 Visual obstacle detection The CAMBADA robots gather their information about the surroundings by means of a robotic vision system. Currently, only the omni directional camera gathers information about obstacles, as no frontal camera is being used at this time. According to RoboCup rules, the robots are mainly black. Since during the game robots play autonomously, all obstacles in the field are the robots themselves (occasionally the referee, which is recommended to wear black/dark pants). The vision algorithm detects the obstacles by evaluating blobs of black color inside the field of play [26]. Through the mapping of image positions to real metric positions [27], obstacles are identified by their center (triangle on the processed image, Fig. 13 b) and left and right limits (squares on the processed image, Fig. 13b). This is done by searching black regions on the scanlines of the vision algorithm [23], already referred in Section 3. The detection of black color on the scanlines is analyzed both in angular intervals and length intervals, to define the limits of each black blob (considering their base points which are represented by the first black pixel in each scanline). Since the vision system is a non-SVP hyperbolic catadioptric system [27], the size of objects on the image varies with the distance to the robot. Due to an inverse distance map calculation, by exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface, the relation of distances in the image and the real world is known. Fig. 14 is an illustration of how the distance in pixels, from the center of the image, is mapped to the distance in meters, on the ground plane. Through the function represented in Fig. 14, it is possible to create a normalized relation of blobs width and length with the distance. Sometimes an obstacle is separated in several blobs, mainly due to the noise in the image and problems in color classification, which leads to failure in the detection of black regions in the scanlines. To avoid these situations, an offset is considered to decide when the angular space between blobs is considered enough to represent a real obstacle separation. The same principle is considered concerning the position of the black area in consecutive scanlines. The separation offsets of a blob close to the robot are bigger than the ones at a high distance, to maintain coherent precision. The angular separation offset is considered for situations where robots are side-by-side, at the same distance, but there is no visual contact between each blob; the length separation offset is checked for situations where, on consecutive scanlines, there are blobs with visual contact but the robots are actually at different distances. Both situations are depicted in Fig. 15 . For each detected blob, their number of pixels is calculated and an estimation of the obstacles left and right limits, as well as their centers, is made. This information is made available to the integration process for filtering and treatment. 5.2 Obstacle selection and identification With the objective of refining the information of the obstacles, and have more meaningful and human readable information, the obstacles are selected and a matching is attempted, in order to try to identify them as team mates or opponents. Due to the weak precision at long distances, a first selection of the obstacles is made by selecting only the obstacles closer than a given distance as available for identification (currently 5m). Also, obstacles that are smaller than 10cm wide or outside the field of play margin are ignored. This is done because the MSL robots are rather big, and in-game situations small obstacles are not present inside the field. Also, it would be pointless to pay attention to obstacles that are outside the field of play, since the surrounding environment is completely ignorable for the game development. To be able to distinguish obstacles, identifying which of them are team mates and which are opponent robots, a fusion between the own visual information of the obstacles and the shared team mates positions is made. By creating a circle around the team mate positions with the robot radius (considered 22cm), a matching of the estimated center of visible obstacle area is made (Fig. 16 ), and the obstacle is identified as the corresponding team mate in case of a positive matching (Figs. 17 c and 18c). This matching consists on the existence of interception points between the team mate circle and the obstacle circle or if the obstacle center is inside the team mate circle (the obstacle circle can be smaller, and thus no interception points would exist). Since the detected obstacles can be large blobs, the above described identification algorithm cannot be applied directly to the visually detected obstacles. If the detected obstacle fulfills the minimum size requisites already described, it is selected as candidate for being a robot obstacle. Its size is evaluated and classified as robot if it does not exceed the maximum size allowed for MSL robots [2] (Fig. 17a and b). If the obstacle exceeds the maximum size of an MSL robot, a division of the obstacle is made, by analyzing its total size which is used to estimate how many robots are in that obstacle. This may be a common situation, robots clashing together and thus creating a compact black blob, originating a big obstacle if they are sufficiently lined up (Fig. 18 a and b). Although the computations for obstacle identification were in use during RoboCup 2009, their results are yet to be considered in the team strategy. Currently, obstacles are always considered unfriendly and thus to be avoided. Due to this fact, there is currently no data of in-game results for this part of the work. Several captures of the obstacle identification algorithm described earlier were performed and analyzed, to further illustrate the effectiveness of the algorithm. The laboratory used for the tests receives natural light which can affect the vision processing algorithms. The presented results are not treated in any way to diminish the effects of natural light, as we are interested in understanding if the algorithms can cope with those conditions which can be found in real situations. In the first test situation, a robot was positioned on the field at (−0.05,1.88) while broadcasting its position. This robot will be referred to as pivot. Another robot was moving on a rectangular path around the pivot, and a capture of its data was done. This robot will be referred to as observer. This scenario is intended to give some insight about the performance of the identification when the team mates are static or nearly static (as is the case of set plays during the games. In these situations it is important to analyze passing lines). Fig. 19 is a graphic representation of the acquired data, with the pivot represented in black. The blue dots are the positions of the path taken by the observer, which covers the rectangular path for three times. In each cycle, the center of the obstacle perceived by the observer is represented by a red ‘×’. It is visible that, as expected, the obstacle position perceived by the observer is not exactly the pivot position. The capture in question is composed of 677 cycles. The identification of the obstacle as the correspondent team mate failed to succeed in only one cycle, which corresponds to a 99.85% success rate. Considering that the pivot has 22cm radius (although it is slightly bigger), the mean of the centers of the perceived obstacle is within the real area occupied by the pivot, at nearly 16cm with a standard deviation of 10cm (Table 1 ). Another test scenario was considered for evaluation of the algorithm performance for moving obstacles. Several captures were performed to evaluate the performance of the algorithm when identifying a moving team mate. This set of six captures consisted on a robot observing a team mate moving around and registering the data about the obstacles. The path taken by the moving team mate is represented in Fig. 20 . The number of failed identifications was greater when the moving robot was farther from the observer, as expected due to the noisy nature of the measurements. The captures were performed throughout the day, with different lighting conditions but with the same robot calibration. Table 2 summarizes this set of captures, which revealed a total mean identification ratio of approximately 71%. 5.3 Obstacle sharing With the purpose of improving the global perception of the team robots, the sharing of locally known information is an important feature. Obstacle sharing allows the team robots to have a more global perception of the field occupancy, allowing them to estimate, for instance, passing and dribbling corridors more effectively. However, one has to keep in mind that, mainly due to illumination conditions and eventual reflective materials, some of the detected obstacles may not be exactly robots, but dark shadowy areas. If that is the case, the simple sharing of obstacles would propagate an eventually false obstacle among the team. Thus the algorithm for sharing the obstacles makes a fusion of the several team mates information. The fusion of the information is done mate by mate. After building the worldstate by its own means, the agent checks all the available obstacle information provided by team mates, one by one. Their obstacles are matched with the own ones. If the agent does not know an obstacle shared by the team mate, it keeps it in a temporary list of unconfirmed obstacles. This is done to all the team mates obstacles. When another team mate shares a common obstacle, that same obstacle is confirmed and is transferred to the local list of obstacles. In the current cycle, the temporary obstacles that were not confirmed are not considered. A robot does not use negative information from other robots to remove obstacles it actually saw from its local world model. An outline of the algorithm is presented next. for c:=1 to total_number_of_team_mates for o:=1 to total_obstacles_of_team_mate for m:=1 to total_own_obstacles if m matches o I already know this obstacle, do nothing else if previously known by another team mate obstacle confirmed and added else obstacle considered temporarily waits for confirmation by another team mate endif endif endfor endfor endfor The matching of the team mate obstacles with the own obstacles is done in a way similar to the matching of the obstacle identification with the team mate position described earlier. The CAMBADA team mate position in Fig. 16 is replaced by the current team mate obstacle for the matching test. Fig. 21 shows a situation where robot 2, in the goal area was too far to see the obstacle on the middle of the field. Thus, it considered the obstacle in question, only because it is identified by both robots 5 and 6, as visible in the figure. 6 Conclusion and future work The techniques chosen for information and sensor fusion proved to be effective in accomplishing their objectives. The Kalman filter allows to filter the noise on the ball position and provides an important prediction feature which allows fast detection of deviations of the ball path. The linear regression used to estimate the velocity is also effective, and combined with the deviation detection based on the Kalman filter prediction error, provides a faster way to recalculate the velocity in the new trajectory. The improvement on obstacle treatment allows modifications on the overall team strategy, particularly regarding passing possibilities. It also allows the improvement of the robots movement, since team mate obstacles can have a different treatment than the opponents, because team mates have velocities and other information available. The CAMBADA team obtained the 1st place in the last years of the Portuguese robotics open (Robótica 2007, Robótica 2008, Robótica 2009 and Robótica 2010), and internationally achieved 5th place in RoboCup 2007, 1st place in RoboCup 2008, 3rd place in RoboCup 2009 and 2nd place in GermanOpen 2010. Although the described work proved to be effective and helped to achieve good results, improving is always the aim for this kind of project. Thus, improvements on the localization algorithm are desired, as well as a different way to disambiguate symmetric positions to eventually complement or replace the compass. Another path to follow would be the improving of team strategy based on obstacle identification, creating new forms of cooperation and set plays for in-game situations. Acknowledgment This work was partially supported by project ACORD Adaptive Coordination of Robotic Teams, FCT/PTDC/EIA/70695/2006. References [1] Kitano H, Asada M, Kuniyoshi Y, Noda I, Osawa E. RoboCup: the robot world cup initiative. In: Proceedings of the first international conference on autonomous agents. New York (NY, USA): ACM; 1997. p. 340–7. [2] MSL Technical Committee 1997–2009. Middle size robot league rules and regulations for 2009; 2008. [3] Elmenreich W. Sensor fusion in time-triggered systems. Ph.D. thesis. Vienna (Austria): Technische Universitat Wien, Institut fur Technische Informatik; 2002. [4] N. Metropolis S. Ulam The Monte Carlo method J Am Stat Assoc 44 247 1949 335 341 [5] R. Kalman A new approach to linear filtering and prediction problems J Basic Eng 82 1 1960 35 45 [6] Wan E, Merwe RVD. The unscented Kalman filter for nonlinear estimation. In: IEEE adaptive systems for signal processing, communications, and control symposium; 2000. p. 153–8. [7] R. Luo C. Yih K. Su Multisensor fusion integration: approaches, applications, and future research directions IEEE Sens J 2 2 2002 107 119 [8] J. Leonard H. Durrant-Whyte Mobile robot localization by tracking geometric beacons IEEE Trans Robotics Autom 7 3 1991 376 382 [9] Dellaert F, Fox D, Burgard W, Thrun S. Monte Carlo localization for mobile robots. In: IEEE international conference on robotics and automation; 1999. p. 1322–8. [10] D. Fox W. Burgard S. Thrun Markov localization for mobile robots in dynamic environments J Artif Intell Res 11 1999 391 427 [11] A. Mourikis S. Roumeliotis Performance analysis of multirobot cooperative localization IEEE Trans Robotics 22 4 2006 666 681 [12] H. Durrant-Whyte T. Henderson Multisensor data fusion B. Siciliano O. Khatib Springer handbook of robotics 2008 Springer [13] A. Bejczy J. Dias Editorial: integration of visual and inertial sensors J Robotic Syst 21 2 2004 41 42 [14] G. Alenyá E. Martínez C. Torras Fusing visual and inertial sensing to recover robot ego-motion J Robotic Syst 21 1 2004 23 32 [15] S. Chroust M. Vincze Fusion of vision and inertial data for motion and structure estimation J Robotic Syst 21 2 2004 73 83 [16] W.B.S. Thrun D. Fox Probabilistic robotics 2005 The MIT Press [17] B. Siciliano O. Khatib Springer handbook of robotics 2008 Springer [18] M. Lauer S. Lange M. Riedmiller Modeling moving objects in a dynamically changing robot application U. Furbach KI 2005: advances in artificial intelligence Lecture notes in computer science vol. 3698 2005 Springer 291 303 [19] Xu Y, Jiang C, Tan Y. SEU-3D 2006 soccer simulation team description. In: CD proc of RoboCup symposium 2006, Bremen, Germany; 2006. [20] Marcelino P, Nunes P, Lima P, Ribeiro MI. Improving object localization through sensor fusion applied to soccer robots. In: Proc scientific meeting of the portuguese robotics open – Robótica 2003, Lisbon, Portugal; 2003. [21] A. Ferrein L. Hermanns G. Lakemeyer Comparing sensor fusion techniques for ball position estimation A. Bredenfeld A. Jacoff I. Noda Y. Takahashi RoboCup 2005: robot soccer world cup IX Lecture notes in computer science vol. 4020 2006 Springer 154 165 [22] M. Lauer S. Lange M. Riedmiller Calculating the perfect match: an efficient and accurate approach for robot self-localization A. Bredenfeld A. Jacoff I. Noda Y. Takahashi RoboCup 2005: robot soccer world cup IX Lecture notes in computer science vol. 4020 2006 Springer 142 153 [23] Neves A, Martins D, Pinho A. A hybrid vision system for soccer robots using radial search lines. In: Lopes LS, Silva F, Santos V, editors. Proc of the 8th conference on autonomous robot systems and competitions, Portuguese robotics open – Robótica 2008, Aveiro, Portugal; 2008. p. 51–5. [24] Bishop G, Welch G. An introduction to the Kalman filter. In: Proc of SIGGRAPH, Course 8, No. NC 27599-3175. NC (USA): Chapel Hill; 2001. [25] Motulsky H, Christopoulos A. Fitting models to biological data using linear and nonlinear regression. GraphPad Software Inc.; 2003. [26] A. Neves G. Corrente A. Pinho An omnidirectional vision system for soccer robots J. Neves M.F. Santos J.M. Machado Progress in artificial intelligence Lecture notes in artificial intelligence vol. 4874 2007 Springer 499 507 [27] B. Cunha J. Azevedo N. Lau L. Almeida Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system U. Visser F. Ribeiro T. Ohashi F. Dellaert RoboCup 2007: robot soccer world cup XI Lecture notes in artificial intelligence vol. 5001 2008 Springer 417 424 "
    },
    {
        "doc_title": "Communicating among robots in the RoboCup Middle-Size League",
        "doc_scopus_id": "77951004199",
        "doc_doi": "10.1007/978-3-642-11876-0_28",
        "doc_eid": "2-s2.0-77951004199",
        "doc_date": "2010-04-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Best practice",
            "Communication protocols",
            "Fundamental component",
            "Limited bandwidth",
            "Mobile autonomous robots",
            "Multirobots",
            "RoboCup",
            "Robotic soccer",
            "Wireless communication protocols",
            "Wireless communications",
            "Wireless medium"
        ],
        "doc_abstract": "The RoboCup Middle-Size League robotic soccer competitions pose a real cooperation problem for teams of mobile autonomous robots. In the current state-of-practice cooperation is essential to overcome the opponent team and thus a wireless communication protocol and associated middleware are now fundamental components in the multi-robots system architecture. Nevertheless, the wireless communication has relatively low reliability and limited bandwidth. Since it is shared by both teams, it is a fundamental resource that must be used parsimoniously. Curiously, to the best of our knowledge, no previous study on the effective use of the wireless medium in actual game situations was done. In this paper we show how current teams use the wireless medium and we propose a set of best practices towards a more efficient utilization. Then, we present a communication protocol and middleware that follow such best practices and have been successfully used by one particular MSL team in the past four years. © 2010 Springer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sensor and information fusion applied to a robotic soccer team",
        "doc_scopus_id": "77950996272",
        "doc_doi": "10.1007/978-3-642-11876-0_32",
        "doc_eid": "2-s2.0-77950996272",
        "doc_date": "2010-04-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Adaptive buffer",
            "Information fusion techniques",
            "Localisation",
            "Multi-agent environment",
            "Noise variations",
            "Obstacle detection",
            "Robotic soccer team",
            "Sensor informations",
            "Team cooperation",
            "Team members",
            "Team performance",
            "Velocity estimation",
            "Visual information",
            "Visual sensor",
            "World model"
        ],
        "doc_abstract": "This paper is focused on the sensor and information fusion techniques used by a robotic soccer team. Due to the fact that the sensor information is affected by noise, and taking into account the multi-agent environment, these techniques can significantly improve the accuracy of the robot world model. One of the most important elements of the world model is the robot self-localisation. Here, the team localisation algorithm is presented focusing on the integration of visual and compass information. To improve the ball position and velocity reliability, two different techniques have been developed. A study of the visual sensor noise is presented and, according to this analysis, the resulting noise variation depending on the distance is used to define a Kalman filter for ball position. Moreover, linear regression is used for velocity estimation purposes, both for the ball and the robot. This implementation of linear regression has an adaptive buffer size so that, on hard deviations from the path (detected using the Kalman filter), the regression converges more quickly. A team cooperation method based on sharing of the ball position is presented. Besides the ball, obstacle detection and identification is also an important challenge for cooperation purposes. Detecting the obstacles is ceasing to be enough and identifying which obstacles are team mates and opponents is becoming a need. An approach for this identification is presented, considering the visual information, the known characteristics of the team robots and shared localisation among team members. The same idea of distance dependent noise, studied before, is used to improve this identification. Some of the described work, already implemented before RoboCup2008, improved the team performance, allowing it to achieve the 1st place in the Portuguese robotics open Robótica2008 and in the RoboCup2008 world championship. © 2010 Springer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Predictive control for behavior generation of omni-directional robots",
        "doc_scopus_id": "71049152967",
        "doc_doi": "10.1007/978-3-642-04686-5_23",
        "doc_eid": "2-s2.0-71049152967",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Behavior generation",
            "Continuous control",
            "Current peak",
            "Cycle time",
            "Discrete-time control",
            "Maximum acceleration",
            "Maximum velocity",
            "Medium size leagues",
            "Motion stability",
            "Omni-directional motion",
            "Omnidirectional robots",
            "Physical constraints",
            "Predictive control",
            "RoboCup",
            "Robot controls",
            "Robotic soccer team",
            "System delay"
        ],
        "doc_abstract": "This paper describes the approach developed by the CAMBADA robotic soccer team to address physical constraints regarding omni-directional motion control, with special focus on system delay. CAMBADA robots carry inherent delays which associated with discrete time control results in non-instant, non-continuous control degrading the performance over time. Besides a natural maximum velocity, CAMBADA robots have also a maximum acceleration limit implemented at software level to provide motion stability as well as current peaks avoidance on DC motors. Considering the previous constraints, such as the cycle time and the overall sensor-action delay, compensations can be made to improve the robot control. Since CAMBADA robots are among the slowest robots in the RoboCup Medium Size League, such compensations can help to improve both several behaviours as well as a better field coverage formation. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Obstacle detection, identification and sharing on a robotic soccer team",
        "doc_scopus_id": "71049141838",
        "doc_doi": "10.1007/978-3-642-04686-5_29",
        "doc_eid": "2-s2.0-71049141838",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Apriori",
            "Multi-agent applications",
            "Obstacle detection",
            "Position information",
            "Robotic soccer",
            "Robotic soccer team",
            "Search lines",
            "Visual detection",
            "World model"
        ],
        "doc_abstract": "When building a representation of the environment for a robot in a multi-agent application, as is the case of robotic soccer, sensor and information fusion of several elements of the environment are an important task. To build an increasingly better world model, one of the aspects that one should consider is the treatment of obstacles. This paper gives an insight of the general steps necessary for a good obstacle representation in the robot world model. A first step is the visual detection of the obstacles in the image acquired by the robot. This is done using an algorithm based on radial search lines and colour-based blobs detection, where each obstacle is identified and delimited. After having the visually detected obstacles, a fusion with a-priori known information about the obstacles characteristics allows the obstacle separation and filtering, so that obstacles that don't fill the criteria are discarded. With the position information shared by team mates, the matching of the obstacles and the team mates positions is also possible, thus identifying each of them. Finally, and with the purpose of having a team world model as coherent as possible, the robots are able to share the obstacle information of each other. The work presented in this paper was developed for the CAMBADA robotic soccer team. After achieving the 1st place in the Portuguese robotics open Robótica2008 and in the Robocup2008 world championship, the correct treatment of obstacles was one of the new challenges proposed among the team to improve the performance for the next competitions. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system",
        "doc_scopus_id": "50249091617",
        "doc_doi": "10.1007/978-3-540-68847-1_43",
        "doc_eid": "2-s2.0-50249091617",
        "doc_date": "2008-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autonomous mobile robots",
            "Back-propagation",
            "Catadioptric vision",
            "Distance maps",
            "Distributed architectures",
            "General solutions",
            "International symposium",
            "Mathematical properties",
            "Mirror surfaces",
            "Mobile robotics",
            "Non-SVP catadioptric",
            "Omnidirectional vision",
            "RoboCup",
            "Robot vision",
            "Robot-soccer",
            "Robotic vision",
            "Visualization",
            "World Cup"
        ],
        "doc_abstract": "The use of single viewpoint catadioptric vision systems is a common approach in mobile robotics, despite the constraints imposed by those systems. A general solution to calculate the robot centered distances map on non-SVP catadioptric setups, exploring a back-propagation ray-tracing approach and the mathematical properties of the mirror surface is discussed in this paper. Results from this technique applied in the robots of the CAMBADA team (Cooperative Autonomous Mobile Robots with Advanced Distributed Architecture) are presented, showing the effectiveness of the solution. © 2008 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On-line control of light intensity in a microalgal bioreactor using a novel automatic system",
        "doc_scopus_id": "42749094022",
        "doc_doi": "10.1016/j.enzmictec.2007.12.002",
        "doc_eid": "2-s2.0-42749094022",
        "doc_date": "2008-06-05",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Bioengineering",
                "area_abbreviation": "CENG",
                "area_code": "1502"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Applied Microbiology and Biotechnology",
                "area_abbreviation": "IMMU",
                "area_code": "2402"
            }
        ],
        "doc_keywords": [
            "Biomass on-line measurement",
            "Docosahexaenoic acids",
            "Pavlova lutheri",
            "Photobioreactor"
        ],
        "doc_abstract": "The influence of light intensity upon biomass and fatty acid productivity by the microalga Pavlova lutheri was experimentally studied using a novel device. This device was designed to automatically adjust light intensity in a photobioreactor: it takes on-line measurements of biomass concentration, and was successfully tested to implement a feedback control of light based on the growth rate variation. Using said device, batch and semicontinuous cultures of P. lutheri were maintained at maximum growth rates and biomass productivities - hence avoiding photoinhibition, and consequent waste of radiant energy. Several cultures were run with said device, and their performances were compared with those of control cultures submitted to constant light intensity; the biomass levels attained, as well as the yields of eicosapentaenoic and docosahexaenoic acids were calculated - and were consistently higher than those of their uncontrolled counterpart. © 2008 Elsevier Inc. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271361 291210 291808 291832 291847 291850 31 Enzyme and Microbial Technology ENZYMEMICROBIALTECHNOLOGY 2007-12-23 2007-12-23 2010-11-18T19:11:58 S0141-0229(07)00377-8 S0141022907003778 10.1016/j.enzmictec.2007.12.002 S300 S300.1 FULL-TEXT 2015-05-14T04:05:43.510284-04:00 0 0 20080605 2008 2007-12-23T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0141-0229 01410229 42 42 7 7 Volume 42, Issue 7 5 554 559 554 559 20080605 5 June 2008 2008-06-05 2008 Research papers article fla Copyright © 2008 Elsevier Inc. All rights reserved. ONLINECONTROLLIGHTINTENSITYINAMICROALGALBIOREACTORUSINGANOVELAUTOMATICSYSTEM MEIRELES L 1 Introduction 2 Experimental 2.1 Culture conditions 2.2 Culture performance 2.3 Processing setup 2.4 Analytical assays 2.5 Statistical analyses 3 Results 4 Discussion 5 Conclusions References YAMAGUCHI 1996 487 502 K BOROWITZKA 1996 185 191 M MIRON 1999 249 270 A OGBONNA 1999 289 297 J DUBINSKY 1985 61 65 Z GRIMA 1993 599 605 E EVERS 1991 254 259 E LEPAGE 1984 1391 1396 G THOMPSON 1996 379 391 P VOLKMAN 1991 1855 1859 J DYERBERG 1986 125 134 J REIS 1996 83 88 A MEIRELES 2003 50 55 L BOROWITZKA 1988 456 465 M MICROALGALBIOTECHNOLOGY ALGALGROWTHMEDIASOURCESALGALCULTURES MEIRELES 2002 1408 1413 L COHEN 1988 328 332 Z RICHARDSON 1983 157 191 K TATSUZAWA 1995 397 400 H MEIRELESX2008X554 MEIRELESX2008X554X559 MEIRELESX2008X554XL MEIRELESX2008X554X559XL item S0141-0229(07)00377-8 S0141022907003778 10.1016/j.enzmictec.2007.12.002 271361 2010-12-28T03:21:08.934525-05:00 2008-06-05 true 390526 MAIN 6 70320 849 656 IMAGE-WEB-PDF 1 si3 583 39 126 si2 1984 81 315 si1 593 21 134 gr1 15399 426 363 gr1 938 93 79 gr2 22269 233 359 gr2 2030 81 125 gr3 11315 254 355 gr3 1124 90 125 gr4 15080 259 356 gr4 1282 91 125 gr5 13366 257 360 gr5 1133 89 125 gr6 17263 250 356 gr6 1543 88 125 EMT 7750 S0141-0229(07)00377-8 10.1016/j.enzmictec.2007.12.002 Elsevier Inc. Fig. 1 Schematic diagram of the feedback protocol used to control light intensity. The previous and current values for biomass growth rate are denoted as μ 0 and μ 1, respectively. The predefined threshold for μ is denoted as Th. Fig. 2 Evolution of ash-free dry weight (AFDW) with time, for various initial biomass concentrations ((♢) 0.341mgL−1; (□) 0.592mgL−1) and cell number ((○) 0.341mgL−1; (▵) 0.592mgL−1), in the semicontinuous QSS stages. Fig. 3 Variation of growth rate with average light intensity (I av) for various initial biomass concentrations ((○) 0.341mgL−1; (□) 0.592mgL−1) – as obtained from Eq. (2), in the semicontinuous QSS stages. Fig. 4 Evolution of ash-free dry weight (AFDW) with time, under constant light intensity ((○) 10Wm−2; (□) 40Wm−2) and light control (▵), in batch cultures. Fig. 5 Evolution of ash-free dry weight (AFDW) with time, under constant light intensity ((○) 10Wm−2; (♢) 25Wm−2; (□) 40Wm−2) and light control (▵), in semicontinuous cultures. Fig. 6 Evolution of average light intensity (I av) with time, under light control, in batch (□) and semicontinuous cultures (○). Table 1 Specific growth rate (μ max and μ average) and productivity (Ψ) of batch and semicontinuous cultures, under constant and controlled light intensity I 0 (Wm−2) μ average (h−1) μ max (h−1) Ψ (mgL−1 d−1) Batch 10 0.0085 0.0104 67.69 40 0.0091 0.0112 111.5 Controlled 0.0103 0.0109 113.6 Semicontinuous 10 – 0.0071±0.0004 103.6±16.5 25 – 0.0085±0.0002 151.1±5.8 40 – 0.0115±0.0013 223.3±9.5 Controlled – 0.0129±0.0005 229.3±5.1 Table 2 Tukey's test of homogeneous subsets for growth of semicontinuous cultures, under constant and controlled light intensity I 0 (Wm−2) Number of replicates Subset for α =0.01 1 2 3 10 3 0.00707 25 3 0.00847 40 3 0.01188 Controlled 3 0.01293 Significance 1.000 1.000 0.037 Table 3 Fatty acid composition of batch and semicontinuous cultures, under constant and controlled light intensity Fatty acid content (%, w/w) I 0 (Wm−2) Controlled 10 25 40 Semicontinuous Batch Semicontinuous Batch Semicontinuous Batch Semicontinuous Batch C14:0 8.92 8.08 9.56 9.50 8.89 – 10.04 8.61 C16:0 24.67 29.47 22.56 26.01 23.47 – 23.07 30.13 C16:1(n-7) 25.20 18.36 26.05 24.84 26.10 – 28.16 27.07 C18:0 0.47 2.98 0.42 0.50 0.43 – 0.57 0.70 C18:1(n-9) 1.91 6.92 1.87 2.03 2.77 – 2.38 2.96 C18:1(n-7) 1.48 2.92 1.89 2.24 1.32 – 1.16 1.37 C18:2(n-6) 2.60 2.83 2.65 1.59 3.35 – 3.31 3.39 C18:3(n-3) 1.49 1.00 1.49 1.72 1.44 – 0.80 0.79 C18:4(n-3) 4.54 3.01 6.39 5.56 5.57 – 5.37 3.65 C20:3(n-3) 0.67 0.65 0.49 0.98 0.18 – 0.10 0.92 C20:5(n-3) 19.19 16.79 17.85 18.12 18.27 – 16.06 12.68 C22:6(n-3) 8.85 6.99 8.78 6.90 8.21 – 8.98 7.73 Table 4 Overall yield (mgL−1 d−1) of EPA and DHA of batch and semicontinuous cultures, under constant and controlled light intensity I 0 (Wm−2) EPA DHA Batch 10 0.403 0.153 40 0.756 0.460 Controlled 0.820 0.452 Semicontinuous 10 0.556 0.291 25 0.933 0.419 40 0.763 0.427 Controlled 1.105 0.560 On-line control of light intensity in a microalgal bioreactor using a novel automatic system Luís A. Meireles a A. Catarina Guedes a Catarina R. Barbosa a José L. Azevedo b João P. Cunha b F. Xavier Malcata a ⁎ a Escola Superior de Biotecnologia, Universidade Católica Portuguesa, Rua Dr. António Bernardino de Almeida, P-4200-072 Porto, Portugal b Departamento de Electrónica e Telecomunicações – IEETA, Universidade de Aveiro, P-3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 22 5580004; fax: +351 22 5090351. The influence of light intensity upon biomass and fatty acid productivity by the microalga Pavlova lutheri was experimentally studied using a novel device. This device was designed to automatically adjust light intensity in a photobioreactor: it takes on-line measurements of biomass concentration, and was successfully tested to implement a feedback control of light based on the growth rate variation. Using said device, batch and semicontinuous cultures of P. lutheri were maintained at maximum growth rates and biomass productivities – hence avoiding photoinhibition, and consequent waste of radiant energy. Several cultures were run with said device, and their performances were compared with those of control cultures submitted to constant light intensity; the biomass levels attained, as well as the yields of eicosapentaenoic and docosahexaenoic acids were calculated – and were consistently higher than those of their uncontrolled counterpart. Keywords Pavlova lutheri EPA DHA Biomass on-line measurement 1 Introduction Microalgae are currently cultivated to produce a vast number of high added-value products, e.g. pigments and polyunsaturated fatty acids [1]. Production on the industrial level is usually performed in open ponds or raceways; however, they often lead to low biomass productivity, so they are restricted to only a few species [2]. Several closed systems – usually of the tubular or flat-panel types, have been developed [3]; however, they present difficulties for effective control, require a large area of land and are expensive to operate. Therefore, compact and sterilizable photobioreactors are urged [4]. One of the major parameters that affect microalga growth is light [2]; hence, a light control system is desirable for closed photobioreactors. It is well known that, in a batch culture run under constant light intensity provided externally, the amount of light actually available to cells is affected by mutual shading [5,6]; this affects negatively both their growth rate and biochemical composition. On the other hand, excess light can cause photoinhibition, thus wasting energy and promoting cell death. Therefore, assessment of the light available for photosynthesis throughout culture time is an important step toward accurate and continuous control of light intensity. A model that describes light-limited growth of microalgae in steady-state, continuous cultures was proposed by Evers [7] – and later applied by Grima et al. [6]; this model takes into account the average light intensity inside a photobioreactor, as a function of incident light and biomass concentration. Use of this type of models allows calculation of the average light intensity associated with the maximum growth rate, hence avoiding photoinhibition; this feature is of major importance in what concerns light control. In this work, such a model was (for the first time) successfully applied to semicontinuous cultures under quasi-steady-state conditions. Recall that Pavlova lutheri is widely employed to feed fish, bivalves and crustaceans, owing to its high content of polyunsaturated fatty acids [8–10] – mainly eicosapentaenoic (EPA) and docosahexaenoic (DHA) acids, which are claimed to be beneficial for human health [11,12]. Our research effort encompassed growth of P. lutheri in a closed photobioreactor, operated either batch- or semicontinuous-wise. An automatic feedback system for control of light intensity – that uses information pertaining to growth rate, was thus designed and tested. The production rates of both EPA and DHA by that microalga were assayed under various conditions of light and growth rate, so as to elucidate the features of said controlled process in a model system. 2 Experimental 2.1 Culture conditions A genetically improved strain (II#2) of P. lutheri [13] was employed, using artificial sea water, ASW [14], as cultivation medium. Cultures of 1.5L were performed in a 2-L bioreactor (Braun, Germany), with height and internal diameter of 240 and 130mm, respectively – under both batch and semicontinuous modes. The temperature was maintained at 20±0.5°C via a refrigeration jacket, the stirring rate was set to 50rpm, and the pH was kept at 8.0±0.2 by addition of 1M NaOH or HCl, as appropriate. All these parameters were controlled with a Biostat B unit (Braun). Pure air, enriched with 0.3% (v/v) CO2, was bubbled at the bottom of the bioreactor, at a volumetric flow rate of 0.54 L L culture − 1 min − 1 . 2.2 Culture performance Quasi-steady-state (QSS), semicontinuous cultures were obtained with daily dilutions, and adjusted every day so as to maintain the same initial biomass concentration at each new stage. For every light intensity level considered, several stages (3–7, depending on the actual culture conditions at stake) were performed – until three consecutive (equal) dilutions were made, in order to maintain the initial biomass concentration; the culture was considered to have achieved QSS immediately afterwards. Two different initial biomass concentrations were tested, and submitted to several incident light intensities (I 0) – so as to obtain desired average light intensities, regardless of I 0 and biomass concentration. 2.3 Processing setup A biomass monitoring system – reported elsewhere in detail by Meireles et al. [15], was coupled to an electronic device for light control. Light was provided by a set of 16 fluorescent lamps (OSRAM 18W/21-840) placed vertically, and uniformly distributed around the reactor walls at ca. 10cm from its surface. Assurance of a good spatial distribution of light is crucial, so lamps were used in pairs of opposite lamps; each pair accounted for 8Wm−2. An electronic device, connected to the computer via an RS232 serial port, was built and installed; this device was aimed at controlling the number of lamps switched on at each time. Periodical measurements of biomass concentration in the bioreactor were performed by said device, according to a pre-defined schedule (i.e. 4h); the specific growth rate (μ), calculated from each pair of two consecutive samples, was calculated and compared with the previous value – and a decision was automatically taken, according to the flowchart depicted in Fig. 1 . The threshold (in Fig. 1) was pre-defined, and depended on the culture being studied; said figure is the minimum variation of μ that can be taken as significant (and not merely caused by background noise, of the biomass monitoring system or the culture). The initial light intensity was manually set to slightly above light limitation; when a sample was taken for which μ was not above the aforementioned threshold, the system assumed that there was no variation in biomass concentration – so light intensity was maintained (this step is crucial when there is a lag phase, characterized by a low growth rate); when a sample was taken for which μ was above said threshold, the system assumed that biomass concentration had undergone an increase – so one of two actions was taken: when the current value of μ was not below the previous one, the system assumed that light is not limiting, so light intensity was maintained; conversely, when the current value of μ was lower than the previous one, the system assumed that the culture is light-limited, so light intensity was increased by one increment (i.e. 8Wm−2). The software developed previously for the biomass monitoring system [15] was essentially used as such, but a new Control menu was added. In this improved version, it was possible to define whether light would be automatically controlled or not, to include calibration curves for biomass, and to input growth rate thresholds. A new field was also included in the Status menu – where the current light intensity was recorded, so that it could also be deliberately changed. 2.4 Analytical assays The incident light intensity (I 0) was measured using a luxmeter (LI-1000 Data Logger, from LI-COR, USA). The average light intensity (I av) inside the reactor was calculated following the model by Evers [7]. This model is able to describe the light distribution within a cylindrical transparent vessel, assuming that the attenuation of I 0, caused by mutual-shading, obeys Lambert–Beer's law; the final equation reads (1) I av ( C ) = I 0 π R ∫ 0 R ∫ 0 π exp ( − K a C [ ( R − S ) cos φ + R 2 − ( R − S ) 2 sin 2 ϕ ] ) d φ d S where C is the biomass concentration; R is the radius of the vessel; S is the distance of a given point to the reactor wall; ϕ is the angle of the light path with the axis of symmetry of the reactor; and K a is the absorption coefficient of the microalga. In the case of P. lutheri, the value taken for K a was 0.428m2 g−1 – following the method described by Grima et al. [6]. The ash-free dry weight (AFDW) was determined by filtering 10ml of culture through pre-conditioned GF/C glass fiber filters (Whatman, UK), drying at 100°C to constant weight, and finally heating to 550°C for 1h. Fatty acid analysis took place on freeze-dried samples, and used gas chromatography after direct trans-methylation – according to the acidic method described by Lepage and Roy [8], after the modifications introduced by Cohen et al. [16]. Heptadecanoic acid was used as internal standard, and acetyl chloride was used as catalyst. The resulting esters were analyzed in a gas chromatograph (Perkin Elmer, USA), using detection by flame ionization; resolution was via a 60-m fused silica, capillary column Supelcowax-10 (Supelco, USA), using helium as carrier gas in splitless mode; sample injection and detection occurred at 250 and 270°C, respectively; and the oven heating program included an increase of the column temperature linearly from 170 to 250°C, at a rate of 1°Cmin−1. Pure standards of free fatty acids (Sigma, USA) were used for tentative identification and quantification, based on comparison of retention times. 2.5 Statistical analyses Statistical analyses of variance (ANOVA) and Tukey's tests (when ANOVA indicated at least one significantly different result) were applied to the data generated in the semicontinuous cultures, both under constant and controlled light intensities, taking advantage of the three replicates available – so as to identify differences in growth rate. 3 Results To study the influence of I av on the growth rate of P. lutheri, several QSS stages – characterized by as many values of I av, were achieved, so as to correspond to distinct biomass concentrations and incident lights. Two biomass concentrations were accordingly chosen, coupled with a wide range of I av – in attempts to reduce mutual correlation of the data. The biomass concentrations in the various QSS stages are depicted in Fig. 2 , in terms of cell number and ash-free dry wheight. The model by Evers [7] and Grima et al. [6] assumes that uptake of light depends on light intensity: said functionality is characterized by a hyperbolic curve containing an exponential parameter (n), and is able to fit growth kinetics to the sigmoidal effect observed at low light intensities. The Hill-type rate expression finally takes the form (2) μ = μ max I 0 n I 0 n + I k n − m where I k is a constant that represents the affinity of cells to light; and m is the specific maintenance rate. For each QSS stage, μ was duly calculated based on the slope of the log(C) vs. time curve. The variation of μ as a function of I av is plotted in Fig. 3 . Non-linear regression analysis was performed – and the best estimates found for the parameters in Eq. (2) were:μ max =0.0148h−1; I k =0.3Wm−2; m =−0.0018h−1; n =1.96; and r 2 =0.999. Several batch and semicontinuous cultures were performed using the proposed light control device, and compared with runs carried out under constant light intensity. The initial light intensity in the controlled cultures was set at ca. 0.7Wm−2, whereas three constant values for I 0 (i.e. 10, 25 and 40Wm−2) were tested (except for 25Wm−2 in batch mode). Comparison between the controlled and the constant light cultures – in both batch and semicontinuous modes, is presented in Figs. 4 and 5 , respectively; the kinetic parameters are in turn tabulated in Table 1 . The results of Tukey's tests, applied to homogeneous subsets – in terms of differences in μ max of semicontinous cultures, are included in Table 2 . Significant differences (P <0.01) do exist between the growth rates – except for cultures grown at 40Wm−2 and under light control. The typical variations of I av, throughout batch and semicontinuous culture time under controlled light, are shown in Fig. 6 ; the overall average light intensity was 0.622 and 0.661Wm−2, in the batch and semicontinuous modes of operation, respectively. Each increment corresponds to the increase in I 0 introduced by switching on an extra set of lamps; the value for I av then decreases, until a new light increment is triggered. The results pertaining to the fatty acid profile of the various cultures, performed with and without light control, are presented in Table 3 – for the batch and semicontinuous modes. The overall EPA and DHA yields of these cultures are summarized in Table 4 . Owing to the higher biomass productivity – associated with the higher EPA and DHA yields, light-controlled cultures eventually exhibited the higher EPA and DHA productivities. 4 Discussion Inspection of the results depicted in Figs. 2 and 3, coupled with Eq. (2), one concludes that it is possible to anticipate when cultures will be light-limited or not; in order to attain growth rates close to the maximum (hence avoiding excess I 0, that would lead to both photoinhibition and energy waste), it was assumed that I av should lie in the vicinity of 0.65Wm−2. The first datum in Fig. 3 was obtained when I 0 was set equal to zero (i.e. no light was provided to the reactor) – and thus represents the maintenance rate (m) obtained from Eq. (2). The data in Table 1 indicate that, under constant light intensity, both the growth rate and the productivity increase with increasing I 0 – as expected, and as previously confirmed [5,17]. However, there was no statistically significant difference between the maximum growth rate and the productivity of cultures run at the highest I 0 tested (i.e. 40Wm−2) and of cultures with on-line control of light intensity. Finally, it is apparent in Fig. 4 that the batch cultures using on-line control of light experienced no lag-phase – unlike what happened with cultures run under 40Wm−2; this result is easily accounted for by photoinhibition, which comes into play when excess light is provided at the startup of experiments. In view of the above, it was possible to achieve the maximum productivity with lower consumption of radiant energy, and with essentially no photoinhibition – provided that the light control system kept the light intensity below limitation at all times. (Note that it was considered that the actual value of I av should be ca. 0.65Wm−2; and that the light intensity, in batch and semicontinuous modes of operation under light control, was accordingly maintained in the vicinity of that value – with an actual average light intensity of 0.622 and 0.661Wm−2, respectively.) The maximum growth rate achieved (i.e. 0.0129h−1, see Table 1) is close to the maximum one predicted by Eq. (2); the slight gap between them is probably due to limitations arising from non uniform nutrient distribution. The major fatty acid residue constituents of the microalgal lipids were C16:0, C16:1(n-7), C18:4(n-3), C20:5(n-3) – EPA and C22:6(n-3) – DHA, which accounted for more than 75% of the total fatty acid inventory; similar results were reported for P. lutheri by Tatsuzawa et al. [18]. It is important to emphasize that, for the experimental light-controlled cultures, the fractions of EPA and DHA, referred to total fatty acid content, were rather high; EPA accounted for ca. 17 and 19% of the total fatty acids in batch and semicontinuous cultures, respectively, whereas DHA accounted for 7 and 9%, respectively. The amount of saturated fatty acids increased with increasing light intensity – as reported previously by Grima et al. [6]. Among the physiologically (and hence commercially) most important PUFA, EPA presented an atypical variation – with a slight increase in content at medium light intensity (25Wm−2), followed by a decrease at the highest light intensity; DHA content was, in turn, highest at the highest light intensity. In light-controlled cultures, the content of C14:0, C16:0 and C16:1(n-7) was always lower than in their counterparts run at a constant light intensity 40Wm−2 (except for C16:0 in semicontinuous mode). Moreover, both EPA and DHA contents lied near the maximum observed, under all conditions tested. Although the final concentration of individual fatty acids is an important feature, the fatty acid and the biomass yields are more important features towards eventually wider applicability; the results in Table 3, pertaining to semicontinuous cultures make it apparent that – despite a slightly smaller concentration, EPA and DHA yields were considerably higher in controlled cultures (likely because of the higher biomass productivity attained). 5 Conclusions Microalgae are currently used as polyunsaturated fatty acid-rich biomass in the aquaculture industry, so systems that automatically monitor and control algal culture parameters are urged. Since light is a crucial parameter in microalgal growth and metabolite production, its control is required if the highest production yields are sought. In this research effort, a simple system aimed at controlling light intensity in a microalgal bioreactor was designed and tested; said system permitted maximum biomass and PUFA yields to be achieved at the minimum expense of light energy – and performed better than resorting to the classical, constant light processing approach. References [1] K. Yamaguchi Recent advances in microalgal bioscience in Japan, with special reference to utilization of biomass and metabolites: a review J Appl Phycol 8 1996 487 502 [2] M.A. Borowitzka Closed algal photobioreactors: design considerations for large-scale systems J Mar Biotech 4 1996 185 191 [3] A.S. Mirón A.C. Gómez F.G. Camacho E.M. Grima Y. Chisti Comparative evaluation of compact photobioreactors for large-scale monoculture of microalgae J Biotechnol 70 1999 249 270 [4] J.C. Ogbonna T. Soejima H. Tanaka An integrated solar and artificial light system for internal illumination of photobioreactors J Biotechnol 70 1999 289 297 [5] Z. Dubinsky R. Matsukawa I. Karube Photobiological aspects of algal mass culture J Mar Biotech 2 1985 61 65 [6] E.M. Grima J.A.S. Perez F.G. Camacho J.L.G. Sanchez D.L. Alonso n-3 PUFA productivity in chemostat cultures of microalgae Appl Microbiol Biotechnol 38 1993 599 605 [7] E.G. Evers A model for light-limited continuous cultures – growth, shading, and maintenance Biotechnol Bioeng 38 1991 254 259 [8] G. Lepage C.C. Roy Improved recovery of fatty acid through direct transesterification without prior extraction or purification J Lipid Res 25 1984 1391 1396 [9] P.A. Thompson M. Guo P.J. Harrison Nutritional value diets that vary in fatty acid composition for larval Pacific oyster (Crassostres gigas) Aquaculture 143 1996 379 391 [10] J.K. Volkman G.A. Dunstan S.W. Jeffrey P.S. Kearney Fatty-acids from microalgae of the genus Pavlova Phytochemistry 30 1991 1855 1859 [11] J. Dyerberg Linolenate-derived polyunsaturated fatty-acids and prevention of atherosclerosis Nutr Rev 44 1986 125 134 [12] A. Reis L. Gouveia V. Veloso H.L. Fernandes J.A. Empis J.M. Novais Eicosapentaenoic acid-rich biomass production by the microalga Phaeodactylum tricornutum in a continuous-flow reactor Biores Technol 55 1996 83 88 [13] L.A. Meireles A.C. Guedes F.X. Malcata Increase of the yields of eicosapentaenoic and docosahexaenoic acids by the microalga Pavlova lutheri following random mutagenesis Biotechnol Bioeng 81 2003 50 55 [14] M.A. Borowitzka Algal growth media and sources of algal cultures M.A. Borowitzka J.L. Borowitzka Micro-algal Biotechnology 1988 Cambridge 456 465 [15] L.A. Meireles J.L. Azevedo J.P. Cunha F.X. Malcata On-line determination of biomass in a microalga bioreactor using a novel computerized flow injection analysis system Biotechnol Prog 18 2002 1408 1413 [16] Z. Cohen A. Vonshak A. Richmond Effect of environmental conditions on fatty acid composition of the red alga Porphyridium cruentum: correlation to growth rate J Phycol 24 1988 328 332 [17] K. Richardson J. Beardall J.A. Raven Adaptation of unicellular algae to irradiance – an analysis of strategies New Phytologist 93 1983 157 191 [18] H. Tatsuzawa E. Takizawa Changes in lipid and fatty acid composition of Pavlova lutheri Phytochemistry 40 1995 397 400 "
    },
    {
        "doc_title": "Hierarchical distributed architectures for autonomous mobile robots: A case study",
        "doc_scopus_id": "47849085902",
        "doc_doi": "10.1109/EFTA.2007.4416889",
        "doc_eid": "2-s2.0-47849085902",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Actuation systems",
            "Autonomous mobile robots",
            "Case studies",
            "Complex control",
            "Controller-area network",
            "Distributed architectures",
            "Distributed hardware",
            "Distributed sensing",
            "Dynamic environments",
            "Emerging technologies",
            "Global coordination",
            "Hardware architectures",
            "International conferences",
            "Micro-controller",
            "Mutual interference",
            "Portugal",
            "Soccer robots",
            "Vision sensing"
        ],
        "doc_abstract": "Robots are becoming commonplace in unstructured and dynamic environments, ranging from homes to offices, public sites, catastrophe sites, military scenarios. Achieving adequate performance in such circumstances requires complex control architectures, mixing adequately deliberative and reactive capabilities. This mixing needs to be properly addressed from both the software and hardware architectures point of view and, particularly, the mapping of the former onto the latter, in order to reduce mutual interference between concurrent behaviors and support the desired coordination with adequate level of reactivity. This paper discusses the benefits of using hierarchical distributed hardware architectures and presents the case study of the CAMBADA soccer robots developed at the University of Aveiro, Portugal. These robots use a distributed hardware architecture with a central computer to carry out vision sensing, global coordination and deliberative functions and a low-level distributed sensing and actuation system based on a set of simple microcontroller nodes interconnected with a Controller Area Network (CAN). © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On-line determination of biomass in a microalga bioreactor using a novel computerized flow injection analysis system",
        "doc_scopus_id": "2242454126",
        "doc_doi": "10.1021/bp020283u",
        "doc_eid": "2-s2.0-2242454126",
        "doc_date": "2002-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "A flow injection analysis (FIA) device has been developed, which is able to assay successfully for biomass in a microalga bioreactor. The device is fully computerized and is operated via diluting small aliquots of the culture followed by measuring optical density (OD); this figure is then accurately correlated with biomass, in terms of both cell number and ash-free dry weight, during the entire culture time. Furthermore, the device is not expensive, is highly versatile, and is easy to operate owing to specifically developed, user-friendly software. The growth rate and biomass productivity of Pavlova lutheri, cultivated under batch and semicontinuous modes, were monitored as experimental testing model.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Micro-Rato Contest: A popular approach to improve self-study in electronics and computer science",
        "doc_scopus_id": "0034510626",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0034510626",
        "doc_date": "2000-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Robotic competitions for students have been around for more than two decades. Although the benefits of these competitions can be exploited at various levels of education, recent examples have been following a trend towards higher complexity. In order to bring the pedagogical advantages of robotics competitions down to a level where they can be exploited by undergraduate and high school students, a somewhat different strategy is presented in this paper. Since 1995, the authors have been organising the Micro-Rato Contest of the University of Aveiro, a competition of small autonomous and mobile robots. This paper provides an overview of the contest, describing the strategy followed by the organizers, the rules and technical specifications, as well as a brief profile of some of the robots.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Grafcet's macro-action concept: An implementation view",
        "doc_scopus_id": "0033283809",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0033283809",
        "doc_date": "1999-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Grafcet evolution systems",
            "Grafcet macro-action concept"
        ],
        "doc_abstract": "Some implementation aspects of the Grafcet's macro-action concept are presented. Firstly, two different interpretation algorithms are discussed, each of them observing the evolution rules specified by the Grafcet standard, but leading to different results. After that, an algorithm developed to validate the coherence of the macro-actions of the global grafcet is presented. Finally, some details regarding code generation of the Grafcet evolution system are discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Affordable tools for teaching embedded systems",
        "doc_scopus_id": "0032271202",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0032271202",
        "doc_date": "1998-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Embedded systems teaching",
            "In-house development",
            "Laboratory equipments",
            "Low-cost solution"
        ],
        "doc_abstract": "Teaching embedded systems requires the choice of suitable tools. Factors such as cost and reliability can play an important role in the decision that affect students access to the equipment, conditioning their performance and enthusiasm. A possible option, which we advocate in this paper, is the in-house development of low-cost solutions, providing access to laboratory equipment without major restrictions, encouraging the interest for electronics and self-study. Several tools for teaching embedded systems are presented. These share the same characteristics, namely the fact that they are relatively easy to develop, and that they are inexpensive.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sleep data integration and analysis an object oriented approach",
        "doc_scopus_id": "0027885116",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0027885116",
        "doc_date": "1993-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This work is about an environment in which it is possible to relate and treat sleep EEG, respiratory and cardiac data coming from different sources. Some data treatment is done on-line and full data treatment is allowed off-line. The on-line treatment includes Sleep Staging using an Artificial Neural Network (ANN). This paper will try to enhance the advantages of Object Oriented Programming in the package we developed and the consequent reliability of the system architecture.",
        "available": false,
        "clean_text": ""
    }
]