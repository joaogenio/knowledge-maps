[
    {
        "doc_title": "Feature-Based Classification of Archaeal Sequences Using Compression-Based Methods",
        "doc_scopus_id": "85129799575",
        "doc_doi": "10.1007/978-3-031-04881-4_25",
        "doc_eid": "2-s2.0-85129799575",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambients",
            "Archaeal",
            "Archaeal sequence",
            "Archaeon",
            "Carbon fixation",
            "Feature-based classification",
            "Features selection",
            "Nitrogen-cycling",
            "Single-celled organisms",
            "Taxonomic identifications"
        ],
        "doc_abstract": "© 2022, Springer Nature Switzerland AG.Archaea are single-celled organisms found in practically every habitat and serve essential functions in the ecosystem, such as carbon fixation and nitrogen cycling. The classification of these organisms is challenging because most have not been isolated in a laboratory and are only found in ambient samples by their gene sequences. This paper presents an automated classification approach for any taxonomic level based on an ensemble method using non-comparative features. This methodology overcomes the problems of reference-based classification since it classifies sequences without resorting directly to the reference genomes, using the features of the biological sequences instead. Overall we obtained high results for classification at different taxonomic levels. For example, the Phylum classification task achieved 96% accuracy, whereas 91% accuracy was achieved in the genus identification task of archaea in a pool of 55 different genera. These results show that the proposed methodology is a fast, highly-accurate solution for archaea identification and classification, being particularly interesting in the applied case due to the challenging classification of these organisms. The method and complete study are freely available, under the GPLv3 license, at https://github.com/jorgeMFS/Archaea2.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The haplotype-resolved chromosome pairs of a heterozygous diploid African cassava cultivar reveal novel pan-genome and allele-specific transcriptome features",
        "doc_scopus_id": "85128000485",
        "doc_doi": "10.1093/gigascience/giac028",
        "doc_eid": "2-s2.0-85128000485",
        "doc_date": "2022-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Alleles",
            "Chromosomes",
            "Diploidy",
            "Haplotypes",
            "Manihot",
            "Plant Breeding",
            "Sequence Analysis, DNA",
            "Transcriptome"
        ],
        "doc_abstract": "© 2022 The Author(s) 2022. Published by Oxford University Press GigaScience.Background: Cassava (Manihot esculenta) is an important clonally propagated food crop in tropical and subtropical regions worldwide. Genetic gain by molecular breeding has been limited, partially because cassava is a highly heterozygous crop with a repetitive and difficult-to-assemble genome. Findings: Here we demonstrate that Pacific Biosciences high-fidelity (HiFi) sequencing reads, in combination with the assembler hifiasm, produced genome assemblies at near complete haplotype resolution with higher continuity and accuracy compared to conventional long sequencing reads. We present 2 chromosome-scale haploid genomes phased with Hi-C technology for the diploid African cassava variety TME204. With consensus accuracy >QV46, contig N50 >18 Mb, BUSCO completeness of 99%, and 35k phased gene loci, it is the most accurate, continuous, complete, and haplotype-resolved cassava genome assembly so far. Ab initio gene prediction with RNA-seq data and Iso-Seq transcripts identified abundant novel gene loci, with enriched functionality related to chromatin organization, meristem development, and cell responses. During tissue development, differentially expressed transcripts of different haplotype origins were enriched for different functionality. In each tissue, 20-30% of transcripts showed allele-specific expression (ASE) differences. ASE bias was often tissue specific and inconsistent across different tissues. Direction-shifting was observed in <2% of the ASE transcripts. Despite high gene synteny, the HiFi genome assembly revealed extensive chromosome rearrangements and abundant intra-genomic and inter-genomic divergent sequences, with large structural variations mostly related to LTR retrotransposons. We use the reference-quality assemblies to build a cassava pan-genome and demonstrate its importance in representing the genetic diversity of cassava for downstream reference-guided omics analysis and breeding. Conclusions: The phased and annotated chromosome pairs allow a systematic view of the heterozygous diploid genome organization in cassava with improved accuracy, completeness, and haplotype resolution. They will be a valuable resource for cassava breeding and research. Our study may also provide insights into developing cost-effective and efficient strategies for resolving complex genomes with high resolution, accuracy, and continuity.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of Low-Copy Human Virus DNA upon Prolonged Formalin Fixation",
        "doc_scopus_id": "85123024507",
        "doc_doi": "10.3390/v14010133",
        "doc_eid": "2-s2.0-85123024507",
        "doc_date": "2022-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Infectious Diseases",
                "area_abbreviation": "MEDI",
                "area_code": "2725"
            },
            {
                "area_name": "Virology",
                "area_abbreviation": "IMMU",
                "area_code": "2406"
            }
        ],
        "doc_keywords": [
            "DNA, Viral",
            "Formaldehyde",
            "Humans",
            "Kidney",
            "Liver",
            "Lung",
            "Molecular Diagnostic Techniques",
            "Nucleic Acid Hybridization",
            "Real-Time Polymerase Chain Reaction",
            "Sequence Analysis, DNA",
            "Tissue Fixation"
        ],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.Formalin fixation, albeit an outstanding method for morphological and molecular preser-vation, induces DNA damage and cross-linking, which can hinder nucleic acid screening. This is of particular concern in the detection of low-abundance targets, such as persistent DNA viruses. In the present study, we evaluated the analytical sensitivity of viral detection in lung, liver, and kidney specimens from four deceased individuals. The samples were either frozen or incubated in formalin (±paraffin embedding) for up to 10 days. We tested two DNA extraction protocols for the control of efficient yields and viral detections. We used short-amplicon qPCRs (63–159 nucleotides) to detect 11 DNA viruses, as well as hybridization capture of these plus 27 additional ones, followed by deep sequencing. We observed marginally higher ratios of amplifiable DNA and scantly higher viral genoprevalences in the samples extracted with the FFPE dedicated protocol. Based on the findings in the frozen samples, most viruses were detected regardless of the extended fixation times. False-negative calls, particularly by qPCR, correlated with low levels of viral DNA (<250 copies/million cells) and longer PCR amplicons (>150 base pairs). Our data suggest that low-copy viral DNAs can be satisfactorily investigated from FFPE specimens, and encourages further examination of historical materials.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Revisiting the Neurospora crassa mitochondrial genome",
        "doc_scopus_id": "85111609935",
        "doc_doi": "10.1111/lam.13538",
        "doc_eid": "2-s2.0-85111609935",
        "doc_date": "2021-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Applied Microbiology and Biotechnology",
                "area_abbreviation": "IMMU",
                "area_code": "2402"
            }
        ],
        "doc_keywords": [
            "DNA, Fungal",
            "DNA, Mitochondrial",
            "Genome, Mitochondrial",
            "Neurospora",
            "Neurospora crassa"
        ],
        "doc_abstract": "© 2021 The Society for Applied MicrobiologyThe mitochondrial genome of Neurospora crassa has been less studied than its nuclear counterpart, yet it holds great potential for understanding the diversity and evolution of this important fungus. Here we describe a new mitochondrial DNA (mtDNA) complete sequence of a N. crassa wild type strain. The genome with 64 839 bp revealed 21 protein-coding genes and several hypothetical open reading frames with no significant homology to any described gene. Five large repetitive regions were identified across the genome, including partial or complete genes. The largest repeated region holds a partial nd2 section that was also detected in Neurospora intermedia, suggesting a rearrangement that occurred before the N. crassa speciation. Interestingly, N. crassa has a palindrome adjacent to the partial nd2 repeated region possibly related to the genomic rearrangement, which is absent in N. intermedia. Finally, we compared the sequences of the three available N. crassa complete mtDNAs and found low levels of intraspecific variability. Most differences among strains were due to small indels in noncoding regions. The revisiting of the N. crassa mtDNA forms the basis for future studies on mitochondrial genome organization and variability.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic analysis of artistic paintings using information-based measures",
        "doc_scopus_id": "85100446723",
        "doc_doi": "10.1016/j.patcog.2021.107864",
        "doc_eid": "2-s2.0-85100446723",
        "doc_date": "2021-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Artistic paintings",
            "Automatic analysis",
            "Block decomposition",
            "Community IS",
            "Computational analysis",
            "Correlation function",
            "Hidden patterns",
            "Local information"
        ],
        "doc_abstract": "© 2021 Elsevier LtdThe artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artists’ style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website (http://panther.web.ua.pt) for fast author characterization and authentication.",
        "available": true,
        "clean_text": "serial JL 272206 291210 291718 291872 291874 31 Pattern Recognition PATTERNRECOGNITION 2021-02-01 2021-02-01 2021-02-08 2021-02-08 2021-03-02T12:42:57 S0031-3203(21)00051-0 S0031320321000510 10.1016/j.patcog.2021.107864 S300 S300.1 FULL-TEXT 2022-06-12T13:56:47.120829Z 0 0 20210601 20210630 2021 2021-02-01T16:28:18.995615Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref vitae 0031-3203 00313203 true 114 114 C Volume 114 18 107864 107864 107864 202106 June 2021 2021-06-01 2021-06-30 2021 Regular papers Objects and image analysis article fla © 2021 Elsevier Ltd. All rights reserved. AUTOMATICANALYSISARTISTICPAINTINGSUSINGINFORMATIONBASEDMEASURES SILVA J 1 Introduction 2 Related work 3 Methods 3.1 Information-based measures 3.1.1 Normalized compression (NC) 3.1.2 Normalized block decomposition method (NBDM) 3.1.3 Local complexity analysis using the normalized compression 3.2 Two-point height difference correlation function 3.2.1 Assessment pipeline 3.2.2 Finding an effective data compressor 4 Results 4.1 Comparison of NC and BDM 4.2 Information-based measures in images of artistic paintings 4.2.1 Global measures analysis 4.2.2 Combining the NC with the roughness exponent of HDC function 4.2.3 Local complexity of paintings 4.2.4 Evaluation of measures for classification purposes 5 Discussion 6 Conclusions Website CRediT authorship contribution statement Acknowledgements Appendix A Supplementary materials References WEISBERG 2006 R CREATIVITYUNDERSTANDINGINNOVATIONINPROBLEMSOLVINGSCIENCEINVENTIONARTS HERTZMANN 2018 18 A ARTS CANCOMPUTERSCREATEART KHAN 2014 1385 1397 F LYU 2004 17006 17010 S KIM 2014 7370 D ZHANG 2017 34 H ZENIL 2018 605 H DELAHAYE 2012 63 77 J SOLERTOSCANO 2014 F SMIERS 2003 J ARTSUNDERPRESSUREPROTECTINGCULTURALDIVERSITYINAGEGLOBALISATION FERREIRA 2014 12 19 P INTERNATIONALCONFERENCEIMAGEANALYSISRECOGNITION AMETHODDETECTREPEATEDUNKNOWNPATTERNSINIMAGE PINHO 2011 584 588 A 201119THEUROPEANSIGNALPROCESSINGCONFERENCE FINDINGUNKNOWNREPEATEDPATTERNSINIMAGES PRATAS 2012 158 165 D INTERNATIONALCONFERENCEIMAGEANALYSISRECOGNITION DETECTIONUNKNOWNLOCALLYREPEATINGPATTERNSINIMAGES ROMASHCHENKO 2002 111 123 A NIVEN 2009 49 63 R MANTACI 2008 411 429 S SHANNON 1948 379 423 C KOLMOGOROV 1965 1 7 A SOLOMONOFF 1964 1 22 R SOLOMONOFF 1964 224 254 R CHAITIN 1966 547 569 G SOLERTOSCANO 2017 F GAUVRIT 2017 N LI 2001 149 154 M CILIBRASI 2005 1523 1545 R CILIBRASI 2006 2309 2313 R 2006IEEEINTERNATIONALSYMPOSIUMINFORMATIONTHEORY AUTOMATICEXTRACTIONMEANINGWEB CEBRIAN 2007 1895 1900 M COHEN 2014 1602 1614 A PRATAS 2017 259 266 D IBERIANCONFERENCEPATTERNRECOGNITIONIMAGEANALYSIS APPROXIMATIONKOLMOGOROVCOMPLEXITYFORDNASEQUENCES MANICCAM 2004 475 486 S LU 2016 Z LOSSLESSINFORMATIONHIDINGINIMAGES PRATAS 2016 231 240 D 2016DATACOMPRESSIONCONFERENCEDCC EFFICIENTCOMPRESSIONGENOMICSEQUENCES PRATAS 2017 265 272 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS SUBSTITUTIONALTOLERANTMARKOVMODELSFORRELATIVECOMPRESSIONDNASEQUENCES PINHO 2008 1 5 A 200816THEUROPEANSIGNALPROCESSINGCONFERENCE INVERTEDREPEATSAWAREFINITECONTEXTMODELSFORDNACODING RISSANEN 1978 465 471 J LI 2004 3250 3264 M LI 2008 M INTRODUCTIONKOLMOGOROVCOMPLEXITYAPPLICATIONS TAYLOR 1999 R JOHNSON 2008 37 48 C LI 2004 340 353 J BRESSAN 2008 113 116 M 200815THIEEEINTERNATIONALCONFERENCEIMAGEPROCESSING ANALYSISRELATIONSHIPBETWEENPAINTERSBASEDWORK OLSHAUSEN 2010 1027 B HUGHES 2010 1279 1283 J STORK 2008 68100J D COMPUTERIMAGEANALYSISINSTUDYART IMAGEANALYSISPAINTINGSBYCOMPUTERGRAPHICSSYNTHESISINVESTIGATIONILLUMINATIONINGEORGESDELATOURSCHRISTINCARPENTERSSTUDIO LETTNER 2008 68100C M COMPUTERIMAGEANALYSISINSTUDYART ESTIMATINGORIGINALDRAWINGTRACEPAINTEDSTROKES SHAHRAM 2008 68100D M COMPUTERIMAGEANALYSISINSTUDYART RECOVERINGLAYERSBRUSHSTROKESTHROUGHSTATISTICALANALYSISCOLORSHAPEAPPLICATIONVANGOGHSSELFPORTRAITGREYFELTHAT HEDGES 2008 681009 S COMPUTERIMAGEANALYSISINSTUDYART IMAGEANALYSISRENAISSANCECOPPERPLATEPRINTS PETROV 2002 197 202 V MACHADO 2019 614 626 J PENG 2015 3057 3061 K 2015IEEEINTERNATIONALCONFERENCEIMAGEPROCESSINGICIP CROSSLAYERFEATURESINCONVOLUTIONALNEURALNETWORKSFORGENERICCLASSIFICATIONTASKS MAO 2017 1183 1191 H PROCEEDINGS25THACMINTERNATIONALCONFERENCEMULTIMEDIA DEEPARTLEARNINGJOINTREPRESENTATIONSVISUALARTS CHU 2018 2491 2502 W HAMMER 2000 442 464 D HENRIQUES 2013 1101 1106 T TERWIJN 2009 S RYBALOV 2007 268 270 A BLOEM 2014 336 350 P INTERNATIONALCONFERENCEALGORITHMICLEARNINGTHEORY ASAFEAPPROXIMATIONFORKOLMOGOROVCOMPLEXITY SOKAL 1958 1409 1438 R KRUSKAL 1956 48 50 J LLOYD 1982 129 137 S TAUBMAN 2002 D HOSSEINI 2019 68 76 M CLEARY 1984 396 402 J MAHONEY 2005 M TECHNICALREPORT ADAPTIVEWEIGHINGCONTEXTMODELSFORLOSSLESSDATACOMPRESSION RISSANEN 1979 149 162 J MOFFAT 1998 256 294 A KNOLL 2012 377 386 B 2012DATACOMPRESSIONCONFERENCE AMACHINELEARNINGPERSPECTIVEPREDICTIVECODINGPAQ8 WANG 2017 3462 3471 X IEEECVPR CHESTXRAY8HOSPITALSCALECHESTXRAYDATABASEBENCHMARKSWEAKLYSUPERVISEDCLASSIFICATIONLOCALIZATIONCOMMONTHORAXDISEASES SHAPIRO 1978 175 214 C ROSENBERG 1994 H TRADITIONNEW GARRARD 2007 L COLOURFIELDPAINTINGMINIMALCOOLHARDEDGESERIALPOSTPAINTERLYABSTRACTARTSIXTIESPRESENT HOCKNEY 2006 D SECRETKNOWLEDGEREDISCOVERINGLOSTTECHNIQUESOLDMASTERS YANG 2016 1 6 S 2016EIGHTHINTERNATIONALCONFERENCEQUALITYMULTIMEDIAEXPERIENCEQOMEX COMPUTATIONALMODELINGARTISTICINTENTIONQUANTIFYLIGHTINGSURPRISEFORPAINTINGANALYSIS FICHNERRATHUS 2011 L FOUNDATIONSARTDESIGNENHANCEDMEDIAEDITION CHEN 2016 785 794 T PROCEEDINGS22NDACMSIGKDDINTERNATIONALCONFERENCEKNOWLEDGEDISCOVERYDATAMINING XGBOOSTASCALABLETREEBOOSTINGSYSTEM NANNI 2017 158 172 L SILVAX2021X107864 SILVAX2021X107864XJ 2023-02-08T00:00:00.000Z 2023-02-08T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 2022-06-07T13:51:03.718Z Scientific Employment Stimulus CI-CTTI-94-ARH/2019 Foundation for Science and Technology SFRH/BD/137000/2018 SFRH/BD/141851/2018 UID/CEC/00127/2019 FCT Fundação para a Ciência e a Tecnologia Fundalo para a Ciłncia e a Tecnologia This work was funded by National Funds through the FCT - Foundation for Science and Technology, in the context of the project UID/CEC/00127/2019 and the research grants SFRH/BD/141851/2018 and SFRH/BD/137000/2018 for J.M.S and R.A, respectively. D.P. is funded by national funds through FCT - Fundalo para a Ciłncia e a Tecnologia, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019. item S0031-3203(21)00051-0 S0031320321000510 10.1016/j.patcog.2021.107864 272206 2021-04-24T03:25:50.185202Z 2021-06-01 2021-06-30 true 3739076 MAIN 13 55894 849 656 IMAGE-WEB-PDF 1 gr1 15979 221 382 gr2 78329 280 760 gr3 193217 586 756 gr4 252946 579 760 gr5 56425 375 756 gr6 230839 574 760 gr7 204219 761 761 gr1 4886 127 219 gr2 7699 81 219 gr3 24575 164 211 gr4 27479 164 215 gr5 7038 109 219 gr6 28301 164 217 gr7 12502 164 164 gr1 124625 1174 2032 gr2 845953 1489 4039 gr3 2593723 3113 4017 gr4 3278848 3076 4039 gr5 538446 1994 4016 gr6 2253011 3050 4039 gr7 2190689 4042 4042 mmc1 false 6905460 APPLICATION si7 3360 si17 2830 si31 4158 si1 1201 si40 2942 si50 5831 si8 3227 si9 4679 si10 1707 si11 1524 si12 9785 si13 2878 si14 1151 si15 5646 si16 13543 si18 3320 si19 13222 si2 687 si20 4753 si30 4648 si21 4784 si22 5274 si23 3195 si24 1272 si25 1568 si26 891 si27 11153 si28 4355 si29 3865 si3 875 si32 1743 si33 1573 si34 2100 si35 10893 si36 1389 si37 930 si38 1800 si39 3102 si4 1910 si41 1085 si42 1241 si43 13196 si44 1004 si45 1734 si46 1486 si47 3197 si48 17639 si49 6487 si5 1429 si51 3688 si52 1170 si53 871 si54 2793 si55 2746 si6 3709 am 10718106 PR 107864 107864 S0031-3203(21)00051-0 10.1016/j.patcog.2021.107864 Elsevier Ltd Fig. 1 Benchmark of lossless data compression tools specifically for the processed dataset of artistic paintings. The y -axis depicts the sum of the number of bytes to compress the dataset, where each image was compressed individually using each tool. Fig. 1 Fig. 2 Information-based measures evaluation. (A) Impact of increasing pseudo-random substitution on information-based measures: NC (approximated using the PAQ8 algorithm) and two BDM normalizations (NBDM 1 and NBDM 2 ). (B) Values of the NC and NBDM 1 for different types of images. (C) Image transformation pipeline leading to BDM underestimation of the amount of information contained in the transformed object. Fig. 2 Fig. 3 Examples of artistic paintings with different levels of complexity where painting images were quantized to 8 bits. The NC and NBDM 1 values of each painting are displayed in its lower right corner. Fig. 3 Fig. 4 Average Normalized Block Decomposition Method using NBDM 1 (A), and Average Normalized Compression (B) for each author where images of paintings where quantized for 4, 6, and 8 bits. The authors are sort given the value of NBDM 1 and NC, respectively. To see this result in more detail, please visit the website associated with the article. Fig. 4 Fig. 5 Combining the HDC with NC. (A) Average and standard deviation for each style in NC and α , respectively. (B) Results grouped by styles using average NC and average α of HDC for each artist labeled on the dataset. Fig. 5 Fig. 6 Heat maps of the local complexity matrix (fingerprint) of some authors, computed with the NC. This fingerprint shows the author’s range of complexity and the locations in the canvas painted with more detail (or complexity). To see all matrices, please visit the website associated with this article. Fig. 6 Fig. 7 Artists’ phylogenetic tree computed recurring to the UPGMA algorithm. Each artist has a sample painting and a colour associated with one of his styles (the colour was chosen based on nearest leaves) assigned to him, as well as a description of some styles usually associated with the author. To obtain an improved view of the tree, please visit the website related to this article. Fig. 7 Table 1 Accuracy results obtained for the test set in style and author classification task using state-of-the-art (SoA), state-of-the-art with regional complexity (RC) and ensemble with our measures (RC and HDC). Table 1 Classification Task Number of Classes Number of Images SoA Baseline SoA Baseline + RC SoA Baseline + RC + HDC Style 13 2338 0.622 0.644 0.650 Author 91 4266 0.480 0.490 0.500 Automatic analysis of artistic paintings using information-based measures Automatic analysis of artistic paintings using information-based measures Jorge Miguel Silva Conceptualization Validation Writing - original draft Writing - review & editing ⁎ a b Diogo Pratas Conceptualization Validation Writing - original draft Writing - review & editing a b c Rui Antunes Conceptualization Validation Writing - original draft Writing - review & editing a b Sérgio Matos Conceptualization Validation Writing - original draft Writing - review & editing a b Armando J. Pinho Conceptualization Validation Writing - original draft Writing - review & editing a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal b Department of Electronics, Telecomunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecomunications and Informatics University of Aveiro Portugal Department of Electronics, Telecomunications and Informatics, University of Aveiro, Portugal c Department of Virology, University of Helsinki, Finland Department of Virology University of Helsinki Finland Department of Virology, University of Helsinki, Finland ⁎ Corresponding author at: Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal. Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal The artistic community is increasingly relying on automatic computational analysis for authentication and classification of artistic paintings. In this paper, we identify hidden patterns and relationships present in artistic paintings by analysing their complexity, a measure that quantifies the sum of characteristics of an object. Specifically, we apply Normalized Compression (NC) and the Block Decomposition Method (BDM) to a dataset of 4,266 paintings from 91 authors and examine the potential of these information-based measures as descriptors of artistic paintings. Both measures consistently described the equivalent types of paintings, authors, and artistic movements. Moreover, combining the NC with a measure of the roughness of the paintings creates an efficient stylistic descriptor. Furthermore, by quantifying the local information of each painting, we define a fingerprint that describes critical information regarding the artists’ style, their artistic influences, and shared techniques. More fundamentally, this information describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. Finally, we demonstrate that regional complexity and two-point height difference correlation function are useful auxiliary features that improve current methodologies in style and author classification of artistic paintings. The whole study is supported by an extensive website for fast author characterization and authentication. Keywords Image analysis Data compression BDM Artistic paintings Algorithmic information theory 1 Introduction Artistic paintings are concrete visual expressions of human evolution and creativity to share emotions, values, visions, beliefs, and trends of history and culture. The creation, interpretation, and analysis of artistic paintings are social, contextual, subjective, passive, and, beyond superficial characteristics, complex to compute and automatize [1]. In particular, it is theorized that art is an output of social agents, particularly a human experience, that can only be imitated by machines [2]. One of the non-trivial characteristic analysis of artistic paintings is related to the process of measuring the information contained in those paintings. Artistic paintings contain information related to schools, periods, and artists [3]. The artistic community widely uses automatic computational analysis of artistic paintings for authentication of artistic paintings [4,5]. Currently, this process does not substitute human experts completely; however, it is an essential additional control for fraud and mislead detections [6]. Furthermore, applying new techniques and pre-existing ones that are new to the field, can be useful not only for authorship attribution and fraud detection but also for art style categorization and organization, and even for art content explanation. In this paper, we introduce novel solutions for automatic computational analysis of artistic paintings and for the problem of artist authentication. When addressing artist authentication, several questions arise: What defines a painter’s style? How does the author expose information? How does the author differs and relates to other artists? Furthermore, taking inspiration from information theory: How do we best quantify information in a painting? How is the information utilized across the canvas? Moreover, what can information quantification tell us about the author’s style, way of painting, and relationships with other authors? These complex questions are at the core of this paper’s development, where we describe and compare solutions for unsupervised measures of probabilistic and algorithmic information in images (2D) of artistic paintings. Our contributions are as follows: • We perform a direct comparison between state-of-the-art unsupervised probabilistic and algorithmic information measures to specify each measure’s strengths and weaknesses. • We show that hidden patterns and relationships present in artistic paintings can be identified by analysing their complexity. • We show an efficient stylistic descriptor by combining the Normalized Compression and a measure of the paintings’ roughness. • We propose a new descriptor of the artists’ style, artistic influences, and shared techniques. • We show that average local complexity describes how each author typically composes and distributes the elements across the canvas and, therefore, how their work is perceived. • We demonstrate that these measures can serve as useful auxiliary features capable of improving current methodologies in the classification of artistic paintings. To explain how we achieve this, we first compare the Normalized Compression (NC), employing a data compression tool chosen after a competitive benchmark, with the Block Decomposition Method (BDM) [7], and the inherent Coding Theorem Method (CTM) measures [8,9]. The BDM is an information-based measure that uses small Turing machines to approximate the algorithmic information, approximating to the Shannon entropy as a fallback mechanism. After this comparison, we make use of the average NC of each artist together with the roughness exponent α of the two-point height difference correlation function (HDC), to group artists by style. Furthermore, we provide a local complexity matrix that characterizes each artist using the NC and use it to construct a phylogenetic tree that portraits the relationship between artists in terms of exposing information to the observer. Finally, we use the regional complexity fingerprints and the roughness exponent α as useful auxiliary features that, combined with state-of-the-art approaches, improve the results of style and artist classification tasks. The remaining of this paper is organized as follows. In the next section, we describe related work, followed by a description of the methods. We present the major results in the next section, with further results presented in Supplementary Material. Finally, we discuss the results obtained, draw final conclusions, and point out possible future lines of work. 2 Related work Measuring the information contained in paintings requires fast, efficient, and automatic computation due to the diversity and large quantity of the existing artistic paintings [10]. To measure the information (or complexity) contained in paintings, we first need to define what is the quantity of information of an image. We define the quantity of information of an image as the smallest number of bits required by a model to represent an image losslessly. To perform this task, the model searches for unknown patterns of similarity between sub-regions of the image [11–13] and uses this information to create this compressed representation of the image, relying exclusively in the patterns of the two-dimensional pixels without using exogenous information. There are several approaches to quantify the amount of information. Kolmogorov described three, namely combinatorial [14–16], probabilistic [17], and algorithmic [18]. Independently, the works of Solomonoff [19,20] and Chaitin [21] addressed the same lines. While the Kolmogorov complexity is non-computable, it can be approximated with programs for such purpose, such as data compressors, using probabilistic and algorithmic schemes. Practical applications to approximate the Kolmogorov complexity for multiple dimensional digital objects have been developed using Turing machines [7,9,22,23] and data compressors [24–29]. Recently, Zenil et al. have shown that this methodology has a closer connection to algorithmic information than other measures based on statistical regularities [7], namely fast lossless compression methods, for sources that follow algorithmic schemes. The majority of the lossless compression algorithms are limited to finding simple statistical regularities as they have been designed for fast storage reduction [30,31]; accordingly, they provide slight improvements over the Shannon entropy [17]. However, there are several which are designed for efficient compression at the expense of more computational resources. For example, lossless compression algorithms, such as GeCo [32], are hybrids between probabilistic and algorithmic schemes. Besides having several context models of different orders, GeCo uses sub-programs that allow substitution [33] and reverse complement modeling [34]. These last two are sub-programs of probabilistic and algorithmic information nature. Another example is PAQ8 [35], a general-purpose compressor that combines multiple context models using a neural network, transform functions, secondary structure predictors, and other simple sub-programs. Usually, the problem is how to find fast and efficient algorithmic models for data compression. Lossless data compressors are tightly related to the concept of minimal description length [36] and algorithmic probability [25,37,38]. Therefore, representative algorithms can be efficiently embedded in these data compressors, including small Turing machines. The idea of automatic computational analysis of artistic paintings is mature [4,5], and the artistic community has widely relied on it for authentication of artistic paintings. Specifically, the characteristics of artistic paintings have been analysed through several statistical techniques and properties, namely fractal [39], wavelet-based [4], hidden Markov models [40,41], Fisher kernel based [42], sparse coding model [43,44], color and brightness [5], illumination [45], stroke [46,47], Print Index [48], and entropy-based analysis [49,50]. Recently, the work of Machado and Lopes [50], using fractional calculus, showed the potentiality of measures based on entropy to describe hierarchical clustering of paintings and their correlation with artistic movements. Regarding style and author classification, several recent works have proposed the usage of Convolutional Neural Networks (CNNs). A straightforward approach is to combine features extracted from multiple CNN layers, such as proposed by Peng et al. [51]. Another more effective approach is based on representing images by the principal components of a Gram matrix that captures correlations across the different feature maps obtained from a convolutional layer of a pretrained deep CNN, such as VGG16 or VGG19. Mao et al. [52] combine this representation with the features from all the five convolutional blocks of the VGG16, learning a joint representation that can simultaneously capture content and style of visual arts. On the other hand, Chu et al. [53] apply a support vector machine (SVM) to the Gram representation to perform author and style classification. Then, they improve the results by automatically learning correlations between feature maps. 3 Methods In this section, we describe the measures used, their normalizations, the methodology, and the compression benchmark performed. 3.1 Information-based measures Algorithmic information [18–21] differs from a perspective of pure probabilistic information [17] because it considers that the source, rather than generating symbols from a probabilistic ergodic function, creates structures that represent algorithmic schemes [54,55]. Therefore, to reverse the problem, there is the need to identify the program(s) and parameter(s) that generate the outcome(s) [18,21,38]. However, the algorithmic information, K ( x ) , is non-computable [56], mostly because of the halting problem [57]. Therefore, we have to rely on approximations. Namely, in this subsection, we describe the Normalized Compression and two BDM normalizations. Then, we establish the local application of the Normalized Compression to create a complexity matrix for each author and the methods used to create a distance matrix and the phylogenetic tree. Finally, we describe a non-information-based measure, the two-point height difference correlation function. 3.1.1 Normalized compression (NC) An efficient compressor, C ( x ) , gives a possible approximation for the Kolmogorov complexity ( K ( x ) ), where K ( x ) < C ( x ) ≤ | x | ( | x | is the length of string x in the appropriate scale). Usually, an efficient data compressor is a program that approximates both probabilistic and algorithmic sources using affordable computational resources (Time and RAM). Although the algorithmic nature may be more complex to model, data compressors may have embedded sub-programs to handle this nature. For a definition of safe approximation, see [58]. The normalized version, known as the Normalized Compression (NC), is defined by (1) NC ( x ) = C ( x ) | x | log 2 | A | = C ( x ) | x | , where x is a string, C ( x ) is the compressed size of x in bits, | A | the number of different elements in x (size of the alphabet) and | x | the length of x . Since we consider a binary matrix of each image, | A | = 2 , log 2 2 = 1 . Given the normalization, the NC enables to compare the information contained in the strings independently from their sizes [29]. If the compressor is efficient, then the compressor can approximate the quantity of probabilistic-algorithmic information in data using affordable computational resources. 3.1.2 Normalized block decomposition method (NBDM) Another possible approximation to the Kolmogorov complexity is given by the use of small Turing machines, where these small computer programs approximate the components of a broader representation. The Coding Theorem Method uses the algorithmic probability between a string’s production frequency from a random program and its algorithmic complexity. As such, the more frequent a string is, the lower Kolmogorov complexity it has; and strings of lower frequency have higher Kolmogorov complexity. The Block Decomposition Method (BDM) extends the power of a CTM, approximating local estimations of algorithmic information based on Solomonoff-Levin’s algorithmic probability theory. In practice, it approximates the algorithmic information and, when it loses accuracy, it approximates the Shannon entropy. Since in this article we intend to perform a direct comparison of both measures, we first considered the normalization of the BDM (NBDM 1 ), given by the number of elements (length) of the digital object as (2) NBDM 1 ( x ) = B D M ( x ) | x | log 2 | A | = B D M ( x ) | x | . However, the normalization of the BDM is usually performed using a minimum complexity object (BDM M i n ) and a maximum complexity object (BDM M a x ). A minimum complexity object is filled with only one symbol, like a binary string of only zeros. In contrast, a maximum complexity object is an object that, when decomposed (by a given decomposition algorithm), yields slices that cover the highest CTM values and are repeated only after all possible slices of a given shape have been used once. Using these two objects, the NBDM 2 for a given string can be computed as (3) NBDM 2 ( x ) = B D M ( x ) − B D M M i n B D M M a x − B D M M i n , where B D M ( x ) is the BDM value of that string, B D M M i n is the minimum complexity object, and B D M M a x is the maximum complexity object. Kolmogorov complexity is invariant only up to a constant factor, which depends on the choice of a description language K = K ′ + L , where K is the total complexity, K ′ is the description of the object and L is the description of the language. As such, by performing the normalization according to Eq. (3), the normalization is aiming to remove the constant factor as (4) K − K M i n K M a x − K M i n = K ′ + L − K M i n ′ − L K M a x ′ + L − K M i n ′ − L = K ′ − K M i n ′ K M a x ′ − K M i n ′ , where K M a x and K M i n are the maximum and minimum Kolmogorov complexity objects and K M a x ′ and K M i n ′ are the maximum and minimum Kolmogorov complexity description of the objects. In this article, we perform a direct comparison between the NC and the NBDM 1 . Furthermore, we compare the two types of BDM normalization and their impact on the results. 3.1.3 Local complexity analysis using the normalized compression The Normalized Compression (NC) was used to approximate the local (or regional) complexity of images of artistic paintings. To that end, all of the dataset images were divided into 16 × 16 blocks (256 equal regions) and the NC was computed for each block, generating a complexity matrix. Other patch sizes were also tested, specifically patch sizes of 8 × 8 and 32 × 32 blocks. Following this operation, the average complexity matrix was generated for each author, using the complexity matrices of their paintings. The average complexity matrices were then used to obtain a similarity matrix, in which the distance between matrices was determined as (5) d ( A , B ) = ∑ i = 0 n ∑ j = 0 n ∣ a i j − b i j ∣ , where d is the distance between the complexity matrix A and B , and a i j and b i j are the complexity values at the index i and j of matrices A and B , respectively. Subsequently, using the similarity matrix, a phylogenetic tree was computed recurring to two methods, namely UPGMA (unweighted pair group method with arithmetic mean) [59] and the Kruskal minimum spanning tree algorithm [60], in order to portrait complexity relationships among different authors. 3.2 Two-point height difference correlation function The two-point height difference correlation (HDC) function was computed to quantify brightness contrast as (6) HDC ( r ) = [ h ( x → + r → ) − h ( x → ) ] 2 ¯ = 1 N r ∑ x → , | r → | = r [ h ( x → + r → ) − h ( x → ) ] 2 , where the r is the distance between two-pixel points, over-bar represents the spatial average at a fixed distance r for all possible points; N r is the number of possible pairs at a distance r , h ( x ) is pixel intensity at the position x . Using the HDC function, its roughness exponent α was determined as (7) α = log 10 ( H D C ( r f i n a l ) ) − log 10 ( H D C ( r i n i t i a l ) ) log 10 ( r f i n a l ) − log 10 ( r i n i t i a l ) , where ( α ) is the slope of the HDC curve in a double logarithmic plot of the surface growth model. The slope was calculated from r i n i t i a l = 10 to r f i n a l , which matches the point where the HDC function saturates, approximately 30% of the image’s width. 3.2.1 Assessment pipeline In order to fairly evaluate the information-based measures, we designed a pipeline for processing images. It respects the following steps: Obtaining the dataset images; converting the images to PGM format; quantization of the images to 8 bits (256 levels) using the Lloyd-Max algorithm; binarization of the images (conversion to 01 format in ASCII) and finally, applying the information-based measurements (NC, NBDM 1 and NBDM 2 ). Quantization was performed using the Lloyd-Max algorithm [61,62] since reducing the precision of the pixels (alphabet) in images enables the filtering of small variations that might occur during the digitalization process. Binarization to 01 format in ASCII was performed since the BDM currently only supports a small alphabet. 3.2.2 Finding an effective data compressor To compute the NC, we have to find an effective data compressor, meaning, a compressor that best represents each image, while using reasonable resources. Since our aim is later to apply this measure to a dataset of artistic painting, we compared seven compression tools, namely GZIP [63], BZIP2 [64], XZ [65], LZMA [66], AC [67], PPMD [68], and PAQ8 [35]. As depicted in Fig. 1 , the PAQ8 tool shows the best compression ratio for this dataset. In fact, it shows an improvement of ≈ 26 % to the second best tool (XZ). The disadvantage is the use of higher RAM and substantially more computational time. Nevertheless, since our purpose is to find the number of bits of a shortest program to reproduce the image, it is affordable to spend these computational resources. Therefore, we used the PAQ8 tool to compress each of the quantized images. The code was compiled using the package provided from Buchner [69]. The PAQ8 version used was kx v7. PAQ8kx v7 is an archiver that achieves the highest compression rates at the expense of speed and memory (approximately 1,6 GB of RAM for this dataset). We used the mode that usually provides the highest compression ratio (command parameter: “-8”). The PAQ8 compressor uses a context mixing algorithm between a large number of models independently predicting each quantized pixel’s next bit [70]. The predictions are combined using a neural network and arithmetic coding [71,72]. For automatic installation, use the script Install.sh, while for more information of PAQ, see the work of Knoll and Freitas [73]. The computations ran in a single core Ubuntu Linux computer running at 2.13 GHz with 1.6 GB of RAM. Using this machine, the compression of the whole dataset with PAQ8 required approximately 270 h of real-time, without parallelization. 4 Results 4.1 Comparison of NC and BDM In order to compare NC with BDM, we performed three types of tests. Namely, we compared the robustness of both measures according to increasing rates of random pixel changes in paintings, tested their application on different types of images, and made an assessment of the minimal information bounds. In the first test, we assessed the impact of an increasing rate of pixel editions using a pseudo-random uniform distribution and compared both information-based measures. This approach is not identical to image noise, but rather a pure edition of pixels. For the purpose, for each of the three authors (Theodore Gericault, Marc Chagall, and Rene Magritte) we select a painting, making 50 adulterated copies of each painting with increasing edition rate (from 1 to 50%). Finally, we measured the NC (Eq. (1)), the NBDM 1 (Eq. (2)), and NBDM 2 (Eq. (3)) in all the paintings. Fig. 2 (A) depicts the values obtained for the NC and BDM. The results show that, when using the same type of normalization, NC is more robust to the increment of pixel edition than NBDM (NBDM 1 ). On the other hand, whereas NBDM 1 considers the normalization by the length of the input object, NBDM 2 performs a normalization that aims to mimic the removal of the constant factor related to Kolmogorov complexity (see Eq. (4)). Since the NBDM 2 normalization does not take into account the constant of the description language, it shows a more robust behavior than NBDM 1 , which increases rapidly with the increase of pixel edition. Since NC and NBDM 1 have the same type of normalization, we will focus on comparing these normalizations from now on. In the second test, we applied both measures to six datasets with distinct nature (9 images each) to understand how NBDM 1 and NC behave with different types of images. The six datasets were: artistic images from 2 different datasets [3,74]; cellular automata images; diabetic retinopathy images [75]; chest computed radiography (CR) images [76] and photographic images [77]. The results are depicted in Fig. 2(B). Overall, the majority of the datasets show similar behavior regarding the NC and NBDM 1 . The exceptions to this are the CR and cellular automata datasets, which exhibit a more algorithmic behavior. The latter dataset is constituted by images created with small programs with simple rules. Whereas the compressor has difficulty compressing this type of images, the BDM can point to their algorithmic nature, and, thus attribute them with minimal value. This outcome shows the importance of the BDM in the detection of simple algorithmic outputs embedded into data. In the last test, we selected one of the most complex images identified by the NBDM in the last subsection to test if the BDM could accommodate specific data alterations. This test is depicted in Fig. 2 (C). After the binarization process, we performed a super-sample image transformation where each char was amplified to a 4 × 4 representation. This value was selected since the BDM has the default block size value of 4 × 4 in 2D structures. After this operation, the BDM was computed for the original and the super-sampled image. While the original image was measured with 370981 bits, the super-sampled image had only 79 bits. This abrupt decrease in the complexity value indicates that the BDM underestimates the amount of information contained in the object. The BDM analyses object information in blocks instead of looking at the whole object. Specifically, blocks analysed by the BDM (default block size value of 4 × 4 in 2D structures) have the same size as the super-sample image transformation (each char was amplified to a 4 × 4 representation); therefore, the complexity attributed to each block is approximately zero (since each block is composed of all zeros or ones), and hence the overall value attributed to the complexity of the object will drop dramatically. This analysis shows that BDM is not prepared to deal with the information associated with the choice of the model, unlike the NC. The NC relies on the use of a lossless data compressor, bounded by a maximum information channel capacity. From these three tests, we are able to notice some advantages and limitations of both measures. Ranking these measures is not a fair task because they have different characteristics and nature. Therefore, in the remainder of the article, we use the NC and NBDM in a combined mode to recover insights and characteristics from the images of the artistic paintings. 4.2 Information-based measures in images of artistic paintings Herein, we investigate the use of information measures to analyse a dataset of artistic paintings. This dataset [3] contains 4,266 images of artistic paintings from 91 authors, with approximate geometric sizes. The 91 authors are well-known painters, such as Claude Monet, Frida Kahlo, Henri Matisse, Jackson Pollock, Picasso, Rembrandt, and Salvador Dali. In the following subsections, we present the results of applying the measures, combining the NC with the HDC function, measuring local complexity for different authors and constructing a phylogenetic tree, as well as using these features to improve style and artist classification. We also measure the impact of normalizing these images by performing image normalization and then applying the measures mentioned above in the dataset. Afterwards, we compared the average variation difference and the percentage difference between the results obtained for each author. The results are shown in the Supplementary Material in Section A.1. 4.2.1 Global measures analysis In this subsection, we measure an approximation to the Kolmogorov complexity for the dataset of artistic paintings. The same pipeline, described in the methods section, was used, with the difference that the Lloyd-Max algorithm quantization was set to 16, 64, and 256 levels (4, 6, and 8 bits respectively). Important to note that Lloyd-Max algorithm forced normalization of the images for the 16 and 64 levels, while the 256 level was the original level of the images, and, as such, these images were not normalized. This process was performed to evaluate the impact of the quantization on the measures used to approximate the Kolmogorov complexity in artistic painting images. From the results obtained from the measures, we show unknown characteristics and insights into temporal traits. In general, the complexity of each painting follows the example of Fig. 3 . Paintings with low complexity are classified as abstract and minimalist, following simple patterns. As the complexity increases, we start to recognize paintings with different local complexities, meaning, there are regions with high complexity and detail (generally on the center/bottom of the paintings) surrounded by low complexity regions (same color background) namely known as chiaroscuro. This pattern begins fading, as the complexity increases since the highest complexity paintings are also the most irregular, detailed, and convoluted. Regarding the average complexity values for each artist, Fig. 4 shows the average of NBDM 1 and NC, respectively. Each artist has an associated color, and lines of the same color illustrate its relative positional deviation in different quantizations. The same results for NBDM 2 are exposed and discussed in the Supplementary Material. Noticeably, quantization impacts the NBDM 1 more than the NC, since the relative positioning between authors varies more in the former. On average, the variation is 13.4 ± 11.37 relative positions of each author in NBDM 1 , while in NC, the variation is 4.9 ± 4.3 positions. Despite the higher variation present in the NBDM 1 , both measures are capable of detecting styles with low and high complexity. Artists such as Mark Rothko, Lucio Fontana, Piet Mondrian, El Lissitzky can be easily identified on the low side of the complexity spectrum. Minimalism, Abstract Expressionism, and Constructivism movements are associated with these styles. On the other hand, artists from Abstract Expressionism, such as Willem de Kooning, Jackson Pollock, and Jasper Johns, characterize the highest complexity side of the spectrum, as well as other artists with a more detailed and convoluted style, like Gustav Klimt and Vincent van Gogh. Abstract Expressionism is characterized by aggressive features combined with random and geometric features and spontaneity [78]. The reason for Abstract Expressionism artists being present at both extremes of the complexity spectrum is because this style itself divided into two opposites, Action Painting and Color Field. In Action Painting, the paint was thrown directly on the canvas, through instinctive gestures, where chance and randomness determined the evolution of painting [79]. This style is characteristic of artists like Jackson Pollock (known for the technique of ǣdrippingǥ) and Willem de Kooning. On the other hand, Color Field is more mystical and meditative. This style of painting has few elements in the frames, indefinite limits, and explores the sensory effects of color, as well as the subtlety of chromatic relations [80]. A specific example of an artist that followed this trend was Mark Rothko. In all cases, Jackson Pollock had complexity values utterly different from other artists, the average complexity of his paintings being approximate to random (normalized value close to 1). Although he denied his paintings were random, similar results were also found in previous work, which defined Jackson Pollock’s dripping paintings as not typical artworks [5]. 4.2.2 Combining the NC with the roughness exponent of HDC function We used the average NC together with the roughness exponent ( α ) of the two-point height difference correlation (HDC) function, which measures the roughness exponents of brightness surfaces, to assess the ability of these measures to distinguish different styles. Accordingly, we made usage of style labeled paintings available in the dataset. From these labeled images, we computed for its author the average NC and the value of α . The roughness exponent was used as an additional measure since it has proven to be capable of some differentiation between styles [5]. We discarded the usage of BDM due to quantization impacting it more than the NC. Using the average NC and α of each labeled painter, we created a scatter plot (Fig. 5 ) and represented each artistic movement as an ellipse, with the center in the points’ center of mass and with a width corresponding to the standard deviation. As shown in Fig. 5(A), both measures alone are not capable of efficiently separating styles, However, when combined, the styles are well confined into different regions (except for Abstract Expressionism), showing that together these measures are representatives of artistic movements. The roughness exponent α captures the level of brightness and relative spatial position and is correlated to variations in painting techniques and genres [5]. The NC adds to the level of brightness and relative spatial position provided by the HDC, the notion of average information present in each artist’s painting. This amount of information differs depending on the artistic movement and historical circumstances. Interestingly, similar to NC, the roughness exponent of the HDC varies greatly in Abstract Expressionism, being that in this artistic movement, there is an inverse correlation between the NC and α . Namely, artists like Jackson Pollock and Willem de Kooning (Action Painting) presented a high average NC and a low α , whereas, Mark Rothko (Color Field) had polar results. This atypical behavior corroborates the big difference between the two currents of Abstract Expressionism. The Action Painting usage of instinctive gestures and randomness creates high NC values and spatial correlation approaching a random image. In contrast, in Color Field, we get more minimalist images with high spatial contrast between regions but low complexity. 4.2.3 Local complexity of paintings In this section, we divided the images into identical quadrilateral sizes and measure the algorithmic information for each one ( 16 × 16 blocks). Then, we computed the average of each quadrilateral for all the paintings for each painter. The results are shown in Fig. 6 , illustrating the same authors as those in Fig. 3. Note however that matrices of Fig. 6 were computed using all the authors’ paintings present in the dataset. The complete results are available on the website associated with this article. The same computation was repeated for blocks of sizes 8 × 8 and 32 × 32 . Analysis of these results, included in the Supplementary Material, show that 16 × 16 is the minimum patch size for which the differences in the compression rate are noticeable and can therefore be used as a measure between paintings. All artists have a unique complexity matrix (fingerprint). This fingerprint shows, on average, where artists paint with more detail and give more emphasis as well as the average range of complexity the artist operates. For instance, Jackson Pollock and Jasper Johns show high complexity values dispersed over the canvas. At the same time, artists like Francis Bacon and George de la Tour focus more on the center of the canvas, and Mark Rothko and Piet Mondrian give their highest complexities around the borders of paintings. Since the 16 × 16 fingerprints conveyed the best results regarding detail and differentiation (see Supplementary Material in Section A.2), the phylogenetic trees were constructed utilizing the distance computed from the fingerprints with block size. Concretely, two phylogenetic trees were constructed to portray the relations between different artists. One tree was constructed using the UPGMA algorithm, which is illustrated in Fig. 7 , and another tree was build using the Kruskal minimum spanning tree algorithm [60], which is depicted in the Figure S3 of the Supplementary Material in Section A.4. The tree shows the fingerprint’s capacity of grouping artists from the same artistic movements mutually. Broad groupings of artists from styles are present in the tree, namely, Renaissance, Baroque, Romanticists, Impressionists, Surrealism, Cubism, and Abstract Expressionism. Also, the tree shows smaller groupings of sister leaf-nodes with the same style. On the other hand, the tree depicts relationships of influence between authors of different artistic movements. This relation is seen in the case of Titian, who influenced Diego Velazquez; Caravaggio, who influenced Francisco de Zurbarán; Frida Kahlo, who influenced Amedeo Modigliani; Sandro Botticelli who influenced William Blake; Claude Lorrain who influenced Joseph Mallord William Turner; and Peter Paul Rubens who influenced Jean-Antoine Watteau. On the other hand, some authors seem unrelated in style and influence, for instance, Francis Bacon and Georges de la Tour, George Braque and Hieronymus Bosch, Peter Paul Rubens and Frida Kahlo, Max Ernst and Giorgione, and Rembrandt van Rijn and Roy Lichtenstein. There can be many reasons for this to occur, for instance, the number of regions the images were divided can be sub-optimal for some images of artistic paintings, decreasing the sensitivity of the measure and jeopardizing the tree’s construction. On the other hand, the algorithm used to measure the similarity between matrices or the algorithm used to construct the tree (UPGMA) may not be the most appropriate for all cases, however, we have tested the Kruskal minimum spanning tree algorithm which yielded similar results (see Supplementary Material in section ). Additionally, these seemingly unrelated connections could reveal undiscovered elements and relationships. For instance, one of Roy Lichtenstein’s early artistic idols was Rembrandt van Rijn. Moreover, if artists are not related regarding the artistic movement or influence, the vicinity among them could be representing another property. This aspect is not necessarily related to the period or movement the artists were inserted in, but rather, the way authors projected their compositions, ideas, and impressions onto the canvas. Complexity can be approximated by the total number of properties transmitted by an object and detected by an observer. By dividing images into blocks of equal size and evaluating its local complexity, we are quantifying the local information being transmitted. On the other hand, by averaging the canvas results per artist, we obtain a matrix that describes how the author exposes information to the observer. This information intertwines various notions critical to how the work is perceived, such as composition which describes where the artist places the subject and how the background elements support it, as well as the unity, balance, movement, rhythm, focus, contrast, pattern, and proportion of the painting. For instance, the proximity between Hans Holbein and Vermeer could be due to both of them having used optics to achieve precise positioning in their compositions, namely by performing a combination of curved mirrors, camera obscura, and camera lucida [81]. Another example that this information can convey is space by depicting where positive (subject itself, which is usually more detailed) and negative (the area of painting around it) spaces are on the canvas. Artists can play with a balance between these two spaces to further influence how viewers interpret their work. Therefore, the similarity between different artists concerning the regional (local) complexity can reflect the similarity in thought regarding their approaches to painting. For instance, the proximity between Francis Bacon and Georges de la Tour could be due to the former being heavily influenced by the Baroque style and having made dramatic use of contrasts of light and shadow. These methods are characteristic of the chiaroscuro principle and its radicalization in the Tenebrista school (signature style of Georges de la Tour) [82,83]. The intense contrasts of light and shadow highlight the characters, and although exaggerated, it is lighting that increases the feeling of realism, making the muscles and facial expressions more evident. Simultaneously, the presence of large blackened areas highlights the chromatic research and the illuminated space, which acquire their value as elements of the composition. We conclude that this novel technique is a unique descriptor of the authors’ paintings since it not only aggregates authors of the same style close to each other and demonstrates the influences that authors had on others. It also serves as an insight into the way the artist projects its art. 4.2.4 Evaluation of measures for classification purposes To quantitatively evaluate the use of these measures for classification purposes, we assessed their impact when used as additional features to improve state-of-the-art classification methods. For this purpose, we recreated a recently published state-of-the-art method as a baseline and improved the results by combining our proposed measures. Based on current methods [52,53], we extracted a Gram representation using the first convolutional layer from the fifth convolutional block of the VGG16 network which was pre-trained with the imagenet dataset (no significant result difference was found between the use of VGG16 or VGG19). Principal component analysis (PCA) was applied to the Gram matrix to reduce the dimensionality and, finally, this vector was provided to an SVM to perform classification. Afterwards, the features obtained from computing the HDC and the regional complexity were used for author and style classification using the XGBoost classifier [84] and combined with the baseline classifier via a Voting Classifier ensemble. The results of the baseline and ensemble classifiers, applied to the Paintings91 dataset in the author and style classification task using the labels provided in the dataset, are shown in Table 1 . The results show that the inclusion of the Regional Complexity, increased the accuracy of the results 2.2 p.p. and 1.0 p.p in the style and author classification tasks respectively. Moreover, the overall inclusion of the proposed measures (HDC + RC) increased the accuracy in both classification tasks by 2.8 p.p. and, 2.0 p.p. in the style and author classification tasks, respectively. These results indicate that these predictors are useful auxiliary features capable of improving current methodologies in the classification of artistic paintings. This is congruent with the results obtained with Nanny et al. [85], since handcrafted features and non-handcrafted features seem to extract different information from the input images and, as a result, the fusion of the two types of features improves the results obtained when using non-handcrafted features only. Furthermore, regional complexity (RC) has a higher impact on the improvement of the accuracy than the HDC features, demonstrating the importance and distinction of Regional Complexity as a feature. 5 Discussion In this work, we develop, use, and compare unsupervised pattern recognition techniques to quantify information in images of artistic paintings. We rely on two approaches, namely data compression using the Normalized Compression (NC), and the Block Decomposition Method (BDM), to estimate information of both probabilistic and algorithmic sources. To approximate the NC, we benchmark a set of data compressors, where we show that the most effective for this dataset is PAQ8. Subsequently, this article is organized into two broad sections. The first is the evaluation and comparison of information-based measures; the second is applying these information-based measures to a dataset of artistic paintings. On the measure evaluation section, we assessed the NC and BDM using three tests. In the first test, we evaluated the NC and two normalizations of the BDM, regarding their robustness when images undergo uniform pixel editing and their behavior when applied to different types of datasets. We found that in terms of uniform pixel editing, the NC is more robust than BDM with the same kind of normalization. The NC is a measure of compression (in this case, using the PAQ compressor) that makes use of the digital object in its entirety to create the shortest possible representation without loss of information. In contrast, BDM divides the digital object into blocks and, based on the complexity of the blocks, estimates the image complexity in its entirety. This means that BDM cannot determine the information shared between the blocks, which causes it to increase, when compared to the NC, with the increase in uniform pixel editing. In the second test, we compared both measures using different image natures. We found that the results of the NC and NBDM are similar, except for the computed radiography and the cellular automata dataset, which exhibited a more algorithmic behavior. The cellular automata data was created with small programs with simple rules. While the compressor had difficulty compressing this data, BDM could approximate their algorithmic nature and thus assign them a value close to a minimal complexity value. The ability to identify an algorithmic nature incorporated in the data demonstrates the relevance of BDM as a measure. In the third test, we found that a super sample image transformation causes an underestimation of the amount of information contained in the object by BDM. Again, this is due to BDM analysing the object in blocks, instead of using the object in its entirety. Since the ampliation size was the same as the blocks analysed by BDM, the complexity attributed to each block was approximately zero. Consequently, the overall value attributed to the image complexity decreased dramatically. This aspect demonstrated that BDM cannot handle information contained between each block and can easily underestimate the amount of information present in a digital object. In the second phase, we applied these measures to estimate the complexity of a dataset of paintings. We calculated the NC and NBDM in this dataset with different quantizations and assessed the results in terms of average complexity per author. Afterward, we combined the NC with the exponent of the roughness of the HDC function in the labeled paintings of the dataset. Finally, we computed the average regional complexity of each author regarding their paintings and built a phylogenetic tree. We found that paintings with low complexity are abstract, minimalist, and follow simple patterns. Paintings with a slightly higher average complexity possess different regional complexities, specifically, a region with high complexity and detail surrounded by a background of low complexity. With more complexity, this noticeable pattern begins to fade, and the most complex paintings are globally irregular, detailed, and convoluted. Regarding the average complexity values for each artist, we found that NC and NBDM behave similarly, where quantization impacted more the NBDM. We also found that the low side of the complexity spectrum was characterized by Abstract Expressionism, Minimalism, Constructivism movements, with authors such as Mark Rothko, Lucio Fontana, Piet Mondrian, and El Lissitzky. Also, artists from Abstract Expressionism characterized the high complexity side of the spectrum, such as Willem de Kooning, Jackson Pollock, and Jasper Johns, as well as other artists with a more detailed and convoluted style, like Gustav Klimt and Vincent van Gogh. Due to two different currents (Color Field with authors with low average complexity and Action Painting with authors with high complexity), Abstract Expressionism was present at the polar ends of the spectrum. In all cases, Jackson Pollock had average complexity values that were utterly different from other artists, being the average complexity of his paintings close to random. Although he denied being a creator of random paintings, this result and others [5] seem to indicate that Jackson Pollock’s dripping paintings are not typical artworks, and this is possibly related to the inclusion of many symbolic layers and dispersion intentions over the canvas by the author. When evaluating the artists’ average NC together with the roughness exponent ( α ) of the HDC function in the labels images of the dataset, we found that styles are well confined into different regions, showing that the combination of these measures gives a robust representation of artistic movements. The NC adds to the level of brightness and relative spatial position evidenced by the roughness exponent, the notion of average information present in each artist’s painting, which is consistent within the same style and historical circumstances. We also find that in Abstract Expressionism, the NC is inversely correlated to α . Concretely, artists related to Colour Field painting presented a high α and low NC, whereas artists related to Action Painting presented the exact polar results (low α and high NC). Finally, we divided the image into equal quadrilateral parts and estimated the local complexity of each painting on the dataset and used it to ascertain each artists average regional matrix (fingerprint). Complexity can be thought of as a measure of the total number of properties transmitted by an object and detected by an observer. By dividing images into blocks of equal size and evaluating its local complexity, we quantified the local information being transmitted. Furthermore, by averaging the canvas results per artist, we obtain a unique fingerprint that describes how the author exposes information to the observer. Among other things, these fingerprints give specific insights regarding each artist’s way of painting, showing where, on average, artists paint with more detail and give more emphasis, while also providing insights into each artist’s range of complexity. Using these matrices, we computed a distance matrix and utilized it to construct a phylogenetic tree. We discovered that these phylogenetic trees aggregated authors of the same style close to each other, as well as artists’ influence relationships, like Francis Bacon and Georges de la Tour, and George Braque and Hieronymus Bosch. Furthermore, we observed proximity between artists due to shared methods and techniques which are not correlated with the temporal era or artistic movement. An example of this occurrence is the proximity between Hans Holbein and Vermeer which don’t share styles, but both used optics to achieve precise positioning in their compositions. This evidence shows that artists’ fingerprints contain critical information into how the work is perceived, such as composition, unity, balance, movement, rhythm, focus, contrast, pattern, and proportion of the painting and space. Finally, we show that these measures improve current methodologies in the classification of artistic paintings and thus extract information which differs from non-handcrafted features. Furthermore, regional complexity provided the largest increase in accuracy on the classification tasks, showing its relevance as a descriptor of images of artistic paintings. 6 Conclusions In this paper, we introduce novel solutions to the field of computer analysis of artistic paintings and the problem of artist classification and authentication. Specifically, we assessed the viability of unsupervised measures that approximate the quantity of probabilistic and algorithmic information for performing these tasks. Our direct comparison between NC and BDM allowed us to understand the strengths and weaknesses of both measures. Although BDM has difficulty dealing with uniform pixel edition and full information quantification given the block representability, it serves as a useful tool for measure and indentification of data content having similarity to simple algorithms. On the other hand, the NC is more robust to data alterations (pixel edition and quantization) and is able to measure the quantity of information without underestimation. Regarding the application of information-based measures in artistic paintings, we studied and developed techniques that can be valuable for art authorship attribution and validation, art style categorization and organization, and art content explanation. Namely, the NC proved to be a robust measure that as a whole gives us some insight regarding the complexity of different styles showing hidden patterns and relationships present in artistic paintings that share the same range in complexity. Furthermore, it could be a stylistic descriptor when coupled with the roughness exponent α . On the other hand, fingerprints depict how each author perform typical content distribution on canvas. Thus, they can provide a suitable means of art content explanation, as well as being valuable for art authorship attribution and validation. Moreover, since they provided insights regarding the artists’ way of painting, they can be used as a means of relating authors, being therefore useful for depicting artists’ stylistic influences, and shared techniques. Additionally, using the distance between the artists’ regional complexity, we also find some interesting links between authors regarding the usage of space, technique, composition, rhythm, and proportion. Finally, we demonstrated that the regional complexity and the HDC function of the paintings could serve as useful auxiliary features capable of improving current methodologies in author and style classification of images of artistic paintings. Regarding future continuations to this study, there are many possible future lines of work that can be considered. For example, in this work we analysed the images of paintings by converting them to monochrome, and it would be interesting to separate the colour channels and to analyse them separately, therefore studying the influence of colour in the paintings in terms of complexity. Additionally, it would be interesting to explore how to separate different characteristics of the fingerprint, as well as detecting unknown repeated patterns that appear multiple times in a painting by creating and analysing their complexity surfaces [13]. Lastly, another interesting study would be to replicate the developed work in this article using a competitive compressor that would select the best compressor model for each painting or region. Website A support website to this site can be accessed at This site showcases among other things, the pipeline of this study, the author’s average NC and NBDM variation for different quantization levels, the results of combining the NC with the roughness exponent of HDC function ( α ), a complete catalogue of each author’s fingerprints as well as several examples of each author’s paintings, and the computed phylogenetic trees with a magnifier to allow a better observation of the results. CRediT authorship contribution statement Jorge Miguel Silva: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Diogo Pratas: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Rui Antunes: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Sérgio Matos: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Armando J. Pinho: Conceptualization, Validation, Writing - original draft, Writing - review & editing. Declaration of Competing Interest The authors declare no competing interests. Acknowledgements This work was funded by National Funds through the FCT - Foundation for Science and Technology, in the context of the project UID/CEC/00127/2019 and the research grants SFRH/BD/141851/2018 and SFRH/BD/137000/2018 for J.M.S and R.A, respectively. D.P. is funded by national funds through FCT - Fundalo para a Ciłncia e a Tecnologia, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019. Supplementary material Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.patcog.2021.107864. Appendix A Supplementary materials Supplementary Data S1 Supplementary Raw Research Data. This is open data under the CC BY license Supplementary Data S1 References [1] R.W. Weisberg Creativity: Understanding Innovation in Problem Solving, Science, Invention, and the Arts 2006 John Wiley & Sons R. W. Weisberg, Creativity: Understanding innovation in problem solving, science, invention, and the arts, John Wiley & Sons, 2006. [2] A. Hertzmann Can computers create art? Arts vol. 7 2018 Multidisciplinary Digital Publishing Institute 18 A. Hertzmann, Can computers create art?, in: Arts, volume 7, Multidisciplinary Digital Publishing Institute, 2018, p. 18. [3] F.S. Khan S. Beigpour J. Van de Weijer M. Felsberg Painting-91: a large scale database for computational painting categorization Mach. Vis. Appl. 25 6 2014 1385 1397 F. S. Khan, S. Beigpour, J. Van de Weijer, M. Felsberg, Painting-91: a large scale database for computational painting categorization, Machine vision and applications 25(6) (2014) 1385–1397. [4] S. Lyu D. Rockmore H. Farid A digital technique for art authentication Proc. Natl. Acad. Sci. 101 49 2004 17006 17010 S. Lyu, D. Rockmore, H. Farid, A digital technique for art authentication, Proceedings of the National Academy of Sciences 101(49) (2004) 17006–17010. [5] D. Kim S.-W. Son H. Jeong Large-scale quantitative analysis of painting arts Sci. Rep. 4 2014 7370 D. Kim, S.-W. Son, H. Jeong, Large-scale quantitative analysis of painting arts, Scientific reports 4 (2014) 7370. [6] H. Zhang S. Sfarra K. Saluja J. Peeters J. Fleuret Y. Duan H. Fernandes N. Avdelidis C. Ibarra-Castanedo X. Maldague Non-destructive investigation of paintings on canvas by continuous wave terahertz imaging and flash thermography J. Nondestruct. Eval. 36 2 2017 34 H. Zhang, S. Sfarra, K. Saluja, J. Peeters, J. Fleuret, Y. Duan, H. Fernandes, N. Avdelidis, C. Ibarra-Castanedo, X. Maldague, Non-destructive investigation of paintings on canvas by continuous wave terahertz imaging and flash thermography, Journal of Nondestructive Evaluation 36(2) (2017) 34. [7] H. Zenil S. Hernández-Orozco N.A. Kiani F. Soler-Toscano A. Rueda-Toicen J. Tegnér A decomposition method for global evaluation of Shannon entropy and local estimations of algorithmic complexity Entropy 20 8 2018 605 H. Zenil, S. Hernández-Orozco, N. A. Kiani, F. Soler-Toscano, A. Rueda-Toicen, J. Tegnér, A decomposition method for global evaluation of Shannon entropy and local estimations of algorithmic complexity, Entropy 20(8) (2018) 605. [8] J.-P. Delahaye H. Zenil Numerical evaluation of algorithmic complexity for short strings: a glance into the innermost structure of randomness Applied Mathematics and Computation 219 1 2012 63 77 10.1016/j.amc.2011.10.006 J.-P. Delahaye, H. Zenil, Numerical evaluation of algorithmic complexity for short strings: A glance into the innermost structure of randomness, Applied Mathematics and Computation 219(1) (2012) 63–77. Towards a Computational Interpretation of Physical Theories. 10.1016/j.amc.2011.10.006 Towards a Computational Interpretation of Physical Theories. [9] F. Soler-Toscano H. Zenil J.-P. Delahaye N. Gauvrit Calculating Kolmogorov complexity from the output frequency distributions of small Turing machines PloS one 9 5 2014 F. Soler-Toscano, H. Zenil, J.-P. Delahaye, N. Gauvrit, Calculating Kolmogorov complexity from the output frequency distributions of small Turing machines, PloS one 9(5) (2014). [10] J. Smiers Arts Under Pressure: Protecting Cultural Diversity in the Age of Globalisation 2003 Zed Books J. Smiers, Arts under pressure: Protecting cultural diversity in the age of globalisation, Zed Books, 2003. [11] P.J. Ferreira A.J. Pinho A method to detect repeated unknown patterns in an image International Conference Image Analysis and Recognition 2014 Springer 12 19 P. J. Ferreira, A. J. Pinho, A method to detect repeated unknown patterns in an image, in: International Conference Image Analysis and Recognition, Springer, 2014, pp. 12–19. [12] A.J. Pinho P.J. Ferreira Finding unknown repeated patterns in images 2011 19th European Signal Processing Conference 2011 IEEE 584 588 A. J. Pinho, P. J. Ferreira, Finding unknown repeated patterns in images, in: 2011 19th European Signal Processing Conference, IEEE, 2011, pp. 584–588. [13] D. Pratas A.J. Pinho On the detection of unknown locally repeating patterns in images International Conference Image Analysis and Recognition 2012 Springer 158 165 D. Pratas, A. J. Pinho, On the detection of unknown locally repeating patterns in images, in: International Conference Image Analysis and Recognition, Springer, 2012, pp. 158–165. [14] A. Romashchenko A. Shen N. Vereshchagin Combinatorial interpretation of Kolmogorov complexity Theor. Comput. Sci. 271 1–2 2002 111 123 A. Romashchenko, A. Shen, N. Vereshchagin, Combinatorial interpretation of Kolmogorov complexity, Theoretical Computer Science 271(1-2) (2002) 111–123. [15] R.K. Niven Combinatorial entropies and statistics Eur. Phys. J. B 70 1 2009 49 63 R. K. Niven, Combinatorial entropies and statistics, The European Physical Journal B 70(1) (2009) 49–63. [16] S. Mantaci A. Restivo G. Rosone M. Sciortino A new combinatorial approach to sequence comparison Theory Comput. Syst. 42 3 2008 411 429 S. Mantaci, A. Restivo, G. Rosone, M. Sciortino, A new combinatorial approach to sequence comparison, Theory of Computing Systems 42(3) (2008) 411–429. [17] C.E. Shannon A mathematical theory of communication Bell Syst. Tech. J. 27 3 1948 379 423 C. E. Shannon, A mathematical theory of communication, Bell system technical journal 27(3) (1948) 379–423. [18] A.N. Kolmogorov Three approaches to the quantitative definition of information Probl. Inf. Transm. 1 1 1965 1 7 A. N. Kolmogorov, Three approaches to the quantitative definition of information’, Problems of information transmission 1(1) (1965) 1–7. [19] R.J. Solomonoff A formal theory of inductive inference. Part I Inf. Control 7 1 1964 1 22 R. J. Solomonoff, A formal theory of inductive inference. Part I, Information and control 7(1) (1964a) 1–22. [20] R.J. Solomonoff A formal theory of inductive inference. Part II Inf. Control 7 2 1964 224 254 R. J. Solomonoff, A formal theory of inductive inference. Part II, Information and control 7(2) (1964b) 224–254. [21] G.J. Chaitin On the length of programs for computing finite binary sequences J. ACM (JACM) 13 4 1966 547 569 G. J. Chaitin, On the length of programs for computing finite binary sequences, Journal of the ACM (JACM) 13(4) (1966) 547–569. [22] F. Soler-Toscano H. Zenil A computable measure of algorithmic probability by finite approximations with an application to integer sequences Complexity 2017 2017 F. Soler-Toscano, H. Zenil, A computable measure of algorithmic probability by finite approximations with an application to integer sequences, Complexity 2017 (2017). [23] N. Gauvrit H. Zenil F. Soler-Toscano J.-P. Delahaye P. Brugger Human behavioral complexity peaks at age 25 PLoS Comput. Biol. 13 4 2017 N. Gauvrit, H. Zenil, F. Soler-Toscano, J.-P. Delahaye, P. Brugger, Human behavioral complexity peaks at age 25, PLoS computational biology 13(4) (2017). [24] M. Li J.H. Badger X. Chen S. Kwong P. Kearney H. Zhang An information-based sequence distance and its application to whole mitochondrial genome phylogeny Bioinformatics 17 2 2001 149 154 M. Li, J. H. Badger, X. Chen, S. Kwong, P. Kearney, H. Zhang, An information-based sequence distance and its application to whole mitochondrial genome phylogeny, Bioinformatics 17(2) (2001) 149–154. [25] R. Cilibrasi P.M.B. Vitányi Clustering by compression IEEE Trans. Inf. Theory 51 4 2005 1523 1545 R. Cilibrasi, P. M. B. Vitányi, Clustering by compression, IEEE Transactions on Information theory 51(4) (2005) 1523–1545. [26] R. Cilibrasi P. Vitanyi Automatic extraction of meaning from the web 2006 IEEE International Symposium on Information Theory 2006 2309 2313 R. Cilibrasi, P. Vitanyi, Automatic extraction of meaning from the web, in: 2006 IEEE International Symposium on Information Theory, 2006, pp. 2309–2313. [27] M. Cebrián M. Alfonseca A. Ortega The normalized compression distance is resistant to noise IEEE Trans. Inf. Theory 53 5 2007 1895 1900 M. Cebrián, M. Alfonseca, A. Ortega, The normalized compression distance is resistant to noise, IEEE Transactions on Information Theory 53(5) (2007) 1895–1900. [28] A.R. Cohen P.M.B. Vitányi Normalized compression distance of multisets with applications IEEE Trans. Pattern Anal. Mach.Intell. 37 8 2014 1602 1614 A. R. Cohen, P. M. B. Vitányi, Normalized compression distance of multisets with applications, IEEE transactions on pattern analysis and machine intelligence 37(8) (2014) 1602–1614. [29] D. Pratas A.J. Pinho On the approximation of the Kolmogorov complexity for DNA sequences Iberian Conference on Pattern Recognition and Image Analysis 2017 Springer 259 266 D. Pratas, A. J. Pinho, On the approximation of the Kolmogorov complexity for DNA sequences, in: Iberian Conference on Pattern Recognition and Image Analysis, Springer, 2017, pp. 259–266. [30] S.S. Maniccam N. Bourbakis Lossless compression and information hiding in images Pattern Recognit. 37 3 2004 475 486 S. S. Maniccam, N. Bourbakis, Lossless compression and information hiding in images, Pattern Recognition 37(3) (2004) 475–486. [31] Z.-M. Lu S.-Z. Guo Lossless Information Hiding in Images 2016 Syngress Z.-M. Lu, S.-Z. Guo, Lossless information hiding in images, Syngress, 2016. [32] D. Pratas A.J. Pinho P.J. Ferreira Efficient compression of genomic sequences 2016 Data Compression Conference (DCC) 2016 IEEE 231 240 D. Pratas, A. J. Pinho, P. J. Ferreira, Efficient compression of genomic sequences, in: 2016 Data Compression Conference (DCC), IEEE, 2016, pp. 231–240. [33] D. Pratas M. Hosseini A.J. Pinho Substitutional tolerant Markov models for relative compression of DNA sequences International Conference on Practical Applications of Computational Biology & Bioinformatics 2017 Springer 265 272 D. Pratas, M. Hosseini, A. J. Pinho, Substitutional tolerant markov models for relative compression of DNA sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2017, pp. 265–272. [34] A.J. Pinho A.J.R. Neves P.J. Ferreira Inverted-repeats-aware finite-context models for DNA coding 2008 16th European Signal Processing Conference 2008 IEEE 1 5 A. J. Pinho, A. J. R. Neves, P. J. Ferreira, Inverted-repeats-aware finite-context models for DNA coding, in: 2008 16th European Signal Processing Conference, IEEE, 2008, pp. 1–5. [35] M. Mahoney, Data Compression Programs, 2020. accessed May 16, [36] J. Rissanen Modeling by shortest data description Automatica 14 5 1978 465 471 J. Rissanen, Modeling by shortest data description, Automatica 14(5) (1978) 465–471. [37] M. Li X. Chen X. Li B. Ma P.M.B. Vitányi The similarity metric IEEE Trans. Inf. Theory 50 12 2004 3250 3264 M. Li, X. Chen, X. Li, B. Ma, P. M. B. Vitányi, The similarity metric, IEEE transactions on Information Theory 50(12) (2004) 3250–3264. [38] M. Li P. Vitányi An Introduction to Kolmogorov Complexity and its Applications vol. 3 2008 Springer M. Li, P. Vitányi, et al., An introduction to Kolmogorov complexity and its applications, volume 3, Springer, 2008. [39] R.P. Taylor A.P. Micolich D. Jonas Fractal analysis of Pollock’s drip paintings Nature 399 6735 1999 R. P. Taylor, A. P. Micolich, D. Jonas, Fractal analysis of Pollock’s drip paintings, Nature 399(6735) (1999) 422–422. 422–422 [40] C.R. Johnson E. Hendriks I.J. Berezhnoy E. Brevdo S.M. Hughes I. Daubechies J. Li E. Postma J.Z. Wang Image processing for artist identification IEEE Signal Process. Mag. 25 4 2008 37 48 C. R. Johnson, E. Hendriks, I. J. Berezhnoy, E. Brevdo, S. M. Hughes, I. Daubechies, J. Li, E. Postma, J. Z. Wang, Image processing for artist identification, IEEE Signal Processing Magazine 25(4) (2008) 37–48. [41] J. Li J.Z. Wang Studying digital imagery of ancient paintings by mixtures of stochastic models IEEE Trans. Image Process. 13 3 2004 340 353 J. Li, J. Z. Wang, Studying digital imagery of ancient paintings by mixtures of stochastic models, IEEE Transactions on Image Processing 13(3) (2004) 340–353. [42] M. Bressan C. Cifarelli F. Perronnin An analysis of the relationship between painters based on their work 2008 15th IEEE International Conference on Image Processing 2008 IEEE 113 116 M. Bressan, C. Cifarelli, F. Perronnin, An analysis of the relationship between painters based on their work, in: 2008 15th IEEE International Conference on Image Processing, IEEE, 2008, pp. 113–116. [43] B.A. Olshausen M.R. DeWeese Applied mathematics: the statistics of style Nature 463 7284 2010 1027 B. A. Olshausen, M. R. DeWeese, Applied mathematics: The statistics of style, Nature 463(7284) (2010) 1027. [44] J.M. Hughes D.J. Graham D.N. Rockmore Quantification of artistic style through sparse coding analysis in the drawings of Pieter Bruegel the Elder Proc. Natl. Acad. Sci. 107 4 2010 1279 1283 J. M. Hughes, D. J. Graham, D. N. Rockmore, Quantification of artistic style through sparse coding analysis in the drawings of Pieter Bruegel the Elder, Proceedings of the National Academy of Sciences 107(4) (2010) 1279–1283. [45] D.G. Stork Y. Furuichi Image analysis of paintings by computer graphics synthesis: an investigation of the illumination in Georges de la Tour’s Christ in the carpenter’s studio Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100J D. G. Stork, Y. Furuichi, Image analysis of paintings by computer graphics synthesis: an investigation of the illumination in Georges de la Tour’s Christ in the carpenter’s studio, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100J. [46] M. Lettner R. Sablatnig Estimating the original drawing trace of painted strokes Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100C M. Lettner, R. Sablatnig, Estimating the original drawing trace of painted strokes, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100C. [47] M. Shahram D.G. Stork D. Donoho Recovering layers of brush strokes through statistical analysis of color and shape: an application to van Gogh’s self portrait with grey felt hat Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 68100D M. Shahram, D. G. Stork, D. Donoho, Recovering layers of brush strokes through statistical analysis of color and shape: an application to van Gogh’s\" self portrait with grey felt hat\", in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 68100D. [48] S.B. Hedges Image analysis of renaissance copperplate prints Computer Image Analysis in the Study of Art vol. 6810 2008 International Society for Optics and Photonics 681009 S. B. Hedges, Image analysis of renaissance copperplate prints, in: Computer image analysis in the study of art, volume 6810, International Society for Optics and Photonics, 2008, p. 681009. [49] V.M. Petrov Entropy and stability in painting: an information approach to the mechanisms of artistic creativity Leonardo 35 2 2002 197 202 V. M. Petrov, Entropy and stability in painting: An information approach to the mechanisms of artistic creativity, Leonardo 35(2) (2002) 197–202. [50] J.T. Machado A.M. Lopes Artistic painting: a fractional calculus perspective Appl. Math. Modell. 65 2019 614 626 J. T. Machado, A. M. Lopes, Artistic painting: A fractional calculus perspective, Applied Mathematical Modelling 65 (2019) 614–626. [51] K.-C. Peng T. Chen Cross-layer features in convolutional neural networks for generic classification tasks 2015 IEEE International Conference on Image Processing (ICIP) 2015 IEEE 3057 3061 K.-C. Peng, T. Chen, Cross-layer features in convolutional neural networks for generic classification tasks, in: 2015 IEEE International Conference on Image Processing (ICIP), IEEE, 2015, pp. 3057–3061. [52] H. Mao M. Cheung J. She DeepArt: learning joint representations of visual arts Proceedings of the 25th ACM International conference on Multimedia 2017 1183 1191 H. Mao, M. Cheung, J. She, Deepart: Learning joint representations of visual arts, in: Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1183–1191. [53] W. Chu Y. Wu Image style classification based on learnt deep correlation features IEEE Trans. Multimed. 20 9 2018 2491 2502 W. Chu, Y. Wu, Image style classification based on learnt deep correlation features, IEEE Transactions on Multimedia 20(9) (2018) 2491–2502. [54] D. Hammer A. Romashchenko A. Shen N. Vereshchagin Inequalities for Shannon entropy and Kolmogorov complexity J. Comput. Syst. Sci. 60 2 2000 442 464 D. Hammer, A. Romashchenko, A. Shen, N. Vereshchagin, Inequalities for Shannon entropy and Kolmogorov complexity, Journal of Computer and System Sciences 60(2) (2000) 442–464. [55] T. Henriques H. Gonçalves L. Antunes M. Matias J.a. Bernardes C. Costa-Santos Entropy and compression: two measures of complexity J. Eval. Clin. Pract. 19 6 2013 1101 1106 T. Henriques, H. Gonçalves, L. Antunes, M. Matias, J. a. Bernardes, C. Costa-Santos, Entropy and compression: two measures of complexity, Journal of Evaluation in Clinical Practice 19(6) (2013) 1101–1106. [56] S. Terwijn L. Torenvliet P.M.B. Vitányi Nonapproximablity of the normalized information distance CoRR 2009 abs/0910.4353 S. Terwijn, L. Torenvliet, P. M. B. Vitányi, Nonapproximablity of the normalized information distance, CoRR abs/0910.4353 (2009). 0910.4353 [57] A. Rybalov On the strongly generic undecidability of the halting problem Theor. Comput. Sci. 377 1–3 2007 268 270 A. Rybalov, On the strongly generic undecidability of the halting problem, Theoretical Computer Science 377(1-3) (2007) 268–270. [58] P. Bloem F. Mota S. de Rooij L. Antunes P. Adriaans A safe approximation for Kolmogorov complexity International Conference on Algorithmic Learning Theory 2014 Springer 336 350 P. Bloem, F. Mota, S. de Rooij, L. Antunes, P. Adriaans, A safe approximation for Kolmogorov complexity, in: International Conference on Algorithmic Learning Theory, Springer, 2014, pp. 336–350. [59] R.R. Sokal A statistical method for evaluating systematic relationships Univ. Kansas, Sci. Bull. 38 1958 1409 1438 R. R. Sokal, A statistical method for evaluating systematic relationships, Univ. Kansas, Sci. Bull. 38 (1958) 1409–1438. [60] J.B. Kruskal On the shortest spanning subtree of a graph and the traveling salesman problem Proc. Am. Math. Soc. 7 1 1956 48 50 J. B. Kruskal, On the shortest spanning subtree of a graph and the traveling salesman problem, Proceedings of the American Mathematical society 7(1) (1956) 48–50. [61] S. Lloyd Least squares quantization in PCM IEEE Trans. Inf. Theory 28 2 1982 129 137 S. Lloyd, Least squares quantization in PCM, IEEE transactions on information theory 28(2) (1982) 129–137. [62] D.S. Taubman M.W. Marcellin JPEG2000: image compression fundamentals Stand. Pract. 11 2 2002 D. S. Taubman, M. W. Marcellin, JPEG2000: Image compression fundamentals, Standards and Practice 11(2) (2002). [63] J. Gailly, M. Adler, The gzip home page, 2020. accessed May 16, [64] bzip2, 2020. accessed May 16, [65] L. Collin, XZ Utils, 2020. accessed May 16, [66] I. Pavlov, 7-Zip, 2020. accessed May 16, [67] M. Hosseini D. Pratas A.J. Pinho AC: a compression tool for amino acid sequences Interdiscip. Sci. Comput. Life Sci. 11 1 2019 68 76 M. Hosseini, D. Pratas, A. J. Pinho, AC: A compression tool for amino acid sequences, Interdisciplinary Sciences: Computational Life Sciences 11(1) (2019) 68–76. [68] J. Cleary I. Witten Data compression using adaptive coding and partial string matching IEEE Trans. Commun. 32 4 1984 396 402 J. Cleary, I. Witten, Data compression using adaptive coding and partial string matching, IEEE transactions on Communications 32(4) (1984) 396–402. [69] A.J. Buchner, PAQ, 2020. accessed May 16, [70] M.V. Mahoney Adaptive weighing of context models for lossless data compression Technical Report 2005 Florida Institute of Technology CS Department of the W University Blvd M. V. Mahoney, Adaptive weighing of context models for lossless data compression, Technical Report, Florida Institute of Technology CS Department of the W University Blvd, 2005. [71] J. Rissanen G.G. Langdon Arithmetic coding IBM J. Res. Dev. 23 2 1979 149 162 J. Rissanen, G. G. Langdon, Arithmetic coding, IBM Journal of research and development 23(2) (1979) 149–162. [72] A. Moffat R.M. Neal I.H. Witten Arithmetic coding revisited ACM Trans. Inf. Syst. (TOIS) 16 3 1998 256 294 A. Moffat, R. M. Neal, I. H. Witten, Arithmetic coding revisited, ACM Transactions on Information Systems (TOIS) 16(3) (1998) 256–294. [73] B. Knoll N. de Freitas A machine learning perspective on predictive coding with PAQ8 2012 Data Compression Conference 2012 IEEE 377 386 B. Knoll, N. de Freitas, A machine learning perspective on predictive coding with PAQ8, in: 2012 Data Compression Conference, IEEE, 2012, pp. 377–386. [74] Best Artworks of All Time, 2020. accessed May 18, [75] Diabetic Retinopathy Detection, 2020. accessed May 18, [76] X. Wang Y. Peng L. Lu Z. Lu M. Bagheri R.M. Summers ChestX-ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases IEEE CVPR 2017 3462 3471 X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases, in: IEEE CVPR, 2017, pp. 3462–3471. [77] COCO - Common Objects in Context, 2020. accessed May 18, [78] C. Shapiro Abstract expressionism: the politics of apolitical painting Prospects 3 1978 175 214 C. Shapiro, et al., Abstract Expressionism: The politics of apolitical painting, Prospects 3 (1978) 175–214. [79] H. Rosenberg The Tradition of The New 1994 Hachette Books H. Rosenberg, The Tradition Of The New, Hachette Books, 1994. [80] L. Garrard Colourfield Painting: Minimal, Cool, Hard Edge, Serial and Post-Painterly Abstract Art from the Sixties to the Present 2007 Crescent Moon Publishing L. Garrard, Colourfield painting: Minimal, Cool, Hard Edge, Serial and Post-painterly Abstract Art from the Sixties to the present, Crescent Moon Publishing, 2007. [81] D. Hockney Secret Knowledge: Rediscovering the Lost Techniques of the Old Masters 2006 Viking Studio D. Hockney, Secret Knowledge: Rediscovering the Lost Techniques of the Old Masters, Viking Studio, 2006. [82] S. Yang G. Cheung P. Le Callet J. Liu Z. Guo Computational modeling of artistic intention: quantify lighting surprise for painting analysis 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX) 2016 IEEE 1 6 S. Yang, G. Cheung, P. Le Callet, J. Liu, Z. Guo, Computational modeling of artistic intention: Quantify lighting surprise for painting analysis, in: 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), IEEE, 2016, pp. 1–6. [83] L. Fichner-Rathus Foundations of Art and Design: An Enhanced Media Edition 2011 Cengage Learning L. Fichner-Rathus, Foundations of art and design: An enhanced media edition, Cengage Learning, 2011. [84] T. Chen C. Guestrin XGBoost: a scalable tree boosting system Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD ’16 2016 ACM New York, NY, USA 785 794 10.1145/2939672.2939785 T. Chen, C. Guestrin, XGBoost: A scalable tree boosting system, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, ACM, New York, NY, USA, 2016, pp. 785–794. 10.1145/2939672.2939785 [85] L. Nanni S. Ghidoni S. Brahnam Handcrafted vs. non-handcrafted features for computer vision classification Pattern Recognit. 71 2017 158 172 10.1016/j.patcog.2017.05.025 L. Nanni, S. Ghidoni, S. Brahnam, Handcrafted vs. non-handcrafted features for computer vision classification, Pattern Recognition 71 (2017) 158–172. 10.1016/j.patcog.2017.05.025 Jorge Miguel Silva is a Ph.D. Student and Research Fellow at the Institute of Electronics and Informatics Engineering of Aveiro (IEETA), a Computer Science and Engineering research unit. He holds a Master’s degree in Bioengineering from the Faculty of Engineering at the University of Porto. His research interests are Information Theory, Artificial Intelligence and Compression. Diogo Pratas is a computer scientist at IEETA/DETI at the University of Aveiro. Diogo’s research interests include computational biology, data compression, and information theory. Diogo earned his Ph.D. in computer science from the University of Aveiro and holds a B.S. degree in information and communications technologies also from the University of Aveiro, with a segment carried at the Pontifical University of Salamanca. He joined IEETA/DETI in 2009, after being working some years in the private sector. Diogo actively collaborates with the Department of Virology at the University of Helsinki, namely in the identification and reconstruction of viral genomes, and the respective human-hosts, at multi-organ level. Diogo is a member of the Super Dimension Fortress, the International Society for Computational Biology, and the Portuguese Association for Pattern Recognition. Rui Antunes is a Ph.D. Student and Research Fellow at IEETA, a Computer Science and Engineering research unit. He holds a Master’s degree in Electronic and Telecommunications Engineering from University of Aveiro. His research interests include artificial intelligence, machine learning, natural language processing, and information extraction. Sergio Matos graduated in Systems Engineering and Computing by the University of Algarve (Portugal) in 1999 and received his Ph.D. in computer engineering in 2007 by the University of Leicester (UK), where he developed a system, based on speech processing and recognition technologies, for monitoring patients with acute or chronic cough. Sergio Matos is an integrated member of the Biomedical Informatics and Technologies group at IEETA, University of Aveiro, since 2008 and Assistant Professor at the Department of Electronics, Telecommunications and Informatics since 2018. His research interests are information retrieval, text mining, NLP and machine-learning. Armando J. Pinho received the Electronics and Telecommunications Engineering degree from the University of Aveiro, Portugal, in 1988, the Master’s degree in electrical and computers engineering from IST, Technical University of Lisbon, Portugal, in 1991 and the Ph.D. in electrical engineering from the University of Aveiro, in 1996. Currently, Armando J. Pinho is an Associate Professor with habilitation at the Department of Electronics, Telecommunications and Informatics of the University of Aveiro, and a researcher at the Signal Processing Laboratory of the Institute of Electronics and Telematics Engineering of Aveiro - IEETA. His main research activity is in the area of image coding, data compression, and computational biology. He also has interests in other areas of research, such as bioinformatics and image and video analysis. "
    },
    {
        "doc_title": "The Human Bone Marrow Is Host to the DNAs of Several Viruses",
        "doc_scopus_id": "85105418944",
        "doc_doi": "10.3389/fcimb.2021.657245",
        "doc_eid": "2-s2.0-85105418944",
        "doc_date": "2021-04-22",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Microbiology",
                "area_abbreviation": "IMMU",
                "area_code": "2404"
            },
            {
                "area_name": "Immunology",
                "area_abbreviation": "IMMU",
                "area_code": "2403"
            },
            {
                "area_name": "Microbiology (medical)",
                "area_abbreviation": "MEDI",
                "area_code": "2726"
            },
            {
                "area_name": "Infectious Diseases",
                "area_abbreviation": "MEDI",
                "area_code": "2725"
            }
        ],
        "doc_keywords": [
            "Bone Marrow",
            "Bone Marrow Transplantation",
            "DNA, Viral",
            "Hepatitis B virus",
            "High-Throughput Nucleotide Sequencing",
            "Humans"
        ],
        "doc_abstract": "© Copyright © 2021 Toppinen, Sajantila, Pratas, Hedman and Perdomo.The long-term impact of viruses residing in the human bone marrow (BM) remains unexplored. However, chronic inflammatory processes driven by single or multiple viruses could significantly alter hematopoiesis and immune function. We performed a systematic analysis of the DNAs of 38 viruses in the BM. We detected, by quantitative PCRs and next-generation sequencing, viral DNA in 88.9% of the samples, up to five viruses in one individual. Included were, among others, several herpesviruses, hepatitis B virus, Merkel cell polyomavirus and, unprecedentedly, human papillomavirus 31. Given the reactivation and/or oncogenic potential of these viruses, their repercussion on hematopoietic and malignant disorders calls for careful examination. Furthermore, the implications of persistent infections on the engraftment, regenerative capacity, and outcomes of bone marrow transplantation deserve in-depth evaluation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A semi-automatic methodology for analysing distributed and private biobanks",
        "doc_scopus_id": "85098456035",
        "doc_doi": "10.1016/j.compbiomed.2020.104180",
        "doc_eid": "2-s2.0-85098456035",
        "doc_date": "2021-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Access restriction",
            "Distributed repositories",
            "Genetic variants",
            "Genomic studies",
            "Practical schemes",
            "Privacy issue",
            "Rare disease",
            "Semi-automatics",
            "Biological Specimen Banks",
            "COVID-19",
            "Humans",
            "Public Health",
            "SARS-CoV-2"
        ],
        "doc_abstract": "© 2020 The AuthorsPrivacy issues limit the analysis and cross-exploration of most distributed and private biobanks, often raised by the multiple dimensionality and sensitivity of the data associated with access restrictions and policies. These characteristics prevent collaboration between entities, constituting a barrier to emergent personalized and public health challenges, namely the discovery of new druggable targets, identification of disease-causing genetic variants, or the study of rare diseases. In this paper, we propose a semi-automatic methodology for the analysis of distributed and private biobanks. The strategies involved in the proposed methodology efficiently enable the creation and execution of unified genomic studies using distributed repositories, without compromising the information present in the datasets. We apply the methodology to a case study in the current Covid-19, ensuring the combination of the diagnostics from multiple entities while maintaining privacy through a completely identical procedure. Moreover, we show that the methodology follows a simple, intuitive, and practical scheme.",
        "available": true,
        "clean_text": "serial JL 271150 291210 291870 291901 31 90 Computers in Biology and Medicine COMPUTERSINBIOLOGYMEDICINE 2020-12-18 2020-12-18 2020-12-22 2020-12-22 2021-02-18T04:42:31 S0010-4825(20)30511-4 S0010482520305114 10.1016/j.compbiomed.2020.104180 S300 S300.1 FULL-TEXT 2021-04-08T04:29:49.090165Z 0 0 20210301 20210331 2021 2020-12-18T01:57:37.637417Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst ref 0010-4825 00104825 UNLIMITED FCT true 130 130 C Volume 130 5 104180 104180 104180 202103 March 2021 2021-03-01 2021-03-31 2021 Regular Articles article fla © 2020 The Authors. Published by Elsevier Ltd. ASEMIAUTOMATICMETHODOLOGYFORANALYSINGDISTRIBUTEDPRIVATEBIOBANKS ALMEIDA J 1 Introduction 2 Genomics interoperability 3 Methodology 3.1 The Genomics Catalogue 3.2 The genomics and proteomics toolkit 3.3 The workflow manager 3.4 Overview 4 Results 5 Research application 5.1 Study example: SARS-CoV-2 5.2 Validation 6 Discussion 7 Conclusions References JOHNSTON 2018 S2 S6 J KAYE 2012 415 431 J JALILI 2017 90 109 V COPPOLA 2019 172 L LITTON 2018 233 241 J LIU 2015 55 68 A BIOBANKINGIN21STCENTURY BIOBANKINGFORPERSONALIZEDMEDICINE AMORIM 2020 102333 A LANGHOF 2017 293 300 H KULYNYCH 2017 94 132 J CLAES 2014 P MCLAREN 2016 814 822 P GRISHIN 2019 1115 1117 D MASSEROLI 2016 3 11 M ALMEIDA 2019 466 473 J PROCEEDINGS12THINTERNATIONALJOINTCONFERENCEBIOMEDICALENGINEERINGSYSTEMSTECHNOLOGIESVOLUME5HEALTHINFHEALTHINF STRATEGIESACCESSPATIENTCLINICALDATADISTRIBUTEDDATABASES GAZIANO 2016 214 223 J SUDLOW 2015 C HOSSEINI 2019 146 148 M TOPPINEN 2020 102353 M CHO 2018 547 551 H IOANNIDIS 2006 609 614 J SHRINGARPURE 2015 631 646 S 2012 57 74 2015 68 74 SILVA 2018 33 42 L OLIVEIRA 2019 J ALMEIDA 2020 100535 J PRATAS 2020 D LIEW 2017 66 C HOLL 2014 352 362 S WOLSTENCROFT 2013 W557 W561 K GOECKS 2010 R86 J ALMEIDA 2019 121 J WU 2020 265 269 F HEENEY 2011 17 25 C PRATAS 2018 1177 1181 D 201826THEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO METAGENOMICCOMPOSITIONANALYSISSEDIMENTARYANCIENTDNAISLEWIGHT PRATAS 2018 445 D ZIELEZINSKI 2019 144 A HUANG 2011 593 594 W LIN 2004 Z PRATAS 2020 D VAYENA 2016 e1001937 E HRIPCSAK 2015 574 G BOS 2018 64 I TKACHENKO 2018 221 235 O PROCEEDINGS2018ASIACONFERENCECOMPUTERCOMMUNICATIONSSECURITY LARGESCALEPRIVACYPRESERVINGSTATISTICALCOMPUTATIONSFORDISTRIBUTEDGENOMEWIDEASSOCIATIONSTUDIES GOLDREICH 1998 O KANNAN 2016 603 615 L HUFSKY 2020 F HOFFMANN 2020 M SCHAFFER 2020 1 23 A ALMEIDAX2021X104180 ALMEIDAX2021X104180XJ Full 2020-12-17T17:09:46Z FundingBody Portugal Institutes 2021-12-22T00:00:00.000Z 2021-12-22T00:00:00.000Z This is an open access article under the CC BY-NC-ND license. © 2020 The Authors. Published by Elsevier Ltd. 2020-12-17T17:04:36.024Z IMI Innovative Medicines Initiative FCT Fundação para a Ciência e a Tecnologia item S0010-4825(20)30511-4 S0010482520305114 10.1016/j.compbiomed.2020.104180 271150 2021-04-08T04:29:49.090165Z 2021-03-01 2021-03-31 UNLIMITED FCT true 1912409 MAIN 8 55182 849 656 IMAGE-WEB-PDF 1 gr1 102992 240 388 gr2 91558 263 535 gr3 115255 352 535 gr4 115141 254 535 gr1 76156 135 219 gr2 70900 107 219 gr3 76244 144 219 gr4 75175 104 219 gr1 251665 1064 1721 gr2 265461 1163 2370 gr3 434092 1559 2369 gr4 373059 1125 2370 am 1762750 CBM 104180 104180 S0010-4825(20)30511-4 10.1016/j.compbiomed.2020.104180 The Authors Fig. 1 The main actors involved in each study: Researcher (R), Study Manager (SM), and the Data Owners (DO). Fig. 1 Fig. 2 The Genomics Catalogue showing the demo repositories and the first stage of the study request. Fig. 2 Fig. 3 The TASKA workflow manager with an example study to analyse the complexity profile region in genomics data. Fig. 3 Fig. 4 Overview of the proposed methodology involving the three main actors and their responsibilities. Fig. 4 Table 1 The five most representative reference sequences according to the RS (Relative Similarity). ID stands for the order of the top output, length for the size of the reference genome, and GID for the sequence global identifier. Table 1 ID Length RS (%) Reference GID Virus name 1 124884 96.2539 NC_001348.1 HHV3 2 29903 95.4904 NC_045512.2 SARS-CoV-2 3 16568 78.2749 NC_012920.1 MT 4 5028 1.3435 NC_004295.1 HEVv9 5 162114 0.0000 NC_000898.1 HHV 6 Table 2 The twenty most representative SARS-CoV-2 genome sequences according to the RS (Relative Similarity). ID stands for the order of the top output, length for the size of the reference genome, and GID for the sequence global identifier. Table 2 ID Length RS (%) GID 1 29893 97.6044 MT007544.1 2 29854 97.4854 MN996530.1 3 29862 97.4833 MT192759.1 4 29833 97.4813 MT039873.1 5 29881 97.4744 MN988669.1 6 29881 97.4744 MN988668.1 7 29903 97.4724 MN908947.3 8 29903 97.4724 NC_045512.2 9 29891 97.4720 MN996528.1 10 29890 97.4720 MT019532.1 11 29860 97.4695 MT093631.2 12 29825 97.4668 MN996527.1 13 29883 97.4646 MN994468.1 14 29811 97.4619 MT072688.1 15 29882 97.4599 MT184907.2 16 29882 97.4599 MT184909.2 17 29882 97.4599 MT184912.1 18 29882 97.4599 MT159711.2 19 29882 97.4599 MT159714.2 20 29882 97.4599 MT159719.2 ☆ This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. J.R. Almeida is funded by the National Science Foundation (FCT), under the grant SFRH/BD/147837/2019. D. Pratas is funded by national funds through FCT, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019 A semi-automatic methodology for analysing distributed and private biobanks João Rafael Almeida a b ∗ Diogo Pratas a c José Luís Oliveira a ∗∗ a DETI/IEETA, University of Aveiro, Aveiro, Portugal DETI/IEETA University of Aveiro Aveiro Portugal DETI/IEETA, University of Aveiro, Aveiro, Portugal b Department of Computation, University of A Coruña, A Coruña, Spain Department of Computation University of A Coruña A Coruña Spain Department of Computation, University of A Coruna, A Coruna, Spain c Department of Virology, University of Helsinki, Helsinki, Finland Department of Virology University of Helsinki Helsinki Finland Department of Virology, University of Helsinki, Helsinki, Finland ∗ Corresponding author. DETI/IEETA, University of Aveiro, Aveiro, Portugal. DETI/IEETA University of Aveiro Aveiro Portugal ∗∗ Corresponding author. Privacy issues limit the analysis and cross-exploration of most distributed and private biobanks, often raised by the multiple dimensionality and sensitivity of the data associated with access restrictions and policies. These characteristics prevent collaboration between entities, constituting a barrier to emergent personalized and public health challenges, namely the discovery of new druggable targets, identification of disease-causing genetic variants, or the study of rare diseases. In this paper, we propose a semi-automatic methodology for the analysis of distributed and private biobanks. The strategies involved in the proposed methodology efficiently enable the creation and execution of unified genomic studies using distributed repositories, without compromising the information present in the datasets. We apply the methodology to a case study in the current Covid-19, ensuring the combination of the diagnostics from multiple entities while maintaining privacy through a completely identical procedure. Moreover, we show that the methodology follows a simple, intuitive, and practical scheme. Keywords Genomics studies Distributed biobanks Genomics cross-exploration Coronavirus 2 SARS-CoV-2 Secondary use 1 Introduction The continuous evolution of sequencing technology prompted a new step in clinical research, in which it is possible to scan the whole-genome of individual DNA samples at an acceptable cost and time [1,2]. This increasing generation of genomics data also led to big data problems which require the reorganization of current biobank policies [3]. In fact, this can be considered an opportunity to build infrastructures that allow the linkage of existing biobanks and bio clinical projects [4]. Biobanking currently represents a new research field that involves international infrastructures and government agencies requiring the creation of policies to provide ethical and legal guidelines for public health [5]. The need for high-quality and clinically annotated biospecimens for personalized medicine and forensic applications is raising new research challenges [6,7]. However, other major problems have followed this growth, namely the evolution of biobanking in a decentralized way, with heterogeneous procedures for data collection and storage, as well as different legal policies for data access [8]. One of the key challenges is to find the right balance between preserving the privacy of the subjects in the study and the data availability for sharing the results through global research networks [2]. Although genomics datasets are not linked to medical records, which preserves subject identity [9], some authors tried to reverse the process only using the DNA present in the datasets. Claes et al. [10] show it is possible to model a 3D face using genomics data obtaining a similar facial appearance. Privacy issues are one of the main obstructions in health research, including in the area of genomics [11,12]. Answers to biomedical questions may currently be hidden in private data repositories that are not explored due to the lack of methodologies to analyse this data [13]. The problem can be addressed at different levels, from biomedical data discovery to multi-repository analysis, i. e., there are gaps in the way biobanks are exposed to the research community and the methodologies currently available are not designed to simplify the exploration of multiple and private repositories. The idea of distributing research questions over distinct organisations and databases has been applied to allow the secondary use of multiple Electronic Health Records (EHR) [14]. However, no such experience has been proposed in the genomics field, which could help in discovering new disease biomarkers, especially in rare diseases. In short, we can identify three main issues that are currently present in genomics studies: • privacy issues: the genomics dataset can contain private information that allows subject identification, which causes access problems; • data interoperability: the metadata of the private repositories are publicly available following heterogeneous description; • data access: since sharing data will violate privacy issues, the exploration of multiple repositories is currently a poorly explored subject. Therefore, for the success of genomics research, it is essential to create solutions that decrease the impact of these issues. In this paper, we present a methodology that allows the exploration of distributed genomics repositories, without giving researchers direct contact with the data. This ensures data privacy and speeds up the process of exploring multiple datasets, which is imperative to increase the impact of new findings. 2 Genomics interoperability Exploring distributed and private biobanks without compromising ethical regulations is a complex subject already studied by the research community. Almost all sequenced genomes are currently stored in protected repositories with strict access rules or securely encrypted [15–18]. However, accessing those datasets allows analysis of a larger number of subjects in order to identify genetic variants that are statistically correlated [19]. This is essential to detect genetic signals expressed with small effect size or in rare variants [20]. The problem with ensuring subjects’ privacy when analyzing their genomics information is crucial. Cho et al. [19] described an approach for genome-wide association studies, using cryptographic techniques for secure data analysis while maintaining the confidentiality of genotypes and phenotypes. Although they ensure a private and secure environment, the data needs to be shared with the participants involved in each study. This solution is very useful, but this scenario requires that all the institutions follow this architecture. Another technique for genomics data sharing is the use of web services that provide only allele-presence information, designated as “beacon”. This was created in the Beacon Project by the Global Alliance for Genomics & Health (GA4GH) 1 1 where users can perform queries about genomics information in the institutional beacons [21]. However, some authors have already studied the risk of using this technique, where users can perform as many queries as they want, based on the limitations of the services. Shringarpure et al. [21] show through simulations that in a beacon with 1 000 subjects, re-identification was possible with 5 000 queries. With the extraordinary advances in genomics, several consortia have collected and made publicly available some of those data after processing, mainly due to the complexity in sharing genomics data and aiming to explore human genes. The Encyclopedia of DNA Elements (ENCODE) project mapped genomics regions of transcription, transcription factor association, chromatin structure and histone modification [22]. This project aims to identify functional elements in the human genomes. The data collected during the project enabled the assignment of biochemical functions for 80% of the genome [22]. In the ENCODE Portal 2 2 the data is accessible to the research community. Another project with the goal of studying the human genome is the 1000 Genomes Project. During the project, the genomes of 2 504 individuals from 26 populations [23] were reconstructed. This allowed the establishment of a detailed catalogue of human genetic variations. Other projects aim at genomics data collection and analysis, but focus on specific goals. In these approaches, the reuse of the collected data is complex and privacy assurance can be questionable. Therefore, the goal of this methodology is to create strategies to study the genomics raw data that have been collected over the last decades without compromising any privacy issue or institutional rule. 3 Methodology The proposed methodology simplifies the process of studying genomics data in distributed and private repositories. This approach streamlines the entire process without removing full control of the data from the Data Owners. The data is kept in the Data Owners’ repositories and the analyses are performed locally. At the end, each Data Owner can share the results of the study without ever exposing the data repositories. In each study there are three different roles for the various participants (Fig. 1 ): • the Data Owner (DO): the entity responsible for managing the genomics repositories of each organisation; • the Researcher (R): the entity interested in analysing the data repositories and in performing the study; • the Study Manager (SM): the entity who coordinates the study and the distinct actors in the study. 3.1 The Genomics Catalogue The main purpose of this platform is to simplify genomics data exposure and sharing, without giving access to the data. Besides, the platform contains study management features, where users can select the repositories of interest and make research questions. Fig. 2 shows the platform with several repositories and the creation of a study request, i.e. a brief description of the research question to be tackled. This platform was built on top of Montra Framework [24] which was designed as an general engine to build data catalogues of biomedical databases. This system has a flexible data skeleton used to characterise data entities, and is easy to customise for different purposes. The tool also ensures access control and data privacy, following rule-based access policies that are essential to control what data is visible for the different access levels. Moreover, the data available in the platform only characterise the repositories’ content, which does not expose any sensitive information. The Montra Framework was previously used in other initiatives of biomedical data sharing, such as EMIF-Catalogue. 3 3 This web platform, developed in the European Medical Information Framework project (EMIF), 4 4 allows data owners to share information about their databases at different levels of access [25]. Similar to the Genomics Catalogue, the goal was to share metadata information about private data repositories in order to help researchers to find datasets of interest. 3.2 The genomics and proteomics toolkit The genomics and proteomics data in each repository usually follow standard formats. Therefore, we used GTO, 5 5 a standalone toolkit to unify pipelines operating both at genomic and proteomic levels. This toolkit is composed of applications that allow the creation of workflows for identification of metagenomic composition in FASTQ reads, detection and visualisation of genomic rearrangements, mapping and visualisation of variation, localisation of low complexity regions, simulation of sequences with specific SNP and structural variant rates, among many other features [26]. GTO is open-source and written in a low-level language (e.g. C language) without external dependencies, built for ultra-fast computations and flexible integrations. The advantage of using open-source tools is the possibility to compile the code locally and have deep knowledge of how the tools are operating over the data. This allows technical teams from the DOs’ institutions to make sure there are no information leaks while maintaining faster computations. The toolkit supports Unix-like pipelines for easy integration of the available tools, which allows the creation of multiple processing workflows to answer research questions. This feature allows using a chain of processes, which can be easily shared between the DOs and executed locally, as shown in the following example: tool_1 < input | tool_2 | tool_3 > output To take full advantage of the GTO tools, end-users need to have basic shell script knowledge in order to build the analytic scripts. Therefore, script construction and validation is the responsibility of the SM, not invalidating the possibility of this being provided by the researcher. Several validated examples of pipelines built using this toolkit are currently available at and have been used in many projects, for example, in a component of the development and integration of TRACESPipe [27]. 3.3 The workflow manager Studies have been simplified through the use of scientific workflow systems, which allow the combination and execution of computational processes, in cascade and over distributed environments [28,29]. One popular system is Taverna, 6 6 an open-source scientific workflow management system used to facilitate computer simulation of repeatable scientific experiments [30]. Galaxy 7 7 is another commonly used scientific workflow management system, with a user-friendly web interface that allows collaborative discussion of results and studies’ replication [31]. However, in our proposal, the study workflow can be coordinated independently of the solution chosen. In previous work, we developed TASKA, 8 8 which is a task/workflow management system designed as a modular web platform to facilitate studies' execution, team coordination, task scheduling and researcher collaboration [32]. In our pipeline, the study's coordination and monitoring were performed through this system, which contains different types of tasks essential for managing genomic studies: • a Simple task, where instructions are provided about what must be done. For instance, running a script attached to the task when there is no direct connection to execute it automatically in the repository. • a Form task, which allows the construction of a simple online questionnaire (text, multiple choices, etc.) which is then completed by assigned users. This task is used in our proposal for the DOs to indicate their availability to provide answers in each study or answer the research question based on the script execution. • a Processing task, designed to execute operations automatically without user interaction. For instance, running a pipeline built using the GTO toolkit over the data, when a direct connection is available (i.e., when the repository has a public API). The SM conducting the study is responsible for task assignment, scheduling management, and results compilation at the end. Fig. 3 shows a simple workflow to run a study aiming to analyse the complexity profile region in genomics data. The script was attached in the first task, and it will be executed by each DO locally in the second task. Then, the results are uploaded in the third task, when the SM will analyse, compile and answer the question in the final task. 3.4 Overview The proposed methodologies centralise the main information in the Repository Catalogue, which is the portal to present and manage the data available. In this platform, researchers can search for data sources of interest, create study requests and monitor the study status. However, other users with more advanced knowledge about the agents involved in the platform are responsible for managing the study (SM), by contacting and forwarding the request to the DO, and answering the researchers. This procedure is conducted with the support of TASKA, which helps during the monitoring of all tasks involved in the process. The researcher starts by formulating the study request, which can be a simple question or the analysis script if they have more advanced technical knowledge. The request is made in the Genomics Catalogue, where the repositories of interest for the study are also selected. Then, the SM analyses the request, evaluates the suitability of the question and the DO's willingness to participate in this study. During this stage, the SM analyses or builds the script using the GTO tools, and shares it with the DO using TASKA. After receiving all the DO's answers, the SM compiles the information and replies to the researcher. The researcher does not need to know who the Dos are or how to process the genomics data technically using the GTO tools. This is one of the SM's responsibilities. In addition, the SM knows the characteristics of the different repositories and who they need to contact to obtain the information. On the other hand, the DO's responsibilities are running the script provided by the SM locally and determining whether the results can be shared. This methodology gives DOs full control of the data, only sharing the results that do not violate the organisational policy. During this process, administrative issues and governance board approvals are also included in the protocol. These tasks are contemplated and managed in TASKA, where all the participants are able to monitor the study status. 4 Results The proposed methodology can be summarised in Fig. 4 , where the three different actors and their responsibilities in the study are well defined. The goal is to show how each entity can proceed in order to collaborate in the study and provide an answer to a research question. Additionally, in this collaboration, the source data is never exposed and the DOs have full control of what they want to share. The following set of steps reproduces the study feasibility using the proposed methodology. The governance and contractual aspects were ignored in this description, although this can be integrated into this workflow at any time. • Step 1: Creating study request - The methodology starts in the Genomics Catalogue, where the researcher creates a study request. - In this procedure, the researcher fills in a form describing the study objective, the datasets of interest and the research question. - These datasets can be discovered using the platform's search and compare features. - An example of a research question can be “Does a patient sequenced (FASTQ) sample contain SARS-CoV-2?” • Step 2: Feasibility and script definition - The SM is notified about the study request and analyses the feasibility based on the research question. - Assuming that the study request is feasible, the SM builds the analytic script using the tools in the GTO toolkit. - The script can also be created by the researcher, although we are presenting a scenario where this entity's technical knowledge is low. On the other hand, the SM needs to know how to work with these tools as well as how to contact the DO of each repository. - If a research study is not feasible, the workflow ends at this step. • Step 3: Defining and starting the study workflow - In TASKA, the SM creates a workflow of different tasks, in which some of these are assigned to the DO associated with the datasets of interest. - This tool manages the study flow and communications between the SM and the DO, in order to simplify the results. - The use of a workflow system in this step facilitates the tracking of studies in progress and keeps all the information in a centralised platform (i.e., message exchange between the SM and DOs, the status of each task, among others). • Step 4: Running script locally - The script is shared with the DO to be manually executed in the local computational resources. - This allows full control of what is happening with the data and assessing whether the result of the script execution can be shared to the study. - Whenever the repository is publicly available, the SM can also configure a different task in TASKA to execute this automatically. However, this is not the main focus of our methodology. • Step 5: Exporting and sending results - Locally in each DO institution, the script produces the results in different formats (i. e., SVG images showing statistical data, tabular information, small sets of reads, among others). - Before sending the results to the SM, the DOs can decide about how to proceed based on the script output. For example, the DOs can filter part of the results before sharing them with the SM. - The results are attached in TASKA, which notifies the SM about the study progress. • Step 6: Aggregating results - After all DOs complete their tasks, the SM can access the results uploaded in TASKA. - The upload results differ based on the research question. Therefore, when the SM defines the study workflow, this task must be designed according to the DOs' outputs. - The SM then compiles the collected data. In particular cases, for example, in the presence of multiple datasets, the GTO tools can again be used to perform this aggregation task. • Step 7: Results evaluation and reporting - Finally, the researcher receives a notification regarding the study results in the Genomics Catalogue. - In this catalogue, the researcher can access the results, and if required, additional information can be requested. The proposed methodology has supporting tools to manage the execution of genomics research studies over multiple and private biobanks. The possibility of isolating the data analysis locally is essential to preserve privacy rules, which allows the analysis of unexplored datasets, increasing the studies’ dimension. 5 Research application The proposed methodology shows the feasibility of performing distributed research studies over private genomics repositories. To develop this methodology, we use open-source tools in all the study steps, where the goal is to show successful outcomes applied to contemporary problems. Nevertheless, the same structure can be established using other tools. For instance, in this case, we use GTO to perform the study analysis, but this toolkit can use other tools that are not currently available to satisfy the study requirements. The same is true with the study manager. Although we used TASKA, another tool can be used for the study coordination between the SM and DOs. Therefore, using the proposed tools, we create a scenario to validate this methodology. 5.1 Study example: SARS-CoV-2 The proposed scenario to evaluate the feasibility of this methodology is based on the impact of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) in 2020. SARS-CoV-2 is a positive-sense single-strand RNA virus with a tracked origin to a food market in Wuhan, China, in December 2019 which can cause COVID-19 [33]. The standard diagnostic method is real-time RT-qPCR (Reverse Transcriptase quantitative Polymerase Chain Reaction) applied to sub-regions of SARS-CoV-2. However, when dealing with a human sample, which potentially may contain SARS-CoV-2, privacy issues need careful consideration. Some issues are related to the presence of human genetic material that may reveal the individual's characteristics. Accordingly, to address two questions such as “Does this human sample contain SARS-CoV-2? If yes, what is the closest sequenced genome?” the diagnostic must be followed in accordance with preservation of the host's medical and forensic peculiarities. The problem escalates when different institutions need to report the results without sharing the samples and compromising individual privacy while maintaining identical characteristics of the analysis [34]. The following research application shows how to solve this problem. According to the methodology presented in this paper, a pipeline for SARS-CoV-2 is built using GTO and shared between institutions. Then, each institution runs the shared pipeline on their private server with the corresponding private data. Then the results are communicated to the original source and aggregated. In this subsection, we show in detail how to address this question, namely a description of all the materials used and the associated characteristics. The pipelines used in this task are freely available at and described as follows: • gto_build_xs_viral_db.sh: Designed to build a viral and human mitogenome dataset (references only). Since this script downloads the FASTA genomes forming the database, the script can be shared with the DO or in more restricted cases, the produced database can be compressed and then sent to the DOs. • gto_create_sars_dataset.sh: Creates a viral dataset considering the following set of ids: “AY386330.1″, “JN561323.2″, “X04370.1″, “MT007544″, “AB041963.1″, “MG921180.1″, “NC_012920.1”. This dataset was used by the SM to validate the processing pipeline, locally. • gto_sars_simulation_detection.sh: Builds a small FASTA database with viral references and human mitogenome. Then, it creates a FASTQ dataset containing SARS-CoV-2 and the human mitogenome (among others) and maps the existence of human mitogenome and viruses. This script has as input the database created with the gto_build_xs_viral_db.sh. • gto_download_sars2.sh: Downloads 93 SARS-CoV-2 genome sequences from the NCBI repositories. In order to avoid multiple calls to NCBI repositories, the result of this script can be compressed and then sent to the DO. • gto_find_best_sars2.sh: It finds the most similar SARS-CoV-2 genome sequences according to the RS (Relative Similarity) in the dataset produced by gto_sars_simulation_detection.sh. The SM with the five described pipelines can evaluate the feasibility of the research question by using synthetic data, and then send the processing pipelines to the DOs. Finally, the DOs run the pipelines in their private repositories and communicate the results. 5.2 Validation The proposed scenario is fully automatic metagenomic identification of viral content using the FASTQ sample that contains a SARS-Cov-2 genome. The pipeline contains filtering, trimming and mapping operations over the reads, and then performs sensitive identification of the most representative genomes followed by a ranking of abundance. This pipeline was validated using a semi-synthetic viral dataset of SARS-CoV-2 genomes with applied degrees of substitutions and block permutations shuffled with noisy synthetic sequences. The intention is to perform a metagenomic analysis on this dataset without informing the program what organisms are contained in the sample since the program needs to infer the results. Then, we compare the results with the ground truth. If the results are similar to the ground truth, then the pipeline is validated and can be shared with the DOs. For this purpose, GTO uses falcon-meta technology [35,36] which relies on assembly-free and alignment-free [37] comparison of each reference according to the whole reads. The dataset was constructed using gto_create_sars_dataset.sh which simulates synthetic reads (uniform distribution) merged with the following genomes and associated modifications: • SARS-CoV-2: one virus without editions (GID: MT007544.1); • MT: one human mitogenome with 1% of random substitutions (GID: NC_012920.1); • B19V: one B19 parvovirus with random permutations of 500 bases (GID: AY386330.1); • HPV: one human papillomavirus with 5% of random substitutions and random permutations of 300 bases (GID: MG921180.1); • HHV3: one human Herpesvirus 3 with permuted blocks of 300 bases (GID: X04370.1); • TTV: one human Torque teno virus with 5% of editions (GID: AB041963.1); • 300,000 bases of pseudo-random DNA simulated with a uniform distribution. After merging all FASTA sequences, ART [38] was used to generate the paired-end FASTQ reads. Meanwhile, another workflow example was used to create the viral and human mitogenome database (gto_build_xs_viral_db.sh) limited to reference sequences. Then, the pipeline (gto_sars_simulation_detection.sh), which uses the previous two pipelines for simulation, performs the metagenomic detection. The results are presented in Table 1 . According to Table 1, the FASTQ sample, among multiple genomes and the human mitogenome, contains a SARS-CoV-2 genome with a similarity match of 95.4% to the reference genome. Notice that the presence of the mitogenome would be a privacy issue that would lead to identification of the individual's forensic and medical characteristics [39]. Nevertheless, since the data is processed in a private machine, this information is not exported to external entities, respecting privacy concerns. The following procedure addresses the question of identifying the closest similar SARS-CoV-2 genome according to the existing sequencing database. Accordingly, we use the gto_download_sars2.sh pipeline to automatically download 93 SARS-CoV-2 genomes. Then, using the same FASTQ dataset (created with gto_create_sars_dataset.sh) and the previous set of 93 SARS-CoV-2 genomes, we perform an analysis of the most similar SARS-CoV-2 genome according to the reads (using gto_find_best_sars2.sh). These results are reported in Table 2 . As can be seen, the highest match is MT007544.1. Despite the well-known similarity between all the SARS-CoV-2 genomes [40], we were able to identify the exact genome from the set, showing the efficiency of this approach. This research application contains a practical and efficient methodology to solve current Covid-19 diagnostic challenges, effectively solving privacy issues that prevent cooperation between many scientific and industrial entities. Although the pipelines presented refer to a specific disease, the purpose of this methodology is easy creation and execution of genomic pipelines in the Dos’ facilities. 6 Discussion The proposed system has the goal of creating a methodology that helps researchers to conduct distributed studies analysing human genomes. The current tools designed for genome analysis typically require direct access to patients’ data, i.e., and researchers need to access data stored in the biobanks. However, this access often raises privacy issues. Our methodology addresses this problem by offering a straightforward workflow that simplifies communication and the exchange of results between all the actors involved in a study. The success of this methodology is due to the combination of several computational tools and algorithms that together provide a collaborative workflow, which can be customised for each research question, while ensuring privacy issues related to the raw data [41]. Therefore, the distributed analysis of private genomics information applied to the proposed scenario can be discussed based on two perspectives: 1) the technical challenges in conducting distributed studies; and 2) methodologies to study the study example (SARS-CoV-2). The idea of performing distributed medical studies is already applied in specific areas or with specific data types. For instance, Observational Health Data Sciences and Informatics (OHDSI) 9 9 is an international organisation aiming to develop methodologies to support large-scale observational studies in health care data. The data used in these studies contains only the patients' structured information present in the Electronic Health Records (EHR) systems after being anonymised and harmonised into a relational common data model (CDM) [42]. OHDSI has an ecosystem of tools that allows preparation of the data in each data custodian as well as tools to define the study and spread it through the network of data owners [14]. The problem is they are only concerned with performing observational studies using the EHRs’ structured information. The European Medical Information Framework project (EMIF) 10 10 also had the goal of performing distributed medical studies. One of the tracks in this project aimed to accelerate the discovery and validation of new biomarkers to diagnose Alzheimer's Disease (AD) in the predementia stage, and to predict the rate of decline [43]. In this case, the distributed analysis follows a procedure similar to the OHDSI in the case of the structured information, but the analysis of biomarkers was performed in controlled environments. The proposed solution does not require the researcher to access the data. The previous methodologies are focused on providing strategies to study patients' information without exposing this information. However, both were designed to work with structured information. When the goal is to study human genomic data without violating patients’ privacy, there are different techniques. Tkachenko et al. [44] created a solution aiming for data privacy preservation for Genome-Wide Association Studies (GWAS) based on Secure Multi-Party Computation (SMPC). This cryptographic technique has the potential to enable secure data privacy, since it provides ways for entities to jointly compute a function using their inputs, without exposing these inputs [45]. This is a different way of addressing the distributed analysis of genomic information. However, the strategies that were based on SMPC require that all the entities adopt and implement a cryptographic system. Although considered secure, data owners usually feel more confident keeping the data undisclosed, which is the main goal of our methodology. The GTO toolkit used in the data analysis was chosen due to the richness of its algorithms, but the methodology is flexible enough to incorporate other tools. Another reason for this choice is that GTO also contains external tools. It is important to have a well-defined set of tools in the framework that can be used by all researchers. This practice will allow replication of the genomic pipelines built using those tools as well as confirmation of the results of each study. More importantly, data owners need to feel confident in installing and using those tools in their local environments. Thus, open-source or certified tools are the appropriate option for this step [46]. We focused on using only open-source tools and toolkits to have the full methodology available to the community. The research application was based on data from the COVID-19 pandemic. However, many other studies can be performed to address other research questions related to different diseases and/or genomes. This disease was chosen to prove that, with this methodology, it is possible to simplify the genomic analysis over multiple entities without sharing or exposing the original data. A great advance of our methodology is that it can be used during a pandemic, to obtain timely results, which are often delayed by ethical and legal issues. The only condition is the willing cooperation of researchers and data custodians in respecting data-access policies that may exist in different countries, institutions and groups. Other authors created computational approaches aiming to detect SARS-CoV-2 infections, potential drugs and therapeutic strategies regarding this disease as well as tracking and studying the evolution of the COVID-19 pandemic. Hufsky et al. [47] presented a review of tools with these goals, and each tool presented is currently free to use and available online. The European Virus Bioinformatics Center curates a list of bioinformatics tools specifically for SARS-CoV-2, 11 11 and some of them were also analysed by Hufsky et al. [47]. Moreover, we identified from that list some tools with a similar goal as the pipelines presented in the research application. PriSeT 12 12 is a tool for computing SARS-CoV-2 specific primers for RT-PCR tests, enabling the detection of SARS-CoV-2 in a sample [48]. VADR 13 13 is a tool designed for validation and annotation of SARS-CoV-2 [48]. The annotation system was built based on the analysis of input nucleotide sequences using models created from curated RefSeqs [49]. The advantage of using a toolkit such as GTO is the possibility of building pipelines capable of performing similar tasks to those the tools presented were designed for. Since the goal of this work is to provide a generic solution, it is important to choose flexible software, without invalidating the possibility of integrating others. The results of the study example allowed validation of the toolkit analysis using public datasets. Researchers can perform a similar task, creating the pipelines in the research analysis. These pipelines can be easily shared with the data owners and the outputs do not expose any patient information. In fact, if the pipelines are a risk to patients’ privacy, the data owners can simply refuse to share the results. Therefore, with this methodology, it is possible to conduct studies without disclosing the data present in each location. 7 Conclusions Many private biobanks lack direct and cross-exploration due to privacy issues. Although many analysis pipelines exist, the critical issue of the multiple dimensionalities and sensitivity of the data remains an obstacle. In this paper, we proposed a new methodology that allows the exploration of private genomic and proteomic repositories without exposing their content. The privacy issues associated with sensitive data are solved with this methodology since the DOs have full control of the data, refusing unauthenticated access to the repositories. This methodology allows the exploration of multiple repositories, aggregating the results, and increasing the strength of scientific findings by analyzing higher volumes of data. The proposed methodology is a step forward in the cross-analysis of biobanks because it allows sharing architectural pipelines while preserving data privacy, through a process that is intuitive, simple and practical. References [1] J. Johnston J.D. Lantos A. Goldenberg F. Chen E. Parens B.A. Koenig N. Ethics P.A. Board Sequencing newborns: a call for nuanced use of genomic technologies Hastings Cent. Rep. 48 2018 S2 S6 J. Johnston, J. D. Lantos, A. Goldenberg, F. Chen, E. Parens, B. A. Koenig, N. Ethics, P. A. Board, Sequencing newborns: a call for nuanced use of genomic technologies, Hastings Center Report 48 (2018) S2-S6. [2] J. Kaye The tension between data sharing and the protection of privacy in genomics research Annu. Rev. Genom. Hum. Genet. 13 2012 415 431 10.1146/annurev-genom-082410-101454 J. Kaye, The tension between data sharing and the protection of privacy in genomics research, Annual review of genomics and human genetics 13 (2012) 415-431. doi:10.1146/annurev-genom-082410-101454. [3] V. Jalili M. Matteucci M. Masseroli S. Ceri Indexing next-generation sequencing data Inf. Sci. 384 2017 90 109 10.1016/j.ins.2016.08.085 V. Jalili, M. Matteucci, M. Masseroli, S. Ceri, Indexing next-generation sequencing data, Information Sciences 384 (2017) 90-109. doi:10.1016/j.ins.2016.08.085. [4] L. Coppola A. Cianflone A.M. Grimaldi M. Incoronato P. Bevilacqua F. Messina S. Baselice A. Soricelli P. Mirabelli M. Salvatore Biobanking in health care: evolution and future directions J. Transl. Med. 17 1 2019 172 10.1186/s12967-019-1922-3 L. Coppola, A. Cianflone, A. M. Grimaldi, M. Incoronato, P. Bevilacqua, F. Messina, S. Baselice, A. Soricelli, P. Mirabelli, M. Salvatore, Biobanking in health care: evolution and future directions, Journal of translational medicine 17 (1) (2019) 172. doi:10.1186/s12967-019-1922-3. [5] J.-E. Litton Launch of an infrastructure for health research: BBMRI-ERIC Biopreserv. Biobanking 16 3 2018 233 241 10.1089/bio.2018.0027 J.-E. Litton, Launch of an infrastructure for health research: BBMRI-ERIC, Biopreservation and biobanking 16 (3) (2018) 233-241. doi:10.1089/bio.2018.0027. [6] A. Liu K. Pollard Biobanking for personalized medicine Biobanking in the 21st Century 2015 Springer 55 68 10.1007/978-3-319-20579-3_5 A. Liu, K. Pollard, Biobanking for personalized medicine, in: Biobanking in the 21st Century, Springer, 2015, pp. 55-68. doi:10.1007/978-3-319-20579-3_5. [7] A. Amorim F. Pereira C. Alves O. García Species assignment in forensics and the challenge of hybrids Forensic Sci. Int.: Genetics 48 2020 102333 10.1016/j.fsigen.2020.102333 A. Amorim, F. Pereira, C. Alves, O. Garcia, Species assignment in forensics and the challenge of hybrids, Forensic Science International: Genetics 48 (2020) 102333. doi:10.1016/j.fsigen.2020.102333. [8] H. Langhof H. Kahrass S. Sievers D. Strech Access policies in biobank research: what criteria do they include and how publicly available are they? A cross-sectional study Eur. J. Hum. Genet. 25 3 2017 293 300 10.1038/ejhg.2016.172 H. Langhof, H. Kahrass, S. Sievers, D. Strech, Access policies in biobank research: what criteria do they include and how publicly available are they? A cross-sectional study, European Journal of Human Genetics 25 (3) (2017) 293-300. doi:10.1038/ejhg.2016.172. [9] J. Kulynych H.T. Greely Clinical genomics, big data, and electronic medical records: reconciling patient rights with research when privacy and science collide J. Law Biosci. 4 1 2017 94 132 10.1093/jlb/lsw061 J. Kulynych, H. T. Greely, Clinical genomics, big data, and electronic medical records: reconciling patient rights with research when privacy and science collide, Journal of Law and the Biosciences 4 (1) (2017) 94-132. doi:10.1093/jlb/lsw061. [10] P. Claes D.K. Liberton K. Daniels K.M. Rosana E.E. Quillen L.N. Pearson B. McEvoy M. Bauchet A.A. Zaidi W. Yao Modeling 3D facial shape from DNA PLoS Genet. 10 3 2014 10.1371/journal.pgen.1004224 P. Claes, D. K. Liberton, K. Daniels, K. M. Rosana, E. E. Quillen, L. N. Pearson, B. McEvoy, M. Bauchet, A. A. Zaidi, W. Yao, et al., Modeling 3D facial shape from DNA, PLoS genetics 10 (3) (2014). doi:10.1371/journal.pgen.1004224. [11] P.J. McLaren J.L. Raisaro M. Aouri M. Rotger E. Ayday I. Bartha M.B. Delgado Y. Vallet H.F. Günthard M. Cavassini Privacy-preserving genomic testing in the clinic: a model using HIV treatment Genet. Med. 18 8 2016 814 822 10.1038/gim.2015.167 P. J. McLaren, J. L. Raisaro, M. Aouri, M. Rotger, E. Ayday, I. Bartha, M. B. Delgado, Y. Vallet, H. F. Gunthard, M. Cavassini, et al., Privacy-preserving genomic testing in the clinic: a model using HIV treatment, Genetics in medicine 18 (8) (2016) 814-822. doi:10.1038/gim.2015.167. [12] D. Grishin K. Obbad G.M. Church Data privacy in the age of personal genomics Nat. Biotechnol. 37 10 2019 1115 1117 10.1038/s41587-019-0271-3 D. Grishin, K. Obbad, G. M. Church, Data privacy in the age of personal genomics, Nature biotechnology 37 (10) (2019) 1115-1117. doi:10.1038/s41587-019-0271-3. [13] M. Masseroli A. Kaitoua P. Pinoli S. Ceri Modeling and interoperability of heterogeneous genomic big data for integrative processing and querying Methods 111 2016 3 11 10.1016/j.ymeth.2016.09.002 M. Masseroli, A. Kaitoua, P. Pinoli, S. Ceri, Modeling and interoperability of heterogeneous genomic big data for integrative processing and querying, Methods 111 (2016) 3-11. doi:10.1016/j.ymeth.2016.09.002. [14] J.R. Almeida O. Fajarda A. Pereira J.L. Oliveira Strategies to access patient clinical data from distributed databases Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 5 HEALTHINF: HEALTHINF 2019 INSTICC, SciTePress 466 473 10.5220/0007576104660473 J. R. Almeida, O. Fajarda, A. Pereira, J. L. Oliveira, Strategies to Access Patient Clinical Data from Distributed Databases, in: Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 5 HEALTHINF: HEALTHINF, INSTICC, SciTePress, 2019, pp. 466-473. doi:10.5220/0007576104660473. [15] J.M. Gaziano J. Concato M. Brophy L. Fiore S. Pyarajan J. Breeling S. Whitbourne J. Deen C. Shannon D. Humphries Million Veteran Program: a mega-biobank to study genetic influences on health and disease J. Clin. Epidemiol. 70 2016 214 223 10.1016/j.jclinepi.2015.09.016 J. M. Gaziano, J. Concato, M. Brophy, L. Fiore, S. Pyarajan, J. Breeling, S. Whitbourne, J. Deen, C. Shannon, D. Humphries, et al., Million Veteran Program: a mega-biobank to study genetic influences on health and disease, Journal of clinical epidemiology 70 (2016) 214-223. doi:10.1016/j.jclinepi.2015.09.016. [16] C. Sudlow J. Gallacher N. Allen V. Beral P. Burton J. Danesh P. Downey P. Elliott J. Green M. Landray UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age PLoS Med. 12 3 2015 10.1371/journal.pmed.1001779 C. Sudlow, J. Gallacher, N. Allen, V. Beral, P. Burton, J. Danesh, P. Downey, P. Elliott, J. Green, M. Landray, et al., UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age, PLoS medicine 12 (3) (2015). doi:10.1371/journal.pmed.1001779. [17] M. Hosseini D. Pratas A.J. Pinho Cryfa: a secure encryption tool for genomic data Bioinformatics 35 1 2019 146 148 10.1093/bioinformatics/bty645 M. Hosseini, D. Pratas, A. J. Pinho, Cryfa: a secure encryption tool for genomic data, Bioinformatics 35 (1) (2019) 146-148. doi:10.1093/bioinformatics/bty645. [18] M. Toppinen D. Pratas E. Väisänen M. Söderlund-Venermo K. Hedman M.F. Perdomo A. Sajantila The landscape of persistent human DNA viruses in femoral bone Forensic Sci. Int.: Genetics 48 2020 102353 10.1016/j.fsigen.2020.102353 M. Toppinen, D. Pratas, E. Vaisanen, M. Soderlund-Venermo, K. Hedman, M. F. Perdomo, A. Sajantila, The landscape of persistent human DNA viruses in femoral bone, Forensic Science International: Genetics 48 (2020) 102353. doi:10.1016/j.fsigen.2020.102353. [19] H. Cho D.J. Wu B. Berger Secure genome-wide association analysis using multiparty computation Nat. Biotechnol. 36 6 2018 547 551 10.1038/nbt.4108 H. Cho, D. J. Wu, B. Berger, Secure genome-wide association analysis using multiparty computation, Nature biotechnology 36 (6) (2018) 547-551. doi:10.1038/nbt.4108. [20] J.P. Ioannidis T.A. Trikalinos M.J. Khoury Implications of small effect sizes of individual genetic variants on the design and interpretation of genetic association studies of complex diseases Am. J. Epidemiol. 164 7 2006 609 614 10.1093/aje/kwj259 J. P. Ioannidis, T. A. Trikalinos, M. J. Khoury, Implications of small effect sizes of individual genetic variants on the design and interpretation of genetic association studies of complex diseases, American journal of epidemiology 164 (7) (2006) 609-614. doi:10.1093/aje/kwj259. [21] S.S. Shringarpure C.D. Bustamante Privacy risks from genomic data-sharing beacons Am. J. Hum. Genet. 97 5 2015 631 646 10.1016/j.ajhg.2015.09.010 S. S. Shringarpure, C. D. Bustamante, Privacy risks from genomic data-sharing beacons, The American Journal of Human Genetics 97 (5) (2015) 631-646. doi:10.1016/j.ajhg.2015.09.010. [22] ENCODE Project Consortium and others An integrated encyclopedia of DNA elements in the human genome Nature 489 2012 57 74 10.1038/nature11247 (7414) ENCODE Project Consortium and others, An integrated encyclopedia of DNA elements in the human genome, Nature 489 (7414) (2012) 57-74. doi:10.1038/nature11247. [23] 1000 Genomes Project Consortium and others, A global reference for human genetic variation Nature 526 2015 68 74 10.1038/nature15393 (7571) 1000 Genomes Project Consortium and others, A global reference for human genetic variation, Nature 526 (7571) (2015) 68-74. doi:10.1038/nature15393. [24] L.B. Silva A. Trifan J.L. Oliveira Montra: an agile architecture for data publishing and discovery Comput. Methods Progr. Biomed. 160 2018 33 42 10.1016/j.cmpb.2018.03.024 L. B. Silva, A. Trifan, J. L. Oliveira, Montra: An agile architecture for data publishing and discovery, Computer methods and programs in biomedicine 160 (2018) 33-42. doi:10.1016/j.cmpb.2018.03.024. [25] J.L. Oliveira A. Trifan L.A.B. Silva EMIF Catalogue: a collaborative platform for sharing and reusing biomedical data Int. J. Med. Inf. 2019 10.1016/j.ijmedinf.2019.02.006 J. L. Oliveira, A. Trifan, L. A. B. Silva, EMIF Catalogue: A collaborative platform for sharing and reusing biomedical data, International Journal of Medical Informatics (2019). doi:10.1016/j.ijmedinf.2019.02.006. [26] J.R. Almeida A.J. Pinho J.L. Oliveira O. Fajarda D. Pratas GTO: a toolkit to unify pipelines in genomic and proteomic research SoftwareX 12 2020 100535 10.1016/j.softx.2020.100535 J. R. Almeida, A. J. Pinho, J. L. Oliveira, O. Fajarda, D. Pratas, GTO: A toolkit to unify pipelines in genomic and proteomic research, SoftwareX 12 (2020) 100535. doi:10.1016/j.softx.2020.100535. [27] D. Pratas M. Toppinen L. Pyöriä K. Hedman A. Sajantila M.F. Perdomo A hybrid pipeline for reconstruction and analysis of viral genomes at multi-organ level GigaScience 9 8 2020 10.1093/gigascience/giaa086 giaa086 D. Pratas, M. Toppinen, L. Pyoria, K. Hedman, A. Sajantila, M. F. Perdomo, A hybrid pipeline for reconstruction and analysis of viral genomes at multi-organ level, GigaScience 9 (8) (2020) giaa086. doi:10.1093/gigascience/giaa086. [28] C.S. Liew M.P. Atkinson M. Galea T.F. Ang P. Martin J.I.V. Hemert Scientific workflows: moving across paradigms ACM Comput. Surv. 49 4 2017 66 10.1145/3012429 C. S. Liew, M. P. Atkinson, M. Galea, T. F. Ang, P. Martin, J. I. V. Hemert, Scientific workflows: moving across paradigms, ACM Computing Surveys (CSUR) 49 (4) (2017) 66. doi:10.1145/3012429. [29] S. Holl O. Zimmermann M. Palmblad Y. Mohammed M. Hofmann-Apitius A new optimization phase for scientific workflow management systems Future Generat. Comput. Syst. 36 2014 352 362 10.1016/j.future.2013.09.005 S. Holl, O. Zimmermann, M. Palmblad, Y. Mohammed, M. Hofmann-Apitius, A new optimization phase for scientific workflow management systems, Future generation computer systems 36 (2014) 352-362. doi:10.1016/j.future.2013.09.005. [30] K. Wolstencroft R. Haines D. Fellows A. Williams D. Withers S. Owen S. Soiland-Reyes I. Dunlop A. Nenadic P. Fisher The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud Nucleic Acids Res. 41 W1 2013 W557 W561 10.1093/nar/gkt328 K. Wolstencroft, R. Haines, D. Fellows, A. Williams, D. Withers, S. Owen, S. Soiland-Reyes, I. Dunlop, A. Nenadic, P. Fisher, et al., The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud, Nucleic acids research 41 (W1) (2013) W557-W561. doi:10.1093/nar/gkt328. [31] J. Goecks A. Nekrutenko J. Taylor Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences Genome Biol. 11 8 2010 R86 10.1186/gb-2010-11-8-r86 J. Goecks, A. Nekrutenko, J. Taylor, Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences, Genome biology 11 (8) (2010) R86. doi:10.1186/gb-2010-11-8-r86. [32] J.R. Almeida R. Gini G. Roberto P. Rijnbeek J.L. Oliveira TASKA: a modular task management system to support health research studies BMC Med. Inf. Decis. Making 19 1 2019 121 10.1186/s12911-019-0844-6 J. R. Almeida, R. Gini, G. Roberto, P. Rijnbeek, J. L. Oliveira, TASKA: A modular task management system to support health research studies, BMC medical informatics and decision making 19 (1) (2019) 121. doi:10.1186/s12911-019-0844-6. [33] F. Wu S. Zhao B. Yu Y.-M. Chen W. Wang Z.-G. Song Y. Hu Z.-W. Tao J.-H. Tian Y.-Y. Pei A new coronavirus associated with human respiratory disease in China Nature 579 2020 265 269 10.1038/s41586-020-2008-3 (7798) F. Wu, S. Zhao, B. Yu, Y.-M. Chen, W. Wang, Z.-G. Song, Y. Hu, Z.-W. Tao, J.-H. Tian, Y.-Y. Pei, et al., A new coronavirus associated with human respiratory disease in China, Nature 579 (7798) (2020) 265-269. doi:10.1038/s41586-020-2008-3. [34] C. Heeney N. Hawkins J. de Vries P. Boddington J. Kaye Assessing the privacy risks of data sharing in genomics Public Health Genom. 14 1 2011 17 25 10.1159/000294150 C. Heeney, N. Hawkins, J. de Vries, P. Boddington, J. Kaye, Assessing the privacy risks of data sharing in genomics, Public health genomics 14 (1) (2011) 17-25. doi:10.1159/000294150. [35] D. Pratas A.J. Pinho Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight 2018 26th European Signal Processing Conference (EUSIPCO) 2018 IEEE 1177 1181 10.23919/EUSIPCO.2018.8553297 D. Pratas, A. J. Pinho, Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight, in: 2018 26th European Signal Processing Conference (EUSIPCO), IEEE, 2018, pp. 1177-1181. doi:10.23919/EUSIPCO.2018.8553297. [36] D. Pratas M. Hosseini G. Grilo A.J. Pinho R.M. Silva T. Caetano J. Carneiro F. Pereira Metagenomic composition analysis of an ancient sequenced polar bear jawbone from svalbard Genes 9 9 2018 445 10.3390/genes9090445 D. Pratas, M. Hosseini, G. Grilo, A. J. Pinho, R. M. Silva, T. Caetano, J. Carneiro, F. Pereira, Metagenomic Composition Analysis of an Ancient Sequenced Polar Bear Jawbone from Svalbard, Genes 9 (9) (2018) 445. doi:10.3390/genes9090445. [37] A. Zielezinski H.Z. Girgis G. Bernard C.-A. Leimeister K. Tang T. Dencker A.K. Lau S. Röhling J.J. Choi M.S. Waterman Benchmarking of alignment-free sequence comparison methods Genome Biol. 20 1 2019 144 10.1186/s13059-019-1755-7 A. Zielezinski, H. Z. Girgis, G. Bernard, C.-A. Leimeister, K. Tang, T. Dencker, A. K. Lau, S. Rohling, J. J. Choi, M. S. Waterman, et al., Benchmarking of alignment-free sequence comparison methods, Genome biology 20 (1) (2019) 144. doi:10.1186/s13059-019-1755-7. [38] W. Huang L. Li J.R. Myers G.T. Marth ART: a next-generation sequencing read simulator Bioinformatics 28 4 2011 593 594 10.1093/bioinformatics/btr708 W. Huang, L. Li, J. R. Myers, G. T. Marth, ART: a next-generation sequencing read simulator, Bioinformatics 28 (4) (2011) 593-594. doi:10.1093/bioinformatics/btr708. [39] Z. Lin A.B. Owen R.B. Altman Genomic research and human subject privacy Science 305 2004 10.1126/science.1095019 (5681) 183–183 Z. Lin, A. B. Owen, R. B. Altman, Genomic Research and Human Subject Privacy, Science 305 (5681) (2004) 183-183. doi:10.1126/science.1095019. [40] D. Pratas J.M. Silva Persistent minimal sequences of SARS-CoV-2 Bioinformatics 07 2020 10.1093/bioinformatics/btaa686 D. Pratas, J. M. Silva, Persistent minimal sequences of SARS-CoV-2, Bioinformatics (07 2020). doi:10.1093/bioinformatics/btaa686. [41] E. Vayena U. Gasser Between openness and privacy in genomics PLoS Med. 13 1 2016 e1001937 10.1371/journal.pmed.1001937 E. Vayena, U. Gasser, Between openness and privacy in genomics, PLoS medicine 13 (1) (2016) e1001937. doi:10.1371/journal.pmed.1001937. [42] G. Hripcsak J.D. Duke N.H. Shah C.G. Reich V. Huser M.J. Schuemie M.A. Suchard R.W. Park I.C.K. Wong P.R. Rijnbeek Observational health data sciences and Informatics (OHDSI): opportunities for observational researchers Stud. Health Technol. Inf. 216 2015 574 G. Hripcsak, J. D. Duke, N. H. Shah, C. G. Reich, V. Huser, M. J. Schuemie, M. A. Suchard, R. W. Park, I. C. K. Wong, P. R. Rijnbeek, et al., Observational Health Data Sciences and Informatics (OHDSI): opportunities for observational researchers, Studies in health technology and informatics 216 (2015) 574. [43] I. Bos S. Vos R. Vandenberghe P. Scheltens S. Engelborghs G. Frisoni J.L. Molinuevo A. Wallin A. Lleó J. Popp The EMIF-AD Multimodal Biomarker Discovery study: design, methods and cohort characteristics Alzheimer's Res. Ther. 10 1 2018 64 10.1186/s13195-018-0396-5 I. Bos, S. Vos, R. Vandenberghe, P. Scheltens, S. Engelborghs, G. Frisoni, J. L. Molinuevo, A. Wallin, A. Lleo, J. Popp, et al., The EMIF-AD Multimodal Biomarker Discovery study: design, methods and cohort characteristics, Alzheimer’s research & therapy 10 (1) (2018) 64. doi:10.1186/s13195-018-0396-5. [44] O. Tkachenko C. Weinert T. Schneider K. Hamacher Large-scale privacy-preserving statistical computations for distributed genome-wide association studies Proceedings of the 2018 on Asia Conference on Computer and Communications Security 2018 ASIACCS ’18, Association for Computing Machinery New York, NY, USA 221 235 10.1145/3196494.3196541 O. Tkachenko, C. Weinert, T. Schneider, K. Hamacher, Large-Scale Privacy-Preserving Statistical Computations for Distributed Genome-Wide Association Studies, in: Proceedings of the 2018 on Asia Conference on Computer and Communications Security, ASIACCS ’18, Association for Computing Machinery, New York, NY, USA, 2018, p. 221-235. doi:10.1145/3196494.3196541. [45] O. Goldreich Secure multi-party computation, Manuscript Preliminary Version 78 1998 O. Goldreich, Secure multi-party computation, Manuscript. Preliminary version 78 (1998). [46] L. Kannan M. Ramos A. Re N. El-Hachem Z. Safikhani D.M. Gendoo S. Davis D. Gomez-Cabrero R. Castelo K.D. Hansen Public data and open source tools for multi-assay genomic investigation of disease Briefings Bioinf. 17 4 2016 603 615 10.1093/bib/bbv080 L. Kannan, M. Ramos, A. Re, N. El-Hachem, Z. Safikhani, D. M. Gendoo, S. Davis, D. Gomez-Cabrero, R. Castelo, K. D. Hansen, et al., Public data and open source tools for multi-assay genomic investigation of disease, Briefings in bioinformatics 17 (4) (2016) 603-615. doi:10.1093/bib/bbv080. [47] F. Hufsky K. Lamkiewicz A. Almeida A. Aouacheria C. Arighi A. Bateman J. Baumbach N. Beerenwinkel C. Brandt M. Cacciabue Computational strategies to combat COVID-19: useful tools to accelerate SARS-CoV-2 and coronavirus research Briefings Bioinf. 11 2020 10.1093/bib/bbaa232 F. Hufsky, K. Lamkiewicz, A. Almeida, A. Aouacheria, C. Arighi, A. Bateman, J. Baumbach, N. Beerenwinkel, C. Brandt, M. Cacciabue, et al., Computational strategies to combat COVID-19: useful tools to accelerate SARS-CoV-2 and coronavirus research, Briefings in Bioinformatics (11 2020). doi:10.1093/bib/bbaa232. [48] M. Hoffmann M.T. Monaghan K. Reinert PriSeT: efficient de novo primer discovery BioRxiv 2020 10.1101/2020.04.06.027961 M. Hoffmann, M. T. Monaghan, K. Reinert, PriSeT: Efficient De Novo Primer Discovery, bioRxiv (2020). doi:10.1101/2020.04.06.027961. [49] A.A. Schäffer E.L. Hatcher L. Yankie L. Shonkwiler J.R. Brister I. Karsch-Mizrachi E.P. Nawrocki VADR: validation and annotation of virus sequence submissions to GenBank BMC Bioinf. 21 2020 1 23 10.1186/s12859-020-3537-3 A. A. Schaffer, E. L. Hatcher, L. Yankie, L. Shonkwiler, J. R. Brister, I. Karsch-Mizrachi, E. P. Nawrocki, VADR: validation and annotation of virus sequence submissions to GenBank, BMC Bioinformatics 21 (2020) 1-23. doi:10.1186/s12859-020-3537-3. "
    },
    {
        "doc_title": "Ac2: An efficient protein sequence compression tool using artificial neural networks and cache-hash models",
        "doc_scopus_id": "85105434500",
        "doc_doi": "10.3390/e23050530",
        "doc_eid": "2-s2.0-85105434500",
        "doc_date": "2021-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.Recently, the scientific community has witnessed a substantial increase in the generation of protein sequence data, triggering emergent challenges of increasing importance, namely efficient storage and improved data analysis. For both applications, data compression is a straightforward solution. However, in the literature, the number of specific protein sequence compressors is relatively low. Moreover, these specialized compressors marginally improve the compression ratio over the best general-purpose compressors. In this paper, we present AC2, a new lossless data compressor for protein (or amino acid) sequences. AC2 uses a neural network to mix experts with a stacked generalization approach and individual cache-hash memory models to the highest-context orders. Compared to the previous compressor (AC), we show gains of 2–9% and 6–7% in reference-free and reference-based modes, respectively. These gains come at the cost of three times slower computations. AC2 also improves memory usage against AC, with requirements about seven times lower, without being affected by the sequences’ input size. As an analysis application, we use AC2 to measure the similarity between each SARS-CoV-2 protein sequence with each viral protein sequence from the whole UniProt database. The results consistently show higher similarity to the pangolin coronavirus, followed by the bat and human coronaviruses, contributing with critical results to a current controversial subject. AC2 is available for free download under GPLv3 license.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Persistent minimal sequences of SARS-CoV-2",
        "doc_scopus_id": "85098462040",
        "doc_doi": "10.1093/bioinformatics/btaa686",
        "doc_eid": "2-s2.0-85098462040",
        "doc_date": "2020-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "COVID-19",
            "Humans",
            "Protein Binding",
            "SARS-CoV-2",
            "Spike Glycoprotein, Coronavirus"
        ],
        "doc_abstract": "© 2020 The Author(s) 2020. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.Motivation: Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused more than 14 million cases and more than half million deaths. Given the absence of implemented therapies, new analysis, diagnosis and therapeutics are of great importance. Results: Analysis of SARS-CoV-2 genomes from the current outbreak reveals the presence of short persistent DNA/RNA sequences that are absent from the human genome and transcriptome (PmRAWs). For the PmRAWs with length 12, only four exist at the same location in all SARS-CoV-2. At the gene level, we found one PmRAW of size 13 at the Spike glycoprotein coding sequence. This protein is fundamental for binding in human ACE2 and further use as an entry receptor to invade target cells. Applying protein structural prediction, we localized this PmRAW at the surface of the Spike protein, providing a potential targeted vector for diagnostics and therapeutics. In addition, we show a new pattern of relative absent words (RAWs), characterized by the progressive increase of GC content (Guanine and Cytosine) according to the decrease of RAWs length, contrarily to the virus and host genome distributions. New analysis shows the same property during the Ebola virus outbreak. At a computational level, we improved the alignment-free method to identify pathogen-specific signatures in balance with GC measures and removed previous size limitations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Efficient DNA sequence compression with neural networks",
        "doc_scopus_id": "85096082682",
        "doc_doi": "10.1093/gigascience/giaa119",
        "doc_eid": "2-s2.0-85096082682",
        "doc_date": "2020-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Base Sequence",
            "High-Throughput Nucleotide Sequencing",
            "Neural Networks, Computer",
            "Sequence Analysis, DNA",
            "Software"
        ],
        "doc_abstract": "© 2020 The Author(s). Published by Oxford University Press GigaScience.Background: The increasing production of genomic data has led to an intensified need for models that can cope efficiently with the lossless compression of DNA sequences. Important applications include long-term storage and compression-based data analysis. In the literature, only a few recent articles propose the use of neural networks for DNA sequence compression. However, they fall short when compared with specific DNA compression tools, such as GeCo2. This limitation is due to the absence of models specifically designed for DNA sequences. In this work, we combine the power of neural networks with specific DNA models. For this purpose, we created GeCo3, a new genomic sequence compressor that uses neural networks for mixing multiple context and substitution-tolerant context models. Findings: We benchmark GeCo3 as a reference-free DNA compressor in 5 datasets, including a balanced and comprehensive dataset of DNA sequences, the Y-chromosome and human mitogenome, 2 compilations of archaeal and virus genomes, 4 whole genomes, and 2 collections of FASTQ data of a human virome and ancient DNA. GeCo3 achieves a solid improvement in compression over the previous version (GeCo2) of 2.4%, 7.1%, 6.1%, 5.8%, and 6.0%, respectively. To test its performance as a reference-based DNA compressor, we benchmark GeCo3 in 4 datasets constituted by the pairwise compression of the chromosomes of the genomes of several primates. GeCo3 improves the compression in 12.4%, 11.7%, 10.8%, and 10.1% over the state of the art. The cost of this compression improvement is some additional computational time (1.7-3 times slower than GeCo2). The RAM use is constant, and the tool scales efficiently, independently of the sequence size. Overall, these values outperform the state of the art. Conclusions: GeCo3 is a genomic sequence compressor with a neural network mixing approach that provides additional gains over top specific genomic compressors. The proposed mixing method is portable, requiring only the probabilities of the models as inputs, providing easy adaptation to other data compressors or compression-based data analysis tools. GeCo3 is released under GPLv3 and is available for free download at https://github.com/cobilab/geco3.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The landscape of persistent human DNA viruses in femoral bone",
        "doc_scopus_id": "85087658242",
        "doc_doi": "10.1016/j.fsigen.2020.102353",
        "doc_eid": "2-s2.0-85087658242",
        "doc_date": "2020-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Pathology and Forensic Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2734"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Aged",
            "Aged, 80 and over",
            "DNA, Viral",
            "Female",
            "Femur",
            "Forensic Genetics",
            "Genotype",
            "High-Throughput Nucleotide Sequencing",
            "Humans",
            "Male",
            "Middle Aged",
            "Parvovirus B19, Human",
            "Polymerase Chain Reaction",
            "Sequence Analysis, DNA"
        ],
        "doc_abstract": "© 2020The imprints left by persistent DNA viruses in the tissues can testify to the changes driving virus evolution as well as provide clues on the provenance of modern and ancient humans. However, the history hidden in skeletal remains is practically unknown, as only parvovirus B19 and hepatitis B virus DNA have been detected in hard tissues so far. Here, we investigated the DNA prevalences of 38 viruses in femoral bone of recently deceased individuals. To this end, we used quantitative PCRs and a custom viral targeted enrichment followed by next-generation sequencing. The data was analyzed with a tailor-made bioinformatics pipeline. Our findings revealed bone to be a much richer source of persistent DNA viruses than earlier perceived, discovering ten additional ones, including several members of the herpes- and polyomavirus families, as well as human papillomavirus 31 and torque teno virus. Remarkably, many of the viruses found have oncogenic potential and/or may reactivate in the elderly and immunosuppressed individuals. Thus, their persistence warrants careful evaluation of their clinical significance and impact on bone biology. Our findings open new frontiers for the study of virus evolution from ancient relics as well as provide new tools for the investigation of human skeletal remains in forensic and archaeological contexts.",
        "available": true,
        "clean_text": "serial JL 273599 291210 291771 291825 291856 31 Forensic Science International: Genetics FORENSICSCIENCEINTERNATIONALGENETICS 2020-07-08 2020-07-08 2020-07-12 2020-07-12 2020-10-26T11:55:16 S1872-4973(20)30126-5 S1872497320301265 10.1016/j.fsigen.2020.102353 S300 S300.1 FULL-TEXT 2021-02-10T11:36:42.06775Z 0 0 20200901 20200930 2020 2020-07-08T15:31:15.247977Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor highlightsabst primabst pubtype ref specialabst 1872-4973 18724973 true 48 48 C Volume 48 20 102353 102353 102353 202009 September 2020 2020-09-01 2020-09-30 2020 Original papers article fla © 2020 Published by Elsevier B.V. LANDSCAPEPERSISTENTHUMANDNAVIRUSESINFEMORALBONE TOPPINEN M 1 Introduction 2 Materials and methods 2.1 Study subjects 2.2 Specimen collection and preparation 2.3 DNA extraction 2.4 Quantitative PCRs 2.5 PCR inhibition tests 2.6 Library preparation, viral enrichment, and sequencing 2.7 NGS data analysis 2.8 Statistical analysis 3 Results 3.1 Viral DNA prevalences in the external and internal surfaces of the femoral bone 3.2 Viruses are highly prevalent in human femoral bone 3.3 Viral capture and sequence analysis 4 Discussion 5 Conclusions Contributions Acknowledgments Appendix A Supplementary data References HOLMES 2008 307 328 E ZHONG 2009 144 152 S IKEGAYA 2007 41 46 H MIYAMORI 62015 D TRACINGJOMONYAYOIANCESTRIESINJAPANUSINGALDH2JCVIRUSGENOTYPEDISTRIBUTIONS IKEGAYA 2004 169 172 H IKEGAYA 2008 78 85 H INOUE 2010 903 908 H FORNI 2019 D KOLB 2013 e76267 A BOWEN 2018 1 46 C IKEGAYA 2004 169 172 H AGOSTINI 1997 14542 14546 H SUGIMOTO 1997 9191 9196 C SUGIMOTO 2002 322 335 C SHACKELTON 2006 9928 9933 L SHARP 2011 436 441 P KITCHEN 2008 A ZHONG 2007 193 198 S SMITH 2002 J AGESPECIFICPREVALENCEINFECTIONHERPESSIMPLEXVIRUSTYPES21AGLOBALREVIEW NORJA 2006 7450 7453 P TOPPINEN 2015 17226 M MUHLEMANN 2018 7557 7562 B MUHLEMANN 2018 418 423 B KRAUSEKYORA 2018 e36666 B GOH 2009 489 491 S DUMOULIN 2011 1382 1388 A HOFFMAN 2008 2671 2680 N TOPPINEN 2015 40 45 M PYORIA 2020 L PISTELLO 2001 189 195 M MAGGI 2001 418 422 F MCNEES 2005 52 62 A PRATAS 2020 D LIEBERMAN 2016 619 628 P KUTLUAY 2009 456 466 S HIGGINS 2015 1 17 D PEDERSEN 2014 454 466 J ORLANDO 2015 395 408 L PYORIA 2017 14930 L BERGER 2007 1147 1152 C RANDHAWA 2005 504 509 P HINO 2007 45 57 S TOPPINENX2020X102353 TOPPINENX2020X102353XM 2021-07-12T00:00:00.000Z 2021-07-12T00:00:00.000Z © 2020 Published by Elsevier B.V. 2020-07-15T01:09:11.779Z S1872497320301265 Kone Foundation Koneen SÃ¤Ã¤tiÃ¶ Finska Läkaresällskapet Finska LÃ¤karesÃ¤llskapet Finnish Society of Sciences and Letters Suomen Tiedeseura Finnish Medical Society Finska LÃ¤karesÃ¤llskapet Life and Health Medical Foundation Magnus Ehrnrooth Foundation Magnus Ehrnroothin SÃ¤Ã¤tiÃ¶ University of Helsinki and Helsinki University Hospital Finnish Cultural Foundation SKR Suomen Kulttuurirahasto Juhani Aho Foundation for Medical Research Jane and Aatos Erkko Foundation J&AE Jane ja Aatos Erkon SÃ¤Ã¤tiÃ¶ Sigrid Jusélius Foundation Sigrid JusÃ©liuksen SÃ¤Ã¤tiÃ¶ We would like to thank Teemu Smura, Anna-Maija Sulonen and the Sequencing unit of the Institute for Molecular Medicine Finland for their support with NGS, and Anna-Liina Mustaniemi for instruction on bone handling. This study was funded by grants from the Finnish Medical Society (MFP) , the Jane and Aatos Erkko Foundation (KH) , the Finnish Cultural Foundation (MT, MFP) , the Juhani Aho Foundation for Medical Research (MT) , the Sigrid Jusélius Foundation (MS-V, KH) , the Life and Health Medical Foundation (MS-V, AS) , the Magnus Ehrnrooth Foundation (AS, KH) , Finnish Society of Sciences and Letters (KH, AS) , the Kone Foundation (AS) , the Research Funds of University of Helsinki and Helsinki University Hospital (KH) , and Finska Läkaresällskapet (KH, MFP) . Fundação para a Ciência e a Tecnologia, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019 (DP). item S1872-4973(20)30126-5 S1872497320301265 10.1016/j.fsigen.2020.102353 273599 2021-02-10T11:36:42.06775Z 2020-09-01 2020-09-30 true 2602176 MAIN 7 53747 849 656 IMAGE-WEB-PDF 1 gr4 111255 996 389 gr1 19786 274 470 gr3 48950 592 565 gr2 40344 206 659 ga1 true 32766 195 500 gr4 5031 164 64 gr1 5221 128 219 gr3 5527 163 156 gr2 6057 68 219 ga1 true 7600 86 219 gr4 938118 4417 1725 gr1 131616 1215 2083 gr3 390499 2620 2500 gr2 262809 910 2917 ga1 true 215869 865 2213 mmc1 mmc1.docx docx 21619 APPLICATION am 1251795 FSIGEN 102353 102353 S1872-4973(20)30126-5 10.1016/j.fsigen.2020.102353 Fig. 1 Viral loads in the external and internal surfaces of the femoral bone. The numbers of cells as well as the DNA copies of torque teno virus (TTV), parvovirus B19 (B19V), and Epstein-Barr virus (EBV) from the external and internal surfaces of femoral bone were determined. No significant differences between these two locations were observed (p > 0.5). The cell counts are expressed per 1 μL of DNA extract and the viral DNA copy numbers per 1E6 cells. Fig. 1 Fig. 2 Viral DNA occurrence by NGS and qPCR in bone per study subject. The viral findings are presented as green (NGS+, qPCR+), blue (NGS+, qPCR-), orange (NGS-, qPCR+), and white (NGS-, qPCR-). *The cause of death (COD) is given according to the WHO ICD10 classification. ^The manner of death (MOD) is presented as 1=disease, 2=occupational disease, 3=injury, 5=suicide. The results are presented from left to right according to the highest and lowest virus prevalences in bone. B19V = parvovirus B19; HSV1=herpes simplex virus-1; VZV = varicella-zoster virus; EBV = Epstein-Barr virus; CMV = cytomegalovirus; HHV6B = human herpesvirus 6B; HHV7= human herpesvirus 7; JCPyV = JC polyomavirus; MCPyV = Merkel cell polyomavirus; HPV = human papillomavirus; HBV = hepatitis B virus; TTV = torque teno virus (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article). Fig. 2 Fig. 3 DNA fragment analysis of total DNA. Fragment length distribution of patterns of genomic DNA for four representative samples as established with LabChip GX. Fig. 3 Fig. 4 Coverage profiles of reconstructed viral DNA sequences. NGS coverage profiles (breadth and depth) of representative viruses. Fig. 4 Table 1 Viral DNA prevalence in human femoral bone. Table 1 Family Virus Genome size (kb) Geno-prevalence NGS + cases qPCR + cases Breadth coverage (range or value of single sample) Viral load (copies/ million cells) Parvoviridae B19V 5.6 74.1 % 17 16 4.5−100.0 % 2.4E3 Herpesviridae HSV1 152 3.7 % 1 1 3.4 % 4.9E1 VZV 125 3.7 % 1 1 13.6 % 2.0E0 EBV 170 25.9 % 6 4 1.0−3.3 % 2.1E1 CMV 236 3.7 % 0 1 – 9.8E1 HHV6B 162 11.1 % 3 0 2.0−3.1 % – HHV7 150 18.5 % 5 0 4.8−12.0 % – Polyomaviridae JCPyV 5.1 14.8 % 3 2 3.7−14.8 % 2.7E1 MCPyV 5.4 33.3 % 9 4 1.4−58.6 % 8.7E3 Papillomaviridae HPV 8 22.2 % 6 1* 1.8−89.6 % 8.7E3 Hepadnaviridae HBV 3.2 7.4 % 2 1 15.6−44.5 % 1.7E3 Anelloviridae TTV 3.8 81.5 % 9 22 2.1−56.3 % 1.9E4 * Only HPV type 31 qPCR was performed. B19V: parvovirus B19; HSV1: herpes simplex virus-1; VZV: varicella-zoster virus; EBV: Epstein-Barr virus; CMV: cytomegalovirus; HHV6B: human herpesvirus 6B; HHV7: human herpesvirus 7; JCPyV: JC polyomavirus; MCPyV: Merkel cell polyomavirus; HPV: human papillomavirus; HBV: hepatitis B virus; TTV: torque teno virus. Research paper The landscape of persistent human DNA viruses in femoral bone Mari Toppinen a Diogo Pratas a b c Elina Väisänen a Maria Söderlund-Venermo a Klaus Hedman a d Maria F. Perdomo a ** 1 Antti Sajantila e f * 1 a Department of Virology, University of Helsinki, Finland Department of Virology University of Helsinki Finland Department of Virology, University of Helsinki, Finland b Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal Department of Electronics, Telecommunications and Informatics University of Aveiro Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal c Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal d HUSLAB, Helsinki University Hospital, Finland HUSLAB Helsinki University Hospital Finland HUSLAB, Helsinki University Hospital, Finland e Department of Forensic Medicine, University of Helsinki, Finland Department of Forensic Medicine University of Helsinki Finland Department of Forensic Medicine, University of Helsinki, Finland f Forensic Medicine Unit, Finnish Institute of Health and Welfare, Finland Forensic Medicine Unit Finnish Institute of Health and Welfare Finland Forensic Medicine Unit, Finnish Institute of Health and Welfare, Finland ⁎ Corresponding author. ⁎⁎ Corresponding author. 1 Shared last authorship. Graphical abstract The imprints left by persistent DNA viruses in the tissues can testify to the changes driving virus evolution as well as provide clues on the provenance of modern and ancient humans. However, the history hidden in skeletal remains is practically unknown, as only parvovirus B19 and hepatitis B virus DNA have been detected in hard tissues so far. Here, we investigated the DNA prevalences of 38 viruses in femoral bone of recently deceased individuals. To this end, we used quantitative PCRs and a custom viral targeted enrichment followed by next-generation sequencing. The data was analyzed with a tailor-made bioinformatics pipeline. Our findings revealed bone to be a much richer source of persistent DNA viruses than earlier perceived, discovering ten additional ones, including several members of the herpes- and polyomavirus families, as well as human papillomavirus 31 and torque teno virus. Remarkably, many of the viruses found have oncogenic potential and/or may reactivate in the elderly and immunosuppressed individuals. Thus, their persistence warrants careful evaluation of their clinical significance and impact on bone biology. Our findings open new frontiers for the study of virus evolution from ancient relics as well as provide new tools for the investigation of human skeletal remains in forensic and archaeological contexts. Keywords DNA viruses Femoral bone NGS Human provenance Parvovirus B19 genotype 2, Papillomavirus 31 1 Introduction DNA viruses commonly establish persisting infections in humans, remaining thus, their genetic material imprinted in the tissues throughout life. These DNAs exhibit phylogeographies resembling those of humans [1], pointing to shared evolutionary and dispersal paths that can, together, add to the understanding of human population history and migrations [2–8]. In addition, the specific geographical distributions of DNA viruses, in both global [2,9] and local [5,10,11] scales, may provide new insights into the origins of unidentified individuals. In this regard, the most widely studied virus is JC polyomavirus (JCPyV), a highly prevalent pathogen with three main phylogenetic clusters spread across Africa, Asia, and Europe. JCPyV´s genotype-specific global spread has been proposed as an indicator of the provenances of both modern [5] and ancient humans [12–14]. Although its timescale of evolution has been debated [15–17], recent work by Forni et al. [8], based on ∼1100 worldwide strains, supports co-dispersal of this virus with major human migratory routes as well as its co-divergence with human mitochondrial and nuclear markers. In addition to the spatial and temporal dimensions, viruses also display intriguing age-dependent distributions [18–20]. This is the case with genotype 2 of parvovirus B19, the traces of which went missing in Europe around 1970s. Thus, contemporary DNA findings of this virus variant in tissues are confined to elderly individuals or historic human remains [21]. Despite these encouraging data, the utility of the viral DNAs has not been fully addressed in forensic and anthropological settings. Reasons for this include the lack of a comprehensive picture of the overall distribution of viral DNAs in various organs, limiting the use of some human tissues in versatile scenarios, and most importantly, the fact that their prevalences in bones and teeth are almost entirely unknown. Of the latter, we were the first to detect parvovirus B19 DNA in bones from soldiers of World War II [21], followed by findings of this same virus [22] and of hepatitis B [23,24] virus in archaeological remains dated from the Neolithic to medieval times. These singular discoveries confirm the presence and preservation of viral DNA in hard tissues and call for the investigation of the full landscape of viral DNAs that here persist. In this study, we searched for 38 highly prevalent DNA viruses in the femoral bones of recently deceased individuals. To this end, we used in-house quantitative PCRs (qPCRs) and virus-targeted enrichment coupled with next-generation sequencing (NGS). Our findings significantly expand the present knowledge on the virome of human bone, opening new frontiers for the study of virus evolution as well new tools for the investigation of forensic and anthropological cases. 2 Materials and methods 2.1 Study subjects The study cohort consisted of 27 deceased individuals of Finnish origin, aged 36–85 years (mean 67.6) with a male:female ratio of 19:8. The postmortem interval ranged from 4 to 30 days (mean 8). The manners of death were disease (n = 19), occupational disease (n = 1), injury (n = 5), and suicide (n = 2). In none of the cases the medical records revealed the use of immunosuppressants or pre-conditions related to infectious-diseases, except for one with a history of herpes-zoster a few weeks before death. The cause and manner of death are presented in Fig. 2. The study protocol was reviewed by the Ethics Committee of Helsinki and Uusimaa Hospital District (approval 164/13/03/00/2014). 2.2 Specimen collection and preparation A 5−10 cm piece of the femoral diaphysis was collected and cut longitudinally with an oscillating tip saw. The bone was cleaned using a toothbrush and washed sequentially in distilled water, 0.1 % sodium hypochlorite, and 96.1 w/w ethanol. The bones were let to dry for 5–7 days at room temperature under flow in a laminar hood. From each individual, the external and internal surfaces of femoral bone were sampled using a dentist drill (Schick Qube) except for three cases, in which the bone was cryomilled (Spex 6775 Freezer/mill; Spex). The sample preparations and drillings were performed in enclosed dedicated facilities, using full-body protection suits. 2.3 DNA extraction Approximately 0.1−0.4 g of bone powder were lysed and extracted as specified before [21] and eluted in a final volume of 50−100 μl. The DNA extractions were performed in enclosed facilities, dedicated to the extraction of human DNA. No viral work is performed in these rooms. The total DNA was quantitated with Quant-iT™ PicoGreen™ dsDNA Assay Kit (Thermo Fisher Scientific). The quality of the total DNA was evaluated with the Genomic DNA Reagent Kit in a LabChip GX Instrument (Perkin Elmer). 2.4 Quantitative PCRs The quantifications of parvovirus B19, Merkel cell, JC and BK polyomaviruses, and nine human herpesviruses as well as of the human single-copy gene RNase P were performed with in-house quantitative PCRs (qPCRs) as described [25–29]. The quantification of hepatitis B virus and human papillomavirus type 31 DNAs were performed with commercial kits (Hepatitis B Virus PCR Kit, GeneProof; Genesig Human papillomavirus 31 Standard kit, PrimerDesign) according to the manufacturer instructions. For the quantification of torque teno virus, a qPCR was optimized [30,31] to amplify and detect the conserved untranslated region (UTR) of the virus using the following primers: AMTS fwd (5′-GTGCCGNAGGTGAGTTTA-'3), AMTAS rev (5′-AGCCCGGCCAGTCC-'3), AMTASgr4 rev (5′-AGCCCGGCCAGACC-'3) and AMTPTU probe (5′-FAM-TCAAGGGGCAATTCGGGCT-BHQ1-'3). The qPCR reaction consisted of 1x Maxima probe qPCR Master mix (Thermo Fisher Scientific) with 0.03 μM of ROX passive reference dye, 0.5 μM of each of the primers, 0.4 μM of the probe, 5 μL of the template, and nuclease-free water to a final volume of 25 μL. After initial denaturation at +95 °C for 10 min, the qPCR cycles were +95 °C for 15 s and +55 °C for 1 min for 45 cycles. The qPCR amplicons were 63–154 nucleotides in length. Plasmid dilution series were used in all the qPCR runs as positive controls and to create standard curves for quantification. The plasmids of parvovirus B19, human herpesvirus 1–8, Merkel cell, JC and BK polyomaviruses, and RNaseP are described elsewhere [29,32]. For torque teno virus, a plasmid, named 10B, containing 1184 nucleotides of the virus was cloned from a healthy blood donor’s plasma (GenBank MT448658). The virus amplifications were completed with AriaMx Real-Time PCR System except for those of torque teno virus, RNaseP, and human papillomavirus 31 that were analyzed with Stratagene Mx3005 P qPCR System (both Agilent). The qPCR mixes, sample handling, plasmid dilutions, and amplifications, were performed each in completely separate rooms, following strict protocols and work-flows to prevent contamination. Negative controls (PCR-grade water) were included in every step starting from the DNA extraction. 2.5 PCR inhibition tests PCR efficiency due to carryover of inhibitors following DNA extraction was controlled using DNA extracts from bone together with or in parallel to a pre-quantified RNaseP plasmid [32]. To evaluate the performance of different polymerases and the quantification accuracy, four different commercial master mixes were tested. From this, Maxima probe qPCR master mix was selected and used throughout the study. The impact of residual EDTA on the qPCR performance was examined by testing varying EDTA concentrations in the lysis buffer (0.5–500 nM) and by adding excess EDTA (0.05–50 nM) to the qPCR reaction. In addition, pre-quantified plasmid dilutions were extracted following the bone extraction protocol (i.e. lysis buffer containing 500 nM EDTA). The impact of residual Ca2+-ions from bone in the extracts was investigated by addition of MgCl2 to the qPCR reactions (in final concentrations of 4–7 mM). 2.6 Library preparation, viral enrichment, and sequencing The sequencing libraries were prepared on 10–1000 ng of total DNA using the KAPA HyperPlus library preparation kit (Roche), following the manufacturer protocol with two modifications: 1) mechanical fragmentation with a Covaris E220 of the DNA with target fragments of 200 nt and 2) the use of xGen Dual Index UMI Adapters (Integrated DNA Technologies). After sonication, the fragment length distributions were analyzed with the DNA High Sensitivity Reagent Kit in a LabChip GX Instrument (Perkin Elmer). The viral enrichment was performed using two consecutive rounds of hybridization on individual samples following recommendations for low input DNA (MyBaits v4 kit; Arbor Biosciences). For each library, 200 ng per round of biotinylated RNA-baits were used. The baits were 100 nt in length and designed with 2X tiling (Supplementary Fig. 1 for a list of viruses). xGen Universal Blockers-TS Mix (Integrated DNA Technologies) were used to block unspecific binding to the adapters during hybridization. During library preparation and viral enrichment, the libraries were amplified 3 × 13–25 cycles. The clean-up steps were performed with either KAPA Pure Beads (Roche) or MinElute PCR Purification Kit (Qiagen). The enriched libraries were quantified with KAPA Library Quantification Kit (Roche) using Stratagene 3005 P qPCR System (Agilent) and subsequently pooled for sequencing on NovaSeq 6000 (SP PE151 reagent kit; Illumina). 2.7 NGS data analysis The viral genomic sequences were reconstructed after removal of PCR duplicates, using a customized bioinformatics pipeline (TRACESPipe [33]; available for download at The consensus, as well as single sequences (when in low coverage), were confirmed by BLAST (NCBI). The highest similarity was used to classify the virus genotype. For parvovirus B19, the sequences covering >70 % of the viral genome (n = 7) were aligned with previously published full or near-full length sequences in EMBL-EBI Clustal Omega and analyzed with Bioedit v.7.2.5 (Ibis Biosciences). For the following viruses, the consensus sequences will be available in GenBank with respective accession numbers: parvovirus B19 (7 sequences; MT410184-MT410190); human papillomavirus type 31 (MT410191); hepatitis B virus (MT410192); Merkel cell polyomavirus (MT410193). 2.8 Statistical analysis The differences in viral or cell copy numbers in the external and internal surfaces of femoral bone were calculated with Student’s t-test in RStudio (version 1.0.153). 3 Results 3.1 Viral DNA prevalences in the external and internal surfaces of the femoral bone To determine the most optimal site for sampling, we first investigated the differences in prevalence and quantity of persisting viral DNAs in the external and internal surfaces of 27 femoral bones (Table 1 ). To this end, we examined three ubiquitous viruses (torque teno, parvovirus B19, and Epstein-Barr virus) known to infect >70 % of the global population and to persist in several soft tissues in the body. To control for bias by uneven amounts of DNA in the extracts, we performed in parallel a qPCR for the human single-copy gene RNAse P. We found no significant differences in the viral DNA prevalences (p > 0.1) nor copy numbers (p values >0.5) between these two surfaces. Hence, subsequent analyses were performed only on samples taken from the external surface. 3.2 Viruses are highly prevalent in human femoral bone We then investigated the prevalences of altogether 38 persistent virus genomes using targeted enrichment and confirmatory qPCRs. Overall, the 27 study subjects harbored on average 2.6 virus-types in their femoral bones, with a maximum of seven in one individual. Altogether, we detected 12 different virus-types in 92.6 % of the bones, with only two individuals (> 60 years of age) being completely negative for all viruses tested. The viral findings are presented in Table 1 and Fig. 2 . The viruses most prevalent in bone were torque teno virus and parvovirus B19 with genoprevalences of 81.5 % and 74.1 %, respectively (Table 1). The third most prevalent was Merkel cell polyomavirus at 33.3 %, followed by Epstein-Barr virus (25.9 %), human papillomavirus (22.2 %), human herpesvirus 7 (18.5 %) and JC polyomavirus (14.8 %). Other viral sequences detected were of herpes simplex 1, varicella-zoster, cytomegalovirus, human herpesvirus 6B, and hepatitis B virus. The median viral copy numbers per one million cells were 1.9E4 for torque teno virus, 2.4E3 for parvovirus B19, 8.7E3 for Merkel cell polyomavirus, and 2.1E1 for Epstein-Barr virus (Table 1). The most common co-occurrences were of parvovirus B19 and torque teno virus, found in 70.3 % of the samples. Interestingly, we detected in the bone of one individual three cancer-associated viruses: hepatitis B virus, human papillomavirus type 31, and Merkel cell polyomavirus. We verified the accuracy of the qPCR results by examining the patterns of DNA fragmentation in the extracts and by evaluating the impact of potential inhibitors carried over during lysis and extraction. The quality of the total DNA in each sample was analyzed with a LabChip GX Instrument, which revealed 100–500 nt fragments in addition to intact genomic DNA (≤40 kb; Fig. 3 ). We found no PCR inhibition accountable to excess EDTA or Ca2+ in the extracts. 3.3 Viral capture and sequence analysis As established from the qPCR results, the persisting viral DNA quantities were far below 1% of total DNA present in a sample. Thus, to enrich this fraction, we performed in-solution capture with biotinylated RNA oligonucleotides prior to sequencing in Novaseq 6000. Subsequently, we analyzed the NGS data with a custom pipeline, TRACESPipe [33], which reconstructs the viral sequences using both reference-based alignment and de-novo assembly. We reconstructed a total of 15 viral genomic sequences, with a minimum of 15 % breadth coverage. The highest qualities were attained for parvovirus B19 (n = 7; average breadth coverage 86.1 %), human papillomavirus (n = 1; breadth coverage 89.6 %), Merkel cell polyomavirus (n = 1; breath coverage 58.6 %), JC polyomavirus (n = 1; breadth coverage 14.8 %), hepatitis B virus (n = 2; breadth coverages 44.5 % and 15.6 %), and torque teno virus (n = 3; breadth coverages 56.3 %, 30.6 % and 24.9 %). The breadth and depth coverages of representative viruses are presented in Fig. 4 . By NCBI BLAST, we found close relation of the reconstructed genomes to previously published sequences. We recovered seven full/near-full B19 V genomic sequences, three of which were of genotype 1 (95.16%–99.61% similarity to AY504945.1) and four of genotype 2 (90.1%–98.6%, similarity to AB550331.1). The latter genotype represents an extinct form of this virus and was found in the present study exclusively in individuals older than 68 years. Of this genotype, only a single full-length sequence has ever been published (AB550331.1). We found that this genotype’s hairpins present similar flip and flop configurations to those of genotype 1. The human papillomavirus sequence showed 99.1 % similarity to type 31 (KU298889.1); Merkel cell polyomavirus 98.7 % to the European/Caucasian type (KF266963.1) and JC polyomavirus 100.0 % to genotype 1 (MF662198.1). The two hepatitis B virus sequences matched 99.4 % and 99.6 % to genotypes D (JX898691.1) and A (MN507849.1), respectively; and the torque teno virus sequences showed 90.2 %, 99.0 % and 96.2 % similarity to strains AF122920.1, KT163880.1 and FR751497.1, respectively. Moreover, we confirmed by BLAST unique sequences (mean length 100 nt) mapping to the following viruses: herpes simplex 1 (1 case, 28 reads), varicella-zoster (1 case, 362 reads), Epstein Barr (6 cases, altogether 270 reads), human herpesvirus 6B (3 cases, total of 66 reads), human herpesvirus 7 (5 cases, total of 274 reads), JC polyomavirus (3 additional cases, total of 13 reads), Merkel cell polyomavirus (8 additional cases, total of 15 reads), human papillomavirus (5 additional cases, total of 14 reads), torque teno virus (6 additional cases, total of 26 reads), and parvovirus B19 (10 additional cases, total of 673 reads). 4 Discussion The analysis of viral DNAs shows great potential as complementary markers for human identification as well as for estimation of provenance and migration. However, one limitation of their use in these contexts is insufficient knowledge of the landscape of viruses persisting in the host, in particular in human bone, with only parvovirus B19 [21,22] and hepatitis B [23] virus having been detected in this tissue so far. To this end, we systematically explored the prevalences of 38 ubiquitous viruses in human femoral bone. We discovered an unprecedented number of viral DNAs, detecting up to seven per individual. Besides the already known, we report here on ten new viruses including several members of the herpesvirus family (herpes simplex-1, varicella-zoster, Epstein-Barr, cytomegalovirus, human herpesviruses 6B and 7), JC- and Merkel cell polyomaviruses, human papillomavirus 31, and torque teno virus. Intriguingly, a common feature shared by these viruses (except for HHV-6B) is their persistence in soft tissues in episomal form [34]. Although the methods used in this study prevent us from confirming whether this is also the case in bone, the low copy numbers detected are indicative of a quiescent infection. Moreover, some of them (e.g. herpes-, polyoma-, and papillomaviruses) are known to establish latency as densely packed nucleosomes, as mechanism to regulate gene expression [35,36]. Thus, the circular form and the tight histone packaging may confer DNA viruses superior preservation in relation to the host DNA (nuclear and mitochondrial) [37–39]. In this regard, the analysis of viral DNA could be of utmost value for forensic investigations or studies of ancient human remains dealing with highly compromised samples. One challenging factor in the study of the human virome continues to be its extremely low proportion in relation to other sources of DNA. With this in mind, we carried out our screening using two approaches, qPCR and NGS. We found that the viral DNAs were exceeded on average a 1000-fold by the nuclear DNA alone, in line with the loads reported for persisting viruses in soft tissues [40–42]. By targeted enrichment and NGS we identified a higher number of viruses than by qPCR, an observation that can in part be explained by moderate DNA fragmentation in the samples. The only exception was torque teno virus (TTV), for which we found a significantly higher number of positive samples by qPCR. This circular ssDNA virus exhibits substantial heterogeneity [43], which was under-represented by the three reference sequences upon which our baits were designed. Hence, a better characterization of it could require a broader bait coverage of its five genogroups together with analysis at the amino acid level, as certain motifs are more likely to be shared by different TTVs. The qPCR on the other hand, targeted the conserved UTR region, to recognize a wide a repertoire of TT-viruses. Unsurprisingly, larger genome coverages were recovered from the viruses with the highest copy numbers (∼log3 copies/million cells). However, many of the viral loads fell below this threshold, whereby, follow-up singleplex enrichments may be beneficial to increase the analytical resolution of the data. Importantly, even for the low coverage genomes, we confirmed the sequences retrieved to be virus-specific and hence, genuine findings of the femoral bone. In all, we unveil that the human femoral bone is a much richer source of persistent DNA viruses than earlier known. We propose that a “Human Virome Panel” could be built as an efficient tool for assessment of human provenance in conjunction with the standard human DNA profiling. Indeed, the phylogeographical distributions of JC [12,13,42], BK [2], varicella-zoster [7], and Epstein Barr [6] viruses have been examined in this context and shown to add power to forensic cases and anthropological studies. Undoubtedly, larger cohorts, both global and local, are required to validate the benefits and boundaries of such panel in multiple taphonomic conditions. This work could also help to identify patterns in the age distributions of certain viruses and their genotypes [18,19], to support biometric estimations. Moreover, our data raise intriguing questions on the clinical significance of the long-term presence of these viruses – or their genomes – in the human skeleton. Indeed, many of the viruses we found can reactivate, and several have oncogenic potential. Among the latter, we detected Merkel cell polyomavirus and human papillomavirus 31, two oncoviruses which, remarkably, are mucocutaneous. Certainly, these unexpected findings warrant further studies on the transcriptional, translational and reactivation potential of these pathogens as bone residents, particularly among the elderly and immune suppressed. 5 Conclusions Our work substantially expands the current knowledge on the spectrum of DNA viruses persisting in human bone and opens new perspectives on their applicability in the investigation of human skeletal remains. It also supports the search for viruses from ancient relics, which can foreseeably remodel our understanding of virus evolution. Contributions M.T., K.H., M.F.P and A.S designed the study. M.T. executed the experiments. MT, D.P. and M.F.P analyzed the data, E.V. and M.S.V contributed with a plasmid and qPCR optimization, M.T., M.F.P. and A.S. drafted the manuscript. All authors commented, edited and approved the final version of the manuscript. Declaration of Competing Interest The authors declare no conflict of interest. Acknowledgments We would like to thank Teemu Smura, Anna-Maija Sulonen and the Sequencing unit of the Institute for Molecular Medicine Finland for their support with NGS, and Anna-Liina Mustaniemi for instruction on bone handling. This study was funded by grants from the Finnish Medical Society (MFP), the Jane and Aatos Erkko Foundation (KH), the Finnish Cultural Foundation (MT, MFP), the Juhani Aho Foundation for Medical Research (MT), the Sigrid Jusélius Foundation (MS-V, KH), the Life and Health Medical Foundation (MS-V, AS), the Magnus Ehrnrooth Foundation (AS, KH), Finnish Society of Sciences and Letters (KH, AS), theKone Foundation (AS), the Research Funds of University of Helsinki and Helsinki University Hospital (KH), and Finska Läkaresällskapet (KH, MFP). Fundação para a Ciência e a Tecnologia, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019 (DP). Appendix A Supplementary data Supplementary material related to this article can be found, in the online version, at Appendix A Supplementary data The following is Supplementary data to this article: References [1] E.C. Holmes Evolutionary history and phylogeography of human viruses Annu. Rev. Microbiol. 62 2008 307 328 10.1146/annurev.micro.62.081307.162912 E.C. Holmes, Evolutionary History and Phylogeography of Human Viruses, Annu. Rev. Microbiol. 62 (2008) 307–328. [2] S. Zhong P.S. Randhawa H. Ikegaya Q. Chen H.-Y. Zheng M. Suzuki T. Takeuchi A. Shibuya T. Kitamura Y. Yogo Distribution patterns of BK polyomavirus (BKV) subtypes and subgroups in American, European and Asian populations suggest co-migration of BKV and the human race J. Gen. Virol. 90 2009 144 152 10.1099/vir.0.83611-0 S. Zhong, P.S. Randhawa, H. Ikegaya, Q. Chen, H.-Y. Zheng, M. Suzuki, T. Takeuchi, A. Shibuya, T. Kitamura, Y. Yogo, Distribution patterns of BK polyomavirus (BKV) subtypes and subgroups in American, European and Asian populations suggest co-migration of BKV and the human race., J. Gen. Virol. 90 (2009) 144–52. [3] H. Ikegaya H. Motani P. Saukko K. Sato T. Akutsu K. Sakurada BK virus genotype distribution offers information of tracing the geographical origins of unidentified cadaver Forensic Sci. Int. 173 2007 41 46 10.1016/j.forsciint.2007.01.022 H. Ikegaya, H. Motani, P. Saukko, K. Sato, T. Akutsu, K. Sakurada, BK virus genotype distribution offers information of tracing the geographical origins of unidentified cadaver, Forensic Sci. Int. 173 (2007) 41–46. [4] D. Miyamori N. Ishikawa N. Idota Y. Kakiuchi S. Mclean T. Kitamura H. Ikegaya Tracing Jomon and Yayoi Ancestries in Japan Using ALDH2 and JC Virus Genotype Distributions Investig. Genet. 6,2015, 14 10.1186/s13323-015-0031-1 D. Miyamori, N. Ishikawa, N. Idota, Y. Kakiuchi, S. Mclean, T. Kitamura, H. Ikegaya, Tracing Jomon and Yayoi ancestries in Japan using ALDH2 and JC virus genotype distributions, (2015). [5] H. Ikegaya H. Iwase Trial for the geographical identification using JC viral genotyping in Japan Forensic Sci. Int. 139 2004 169 172 10.1016/J.FORSCIINT.2003.10.019 H. Ikegaya, H. Iwase, Trial for the geographical identification using JC viral genotyping in Japan, Forensic Sci. Int. 139 (2004) 169–172. [6] H. Ikegaya H. Motani K. Sakurada K. Sato T. Akutsu M. Yoshino Forensic application of Epstein-Barr virus genotype: correlation between viral genotype and geographical area J. Virol. Methods 147 2008 78 85 10.1016/j.jviromet.2007.08.010 H. Ikegaya, H. Motani, K. Sakurada, K. Sato, T. Akutsu, M. Yoshino, Forensic application of Epstein-Barr virus genotype: Correlation between viral genotype and geographical area, J. Virol. Methods. 147 (2008) 78–85. [7] H. Inoue H. Motani-Saitoh K. Sakurada H. Ikegaya D. Yajima M. Hayakawa Y. Sato K. Otsuka K. Kobayashi S. Nagasawa H. Iwase Determination of the geographical origin of unidentified cadavers based on geographical differences in genotype of varicella-zoster virus J. Med. Virol. 82 2010 903 908 10.1002/jmv.21666 H. Inoue, H. Motani-Saitoh, K. Sakurada, H. Ikegaya, D. Yajima, M. Hayakawa, Y. Sato, K. Otsuka, K. Kobayashi, S. Nagasawa, H. Iwase, Determination of the geographical origin of unidentified cadavers based on geographical differences in genotype of varicella-zoster virus, J. Med. Virol. 82 (2010) 903–908. [8] D. Forni R. Cagliani M. Clerici U. Pozzoli M. Sironi You will never walk alone: codispersal of JC polyomavirus with human populations Mol. Biol. Evol. 37, 2019, 442–454 10.1093/molbev/msz227 D. Forni, R. Cagliani, M. Clerici, U. Pozzoli, M. Sironi, You Will Never Walk Alone: Codispersal of JC Polyomavirus with Human Populations, Mol. Biol. Evol. (2019). [9] A.W. Kolb C. Ané C.R. Brandt Using HSV-1 genome phylogenetics to track past human migrations PLoS One 8 2013 e76267 10.1371/journal.pone.0076267 A.W. Kolb, C. Ané, C.R. Brandt, Using HSV-1 Genome Phylogenetics to Track Past Human Migrations, PLoS One. 8 (2013). [10] C.D. Bowen H. Paavilainen D.W. Renner J. Palomäki J. Lehtinen T. Vuorinen P. Norberg V. Hukkanen M. Szpara HSV-1 strains circulating in Finland demonstrate uncoupling of geographic and phenotypic variation BioRxiv. 2018 1 46 H. Ikegaya, H.Y. Zheng, P.J. Saukko, L. Varesmaa-Korhonen, T. Hovi, T. Vesikari, H. Suganami, T. Takasaka, C. Sugimoto, Y. Ohasi, T. Kitamura, Y. Yogo, Genetic diversity of JC virus in the Saami and the Finns: Implications for their population history, Am. J. Phys. Anthropol. (2005). [11] H. Ikegaya H. Iwase Trial for the geographical identification using JC viral genotyping in Japan Forensic Sci. Int. 139 2004 169 172 10.1016/J.FORSCIINT.2003.10.019 C.D. Bowen, H. Paavilainen, D.W. Renner, J. Palomäki, J. Lehtinen, T. Vuorinen, P. Norberg, V. Hukkanen, M. Szpara, HSV-1 strains circulating in Finland demonstrate uncoupling of geographic and phenotypic variation, BioRxiv. (2018) 1–46. [12] H.T. Agostini R. Yanagihara V. Davis C.F. Ryschkewitsch G.L. Stoner Asian genotypes of JC virus in Native Americans and in a Pacific Island population: Markers of viral evolution and human migration Proc. Natl. Acad. Sci. U. S. A. 94 1997 14542 14546 10.1073/pnas.94.26.14542 H. Ikegaya, H. Iwase, Trial for the geographical identification using JC viral genotyping in Japan, Forensic Sci. Int. 139 (2004) 169–172. [13] C. Sugimoto T. Kitamura J. Guo M.N. Al-Ahdal S.N. Shchelkunov B. Otova P. Ondrejka J.Y. Chollet S. El-Safi M. Ettayebi G. Grésenguet T. Kocagöz S. Chaiyarasamee K.Z. Thant S. Thein K. Moe N. Kobayashi F. Taguchi Y. Yogo Typing of urinary JC virus DNA offers a novel means of tracing human migrations Proc. Natl. Acad. Sci. U. S. A 94 1997 9191 9196 H.T. Agostini, R. Yanagihara, V. Davis, C.F. Ryschkewitsch, G.L. Stoner, Asian genotypes of JC virus in Native Americans and in a Pacific Island population: Markers of viral evolution and human migration, Proc. Natl. Acad. Sci. U. S. A. 94 (1997) 14542–14546. [14] C. Sugimoto M. Hasegawa H.Y. Zheng V. Demenev Y. Sekino K. Kojima T. Honjo H. Kida T. Hovi T. Vesikari J.A. Schalken K. Tomita Y. Mitsunobu H. Ikegaya N. Kobayashi T. Kitamura Y. Yogo JC virus strains indigenous to northeastern Siberians and Canadian Inuits are unique but evolutionally related to those distributed throughout Europe and Mediterranean areas J. Mol. Evol. 55 2002 322 335 10.1007/s00239-001-2329-2 C. Sugimoto, T. Kitamura, J. Guo, M.N. Al-Ahdal, S.N. Shchelkunov, B. Otova, P. Ondrejka, J.Y. Chollet, S. El-Safi, M. Ettayebi, G. Grésenguet, T. Kocagöz, S. Chaiyarasamee, K.Z. Thant, S. Thein, K. Moe, N. Kobayashi, F. Taguchi, Y. Yogo, Typing of urinary JC virus DNA offers a novel means of tracing human migrations., Proc. Natl. Acad. Sci. U. S. A. 94 (1997) 9191–6. (accessed March 20, 2018). [15] L.A. Shackelton A. Rambaut O.G. Pybus E.C. Holmes JC virus evolution and its association with human populations J. Virol. 80 2006 9928 9933 10.1128/jvi.00441-06 C. Sugimoto, M. Hasegawa, H.Y. Zheng, V. Demenev, Y. Sekino, K. Kojima, T. Honjo, H. Kida, T. Hovi, T. Vesikari, J.A. Schalken, K. Tomita, Y. Mitsunobu, H. Ikegaya, N. Kobayashi, T. Kitamura, Y. Yogo, JC virus strains indigenous to northeastern Siberians and Canadian Inuits are unique but evolutionally related to those distributed throughout Europe and Mediterranean areas, J. Mol. Evol. 55 (2002) 322–335. [16] P.M. Sharp P. Simmonds Evaluating the evidence for virus/host co-evolution Curr. Opin. Virol. 1 2011 436 441 10.1016/j.coviro.2011.10.018 L.A. Shackelton, A. Rambaut, O.G. Pybus, E.C. Holmes, JC Virus Evolution and Its Association with Human Populations, J. Virol. 80 (2006) 9928–9933. [17] A. Kitchen M.M. Miyamoto C.J. Mulligan Utility of DNA viruses for studying human host history: case study of JC virus Mol. Phylogenet. Evol. 46, 2008, 673-682 10.1016/j.ympev.2007.09.005 P.M. Sharp, P. Simmonds, Evaluating the evidence for virus/host co-evolution, Curr. Opin. Virol. 1 (2011) 436–441. [18] S. Zhong H.-Y. Zheng M. Suzuki Q. Chen H. Ikegaya N. Aoki S. Usuku N. Kobayashi S. Nukuzuma Y. Yasuda N. Kuniyoshi Y. Yogo T. Kitamura Age-related urinary excretion of BK polyomavirus by nonimmunocompromised individuals J. Clin. Microbiol. 45 2007 193 198 10.1128/JCM.01645-06 A. Kitchen, M.M. Miyamoto, C.J. Mulligan, Utility of DNA viruses for studying human host history: Case study of JC virus, Mol. Phylogenet. Evol. (2008). [19] J.S. Smith N.J. Robinson Age-Specific Prevalence of Infection With Herpes Simplex Virus Types 2 and 1: a Global Review J. Inf. Dis. 186, 2002, S3–S28 S. Zhong, H.-Y. Zheng, M. Suzuki, Q. Chen, H. Ikegaya, N. Aoki, S. Usuku, N. Kobayashi, S. Nukuzuma, Y. Yasuda, N. Kuniyoshi, Y. Yogo, T. Kitamura, Age-related urinary excretion of BK polyomavirus by nonimmunocompromised individuals., J. Clin. Microbiol. 45 (2007) 193–8. [20] P. Norja K. Hokynar L.-M. Aaltonen R. Chen A. Ranki E.K. Partio O. Kiviluoto I. Davidkin T. Leivo A.M. Eis-Hübinger B. Schneider H.-P. Fischer R. Tolba O. Vapalahti A. Vaheri M. Söderlund-Venermo K. Hedman Bioportfolio: lifelong persistence of variant and prototypic erythrovirus DNA genomes in human tissue Proc. Natl. Acad. Sci. U. S. A. 103 2006 7450 7453 J.S. Smith, N.J. Robinson, Age-Specific Prevalence of Infection with Herpes Simplex Virus Types 2 and 1: A Global Review, 2002. [21] M. Toppinen M.F. Perdomo J.U. Palo P. Simmonds S.J. Lycett M. Söderlund-Venermo A. Sajantila K. Hedman Bones hold the key to DNA virus history and epidemiology Sci. Rep. 5 2015 17226 10.1038/srep17226 P. Norja, K. Hokynar, L.-M. Aaltonen, R. Chen, A. Ranki, E.K. Partio, O. Kiviluoto, I. Davidkin, T. Leivo, A.M. Eis-Hübinger, B. Schneider, H.-P. Fischer, R. Tolba, O. Vapalahti, A. Vaheri, M. Söderlund-Venermo, K. Hedman, Bioportfolio: Lifelong persistence of variant and prototypic erythrovirus DNA genomes in human tissue, Proc. Natl. Acad. Sci. U. S. A. 103 (2006) 7450–7453. (accessed June 8, 2018). [22] B. Mühlemann A. Margaryan P. de B. Damgaard M.E. Allentoft L. Vinner A.J. Hansen A. Weber V.I. Bazaliiskii M. Molak J. Arneborg W. Bogdanowicz C. Falys M. Sablin V. Smrčka S. Sten K. Tashbaeva N. Lynnerup M. Sikora D.J. Smith R.A.M. Fouchier C. Drosten K.-G. Sjögren K. Kristiansen E. Willerslev T.C. Jones Ancient human parvovirus B19 in Eurasia reveals its long-term association with humans Proc. Natl. Acad. Sci. U. S. A. 115 2018 7557 7562 10.1073/pnas.1804921115 M. Toppinen, M.F. Perdomo, J.U. Palo, P. Simmonds, S.J. Lycett, M. Söderlund-Venermo, A. Sajantila, & K. Hedman, Bones hold the key to DNA virus history and epidemiology, Sci. Rep. 5 (2015). [23] B. Mühlemann T.C. Jones P. De Barros Damgaard M.E. Allentoft I. Shevnina A. Logvin E. Usmanova I.P. Panyushkina B. Boldgiv T. Bazartseren K. Tashbaeva V. Merz N. Lau V. Smrčka D. Voyakin E. Kitov A. Epimakhov D. Pokutta M. Vicze T.D. Price V. Moiseyev A.J. Hansen L. Orlando S. Rasmussen M. Sikora L. Vinner A.D.M.E. Osterhaus D.J. Smith D. Glebe R.A.M. Fouchier C. Drosten K.G. Sjögren K. Kristiansen E. Willerslev Ancient hepatitis B viruses from the Bronze Age to the Medieval period Nature 557 2018 418 423 10.1038/s41586-018-0097-z B. Mühlemann, A. Margaryan, P. de B. Damgaard, M.E. Allentoft, L. Vinner, A.J. Hansen, A. Weber, V.I. Bazaliiskii, M. Molak, J. Arneborg, W. Bogdanowicz, C. Falys, M. Sablin, V. Smrčka, S. Sten, K. Tashbaeva, N. Lynnerup, M. Sikora, D.J. Smith, R.A.M. Fouchier, C. Drosten, K.-G. Sjögren, K. Kristiansen, E. Willerslev, T.C. Jones, Ancient human parvovirus B19 in Eurasia reveals its long-term association with humans., Proc. Natl. Acad. Sci. U. S. A. 115 (2018) 7557–7562. [24] B. Krause-Kyora J. Susat F.M. Key D. Kühnert E. Bosse A. Immel C. Rinne S.-C. Kornell D. Yepes S. Franzenburg H.O. Heyne T. Meier S. Lösch H. Meller S. Friederich N. Nicklisch K.W. Alt S. Schreiber A. Tholey A. Herbig A. Nebel J. Krause Neolithic and medieval virus genomes reveal complex evolution of hepatitis B Elife 7 2018 e36666 10.7554/eLife.36666 B. Mühlemann, T.C. Jones, P. De Barros Damgaard, M.E. Allentoft, I. Shevnina, A. Logvin, E. Usmanova, I.P. Panyushkina, B. Boldgiv, T. Bazartseren, K. Tashbaeva, V. Merz, N. Lau, V. Smrčka, D. Voyakin, E. Kitov, A. Epimakhov, D. Pokutta, M. Vicze, T.D. Price, V. Moiseyev, A.J. Hansen, L. Orlando, S. Rasmussen, M. Sikora, L. Vinner, A.D.M.E. Osterhaus, D.J. Smith, D. Glebe, R.A.M. Fouchier, C. Drosten, K.G. Sjögren, K. Kristiansen, E. Willerslev, Ancient hepatitis B viruses from the Bronze Age to the Medieval period, Nature. 557 (2018) 418–423. [25] S. Goh C. Lindau A. Tiveljung-Lindell T. Allander Merkel cell polyomavirus in respiratory tract secretions Emerg. Infectious Dis. 15 2009 489 491 10.3201/eid1503.081206 B. Krause-Kyora, J. Susat, F.M. Key, D. Kühnert, E. Bosse, A. Immel, C. Rinne, S.-C. Kornell, D. Yepes, S. Franzenburg, H.O. Heyne, T. Meier, S. Lösch, H. Meller, S. Friederich, N. Nicklisch, K.W. Alt, S. Schreiber, A. Tholey, A. Herbig, A. Nebel, J. Krause, Neolithic and medieval virus genomes reveal complex evolution of hepatitis B, Elife. 7 (2018). [26] A. Dumoulin H.H. Hirsch Reevaluating and optimizing polyomavirus BK and JC real-time PCR assays to detect rare sequence polymorphisms J. Clin. Microbiol. 49 2011 1382 1388 10.1128/JCM.02008-10 S. Goh, C. Lindau, A. Tiveljung-Lindell, T. Allander, Merkel cell polyomavirus in respiratory tract secretions., Emerg. IIfectious Dis. 15 (2009) 489–91. [27] N.G. Hoffman L. Cook E.E. Atienza A.P. Limaye K.R. Jerome Marked variability of BK virus load measurement using quantitative real-time PCR among commonly used assays J. Clin. Microbiol. 46 2008 2671 2680 10.1128/JCM.00258-08 A. Dumoulin, H.H. Hirsch, Reevaluating and optimizing polyomavirus BK and JC real-time PCR assays to detect rare sequence polymorphisms., J. Clin. Microbiol. 49 (2011) 1382–8. [28] M. Toppinen P. Norja L.-M. Aaltonen S. Wessberg L. Hedman M. Söderlund-Venermo K. Hedman A new quantitative PCR for human parvovirus B19 genotypes J. Virol. Methods 218 2015 40 45 10.1016/j.jviromet.2015.03.006 N.G. Hoffman, L. Cook, E.E. Atienza, A.P. Limaye, K.R. Jerome, Marked variability of BK virus load measurement using quantitative real-time PCR among commonly used assays., J. Clin. Microbiol. 46 (2008) 2671–80. [29] L. Pyöriä M. Jokinen M. Toppinen H. Salminen T. Vuorinen V. Hukkanen C. Schmotz E. Elbasani P.M. Ojala K. Hedman H. Välimaa M.F. Perdomo HERQ-9 Is a New Multiplex PCR for Differentiation and Quantification of All Nine Human Herpesviruses MSphere .. 5 2020 10.1128/mSphere.00265-20 M. Toppinen, P. Norja, L.-M. Aaltonen, S. Wessberg, L. Hedman, M. Söderlund-Venermo, K. Hedman, A new quantitative PCR for human parvovirus B19 genotypes, J. Virol. Methods. 218 (2015) 40–45. [30] M. Pistello A. Morrica F. Maggi M.L. Vatteroni G. Freer C. Fornai F. Casula S. Marchi P. Ciccorossi P. Rovero M. Bendinelli TT virus levels in the plasma of infected individuals with different hepatic and extrahepatic pathology J. Med. Virol. 63 2001 189 195 (accessed February 23, 2018) L. Pyöriä, M. Jokinen, M. Toppinen, H. Salminen, T. Vuorinen, V. Hukkanen, C. Schmotz, E. Elbasani, P.M. Ojala, K. Hedman, H. Välimaa, M.F. Perdomo, HERQ-9 Is a New Multiplex PCR for Differentiation and Quantification of All Nine Human Herpesviruses. MSphere. 5 (2020). 10.1128/mSphere.00265-20 [31] F. Maggi C. Fornai M.L. Vatteroni G. Siciliano F. Menichetti C. Tascini S. Specter M. Pistello M. Bendinelli Low prevalence of TT virus in the cerebrospinal fluid of viremic patients with central nervous system disorders J. Med. Virol. 65 2001 418 422 M. Pistello, A. Morrica, F. Maggi, M.L. Vatteroni, G. Freer, C. Fornai, F. Casula, S. Marchi, P. Ciccorossi, P. Rovero, M. Bendinelli, TT virus levels in the plasma of infected individuals with different hepatic and extrahepatic pathology., J. Med. Virol. 63 (2001) 189–95. (accessed February 23, 2018). [32] A.L. McNees Z.S. White P. Zanwar R.A. Vilchez J.S. Butel Specific and quantitative detection of human polyomaviruses BKV, JCV, and SV40 by real time PCR J. Clin. Virol. 34 2005 52 62 [pii] F. Maggi, C. Fornai, M.L. Vatteroni, G. Siciliano, F. Menichetti, C. Tascini, S. Specter, M. Pistello, M. Bendinelli, Low prevalence of TT virus in the cerebrospinal fluid of viremic patients with central nervous system disorders., J. Med. Virol. 65 (2001) 418–22. [33] D. Pratas M. Toppinen L. Pyöriä K. Hedman A. Sajantila M.F. Perdomo A hybrid pipeline for reconstruction and analysis of viral genomes at multi-organ level. Gigascience In press 2020 In press A.L. McNees, Z.S. White, P. Zanwar, R.A. Vilchez, J.S. Butel, Specific and quantitative detection of human polyomaviruses BKV, JCV, and SV40 by real time PCR, J. Clin. Virol. 34 (2005) 52–62. [pii]. [34] P.M. Lieberman Epigenetics and genetics of viral latency Cell Host Microbe 19 2016 619 628 10.1016/j.chom.2016.04.008 D. Pratas, M. Toppinen, L. Pyöriä, K. Hedman, A. Sajantila, M.F. Perdomo, A hybrid pipeline for reconstruction and analysis of viral genomes at multi-organ level. Gigascience. In press (2020) [35] S.B. Kutluay S.J. Triezenberg Role of chromatin during herpesvirus infections Biochim. Biophys. Acta - Gen. Subj. 1790 2009 456 466 10.1016/j.bbagen.2009.03.019 P.M. Lieberman, Epigenetics and Genetics of Viral Latency, Cell Host Microbe. 19 (2016) 619–628. [36] B.I. Milavetz, L. Balakrishnan, Viral Epigenetics, (n.d.). [37] D. Higgins A.B. Rohrlach J. Kaidonis G. Townsend J.J. Austin Differential nuclear and mitochondrial DNA preservation in post-mortem teeth with implications for forensic and ancient DNA studies PLoS One 10 2015 1 17 10.1371/journal.pone.0126935 B.I. Milavetz, L. Balakrishnan, Viral Epigenetics, (n.d.). [38] J.S. Pedersen E. Valen A.M. Velazquez B.J. Parker M. Rasmussen S. Lindgreen B. Lilje D.J. Tobin T.K. Kelly S. Vang R. Andersson P.A. Jones C.A. Hoover A. Tikhonov E. Prokhortchouk E.M. Rubin A. Sandelin M.T. Gilbert A. Krogh E. Willerslev L. Orlando Genome-wide nucleosome map and cytosine methylation levels of an ancient human genome Genome Res. 24 2014 454 466 10.1101/gr.163592.113 D. Higgins, A.B. Rohrlach, J. Kaidonis, G. Townsend, J.J. Austin, Differential nuclear and mitochondrial DNA preservation in post-mortem teeth with implications for forensic and ancient DNA studies, PLoS One. 10 (2015) 1–17. [39] L. Orlando M.T.P. Gilbert E. Willerslev Reconstructing ancient genomes and epigenomes Nat. Rev. Genet. 16 2015 395 408 10.1038/nrg3935 J.S. Pedersen, E. Valen, A.M. Velazquez, B.J. Parker, M. Rasmussen, S. Lindgreen, B. Lilje, D.J. Tobin, T.K. Kelly, S. Vang, R. Andersson, P.A. Jones, C.A. Hoover, A. Tikhonov, E. Prokhortchouk, E.M. Rubin, A. Sandelin, M.T. Gilbert, A. Krogh, E. Willerslev, L. Orlando, Genome-wide nucleosome map and cytosine methylation levels of an ancient human genome, Genome Res. 24 (2014) 454–466. [40] L. Pyöriä M. Toppinen E. Mäntylä L. Hedman L.-M. Aaltonen M. Vihinen-Ranta T. Ilmarinen M. Söderlund-Venermo K. Hedman M.F. Perdomo Extinct type of human parvovirus B19 persists in tonsillar B cells Nat. Commun. 8 2017 14930 10.1038/ncomms14930 L. Orlando, M.T.P. Gilbert, E. Willerslev, Reconstructing ancient genomes and epigenomes, Nat Rev Genet. 16 (2015) 395–408. [41] C. Berger M. Hug C. Gysin L. Molinari M. Frei W. Bossart D. Nadal Distribution patterns of β- and γ-herpesviruses within Waldeyer’s ring organs J. Med. Virol. 79 2007 1147 1152 10.1002/jmv.20899 L. Pyöriä, M. Toppinen, E. Mäntylä, L. Hedman, L.-M. Aaltonen, M. Vihinen-Ranta, T. Ilmarinen, M. Söderlund-Venermo, K. Hedman, M.F. Perdomo, Extinct type of human parvovirus B19 persists in tonsillar B cells, Nat. Commun. 8 (2017) 14930. [42] P. Randhawa R. Shapiro A. Vats Quantitation of DNA of polyomaviruses BK and JC in human kidneys J. Infect. Dis. 192 2005 504 509 10.1086/431522 C. Berger, M. Hug, C. Gysin, L. Molinari, M. Frei, W. Bossart, D. Nadal, Distribution patterns of β- and γ-herpesviruses within Waldeyer’s ring organs, J. Med. Virol. 79 (2007) 1147–1152. [43] S. Hino H. Miyata Torque teno virus (TTV): current status Rev. Med. Virol. 17 2007 45 57 10.1002/rmv.524 P. Randhawa, R. Shapiro, A. Vats, Quantitation of DNA of Polyomaviruses BK and JC in Human Kidneys, J. Infect. Dis. 192 (2005) 504–509. "
    },
    {
        "doc_title": "A hybrid pipeline for reconstruction and analysis of viral genomes at multi-organ level",
        "doc_scopus_id": "85089714610",
        "doc_doi": "10.1093/gigascience/giaa086",
        "doc_eid": "2-s2.0-85089714610",
        "doc_date": "2020-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Base Sequence",
            "Genome, Viral",
            "Genomics",
            "Humans",
            "Sequence Alignment",
            "Software"
        ],
        "doc_abstract": "© The Author(s) 2020.Background: Advances in sequencing technologies have enabled the characterization of multiple microbial and host genomes, opening new frontiers of knowledge while kindling novel applications and research perspectives. Among these is the investigation of the viral communities residing in the human body and their impact on health and disease. To this end, the study of samples from multiple tissues is critical, yet, the complexity of such analysis calls for a dedicated pipeline. We provide an automatic and efficient pipeline for identification, assembly, and analysis of viral genomes that combines the DNA sequence data from multiple organs. TRACESPipe relies on cooperation among 3 modalities: Compression-based prediction, sequence alignment, and de novo assembly. The pipeline is ultra-fast and provides, additionally, secure transmission and storage of sensitive data. Findings: TRACESPipe performed outstandingly when tested on synthetic and ex vivo datasets, identifying and reconstructing all the viral genomes, including those with high levels of single-nucleotide polymorphisms. It also detected minimal levels of genomic variation between different organs. Conclusions: TRACESPipe's unique ability to simultaneously process and analyze samples from different sources enables the evaluation of within-host variability. This opens up the possibility to investigate viral tissue tropism, evolution, fitness, and disease associations. Moreover, additional features such as DNA damage estimation and mitochondrial DNA reconstruction and analysis, as well as exogenous-source controls, expand the utility of this pipeline to other fields such as forensics and ancient DNA studies. TRACESPipe is released under GPLv3 and is available for free download at https://github.com/viromelab/tracespipe.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GTO: A toolkit to unify pipelines in genomic and proteomic research",
        "doc_scopus_id": "85086630979",
        "doc_doi": "10.1016/j.softx.2020.100535",
        "doc_eid": "2-s2.0-85086630979",
        "doc_date": "2020-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "C language",
            "Life-sciences",
            "Modular architectures",
            "Next-generation sequencing",
            "Proteomic"
        ],
        "doc_abstract": "© 2020 The AuthorsNext-generation sequencing triggered the production of a massive volume of publicly available data and the development of new specialised tools. These tools are dispersed over different frameworks, making the management and analyses of the data a challenging task. Additionally, new targeted tools are needed, given the dynamics and specificities of the field. We present GTO, a comprehensive toolkit designed to unify pipelines in genomic and proteomic research, which combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of the data. This toolkit combines novel tools with a modular architecture, being an excellent platform for experimental scientists, as well as a useful resource for teaching bioinformatics enquiry to students in life sciences. GTO is implemented in C language and is available, under the MIT license, at https://bioinformatics.ua.pt/gto.",
        "available": true,
        "clean_text": "serial JL 312019 291210 291690 291735 291791 291848 31 90 SoftwareX SOFTWAREX 2020-06-20 2020-06-20 2020-06-20 2020-06-20 2021-06-24T16:39:01 S2352-7110(20)30147-3 S2352711020301473 10.1016/j.softx.2020.100535 S300 S300.3 FULL-TEXT 2021-06-24T16:23:59.139745Z 0 0 20200701 20201231 2020 2020-06-20T23:10:09.350353Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid primabst pubtype ref 2352-7110 23527110 UNLIMITED NONE true 12 12 C Volume 12 21 100535 100535 100535 202007 202012 July–December 2020 2020-07-01 2020-12-31 2020 simple-article osp © 2020 The Authors. Published by Elsevier B.V. GTOATOOLKITUNIFYPIPELINESINGENOMICPROTEOMICRESEARCH ALMEIDA J 1 Motivation and significance 2 Software description 2.1 Software architecture 2.2 Software functionalities 2.2.1 Genomics 2.2.2 Proteomics 2.2.3 General purpose 2.2.4 External tools 3 Illustrative examples 3.1 Bi-directional complexity profiles 3.2 Rearrangements map generation 3.3 Viral metagenomic identification 4 Impact 5 Conclusions Acknowledgements References MARDIS 2017 213 E VANDERAUWERA 2013 G KESSNER 2008 2534 2536 D CHEN 2018 i884 i890 S LIU 2019 4560 4567 Y GRABOWSKI 2019 677 678 S MALYSA 2015 3122 3129 G LIU 2019 2066 2074 Y CHANDAK 2019 2674 2676 S PINHO 2013 e79922 A PINHO 2011 2024 2028 A 201119THEUROPEANSIGNALPROCESSINGCONFERENCE SYMBOLICNUMERICALCONVERSIONDNASEQUENCESUSINGFINITECONTEXTMODELS PRATAS 2019 137 145 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS GECO2OPTIMIZEDTOOLFORLOSSLESSCOMPRESSIONANALYSISDNASEQUENCES HOSSEINI 2019 68 76 M CARVALHO 2018 49 55 J HOSSEINI 2016 56 M AGUEROCHAPIN 2006 723 730 G PRATAS 2015 10203 D ZIELEZINSKI 2019 A BENCHMARKINGALIGNMENTFREESEQUENCECOMPARISONMETHODS FORSLUND 2019 469 504 S EVOLUTIONARYGENOMICS EVOLUTIONPROTEINDOMAINARCHITECTURES ROST 1999 85 94 B PRATAS 2018 D FALCONAMETHODINFERMETAGENOMICCOMPOSITIONANCIENTDNA PRATAS 2018 1177 1181 D 201826THEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO METAGENOMICCOMPOSITIONANALYSISSEDIMENTARYANCIENTDNAISLEWIGHT HUANG 2011 593 594 W DROOP 2016 1883 1884 A AFGAN 2018 W537 W544 E SHEN 2016 e0163962 W DEPRISTO 2011 491 M GOECKS 2010 R86 J BLANKENBERG 2010 1783 1785 D LIU 2017 3364 3372 Y OCHOA 2014 626 633 I DEOROWICZ 2015 11565 S PRATAS 2018 105 113 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS COMPRESSIONAMINOACIDSEQUENCES PRATAS 2014 40 D ESCALONA 2016 459 M ALMEIDAX2020X100535 ALMEIDAX2020X100535XJ Full 2020-06-03T16:10:38Z Author This is an open access article under the CC BY license. © 2020 The Authors. Published by Elsevier B.V. 2021-06-03T16:43:35.966Z Foundation for Science and Technology SFRH/BD/147837/2019 UIDB/00127/2020 FCT Fundação para a Ciência e a Tecnologia Centro 2020 program NETDIAMOND POCI-01-0145-FEDER-016385 This work has received support from the NETDIAMOND, Portugal project ( POCI-01-0145-FEDER-016385 ), co-funded by Centro 2020 program, Portugal 2020, European Union , and from the FCT - Foundation for Science and Technology, Portugal , in the context of the project UIDB/00127/2020. João Almeida is supported by FCT - Foundation for Science and Technology, Portugal (national funds), grant SFRH/BD/147837/2019 . item S2352-7110(20)30147-3 S2352711020301473 10.1016/j.softx.2020.100535 312019 2021-06-24T16:23:59.139745Z 2020-07-01 2020-12-31 UNLIMITED NONE true 1061501 MAIN 6 51343 849 656 IMAGE-WEB-PDF 1 gr3 21082 116 376 gr1 51221 263 470 gr4 24493 141 376 fx1001 4519 27 817 gr2 16382 83 470 gr3 6347 68 219 gr1 10351 123 219 gr4 7605 82 219 fx1001 777 7 219 gr2 3191 39 219 gr3 155334 516 1668 gr1 436164 1166 2083 gr4 160054 624 1667 fx1001 16238 73 2171 gr2 128419 370 2083 si1 667 SOFTX 100535 100535 S2352-7110(20)30147-3 10.1016/j.softx.2020.100535 The Authors Fig. 1 Bi-directional complexity profiles of four types of human Herpesvirus (HHV2, HHV3, HHV4 and HHV7) generated with GTO using the pipeline: gto_complexity_profile_regions.sh. Complexity values below one are highlighted with blue colour while the others with green. Bps stands for bits per symbol where lower values represent redundancy. The length is in Kb (Kilobases) and all profiles use the same scale. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Bi-directional complexity profiles of human titin protein generated with GTO using the pipeline: gto_proteins_complexity_profile_regions.sh. Complexity values below three are highlighted with a red colour while the others with blue. Bps stands for bits per symbol where lower values represent redundancy. The length is in Ks (Kilosymbols). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 3 Rearrangements map generated with GTO using the pipeline: gto_map_rearrangements.sh. The length of both sequences (A and B) is 5 MB. Wave pattern stands for inverted repeated regions. Fig. 4 Rearrangements map generated with GTO using the pipeline: gto_map_rearrangements_proteins.sh. Table 1 The eight most representative reference sequences according to the RS (Relative Similarity). ID stands for the order of the top output, length for the size of the reference genome, and GID for the sequence global identifier. ID Length RS (%) Reference GID Virus name 1 124884 97.767 X04370.1 HHV3 2 5596 96.603 AY386330.1 B19V 3 172764 94.143 DQ279927.1 HHV4 4 154675 81.400 JN561323.2 HHV2 5 154746 80.153 Z86099.2 HHV2 6 2785 78.300 AB041963.1 TTV 7 7372 71.445 MG921180.1 HPV 8 549 47.591 AY034056.1 PHV3-BALF1-gene Original software publication GTO: A toolkit to unify pipelines in genomic and proteomic research João R. Almeida a b ⁎ Armando J. Pinho a José L. Oliveira a Olga Fajarda a Diogo Pratas a c a Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro Aveiro Portugal Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal b Department of Information and Communications Technologies, University of A Coruña, A Coruña, Spain Department of Information and Communications Technologies, University of A Coruña, A Coruña Spain Department of Information and Communications Technologies, University of A Coruña, A Coruña, Spain c Department of Virology, University of Helsinki, Helsinki, Finland Department of Virology, University of Helsinki Helsinki Finland Department of Virology, University of Helsinki, Helsinki, Finland ⁎ Corresponding author at: Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal. Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro Aveiro Portugal Next-generation sequencing triggered the production of a massive volume of publicly available data and the development of new specialised tools. These tools are dispersed over different frameworks, making the management and analyses of the data a challenging task. Additionally, new targeted tools are needed, given the dynamics and specificities of the field. We present GTO, a comprehensive toolkit designed to unify pipelines in genomic and proteomic research, which combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of the data. This toolkit combines novel tools with a modular architecture, being an excellent platform for experimental scientists, as well as a useful resource for teaching bioinformatics enquiry to students in life sciences. GTO is implemented in C language and is available, under the MIT license, at Keywords Genomic Toolkit Proteomic Toolkit Next-generation sequencing Code metadata Current code version v1.5 Permanent link to code/repository used for this code version Legal Code License MIT Code versioning system used GIT Software code languages, tools, and services used C Compilation requirements, operating environments & dependencies GCC and Make If available Link to developer documentation/manual Support email for questions pratas@ua.pt Software metadata Current software version v1.5 Permanent link to executables of this version Legal Software License MIT Computing platforms/Operating Systems Linux and Unix-like Installation requirements & dependencies GCC and Make If available, link to user manual - if formally published include a reference to the publication in the reference list Support email for questions pratas@ua.pt 1 Motivation and significance Next-generation sequencing (NGS) has become an essential tool in genetic and genomic analysis with a substantial impact in the fields of biomedicine and anthropology. The advantages of NGS over traditional methods include its multiplex capability and analytical resolution, making it a time and cost-efficient approach for fast clinical and forensic screening [1]. The development of efficient bioinformatics tools is essential to assess and analyse the large volumes of sequencing data produced by next-generation sequencers. However, more important than that are the computational methods that unify the existing tools, given the notable pace at which these tools become available. Toolkits are sets of tools that combine multiple features in a custom-based manner as some examples show, both in genomics [2] and proteomics [3]. Developing a toolkit requires a specific architecture, namely, taking into account the purpose and technologies, accessibility, compatibility, portability, interoperability, and usability. Moreover, implementation needs to consider efficiency, while maintaining affordable computational resources and the absence of dependencies (standalone use). We contribute with GTO (Genomic Toolkit), a set of tools to unify pipelines operating both at genomic and proteomic levels, with an open licence and free of any dependency. This toolkit includes information theory-based tools for reference-free and reference-based data compression applied to data analysis. Among many applications, this toolkit supports the creation of workflows for identification of metagenomic composition in FASTQ reads, detection and visualisation of genomic rearrangements, mapping and visualisation of variation, localisation of low complexity regions, or simulation of sequences with specific SNP and structural variant rates. The toolkit was designed for Unix/Linux-based systems, built for ultra-fast computations. It supports pipes for easy integration with the sub-programmes as well as external tools. GTO works as LEGOsTM , since it allows the construction of multiple pipelines with many combinations. We support the toolkit with a detailed manual and a website with several examples, including an online manual for fast learning. Due to the variety and distribution of the given tools and their tight interconnection using the command line with pipes, the toolkit is an excellent platform for scientists as well as for empowering students to progress to the scientific aspects of bioinformatics analysis efficiently. Therefore, without the need to install multiple programmes, dependencies, and read different manuals or licences, it is possible to maintain an easy-to-follow connection with all the phases of each pipeline application. 2 Software description GTO is a powerful toolkit composed of more than 75 tools with particular focus on genomics and proteomics, following an integrative and flexible design between the tools. GTO includes tools for information display, randomisation, edition, conversion, extraction, search, calculation, compression, simulation and visualisation. The toolkit can be used in common Linux distributions. We have been using GTO in common personal computers (e.g. a laptop with 8 GB RAM, 128 GB of SSD and an intel-i3 CPU from the 5th generation), but these characteristics can vary according to the data size and the execution requirements. 2.1 Software architecture The tools composing this toolkit aim for key features such as being easy to use, compile and improve and specially designed for work in Unix/Linux command line. These tools can be used in isolation, or combined as one, forming execution workflows. This is technically possible due to the two streams used for the computation, namely the standard input and output. Furthermore, the tools’ aggregation is possible with mechanisms for inter-process communication using message passing, provided by the Unix operating system. This creates a chain of processes in which the output of each process is passed directly as input to the subsequent one, as shown in the following example: In addition to the input/output standard streams, some of the tools accept parameterisation through the definition of arguments when executed. There is also a small set of tools in which the input or output does not make sense to be the standard streams and for those the argument definition is considered. 2.2 Software functionalities The toolkit contains three main groups of tools according to its characteristic: Genomics, Proteomics, and General purpose. The genomics group is subdivided in: FASTQ, FASTA, SEQ (genomic sequences); while the proteomics contains AA (amino acid); the general-purpose tools can be applied to any format sequence. 2.2.1 Genomics The toolkit allows data conversion between different formats namely FASTQ, FASTA and SEQ. It also provides features for filtering and randomising DNA sequences, as well as for analytic purposes followed by simulations of a generation and alteration nature. The SEQ cluster works directly with the DNA sequences without any standard format. These tools allow data extraction, summary, classification and mathematical operations in the field of information theory. Among many examples, which are better described in the supporting website and manual, the toolkit allows preparations of the reads, namely filtering and trimming, the automatic construction of nucleotide reference databases, and comparative genomics. 2.2.2 Proteomics The toolkit has a specific cluster of tools designed to group, compress, and analyse amino acid sequences. These tools allow proteomic analysis based on the amino acids properties, such as electric charge (positive and negative), uncharged side chains, hydrophobic side chains and special cases. The toolkit allows translation of codons into amino acids, permits finding approximate amino acid sequences and performing comparative proteomics analysis. 2.2.3 General purpose This set of tools is complementary to the genomics and proteomics tools, not being designed to work in a specific field, but to assist the pipelines composed of the previously described subsets. These tools provide operations in the symbolical domain, including reversion, segmentation, and permutation; while in the numerical domain they contain tools with low-pass filters (with multiple window types), sum, min and max operations over streams. 2.2.4 External tools External top-performing tools have been integrated in order to increase the variety of functionalities available. The tools integrated are the following: • fastp [4]: enables ultra-fast preprocessing and quality control of FASTQ files. • bfMEM [5]: detects maximal exact matches between a pair of genomes based on bloom filters and rolling hashs. • copMEM [6]: another tool for computing maximal exact matches in a pair of genomes. • qvz [7]: implements a lossy compression algorithm for storing quality scores associated with DNA sequencing. • minicom [8]: a compressor for short reads in FASTQ files that uses large k-minimisers to index the reads. • SPRING [9]: which is reference-free compression tool for FASTQ files. 3 Illustrative examples All the tools in the toolkit were tested with synthetic sequences aiming for individual validation. Therefore, the documented examples are easily replicable with the written tests. Besides applying these tools in controlled environments, the toolkit was also used in several research workflows both as a primary and auxiliary tool. Several complete workflows are available in the repository, under the pipelines folder while an extensive description of the tool can be found in the manual. Next, we include some pipeline examples. 3.1 Bi-directional complexity profiles A workflow example is the computation of bi-directional complexity profiles in any genomic or proteomic sequence [10]. These profiles can localise specific features in the sequences, namely low and high complexity sequences, inverted repeats regions, tandem duplications, among others. The construction of these profiles follows a pipeline formed of many transformations (e.g. reversing, segmenting, inverting) as well as the use of specific low-pass filters after data compression applications [11]. Fig. 1 depicts the complexity profiles of four human Herpesvirus whole genomes using the same scale, where redundant regions are highlighted in blue (below a Bps of one). GTO uses GeCo2 [12] and AC [13] compressors to estimate the local complexity of DNA and amino acid sequences, respectively. However, GTO is not limited to using these data compressors. For example, new models can be tested under this framework, namely with extended alphabets [14]. In general, any data compressor able to output local estimations can be used in the pipeline as an alternative [15]. Analogous to the complexity profiles for DNA sequences, an example using amino acid sequences is given in Fig. 2. This example depicts a bi-directional complexity profile for the largest human protein sequence, titin. Several regions with low complexity are usually associated with specific characteristics, namely loops [16]. 3.2 Rearrangements map generation Another example workflow is in the domain of comparative genomics, namely to map and visualise rearrangements. This workflow is completely automatic from the input of the sequences to the generation of an SVG image, with the associated and transformed regions corresponding to the rearrangements. The pipeline applies smash technology [17,18] for mapping the rearrangements using an alignment-free methodology [19]. To prove the efficiency of the mapping pipeline, we use another pipeline to generate two identical FASTA files with simulated rearrangements between them (gto_simulate_rearragements.sh). After, loading the two FASTA files into the mapping pipeline (gto_map_rearrangements.sh), the output is two files, one with the mapping positions and the other is an SVG image depicting the mapped positions as can be seen in Fig. 3. All the rearrangements have been efficiently mapped with GTO according to the ground truth ( < 1 s of computational time). Analogous to the rearrangements map pipeline, for mapping at proteomic level, we consider the NAV2 HUMAN Neuron navigator 2 and the neuron navigator 2 isoform X15 of Macaca mulatta proteins. Although there are many examples under the proteome evolution [20], these are protein sequences considering identical scale [21]. Additionally, we shuffled the Macaca mulatta proteins using a block size of 300 amino acids. Fig. 4 depicts the proteins map after running the pipeline (gto_map_rearrangements_proteins.sh). Despite a low level of dissimilarity of the sequences with an additional pseudo-random permutation of blocks of 300 symbols, all the regions have been efficiently mapped with GTO ( < 1 s of computational time). 3.3 Viral metagenomic identification A final workflow example is the full automatic metagenomic identification of viral (or any other) content in FASTQ reads. This includes the filtering and trimming of the reads, mapping, and sensitive identification of the most representative genomes, under a ranking of abundance. In this particular example, we generate a semi-synthetic viral dataset containing several real viruses with applied degrees of substitutions and block permutations shuffled with synthetic noisy DNA. This dataset is generated using the gto_create_viral_dataset.sh pipeline. The intention is to perform a metagenomic analysis on this dataset without informing the programme what organisms are contained in the sample since the programme needs to infer the results. Then, we compare the results with the ground truth. If the results are similar to the ground truth, then the methodology is validated. For the purpose, GTO uses falcon-meta technology [22,23] that relies on assembly-free and alignment-free comparison of each reference according to the whole reads. The dataset contains synthetic reads (uniform distribution) merged with the following viruses with the respective modifications: • B19V: two Parvovirus, one with 1% of editions and the other with permuted blocks of 500 bases (GID: AY386330.1); • HHV2: one human Herpesvirus 2 with permuted blocks of size 100 bases (GID: JN561323.2); • HHV3: one human Herpesvirus 3 (GID: X04370.1); • HHV4: two human Herpesvirus 4, one with permuted blocks of 300 bases (GID: DQ279927.1); • TTV: one human Torque teno virus with 5% of editions (GID: AB041963.1); • HPV: one human Papillomavirus with 5% of editions and permuted blocks of 300 bases (GID: MG921180.1). After merging all FASTA sequences, ART [24] was used to generate the paired end FASTQ reads. Meanwhile, another workflow example was used to create the viral database (gto_build_dbs.sh). Then, the pipeline (gto_metagenomics.sh) ran, obtaining the top output presented in Table 1. We can conclude that despite the noise, editions, and permutations applied to real data, all the viruses have been efficiently identified with GTO, including the exact genotype ( < 1.5 min of computational time). 4 Impact Many software application exist to analyse and manipulate sequencing data, namely fqtools [25], GALAXY [26], FASTX-Toolkit [27], SeqKit [28], GATK [29], among others. The fqtools is a suite of tools to view, manipulate and summarise FASTQ data [25]. This software was designed to work specifically with FASTQ files and can be easily integrated into our toolkit. However, the features existent in this software are similar to some of the ones that GTO has in the gto_fastq_* section. Both were written in C which in terms of performance could be similar. GALAXY, is an open and web-based scientific platform for analysing genomic data [30]. This platform integrates several specialised sets of tools, e.g. for manipulating FASTQ files [31]. In this web application, the FASTX-Toolkit was integrated, which is a collection of command-line tools to process FASTA and FASTQ files [27]. The available features in the FASTX-Toolkit are also similar to some of the GTO tools designed to preprocess the FASTA/FASTQ files, which are available in the gto_fastq_* and gto_fasta_* sections. As our goal always was to have an easy to use toolkit written in low-level programming languages and not a web interface, we cannot compare it with GALAXY. However, regarding the FASTX-Toolkit which was also written in C, it is possible to compare and combine it with some of the GTO’s features. The SeqKit is another toolkit used to process FASTA and FASTQ files and it is available for all major operating systems [28]. Comparing the performance and limitations of this toolkit with the fqtools and FASTX-Toolkit is easier than comparing them with GTO, mainly because these three toolkits were designed specifically to manipulate FASTA/FASTQ files. On the other hand, these functionalities are only a fraction of the features that we provide in GTO. The idea never was to create more tools to compete with the ones existing, but instead, aggregate them in order to obtain a more complete toolkit for genomics analysis. This idea of simplifying the development and aggregation of analysis tools for genomic manipulation and analysis is not new. Initially designed as a structured programming framework, the Genome Analysis Toolkit (GATK) is a set of bioinformatics tools for analysing high-throughput sequencing focused on variant discovering and genotyping [2]. The high performance of this toolkit is due to the required infrastructures that a personal computer cannot offer. This is an excellent toolkit that integrates Apache Spark for optimisation, but it is only possible to take advantage of this potential in cloud computing. The efficient performance from some of the presented tools as well as GTO’s tools is due to the use of low-level programming languages (e.g. C language). However, one limitation of this strategy, in which the performance is prioritised, is the lack of a graphical user interface. Moreover, to take full advantage from those tools, the end-users need to have basic shell script knowledge. Nevertheless, GTO combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of data. Therefore, we would like to highlight some important details that characterise this toolkit: • The toolkit aggregates different tools in order to build research pipelines to deal with very large data sets without losing performance due to its modular architecture. Adoption of standard streams to interconnect the tools improved data processing. Throughout this procedure, the disk read/write operations between tools have been removed by sending the output directly to the input of the next tool. • The toolkit can integrate external tools, besides the ones already available. As such, some specific tools that have already been evaluated and used outside this context were aggregated: – For compression purposes, the toolkit integrates GeCo2 [12], which along with HiRGC [32], iDoComp [33] and GDC2 [34] are considered to have some of the best performance for reference-free DNA compression [35]. Regarding the amino acid sequences, the toolkit uses the AC tool for lossless sequence compression. The performance of AC was compared in [36] to several general-purpose lossless compressors and several protein compressors using different proteomes and AC provides on average the best bit-rates. – Concernings simulation, GTO integrates XS [37] which is a FASTQ read simulation tool. Escalona et al. [38] reviewed 23 NGS simulation tools and XS stands out in relation to the others because it is the only one that does not need a reference sequence. – Additionally, we added a section in the toolkit specially designed for tools from other authors. This way, we simplify their integration and installation using GTO. Those were described in Section 2.2.4. • Finally, as briefly presented in Section 3, the toolkit can answer new genomics questions without the need to create new software. 5 Conclusions We contribute with GTO, a toolkit to unify research pipelines, composed of distinct tools aiming at efficient combinations of them towards specific workflows. GTO’s efficient performance is due to the use of low-level programming languages, which increases the processing speed and decreases the RAM of addressing genomics and proteomics data. The flexibility of this toolkit allows the end-user to quickly create new processing pipelines in the genomic and proteomic field as it was described in the examples provided in this manuscript. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This work has received support from the NETDIAMOND, Portugal project (POCI-01-0145-FEDER-016385), co-funded by Centro 2020 program, Portugal 2020, European Union, and from the FCT - Foundation for Science and Technology, Portugal, in the context of the project UIDB/00127/2020. João Almeida is supported by FCT - Foundation for Science and Technology, Portugal (national funds), grant SFRH/BD/147837/2019. References [1] Mardis E.R. DNA sequencing technologies: 2006–2016 Nat Protoc 12 2 2017 213 E. R. Mardis, DNA sequencing technologies: 2006–2016, Nature Protocols 12 (2) (2017) 213. [2] Van der Auwera G.A. Carneiro M.O. Hartl C. Poplin R. Del Angel G. Levy-Moonshine A. From FASTQ data to high-confidence variant calls: the genome analysis toolkit best practices pipeline Curr Protoc Bioinform 43 1 2013 11.10.1-11.10.33 G. A. Van der Auwera, M. O. Carneiro, C. Hartl, R. Poplin, G. Del Angel, A. Levy-Moonshine, T. Jordan, K. Shakir, D. Roazen, J. Thibault, others,From FASTQ data to high-confidence variant calls: the genome analysis toolkit best practices pipeline, Current Protocols in Bioinformatics 43 (1) (2013) 11–10. [3] Kessner D. Chambers M. Burke R. Agus D. Mallick P. ProteoWizard: open source software for rapid proteomics tools development Bioinformatics 24 21 2008 2534 2536 D. Kessner, M. Chambers, R. Burke, D. Agus, P. Mallick, ProteoWizard: open source software for rapid proteomics tools development, Bioinformatics 24 (21) (2008) 2534–2536. [4] Chen S. Zhou Y. Chen Y. Gu J. Fastp: an ultra-fast all-in-one FASTQ preprocessor Bioinformatics 34 17 2018 i884 i890 S. Chen, Y. Zhou, Y. Chen, J. Gu, fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics 34 (17) (2018) i884–i890. [5] Liu Y. Zhang L.Y. Li J. Fast detection of maximal exact matches via fixed sampling of query K-mers and Bloom filtering of index K-mers Bioinformatics 35 22 2019 4560 4567 Y. Liu, L. Y. Zhang, J. Li, Fast detection of maximal exact matches via fixed sampling of query K-mers and Bloom filtering of index K-mers, Bioinformatics 35 (22) (2019) 4560–4567. [6] Grabowski S. Bieniecki W. CopMEM: finding maximal exact matches via sampling both genomes Bioinformatics 35 4 2019 677 678 S. Grabowski, W. Bieniecki, copMEM: finding maximal exact matches via sampling both genomes, Bioinformatics 35 (4) (2019) 677–678. [7] Malysa G. Hernaez M. Ochoa I. Rao M. Ganesan K. Weissman T. QVZ: lossy compression of quality values Bioinformatics 31 19 2015 3122 3129 G. Malysa, M. Hernaez, I. Ochoa, M. Rao, K. Ganesan, T. Weissman, QVZ: lossy compression of quality values, Bioinformatics 31 (19) (2015) 3122–3129. [8] Liu Y. Yu Z. Dinger M.E. Li J. Index suffix–prefix overlaps by (w, k)-minimizer to generate long contigs for reads compression Bioinformatics 35 12 2019 2066 2074 Y. Liu, Z. Yu, M. E. Dinger, J. Li, Index suffix–prefix overlaps by (w, k)-minimizer to generate long contigs for reads compression, Bioinformatics 35 (12) (2019) 2066–2074. [9] Chandak S. Tatwawadi K. Ochoa I. Hernaez M. Weissman T. SPRING: a next-generation compressor for FASTQ data Bioinformatics 35 15 2019 2674 2676 S. Chandak, K. Tatwawadi, I. Ochoa, M. Hernaez, T. Weissman, SPRING: a next-generation compressor for FASTQ data, Bioinformatics 35 (15) (2019) 2674–2676. [10] Pinho A.J. Garcia S.P. Pratas D. Ferreira P.J. DNA sequences at a glance PLoS One 8 11 2013 e79922 A. J. Pinho, S. P. Garcia, D. Pratas, P. J. Ferreira, DNA sequences at a glance, PloS one 8 (11) (2013) e79922. [11] Pinho A.J. Pratas D. Ferreira P.J. Garcia S.P. Symbolic to numerical conversion of DNA sequences using finite-context models 2011 19th European signal processing conference 2011 IEEE 2024 2028 A. J. Pinho, D. Pratas, P. J. Ferreira, S. P. Garcia, Symbolic to numerical conversion of DNA sequences using finite-context models, in: 2011 19th European Signal Processing Conference, IEEE, 2011, 2024–2028. [12] Pratas D. Hosseini M. Pinho A.J. GeCo2: an optimized tool for lossless compression and analysis of DNA sequences International conference on practical applications of computational biology & bioinformatics 2019 Springer 137 145 D. Pratas, M. Hosseini, A. J. Pinho, GeCo2: an optimized tool for lossless compression and analysis of DNA sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2019, 137–145. [13] Hosseini M. Pratas D. Pinho A.J. AC: a compression tool for amino acid sequences Interdiscip Sci: Comput Life Sci 11 1 2019 68 76 M. Hosseini, D. Pratas, A. J. Pinho, AC: a compression tool for amino acid sequences, Interdisciplinary Sciences: Computational Life Sciences 11 (1) (2019) 68–76. [14] Carvalho J.M. Brás S. Pratas D. Ferreira J. Soares S.C. Pinho A.J. Extended-alphabet finite-context models Pattern Recognit Lett 112 2018 49 55 J. M. Carvalho, S. Brás, D. Pratas, J. Ferreira, S. C. Soares, A. J. Pinho, Extended-alphabet finite-context models, Pattern Recognition Letters 112 (2018) 49–55. [15] Hosseini M. Pratas D. Pinho A. A survey on data compression methods for biological sequences Information 7 4 2016 56 M. Hosseini, D. Pratas, A. Pinho, A survey on data compression methods for biological sequences, Information 7 (4) (2016) 56. [16] Agüero-Chapin G. González-Díaz H. Molina R. Varona-Santos J. Uriarte E. González-Díaz Y. Novel 2D maps and coupling numbers for protein sequences. The first QSAR study of polygalacturonases; isolation and prediction of a novel sequence from Psidium guajava L. FEBS Lett 580 3 2006 723 730 G. Agüero-Chapin, H. González-Díaz, R. Molina, J. Varona-Santos, E. Uriarte, Y. González-Díaz, Novel 2D maps and coupling numbers for protein sequences. The first QSAR study of polygalacturonases isolation and prediction of a novel sequence from Psidium guajava L., FEBS Letters 580 (3) (2006) 723–730. [17] Pratas D. Silva R.M. Pinho A.J. Ferreira P.J. An alignment-free method to find and visualise rearrangements between pairs of DNA sequences Sci Rep 5 2015 10203 D. Pratas, R. M. Silva, A. J. Pinho, P. J. Ferreira, An alignment-free method to find and visualise rearrangements between pairs of DNA sequences, Scientific Reports 5 (2015) 10203. [18] Hosseini M, Pratas D, Morgenstern B, Pinho AJ. Smash++: an alignment-free and memory-efficient tool to find genomic rearrangements. GigaScience 9(5). [19] Zielezinski A. Girgis H.Z. Bernard G. Leimeister C.-A. Tang K. Dencker T. Benchmarking of alignment-free sequence comparison methods 2019 BioRxiv A. Zielezinski, H. Z. Girgis, G. Bernard, C.-A. Leimeister, K. Tang, T. Dencker, A. K. Lau, S. Röhling, J. Choi, M. S. Waterman, others,Benchmarking of alignment-free sequence comparison methods, BioRxiv (2019) 611137. [20] Forslund S.K. Kaduk M. Sonnhammer E.L. Evolution of protein domain architectures Evolutionary genomics 2019 Springer 469 504 S. K. Forslund, M. Kaduk, E. L. Sonnhammer, Evolution of protein domain architectures, in: Evolutionary Genomics, Springer, 2019, 469–504. [21] Rost B. Twilight zone of protein sequence alignments Protein Eng 12 2 1999 85 94 B. Rost, Twilight zone of protein sequence alignments, Protein engineering 12 (2) (1999) 85–94. [22] Pratas D. Pinho A.J. Silva R.M. Rodrigues J.M. Hosseini M. Caetano T. FALCON: a method to infer metagenomic composition of ancient DNA 2018 BioRxiv D. Pratas, A. J. Pinho, R. M. Silva, J. M. Rodrigues, M. Hosseini, T. Caetano, P. J. Ferreira, FALCON: a method to infer metagenomic composition of ancient DNA, BioRxiv (2018) 267179. [23] Pratas D. Pinho A.J. Metagenomic composition analysis of sedimentary ancient DNA from the isle of wight 2018 26th european signal processing conference (EUSIPCO) 2018 IEEE 1177 1181 D. Pratas, A. J. Pinho, Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight, in: 2018 26th European Signal Processing Conference (EUSIPCO), IEEE, 2018, 1177–1181. [24] Huang W. Li L. Myers J.R. Marth G.T. ART: a next-generation sequencing read simulator Bioinformatics 28 4 2011 593 594 W. Huang, L. Li, J. R. Myers, G. T. Marth, ART: a next-generation sequencing read simulator, Bioinformatics 28 (4) (2011) 593–594. [25] Droop A.P. Fqtools: an efficient software suite for modern FASTQ file manipulation Bioinformatics 32 12 2016 1883 1884 A. P. Droop, fqtools: an efficient software suite for modern FASTQ file manipulation, Bioinformatics 32 (12) (2016) 1883–1884. [26] Afgan E. Baker D. Batut B. Van Den Beek M. Bouvier D. Čech M. The galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update Nucleic Acids Res 46 W1 2018 W537 W544 E. Afgan, D. Baker, B. Batut, M. Van Den Beek, D. Bouvier, M. Čech, J. Chilton, D. Clements, N. Coraor, B. A. Grüning, others,The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update, Nucleic acids research 46 (W1) (2018) W537–W544. [27] Gordon A, Hannon G et al. Fastx-toolkit, FASTQ/A short-reads preprocessing tools Accessed: 2020-06-17. [28] Shen W. Le S. Li Y. Hu F. SeqKit: a cross-platform and ultrafast toolkit for FASTA/Q file manipulation PLoS One 11 10 2016 e0163962 W. Shen, S. Le, Y. Li, F. Hu, SeqKit: a cross-platform and ultrafast toolkit for FASTA/Q file manipulation, PLoS One 11 (10) (2016) e0163962. [29] DePristo M.A. Banks E. Poplin R. Garimella K.V. Maguire J.R. Hartl C. A framework for variation discovery and genotyping using next-generation DNA sequencing data Nature Genet 43 5 2011 491 M. A. DePristo, E. Banks, R. Poplin, K. V. Garimella, J. R. Maguire, C. Hartl, A. A. Philippakis, G. Del Angel, M. A. Rivas, M. Hanna, others,A framework for variation discovery and genotyping using next-generation DNA sequencing data, Nature Genetics 43 (5) (2011) 491. [30] Goecks J. Nekrutenko A. Taylor J. Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences Genome Biol 11 8 2010 R86 J. Goecks, A. Nekrutenko, J. Taylor, Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences, Genome Biology 11 (8) (2010) R86. [31] Blankenberg D. Gordon A. Von Kuster G. Coraor N. Taylor J. Nekrutenko A. Manipulation of FASTQ data with galaxy Bioinformatics 26 14 2010 1783 1785 D. Blankenberg, A. Gordon, G. Von Kuster, N. Coraor, J. Taylor, A. Nekrutenko, G. Team, Manipulation of FASTQ data with Galaxy, Bioinformatics 26 (14) (2010) 1783–1785. [32] Liu Y. Peng H. Wong L. Li J. High-speed and high-ratio referential genome compression Bioinformatics 33 21 2017 3364 3372 Y. Liu, H. Peng, L. Wong, J. Li, High-speed and high-ratio referential genome compression, Bioinformatics 33 (21) (2017) 3364–3372. [33] Ochoa I. Hernaez M. Weissman T. iDoComp: a compression scheme for assembled genomes Bioinformatics 31 5 2014 626 633 I. Ochoa, M. Hernaez, T. Weissman, iDoComp: a compression scheme for assembled genomes, Bioinformatics 31 (5) (2014) 626–633. [34] Deorowicz S. Danek A. Niemiec M. GDC 2: Compression of large collections of genomes Sci Rep 5 2015 11565 S. Deorowicz, A. Danek, M. Niemiec, GDC 2: Compression of large collections of genomes, Scientific Reports 5 (2015) 11565. [35] Hernaez M, Pavlichin D, Weissman T, Ochoa I. Genomic data compression. Annu Rev Biomed Data Sci 2. [36] Pratas D. Hosseini M. Pinho A.J. Compression of amino acid sequences International conference on practical applications of computational biology & bioinformatics 2018 Springer 105 113 D. Pratas, M. Hosseini, A. J. Pinho, Compression of amino acid sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2018, 105–113. [37] Pratas D. Pinho A.J. Rodrigues J.M. XS: a FASTQ read simulator BMC Res Notes 7 1 2014 40 D. Pratas, A. J. Pinho, J. M. Rodrigues, XS: a FASTQ read simulator, BMC Research Notes 7 (1) (2014) 40. [38] Escalona M. Rocha S. Posada D. A comparison of tools for the simulation of genomic next-generation sequencing data Nature Rev Genet 17 8 2016 459 M. Escalona, S. Rocha, D. Posada, A comparison of tools for the simulation of genomic next-generation sequencing data, Nature Reviews Genetics 17 (8) (2016) 459. "
    },
    {
        "doc_title": "Smash++: An alignment-free and memory-efficient tool to find genomic rearrangements",
        "doc_scopus_id": "85084965113",
        "doc_doi": "10.1093/gigascience/giaa048",
        "doc_eid": "2-s2.0-85084965113",
        "doc_date": "2020-05-20",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Computational Biology",
            "Gene Rearrangement",
            "Genome",
            "Genomics",
            "High-Throughput Nucleotide Sequencing",
            "Sequence Analysis, DNA",
            "Software"
        ],
        "doc_abstract": "© 2020 The Author(s) 2020. Published by Oxford University Press.Background: The development of high-throughput sequencing technologies and, as its result, the production of huge volumes of genomic data, has accelerated biological and medical research and discovery. Study on genomic rearrangements is crucial owing to their role in chromosomal evolution, genetic disorders, and cancer. Results: We present Smash++, an alignment-free and memory-efficient tool to find and visualize small- and large-scale genomic rearrangements between 2 DNA sequences. This computational solution extracts information contents of the 2 sequences, exploiting a data compression technique to find rearrangements. We also present Smash++ visualizer, a tool that allows the visualization of the detected rearrangements along with their self- and relative complexity, by generating an SVG (Scalable Vector Graphics) image. Conclusions: Tested on several synthetic and real DNA sequences from bacteria, fungi, Aves, and Mammalia, the proposed tool was able to accurately find genomic rearrangements. The detected regions were in accordance with previous studies, which took alignment-based approaches or performed FISH (fluorescence in situ hybridization) analysis. The maximum peak memory usage among all experiments was ∼1 GB, which makes Smash++ feasible to run on present-day standard computers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Statistical complexity analysis of turing machine tapes with fixed algorithmic complexity using the best-order markov model",
        "doc_scopus_id": "85078516231",
        "doc_doi": "10.3390/e22010105",
        "doc_eid": "2-s2.0-85078516231",
        "doc_date": "2020-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 by the authors.Sources that generate symbolic sequences with algorithmic nature may differ in statistical complexity because they create structures that follow algorithmic schemes, rather than generating symbols from a probabilistic function assuming independence. In the case of Turing machines, this means that machines with the same algorithmic complexity can create tapes with different statistical complexity. In this paper, we use a compression-based approach to measure global and local statistical complexity of specific Turing machine tapes with the same number of states and alphabet. Both measures are estimated using the best-order Markov model. For the global measure, we use the Normalized Compression (NC), while, for the local measures, we define and use normal and dynamic complexity profiles to quantify and localize lower and higher regions of statistical complexity. We assessed the validity of our methodology on synthetic and real genomic data showing that it is tolerant to increasing rates of editions and block permutations. Regarding the analysis of the tapes, we localize patterns of higher statistical complexity in two regions, for a different number of machine states. We show that these patterns are generated by a decrease of the tape's amplitude, given the setting of small rule cycles. Additionally, we performed a comparison with a measure that uses both algorithmic and statistical approaches (BDM) for analysis of the tapes. Naturally, BDM is efficient given the algorithmic nature of the tapes. However, for a higher number of states, BDM is progressively approximated by our methodology. Finally, we provide a simple algorithm to increase the statistical complexity of a Turing machine tape while retaining the same algorithmic complexity. We supply a publicly available implementation of the algorithm in C++ language under the GPLv3 license. All results can be reproduced in full with scripts provided at the repository.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GeCo2: An Optimized Tool for Lossless Compression and Analysis of DNA Sequences",
        "doc_scopus_id": "85068615307",
        "doc_doi": "10.1007/978-3-030-23873-5_17",
        "doc_eid": "2-s2.0-85068615307",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical applications",
            "Command line interface",
            "Computational resources",
            "DNA data compressions",
            "Genomic sequence",
            "Lossless compression",
            "Lossless data compression",
            "Mixture model"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.The development of efficient DNA data compression tools is fundamental for reducing the storage, given the increasing availability of DNA sequences. The importance is also reflected for analysis purposes, given the search for optimized and new tools for anthropological and biomedical applications. In this paper, we describe the characteristics and impact of the GeCo2 tool, an improved version of the GeCo tool. In the proposed tool, we enhanced the mixture of models, where each context model or tolerant context model has now a specific decay factor. Additionally, specific cache-hash sizes and the ability to run only a context model with inverted repeats was developed. A new command line interface, twelve new pre-computed levels, and several optimizations in the code were also included. The results show a compression improvement using less computational resources (RAM and processing time). This new version permits more flexibility for compression and analysis purposes, namely a higher ability of addressing different characteristics of the DNA sequences. The decompression is performed using symmetric computational resources (RAM and time). The GeCo2 is freely available, under GPLv3 license, at https://github.com/pratas/geco2.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visualization of Similar Primer and Adapter Sequences in Assembled Archaeal Genomes",
        "doc_scopus_id": "85068589420",
        "doc_doi": "10.1007/978-3-030-23873-5_16",
        "doc_eid": "2-s2.0-85068589420",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Approximate search",
            "Archaeal",
            "Genomics",
            "Primer-adapter sequences",
            "Similarity threshold",
            "Synthetic DNA"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.Primer and adapter sequences are synthetic DNA or RNA oligonucleotides used in the process of amplification and sequencing. In theory, while similar primer sequences can be present on assembled genomes, adapter sequences should be trimmed (filtered) and, hence, absent from assembled genomes. However, given ambiguity problems, inefficient parameterization of trimming tools, and others, uncommonly they can be found in assembled genomes, on an exact or approximate state. In this paper, we investigate the occurrence of exact and approximate primer-adapter subsequences in assembled and, specifically, in the whole archaeal genomes of the NCBI database. We present a new method that combines data compression with custom signal processing operations, namely filtering and segmentation, to localize and visualize these regions given a defined similarity threshold. The program is freely available, under GPLv3 license, at https://github.com/pratas/maple.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A reference-free lossless compression algorithm for DNA sequences using a competitive prediction of two classes of weighted models",
        "doc_scopus_id": "85075423993",
        "doc_doi": "10.3390/e21111074",
        "doc_eid": "2-s2.0-85075423993",
        "doc_date": "2019-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2019 by the authors.The development of efficient data compressors for DNA sequences is crucial not only for reducing the storage and the bandwidth for transmission, but also for analysis purposes. In particular, the development of improved compression models directly influences the outcome of anthropological and biomedical compression-based methods. In this paper, we describe a new lossless compressor with improved compression capabilities for DNA sequences representing different domains and kingdoms. The reference-free method uses a competitive prediction model to estimate, for each symbol, the best class of models to be used before applying arithmetic encoding. There are two classes of models: weighted context models (including substitutional tolerant context models) and weighted stochastic repeat models. Both classes of models use specific sub-programs to handle inverted repeats efficiently. The results show that the proposed method attains a higher compression ratio than state-of-the-art approaches, on a balanced and diverse benchmark, using a competitive level of computational resources. An efficient implementation of the method is publicly available, under the GPLv3 license.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A probabilistic method to find and visualize distinct regions in protein sequences",
        "doc_scopus_id": "85075624401",
        "doc_doi": "10.23919/EUSIPCO.2019.8902695",
        "doc_eid": "2-s2.0-85075624401",
        "doc_date": "2019-09-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Alignment-free",
            "Bloom filters",
            "Neanderthals",
            "Palaeoproteomics",
            "Relative uniqueness"
        ],
        "doc_abstract": "© 2019 IEEEStudies on identification of species-specific protein regions, i.e., unique or highly dissimilar regions with respect to close species, will lead us to understanding of evolutionary traits, which can be related to novel functionalities or diseases. In this paper, we propose an alignment-free method to find and visualize distinct regions between two collections of proteins. We applied the proposed method, FRUIT, on multiple synthetic and real datasets to analyze its behavior when different rates of substitutional mutation occur. Testing with different k-mer sizes showed that the higher the mutation rate, the higher the relative uniqueness. We also employed FRUIT to find and visualize distinct regions in modern human proteins relatively to the proteins of Altai, Sidron and Vindija Neanderthals. The results show that four of the most distinct proteins, named ataxin-8, 60S ribosomal protein L26, NADH-ubiquinone oxidoreductase chain 3 and cytochrome c oxidase subunit 2 are involved in SCA8, DBA11, LS and MT-C1D, and MT-C4D diseases, respectively. There is also Interferon-induced transmembrane protein 3, among others, which is part of the immune system. Besides, we report the most similar primate exomes to the found modern human one, in terms of identity, query cover and length of sequences. The reported results can give us insight to the evolution of proteomes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "AC: A Compression Tool for Amino Acid Sequences",
        "doc_scopus_id": "85062690507",
        "doc_doi": "10.1007/s12539-019-00322-1",
        "doc_eid": "2-s2.0-85062690507",
        "doc_date": "2019-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Amino Acid Sequence",
            "Data Compression",
            "High-Throughput Nucleotide Sequencing",
            "Markov Chains",
            "Software"
        ],
        "doc_abstract": "© 2019, International Association of Scientists in the Interdisciplinary Areas.Advancement of protein sequencing technologies has led to the production of a huge volume of data that needs to be stored and transmitted. This challenge can be tackled by compression. In this paper, we propose AC, a state-of-the-art method for lossless compression of amino acid sequences. The proposed method works based on the cooperation between finite-context models and substitutional tolerant Markov models. Compared to several general-purpose and specific-purpose protein compressors, AC provides the best bit-rates. This method can also compress the sequences nine times faster than its competitor, paq8l. In addition, employing AC, we analyze the compressibility of a large number of sequences from different domains. The results show that viruses are the most difficult sequences to be compressed. Archaea and bacteria are the second most difficult ones, and eukaryota are the easiest sequences to be compressed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cryfa: A secure encryption tool for genomic data",
        "doc_scopus_id": "85058744156",
        "doc_doi": "10.1093/bioinformatics/bty645",
        "doc_eid": "2-s2.0-85058744156",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Computational Biology",
            "Data Compression",
            "Genomics",
            "High-Throughput Nucleotide Sequencing",
            "Software"
        ],
        "doc_abstract": "© 2018 The Author(s). Published by Oxford University Press.The ever-increasing growth of high-throughput sequencing technologies has led to a great acceleration of medical and biological research and discovery. As these platforms advance, the amount of information for diverse genomes increases at unprecedented rates. Confidentiality, integrity and authenticity of such genomic information should be ensured due to its extremely sensitive nature. In this paper, we propose Cryfa, a fast secure encryption tool for genomic data, namely in Fasta, Fastq, VCF, SAM and BAM formats, which is also capable of reducing the storage size of Fasta and Fastq files. Cryfa uses advanced encryption standard (AES) encryption combined with a shuffling mechanism, which leads to a substantial enhancement of the security against low data complexity attacks. Compared to AES Crypt, a general-purpose encryption tool, Cryfa is an industry-oriented tool, which is able to provide confidentiality, integrity and authenticity of data at four times more speed; in addition, it can reduce the file sizes to 1/3. Due to the absence of a method similar to Cryfa, we have simulated its behavior with a combination of encryption and compression tools, for comparison purpose. For instance, our tool is nine times faster than its fastest competitor in Fasta files. Also, Cryfa has a very low memory usage (only a few megabytes), which makes it feasible to run on any computer. Availability and implementation Source codes and binaries are available, under GPLv3, at https://github.com/pratas/cryfa. Supplementary informationSupplementary dataare available at Bioinformatics online.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "NET-ASAR: A tool for DNA sequence search based on data compression",
        "doc_scopus_id": "85052975949",
        "doc_doi": "10.1007/978-3-319-98702-6_14",
        "doc_eid": "2-s2.0-85052975949",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Context models",
            "Data compression techniques",
            "Genetic information",
            "Measure of similarities",
            "Reference modeling",
            "Research and development",
            "Similarity",
            "Statistical modeling"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.The great increase in the amount of sequenced DNA has created a problem: the storage of the sequences. As such, data compression techniques, designed specifically to compress genetic information, is an important area of research and development. Likewise, the ability to search similar DNA sequences in relation to a larger sequence, such as a chromosome, has a really important role in the study of organisms and the possible connection between different species. This paper proposes NET-ASAR, a tool for DNA sequence search, based on data compression, or, specifically, finite-context models, by obtaining a measure of similarity between a reference and a target. The method uses an approach based on finite-context models for the creation of a statistical model of the reference sequence and obtaining the estimated number of bits necessary for the encoding of the target sequence, using the reference model. NET-ASAR is freely available, under license GPLv3, at https://github.com/manuelgaspar/NET-ASAR.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A DNA sequence corpus for compression benchmark",
        "doc_scopus_id": "85052935792",
        "doc_doi": "10.1007/978-3-319-98702-6_25",
        "doc_eid": "2-s2.0-85052935792",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Compression algorithms",
            "Computational model",
            "Computational resources",
            "Large volumes",
            "Sequence compression"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.The progress in sequencing technologies and the increasing availability of DNA sequences from extant and extinct organisms is shaping our knowledge about species origin and development, as well as originating an improvement of the computational methods for storage and analysis purposes. Given the large volume of DNA sequences, computational models that efficiently represent diverse DNA sequences using low computational resources are very welcome. Currently, for benchmarking compression algorithms there is absence of a standard corpus that enables a wide and fair comparison. This should be a corpus that reflects the main domains and kingdoms, without being exaggerated in size and number of sequences. In this paper, we provide such DNA sequence corpus, overviewing its elements and furnishing a comparison of some of the algorithms for DNA sequence compression. The corpus is available at https://tinyurl.com/DNAcorpus.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compression of amino acid sequences",
        "doc_scopus_id": "85052918694",
        "doc_doi": "10.1007/978-3-319-98702-6_13",
        "doc_eid": "2-s2.0-85052918694",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Amino acid sequence",
            "Computational resources",
            "Context models",
            "Lossless",
            "Lossless compression",
            "Multiple contexts"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2019.Amino acid sequences are known to be very hard to compress. In this paper, we propose a lossless compressor for efficient compression of amino acid sequences (AC). The compressor uses a cooperation between multiple context and substitutional tolerant context models. The cooperation between models is balanced with weights that benefit the models with better performance, according to a forgetting function specific for each model. We have shown consistently better compression results than other approaches, using low computational resources. The compressor implementation is freely available, under license GPLv3, at https://github.com/pratas/ac.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight",
        "doc_scopus_id": "85059813385",
        "doc_doi": "10.23919/EUSIPCO.2018.8553297",
        "doc_eid": "2-s2.0-85059813385",
        "doc_date": "2018-11-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Ancient dnas",
            "Co-evolution",
            "Composition analysis",
            "Isle of Wight",
            "Metagenomics",
            "New zealand",
            "United kingdom",
            "Whole genome sequences"
        ],
        "doc_abstract": "© EURASIP 2018.The DNA from several organisms is sequenced conjointly in metagenomics. This allows searching for exogenous microorganisms contained in the samples, with the goal of studying the evolution and co-evolution of host-pathogen, namely for building better diagnostics and therapeutics. However, the quantity and quality of the DNA present in the samples is very poor, pushing the responsibility of analysis improvements into the development of better computational methods. Here, we develop a new processing paradigm to infer the metagenomic composition analysis based on the relative compression of whole genome sequences. Using this method, we present the metagenomic composition analysis of a sedimentary ancient DNA sample, dated to 8,000 years before the present, from the Isle of Wight, United Kingdom. The results show several viruses and bacteria expressing high levels of similarity relative to the samples, namely a circular virus similar to the Avon-Heathcote estuary virus 14 sequenced in New Zealand.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Metagenomic composition analysis of an ancient sequenced polar bear jawbone from Svalbard",
        "doc_scopus_id": "85053281877",
        "doc_doi": "10.3390/genes9090445",
        "doc_eid": "2-s2.0-85053281877",
        "doc_date": "2018-09-06",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Genetics (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2716"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 by the authors. Licensee MDPI, Basel, Switzerland.The sequencing of ancient DNA samples provides a novel way to find, characterize, and distinguish exogenous genomes of endogenous targets. After sequencing, computational composition analysis enables filtering of undesired sources in the focal organism, with the purpose of improving the quality of assemblies and subsequent data analysis. More importantly, such analysis allows extinct and extant species to be identified without requiring a specific or new sequencing run. However, the identification of exogenous organisms is a complex task, given the nature and degradation of the samples, and the evident necessity of using efficient computational tools, which rely on algorithms that are both fast and highly sensitive. In this work, we relied on a fast and highly sensitive tool, FALCON-meta, which measures similarity against whole-genome reference databases, to analyse the metagenomic composition of an ancient polar bear (Ursus maritimus) jawbone fossil. The fossil was collected in Svalbard, Norway, and has an estimated age of 110,000 to 130,000 years. The FASTQ samples contained 349 GB of nonamplified shotgun sequencing data. We identified and localized, relative to the FASTQ samples, the genomes with significant similarities to reference microbial genomes, including those of viruses, bacteria, and archaea, and to fungal, mitochondrial, and plastidial sequences. Among other striking features, we found significant similarities between modern-human, some bacterial and viral sequences (contamination) and the organelle sequences of wild carrot and tomato relative to the whole samples. For each exogenous candidate, we ran a damage pattern analysis, which in addition to revealing shallow levels of damage in the plant candidates, identified the source as contamination.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extended-alphabet finite-context models",
        "doc_scopus_id": "85048212515",
        "doc_doi": "10.1016/j.patrec.2018.05.026",
        "doc_eid": "2-s2.0-85048212515",
        "doc_date": "2018-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Biometric identifications",
            "Computational time",
            "Context modeling",
            "Context models",
            "Dissimilarity measures",
            "Kolmogorov complexity",
            "Probability of occurrence",
            "Sample applications"
        ],
        "doc_abstract": "© 2018 Elsevier B.V.The normalized relative compression (NRC) is a recent dissimilarity measure, related to the Kolmogorov complexity. It has been successfully used in different applications, like DNA sequences, images or even ECG (electrocardiographic) signal. It uses a compressor that compresses a target string using exclusively the information contained in a reference string. One possible approach is to use finite-context models (FCMs) to represent the strings. A finite-context model calculates the probability distribution of the next symbol, given the previous k symbols. In this paper, we introduce a generalization of the FCMs, called extended-alphabet finite-context models (xaFCM), that calculates the probability of occurrence of the next d symbols, given the previous k symbols. We perform experiments on two different sample applications using the xaFCMs and the NRC measure: ECG biometric identification, using a publicly available database; estimation of the similarity between DNA sequences of two different, but related, species – chromosome by chromosome. In both applications, we compare the results against those obtained by the FCMs. The results show that the xaFCMs use less memory and computational time to achieve the same or, in some cases, even more accurate results.",
        "available": true,
        "clean_text": "serial JL 271524 291210 291718 291872 291874 31 Pattern Recognition Letters PATTERNRECOGNITIONLETTERS 2018-06-01 2018-06-01 2018-06-14 2018-06-14 2020-11-09T08:44:07 S0167-8655(18)30209-5 S0167865518302095 10.1016/j.patrec.2018.05.026 S300 S300.2 FULL-TEXT 2020-11-09T09:14:08.408487Z 0 0 20180901 2018 2018-06-01T09:39:47.171082Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authlast footnotes grantnumber grantsponsor grantsponsorid highlightsabst orcid primabst ref 0167-8655 01678655 true 112 112 C Volume 112 8 49 55 49 55 20180901 1 September 2018 2018-09-01 2018 article sco © 2018 Elsevier B.V. All rights reserved. EXTENDEDALPHABETFINITECONTEXTMODELS CARVALHO J 1 Introduction 1.1 Compression-based measures 2 Extended-alphabet finite-context models 2.1 Compressing using extended-alphabet finite-context models 2.1.1 Example 2.2 Parameter selection 2.2.1 Selection of α 2.2.2 Selection of d 3 Application 1 – ECG biometric identification 3.1 R-peak detection 3.2 Quantization 3.3 Experimental results 4 Application 2 – DNA sequence relative similarity 5 Conclusions and future work Acknowledgments References PINHO 2011 A 19THEURSIGNALPROCESSCONFEUSIPCO2011 FINDINGUNKNOWNREPEATEDPATTERNSINIMAGES PRATAS 2014 421 422 D 2014DATACOMPRESSIONCONFERENCE ACONDITIONALCOMPRESSIONDISTANCEUNVEILSINSIGHTSGENOMICEVOLUTION COUTINHO 2015 D PINHO 2016 A DATACOMPRESSIONCONFERENCE AUTHORSHIPATTRIBUTIONUSINGCOMPRESSIONDISTANCES PRATAS 2018 D LI 2004 3250 3264 M LI 1997 M INTRODUCTIONKOLMOGOROVCOMPLEXITYAPPLICATIONS BENNETT 1998 1407 1423 C BRAS 2015 5838 5841 S 201537THANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBC ECGBIOMETRICIDENTIFICATIONACOMPRESSIONBASEDAPPROACH CARVALHO 2017 169 176 J PROCEEDINGSPATTERNRECOGNITIONTIMEANALYSIS8THIBERIANCONFERENCEIBPRIA IMPACTACQUISITIONTIMEECGCOMPRESSIONBASEDBIOMETRICIDENTIFICATIONSYSTEMS PINHO 2011 A 18THIEEEINTCONFERIMAGEPROCESS IMAGESIMILARITYUSINGNORMALIZEDCOMPRESSIONDISTANCEBASEDFINITECONTEXTMODELS COUTINHO 2013 D COUTINHO 2010 3858 3861 D PATTERNRECOGNITICPR20THINTCONF ONELEADECGBASEDPERSONALIDENTIFICATIONUSINGZIVMERHAVCROSSPARSING CARVALHO 2016 75 76 J PROCEEDINGS22NDRECPAD IRREGULARITYDETECTIONINECGSIGNALUSINGASEMIFIDUCIALMETHOD GROSSI 2016 12 R KATHIRVEL 2011 408 425 P LIN 2003 2 11 J DMKD03PROCEEDINGS8THACMSIGMODWORKSHOPRESEARCHISSUESINDATAMININGKNOWLEDGEDISCOVERY ASYMBOLICREPRESENTATIONTIMESERIESIMPLICATIONSFORSTREAMINGALGORITHMS RAFAEL 2007 G DIGITALIMAGEPROCESSING SAMPLINGQUANTIZATION FERREIRA 2016 J PINHO 2011 125 128 A IEEEWORKSHOPSTATISTICALSIGNALPROCESSINGPROCEEDINGS BACTERIADNASEQUENCECOMPRESSIONUSINGAMIXTUREFINITECONTEXTMODELS PRATAS 2014 D EUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO2014 EXPLORINGDEEPMARKOVMODELSINGENOMICDATACOMPRESSIONUSINGSEQUENCEPREANALYSIS PRATAS 2016 D DATACOMPRESSIONCONFERENCE EFFICIENTCOMPRESSIONGENOMICSEQUENCES HOBOLTH 2011 349 356 A PINHO 2014 30 37 A CARVALHOX2018X49 CARVALHOX2018X49X55 CARVALHOX2018X49XJ CARVALHOX2018X49X55XJ 2020-06-14T00:00:00.000Z © 2018 Elsevier B.V. All rights reserved. 2020-02-24T09:05:16.874Z S0167865518302095 Foundation for Science and Technology FCT FundaÃ§Ã£o para a CiÃªncia e a Tecnologia COMPETE 2020 PTDC/EEI-SII/6608/2014 UID/CEC/00127/2013 FEDER FEDER European Regional Development Fund FCT FCT FundaÃ§Ã£o para a CiÃªncia e a Tecnologia This work was partially supported by national funds through the FCT - Foundation for Science and Technology , and by European funds through FEDER, under the COMPETE 2020 and Portugal 2020 programs, in the context of the projects UID/CEC/00127/2013 and PTDC/EEI-SII/6608/2014. S. Brás acknowledges the Postdoc Grant from FCT, ref. SFRH/BPD/92342/2013 . item S0167-8655(18)30209-5 S0167865518302095 10.1016/j.patrec.2018.05.026 271524 2020-11-09T09:14:08.408487Z 2018-09-01 true 1617585 MAIN 7 53166 849 656 IMAGE-WEB-PDF 1 gr1 12685 237 244 gr2 53064 434 814 gr3 6149 222 244 gr4 11385 223 244 gr5 67365 321 702 gr6 34775 183 792 gr1 4777 163 168 gr2 5447 117 219 gr3 3263 164 180 gr4 4840 163 179 gr5 15146 100 219 gr6 7193 50 219 gr1 89749 1052 1082 gr2 416247 1921 3604 gr3 45514 986 1082 gr4 78226 987 1081 gr5 466359 1423 3108 gr6 351210 808 3505 si1 243 13 55 si10 1188 44 203 si11 540 18 156 si12 541 19 176 si13 626 24 119 si14 1547 48 270 si15 833 22 253 si16 162 15 17 si17 479 19 92 si18 296 17 60 si19 281 19 50 si2 260 16 59 si20 1069 43 225 si21 975 24 214 si22 614 15 197 si23 1070 56 168 si24 577 16 106 si25 221 16 51 si26 174 17 20 si27 216 13 45 si28 330 16 76 si29 141 15 24 si3 236 14 46 si30 2360 45 430 si31 2418 66 357 si32 1866 42 418 si33 570 16 136 si34 245 16 51 si35 1487 16 373 si36 1940 42 449 si37 1629 42 339 si38 545 16 126 si39 1101 27 233 si4 645 21 155 si40 1072 27 232 si41 422 19 121 si42 239 17 40 si43 616 18 144 si44 599 14 193 si45 641 20 157 si46 1112 43 193 si47 515 16 144 si48 1304 43 318 si49 204 17 24 si5 202 14 46 si50 745 43 145 si51 234 14 55 si52 127 13 23 si53 242 16 56 si6 229 14 47 si7 423 16 98 si8 951 43 176 si9 552 18 160 PATREC 7192 S0167-8655(18)30209-5 10.1016/j.patrec.2018.05.026 Elsevier B.V. Fig. 1 FCM biometry process: context k changing; the blue line represents the biometry accuracy (in %) and the red line represents the time of execution (seconds). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 1 Fig. 2 xaFCM biometry process using high contexts k (fixed); depth d is changing from 1 to 11; the blue line represents the biometry accuracy (in %); the red line represents the time (seconds); the green line represents the accuracy that a FCM with the same context would obtain. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Fig. 3 FCM average model complexity per participant - context k changing. Fig. 3 Fig. 4 xaFCM average model complexity per participant - context d is changing; k is fixed. Fig. 4 Fig. 5 Normalized compression of the chimpanzee (C) chromosomes relatively to the human (H), using: (left) FCM ( k = 12 ); (right) xaFCM ( k = 12 , d = 8 ). Fig. 5 Fig. 6 Profiles of information content of the chimpanzee chromosome 22 relatively to the human chromosome 22 using different models (FCM and xaFCM). Fig. 6 Table 1 FCM representation of the sequence AAABCC. Table 1 Context c v(A|c) v(B|c) v(C|c) v ( c ) = ∑ a ∈ A v ( a | c ) BC 0 0 1 1 CA 1 0 0 1 AB 0 0 1 1 CC 1 0 0 1 AA 1 1 0 2 Table 2 Proposed xaFCM representation of the sequence AAABCC (with d = 1 ). Notice that this model has exactly the same information as the one in Table 1. Table 2 Context c BC C: 1 Total: 1 CA A: 1 Total: 1 AB C: 1 Total: 1 CC A: 1 Total: 1 AA A: 1 B: 1 Total: 2 Table 3 Proposed xaFCM representation of the sequence AAABCC (with d = 2 ). Table 3 Context c BC CA: 1 Total: 1 CA AA: 1 Total: 1 AB CC: 1 Total: 1 CC AA: 1 Total: 1 AA AB: 1 BC: 1 Total: 2 Table 4 CPU time and memory usage (RAM) of the experiments with DNA sequences. Table 4 Parameters Average time to Average time to Average memory Total time to (context k and depth d) learn the Model compress per model run the experiment k = 12 , d = 1 1649.6 s 1580.5 s 5043.2MB 274.4 h k = 12 , d = 8 2181.2 s 269.5 s 14350.3MB 59.5 h Extended-alphabet finite-context models João M. Carvalho ⁎ a Susana Brás a b Diogo Pratas a Jacqueline Ferreira c d Sandra C. Soares c e f Armando J. Pinho a b a Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Portugal b Department of Electronics Telecommunications and Informatics of Aveiro, University of Aveiro, Portugal Department of Electronics Telecommunications and Informatics of Aveiro University of Aveiro Portugal c Department of Education and Psychology, University of Aveiro, Portugal Department of Education and Psychology University of Aveiro Portugal d IBILI, Faculty of Medicine, University of Coimbra, Portugal IBILI, Faculty of Medicine University of Coimbra Portugal e CINTESIS-UA, University of Aveiro, Portugal CINTESIS-UA University of Aveiro Portugal f Department of Clinical Neurosciences, Karolinska Institute, Stockholm, Sweden Department of Clinical Neurosciences Karolinska Institute Stockholm Sweden ⁎ Corresponding author. The normalized relative compression (NRC) is a recent dissimilarity measure, related to the Kolmogorov complexity. It has been successfully used in different applications, like DNA sequences, images or even ECG (electrocardiographic) signal. It uses a compressor that compresses a target string using exclusively the information contained in a reference string. One possible approach is to use finite-context models (FCMs) to represent the strings. A finite-context model calculates the probability distribution of the next symbol, given the previous k symbols. In this paper, we introduce a generalization of the FCMs, called extended-alphabet finite-context models (xaFCM), that calculates the probability of occurrence of the next d symbols, given the previous k symbols. We perform experiments on two different sample applications using the xaFCMs and the NRC measure: ECG biometric identification, using a publicly available database; estimation of the similarity between DNA sequences of two different, but related, species – chromosome by chromosome. In both applications, we compare the results against those obtained by the FCMs. The results show that the xaFCMs use less memory and computational time to achieve the same or, in some cases, even more accurate results. 1 Introduction Data compression models have been used to address several data mining and machine learning problems, usually by means of a formalization in terms of the information content of a string or of the information distance between strings [1–5]. This approach relies on solid foundations of the concept of algorithmic entropy and, because of its non-computability, approximations provided by data compression algorithms [6]. A finite-context model (FCM) calculates the probability distribution of the next symbol, given the previous k symbols. In this work, we propose an extension of the FCMs, which we call extended-alphabet finite-context models (xaFCM). Usually, these models provide better compression ratios, leading to better results for some applications, especially when using small alphabet sizes – and also by performing much less computations. We show this in practice for the ECG biometric identification and DNA sequence similarity. The source code for the compressor was implemented using Python 3.5 and is publicly available under the GPL v3 license. 1 1 1.1 Compression-based measures Compression-based distances are tightly related to the Kolmogorov notion of complexity, also known as algorithmic entropy. Let x denote a binary string of finite length. Its Kolmogorov complexity, K(x), is the length of the shortest binary program x* that computes x in a universal Turing machine and halts. Therefore, K ( x ) = | x * | , the length of x*, represents the minimum number of bits from which x can be computationally retrieved [7]. The Information Distance (ID) and its normalized version, the Normalized Information Distance (NID), were proposed by Bennett et al. almost two decades ago [8] and are defined in terms of the Kolmogorov complexity of the strings involved, as well as the complexity of one when the other is provided. However, since the Kolmogorov complexity of a string is not computable, an approximation (upper bound) for it can be used by means of a compressor. Let C(x) be the number of bits used by a compressor to represent the string x. We will use a measure based on the notion of relative compression [4], denoted by C(x||y), which represents the compression of x relatively to y. This measure obeys the following rules: • C(x||y) ≈ 0 iff string x can be built efficiently from y; • C(x||y) ≈ |x| iff K(x|y) ≈ K(x). Based on these rules, the Normalized Relative Compression (NRC) of the binary string x given the binary string y, is defined as (1) NRC ( x ∥ y ) = C ( x ∥ y ) | x | , where |x| denotes the length of x. A more general formula for the NRC of string x, given string y, where the strings x and y are sequences from an alphabet A = { s 1 , s 2 , ⋯ s | A | } , is given by (2) NRC ( x ∥ y ) = C ( x ∥ y ) | x | log 2 | A | . 2 Extended-alphabet finite-context models 2.1 Compressing using extended-alphabet finite-context models Let A = { s 1 , s 2 , ⋯ s | A | } be the alphabet that describes the objects of interest. An extended-alphabet finite-context model (xaFCM) complies to the Markov property, i.e., it estimates the probability of the next sequence of d > 0 symbols of the information source (depth-d) using the k > 0 immediate past symbols (order-k context). Therefore, assuming that the k past outcomes are given by x n − k + 1 n = x n − k + 1 … x n , the probability estimates, P ( x n + 1 n + d | x n − k + 1 n ) are calculated using sequence counts that are accumulated, while the information source is processed, (3) P ( w | x n − k + 1 n ) = v ( w | x n − k + 1 n ) + α v ( x n − k + 1 n ) + α | A | d where A d = { w 1 , w 2 , ⋯ w | A | , … w | A | d } is an extension of alphabet A to d dimensions, v ( w | x n − k + 1 n ) represents the number of times that, in the past, sequence w ∈ A d was found having x n − k + 1 n as the conditioning context and where (4) v ( x n − k + 1 n ) = ∑ a ∈ A d v ( a | x n − k + 1 n ) denotes the total number of events that has occurred within context x n − k + 1 n . In order to avoid problems with “shifting” of the data, the sequence counts are performed symbol by symbol, when learning a model from a string. Parameter α allows controlling the transition from an estimator initially assuming a uniform distribution to a one progressively closer to the relative frequency estimator. The theoretical information content average provided by the ith sequence of d symbols from the original sequence x, is given by (5) − log 2 P ( X i = t i | x i d − k i d − 1 ) bits, where t i = x i d , x i d + 1 … x ( i + 1 ) d − 1 . As testbed applications, we perform ECG biometric identification and compute a similarity measure between DNA sequences;After processing the first n symbols of x, the total number of bits generated by an order-k with depth-d xaFCM is equal to (6) − ∑ i = 1 n / d log 2 P ( t i | x d i − k d i − 1 ) , where, for simplicity, we assume that n ( m o d d ) = 0 . If we consider a xaFCM with depth d = 1 , then it becomes a regular FCM with the same order k. In that sense, we can consider that a FCM is a particular case of a xaFCM. An intuitive way of understanding how a xaFCM works is to think of it as a FCM which, for each context of length k, instead of counting the number of occurrences of symbols of A , counts the occurrences of sequences w ∈ A d . In other words, for each sequence of length k found, it counts the number of times each sequence of d symbols appeared right after it. Even though, when implemented, this might use more memory to represent the model, an advantage is that it is possible to compress a new sequence of length m, relatively to some previously constructed model, making only m/d accesses to the model. This significantly reduces the time of computation, as we will show in the experimental results presented in Sections 3 and 4. Since, for compressing the first k symbols of a sequence, we do not have enough symbols to represent a context of length k, we always assume that the sequence is “circular”. For long sequences, specially using small contexts/depths, this should not make much difference in terms of compression, but as the contexts/depths increase, this might not be always the case. Since the purpose for which we use these models is to provide an approximation for the number of bits that would be produced by a compressor based on them, whenever we use the word “compression”, in fact we are not performing the compression itself. For that, we would need to use an encoder, which would take more time to compute. It would also be needed to add some side information for the compressor to deal with the circular sequences – but that goes out of scope for our goal. 2.1.1 Example Let x be the circular sequence AAABCC. Using a regular FCM with k = 2 and α = 0.01 , we would build the model from Table 1 to represent x. It is easy to notice that this representation can be implemented using an hash-table of strings to arrays of integers with fixed size (alphabet size + 1 ). However, we propose a different alternative, which consists of building a hash-table of hash-tables. The reason for doing so is that often the number of counts of symbols for each context is very sparse, which would be a waste of memory. To represent exactly the same model, we would build the structure presented in Table 2 . For compressing the sequence x, relatively to itself, we would need C(x||x) bits, where (7) C ( x ∥ x ) = C ( A | C C ) + C ( A | C A ) + C ( A | A A ) + C ( B | A A ) + C ( C | A B ) + C ( C | B C ) and, (8) C ( A | C C ) = C ( A | C A ) = C ( C | A B ) = C ( C | B C ) = − log 2 1 + 0.01 1 + 3 × 0.01 = 0.0283 and (9) C ( A | A A ) = C ( B | A A ) = − log 2 2 + 0.01 1 + 3 × 0.01 = 1.007 , which means C ( x ∥ x ) = 2.1272 or, in other words, it is possible to compress x relatively to itself using just 2.1272 bits. Using a xaFCM, also with k = 2 and α = 0.01 , but with d = 2 , we would build the model presented in Table 3 to represent x. Therefore, (10) C ( x ∥ x ) = C ( A A | C C ) + C ( A B | A A ) + C ( C C | A B ) where, (11) C ( A A | C C ) = C ( C C | A B ) = − log 2 1 + 0.01 1 + 3 2 × 0.01 = 0.110 and (12) C ( A B | A A ) = − log 2 1 + 0.01 2 + 3 2 × 0.01 = 1.049 which means C ( x ∥ x ) = 1.269 or, in other words, using a xaFCM to represent the sequence x it is possible to compress it relatively to itself using just 1.269 bits. Calculating the NRC for both compressors we obtain: • Using FCM – NRC ( x ∥ x ) = 2.1272 6 × log 2 3 = 0.224 ; • Using xaFCM – NRC ( x ∥ x ) = 1.049 6 × log 2 3 = 0.110 . Based on this example, we can infer that, at least for some cases, it is possible to obtain better compression ratios, using xaFCMs instead of traditional FCMs to represent a sequence. 2.2 Parameter selection 2.2.1 Selection of α Since adjusting the α parameter might not be trivial, as it depends on the choice of d as well as on the alphabet size. It is, however, possible to choose α based on a certain desired probability p for a specific outcome. In our experiments, in order to avoid having one more parameter to “tweak”, we are defining α automatically, in a way such that, if sequence w ∈ A d was only found once after a certain context c = x n − k + 1 n ∈ x , and no other sequence ∈ A d was found after that context c (in other words, the total of that line, in the model, is 1), we want to be 90% sure that the same situation happens when compressing a sequence relatively to the learned model. In other words, when we calculate the number of bits, (13) − log 2 P ( X i = t i | c ) needed to compress sequence t i = x i d , x i d + 1 … x i d + d − 1 , we want to choose an α such that (14) P ( X i = t i | c ) = 0 . 9 d . But, since (15) P ( w | c ) = v ( w | c ) + α v ( c ) + α | A | d , where, we have chosen c and w such that v ( c ) = v ( w | c ) = 1 . Therefore (16a) P ( w | c ) = 0 . 9 d ⇔ 1 + α 1 + α | A | d = 0 . 9 d . Since A d is an extension of A to d dimensions, (16b) 1 + α 1 + α | A | d = 0 . 9 d . Also, since both the alphabet size A and the depth d are static parameters, it is easy to solve the equation and choose α in this way. It is also worth mentioning that there is always a possible solution for the equation, since the denominator of the fraction on the left is never equal to zero. 2.2.2 Selection of d The parameter d is an integer greater or equal to one. As mentioned in Section 2.1, when d = 1 , we are using a xaFCM which is equivalent to a FCM of the same order k. Therefore, they both produce exactly the same number of bits. As d increases, so does the RAM needed to store the xaFCM model – but there is not much of an impact (for d = 11 the increase in memory usage is about 10%). The reason for the model complexity to only increase this is that the number of different “leaves” in the hash-tables does not change with the choice of d – only the size of each string stored does. Something to take into account when choosing d is that, the greater the value of d, the harder it would be for an arithmetic encoder to complete its process. Since we only want to compute the NRC, we do not use an encoder. However, to avoid unrealistic results, we want to choose a d that produces an alphabet size of, at most, the MaximumValue(integer) − 1 (e.g. 2 31 − 1 ) symbols. For that reason, using an alphabet of size 6, we can say that 1 ≤ d ≤ 11. Often, we are mostly interested in the time it takes to compress a new target sequence, given an already built model representing the reference sequence. With this application in mind, we can say for sure that the d should be as big as possible, since, as mentioned before, less computations need to be done to compress a new target sequence and, therefore, much less time is needed. Results from real experiments can be seen in the next section. 3 Application 1 – ECG biometric identification In previous works, we have addressed the topic of ECG based biometric identification using a measure of similarity related to the Kolmogorov complexity, called the normalized relative compression (NRC). To attain the goal, we built finite-context models (FCM) to represent each individual [9,10] – a compression-based approach that has been shown successful for different pattern recognition applications [2,4,11]. Other recent works, also based on a compression approach, use the Ziv–Merhav cross parsing algorithm for attaining the same goal [12,13]. Compression-based approaches found in the literature for ECG biometric identification does not seem to take advantage of the fact that the ECG is a quasi-periodical time-series. Since our method uses a semi-fiducial approach (it only detects the R-peak), it is trivial to know where the repetition should happen and take advantage of that fact. From previous results [14], we concluded that, when consecutive heartbeats 2 2 For readability, by “heartbeat” we mean the interval between two consecutive R-peaks. present low levels of noise, their quantization is almost identical. As a consequence of this, we consider that any sequence we analyze is a circular sequence [15]. From this result, it is possible to infer that, compressing the beginning of an heartbeat using the end of the same heartbeat, may be identical to compress it using the end of the previous heartbeat. This may not sound as an advantage, however, this fact allows us to use heartbeats that are not consecutive, when performing the identification of a participant. Since the purpose of this paper is to introduce the method, and not to focus too much in the ECG signal, we do not explore this fact. However, it is already being taken it into account when building the algorithm (one of the arguments that the algorithm accepts as input is the length of the expected repetition – i.e. for this application, how many symbols has one heartbeat), because it will be important for building a real system, as we expect more noise to be present and, therefore, some segments need to be discarded when performing the compression [14]. 3.1 R-peak detection The development of a robust automatic R-peak detector is essential, but it is still a challenging task, due to irregular heart rates, various amplitude levels and QRS morphologies, as well as all kinds of noise and artifacts [16]. We decided to use a semi-fiducial method for segmenting the ECG signal and, since this was not the major focus of the work, we used a preexisting implementation to detect R-peaks, based on the method proposed in [16]. The reason for using a semi-fiducial approach is that fiducial methods have a higher error of detection, while detecting the R-peaks is, nowadays, an almost trivial process [16]. The method used detects the R-peaks by calculating the average points between the Q and S points (from the QRS-complexes) – this may not give the real local maxima corresponding to the R-peaks, but it produces a very close point. For more information regarding the process used for detecting the R-peaks check [16]. The process was already validated by its authors using the standard MIT-BIH arrhythmia database, achieving an average sensitivity of 99.94% and a positive predictivity of 99.96%. It uses bandpass filtering and differentiation operations, aiming to enhance the QRS complexes and to reduce out-of-band noise. A nonlinear transformation is used to obtain a positive-valued feature signal, which includes large candidate peaks corresponding to the QRS complex regions. 3.2 Quantization Data compression algorithms are symbolic in nature. Text and DNA sequences are well-known examples of symbolic sequences, with well-defined associated alphabets. Contrarily the ECG signal needs first to be transformed into symbols before data compression can be applied. In this work, we have relied on the SAX (Symbolic Aggregate ApproXimation) representation [17] to transform the ECG into a symbolic time-series. We consider that the signal is already discrete in the time domain, i.e., that it is already sampled. However, we perform re-sampling using the previously detected R-peaks. There is a fundamental trade-off to take into account while performing the choice of the alphabet size: the quality produced versus the amount of data necessary to represent the sequence [18]. We tested the experiments using alphabet sizes from 3 up to 20 symbols and using different numbers of symbols each R-R segment (per heartbeat), and found that a combination of using an alphabet size of 6 and 200 symbols per heartbeat produced a good balance between the complexity of the strings/models and the accuracies obtained for biometric identification, allowing us to proceed with the improvement of the compressors, while these parameters remained static. However, this result does not guarantee that the same will hold true for a different dataset or application, nor does it guarantee that these are the optimal parameters. Future work is needed to perform this choice in a more robust and automatic way. 3.3 Experimental results The database used in our experiments was collected in house [10,19], where 25 participants were exposed to different external stimuli – disgust, fear and neutral. Data were collected on three different days (once per week), at the University of Aveiro, using a different stimulus per day. The data signals were collected during 25 min on each day, giving a total of around 75 minof ECG signal per participant. Before being exposed to the stimuli, during the first 4 min of each data acquisition, the participants watched a movie with a beach sunset and an acoustic guitar soundtrack, and were instructed to try to relax as much as possible. By using a database where the participants were exposed to different stimuli, we can check if the emotional state of participants affects the biometric identification process. The database is publicly available for download in. 3 3 After all the already explained preprocessing steps are complete, the process in which we perform the biometric identification is the following: 1. Use the complete ECG signals from two days, in order to build a xaFCM model that describes each of the participants; 2. For the remaining day, split the signal, such that each segment has 10 consecutive heartbeats inside it; 3. “Compress” (compute the NRC) each of the segments obtained in the previous step using each of the models obtained in the first step; 4. The model which produces a lowest result is chosen as the candidate for biometric identification. The justification for the first step is that we do not want to use any information from the ECG of the day where we are trying to perform the ECG biometric identification, since, if we used that information, our results would not match a real situation. The number of heartbeats needed for ECG biometric identification is undoubtedly useful when building a biometric identification system – any system should ask participants to provide data for identification, using the smallest time interval that is possible, for practical reasons. Based on the results from a previous study [10], we concluded that 10 heartbeats is a good trade-off between collection time (which should be as low as possible) and statistical relevance of the data. All the experiments were implemented and ran using Python 3.5 (Linux 64 bits) on an Intel(R) Core(TM) i7-6700 CPU @ 3.40 GHz, with 32GB of RAM. For simplicity of code, we have not parallelized the process yet – therefore, only one logical core was used for each experiment. In Fig. 1 , it is possible to see a plot with the accuracy obtained for the process described, by using FCM models, with all possible values of k from 1 up to 20. In the red line, it is also possible to see how much time does this process take in total. An important fact is that the time taken to perform the biometry is approximately directly proportional to the size of the context, k, used. Since the purpose of this paper is to show the appropriateness of xaFCM models, in Fig. 2 are shown six examples of the same experiment, but instead of changing the context k, we have chosen a fixed value of k and tested all possible values of d, the depth of the xaFCMs. From these plots, it is possible to see that the time taken to perform the biometry process for the whole database is up to 3–4 times shorter when using high values of d, having, usually, accuracy ratios comparable with the FCMs of the same order k. On the experiments using “lower” values for the context k (in this case, k ≤ 14), it is possible to notice a minor improvement in terms of accuracy as the d increases, at least for the first values of d (d ≤ 7, more or less). This makes us think that increasing the depth d behaves in a similar way to increasing the depth k of the xaFCM, without the additional cost in terms of testing speed (quite the opposite, actually) and the memory needed does not increase so much as it would by increasing k (Fig. 3 ). In higher contexts k we get the same advantages in terms of computing time and memory requirements, however, after a certain point, there is just no real benefit from increasing neither the context k, nor the depth d, since we are looking for “too specific” patterns, that may not appear again on the segments being tested – which, making an analogy to machine learning, we would be overfitting to the training data. Another aspect we wanted to show, regarding the advantages of using xaFCMs, is the model complexity. In order for the biometric identification to be executed fast, in practice, it is needed to have all the participant models previously loaded into memory. This usually does not pose a problem, if there are not many participants, but it may be useful for building a real biometric identification system. In Fig. 3, we can see that by increasing the context k of FCM models, the complexity of each model increases exponentially. From our interpretation, a way to avoid this exponential increase is to use an xaFCM with an order slightly lower and increase its depth d. In order to show this, we display the complexity of such models in Fig. 4 . 4 Application 2 – DNA sequence relative similarity An approach for computing the similarity of a sequence relatively to other is to calculate the NRC using one of them as reference and the other as the target. In previous works, this has been done using FCM compressors [2,20–22]. In order to show that the xaFCMs are also suitable for this application, we ran some simulations using the human and chimpanzee DNA sequences, removing the unknown symbols (N). The idea was to use each chromosome of the human species as reference and then compress each chromosome of chimpanzee as the target, using exclusively the model from the reference. Since we know from evolution theory that these two species are closely related [23], it is expected that, when we are compressing homologous pairs of chromosomes, the NRC should be lower than on the other cases. To perform the experiment, we used the assembled human chromosomes 1–22, X and Y (3.1GB of data in total) and assembled chimpanzee chromosomes 1, 2a, 2b, 3 to 22, X and Y (3.2GB of data in total). 4 4 All the assembled genome data were downloaded from We ran two different simulations: the first one, with a FCM of context k = 12 ; the other with a xaFCM with k = 12 and d = 8 . All the experiments ran on a server with 16-cores 2.13 GHz Intel Xeon CPU E7320 and 256GB of RAM, but the implementation used a single core. Table 4 shows the average times taken by each experiment, as well as the average memory needed to store the xaFCM model to represent the human chromosomes. It is clear from these results that the xaFCMs are almost d times faster than an FCM of the same order k. Another advantage is that the memory needed for the xaFCMs does not increase exponentially with d. The NRC results for the two simulations, with k = 12 , can be seen in Fig. 5 . It is possible to notice that the heatmap corresponding to the FCM shows better compressions on average. However, using the “perfect” relative compressor, we would expect the NRCs to be as low as possible on the diagonal of the matrix, 5 5 Not exactly the diagonal, because of the second chromosome of the chimpanzee is split into 2a and 2b, making the matrix not a square one. since they represent related chromosomes. The other squares should have higher NRCs, as they have more variation. This is exactly what happens on the xaFCM test (bottom one in Fig. 5). This becomes even more clear when we are comparing the compression along the same sequence, as can be seen in Fig. 6 . 5 Conclusions and future work We have shown that xaFCMs are good candidates to represent models for ECG biometric identification. When compared with FCMs, with the same memory usage, better accuracy ratios are usually obtained, using up to around 3–4 times less time to compute the NRCs (depending on the choice of d). The gains in computational speed increase in the DNA sequence given the higher order of data length. Our experiments show that it is possible to use them for DNA sequence pattern recognition, making them a suitable alternative to the traditional FCMs. These are promising results, and it seems appropriate to infer that the xaFCMs can be suitable to some other applications, specially when the problem of memory usage or testing speed are crucial. For that reason, in the near future, we plan to test them in different applications, where FCMs have proven suitable, like image pattern recognition [1,11,24] and authorship attribution [4]. Acknowledgments This work was partially supported by national funds through the FCT - Foundation for Science and Technology, and by European funds through FEDER, under the COMPETE 2020 and Portugal 2020 programs, in the context of the projects UID/CEC/00127/2013 and PTDC/EEI-SII/6608/2014. S. Brás acknowledges the Postdoc Grant from FCT, ref. SFRH/BPD/92342/2013. References [1] A.J. Pinho P. Ferreira Finding unknown repeated patterns in images 19th Eur. Signal Process. Conf. (EUSIPCO 2011) 2011 [2] D. Pratas A.J. Pinho A conditional compression distance that unveils insights of the genomic evolution 2014 Data Compression Conference 2014 IEEE 421 422 10.1109/DCC.2014.58 [3] D.P. Coutinho M.A.T. Figueiredo Text classification using compression-based dissimilarity measures Int. J. Pattern Recognit. Artif. Intell. 29 05 2015 10.1142/S0218001415530043 [4] A.J. Pinho D. Pratas P.J.S.G. Ferreira Authorship attribution using compression distances Data Compression Conference 2016 10.1109/DCC.2016.53 [5] D. Pratas A.J. Pinho R.M. Silva J.a. Rodrigues M. Hosseini T. Caetano P. Ferreira FALCON: a method to infer metagenomic composition of ancient DNA BioRxiv 2018 [6] M. Li X. Chen X. Li The similarity metric IEEE Trans. Inf. Theory 50 12 2004 3250 3264 [7] M. Li P. Vitányi An Introduction to Kolmogorov Complexity and its Applications 3rd Ed. 1997 Springer 10.1016/S0898-1221(97)90213-3 [8] C.H. Bennett P. Gács M. Li Information distance IEEE Trans. Inf. Theory 44 4 1998 1407 1423 [9] S. Brás A.J. Pinho ECG biometric identification: a compression based approach 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2015 5838 5841 10.1109/EMBC.2015.7319719 [10] J.a.M. Carvalho S. Brás J. Ferreira S.C. Soares A.J. Pinho Impact of the Acquisition Time on ECG Compression-based Biometric Identification Systems Proceedings of Pattern Recognition and Time Analysis - 8th Iberian Conference (IbPRIA) 2017 169 176 [11] A.J. Pinho P. Ferreira Image similarity using the normalized compression distance based on finite context models 18th IEEE Int. Confer. Image Process. 2011 [12] D.P. Coutinho H. Silva H. Gamboa A. Fred M. Figueiredo Novel fiducial and non-fiducial approaches to electrocardiogram-based biometric systems IET Biom. 2 2 2013 [13] D.P. Coutinho A. Fred M. Figueiredo One-lead ECG-based personal identification using Ziv–Merhav cross parsing Pattern Recognit. (ICPR), 20th Int. Conf. 2010 3858 3861 [14] J.M. Carvalho A.J. Pinho S. Brás Irregularity detection in ECG signal using a semi-fiducial method Proceedings of the 22nd RecPad 2016 75 76 [15] R. Grossi C.S. Iliopoulos R. Mercas N. Pisanti S.P. Pissis A. Retha F. Vayani Circular sequence comparison: algorithms and applications. Algorithms Mol. Biol. 11 2016 12 10.1186/s13015-016-0076-6 [16] P. Kathirvel M. Sabarimalai S.R.M. Prasanna K.P. Soman An efficient R-peak detection based on new nonlinear transformation and first-order Gaussian differentiator Cardiovasc. Eng. Technol. 2 4 2011 408 425 10.1007/s13239-011-0065-3 [17] J. Lin E. Keogh S. Lonardi B. Chiu A symbolic representation of time series, with implications for streaming algorithms DMKD ’03 Proceedings of the 8th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2003 2 11 [18] G.C. Rafael R.E. Woods Sampling and quantization Digital Image Processing 2007 Prentice Hall PTR 10.1002/0470870109.ch3 [19] J. Ferreira S. Brás C.F. Silva S.C. Soares An automatic classifier of emotions built from entropy of noise Psychophysiology 2016 10.1111/psyp.12808 [20] A.J. Pinho D. Pratas P.J.S.G. Ferreira Bacteria DNA sequence compression using a mixture of finite-context models IEEE Workshop on Statistical Signal Processing Proceedings 2011 125 128 10.1109/SSP.2011.5967637 [21] D. Pratas A.J. Pinho Exploring deep Markov models in genomic data compression using sequence pre-analysis European Signal Processing Conference, EUSIPCO 2014, 2014 [22] D. Pratas A.J. Pinho P.J.S.G. Ferreira Efficient compression of genomic sequences Data Compression Conference 2016 10.1109/DCC.2016.60 [23] A. Hobolth J.Y. Dutheil J. Hawks M.H. Schierup T. Mailund Incomplete lineage sorting patterns among human, chimpanzee, and orangutan suggest recent orangutan speciation and widespread selection Genome Res. 21 3 2011 349 356 10.1101/gr.114751.110 [24] A.J. Pinho D. Pratas P. Ferreira A new compressor for measuring distances among images Image Anal. Recognit. 1 2014 30 37 "
    },
    {
        "doc_title": "Comparison of compression-based measures with application to the evolution of primate genomes",
        "doc_scopus_id": "85048723392",
        "doc_doi": "10.3390/e20060393",
        "doc_eid": "2-s2.0-85048723392",
        "doc_date": "2018-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Mathematical Physics",
                "area_abbreviation": "MATH",
                "area_code": "2610"
            },
            {
                "area_name": "Physics and Astronomy (miscellaneous)",
                "area_abbreviation": "PHYS",
                "area_code": "3101"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 by the authors.An efficient DNA compressor furnishes an approximation to measure and compare information quantities present in, between and acrossDNAsequences, regardless of the characteristics of the sources. In this paper, we compare directly two information measures, the Normalized Compression Distance (NCD) and the Normalized Relative Compression (NRC). These measures answer different questions; the NCD measures how similar both strings are (in terms of information content) and the NRC (which, in general, is nonsymmetric) indicates the fraction of one of them that cannot be constructed using information from the other one. This leads to the problem of finding out which measure (or question) is more suitable for the answer we need. For computing both, we use a state of the art DNA sequence compressor that we benchmark with some top compressors in different compression modes. Then, we apply the compressor on DNA sequences with different scales and natures, first using synthetic sequences and then on real DNA sequences. The last include mitochondrial DNA (mtDNA), messenger RNA (mRNA) and genomic DNA (gDNA) of seven primates. We provide several insights into evolutionary acceleration rates at different scales, namely, the observation and confirmation across the whole genomes of a higher variation rate of the mtDNA relative to the gDNA. We also show the importance of relative compression for localizing similar information regions using mtDNA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Application of Data Compression Models to Handwritten Digit Classification",
        "doc_scopus_id": "85054849437",
        "doc_doi": "10.1007/978-3-030-01449-0_41",
        "doc_eid": "2-s2.0-85054849437",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Algorithmic information theory",
            "Context models",
            "Handwritten digit",
            "Kolmogorov complexity",
            "Probabilistic algorithm"
        ],
        "doc_abstract": "© 2018, Springer Nature Switzerland AG.In this paper, we address handwritten digit classification as a special problem of data compression modeling. The creation of the models—usually known as training—is just a process of counting. Moreover, the model associated to each class can be trained independently of all the other class models. Also, they can be updated later with new examples, even if the old ones are not available anymore. Under this framework, we show that it is possible to attain a classification accuracy consistently above 99.3% on the MNIST dataset, using classifiers trained in less than one hour on a common laptop.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cryfa: A tool to compact and encrypt FASTA files",
        "doc_scopus_id": "85025168586",
        "doc_doi": "10.1007/978-3-319-60816-7_37",
        "doc_eid": "2-s2.0-85025168586",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Advanced Encryption Standard",
            "Compact representation",
            "Encryption methods",
            "Genomic data",
            "Large volumes",
            "Next-generation sequencing",
            "Patient data",
            "Secure protocols"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.NGS (next-generation sequencing) is bringing the need to efficiently handle large volumes of patient data, maintaining privacy laws, such as those with secure protocols that ensure patients DNA confidentiality. Although there are multiple file representations for genomic data, the FASTA format is perhaps the most used and popular. As far as we know, FASTA encryption is being addressed with general purpose encryption methods, without exploring a compact representation. In this paper, we propose Cryfa, a new fast encryption method to store securely FASTA files in a compact form. The main differences between a general encryption approach and Cryfa are the reduction of storage, up to approximately three times, without compromising security, and the possibility of integration with pipelines. The core of the encryption method uses a symmetric approach, the AES (Advanced Encryption Standard). Cryfa implementation is freely available, under license GPLv3, at https://github.com/pratas/cryfa.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Substitutional tolerant markov models for relative compression of DNA sequences",
        "doc_scopus_id": "85025129828",
        "doc_doi": "10.1007/978-3-319-60816-7_32",
        "doc_eid": "2-s2.0-85025129828",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Compression efficiency",
            "Computation time",
            "DNA data",
            "Fundamental operations",
            "Genomic sequence",
            "High-efficiency",
            "Markov model",
            "Model species"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Referential compression is one of the fundamental operations for storing and analyzing DNA data. The models that incorporate relative compression, a special case of referential compression, are being steadily improved, namely those which are based on Markov models. In this paper, we propose a new model, the substitutional tolerant Markov model (STMM), which can be used in cooperation with regular Markov models to improve compression efficiency. We assessed its impact on synthetic and real DNA sequences, showing a substantial improvement in compression, while only slightly increasing the computation time. In particular, it shows high efficiency in modeling species that have split less than 40 million years ago.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the role of inverted repeats in DNA sequence similarity",
        "doc_scopus_id": "85025115709",
        "doc_doi": "10.1007/978-3-319-60816-7_28",
        "doc_eid": "2-s2.0-85025115709",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Chromosomal rearrangement",
            "Compression methods",
            "Computational approach",
            "Computational time and memory",
            "Context modeling",
            "Inverted repeat",
            "Sequence similarity",
            "Statistical characteristics"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.In this paper, we propose a computational approach to quantify inverted repeats. This is important, because it is known that the presence of inverted repeats in genomic data may be associated to certain chromosomal rearrangements. First, we present a reference-based relative compression method, which employs statistical characteristics of the genomic data. Then, for determining the similarity between genomic sequences, we use the normalized relative compression measure, which is light-weight regarding computational time and memory. Testing this approach on various species, including human, chimpanzee, gorilla, chicken, turkey and archaea genomes, we unveil unreported results that may support several evolution insights.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visualization of distinct DNA regions of the modern human relatively to a neanderthal genome",
        "doc_scopus_id": "85021224756",
        "doc_doi": "10.1007/978-3-319-58838-4_26",
        "doc_eid": "2-s2.0-85021224756",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ancient dnas",
            "Bloom filters",
            "Computational tools",
            "DNA patterns",
            "Genome sequences",
            "Paleogenomics",
            "Probabilistic methods",
            "Species specifics"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Species-specific DNA regions are segments that are unique or share high dissimilarity relatively to close species. Their discovery is important, because they allow the localization of evolutionary traits that are often related to novel functionalities and, sometimes, diseases. We have detected distinct DNA regions specific in the modern human, when compared to a Neanderthal high-quality genome sequence obtained from a bone of a Siberian woman. The bone is around 50,000 years old and the DNA raw data totalizes more than 418 GB. Since the data size required for localizing efficiently such events is very high, it is not practical to store the model on a table or hash table. Thus, we propose a probabilistic method to map and visualize those regions. The time complexity of the method is linear. The computational tool is available at http://pratas.github.io/chester. The results, computed in approximately two days using a single CPU core, show several regions with documented neanderthal absent regions, namely genes associated with the brain (neurotransmiters and synapses), hearing, blood, fertility and the immune system. However, it also shows several undocumented regions, that may express new functions linked with the evolution of the modern human.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the approximation of the Kolmogorov complexity for DNA sequences",
        "doc_scopus_id": "85021204963",
        "doc_doi": "10.1007/978-3-319-58838-4_29",
        "doc_eid": "2-s2.0-85021204963",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Archaea",
            "Complete genomes",
            "Finite alphabet",
            "Kolmogorov complexity",
            "Lossless",
            "Natural process",
            "Relative complexity"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.The Kolmogorov complexity furnishes several ways for studying different natural processes that can be expressed using sequences of symbols from a finite alphabet, such as the case of DNA sequences. Although the Kolmogorov complexity is not algorithmically computable, it can be approximated by lossless normal compressors. In this paper, we use a specific DNA compressor to approximate the Kolmogorov complexity and we assess it regarding its normality. Then, we use it on several datasets, that are constituted by different DNA sequences, representing complete genomes of different species and domains. We show several evolution-related insights associated with the complexity, namely that, globally, archaea have higher relative complexity than bacteria and eukaryotes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preface",
        "doc_scopus_id": "85020418014",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85020418014",
        "doc_date": "2017-01-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Authorship Attribution Using Relative Compression",
        "doc_scopus_id": "85010032070",
        "doc_doi": "10.1109/DCC.2016.53",
        "doc_eid": "2-s2.0-85010032070",
        "doc_date": "2016-12-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Authorship attribution",
            "Classification performance",
            "Data set",
            "Information distance",
            "Multiple discriminant analysis",
            "Normalized compression distance"
        ],
        "doc_abstract": "© 2016 IEEE.Authorship attribution is a classical classification problem. We use it here to illustrate the performance of a compression-based measure that relies on the notion of relative compression. Besides comparing with recent approaches that use multiple discriminant analysis and support vector machines, we compare it with the Normalized Conditional Compression Distance (a direct approximation of the Normalized Information Distance) and the popular Normalized Compression Distance. The Normalized Relative Compression (NRC) attained 100% correct classification in the data set used, showing consistency between the compression ratio and the classification performance, a characteristic not always present in other compression-based measures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Efficient Compression of Genomic Sequences",
        "doc_scopus_id": "85007344704",
        "doc_doi": "10.1109/DCC.2016.60",
        "doc_eid": "2-s2.0-85007344704",
        "doc_date": "2016-12-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Compression gain",
            "Context modeling",
            "Context models",
            "Genomic sequence",
            "Hardware specifications",
            "High order model",
            "Reference-free"
        ],
        "doc_abstract": "© 2016 IEEE.The number of genomic sequences is growing substantially. Besides discarding part of the data, the only efficient possibility for coping with this trend is data compression. We present an efficient compressor for genomic sequences, allowing both reference-free and referential compression. This compressor uses a mixture of context models of several orders, according to two model classes: reference and target. A new type of context model, which is capable of tolerating substitution errors, is introduced. For ensuring flexibility regarding hardware specifications, the compressor uses cache-hashes in high order models. The results show additional compression gains over several specific top tools in different levels of redundancy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A survey on data compression methods for biological sequences",
        "doc_scopus_id": "85007393441",
        "doc_doi": "10.3390/info7040056",
        "doc_eid": "2-s2.0-85007393441",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "FASTA",
            "FASTQ",
            "Multi-FASTA",
            "Protein sequences",
            "Reference-free"
        ],
        "doc_abstract": "© 2016 by the authors.The ever increasing growth of the production of high-throughput sequencing data poses a serious challenge to the storage, processing and transmission of these data. As frequently stated, it is a data deluge. Compression is essential to address this challenge-it reduces storage space and processing costs, along with speeding up data transmission. In this paper, we provide a comprehensive survey of existing compression approaches, that are specialized for biological data, including protein and DNA sequences. Also, we devote an important part of the paper to the approaches proposed for the compression of different file formats, such as FASTA, as well as FASTQ and SAM/BAM, which contain quality scores and metadata, in addition to the biological sequences. Then, we present a comparison of the performance of several methods, in terms of compression ratio, memory usage and compression/decompression time. Finally, we present some suggestions for future research on biological data compression.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Three minimal sequences found in Ebola virus genomes and absent from human DNA",
        "doc_scopus_id": "84943639957",
        "doc_doi": "10.1093/bioinformatics/btv189",
        "doc_eid": "2-s2.0-84943639957",
        "doc_date": "2015-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Disease Outbreaks",
            "DNA, Viral",
            "Ebolavirus",
            "Genome, Human",
            "Genome, Viral",
            "Hemorrhagic Fever, Ebola",
            "Humans",
            "Sequence Analysis, DNA",
            "Viral Proteins"
        ],
        "doc_abstract": "© The Author 2014. Published by Oxford University Press.Motivation: Ebola virus causes high mortality hemorrhagic fevers, with more than 25 000 cases and 10000 deaths in the current outbreak. Only experimental therapies are available, thus, novel diagnosis tools and druggable targets are needed. Results: Analysis of Ebola virus genomes from the current outbreak reveals the presence of short DNA sequences that appear nowhere in the human genome. We identify the shortest such sequences with lengths between 12 and 14. Only three absent sequences of length 12 exist and they consistently appear at the same location on two of the Ebola virus proteins, in all Ebola virus genomes, but nowhere in the human genome. The alignment-free method used is able to identify pathogen-specific signatures for quick and precise action against infectious agents, of which the current Ebola virus outbreak provides a compelling example.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An alignment-free method to find and visualise rearrangements between pairs of DNA sequences",
        "doc_scopus_id": "84929429321",
        "doc_doi": "10.1038/srep10203",
        "doc_eid": "2-s2.0-84929429321",
        "doc_date": "2015-05-18",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Animals",
            "Computational Biology",
            "Gene Rearrangement",
            "Genomics",
            "Humans",
            "Web Browser"
        ],
        "doc_abstract": "© 2015, Nature Publishing Group. All rights reserved.Species evolution is indirectly registered in their genomic structure. The emergence and advances in sequencing technology provided a way to access genome information, namely to identify and study evolutionary macro-events, as well as chromosome alterations for clinical purposes. This paper describes a completely alignment-free computational method, based on a blind unsupervised approach, to detect large-scale and small-scale genomic rearrangements between pairs of DNA sequences. To illustrate the power and usefulness of the method we give complete chromosomal information maps for the pairs human-chimpanzee and human-orangutan. The tool by means of which these results were obtained has been made publicly available and is described in detail.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mafco: A compression tool for MAF files",
        "doc_scopus_id": "84929484087",
        "doc_doi": "10.1371/journal.pone.0116082",
        "doc_eid": "2-s2.0-84929484087",
        "doc_date": "2015-03-27",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Data Compression",
            "Genomics",
            "Sequence Alignment",
            "Time Factors"
        ],
        "doc_abstract": "© 2015 Matos et al.In the last decade, the cost of genomic sequencing has been decreasing so much that researchers all over the world accumulate huge amounts of data for present and future use. These genomic data need to be efficiently stored, because storage cost is not decreasing as fast as the cost of sequencing. In order to overcome this problem, the most popular generalpurpose compression tool, gzip, is usually used. However, these tools were not specifically designed to compress this kind of data, and often fall short when the intention is to reduce the data size as much as possible. There are several compression algorithms available, even for genomic data, but very few have been designed to deal with Whole Genome Alignments, containing alignments between entire genomes of several species. In this paper, we present a lossless compression tool, MAFCO, specifically designed to compress MAF (Multiple Alignment Format) files. Compared to gzip, the proposed tool attains a compression gain from 34% to 57%, depending on the data set. When compared to a recent dedicated method, which is not compatible with some data sets, the compression gain of MAFCO is about 9%. Both source-code and binaries for several operating systems are freely available for non-commercial use at: http://bioinformatics.ua.pt/software/mafco.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring deep Markov models in genomic data compression using sequence pre-analysis",
        "doc_scopus_id": "84911892507",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84911892507",
        "doc_date": "2014-11-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Compression algorithms",
            "Context models",
            "Genomic data compressions",
            "Genomic sequence",
            "Hash table",
            "High compressions",
            "Markov model",
            "Textual data"
        ],
        "doc_abstract": "© 2014 EURASIP.The pressure to find efficient genomic compression algorithms is being felt worldwide, as proved by several prizes and competitions. In this paper, we propose a compression algorithm that relies on a pre-analysis of the data before compression, with the aim of identifying regions of low complexity. This strategy enables us to use deeper context models, supported by hash-tables, without requiring huge amounts of memory. As an example, context depths as large as 32 are attainable for alphabets of four symbols, as is the case of genomic sequences. These deeper context models show very high compression capabilities in very repetitive genomic sequences, yielding improvements over previous algorithms. Furthermore, this method is universal, in the sense that it can be used in any type of textual data (such as quality-scores).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new compressor for measuring distances among images",
        "doc_scopus_id": "84921770705",
        "doc_doi": "10.1007/978-3-319-11758-4_4",
        "doc_eid": "2-s2.0-84921770705",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Context models",
            "Digital Objects",
            "Feature selection and extractions",
            "Image similarity",
            "Kolmogorov complexity",
            "Measure of similarities",
            "Measuring distances"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.Ideally, we would like to have a measure of similarity between images that did not require a feature selection and extraction step. In theory, this can be attained using Kolmogorov complexity concepts. In practice, because the Kolmogorov complexity of a digital object cannot be computed, one has to rely on appropriate approximations, the most successful being based on data compression. The application of these ideas to images has been more difficult than to some other areas. In this paper, we suggest a new distance and compare it with two others, showing some of their relative advantages and disadvantages, hoping to contribute to the advance of this promising line of research.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information profiles for DNA pattern discovery",
        "doc_scopus_id": "84903478465",
        "doc_doi": "10.1109/DCC.2014.54",
        "doc_eid": "2-s2.0-84903478465",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "DNA patterns",
            "Fission yeast",
            "Information contents",
            "Schizosaccharomyces pombe"
        ],
        "doc_abstract": "Finite-context modeling is a powerful tool for compressing and hence for representing DNA sequences. We describe an algorithm to detect genomic regularities, within a blind discovery strategy. The algorithm uses information profiles built using suitable combinations of finite-context models. We used the genome of the fission yeast Schizosaccharomyces pombe strain 972 h-for illustration, unveiling locations of low information content, which are usually associated with DNA regions of potential biological interest. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A conditional compression distance that unveils insights of the genomic evolution",
        "doc_scopus_id": "84903434093",
        "doc_doi": "10.1109/DCC.2014.58",
        "doc_eid": "2-s2.0-84903434093",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Genomic sequence",
            "Information contents",
            "Normalized compression distance"
        ],
        "doc_abstract": "We describe a compression-based distance for genomic sequences. Instead of using the usual conjoint information content, as in the classical Normalized Compression Distance (NCD), it uses the conditional information content. To compute this Normalized Conditional Compression Distance (NCCD), we need a normal conditional compressor, that we built using a mixture of static and dynamic finite-context models. Using this approach, we measured chromosomal distances between Hominidae primates and also between Muroidea (rat and mouse), observing several insights of evolution that so far have not been reported in the literature. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "XS: A FASTQ read simulator",
        "doc_scopus_id": "84892407950",
        "doc_doi": "10.1186/1756-0500-7-40",
        "doc_eid": "2-s2.0-84892407950",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014 Pratas et al.Background: The emerging next-generation sequencing (NGS) is bringing, besides the natural huge amounts of data, an avalanche of new specialized tools (for analysis, compression, alignment, among others) and large public and private network infrastructures. Therefore, a direct necessity of specific simulation tools for testing and benchmarking is rising, such as a flexible and portable FASTQ read simulator, without the need of a reference sequence, yet correctly prepared for producing approximately the same characteristics as real data. Findings: We present XS, a skilled FASTQ read simulation tool, flexible, portable (does not need a reference sequence) and tunable in terms of sequence complexity. It has several running modes, depending on the time and memory available, and is aimed at testing computing infrastructures, namely cloud computing of large-scale projects, and testing FASTQ compression algorithms. Moreover, XS offers the possibility of simulating the three main FASTQ components individually (headers, DNA sequences and quality-scores). Conclusions: XS provides an efficient and convenient method for fast simulation of FASTQ files, such as those from Ion Torrent (currently uncovered by other simulators), Roche-454, Illumina and ABI-SOLiD sequencing machines. This tool is publicly available at http://bioinformatics.ua.pt/software/xs/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mfcompress: A compression tool for fasta and multi-fasta data",
        "doc_scopus_id": "84891355058",
        "doc_doi": "10.1093/bioinformatics/btt594",
        "doc_eid": "2-s2.0-84891355058",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Data Compression",
            "Genome",
            "Genomics",
            "Humans",
            "Markov Chains",
            "Software"
        ],
        "doc_abstract": "Motivation: The data deluge phenomenon is becoming a serious problem in most genomic centers. To alleviate it, general purpose tools, such as gzip, are used to compress the data. However, although pervasive and easy to use, these tools fall short when the intention is to reduce as much as possible the data, for example, for medium-and long-term storage. A number of algorithms have been proposed for the compression of genomics data, but unfortunately only a few of them have been made available as usable and reliable compression tools.Results: In this article, we describe one such tool, MFCompress, specially designed for the compression of FASTA and multi-FASTA files. In comparison to gzip and applied to multi-FASTA files, MFCompress can provide additional average compression gains of almost 50%, i.e. it potentially doubles the available storage, although at the cost of some more computation time. On highly redundant datasets, and in comparison with gzip, 8-fold size reductions have been obtained.Availability: Both source code and binaries for several operating systems are freely available for non-commercial use at http://bioinformatics.ua. pt/software/mfcompress/.Contact: Supplementary information: Supplementary data are available at Bioinformatics online. © 2013 The Author .",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A genomic distance for assembly comparison based on compressed maximal exact matches",
        "doc_scopus_id": "84887940267",
        "doc_doi": "10.1109/TCBB.2013.77",
        "doc_eid": "2-s2.0-84887940267",
        "doc_date": "2013-11-25",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Alignment-free",
            "Assembly quality",
            "Genome assembly",
            "Genome sequencing",
            "Genomic distance",
            "Human genomes",
            "Maximal exact matches",
            "Normalized compression distance",
            "Algorithms",
            "Genome, Human",
            "Genomics",
            "Humans",
            "Sequence Analysis, DNA"
        ],
        "doc_abstract": "Genome assemblies are typically compared with respect to their contiguity, coverage, and accuracy. We propose a genome-wide, alignment-free genomic distance based on compressed maximal exact matches and suggest adding it to the benchmark of commonly used assembly quality metrics. Maximal exact matches are perfect repeats, without gaps or misspellings, which cannot be further extended to either their left- or right-end side without loss of similarity. The genomic distance here proposed is based on the normalized compression distance, an information-theoretic measure of the relative compressibility of two sequences estimated using multiple finite-context models. This measure exposes similarities between the sequences, as well as, the nesting structure underlying the assembly of larger maximal exact matches from smaller ones. We use four human genome assemblies for illustration and discuss the impact of genome sequencing and assembly in the final content of maximal exact matches and the genomic distance here proposed. © 2004-2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DNA sequences at a glance",
        "doc_scopus_id": "84896690677",
        "doc_doi": "10.1371/journal.pone.0079922",
        "doc_eid": "2-s2.0-84896690677",
        "doc_date": "2013-11-21",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Data summarization and triage is one of the current top challenges in visual analytics. The goal is to let users visually inspect large data sets and examine or request data with particular characteristics. The need for summarization and visual analytics is also felt when dealing with digital representations of DNA sequences. Genomic data sets are growing rapidly, making their analysis increasingly more difficult, and raising the need for new, scalable tools. For example, being able to look at very large DNA sequences while immediately identifying potentially interesting regions would provide the biologist with a flexible exploratory and analytical tool. In this paper we present a new concept, the \"information profile\", which provides a quantitative measure of the local complexity of a DNA sequence, independently of the direction of processing. The computation of the information profiles is computationally tractable: we show that it can be done in time proportional to the length of the sequence. We also describe a tool to compute the information profiles of a given DNA sequence, and use the genome of the fission yeast Schizosaccharomyces pombe strain 972 h- and five human chromosomes 22 for illustration. We show that information profiles are useful for detecting large-scale genomic regularities by visual inspection. Several discovery strategies are possible, including the standalone analysis of single sequences, the comparative analysis of sequences from individuals from the same species, and the comparative analysis of sequences from different organisms. The comparison scale can be varied, allowing the users to zoom-in on specific details, or obtain a broad overview of a long segment. Software applications have been made available for non-commercial use at http://bioinformatics.ua.pt/ software/dna-at-glance. © 2013 Pinho et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compressing resequencing data with GReEn",
        "doc_scopus_id": "84881097916",
        "doc_doi": "10.1007/978-1-62703-514-9_2",
        "doc_eid": "2-s2.0-84881097916",
        "doc_date": "2013-08-09",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Genome sequencing centers are flooding the scientific community with data. A single sequencing machine can nowadays generate more data in one day than any existing machine could have produced throughout the entire year of 2005. Therefore, the pressure for efficient sequencing data compression algorithms is very high and is being felt worldwide. Here, we describe GReEn (Genome Resequencing Encoding), a compression tool recently proposed for compressing genome resequencing data using a reference genome sequence. © 2013 Springer Science+Business Media New York.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A compression model for DNA multiple sequence alignment blocks",
        "doc_scopus_id": "84876759103",
        "doc_doi": "10.1109/TIT.2012.2236605",
        "doc_eid": "2-s2.0-84876759103",
        "doc_date": "2013-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            }
        ],
        "doc_keywords": [
            "Compression model",
            "Genomics",
            "Lossless compression",
            "Modeling approach",
            "Molecular genomics",
            "Multiple sequence alignments",
            "Per-symbol",
            "Whole genome alignment"
        ],
        "doc_abstract": "A particularly voluminous dataset in molecular genomics, known as whole genome alignments, has gained considerable importance over the last years. In this paper, we propose a compression modeling approach for the multiple sequence alignment (MSA) blocks, which make up most of these datasets. Our method is based on a mixture of finite-context models. Contrarily to other recent approaches, it addresses both the DNA bases and gap symbols at once, better exploring the existing correlations. For comparison with previous methods, our algorithm was tested in the multiz28way dataset. On average, it attained 0.94 bits per symbol, approximately 7% better than the previous best, for a similar computational complexity. We also tested the model in the most recent dataset, multiz46way. In this dataset, that contains alignments of 46 different species, our compression model achieved an average of 0.72 bits per MSA block symbol. © 1963-2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the detection of unknown locally repeating patterns in images",
        "doc_scopus_id": "84864151066",
        "doc_doi": "10.1007/978-3-642-31295-3_19",
        "doc_eid": "2-s2.0-84864151066",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Digital image",
            "False positive",
            "Image complexity",
            "Lossless image compression",
            "Repeated patterns"
        ],
        "doc_abstract": "Detecting unknown repeated patterns that appear multiple times in a digital image is a great challenge. We have addressed this problem in a recent work and we have shown that, using a compression based approach, it is possible to find exact repetitions. In this work, we continue this study, introducing a procedure for detecting unknown repeated patterns that occur in a close vicinity. We use finite-context modelling to pinpoint the possible locations of the repetitions, by exploring the connection between lossless image compression and image complexity. Since repetitions are associated to low complexity regions, the repeating patterns are revealed and easily detected. The experimental results show that the proposed approach provides increased ability to eliminate false positives. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compression of whole genome alignments using a mixture of finite-context models",
        "doc_scopus_id": "84864149326",
        "doc_doi": "10.1007/978-3-642-31295-3_42",
        "doc_eid": "2-s2.0-84864149326",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Arithmetic Coding",
            "Bit per symbols",
            "Compression methods",
            "Compression rates",
            "Data sets",
            "DNA Sequencing",
            "Genomic sequence",
            "Multiple sequence alignments",
            "Per-symbol",
            "Sequence data",
            "Whole genome alignment"
        ],
        "doc_abstract": "In the last years, advances in DNA sequencing technology have caused a giant growth in the amount of available data related with genomic sequences. One of those types of data sets is that resulting from multiple sequence alignments (MSA). In this paper, we propose a compression method for compressing these data sets, using a mixture of finite-context models and arithmetic coding. The method relies on image compression concepts, it was tested in the multiz28way data set and attained a compression rate around 0.93 bits per symbol on the sequence data, better than the ≈∈1 bit per symbol attained by a recently proposed method. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Computation of the normalized compression distance of DNA sequences using a mixture of finite-context models",
        "doc_scopus_id": "84861969601",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861969601",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Comprehensive studies",
            "Compression algorithms",
            "Compression methods",
            "DNA compression",
            "Human chromosomal similarity",
            "Human genomes",
            "Internal models",
            "Normalized-compression distance",
            "Similarity measure"
        ],
        "doc_abstract": "A compression-based similarity measure assesses the similarity between two objects using the number of bits needed to describe one of them when a description of the other is available. For being effective, these measures have to rely on \"normal\" compression algorithms, roughly meaning that they have to be able to build an internal model of the data being compressed. Often, we find that good \"normal\" compression methods are slow and those that are fast do not provide acceptable results. In this paper, we propose a method for measuring the similarity of DNA sequences that balances these two goals. The method relies on a mixture of finite-context models and is compared with other methods, including XM, the state-of-the-art DNA compression technique. Moreover, we present a comprehensive study of the inter-chromosomal similarity of the human genome.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exon: A web-based software toolkit for DNA sequence analysis",
        "doc_scopus_id": "84861206336",
        "doc_doi": "10.1007/978-3-642-28839-5_25",
        "doc_eid": "2-s2.0-84861206336",
        "doc_date": "2012-05-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "DNA compression",
            "DNA sequence analysis",
            "DNA Sequencing",
            "Exponential growth",
            "Genomic-sequence data",
            "On-line analysis",
            "Web-based softwares"
        ],
        "doc_abstract": "Recent advances in DNA sequencing methodologies have caused an exponential growth of publicly available genomic sequence data. By consequence, many computational biologists have intensified studies in order to understand the content of these sequences and, in some cases, to search for association to disease. However, the lack of public available tools is an issue, specially when related to efficiency and usability. In this paper, we present Exon, a user-friendly solution containing tools for online analysis of DNA sequences through compression based profiles. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GReEn: A tool for efficient compression of genome resequencing data",
        "doc_scopus_id": "84857860662",
        "doc_doi": "10.1093/nar/gkr1124",
        "doc_eid": "2-s2.0-84857860662",
        "doc_date": "2012-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Research in the genomic sciences is confronted with the volume of sequencing and resequencing data increasing at a higher pace than that of data storage and communication resources, shifting a significant part of research budgets from the sequencing component of a project to the computational one. Hence, being able to efficiently store sequencing and resequencing data is a problem of paramount importance. In this article, we describe GReEn (Genome Resequencing Encoding), a tool for compressing genome resequencing data using a reference genome sequence. It overcomes some drawbacks of the recently proposed tool GRS, namely, the possibility of compressing sequences that cannot be handled by GRS, faster running times and compression gains of over 100-fold for some sequences. This tool is freely available for non-commercial use at ftp://ftp.ieeta.pt/∼ap/codecs/GReEn1.tar.gz. © 2012 The Author(s).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Symbolic to numerical conversion of DNA sequences using finite-context models",
        "doc_scopus_id": "84863762407",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84863762407",
        "doc_date": "2011-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Information sequences",
            "Numerical information",
            "Numerical representation",
            "Quick Bird",
            "Signal processing technique",
            "Symbolic data",
            "Symbolic sequence"
        ],
        "doc_abstract": "Symbolic sequences can be analysed using two main approaches. One is by means of algorithms specifically designed for processing symbolic sequences. The other uses signal processing techniques, after converting the sequence from symbols to numbers. The latter approach depends on the availability of meaningful numerical representations of the sequences. In this paper, we present a technique that uses finite-context models to generate numerical information sequences from the DNA symbolic data. We give some examples that illustrate the method and show that these information sequences may reveal important structural properties of the DNA sequences. Moreover, the proposed approach is fast, allowing a quick bird's-eye view of whole chromosomes, with the aim of locating potentially interesting regions. © EURASIP, 2011.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Complexity profiles of DNA sequences using finite-context models",
        "doc_scopus_id": "82155191383",
        "doc_doi": "10.1007/978-3-642-25364-5_8",
        "doc_eid": "2-s2.0-82155191383",
        "doc_date": "2011-11-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Compression methods",
            "DNA sequence analysis",
            "DNA sequence data",
            "IMPROVE-A",
            "Information sources",
            "Relative entropy",
            "Source models",
            "Storage spaces",
            "Transmission time"
        ],
        "doc_abstract": "Every data compression method assumes a certain model of the information source that produces the data. When we improve a data compression method, we are also improving the model of the source. This happens because, when the probability distribution of the assumed source model is closer to the true probability distribution of the source, a smaller relative entropy results and, therefore, fewer redundancy bits are required. This is why the importance of data compression goes beyond the usual goal of reducing the storage space or the transmission time of the information. In fact, in some situations, seeking better models is the main aim. In our view, this is the case for DNA sequence data. In this paper, we give hints on how finite-context (Markov) modeling may be used for DNA sequence analysis, through the construction of complexity profiles of the sequences. These profiles are able to unveil structures of the DNA, some of them with potential biological relevance. © 2011 Springer-Verlag Berlin.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compressing the human genome using exclusively Markov models",
        "doc_scopus_id": "80052957011",
        "doc_doi": "10.1007/978-3-642-19914-1_29",
        "doc_eid": "2-s2.0-80052957011",
        "doc_date": "2011-09-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Human genomes",
            "Markov model",
            "Markov models",
            "Markov property",
            "Probability estimate"
        ],
        "doc_abstract": "Models that rely exclusively on the Markov property, usually known as finite-context models, can model DNA sequences without considering mechanisms that take direct advantage of exact and approximate repeats. These models provide probability estimates that depend on the recent past of the sequence and have been used for data compression. In this paper, we investigate some properties of the finite-context models and we use these properties in order to improve the compression. The results are presented using the human genome as example. © 2011 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Bacteria DNA sequence compression using a mixture of finite-context models",
        "doc_scopus_id": "80052249011",
        "doc_doi": "10.1109/SSP.2011.5967637",
        "doc_eid": "2-s2.0-80052249011",
        "doc_date": "2011-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Bacterial genomes",
            "Compression methods",
            "DNA coding",
            "finite-context models",
            "Recursive procedure",
            "Sequence compression"
        ],
        "doc_abstract": "The ability of finite-context models for compressing DNA sequences has been demonstrated on some recent works. In this paper, we further explore this line, proposing a compression method based on eight finite-context models, with orders from two to sixteen, whose probabilities are averaged using weights calculated through a recursive procedure. The method was tested on a total of 2,338 sequences belonging to bacterial genomes, with sizes ranging from 1,286 to 13,033,779 bases, showing better compression results than the state-of-the-art XM DNA coding algorithm and also faster operation. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DNA synthetic sequences generation using multiple competing Markov models",
        "doc_scopus_id": "80052209762",
        "doc_doi": "10.1109/SSP.2011.5967639",
        "doc_eid": "2-s2.0-80052209762",
        "doc_date": "2011-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computational model",
            "DNA compression",
            "Markov model",
            "Sequence generation",
            "Statistical generator",
            "Synthetic DNA",
            "Synthetic sequence"
        ],
        "doc_abstract": "The development and implementation of computational models to represent DNA sequences is a great challenge. Markov models, usually known as finite-context models, have been used for a long time in DNA compression. In a previous work, we have shown that finite-context modelling can also be used for sequence generation. Furthermore, it is known that DNA is better represented by multiple finite-context models. However, the previous generator only allowed a single finite-context model to be used for generating a certain sequence. In this paper, we present results regarding a synthetic DNA generator based on multiple competing finite-context models. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    }
]