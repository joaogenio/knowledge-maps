[
    {
        "doc_title": "Systematic review of question answering over knowledge bases",
        "doc_scopus_id": "85123301494",
        "doc_doi": "10.1049/sfw2.12028",
        "doc_eid": "2-s2.0-85123301494",
        "doc_date": "2022-02-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Critical issues",
            "Data repositories",
            "Natural languages",
            "Querying services",
            "Question Answering",
            "Question answering systems",
            "Semantic data",
            "Semantics Information",
            "Systematic Review",
            "User friendly interface"
        ],
        "doc_abstract": "© 2021 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.Over the years, a growing number of semantic data repositories have been made available on the web. However, this has created new challenges in exploiting these resources efficiently. Querying services require knowledge beyond the typical user’s expertise, which is a critical issue in adopting semantic information solutions. Several proposals to overcome this difficulty have suggested using question answering (QA) systems to provide user-friendly interfaces and allow natural language use. Because question answering over knowledge bases (KBQAs) is a very active research topic, a comprehensive view of the field is essential. The purpose of this study was to conduct a systematic review of methods and systems for KBQAs to identify their main advantages and limitations. The inclusion criteria rationale was English full-text articles published since 2015 on methods and systems for KBQAs. Sixty-six articles were reviewed to describe their underlying reference architectures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Naprt expression regulation mechanisms: Novel functions predicted by a bioinformatics approach",
        "doc_scopus_id": "85121650571",
        "doc_doi": "10.3390/genes12122022",
        "doc_eid": "2-s2.0-85121650571",
        "doc_date": "2021-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Genetics (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2716"
            }
        ],
        "doc_keywords": [
            "Alternative Splicing",
            "Cell Differentiation",
            "Cell Line, Tumor",
            "Computational Biology",
            "Databases, Genetic",
            "Humans",
            "Pentosyltransferases",
            "Transcriptional Activation"
        ],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland.The nicotinate phosphoribosyltransferase (NAPRT) gene has gained relevance in the research of cancer therapeutic strategies due to its main role as a NAD biosynthetic enzyme. NAD metabolism is an attractive target for the development of anti-cancer therapies, given the high energy requirements of proliferating cancer cells and NAD-dependent signaling. A few studies have shown that NAPRT expression varies in different cancer types, making it imperative to assess NAPRT expression and functionality status prior to the application of therapeutic strategies targeting NAD. In addition, the recent finding of NAPRT extracellular form (eNAPRT) suggested the involvement of NAPRT in inflammation and signaling. However, the mechanisms regulating NAPRT gene expression have never been thoroughly addressed. In this study, we searched for NAPRT gene expression regulatory mechanisms in transcription factors (TFs), RNA binding proteins (RBPs) and microRNA (miRNAs) databases. We identified several potential regulators of NAPRT transcription activation, downregulation and alternative splicing and performed GO and expression analyses. The results of the functional analysis of TFs, RBPs and miRNAs suggest new, unexpected functions for the NAPRT gene in cell differentiation, development and neuronal biology.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BIcenter: A collaborative Web ETL solution based on a reflective software approach",
        "doc_scopus_id": "85120002557",
        "doc_doi": "10.1016/j.softx.2021.100892",
        "doc_eid": "2-s2.0-85120002557",
        "doc_date": "2021-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "As extraction",
            "Collaborative pipeline",
            "Collaborative webs",
            "Efficient strategy",
            "Extraction transformation and loadings",
            "New sources",
            "Pentaho data integration",
            "Software approach",
            "Sources of informations",
            "Web extraction"
        ],
        "doc_abstract": "© 2021 The Author(s)The continuous growth of new sources of information has led to an unprecedented increase in the data collected. The dimensionality and heterogeneity of these data requires efficient strategies for searching, accessing and integrating from multiple repositories. The techniques underlying this goal are usually known as Extraction, Transformation and Loading (ETL) pipelines, which aim to organise dispersed data into a common structure. However, despite their popularity and widespread use, these pipelines present a few drawbacks in specific scenarios. In clinical research, for instance, it is quite common to engage multiple researchers, institutions and datasets so that the study findings can have higher impact. This implies cooperation between several entities to design the workflow, even when these entities do not have permission to work directly with the source data, due to privacy and regulatory issues. Furthermore, extending the pipeline to other data sources requires adding new concepts and rules over time, which implies continuous updating of the ETL scripts. This paper presents a collaborative web-based ETL application that allows users to design, share and execute ETL pipelines, across multiple centres. The system is supported by a user-friendly interface in which non-technical users can build the ETL pipelines without the need to grasp the ETL details, and most importantly, without having direct access to the data.",
        "available": true,
        "clean_text": "serial JL 312019 291210 291690 291735 291791 291848 31 90 SoftwareX SOFTWAREX 2021-11-27 2021-11-27 2021-11-27 2021-11-27 2021-12-17T11:55:31 S2352-7110(21)00149-7 S2352711021001497 10.1016/j.softx.2021.100892 S300 S300.1 FULL-TEXT 2022-03-03T05:18:19.676202Z 0 0 20211201 20211231 2021 2021-11-27T11:12:01.405432Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor orcid primabst pubtype ref 2352-7110 23527110 UNLIMITED FCT true 16 16 C Volume 16 39 100892 100892 100892 202112 December 2021 2021-12-01 2021-12-31 2021 Original Software Publication simple-article osp © 2021 The Author(s). Published by Elsevier B.V. BICENTERACOLLABORATIVEWEBETLSOLUTIONBASEDAREFLECTIVESOFTWAREAPPROACH ALMEIDA J 1 Motivation and significance 2 Software description 2.1 Software architecture 2.2 Software functionalities 2.2.1 ETL task editor 2.2.2 Pipeline execution 2.2.3 ETL tasks extensibility 2.2.4 Multi-institutional access control 2.2.5 Collaborative environment 3 Illustrative examples 3.1 Biomarkers from mass spectrometry 3.2 Health databases 4 Impact 5 Conclusions Acknowledgements References KHAN 2017 1 6 M 2017IEEEINTERNATIONALCONFERENCECOMMUNICATIONS BIGDATACHALLENGESOPPORTUNITIESINHYPEINDUSTRY40 ZHUANG 2017 3 14 Y VASSAKIS 2018 3 20 K MOBILEBIGDATA BIGDATAANALYTICSAPPLICATIONSPROSPECTSCHALLENGES ZICARI 2014 103 R BHADANI 2016 1 24 A EFFECTIVEBIGDATAMANAGEMENTOPPORTUNITIESFORIMPLEMENTATION BIGDATACHALLENGESOPPORTUNITIESREALITIES RIKHARDSSON 2018 37 58 P ZHENG 2017 67 82 J MARIANI 2018 M CHANDRA 2018 217 224 P ALI 2018 43 49 A 2018IEEE3RDINTERNATIONALCONFERENCEBIGDATAANALYSIS REALTIMEBIGDATAWAREHOUSINGANALYSISFRAMEWORK HASSAN 2019 3 26 M SECURITYINSMARTCITIESMODELSAPPLICATIONSCHALLENGES BIGDATACHALLENGESOPPORTUNITIESINHEALTHCAREINFORMATICSSMARTHOSPITALS BISWAS 2020 53 61 N KHERDEKAR 2016 2557 2559 V HRIPCSAK 2016 7329 7336 G MAROTI 2014 2432 2441 M CASTERS 2010 M PENTAHOKETTLESOLUTIONSBUILDINGOPENSOURCEETLSOLUTIONSPENTAHODATAINTEGRATION WANG 2020 23 26 M HRIPCSAK 2015 574 G ALMEIDA 2019 466 473 J HEALTHINF STRATEGIESACCESSPATIENTCLINICALDATADISTRIBUTEDDATABASES AORAFAELALMEIDA 2021 100760 J ALMEIDA 2021 103849 J ALMEIDAX2021X100892 ALMEIDAX2021X100892XJ Full 2021-11-04T23:52:34Z FundingBody Portugal Institutes This is an open access article under the CC BY-NC-ND license. © 2021 The Author(s). Published by Elsevier B.V. 2021-12-03T02:45:04.223Z 806968 IMI Innovative Medicines Initiative EU/EFPIA National Science Foundation NSF National Science Foundation FCT SFRH/BD/147837/2019 FCT Fundação para a Ciência e a Tecnologia This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968 . J. R. A. is funded by the National Science Foundation (FCT), Portugal , under the grant SFRH/BD/147837/2019 . 0 item S2352-7110(21)00149-7 S2352711021001497 10.1016/j.softx.2021.100892 312019 2022-03-03T05:18:19.676202Z 2021-12-01 2021-12-31 UNLIMITED FCT true 1011093 MAIN 7 51505 849 656 IMAGE-WEB-PDF 1 gr3 24334 244 602 gr1 34282 195 602 gr4 20383 182 659 fx1001 45767 500 817 gr5 39117 301 602 gr2 29757 283 565 gr3 3625 89 219 gr1 4871 71 219 gr4 2231 61 219 fx1001 3158 134 219 gr5 6091 110 219 gr2 5377 110 219 gr3 182854 1081 2667 gr1 259456 862 2666 gr4 177918 806 2916 fx1001 190838 1329 2171 gr5 390968 1334 2667 gr2 315038 1253 2500 am 9083280 SOFTX 100892 100892 S2352-7110(21)00149-7 10.1016/j.softx.2021.100892 The Author(s) Fig. 1 System architecture with 4 main components: client side; application server; database server; and Kettle server. Fig. 2 ETL pipeline implemented in BIcenter to process a dataset collected from a synthetic weather station. Fig. 3 BIcenter interface for the filter component. Fig. 4 BIcenter interface to display the performance metrics after running the weather station ETL pipeline. Fig. 5 ETL pipeline to calculate biomarkers averages. Original software publication BIcenter: A collaborative Web ETL solution based on a reflective software approach João R. Almeida a b ⁎ Leonardo Coelho a José L. Oliveira a a Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro Aveiro Portugal Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal b Department of Information and Communications Technologies, University of A Coruña, A Coruña, Spain Department of Information and Communications Technologies, University of A Coruña A Coruña Spain Department of Information and Communications Technologies, University of A Coruña, A Coruña, Spain ⁎ Corresponding author at: Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal. Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro Aveiro Portugal The continuous growth of new sources of information has led to an unprecedented increase in the data collected. The dimensionality and heterogeneity of these data requires efficient strategies for searching, accessing and integrating from multiple repositories. The techniques underlying this goal are usually known as Extraction, Transformation and Loading (ETL) pipelines, which aim to organise dispersed data into a common structure. However, despite their popularity and widespread use, these pipelines present a few drawbacks in specific scenarios. In clinical research, for instance, it is quite common to engage multiple researchers, institutions and datasets so that the study findings can have higher impact. This implies cooperation between several entities to design the workflow, even when these entities do not have permission to work directly with the source data, due to privacy and regulatory issues. Furthermore, extending the pipeline to other data sources requires adding new concepts and rules over time, which implies continuous updating of the ETL scripts. This paper presents a collaborative web-based ETL application that allows users to design, share and execute ETL pipelines, across multiple centres. The system is supported by a user-friendly interface in which non-technical users can build the ETL pipelines without the need to grasp the ETL details, and most importantly, without having direct access to the data. Keywords ETL Collaborative pipeline Pentaho Data Integration Code metadata Current code version 1.0.1 Permanent link to code/repository used for this code version Legal Code License MIT Code versioning system used git Software code languages, tools, and services used Scala and Java Play Framework Compilation requirements, operating environments & dependencies Docker If available Link to developer documentation/manual Support email for questions joao.rafael.almeida@ua.pt 1 Motivation and significance An unprecedent amount of data is being generated in many economic sectors, e.g., industry 4.0 and e-health, raising new challenges regarding data collection [1]. The potential value of all these data also led to an increasing interest in big data solutions and data-driven decision-making tools [2]. Despite many efforts in this area already, especially in deep learning algorithms, the process of converting different data sources into a heterogeneous and interoperable repository still has some issues related to data complexity, scalability, timeliness, and privacy policies [3]. Big Data is usually defined as the daily basis production of large volumes of either structured or unstructured data. These volumes of data are noisy and contain a significant amount of invalid or corrupted records that should be discarded [4]. These cleaning operations are usually performed following ad-hoc approaches, which are difficult to generalise. Another challenge is the amount of unstructured data that despite containing valuable information needs to be structured to enable the usage of analytical methods [5]. However, the major challenge in this topic can be data integration when considering multiple heterogeneous data sources. Overcoming these challenges can lead to improvements in several business domains [4,5]. A strategy adopted by some organisations to analyse internal data or from external sources is based on the use of Business Intelligence (BI) tools [6]. BI is a domain that incorporates applications and methodologies aiming to collect, prepare and explore data from diverse sources of information. These tools enable the analytical exploration of data, which can result in reports and dashboards for data visualisation [7]. This concept is focused on access to and exploration of heterogeneous data sources in order to have more information about the business and to make better informed decisions [8]. Currently, BI is used as a hypernym that covers the domains of Data Warehousing (DW) [8], which is the consolidation of data from heterogeneous sources and it can define the foundations of BI methodologies. Most large and medium-sized organisations are currently adopting DW systems to support their BI tools [9]. However, the process of integrating and transforming the data into a data warehouse is time-consuming and requires human validation, independently of the data domains [9–11]. This integration process, usually denominated as ETL (Extract, Transform and Load), is a workflow aiming to collect the raw data and process them through three distinct stages: (1) Extraction, where the data are accessed from heterogeneous sources; (2) Transformation, which manipulates and converts the loaded data into the desired form; and (3) Loading, to store the resulting data into the target database. These operations are processed at the database level and they can be coded using a programming language. However, with the growing complexity of these procedures and the need to involve multiple entities in the design of these workflows, more user-friendly approaches were created [12]. Some of these approaches are focused on documenting the process, while others were specially designed to have a Graphical User Interface (GUI) to specify the ETL workflows [13]. The most relevant and complete tools aiming to simplify the design and creation of ETL processes are Talend Open Studio (TOS) and Pentaho Data Integration (PDI or Kettle) [12,13]. Although these tools enable a good perception of ETL procedures by people with fewer technical skills, these were developed as desktop-based tools, which is a limiting factor for specific requirements. This lack of flexibility in users’ collaboration during the design and definition of the ETL pipelines is a problem for some application domains. For instance, in the medical scenario, when clinical data need to be harmonised into a common data schema for study purposes, this requires collaboration between the technical teams and the medical researchers to define the data of interest [14]. This collaboration is required in different stages: (1) design; (2) implementation; and (3) validation. Desktop-based tools have been used in the ETL domain for years and have been sufficient to build ETL workflows. These tools may have application programming interfaces (API) to empower their backend components. Examples of such open-source ETL projects with the possibility of publicly opening their APIs are Pentaho, Talend and CloverETL. However, in other domains of software development, web solutions are more likely to provide a collaborative environment [15]. An example of a web-based graphical ETL designer tool that uses Pentaho’s ETL API as a backend engine is SpoonWeb. This tool supports approximately half of the core ETL components that PDI offers. However, there is a great effort in maintaining and developing new ETL components for this tool. The objective of BIcenter is the implementation of a reflexive software architecture that enables a simple and dynamic representation of ETL components. This fully web-based platform allows users with low technical knowledge to build and manage ETL processes within several institutions. In addition, users can use a shared environment to build ETL tasks collaboratively. The proposed solution eliminates deployment and maintenance issues by using a single installation and can serve multiple users and institutions. 2 Software description BIcenter is a web-based ETL tool that covers some limitations and problems currently found in building and managing ETL tasks in multi-institution environments. This tool simplifies the description of ETL workflows and helps users without technical expertise to understand such workflows through an intuitive GUI. BIcenter replicates the Kettle features in an HTML5 browser and simplifies some of procedures in Kettle that may require deep technical knowledge of this tool. 2.1 Software architecture The system follows a client–server model, with an architecture that considers four different tiers (Fig. 1). The client side was developed aiming to provide a responsive web application that allows building ETL pipelines and configuration of each individual pipeline step. The drawing features rely on mxGraph components, which communicate with a service on the application side that converts the stored information of the ETL processes into mxGraphModels. To have cross-device support, the platform uses AdminLTE, a fully responsive template based on the Bootstrap Framework that dynamically adjusts the visual components in order to fit in different screen resolutions. The application server, developed using the PlayFramework, controls all application functionalities, maintains the system business logic, and provides a service layer to support the client side. This component communicates with the database server using Java Persistence API (JPA). The MySQL database stores all the system information, including the ETL pipelines and their status, namely execution history, performance metrics and possible issues. To extract this information and to have the system communicating with the Kettle instances, which run autonomously, a Data Integration SDK (DI SDK) was developed. This SDK contains the methods required to build and execute Kettle’s ETL processes. The DI SDK can represent any ETL process using six classes, similar to Kettle [16]. These classes are the following: • TransMeta: class that defines the information about the ETL process and offers methods to save and load these processes to and from XML. It also defines the methods to alter an ETL process, by adding and removing databases, steps, hops, among other components. • Trans: represents the information and operations associated with the concept of an ETL process. This class can load, initialise, run, and monitor execution of the ETL process. • DatabaseMeta: defines the database specific parameters for a certain database type. • StepMeta: is the class that defines the information about a process of an ETL Step. • TransHopMeta: defines a link between two Steps in an ETL process. • BaseStep: represents the information and operations associated with the process of an ETL Step. This class contains methods for initialisation, row processing, and step clean-up. 2.2 Software functionalities BIcenter contains the usual features available on the most popular solutions designed for ETL pipelines, mainly due to being constructed on top of a Kettle instance. However, this system was developed aiming to fill some of the existent gaps in these tools. In this section, we highlight some of the major characteristics of BIcenter. 2.2.1 ETL task editor A common but simple use case to demonstrate the operation of an ETL solution is the processing of data retrieved from a weather station. Therefore, we used a rainfall dataset to demonstrate the basic ETL features in this tool. The ETL pipeline implemented on BIcenter is represented in Fig. 2, and the goal of this pipeline is to count the number of rainy days by month and year. The ETL Steps presented in this pipeline were configured in the web interface. Fig. 3 represents the interface to define the ETL Step to filter the data by rows. This component allows the definition of conditions to be applied in the filter and returns the information that fits in the condition as true to the step designed “Sort By Date”. The data that do not fit in this condition are returned to the step “Sunny Days”. After defining the ETL pipeline, it is possible to run it and also analyse the execution history. This history provides information about each execution state and the current status of each pipeline’s steps. In addition, it is possible to check the performance metrics of the pipeline, detailed by each component. In Fig. 4, these metrics are shown for the pipeline defined in Fig. 2. 2.2.2 Pipeline execution BIcenter can execute the ETL pipelines in local databases or in private and remote servers, without the need for a new local installation. The connection details for these databases or for the remote servers are associated with each institution, and the access of these connections is controlled. When connected directly to a local database, it is necessary to define a data source in the system through a form that generates the connection link between BIcenter and the database. The ETL pipelines are then executed using the databases defined. The private and remote servers have a different behaviour. These servers aim to ensures data protection and isolation, when dealing with sensitive data. Therefore, it was developed using Carte, which is a lightweight HTTP server available on Kettle that allows remote and parallel execution of ETL tasks. BIcenter can perform authenticated requests to the servers that are running Carte. These requests contain the definition of the ETL tasks to be executed. Carte also contains clustering features, enabling a single transformation to be divided and executed in parallel by multiple machines that are running a Carte server. BIcenter contains mechanisms to simplify the process of sending commands to control the deployment, management and monitoring of transformations on the Carte slave server. 2.2.3 ETL tasks extensibility Although Kettle already contains a set of ETL operations, there are always specific scenarios that may require implementation of a new component. BIcenter is deployed by default containing the most common ETL components available in Kettle. However, the system was developed with the objective of supporting the addition of new tasks without requiring the development of additional code, i.e., if a new task is developed on the Kettle instance, BIcenter can recognise it through its definition. These definitions are maintained in a JSON file which is automatically processed during the application start-up. For instance, in Code snippet 1, we show the JSON configuration to add the SortRows task on a BIcenter instance. The definition of a new ETL task using this format requires the component properties and metadata. This setup procedure should be made by an entity with solid knowledge about the Kettle task. However, after defining the new component in the system, this is available to be used by non-technical users in the web interface. 2.2.4 Multi-institutional access control Users can have different roles in the application and can also belong to different institutions. Therefore, rule-based access control (RBAC) mechanisms were incorporated, in which each role maintains a set of permissions. These permissions consist in an association of an operation to a resource. The authentication entity is a facade to a given user group, that can use lightweight directory access protocol (LDAP) or active directory (AD) services. When a user accesses the platform, the underlying user group is determined by trying to authenticate on each configured user group. If authentication succeeds, the user can be instantiated in the database. Depending on the group to which users belong, they may acquire the corresponding roles and institution access. The mechanisms to access and manage the ETL tasks and institutions can be characterised into four distinct types of users: • administrator: entity responsible for moderating the platform. This role contains permissions to create and delete institutions, and manage all the features associated to an institution. • resource manager: entity capable of managing private data sources and execution servers. This role has permissions to create and delete private data sources and execution servers, within specific institutions. • task manager: this entity can build and execute ETL tasks, and is capable of accessing the ETL Task Editor to create and configure ETL tasks, within specific institutions. • data analyst: this is the most limited role, and can inspect task execution history, namely the resulting data, execution logs and performance metrics. 2.2.5 Collaborative environment The ETL Task Editor is the workspace where it is possible to design and define the ETL pipelines. Each pipeline is created for a specific institution, being contained in a closed environment. Therefore, users who have the task manager role within an institution can work in the same ETL pipeline simultaneously. BIcenter does not create real-time working sessions, instead, it provides an environment in which multiple users can contribute to the construction of the ETL process. The goal of this feature is to allow users to access the most updated ETL pipelines without requiring any additional synchronisation procedure. 3 Illustrative examples The major function of this application is to provide a collaborative and private environment with a set of components capable of creating an ETL pipeline. In this section, we present two scenarios focused on showing: (1) a practical scenario, which benefits from the remote execution; and (2) a use case that requires collaboration between several entities to define and run the ETL pipelines. 3.1 Biomarkers from mass spectrometry Mass spectrometry is a physical analytical technique to detect and identify molecules of interest by measuring the mass-to-charge ratio of one or more molecules. These measurements can be used to determine the exact molecular weight of the sample components [17]. This procedure allows the identification of molecules of interest through analysis of the biomarkers present in the samples. In practice, this requires a set of test tubes to be placed in a plate to be applied for screening in the laboratory. However, to fine-tune the data machine, the biomarkers’ mean values need to be calculated periodically. The data for this scenario are protected in the institutions’ facilities, but using BIcenter, we were able to implement the ETL pipeline to calculate the biomarkers’ averages and define a schedule to execute it automatically. Fig. 5 shows the implementation of this pipeline. In the first task, the plates are filtered by date. The output resulting from this filter is merged with the screening values. Then, using several ETL Steps, the average of the values of the selected biomarkers is calculated. The final task writes this information to a database table. This pipeline was easily defined in the web browser and executed remotely in the private servers. 3.2 Health databases The previous example had the goal of presenting a use case, in which we used BIcenter to run ETL pipelines automatically in a schedule and in a protected environment. This example describes a use case in which the collaborative features are essential to optimise the ETL design and validation. OHDSI (Observational Health Data Sciences and Informatics) 1 1 is an international organisation aiming to develop methodologies to support large-scale observational studies in health care data. This organisation was initiated as an outcome of the OMOP (Observational Medical Outcomes Partnership) project, to continue the research started about performing observational studies worldwide. These studies are possible if the institutions involved create an observational database, which contains the data stored in the Electronic Health Record (EHR) systems migrated to a common data schema designed as OMOP CDM (Common Data Model) [18]. The current strategy to migrate the databases makes use of three tools developed in the OHDSI context. These tools are used in meetings between the data owners and the specialised people in the OHDSI ecosystem to document guidelines regarding the ETL pipeline. The output of these tools is used as the requirements by the technical teams to implement the ETL code using a programming language. Although this strategy works in this context, it is time-consuming and not very flexible when the target database needs to be updated. The BIcenter collaborative environment simplifies the involvement of these teams during the design of the ETL procedures. The entities involved define the necessary tasks using the graphical interface to migrate the EHR data into the OMOP CDM database, and in the end, the support of the technical team is reduced without the need to implement the ETL code. Another positive aspect is the simplified maintenance of the ETL pipelines, i.e., these pipelines need to be executed from time to time, in order to keep the observational database updated. However, some new adjustments may be needed in order to incorporate new changes that may arise in the source database, for instance, a new drug that is not yet mapped for its standard definition. Through this collaborative web-based tool, this update and execution can be remotely controlled. 4 Impact Many software applications aim to conduct ETL workflows. Although some of these are currently being used in complex scenarios, they lack flexibility in collaborative environments. BIcenter aims to fill some of these gaps by providing similar features as well established ETL tools, e.g., Kettle. The collaborative environment was a core requirement to develop this application, allowing the same workflow to be defined and shared within a team as well as allowing the pipelines to be executed in multiple institutions. The harmonisation of health data into a common data schema is one scenario that can benefit from this approach. Currently, some initiatives aim to reuse health data to perform analysis of the data in clinical studies [14]. Some of these initiatives include the creation of a network of databases from several institutions [19]. This requires a common data schema and the source data need to be processed in ETL workflows. However, the data owners do not know the target data schema or how to map their database onto this schema. On the other hand, the teams specialised in the target data schema usually do not know the source data or how to map the medical concepts onto their standard definition. The current solutions for this problem are not flexible and do not provide a collaborative environment. The collaborative environment provided by BIcenter can solve this teamwork problem in this and similar scenarios. In a previous work [20], we proposed a methodology aiming to harmonise different cohorts into a standard data schema. This schema, based on the OHDSI Common Data Model, was used in Alzheimer’s Disease cohorts, combining a total of 6669 subjects and 172 clinical concepts. However, considering it was developed in python for a special need, it requires technical support to cope with every new requirement. To avoid this dependency, we migrate the core engine into BIcenter, where we are able to obtain the same harmonisation pipelines, using its Web Task editor. In this way, we expect that end users will be able to build and manage easily their own data transformation procedures. The same strategy was used to upgrade a drug annotator pipeline that identifies drug mentions in clinical notes and exports them to an OMOP CDM database [21]. BIcenter was developed not to only replace tools currently used for ETL workflows, but to extend the diversity of solutions using a web-based approach. To remain compatible with existing tools, it takes advantage of Kettle features to manage the ETL processes locally, instead of building a new core. Despite having a collaborative environment, BIcenter also allows the execution and definition of ETL workflows remotely, without accessing the source and target servers. This strategy simplifies the management of ETL workflows and the strategy used in the implementation provides a layer of security to access private servers. These features simplify the tasks for IT teams responsible for handling data. The software is now released as open-source, however, it can be used in commercial settings as a supportive tool. Currently, there is an initiative in an European project to incentive health institutions to adopt a common data model as mentioned before. In this project, some small and mid-size enterprises (SMEs) received formation to help these institutions to migrate their data into this new schema. BIcenter may be a great asset for this task, supporting all the stages of the ETL process. 5 Conclusions We present BIcenter, a web-based ETL solution that replicates the components available in the Kettle API, using a metadata description, in JSON, and a reflective approach, using Java Reflection. In this way, the addition of new components can be performed without any further coding. The main goal in developing it was to create a collaborative and easy-to-use ETL pipeline environment, without losing the well-established features already available in desktop tools. The final product offers a simple to use workspace to build and run the ETL processes, without the need to perform a new system deployment. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. J. R. A. is funded by the National Science Foundation (FCT), Portugal , under the grant SFRH/BD/147837/2019. References [1] Khan M. Wu X. Xu X. Dou W. Big data challenges and opportunities in the hype of industry 4.0 2017 IEEE international conference on communications 2017 IEEE 1 6 10.1109/ICC.2017.7996801 M. Khan, X. Wu, X. Xu, W. Dou, Big data challenges and opportunities in the hype of Industry 4.0, in: 2017 IEEE International Conference on Communications (ICC), IEEE, 2017, pp. 1–6. doi:10.1109/ICC.2017.7996801. [2] Zhuang Y. Wu F. Chen C. Pan Y. Challenges and opportunities: from big data to knowledge in AI 2.0 Front Inf Technol Electron Eng 18 1 2017 3 14 10.1631/FITEE.1601883 Y. Zhuang, F. Wu, C. Chen, Y. Pan, Challenges and opportunities: from big data to knowledge in AI 2.0, Frontiers of Information Technology & Electronic Engineering 18 (1) (2017) 3–14. doi:10.1631/FITEE.1601883. [3] Vassakis K. Petrakis E. Kopanakis I. Big data analytics: Applications, prospects and challenges Mobile big data 2018 Springer 3 20 10.1007/978-3-319-67925-9_1 K. Vassakis, E. Petrakis, I. Kopanakis, Big Data Analytics: Applications, Prospects and Challenges, in: Mobile big data, Springer, 2018, pp. 3–20. doi:10.1007/978-3-319-67925-9_1. [4] Zicari R.V. Big data: Challenges and opportunities Big Data Comput 564 2014 103 R. V. Zicari, Big Data: Challenges and Opportunities, Big data computing 564 (2014) 103. [5] Bhadani A.K. Jothimani D. Big data: Challenges, opportunities, and realities Effective big data management and opportunities for implementation 2016 IGI Global 1 24 10.4018/978-1-5225-0182-4.ch001 A. K. Bhadani, D. Jothimani, Big Data: Challenges, Opportunities, and Realities, in: Effective big data management and opportunities for implementation, IGI Global, 2016, pp. 1–24. doi:10.4018/978-1-5225-0182-4.ch001. [6] Rikhardsson P. Yigitbasioglu O. Business intelligence & analytics in management accounting research: Status and future focus Int J Account Inf Syst 29 2018 37 58 10.1016/j.accinf.2018.03.001 P. Rikhardsson, O. Yigitbasioglu, Business intelligence & analytics in management accounting research: Status and future focus, International Journal of Accounting Information Systems 29 (2018) 37–58. doi:10.1016/j.accinf.2018.03.001. [7] Zheng J.G. Data visualization for business intelligence Glob Bus Intell 2017 67 82 10.4324/9781315471136-6 J. G. Zheng, Data visualization for business intelligence, Global Business Intelligence (2017) 67–82 doi:10.4324/9781315471136-6. [8] Mariani M. Baggio R. Fuchs M. Höepken W. Business intelligence and big data in hospitality and tourism: a systematic literature review Int J Contemp Hosp Manag 2018 10.1108/IJCHM-07-2017-0461 M. Mariani, R. Baggio, M. Fuchs, W. Höepken, Business intelligence and big data in hospitality and tourism: a systematic literature review, International Journal of Contemporary Hospitality Management doi:10.1108/IJCHM-07-2017-0461. [9] Chandra P. Gupta M.K. Comprehensive survey on data warehousing research Int J Inf Technol 10 2 2018 217 224 10.1007/s41870-017-0067-y P. Chandra, M. K. Gupta, Comprehensive survey on data warehousing research, International Journal of Information Technology 10 (2) (2018) 217–224. doi:10.1007/s41870-017-0067-y. [10] Ali A.R. Real-time big data warehousing and analysis framework 2018 IEEE 3rd international conference on big data analysis 2018 IEEE 43 49 10.1109/ICBDA.2018.8367649 A. R. Ali, Real-time big data warehousing and analysis framework, in: 2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA), IEEE, 2018, pp. 43–49. doi:10.1109/ICBDA.2018.8367649. [11] Hassan M.K. El Desouky A.I. Elghamrawy S.M. Sarhan A.M. Big data challenges and opportunities in healthcare informatics and smart hospitals Security in smart cities: Models, applications, and challenges 2019 Springer 3 26 10.1007/978-3-030-01560-2_1 M. K. Hassan, A. I. El Desouky, S. M. Elghamrawy, A. M. Sarhan, Big Data Challenges and Opportunities in Healthcare Informatics and Smart Hospitals, in: Security in smart cities: Models, applications, and challenges, Springer, 2019, pp. 3–26. doi:10.1007/978-3-030-01560-2_1. [12] Biswas N. Sarkar A. Mondal K.C. Efficient incremental loading in ETL processing for real-time data integration Innov Syst Softw Eng 16 1 2020 53 61 10.1007/s11334-019-00344-4 N. Biswas, A. Sarkar, K. C. Mondal, Efficient incremental loading in ETL processing for real-time data integration, Innovations in Systems and Software Engineering 16 (1) (2020) 53–61. doi:10.1007/s11334-019-00344-4. [13] Kherdekar V.A. Metkewar P.S. A technical comprehensive survey of ETL tools Int J Appl Eng Res 11 4 2016 2557 2559 10.37622/IJAER/11.4.2016.2557-2559 V. A. Kherdekar, P. S. Metkewar, A technical comprehensive survey of etl tools, International Journal of Applied Engineering Research 11 (4) (2016) 2557–2559. doi:10.37622/IJAER/11.4.2016.2557-2559. [14] Hripcsak G. Ryan P.B. Duke J.D. Shah N.H. Park R.W. Huser V. Characterizing treatment pathways at scale using the OHDSI network Proc Natl Acad Sci 113 27 2016 7329 7336 10.1073/pnas.1510502113 G. Hripcsak, P. B. Ryan, J. D. Duke, N. H. Shah, R. W. Park, V. Huser, M. A. Suchard, M. J. Schuemie, F. J. DeFalco, A. Perotte, et al., Characterizing treatment pathways at scale using the OHDSI network, Proceedings of the National Academy of Sciences 113 (27) (2016) 7329–7336. doi:10.1073/pnas.1510502113. [15] Maróti M. Kereskényi R. Kecskés T. Völgyesi P. Lédeczi A. Online collaborative environment for designing complex computational systems Procedia Comput Sci 29 2014 2432 2441 10.1016/j.procs.2014.05.227 M. Maróti, R. Kereskényi, T. Kecskés, P. Völgyesi, A. Lédeczi, Online collaborative environment for designing complex computational systems, Procedia Computer Science 29 (2014) 2432–2441. doi:10.1016/j.procs.2014.05.227. [16] Casters M. Bouman R. Van Dongen J. Pentaho kettle solutions: Building open source ETL solutions with pentaho data integration 2010 John Wiley & Sons M. Casters, R. Bouman, J. Van Dongen, Pentaho Kettle solutions: building open source ETL solutions with Pentaho Data Integration, John Wiley & Sons, 2010. [17] Wang M. Jarmusch A.K. Vargas F. Aksenov A.A. Gauglitz J.M. Weldon K. Mass spectrometry searches using MASST Nature Biotechnol 38 1 2020 23 26 10.1038/s41587-019-0375-9 M. Wang, A. K. Jarmusch, F. Vargas, A. A. Aksenov, J. M. Gauglitz, K. Weldon, D. Petras, R. da Silva, R. Quinn, A. V. Melnik, et al., Mass spectrometry searches using MASST, Nature biotechnology 38 (1) (2020) 23–26. doi:10.1038/s41587-019-0375-9. [18] Hripcsak G. Duke J.D. Shah N.H. Reich C.G. Huser V. Schuemie M.J. Observational health data sciences and informatics (OHDSI): opportunities for observational researchers Stud Health Technol Inf 216 2015 574 10.3233/978-1-61499-564-7-574 G. Hripcsak, J. D. Duke, N. H. Shah, C. G. Reich, V. Huser, M. J. Schuemie, M. A. Suchard, R. W. Park, I. C. K. Wong, P. R. Rijnbeek, et al., Observational Health Data Sciences and Informatics (OHDSI): opportunities for observational researchers, Studies in health technology and informatics 216 (2015) 574. doi:10.3233/978-1-61499-564-7-574. [19] Almeida J.R. Fajarda O. Pereira A. Oliveira J.L. Strategies to access patient clinical data from distributed databases HEALTHINF 2019 466 473 10.5220/0007576104660473 J. R. Almeida, O. Fajarda, A. Pereira, J. L. Oliveira, Strategies to Access Patient Clinical Data from Distributed Databases, in: HEALTHINF, 2019, pp. 466–473. doi:10.5220/0007576104660473. [20] ao Rafael Almeida J. ao Silva L.B. Bos I. Visser P.J. Oliveira J.L. A methodology for cohort harmonisation in multicentre clinical research Inf Med Unlocked 27 2021 100760 10.1016/j.imu.2021.100760 J. R. Almeida, L. B. Silva, I. Bos, P. J. Visser, J. L. Oliveira, A methodology for cohort harmonisation in multicentre clinical research, Informatics in Medicine Unlocked Volume 27 (2021) 100760. doi:10.1016/j.imu.2021.100760. [21] Almeida J.a.R. Silva J.a.F. Matos S. Oliveira J.L. A two-stage workflow to extract and harmonize drug mentions from clinical notes into observational databases J Biomed Inform 120 2021 103849 10.1016/j.jbi.2021.103849 J. R. Almeida, J. F. Silva, S. Matos, J. L. Oliveira, A two-stage workflow to extract and harmonize drug mentions from clinical notes into observational databases, Journal of Biomedical Informatics 120 (2021) 103849. doi:10.1016/j.jbi.2021.103849. "
    },
    {
        "doc_title": "A two-stage workflow to extract and harmonize drug mentions from clinical notes into observational databases",
        "doc_scopus_id": "85109166153",
        "doc_doi": "10.1016/j.jbi.2021.103849",
        "doc_eid": "2-s2.0-85109166153",
        "doc_date": "2021-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Biomedical science",
            "Common data model",
            "Computational analysis",
            "Database schemas",
            "Electronic health record",
            "Observational study",
            "Standard definitions",
            "Unstructured data",
            "Databases, Factual",
            "Delivery of Health Care",
            "Electronic Health Records",
            "Humans",
            "Pharmaceutical Preparations",
            "Workflow"
        ],
        "doc_abstract": "© 2021 Elsevier Inc.Background: The content of the clinical notes that have been continuously collected along patients’ health history has the potential to provide relevant information about treatments and diseases, and to increase the value of structured data available in Electronic Health Records (EHR) databases. EHR databases are currently being used in observational studies which lead to important findings in medical and biomedical sciences. However, the information present in clinical notes is not being used in those studies, since the computational analysis of this unstructured data is much complex in comparison to structured data. Methods: We propose a two-stage workflow for solving an existing gap in Extraction, Transformation and Loading (ETL) procedures regarding observational databases. The first stage of the workflow extracts prescriptions present in patient's clinical notes, while the second stage harmonises the extracted information into their standard definition and stores the resulting information in a common database schema used in observational studies. Results: We validated this methodology using two distinct data sets, in which the goal was to extract and store drug related information in a new Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) database. We analysed the performance of the used annotator as well as its limitations. Finally, we described some practical examples of how users can explore these datasets once migrated to OMOP CDM databases. Conclusion: With this methodology, we were able to show a strategy for using the information extracted from the clinical notes in business intelligence tools, or for other applications such as data exploration through the use of SQL queries. Besides, the extracted information complements the data present in OMOP CDM databases which was not directly available in the EHR database.",
        "available": true,
        "clean_text": "serial JL 272371 291210 291682 291870 291901 31 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2021-06-30 2021-06-30 2021-07-06 2021-07-06 2021-08-12T07:55:59 S1532-0464(21)00178-7 S1532046421001787 10.1016/j.jbi.2021.103849 S300 S300.1 FULL-TEXT 2021-08-12T07:17:07.657509Z 0 0 20210801 20210831 2021 2021-06-30T08:53:51.992008Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst pubtype ref specialabst 1532-0464 15320464 true 120 120 C Volume 120 17 103849 103849 103849 202108 August 2021 2021-08-01 2021-08-31 2021 Original research papers article fla © 2021 Elsevier Inc. All rights reserved. ATWOSTAGEWORKFLOWEXTRACTHARMONIZEDRUGMENTIONSCLINICALNOTESOBSERVATIONALDATABASES ALMEIDA J 1 Introduction 2 Related work 2.1 Extracting patient information from clinical notes 2.2 Data harmonisation into relational models 3 Methods 3.1 Clinical notes analysis 3.1.1 Annotating clinical notes 3.1.2 Post-processing 3.1.2.1 Annotation disambiguation 3.1.2.2 Extraction of additional drug information 3.1.3 Storing extracted information 3.1.4 Practical example 3.2 Data Harmonisation 3.2.1 Data schema 3.2.2 Migration workflow 3.2.2.1 Vocabulary loading stage 3.2.2.2 Concept mapping stage 4 Results 4.1 Use case overview 4.2 Analysing results 4.2.1 Medication extraction 4.2.2 Migrated data 4.3 Error analysis 4.4 Data exploration 5 Discussion 5.1 Main findings 5.2 System synopsis 5.3 Future directions 6 Conclusion CRediT authorship contribution statement Acknowledgements References CHENG 2014 371 H KATEHAKIS 2006 D ELECTRONICHEALTHRECORD PIWOWAR 2010 148 156 H PETERSON 2020 103541 K HRIPCSAK 2016 7329 7336 G SHEIKHALISHAHI 2019 e12239 S NELSON 2011 441 S PIVOVAROV 2015 938 947 R FU 2020 103526 S SOHN 2014 858 865 S WEEKS 2020 407 418 H HENRY 2019 3 12 S WEI 2019 13 21 Q CHEN 2019 56 64 L ARONSON 2010 229 236 A HARRIS 2020 221 230 D SAVOVA 2010 507 513 G MATOS 2018 68 S RANGANATHAN 2018 184 P MURPHY 2010 124 130 S MCMURRY 2013 e55811 A HRIPCSAK 2015 574 G OVERHAGE 2011 54 60 J BADGER 2019 103185 J LIU 2020 e17376 S PARK 2021 e23983 J GARZA 2016 333 341 M BODENREIDER 2004 D267 D270 O UZUNER 2010 514 518 O BURN 2020 1 11 E MARKUS 2021 103655 A ALMEIDAX2021X103849 ALMEIDAX2021X103849XJ 2022-07-06T00:00:00.000Z 2022-07-06T00:00:00.000Z © 2021 Elsevier Inc. All rights reserved. 2021-07-09T23:48:40.537Z ACES ACES College of Agricultural, Consumer and Environmental Sciences, University of Illinois at Urbana-Champaign FCT PD/BD/142878/2018 SFRH/BD/147837/2019 FCT Fundação para a Ciência e a Tecnologia EU/EFPIA 806968 IMI Innovative Medicines Initiative USF Despertar This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. João Figueira Silva and João Rafael Almeida are funded by the FCT - Foundation for Science and Technology (national funds) under the grants PD/BD/142878/2018 and SFRH/BD/147837/2019 respectively. The authors would like to acknowledge with gratitude, the support provided by Ana Isabel Morais, GP at USF Despertar, ACES, Gondomar, Portugal and Guilherme Oliveira, GP at USF Esgueira, Aveiro, Portugal, in the validation of the medical information present in this manuscript. This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. Jo?o Figueira Silva and Jo?o Rafael Almeida are funded by the FCT - Foundation for Science and Technology (national funds) under the grants PD/BD/142878/2018 and SFRH/BD/147837/2019 respectively. The authors would like to acknowledge with gratitude, the support provided by Ana Isabel Morais, GP at USF Despertar, ACES, Gondomar, Portugal and Guilherme Oliveira, GP at USF Esgueira, Aveiro, Portugal, in the validation of the medical information present in this manuscript. item S1532-0464(21)00178-7 S1532046421001787 10.1016/j.jbi.2021.103849 272371 2021-08-12T07:17:07.657509Z 2021-08-01 2021-08-31 true 1083397 MAIN 11 56273 849 656 IMAGE-WEB-PDF 1 gr3 47336 421 428 gr4 34899 293 512 gr1 27612 245 511 gr2 30743 231 510 ga1 true 27138 184 500 gr3 8508 163 166 gr4 7730 125 219 gr1 6740 105 219 gr2 6609 99 219 ga1 true 6871 81 219 gr3 369230 1864 1894 gr4 220153 1300 2269 gr1 180226 1086 2263 gr2 184092 1023 2259 ga1 true 156535 814 2213 am false 1366485 YJBIN 103849 103849 S1532-0464(21)00178-7 10.1016/j.jbi.2021.103849 Elsevier Inc. Fig. 1 Migration pipeline used in the EMIF-AD project aiming to harmonise and validate health data, recorded from patients suffering from Alzheimer’s Diseases, into the OMOP CDM schema. Although the EMIF-AD datasets were migrated in the format of CSV files, this approach can be used directly from an EHR database (grey dashed box). Fig. 2 Overview of the extraction of information from clinical text into the matrix format. The red box represents an initial setup phase where the vocabularies are processed and imported in the Neji annotator. Fig. 3 Overview of an example clinical note processed with the first part of the presented pipeline. The clinical note is firstly annotated with Neji; for illustrative purposes, the detected drug mentions are shown highlighted in Neji’s graphical interface. The post-processing module searches the clinical note further for additional drug related information such as dosage, strength and route. Finally, all annotations are restructured into a matrix to be forwarded to the second part of the pipeline. Fig. 4 Overview of the data harmonisation pipeline used to read the extracted data matrix, process and harmonise its concepts into a relational database using the OMOP CDM data schema. The red box represents the vocabulary loading process that can be executed in the setup stage. Table 1 Dataset statistics for the 2009 i2b2 medication extraction challenge full dataset, which is provided with the train and test partitions combined. Concept Valid annotations Drug 9 003 Route 3 406 Dosage 4 482 Table 2 Dataset statistics detailing the number of annotated concepts and relations in the 2018 n2c2 ADE and medication extraction challenge dataset. Concept Relations to Drug Training Test Total Training Test Total Drug 16 225 10 575 26 800 Strength 6 691 4 230 10 921 6 702 4 244 10 946 Route 5 476 3 513 8 989 5 538 3 546 9 084 Dosage 4 221 2 681 6 902 4 225 2 695 6 920 Table 3 Evaluation results from the medication extraction component applied in the validation datasets. PP: Post Processed. 2018 n2c2 2009 i2b2 Source Precision Recall F1-Score Precision Recall F1-Score Neji 0.426 0.797 0.555 0.544 0.776 0.640 PP 0.802 0.705 0.751 0.667 0.556 0.607 Table 4 Results of the mapped concepts in the second part of the methodology, including the database entries in the Drug Exposure table and the predicted amount of entries that were not mapped. 2018 n2c2 2009 i2b2 Unique concepts 961 1049 Mapped (score equals to 1.0) 470 (48.9%) 448 (42.7%) Mapped (score less than 1) 87 (9.1%) 94 (9.0%) Mapped (manually) 221 (23%) 215 (20.5%) Not mapped 183 (19%) 292 (27.8%) Database entries 5316 8998 Discarded entries 1246 3855 Table 5 Analysis of some of the false positives and false negatives annotated by the proposed system. Mentions annotated by the system are highlighted in bold. Missing information Sentence from the clinical note aspirin/ by mouth The patient is taking aspirin and enalapril by mouth. ins (Insulin)/ p.o (by mouth)/ 3 cap (capsules) 5. ins: 3 Cap p.o 10 milligrams 3. lasix: 10 miligrams po iv The patient took an iv dosage after the breakfast containing aspart insulin. PO 2. omeprazole 20 mg Capsule, Delayed Release (E.C.) Sig: Two (2) Capsule, Delayed Release (E.C.) PO DAILY (Daily). Original Research A two-stage workflow to extract and harmonize drug mentions from clinical notes into observational databases João Rafael Almeida Conceptualization Methodology Software Writing - original draft a b ⁎ João Figueira Silva Conceptualization Methodology Software Writing - original draft a 1 Sérgio Matos Writing - review & editing Supervision a José Luís Oliveira Writing - review & editing Supervision a a DETI/IEETA, University of Aveiro, Aveiro, Portugal DETI/IEETA University of Aveiro Aveiro Portugal DETI/IEETA, University of Aveiro, Aveiro, Portugal b Department of Computation, University of A Coruña, A Coruña, Spain Department of Computation University of A Coruña A Coruña Spain Department of Computation, University of A Coruña, A Coruña, Spain ⁎ Corresponding author at: DETI/IEETA, University of Aveiro, Aveiro, Portugal. DETI/IEETA University of Aveiro Aveiro Portugal 1 Equal contribution with first author to this research. Graphical abstract Background The content of the clinical notes that have been continuously collected along patients’ health history has the potential to provide relevant information about treatments and diseases, and to increase the value of structured data available in Electronic Health Records (EHR) databases. EHR databases are currently being used in observational studies which lead to important findings in medical and biomedical sciences. However, the information present in clinical notes is not being used in those studies, since the computational analysis of this unstructured data is much complex in comparison to structured data. Methods We propose a two-stage workflow for solving an existing gap in Extraction, Transformation and Loading (ETL) procedures regarding observational databases. The first stage of the workflow extracts prescriptions present in patient’s clinical notes, while the second stage harmonises the extracted information into their standard definition and stores the resulting information in a common database schema used in observational studies. Results We validated this methodology using two distinct data sets, in which the goal was to extract and store drug related information in a new Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) database. We analysed the performance of the used annotator as well as its limitations. Finally, we described some practical examples of how users can explore these datasets once migrated to OMOP CDM databases. Conclusion With this methodology, we were able to show a strategy for using the information extracted from the clinical notes in business intelligence tools, or for other applications such as data exploration through the use of SQL queries. Besides, the extracted information complements the data present in OMOP CDM databases which was not directly available in the EHR database. Keywords EHR Clinical NLP Clinical notes Information extraction Observational studies OMOP CDM ETL 1 Introduction Medicine has long enjoyed the benefits of technological developments and so has the quality of life of the population in general. Healthcare improvements were accompanied by the creation of new tools and data sources, which brought new knowledge and capabilities to physicians and impacted aspects such as disease prevention, diagnosis, treatment and patient follow-up [1]. These new resources brought the possibility of improving areas such as health research studies, which are composed of many time-consuming and expensive stages (e.g. identifying and recruiting subjects that consent the study, and monitoring them over lengthy periods of time), by lowering their cost and time through the exploration of already existing data, such as data obtained from previous studies or data stored in health-related registry systems [2]. However, challenges also arose with the need to cope with the resulting scale and diversity of medical data. Electronic Health Records (EHR) were created to provide an electronic infrastructure capable of storing administrative and medical data from diverse modalities, centralising data at the patient level [3] and providing a longitudinal view of the patient medical history. The resulting healthcare databases can be explored in health research studies to help increasing the quality of the research, especially when combining data from several databases [4]. Despite the interest in merging data from different databases, this process is severely hindered by the fact that database implementations can greatly vary among institutions. However, some efforts have already been made to tackle this issue, such as Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR) that provides specifications aiming at the standardisation of the healthcare data representation [5]. The concept of data harmonisation for clinical studies has been explored over the last years with the objective of optimising and reusing EHR data. This process gained increased importance given the possibility of inferring medical knowledge by analysing the history of populations sharing similar characteristics [6]. With this in mind, some institutions have made efforts to standardise observational studies using EHR databases, with the objective of enabling the creation of distributed studies using data collected from different medical institutions [7]. However, these strategies only explored the structured patient information present in EHR databases. Apart from structured information (e.g. form fields), data in EHRs can also be stored in unstructured form. Unstructured text is frequently found in clinical notes and is written in natural language, enabling physicians to document complete descriptions of the patient medical status and its progress through time. Owing to this fact free text makes up a significant part of the data stored in EHRs, specially in chronic diseases in which clinical notes outsize structured data [8], and can contain unique information that is not detectable in other data sources [9]. While some of this data can be extracted and structured using coding standards such as RxNorm [10], Drugbank [11] or SNOMED-CT (Systematized Nomenclature of Medicine - Clinical Terms) [12], this mapping task can be extremely complex and time consuming. Moreover, the acknowledged challenging nature of free text makes it difficult to develop automatic information extraction solutions for clinical text. Nonetheless, since clinical text poses great interest, some approaches have been developed for extracting relevant information. Even though this process has historically consisted of having clinical experts manually review clinical notes, a process which cannot scale with the growing rate of generation of medical data [8], much research has been made during the past years in domains such as clinical natural language processing (NLP) with the objective of creating systems capable of automatically annotating and summarising important text content in clinical notes [13,14]. Since distributed observational studies do not explore the large amounts of data stored in clinical notes, there exists an opportunity to leverage those documents to complement structured data with additional information. The present paper explores a methodology to reuse information on patient medication present in the clinical notes by storing this information in a relational data model currently used in distributed observational studies. The presented methodology was validated using two public datasets of clinical notes and is currently available at Summarily, our main contributions in this paper are the following: • This work proposes a methodology to extract relevant concepts from clinical notes to enrich structured OMOP CDM databases, enabling the use of SQL queries for analysing the clinical text information and thus helping in the definition of patient cohorts sharing similar characteristics; • The resulting methodology is available in an open-source implementation and is prepared to be adopted and integrated in the current EHR databases to OMOP CDM migration pipelines, that currently do not explore information stored in clinical notes; • Finally, this work also introduces a new strategy in this scope for semi-automatically harmonising and validating medical concepts present in clinical notes into their standard definition. 2 Related work The proposed methodology uses improved and adapted versions of strategies which have already been used and validated in other scenarios. This section is divided into: 1) an analysis of related work on medication extraction from clinical notes; and 2) the study of strategies to migrate and harmonise semi-structured concepts into standard data schemas. 2.1 Extracting patient information from clinical notes Clinical notes are an important resource as they let physicians document patient status with full descriptions throughout time, which enables the monitoring of the patient trajectory. These notes can contain vast amounts of relevant information such as family history, prescribed medication and medication intake, diagnosis and followed procedures. While this wealth of knowledge stored in clinical free-text remains underexplored, developments in NLP can help leveraging this source of data by effectively extracting and structuring relevant information contained in clinical narratives [8]. Information extraction can typically be divided in two components. The first one is Named Entity Recognition (NER) and consists in the detection of entities of interest in text. In clinical text, these entities can involve mentions from family history, prescribed medication, disorders, laboratory measurements, and others. The second component is Named Entity Normalisation (NEN) and aims to further structure the extracted text by normalising entities according to coding standards [15]. When dealing with clinical text, this process can leverage existing medical terminologies such as RxNorm or Drugbank. In the present work, we focused on extracting information regarding medication from clinical narratives. Similarly to other NLP problems, medication extraction from clinical narratives can currently follow two main paths: heuristics based solutions and machine/deep learning based solutions. Examples of the former are MedXN [16] and MedExtractR [17], two rule-based and dictionary-based solutions designed to extract information regarding medication from clinical notes. MedXN is a reckoned open-source system implemented using Apache’s Unstructured Information Management Architecture (UIMA) that aims to extract and normalise medication mentions using the RxNorm medical terminology [16], whereas MedExtractR is an R programming language package that follows a similar approach but sacrifices some generalising capability by narrowing down the scope of drugs to search for [17]. Regarding deep learning based solutions, recent challenges on adverse drug events and medication extraction such as MADE 2018 [18] and n2c2 2018 track 2 [19] showed a clear growing trend in the use of these solutions, with a clear dominance in model variations based on bidirectional Long Short Term Memory (BiLSTM) networks coupled with attention mechanisms and/or Conditional Random Fields (CRFs) [20,21]. Other model architectures such as the Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) have also been combined obtaining promising results, particularly when using rule-based post-processing mechanisms which can improve system performance [21]. Despite their good performances, the previously mentioned systems still struggle when annotating certain information such as duration, adverse drug events and reasons, similarly to what human annotators experience [19]. More recently, researchers developed a system to process prescription instructions (commonly mentioned as “sigs”) using MetaMap [22] to normalise medication information [23]. However, this system has the limitation of focusing only on electronic prescriptions, disregarding the remaining content in clinical notes which may contain information concerning medication intake. Since our objective was to extract patient information from complete clinical notes, we opted for customisable frameworks capable of generalising and extracting medical concepts from diversified clinical notes. MedXN [16] and cTAKES [24] are examples of open-source UIMA based solutions for information extraction, with cTAKES being a modular and extensible framework and MedXN being a solution specifically designed for medication extraction. Another flexible and modular framework for text processing and annotation is Neji [25]. This open-source system provides annotation services that can be easily configured and extended with new dictionaries and machine learning models, having already been used in previous work to extract family history information from clinical narratives [26]. Resources from Neji and MedXN were used in this work to extract drug related information, as described in more detail in Section 3.1. 2.2 Data harmonisation into relational models Observational studies started being the subject of great interest over the past years due to the increase in information stored in EHRs. An observational study consists in having researchers document the relationship between the exposure and outcome in the study without performing any active intervention on the patients [27]. Since this is a powerful way to advance in the field of medicine, several organisations have invested in this type of research. To do that, these organisations had to build structures, methodologies and tools for querying multiple EHR databases. The i2b2 [28] project was launched with the goal of aggregating databases from multiple institutions without creating a centralised database. The teams involved in this project developed tools to simplify the cohort estimation and feasibility determination of the aggregated databases [29]. Observational Health Data Sciences and Informatics (OHDSI) 2 2 is an organisation whose goal is to develop tools and strategies to support large-scale distributed observational studies in health care data. OHDSI currently supports a community with several open-source solutions to perform medical product safety surveillance using observational databases [30]. In addition to those tools, OHDSI adopted a database schema that uses anonymised patient information and supports standard vocabularies in its structure [31]. Therefore, observational databases structured using the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) can be part of a medical research network, in which studies can be extended to other health institutions. This schema has the patient as central entity of the model, and all the remaining clinical information such as procedures, drugs, measurements and conditions are associated with the patient [32]. Moreover, this model uses an extensive ontology that captures information from various standard medical terminologies to harmonise stored information [33]. Although this schema was designed to store patient information extracted from EHR databases, there exist initiatives to reuse parts of this schema in more specific medical scenarios. A case of success where this data schema was used to harmonise and store patient information collected in cohorts studies was the European Medical Information Framework (EMIF) Alzheimer’s Diseases (AD) study [34]. However, disease-specific scenarios only require the use of a few tables from the CDM. The workflow represented in Fig. 1 shows an overview of the migration pipeline used in the EMIF-AD project for Alzheimer’s Diseases’ cohorts. The strategy was performed using the best practices of Extraction, Transformation and Loading (ETL) procedures, which encouraged its adoption in this work. This workflow begins with the Extraction stage, where patient data stored in CSV files is converted into a proper format to be processed in the pipeline. Then, in the Transformation stage, a semi-automatic concept mapping is done, i.e., all concepts are automatically mapped to their standard definition using string similarity metrics, with the resulting mappings being validated and approved by a clinical researcher familiarised with the original concepts. In the OHDSI community this mapping is performed using Usagi, an open-source tool with a user-friendly interface to simplify the validation of the mappings [35]. Finally, in the Loading stage the cohorts are loaded into the OMOP CDM database. Recently, other authors invested in leveraging health databases with clinical note information [36,37]. Liu et al. [36] proposed a proof-of-concept system for cohort retrieval of clinical data using the OMOP CDM to enhance model portability. In this work, cTAKES was used to extract relevant unstructured concepts from clinical notes. The goal of this system was to retrieve information from clinical note repositories using filters that can only be applied in structured information, and to use this information for the retrieval of more relevant cohorts. Park et al. [37] proposed a framework for the annotation of unstructured information present in EHR databases. The authors used topic modelling to extract information from clinical notes and stored this information in annotations under JSON (JavaScript Object Notation) format. The resulting information is used for statistical analysis and cohort exploration. Although both works used the OMOP CDM schema, neither approach integrate the resulting data with the OMOP CDM database, having the limitation of only indexing this information in unstructured databases. Although the OMOP CDM was initially designed for a smaller purpose, over the last years this data schema was updated and became a complex structure capable of storing different types of medical information focused on the patient. Currently, this data schema contains two tables specially designed for clinical notes. However, the information stored in these tables is not commonly used in observational studies. In comparison with other data schemes used in medical studies, OMOP CDM may be the most used and complete [38], which was also a reason for its use in our proposal. 3 Methods We devised a pipeline that can be divided in two main sections. During the first stage of the proposed methodology clinical notes are annotated to extract relevant information, whilst in the second stage that information is harmonised and migrated into an observational database structured using OMOP CDM. Detailed descriptions of the processes of annotating clinical notes and harmonising resulting data are further provided in Sections 3.1 and 3.2, respectively. The system was divided into two independent components to provide the possibility of swapping one of the components at any time without interfering with the remaining one. In other words, it is possible to change to other annotation techniques for clinical notes information extraction without affecting the migration component, and the same applies to the migration component which can be changed to use a different output data schema without affecting the information extraction component. 3.1 Clinical notes analysis The first part of the pipeline is responsible for the extraction of relevant medical information from free text in clinical notes, and for the storage of extracted data in a matrix structure to be used in the second part of the pipeline. Fig. 2 illustrates an overview of this process. Here, a system reader firstly receives clinical notes as input, reads their content and stores it according to a fixed structure. This reader is implemented using the factory programming pattern, thus a new dataset reader needs to be implemented whenever a new clinical note dataset is to be used. After reading the clinical notes, a Neji web service is used to annotate medication entities in each note, and the resulting annotations are stored and post-processed. Finally, the extracted information is stored in a matrix to be used in Section 3.2. Although we used Neji to annotate the clinical notes, other tools/frameworks can be integrated in this pipeline, or used to replace Neji. Furthermore, this pipeline is not dependent on a specific technology. The main condition is that the output provided at the end of this first part of the pipeline should match the expected input format of the harmonization component. To further facilitate the use of different strategies in the annotation component, the post-processing module incorporates a programmatic parser that can be easily extended to reformat the annotation output into the expected format. 3.1.1 Annotating clinical notes The red dashed box presented in Fig. 2 concerns the setting up of the annotation mechanisms. Our goal was to create a pipeline capable of generalising and working with any type of clinical note, hence we needed a flexible framework for text processing and annotation that could be easily configured with new resources, such as dictionaries and machine learning models. Neji [25] fulfilled the above mentioned requirements and provided an annotation viewer along with easy access to its annotation mechanisms through web services. Furthermore, Neji can be installed locally and used through the command line interface or through a web service, so that users can use it to annotate sensitive data without depending on external services. This was a key aspect for its integration in the pipeline, considering the privacy concerns associated with the manipulation of sensitive patient information. To set up Neji as a medication annotator, we firstly extracted three drug related medical terminologies from the Unified Medical Language System (UMLS) Metathesaurus [39]: RxNorm, DrugBank and AOD (Alcohol and Other Drug Thesaurus). However, these terminologies cover many semantic types and groups, thus to narrow down the scope of the dictionaries we filtered them keeping only entries from the “Chemicals & Drugs” semantic group. The resulting dictionaries were imported to Neji, and a Neji annotation service was configured for the extraction of drug mentions in clinical text. After passing all clinical notes through the system reader, the Neji web service was used to annotate medication entities in each note and the resulting annotations were stored. 3.1.2 Post-processing To filter annotations and perform a further search for additional drug related information, namely drug strength, dosage and route, a post-processing module was developed. This module explores specific vocabularies and integrates resources from Athena and from MedXN [16], namely vocabularies and regular expressions. 3.1.2.1 Annotation disambiguation The post-processing module begins by checking for ambiguous annotations. Since Neji was supplied with three different drug related dictionaries, which may have concept overlap, it is possible to have ambiguous situations where Neji creates multiple annotations for a mention. As an example, in the sentence “the patient took aspirin 600 mg orally”, Neji can annotate “aspirin” with a DrugBank code and “aspirin 600 mg” with a RxNorm code. When there exist multiple annotations associated with a mention, the post-processing module gives higher priority to RxNorm annotations as they are more complex and specific, enabling the distinction of mentions that have strength information incorporated. Since there may exist irrelevant entries in the dictionaries, which results in the annotation of many false positives, disambiguated annotations are subjected to an additional filtering process were possible false positives are removed by checking each annotation against a false positive vocabulary. This vocabulary was manually compiled and integrates part of the MedXN [16] vocabulary along with a list of common medical abbreviations used in clinical text. 3.1.2.2 Extraction of additional drug information Afterwards, considering the sentence or line where Neji detected an entity, the post-processing module uses a vocabulary of possible administration routes to search for the route used to administer the drug within the sentence. This vocabulary was compiled from three main sources: the MedXN vocabulary, a manual list of common route abbreviations and their expansion, and finally a list of SNOMED routes retrieved from Athena which was obtained by searching codes with type “Routes”. Route annotation is of utmost importance since drug administration route is a mandatory field in the Drug Exposure table from OMOP CDM. Therefore, when the post-processing module cannot detect a route for a drug annotation, this field is annotated with “N/A”. The final post-processing step is responsible for extracting strength and dosage information. To extract drug strength, the system firstly checks if the annotated drug mention contains strength information, and if so it directly extracts the strength component, whereas if not the system uses an adjusted version of a MedXN regular expression to try to identify drug strength in the full sentence. Finally, the sentence is processed with a list of regular expressions to detect the presence of dosage information. 3.1.3 Storing extracted information Once the information extraction process is completed, all extracted information is stored in a matrix structured by patient and drug, where each cell holds information on a drug mention (strength, dosage and route). The reason for storing extracted data in this particular format was the fact that the resulting structure is similar to that already used in cohort studies, greatly simplifying the process of migrating it into an OMOP CDM database, as described in the next section. 3.1.4 Practical example Fig. 3 presents the annotation and conversion into a structured matrix of an example clinical note. The clinical note is firstly annotated using Neji, as demonstrated in the second element of the image extracted from the Neji interface. Then the post processing stage searches for additional drug related information in the clinical note, such as dosage, strength and route. The resulting annotations are finally cleaned and restructured in a matrix format, as shown in the bottom element of Fig. 3. 3.2 Data Harmonisation Concept extraction from the text is only the first part of this process. Since the goal is to reuse the extracted information, the second part of the pipeline is responsible for gathering the extracted information from the matrix and storing it into the OMOP CDM data schema. Despite having the data represented in the previously defined structured format, this information still needs to be harmonised and cleaned, which is one of the main tasks of this second component in the proposed methodology. 3.2.1 Data schema The final output of this pipeline is a relational database that adopts the OMOP CDM standards. Therefore, our methodology requires the following tables: • Person: contains the patients’ personal information (i.e. birthday, race, gender and ethnicity). • Drug Exposure: captures the records related with the utilisation of a drug by the patient. • Visit Occurrence: contains the interval times of a Person that received medical services. In our scenario, we may not be able to define the time span due to the end date of those visits. • Note: stores the clinical note in the database. It keeps some information that characterises the note, and in one field it captures the unstructured information recorded by the provider about a patient in free text format. • Note NLP: encodes all output from NLP processes on clinical notes. Each row represents an extracted term. Regarding the Drug Exposure table, it is important to highlight some of its characteristics as these may impact on the text extraction procedure. The Drug Exposure table stores patient information associated with written prescriptions, orders, pharmacy dispensing, among other situations concerning the patient-drug relation. Its structure contains several mandatory fields such as “drug_exposure_id”, “person_id”, “drug_concept_id”, “drug_exposure_start_datetime”, “drug_type_concept_id”, “route_concept_id” and “drug_source _concept_id”. Some of these fields contain references to the standard vocabularies, which helps keeping the record harmonised. Moreover, the table contains additional fields that can be used to characterise drug utilisation, yet these are not mandatory. In addition to this information, the OMOP CDM schema also has a set of tables named “Standardised Vocabularies” which are designed to store the standard vocabularies as well as additional information, such as hierarchical concept relations for example. Additionally, OHDSI provides the Athena 3 3 web platform, which contains the most common vocabularies available in the OMOP CDM schema and facilitates selecting the desired vocabularies to be used in migrated databases. 3.2.2 Migration workflow The component in the proposed methodology responsible for migrating the data to a relational database followed similar principles as represented in Fig. 1. We improved this pipeline to a more generic solution and for this scenario, we used different output tables. Therefore, as presented in Fig. 4 , this second part is divided into two stages: 1) vocabulary loading and 2) raw data harmonisation. 3.2.2.1 Vocabulary loading stage The vocabulary loading stage requires an initial manual procedure, where the user needs to download from the Athena platform the desired vocabularies to use in the methodology. These vocabularies are used in the OHDSI Databases Network in order to allow federated and distributed queries over multiple databases from different countries. In case of building a database only for clinical notes, which is not very common, this stage will load those vocabularies automatically. The vocabularies used in this methodology were RxNorm, which provides normalised names for clinical drugs, and SNOMED, which contains medical terms particularly useful for the standard definition of routes among others. Vocabularies were also useful in the second stage to feed the Usagi tool, which maps the concepts in raw data to their standard definition. However, this second stage is complex because the information needs to be harmonised on different levels: 1) the standard definition for drugs; 2) the standard definition for routes; and 3) the correct field in the data schema. This last harmonisation level is attained automatically by the system, based on the structure of the matrix resulting from the clinical notes analysis component. The remaining harmonisation levels are achieved using Usagi, as this tool already provides suggestions for each concept based on the textual similarity with standard concepts. 3.2.2.2 Concept mapping stage The mapping stage is the most time consuming part of this pipeline. However, it allows a full validation of mapped concepts, while also discarding wrongly annotated concepts. Despite requiring the health professional to validate each mapping individually, empirical experience shows that Usagi’s suggestions are correct for a large portion of the cases with those cases requiring very little time to validate. The tool also provides search mechanisms to simplify the correction of the remaining mappings, in order to accelerate the process. The proposed methodology was built to be integrated in the OHDSI ETL (Extract, Transformation and Loading) procedures. In these procedures, concepts existing in the database are mapped to their standard definition using Usagi. The bottleneck in those procedures is the mapping stage due to the large amount of concepts. However, our proposal is focused on medications extracted from clinical notes, thus it is possible to reuse some of the mappings made in a previous EHR migration (in case such migration was performed), which reduces the time required in our approach. Another aspect concerning concept mapping is that terms are aggregated, i.e., even though a term can occur multiple times in the whole dataset, it is only mapped once in Usagi. Upon completing the mapping stage, the system receives the Usagi output and creates the mappings. While this is the only file being currently used as input, in more complex scenarios an ontology containing more information about the concepts could also be used. An example of such could be the use of range-of-values in order to automatically validate drug strength for each entry of a specific drug. Despite not being explored in our use case, the proposed system was designed taking into account this possibility. Another requirement for this part of the methodology is the need for some of the patient personal information, i.e., birth date, ethnicity, race, location, provider and death date (in case of dead patients). This information is already part of the EHR structured data and can be collected together with the clinical notes processing. 4 Results The proposed system was validated on a medication extraction use case using two public datasets from previous text mining research challenges. It is important to note that the system was not implemented focusing on a particular dataset, i.e., the methodology was tested on these datasets without any prior training on them. This section presents the selected use cases in more detail, followed by an analysis of the performance of the two major components in the pipeline, as well as the overall results. 4.1 Use case overview The present work focused on extracting information regarding medication from clinical narratives, an area of great interest which has been promoted by several international research challenges. For instance, the 2009 i2b2 medication extraction challenge had the objective of extracting medications, dosages, modes of administration, frequency of administration and reason for administration [40], while the n2c2 2018 track 2 on adverse drug events (ADE) and medication extraction in EHRs, also added to that information the relations between drugs and adverse drug events (ADEs) [19]. To validate our proposal, we used the datasets provided in these two challenges. The objective of this work was not to develop a top performing NLP approach for information extraction, but to abstract and have a generalisable annotating system for extracting information from clinical notes, which enabled the validation of the pipeline as a whole. Therefore, we used the full datasets to validate the system. The 2009 i2b2 dataset contains 1249 discharge summaries from which only 252 have gold standard annotations. Even though this dataset has 9003 drug annotations each with additional information (e.g. dosage, route), the challenge enabled the annotation of additional information with “N/A” whenever that information was not present in the text. Table 1 provides statistics on the number of annotated entities in this dataset. The 2018 n2c2 dataset contains 505 discharge summaries from the MIMIC-III database, annotated regarding entities and relations, and was originally split in train and test partitions containing 303 and 202 annotated documents, respectively. Even though the dataset contains annotations for a wider variety of drug related information, in this work we only focused on drugs, dosage, strength and route, and on the relations between drugs and the remaining entity types. Table 2 provides statistics on the number of annotated concepts and relations present in the dataset. The results obtained after running the full pipeline over both datasets are presented in the following section. 4.2 Analysing results 4.2.1 Medication extraction The medication annotation component was designed to extract as many drug entries as possible to populate the OMOP CDM data schema. Since the route field in the Drug Exposure table is mandatory, a “N/A” route is attributed whenever it is not possible to detect the route used to administer a drug. While the 2009 dataset provided the possibility of annotating various entities with “N/A”, the 2018 dataset did not. Therefore, to perform a consistent validation throughout both datasets, we decided to remove all “N/A” annotated entities during the validation process. Moreover, since this component was developed considering the extraction of data from different datasets, it was necessary to develop a single evaluator for assessing system performance across various datasets. The resulting evaluator assesses extraction performance based solely on extracted drug and route mentions, as these two fields are mandatory for the OMOP CDM data schema (dosage and strength are informative yet not mandatory). Therefore, its analysis only considers as true positives those drugs which have an associated route annotated in the gold standard, which considerably reduces the amount of true positives. For instance, despite the existence of 26800 annotated drugs in the 2018 dataset, there are only 8989 annotated routes and 9084 annotated route-drug relations, which translates to a final number of valid drug annotations close to a third of the total annotated drugs. Similarly, the 2009 dataset contains 9003 drug annotations from which only 3406 drug annotations possess valid route annotations. Results obtained from the evaluation of the extraction component in all validation datasets are presented in Table 3 . It is possible to observe in all datasets that Neji annotations have higher recall, while post-processed annotations suffer a decrease in recall but an increase in precision. This behaviour is expected since Neji is used to detect drug mentions indiscriminately (high recall), whereas the post-processing module is responsible for connecting drugs with their respective routes and filtering out drugs without mention of route. Furthermore, Table 3 shows that the system obtained a significantly higher final F1-score in the 2018 dataset than in the 2009 dataset. However, a manual revision of some gold standard annotations of the 2009 dataset revealed some annotation inconsistencies, which can impact the performance of this component. This could be partly explained by the fact that gold standard annotations were not created by a group of medical experts but instead by challenge participants after the challenge terminated. The obtained results represent the baseline of our annotator. These results can be improved, by training models for each data set, which was not our main goal in this work. Results obtained with the developed component were already quite promising, and showed consistency since in both datasets the F1-score was very similar (a difference of 0.03 points was observed). Importantly, these data sets are very different, thus allowing to verify that the proposed annotator is flexible enough to serve as baseline for this methodology. 4.2.2 Migrated data The second part of the methodology was validated differently. In this case, there is no available gold standard to calculate the usual performance metrics. However, the idea of harmonising patient data from raw data stored in the matrix format into OMOP CDM databases has already been explored in other scenarios. In the OHDSI approach, the validation of this migration process is usually performed through manual searches in the resulting database. The cohort harmonization pipeline used in the EMIF-AD project had the goal of converting the data into a common schema which was not compliant with the OMOP CDM. The ETL core of our methodology was redesigned to be fully aligned with the CDM so that one can cope with new versions of this model. Overall, both pipelines share identical processes at a high-level context, i.e., the use of a similar structure for the data source (matrix) and the manual validation by health professionals. In the EMIF-AD scenario, the migration was validated using specific queries associated to a valid range of field values. However, our use case is different as we do not have information for the range-of-values associated with the mapped concepts. Following the proposed pipeline, the information extracted from the datasets was used in the second part of the system to validate this methodology. Table 4 presents some metrics regarding the data harmonisation component. When using Usagi, we defined filters to ensure that only the Drug and Route domains from the RxNorm and SNOMED vocabularies were used. This procedure is important as it disables similarity searches with other concepts which could have a high level of textual similarity but are in fact not related with the medication domain. The mappings were divided into categories since some required more effort to validate than others. These categories are: 1) mappings with score 1, where the health professional only needs to confirm the automatic mapping; 2) direct mappings that had a similarity score lower than 1; and 3) the concepts that required a manual search in the tool’s dictionaries. With the help of two experts, we were able to obtain the mapping values presented in Table 4. As observed in this table, there still exists a percentage of concepts for which the health professionals were not able to provide a mapping. Since both professionals were not familiar with the datasets used in this work, in case of ambiguous mappings or uncertainty about the concept meaning, their decision was to not perform the mapping, hence resulting in a subset of unmapped entries. 4.3 Error analysis The proposed system was built to be specialised in extracting medication information from clinical notes, without being designed for a specific dataset. Despite this goal, analysing the results on validation datasets allows identifying possible limitations in the annotation component. Table 5 presents some examples of the most frequent errors that represent the system limitations. The first example of these limitation concerns is due to the presence of coordinations, which are not currently being processed. In some situations, the text contains more than one medication and only references the way that these are administered at the end, because all mentioned drugs share the same administration route. In this example, the system only detects the route for the last drug, whereas the initial drug (aspirin) is annotated without any route, which leads to false positives and false negatives in the evaluation metrics. The problem represented in the second example is related to unknown abbreviations, which are employed by institutional staff and most commonly present in enumerated points. Since Neji performs exact matching, these are only extracted by the annotation component if they are present in the vocabularies used. Another issue concerning abbreviations or misspelled words is during the extraction of the drug strength or dosage. In this situation we followed two different approaches. The first involved mention disambiguation prioritising RxNorm annotations, when those integrated detailed information such as drug strength. Since this technique only works in specific situations, the other approach consisted in using a conditional regular expression. However, the vocabulary used in this regular expression does not consider spelling errors, such as the one presented in the third example of Table 5. This example shows the impact of the missing “l” that affects the detection of the lasix’s strength. The last two examples in Table 5 are related with the window size used before and after concepts annotated by Neji. These words are used to identify more information about the medication (such as the route, dosage, strength, among others). The first example shows the occurrence of “iv” outside the word window considered before a drug annotation. Similarly, the second example illustrates missed information (“PO”) occurring outside the word window considered after a drug annotation. 4.4 Data exploration Relational databases allow the use of SQL (Structured Query Language) queries to analyse and explore database content. In this format, Business Intelligence (BI) tools can be used to reorganize the information and extract knowledge from the data. With the drug information stored in a relational model and using a standard data schema, one can use other strategies to analyse the data. For instance, the user can easily perform queries over the extracted information, or, in a more distributed scenario, can share these queries with other institutions that adopted this standard model. Therefore, in this section, we present examples of the application of this methodology, even though many other possibilities exist. The use of OMOP CDM as the data schema and the adoption of the standard principles defined by the OHDSI community in an ETL procedure allow the use of these OHDSI community tools. For instance, it is possible to define a study design using the Atlas 4 4 web platform, and then locally run different types of studies such as: 1) Database-Level and Cohort Characterization, as performed on a recent study with COVID-19 patients [41]; 2) Population-Level Estimation, as demonstrated by a study focused on comparing the effectiveness of antihypertensive drugs [42]; and 3) Patient-Level Prediction, which can explore several artificial intelligence models, as described in [43]. Atlas also integrates analytical features, with charts that offer an overview of the database and allow highlighting important details about the database and each table associated to the patient in this data schema. Using the system proposed in this work, cohorts defined using Atlas can consider both the information extracted from clinical notes and the data available in observational databases. The data can also be explored for tracking patient health status, i.e., although prescribed medication is recorded in the EHR systems, clinical notes can contain information about medication that the patient took without being prescribed in the clinical facilities. This can help physicians more closely tracking the medication taken by the patient without needing to sift through large amounts of clinical text records. Even though in the proposed scenario we were interested in mapping only drug related information, this methodology can be applied to other types of concepts that can be found in clinical notes, if such concept types are considered by the OMOP CDM data schema. For instance, concepts such as procedures, observations, examinations among others can be accommodated and analysed using this format. 5 Discussion 5.1 Main findings The proposed methodology creates new opportunities for reusing the information present in clinical notes, namely in research studies. Currently, during the migration of EHR databases into Observational databases, clinical notes are migrated but the information stored in them is not used. Although the OMOP CDM schema has two tables for NLP extracted concepts, study design does not usually consider information stored in these tables because they contain less detailed information. One of the reasons for this is the fact that information stored in these table is not focused on a specific domain, since these tables can store any kind of medical data that is extracted from clinical notes. This property makes it difficult to replicate studies in distributed databases, which is one of the core OHDSI fundamentals. The system herein described was validated in a scenario focused on drug extraction and this system is capable of harmonising extracted information following the OHDSI principles. With this information mapped to its standard definition following the validation practices using Usagi, there is no reason to not use this data in research studies. In fact, if this system is applied to support the migration from an already existing table, some of the Usagi mappings can be reused from the EHR migration, since the source data is the same. While reusing previous EHR mappings does not guarantee all concepts are directly mapped, mainly because of abbreviations and free-text annotations present in clinical notes, this process can considerably reduce the number of concepts in the mapping stage. Although in our proposal we only mapped drug information, if we consider a different scenario where only clinical notes are migrated, as we did to validate the system, it is now possible to perform research studies using information extracted from clinical notes alone. The dataset will be inferior, which may reduce the relevance of the findings due to the level of detail that can be extracted. However, this type of research is possible and using OHDSI tools a new study can be easily performed. Besides this novel concept regarding the use of clinical notes to improve observational studies, it is possible, with the text information stored in homogeneous data schemas and mapped to their standard definitions, to simply analyse the dataset using SQL queries or BI tools. 5.2 System synopsis The system was carefully designed to divide its responsibilities into two components, clinical notes annotation and data harmonisation. The main reason for this strong separation of responsibilities was to provide the possibility of performing future improvements in each component individually whilst maintaining a fully functional pipeline. Since NLP techniques are progressing at a rapid pace, this flexibility enables the information extraction component to be constantly updated, thus helping maintain the complete system up to date. For instance, this enables the future integration of deep learning-based approaches, which have already shown to be successful in clinical text extraction tasks. In this paper we used an English clinical text annotator that was not trained on any dataset since our goal was not to develop a state-of-the-art annotating system. Instead, our objective was to have a generic annotator capable of producing satisfactory and consistent results in various datasets. This way, we could solve a current problem and create new opportunities in the exploration of information available in clinical notes. However, to be useful in many realistic scenarios, it is necessary to be able to switch the English text annotator by an annotator designed for a language other than English. In fact, the OHDSI community is currently spread over the world with many OMOP CDM databases existing in several non-English speaking countries. Based on the error analysis performed in Section 4.3, we were able to identify a few points in which the system can be optimised. Although we used Neji for the information extraction task and this system does not currently support deep learning models, our methodology was designed to incorporate multiple annotators as well. The decision to use a matrix for storing extracted information was made based on previous experience. In the past, we needed to migrate patient clinical data collected in medical studies that was stored in spreadsheets. After studying different alternatives for performing a clean and solid data harmonisation, the principles that we applied in the second component of our methodology were the most aligned with ETL procedures. One key aspect in this methodology is the manual validation using a graphical interface, which cleans wrongly annotated concepts and facilitates the correction of concepts that were incorrectly mapped to other standard definition. As previously mentioned, this is the current procedure used when migrating EHR databases into the OMOP CDM schema. We tried to optimise this procedure as much as possible in our methodology, since it requires a manual interaction with the system. The idea was to incorporate the possibility of loading other mappings in the system, such as previous mappings made in the institution during an EHR migration. An additional possibility would be to develop a pre-mapping in the annotator component, which would then be loaded in Usagi. With this approach, Usagi suggestions would be skipped and only the annotation features would be used. However, this tool has already been validated in several migration procedures and its operation considers mappings based on a hierarchy defined in the standard vocabularies, an aspect that is not so deeply explored in text annotators including Neji. An illustrative example of this was the existence of the term “marijuana” in the 2018 n2c2 dataset. Usagi was able to map with a suggestive mapping score of 0.815 to “Cannabis sativa seed oil” because the vocabulary contains synonyms for this standard concept that are more similar to the mention than the proposed mapping. This type of feature could be developed in the Neji annotator, however this would result in losing future community contributions in the Usagi tool. Overall, the proposed system enhances the information present in observational databases that use the OMOP CDM data schema. Examples of the most recent and similar approaches to the proposed one were already described in the Related Work Section 2.2. Although both works were focused on using NLP to leverage the information of OMOP CDM databases, neither of these integrated the resulting data with the data already existing and extracted from the relational model of the EHR system. The work of Liu et al. [36] is very useful to retrieve clinical notes from the repository based on conditions defined in a cohort. Park et al. [37] used the OMOP CDM database to extract the notes from a standard schema in free-text to be then annotated. However, the extracted information was not loaded in the database in the respective tables. 5.3 Future directions The presented work uses several important concepts currently being studied in the health informatics field. Although we were able to create a methodology respecting the standard principles and using tools already validated in other scenarios, it was possible to identify some future directions and necessities in this subject. Neji currently supports machine learning modules and vocabularies, however, this tool does not support deep-learning models. This feature could be a very useful asset in specific scenarios, namely if a richly annotated dataset with similar characteristics to the target data was available or could be compiled. In this case, it would be possible to train a model and possibly obtain better results in the information extraction stage. Another interesting feature would be the integration of the post-processing features directly in Neji. This way, the public annotating service would be able to provide the final annotations without the need for posterior processing steps like in our approach. The modular architecture of Neji facilitates such adaptations and extensions. Another aspect to be improved in this methodology is the handling of negations in the text by the annotator. Our baseline does not consider the occurrence of negations, which could require the use of dataset-specific regex rules in the post processing stage. Preliminary tests with negation handling resulted in a loss in baseline flexibility, thus we decided to not incorporate this feature. Finally, we identified the need for a tool that aggregates Neji and Usagi features in a single solution. We were not able to find such tool, and we believe that merging these features in a unique and collaborative tool could simplify concept extraction and mappings. This could also provide a different support to the annotators during the mapping stage of EHR databases migration pipelines. This is mainly because by having these features merged, it would be easier to add customizable vocabularies for specific institutions without affecting the standard vocabularies provided by Athena. 6 Conclusion Clinical text enriches and expands physicians’ knowledge about their patients. During patient admission, important information is recorded in the clinical notes which is not currently exploited in medical studies. Although several initiatives to improve information extraction in clinical text exist, this data is not commonly used to reach new health findings in observational studies. In this paper, we propose a methodology that was implemented in the Python programming language aiming at 1) the extraction of medication information in clinical notes and 2) the migration of extracted information into a relational standard data model. The system was divided into two main components, the first being responsible for extracting data from the text, and the second for the mapping, harmonisation and storage of data in the standard data schema. In our case, we used the OMOP CDM schema due to its impact on medical findings through the use of observational studies. The proposed system is currently available at During this work, we demonstrated the potential of the developed system using two different datasets. We were able to show a new, different approach for exploring and visualising extracted information through the use of SQL queries and business intelligence tools. With this, we created a new strategy for analysing medical text which facilitates comparative analyses of different datasets, and that can solve an existing gap in ETL procedures regarding observational databases. CRediT authorship contribution statement João Rafael Almeida: Conceptualization, Methodology, Software, Writing - original draft. João Figueira Silva: Conceptualization, Methodology, Software, Writing - original draft. Sérgio Matos: Writing - review & editing, Supervision. José Luís Oliveira: Writing - review & editing, Supervision. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. João Figueira Silva and João Rafael Almeida are funded by the FCT - Foundation for Science and Technology (national funds) under the grants PD/BD/142878/2018 and SFRH/BD/147837/2019 respectively. The authors would like to acknowledge with gratitude, the support provided by Ana Isabel Morais, GP at USF Despertar, ACES, Gondomar, Portugal and Guilherme Oliveira, GP at USF Esgueira, Aveiro, Portugal, in the validation of the medical information present in this manuscript. References [1] S.J. Nass, L.A. Levit, L.O. Gostin, et al., The value, importance, and oversight of health research, National Academies Press (US), 2009. [2] H.G. Cheng M.R. Phillips Secondary analysis of existing data: opportunities and implementation Shanghai Arch. Psychiatry 26 6 2014 371 H.G. Cheng, M.R. Phillips, Secondary analysis of existing data: opportunities and implementation, Shanghai archives of psychiatry 26 (6) (2014) 371. [3] D.G. Katehakis M. Tsiknakis Electronic health record 2006 Wiley 10.1002/9780471740360.ebs1440 D.G. Katehakis, M. Tsiknakis, Electronic health record, Wiley, 2006. URL [4] H.A. Piwowar W.W. Chapman Public sharing of research datasets: a pilot study of associations J. Informet. 4 2 2010 148 156 H.A. Piwowar, W.W. Chapman, Public sharing of research datasets: a pilot study of associations, Journal of informetrics 4 (2) (2010) 148–156. [5] K.J. Peterson G. Jiang H. Liu A corpus-driven standardization framework for encoding clinical problems with hl7 fhir J. Biomed. Inform. 110 2020 103541 10.1016/j.jbi.2020.103541 K.J. Peterson, G. Jiang, H. Liu, A corpus-driven standardization framework for encoding clinical problems with hl7 fhir, Journal of Biomedical Informatics 110 (2020) 103541. doi:10.1016/j.jbi.2020.103541. [6] G. Hripcsak P.B. Ryan J.D. Duke N.H. Shah R.W. Park V. Huser M.A. Suchard M.J. Schuemie F.J. DeFalco A. Perotte Characterizing treatment pathways at scale using the ohdsi network Proc. Nat. Acad. Sci. 113 27 2016 7329 7336 G. Hripcsak, P.B. Ryan, J.D. Duke, N.H. Shah, R.W. Park, V. Huser, M.A. Suchard, M.J. Schuemie, F.J. DeFalco, A. Perotte, et al., Characterizing treatment pathways at scale using the ohdsi network, Proceedings of the National Academy of Sciences 113 (27) (2016) 7329–7336. [7] J.R. Almeida, O. Fajarda, A. Pereira, J.L. Oliveira, Strategies to access patient clinical data from distributed databases, in: Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 5 HEALTHINF: HEALTHINF, INSTICC, SciTePress, 2019, pp. 466–473. [8] S. Sheikhalishahi R. Miotto J.T. Dudley A. Lavelli F. Rinaldi V. Osmani Natural language processing of clinical notes on chronic diseases: Systematic review JMIR Med Inform 7 2 2019 e12239 10.2196/12239 URL S. Sheikhalishahi, R. Miotto, J.T. Dudley, A. Lavelli, F. Rinaldi, V. Osmani, Natural language processing of clinical notes on chronic diseases: Systematic review, JMIR Med Inform 7 (2) (2019) e12239. doi:10.2196/12239. URL [9] K. Jensen, C. Soguero-Ruiz, K. Oyvind Mikalsen, R.-O. Lindsetmo, I. Kouskoumvekaki, M. Girolami, S. Olav Skrovseth, K.M. Augestad, Analysis of free text in electronic health records for identification of cancer patient trajectories, Sci. Rep. 7 (46226). doi: 10.1038/srep46226. [10] S.J. Nelson K. Zeng J. Kilbourne T. Powell R. Moore Normalized names for clinical drugs: RxNorm at 6 years J. Am. Med. Inform. Assoc. 18 4 2011 441 10.1136/amiajnl-2011-000116 S.J. Nelson, K. Zeng, J. Kilbourne, T. Powell, R. Moore, Normalized names for clinical drugs: RxNorm at 6 years, Journal of the American Medical Informatics Association 18 (4) (2011) 441. URL [11] D. Wishart, C. Knox, A. Guo, S. Shrivastava, M. Hassanali, P. Stothard, Z. Chang, J. Woolsey, Drugbank: a comprehensive resource for in silico drug discovery and exploration, Nucleic Acids Res. 34(Database Issue) (2006) D668–D672. doi:10.1093/nar/gkj067. [12] M.Q. Stearns, C. Price, Kent A. Spackman, A.Y. Wang, SNOMED clinical terms: overview of the development process and project status, in: Proceedings of the AMIA Symposium, American Medical Informatics Association, Washington, DC, USA, 2001, pp. 662–666. [13] R. Pivovarov N. Elhadad Automated methods for the summarization of electronic health records J. Am. Med. Inform. Assoc. 22 5 2015 938 947 10.1093/jamia/ocv032 R. Pivovarov, N. Elhadad, Automated methods for the summarization of electronic health records, Journal of the American Medical Informatics Association 22 (5) (2015) 938–947. URL [14] J. Liang, C.-H. Tsou, A. Poddar, A novel system for extractive clinical note summarization using ehr data, in: Proceedings of the 2nd Clinical Natural Language Processing Workshop, 2019, pp. 46–54. [15] S. Fu D. Chen H. He S. Liu S. Moon K.J. Peterson F. Shen L. Wang Y. Wang A. Wen Y. Zhao S. Sohn H. Liu Clinical concept extraction: A methodology review J. Biomed. Inform. 109 2020 103526 10.1016/j.jbi.2020.103526 S. Fu, D. Chen, H. He, S. Liu, S. Moon, K.J. Peterson, F. Shen, L. Wang, Y. Wang, A. Wen, Y. Zhao, S. Sohn, H. Liu, Clinical concept extraction: A methodology review, Journal of Biomedical Informatics 109 (2020) 103526. doi:10.1016/j.jbi.2020.103526. [16] S. Sohn C. Clark S.R. Halgrim S.P. Murphy C.G. Chute H. Liu MedXN: an open source medication extraction and normalization tool for clinical text J. Am. Med. Inform. Assoc. 21 5 2014 858 865 10.1136/amiajnl-2013-002190 S. Sohn, C. Clark, S.R. Halgrim, S.P. Murphy, C.G. Chute, H. Liu, MedXN: an open source medication extraction and normalization tool for clinical text, Journal of the American Medical Informatics Association 21 (5) (2014) 858–865. doi:10.1136/amiajnl-2013-002190. [17] H.L. Weeks C. Beck E. McNeer M.L. Williams C.A. Bejan J.C. Denny L. Choi medExtractR: A targeted, customizable approach to medication extraction from electronic health records J. Am. Med. Inform. Assoc. 27 3 2020 407 418 10.1093/jamia/ocz207 H.L. Weeks, C. Beck, E. McNeer, M.L. Williams, C.A. Bejan, J.C. Denny, L. Choi, medExtractR: A targeted, customizable approach to medication extraction from electronic health records, Journal of the American Medical Informatics Association 27 (3) (2020) 407–418. doi:10.1093/jamia/ocz207. [18] A. Jagannatha, F. Liu, W. Liu, H. Yu, Overview of the First Natural Language Processing Challenge for Extracting Medication, Indication, and Adverse Drug Events from Electronic Health Record Notes (MADE 1.0), Drug Saf. 42 (1) (2019) 99–111. doi:10.1007/s40264-018-0762-z. [19] S. Henry K. Buchan M. Filannino A. Stubbs O. Uzuner 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records J. Am. Med. Inform. Assoc. 27 1 2019 3 12 10.1093/jamia/ocz166 S. Henry, K. Buchan, M. Filannino, A. Stubbs, O. Uzuner, 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records, Journal of the American Medical Informatics Association 27 (1) (2019) 3–12. doi:10.1093/jamia/ocz166. [20] Q. Wei Z. Ji Z. Li J. Du J. Wang J. Xu Y. Xiang F. Tiryaki S. Wu Y. Zhang C. Tao H. Xu A study of deep learning approaches for medication and adverse drug event extraction from clinical text J. Am. Med. Inform. Assoc. 27 1 2019 13 21 10.1093/jamia/ocz063 Q. Wei, Z. Ji, Z. Li, J. Du, J. Wang, J. Xu, Y. Xiang, F. Tiryaki, S. Wu, Y. Zhang, C. Tao, H. Xu, A study of deep learning approaches for medication and adverse drug event extraction from clinical text, Journal of the American Medical Informatics Association 27 (1) (2019) 13–21. doi:10.1093/jamia/ocz063. [21] L. Chen Y. Gu X. Ji Z. Sun H. Li Y. Gao Y. Huang Extracting medications and associated adverse drug events using a natural language processing system combining knowledge base and deep learning J. Am. Med. Inform. Assoc. 27 1 2019 56 64 10.1093/jamia/ocz141 L. Chen, Y. Gu, X. Ji, Z. Sun, H. Li, Y. Gao, Y. Huang, Extracting medications and associated adverse drug events using a natural language processing system combining knowledge base and deep learning, Journal of the American Medical Informatics Association 27 (1) (2019) 56–64. doi:10.1093/jamia/ocz141. [22] A.R. Aronson F.-M. Lang An overview of MetaMap: historical perspective and recent advances J. Am. Med. Inform. Assoc. 17 3 2010 229 236 10.1136/jamia.2009.002733 A.R. Aronson, F.-M. Lang, An overview of MetaMap: historical perspective and recent advances, Journal of the American Medical Informatics Association 17 (3) (2010) 229–236. URL [23] D.R. Harris D.W. Henderson A. Corbeau sig2db: a workflow for processing natural language from prescription instructions for clinical data warehouses, AMIA Joint Summits on Translational Science proceedings AMIA Joint Summits Translat. Sci. 2020 2020 221 230 URL D.R. Harris, D.W. Henderson, A. Corbeau, sig2db: a workflow for processing natural language from prescription instructions for clinical data warehouses, AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science 2020 (2020) 221–230. URL [24] G.K. Savova J.J. Masanz P.V. Ogren J. Zheng S. Sohn K.C. Kipper-Schuler C.G. Chute Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications J. Am. Med. Inform. Assoc. 17 5 2010 507 513 10.1136/jamia.2009.001560 G.K. Savova, J.J. Masanz, P.V. Ogren, J. Zheng, S. Sohn, K.C. Kipper-Schuler, C.G. Chute, Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications, Journal of the American Medical Informatics Association 17 (5) (2010) 507–513. doi:10.1136/jamia.2009.001560. [25] S. Matos Configurable web-services for biomedical document annotation J. Cheminformat. 10 1 2018 68 10.1186/s13321-018-0317-4 S. Matos, Configurable web-services for biomedical document annotation, Journal of cheminformatics 10 (1) (2018) 68. doi:10.1186/s13321-018-0317-4. [26] J.R. Almeida, S. Matos, Rule-based extraction of family history information from clinical notes, in: Proceedings of the 35th Annual ACM Symposium on Applied Computing, SAC ’20, Association for Computing Machinery, New York, NY, USA, 2020, p. 670–675. doi:10.1145/3341105.3374000. [27] P. Ranganathan R. Aggarwal Study designs: Part 1–an overview and classification Perspect. Clin. Res. 9 4 2018 184 P. Ranganathan, R. Aggarwal, Study designs: Part 1–an overview and classification, Perspectives in clinical research 9 (4) (2018) 184. [28] S.N. Murphy G. Weber M. Mendis V. Gainer H.C. Chueh S. Churchill I. Kohane Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2) J. Am. Med. Inform. Assoc. 17 2 2010 124 130 S.N. Murphy, G. Weber, M. Mendis, V. Gainer, H.C. Chueh, S. Churchill, I. Kohane, Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2), Journal of the American Medical Informatics Association 17 (2) (2010) 124–130. [29] A.J. McMurry S.N. Murphy D. MacFadden G. Weber W.W. Simons J. Orechia J. Bickel N. Wattanasin C. Gilbert P. Trevvett Shrine: enabling nationally scalable multi-site disease studies PloS One 8 3 2013 e55811 A.J. McMurry, S.N. Murphy, D. MacFadden, G. Weber, W.W. Simons, J. Orechia, J. Bickel, N. Wattanasin, C. Gilbert, P. Trevvett, et al., Shrine: enabling nationally scalable multi-site disease studies, PloS one 8 (3) (2013) e55811. [30] G. Hripcsak J.D. Duke N.H. Shah C.G. Reich V. Huser M.J. Schuemie M.A. Suchard R.W. Park I.C.K. Wong P.R. Rijnbeek Observational health data sciences and informatics (ohdsi): opportunities for observational researchers Studies Health Technol. Informat. 216 2015 574 G. Hripcsak, J.D. Duke, N.H. Shah, C.G. Reich, V. Huser, M.J. Schuemie, M.A. Suchard, R.W. Park, I.C.K. Wong, P.R. Rijnbeek, et al., Observational health data sciences and informatics (ohdsi): opportunities for observational researchers, Studies in health technology and informatics 216 (2015) 574. [31] J.M. Overhage P.B. Ryan C.G. Reich A.G. Hartzema P.E. Stang Validation of a common data model for active safety surveillance research J. Am. Med. Inform. Assoc. 19 1 2011 54 60 J.M. Overhage, P.B. Ryan, C.G. Reich, A.G. Hartzema, P.E. Stang, Validation of a common data model for active safety surveillance research, Journal of the American Medical Informatics Association 19 (1) (2011) 54–60. [32] R. Makadia, P.B. Ryan, Transforming the premier perspective hospital database into the observational medical outcomes partnership (omop) common data model, Egems 2(1) (2014). [33] J. Badger E. LaRose J. Mayer F. Bashiri D. Page P. Peissig Machine learning for phenotyping opioid overdose events J. Biomed. Inform. 94 2019 103185 10.1016/j.jbi.2019.103185 J. Badger, E. LaRose, J. Mayer, F. Bashiri, D. Page, P. Peissig, Machine learning for phenotyping opioid overdose events, Journal of Biomedical Informatics 94 (2019) 103185. [34] S. Lovestone, E. Consortium, The european medical information framework: A novel ecosystem for sharing healthcare data across europe, Learning Health Syst. 4(2) (2020) e10214. [35] OHDSI, Usagi (2020). [36] S. Liu Y. Wang A. Wen L. Wang N. Hong F. Shen S. Bedrick W. Hersh H. Liu Implementation of a cohort retrieval system for clinical data repositories using the observational medical outcomes partnership common data model: Proof-of-concept system validation JMIR Med. Informat. 8 10 2020 e17376 S. Liu, Y. Wang, A. Wen, L. Wang, N. Hong, F. Shen, S. Bedrick, W. Hersh, H. Liu, Implementation of a cohort retrieval system for clinical data repositories using the observational medical outcomes partnership common data model: Proof-of-concept system validation, JMIR medical informatics 8 (10) (2020) e17376. [37] J. Park S.C. You E. Jeong C. Weng D. Park J. Roh D.Y. Lee J.Y. Cheong J.W. Choi M. Kang A framework (socratex) for hierarchical annotation of unstructured electronic health records and integration into a standardized medical database: Development and usability study JMIR Med. Informat. 9 3 2021 e23983 J. Park, S.C. You, E. Jeong, C. Weng, D. Park, J. Roh, D.Y. Lee, J.Y. Cheong, J.W. Choi, M. Kang, et al., A framework (socratex) for hierarchical annotation of unstructured electronic health records and integration into a standardized medical database: Development and usability study, JMIR Medical Informatics 9 (3) (2021) e23983. [38] M. Garza G. Del Fiol J. Tenenbaum A. Walden M.N. Zozus Evaluating common data models for use with a longitudinal community registry J. Biomed. Informat. 64 2016 333 341 M. Garza, G. Del Fiol, J. Tenenbaum, A. Walden, M.N. Zozus, Evaluating common data models for use with a longitudinal community registry, Journal of biomedical informatics 64 (2016) 333–341. [39] O. Bodenreider The Unified Medical Language System (UMLS): integrating biomedical terminology Nucleic Acids Res. 32 suppl_1 2004 D267 D270 10.1093/nar/gkh061 O. Bodenreider, The Unified Medical Language System (UMLS): integrating biomedical terminology, Nucleic Acids Research 32 (suppl_1) (2004) D267–D270. doi:10.1093/nar/gkh061. [40] Ö. Uzuner I. Solti E. Cadag Extracting medication information from clinical text J. Am. Med. Inform. Assoc. 17 5 2010 514 518 10.1136/jamia.2010.003947 Ö. Uzuner, I. Solti, E. Cadag, Extracting medication information from clinical text, Journal of the American Medical Informatics Association 17 (5) (2010) 514–518. doi:10.1136/jamia.2010.003947. [41] E. Burn S.C. You A.G. Sena K. Kostka H. Abedtash M.T.F. Abrahão A. Alberga H. Alghoul O. Alser T.M. Alshammari Deep phenotyping of 34,128 adult patients hospitalised with covid-19 in an international network study Nature Commun. 11 1 2020 1 11 E. Burn, S.C. You, A.G. Sena, K. Kostka, H. Abedtash, M.T.F. Abrahão, A. Alberga, H. Alghoul, O. Alser, T.M. Alshammari, et al., Deep phenotyping of 34,128 adult patients hospitalised with covid-19 in an international network study, Nature communications 11 (1) (2020) 1–11. [42] M.A. Suchard, M.J. Schuemie, H.M. Krumholz, S.C. You, R. Chen, N. Pratt, C.G. Reich, J. Duke, D. Madigan, G. Hripcsak, P.B. Ryan, Comprehensive comparative effectiveness and safety of first-line antihypertensive drug classes: a systematic, multinational, large-scale analysis, The Lancet 394 (10211) (2019) 1816–1826, supplementary material. doi: 10.1016/S0140-6736(19)32317-7. [43] A.F. Markus J.A. Kors P.R. Rijnbeek The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies J. Biomed. Inform. 113 2021 103655 10.1016/j.jbi.2020.103655 A.F. Markus, J.A. Kors, P.R. Rijnbeek, The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies, Journal of Biomedical Informatics 113 (2021) 103655. URL "
    },
    {
        "doc_title": "Optimizing blood-brain barrier permeation through deep reinforcement learning for de novo drug design",
        "doc_scopus_id": "85111954864",
        "doc_doi": "10.1093/bioinformatics/btab301",
        "doc_eid": "2-s2.0-85111954864",
        "doc_date": "2021-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Biological Transport",
            "Blood-Brain Barrier",
            "Drug Design",
            "Neural Networks, Computer"
        ],
        "doc_abstract": "© 2021 The Author(s). Published by Oxford University Press.Motivation: The process of placing new drugs into the market is time-consuming, expensive and complex. The application of computational methods for designing molecules with bespoke properties can contribute to saving resources throughout this process. However, the fundamental properties to be optimized are often not considered or conflicting with each other. In this work, we propose a novel approach to consider both the biological property and the bioavailability of compounds through a deep reinforcement learning framework for the targeted generation of compounds. We aim to obtain a promising set of selective compounds for the adenosine A2A receptor and, simultaneously, that have the necessary properties in terms of solubility and permeability across the blood-brain barrier to reach the site of action. The cornerstone of the framework is based on a recurrent neural network architecture, the Generator. It seeks to learn the building rules of valid molecules to sample new compounds further. Also, two Predictors are trained to estimate the properties of interest of the new molecules. Finally, the fine-tuning of the Generator was performed with reinforcement learning, integrated with multi-objective optimization and exploratory techniques to ensure that the Generator is adequately biased. Results: The biased Generator can generate an interesting set of molecules, with approximately 85% having the two fundamental properties biased as desired. Thus, this approach has transformed a general molecule generator into a model focused on optimizing specific objectives. Furthermore, the molecules' synthesizability and drug-likeness demonstrate the potential applicability of the de novo drug design in medicinal chemistry.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A recommender system to help refining clinical research studies",
        "doc_scopus_id": "85107238346",
        "doc_doi": "10.3233/SHTI210174",
        "doc_eid": "2-s2.0-85107238346",
        "doc_date": "2021-07-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Cohort Studies",
            "Information Storage and Retrieval",
            "Search Engine"
        ],
        "doc_abstract": "© 2021 European Federation for Medical Informatics (EFMI) and IOS Press. All rights reserved.The process of refining the research question in a medical study depends greatly on the current background of the investigated subject. The information found in prior works can directly impact several stages of the study, namely the cohort definition stage. Besides previous published methods, researchers could also leverage on other materials, such as the output of cohort selection tools, to enrich and to accelerate their own work. However, this kind of information is not always captured by search engines. In this paper, we present a methodology, based on a combination of content-based retrieval and text annotation techniques, to identify relevant scientific publications related to a research question and to the selected data sources. © 2021 European Federation for Medical Informatics (EFMI) and IOS Press.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An architecture to define cohorts over medical imaging datasets",
        "doc_scopus_id": "85110924376",
        "doc_doi": "10.1109/CBMS52027.2021.00088",
        "doc_eid": "2-s2.0-85110924376",
        "doc_date": "2021-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Biomedical images",
            "Data and metadata",
            "Electronic health record systems",
            "Hierarchical structures",
            "Increasing production",
            "Medical researchers",
            "Observational study",
            "WEB application"
        ],
        "doc_abstract": "© 2021 IEEE.The DICOM standard has been widely adopted for the exchange and management of biomedical images. Its hierarchical structure allows representing data and metadata of medical imaging studies. However, other patient data not directly related with the study, such as prescriptions and treatments, are stored in independent Electronic Health Record (EHR) systems. With the increasing production of medical imaging studies, repositories responsible for storing DICOM images started to contain massive amounts of data. Therefore, retrieving a subset of images based on similar criteria as the ones used in EHR systems, is a complex task for a medical researcher. In this paper, we propose an architecture to define cohorts over medical imaging data sets. This proposal uses a DICOM archive to index and to retrieve images, while the studies' selection is performed through a web application, ATLAS, which is normally used on observational studies upon EHR data. The presented architecture was validated using a public data set with synthetic EHR data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Easing the questioning of semantic biomedical data",
        "doc_scopus_id": "85110837306",
        "doc_doi": "10.1109/CBMS52027.2021.00044",
        "doc_eid": "2-s2.0-85110837306",
        "doc_date": "2021-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Biomedical domain",
            "Distributed repositories",
            "Extract informations",
            "Natural languages",
            "Production environments",
            "Querying services",
            "Semantic database",
            "Semantic technologies"
        ],
        "doc_abstract": "© 2021 IEEE.Researchers have been using semantic technologies as essential tools to structure knowledge. This is particularly relevant in the biomedical domain, where large dataset are continuously generated. Semantic technologies offer the ability to describe data and to map and linking distributed repositories, creating a network where the searching interface is a single entry point. However, the increasing number of semantic data repositories that are publicly available is creating new challenges related to its exploration. Despite being human and machine-readable, these technologies are much more challenging for end-users. Querying services usually require mastering formal languages and that knowledge is beyond the typical user's expertise, being a critical issue in adopting semantic web information systems. In particular, the questioning of biomedical data presents specific challenges for which there are still no mature proposals for production environments. This paper presents a solution to query biomedical semantic databases using natural language. The system is at the intersection between semantic parsing and the use of templates. It makes it possible to extract information in a friendly way for users who are not experts in semantic queries.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A comparative analysis of data platforms for rare diseases",
        "doc_scopus_id": "85110827717",
        "doc_doi": "10.1109/CBMS52027.2021.00041",
        "doc_eid": "2-s2.0-85110827717",
        "doc_date": "2021-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Comparative analysis",
            "Comprehensive analysis",
            "Computational solutions",
            "Heterogeneous database",
            "Inclusion and exclusions",
            "Medical knowledge",
            "Medical researchers",
            "Research communities"
        ],
        "doc_abstract": "© 2021 IEEE.The increasing interest in finding drugs and treatments for rare diseases led to the creation of research studies and clinical trials which data and results have been stored in multiple, heterogeneous databases. The lack of data harmonisation, combined with the need to improve current medical knowledge, has encouraged the research community to create computational solutions to aggregate this information. Although such platforms were created in the same area, orphan diseases, they were normally developed for different purposes, increasing the task complexity for end-users when needing to search for gene-to-phenotype information (e.g. genes, mutations, symptoms, etc.). Aiming to help answer these questions, we conducted a comprehensive analysis of the existent platforms designed to retrieve and visualise information about genetic rare diseases. In this analysis, we found several platforms from which we identified 7 candidates based on a set of inclusion and exclusion criteria. Through this analysis we were able to assess each system's characteristics and identify the most appropriate for distinct use cases and audiences, namely medical researchers, bioinformaticians and patients and relatives.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improvements in lymphocytes detection using deep learning with a preprocessing stage",
        "doc_scopus_id": "85110823741",
        "doc_doi": "10.1109/CBMS52027.2021.00068",
        "doc_eid": "2-s2.0-85110823741",
        "doc_date": "2021-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Adaptive immune systems",
            "Automated image analysis",
            "Classification algorithm",
            "Color normalization",
            "Detection and quantifications",
            "Morphological operations",
            "Pre-processing method",
            "Semantic segmentation"
        ],
        "doc_abstract": "© 2021 IEEE.Lymphocytes are a type of white blood cell that are part of the adaptive immune system and respond to infectious microorganisms. Due to this key role, its detection and quantification allow analyzing the overall status of the immune system. However, the manual detection of lymphocytes in tissue slices is a laborious task, and it depends on the expertise of the observer, reason why an automated image analysis helps to speedup this process. Several different techniques have been used to automatize this task, such as morphological operations, classification algorithms, and, more recently, deep learning approaches. In this work, we propose two preprocessing methods for improving the lymphocytes detection in digital images. Furthermore, this study proposes a change in the ground truth, in order to turn it into a segmentation map, and evaluate semantic segmentation models in a dataset that originally does not allow this approach. Two deep learning models (Segnet and U-Net) with different backbones (VGG16 and Resnet50) were used for the training and test sets. One of the proposed methods showed an F1-score 11% higher than simply using a color normalization. The results were compared with other state-of-the-art studies, showing one of the best-ranked results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cross-evaluation of social mining for classification of depressed online personas",
        "doc_scopus_id": "85111785951",
        "doc_doi": "10.1515/jib-2020-0051",
        "doc_eid": "2-s2.0-85111785951",
        "doc_date": "2021-05-20",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Data Mining",
            "Machine Learning"
        ],
        "doc_abstract": "© 2021 Alina Trifan and José Luis Oliveira published by De Gruyter, Berlin/Boston.With the continuous increase in the use of social networks, social mining is steadily becoming a powerful component of digital phenotyping. In this paper we explore social mining for the classification of self-diagnosed depressed users of Reddit as social network. We conduct a cross evaluation study based on two public datasets in order to understand the impact of transfer learning when the data source is virtually the same. We further complement these results with an experiment of transfer learning in post-partum depression classification, using a corpus we have collected for the matter. Our findings show that transfer learning in social mining might still be at an early stage in computational research and we thoroughly discuss its implications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A semi-automatic methodology for analysing distributed and private biobanks",
        "doc_scopus_id": "85098456035",
        "doc_doi": "10.1016/j.compbiomed.2020.104180",
        "doc_eid": "2-s2.0-85098456035",
        "doc_date": "2021-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Access restriction",
            "Distributed repositories",
            "Genetic variants",
            "Genomic studies",
            "Practical schemes",
            "Privacy issue",
            "Rare disease",
            "Semi-automatics",
            "Biological Specimen Banks",
            "COVID-19",
            "Humans",
            "Public Health",
            "SARS-CoV-2"
        ],
        "doc_abstract": "© 2020 The AuthorsPrivacy issues limit the analysis and cross-exploration of most distributed and private biobanks, often raised by the multiple dimensionality and sensitivity of the data associated with access restrictions and policies. These characteristics prevent collaboration between entities, constituting a barrier to emergent personalized and public health challenges, namely the discovery of new druggable targets, identification of disease-causing genetic variants, or the study of rare diseases. In this paper, we propose a semi-automatic methodology for the analysis of distributed and private biobanks. The strategies involved in the proposed methodology efficiently enable the creation and execution of unified genomic studies using distributed repositories, without compromising the information present in the datasets. We apply the methodology to a case study in the current Covid-19, ensuring the combination of the diagnostics from multiple entities while maintaining privacy through a completely identical procedure. Moreover, we show that the methodology follows a simple, intuitive, and practical scheme.",
        "available": true,
        "clean_text": "serial JL 271150 291210 291870 291901 31 90 Computers in Biology and Medicine COMPUTERSINBIOLOGYMEDICINE 2020-12-18 2020-12-18 2020-12-22 2020-12-22 2021-02-18T04:42:31 S0010-4825(20)30511-4 S0010482520305114 10.1016/j.compbiomed.2020.104180 S300 S300.1 FULL-TEXT 2021-04-08T04:29:49.090165Z 0 0 20210301 20210331 2021 2020-12-18T01:57:37.637417Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst ref 0010-4825 00104825 UNLIMITED FCT true 130 130 C Volume 130 5 104180 104180 104180 202103 March 2021 2021-03-01 2021-03-31 2021 Regular Articles article fla © 2020 The Authors. Published by Elsevier Ltd. ASEMIAUTOMATICMETHODOLOGYFORANALYSINGDISTRIBUTEDPRIVATEBIOBANKS ALMEIDA J 1 Introduction 2 Genomics interoperability 3 Methodology 3.1 The Genomics Catalogue 3.2 The genomics and proteomics toolkit 3.3 The workflow manager 3.4 Overview 4 Results 5 Research application 5.1 Study example: SARS-CoV-2 5.2 Validation 6 Discussion 7 Conclusions References JOHNSTON 2018 S2 S6 J KAYE 2012 415 431 J JALILI 2017 90 109 V COPPOLA 2019 172 L LITTON 2018 233 241 J LIU 2015 55 68 A BIOBANKINGIN21STCENTURY BIOBANKINGFORPERSONALIZEDMEDICINE AMORIM 2020 102333 A LANGHOF 2017 293 300 H KULYNYCH 2017 94 132 J CLAES 2014 P MCLAREN 2016 814 822 P GRISHIN 2019 1115 1117 D MASSEROLI 2016 3 11 M ALMEIDA 2019 466 473 J PROCEEDINGS12THINTERNATIONALJOINTCONFERENCEBIOMEDICALENGINEERINGSYSTEMSTECHNOLOGIESVOLUME5HEALTHINFHEALTHINF STRATEGIESACCESSPATIENTCLINICALDATADISTRIBUTEDDATABASES GAZIANO 2016 214 223 J SUDLOW 2015 C HOSSEINI 2019 146 148 M TOPPINEN 2020 102353 M CHO 2018 547 551 H IOANNIDIS 2006 609 614 J SHRINGARPURE 2015 631 646 S 2012 57 74 2015 68 74 SILVA 2018 33 42 L OLIVEIRA 2019 J ALMEIDA 2020 100535 J PRATAS 2020 D LIEW 2017 66 C HOLL 2014 352 362 S WOLSTENCROFT 2013 W557 W561 K GOECKS 2010 R86 J ALMEIDA 2019 121 J WU 2020 265 269 F HEENEY 2011 17 25 C PRATAS 2018 1177 1181 D 201826THEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO METAGENOMICCOMPOSITIONANALYSISSEDIMENTARYANCIENTDNAISLEWIGHT PRATAS 2018 445 D ZIELEZINSKI 2019 144 A HUANG 2011 593 594 W LIN 2004 Z PRATAS 2020 D VAYENA 2016 e1001937 E HRIPCSAK 2015 574 G BOS 2018 64 I TKACHENKO 2018 221 235 O PROCEEDINGS2018ASIACONFERENCECOMPUTERCOMMUNICATIONSSECURITY LARGESCALEPRIVACYPRESERVINGSTATISTICALCOMPUTATIONSFORDISTRIBUTEDGENOMEWIDEASSOCIATIONSTUDIES GOLDREICH 1998 O KANNAN 2016 603 615 L HUFSKY 2020 F HOFFMANN 2020 M SCHAFFER 2020 1 23 A ALMEIDAX2021X104180 ALMEIDAX2021X104180XJ Full 2020-12-17T17:09:46Z FundingBody Portugal Institutes 2021-12-22T00:00:00.000Z 2021-12-22T00:00:00.000Z This is an open access article under the CC BY-NC-ND license. © 2020 The Authors. Published by Elsevier Ltd. 2020-12-17T17:04:36.024Z IMI Innovative Medicines Initiative FCT Fundação para a Ciência e a Tecnologia item S0010-4825(20)30511-4 S0010482520305114 10.1016/j.compbiomed.2020.104180 271150 2021-04-08T04:29:49.090165Z 2021-03-01 2021-03-31 UNLIMITED FCT true 1912409 MAIN 8 55182 849 656 IMAGE-WEB-PDF 1 gr1 102992 240 388 gr2 91558 263 535 gr3 115255 352 535 gr4 115141 254 535 gr1 76156 135 219 gr2 70900 107 219 gr3 76244 144 219 gr4 75175 104 219 gr1 251665 1064 1721 gr2 265461 1163 2370 gr3 434092 1559 2369 gr4 373059 1125 2370 am 1762750 CBM 104180 104180 S0010-4825(20)30511-4 10.1016/j.compbiomed.2020.104180 The Authors Fig. 1 The main actors involved in each study: Researcher (R), Study Manager (SM), and the Data Owners (DO). Fig. 1 Fig. 2 The Genomics Catalogue showing the demo repositories and the first stage of the study request. Fig. 2 Fig. 3 The TASKA workflow manager with an example study to analyse the complexity profile region in genomics data. Fig. 3 Fig. 4 Overview of the proposed methodology involving the three main actors and their responsibilities. Fig. 4 Table 1 The five most representative reference sequences according to the RS (Relative Similarity). ID stands for the order of the top output, length for the size of the reference genome, and GID for the sequence global identifier. Table 1 ID Length RS (%) Reference GID Virus name 1 124884 96.2539 NC_001348.1 HHV3 2 29903 95.4904 NC_045512.2 SARS-CoV-2 3 16568 78.2749 NC_012920.1 MT 4 5028 1.3435 NC_004295.1 HEVv9 5 162114 0.0000 NC_000898.1 HHV 6 Table 2 The twenty most representative SARS-CoV-2 genome sequences according to the RS (Relative Similarity). ID stands for the order of the top output, length for the size of the reference genome, and GID for the sequence global identifier. Table 2 ID Length RS (%) GID 1 29893 97.6044 MT007544.1 2 29854 97.4854 MN996530.1 3 29862 97.4833 MT192759.1 4 29833 97.4813 MT039873.1 5 29881 97.4744 MN988669.1 6 29881 97.4744 MN988668.1 7 29903 97.4724 MN908947.3 8 29903 97.4724 NC_045512.2 9 29891 97.4720 MN996528.1 10 29890 97.4720 MT019532.1 11 29860 97.4695 MT093631.2 12 29825 97.4668 MN996527.1 13 29883 97.4646 MN994468.1 14 29811 97.4619 MT072688.1 15 29882 97.4599 MT184907.2 16 29882 97.4599 MT184909.2 17 29882 97.4599 MT184912.1 18 29882 97.4599 MT159711.2 19 29882 97.4599 MT159714.2 20 29882 97.4599 MT159719.2 ☆ This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968. J.R. Almeida is funded by the National Science Foundation (FCT), under the grant SFRH/BD/147837/2019. D. Pratas is funded by national funds through FCT, I.P., under the Scientific Employment Stimulus - Institutional Call - CI-CTTI-94-ARH/2019 A semi-automatic methodology for analysing distributed and private biobanks João Rafael Almeida a b ∗ Diogo Pratas a c José Luís Oliveira a ∗∗ a DETI/IEETA, University of Aveiro, Aveiro, Portugal DETI/IEETA University of Aveiro Aveiro Portugal DETI/IEETA, University of Aveiro, Aveiro, Portugal b Department of Computation, University of A Coruña, A Coruña, Spain Department of Computation University of A Coruña A Coruña Spain Department of Computation, University of A Coruna, A Coruna, Spain c Department of Virology, University of Helsinki, Helsinki, Finland Department of Virology University of Helsinki Helsinki Finland Department of Virology, University of Helsinki, Helsinki, Finland ∗ Corresponding author. DETI/IEETA, University of Aveiro, Aveiro, Portugal. DETI/IEETA University of Aveiro Aveiro Portugal ∗∗ Corresponding author. Privacy issues limit the analysis and cross-exploration of most distributed and private biobanks, often raised by the multiple dimensionality and sensitivity of the data associated with access restrictions and policies. These characteristics prevent collaboration between entities, constituting a barrier to emergent personalized and public health challenges, namely the discovery of new druggable targets, identification of disease-causing genetic variants, or the study of rare diseases. In this paper, we propose a semi-automatic methodology for the analysis of distributed and private biobanks. The strategies involved in the proposed methodology efficiently enable the creation and execution of unified genomic studies using distributed repositories, without compromising the information present in the datasets. We apply the methodology to a case study in the current Covid-19, ensuring the combination of the diagnostics from multiple entities while maintaining privacy through a completely identical procedure. Moreover, we show that the methodology follows a simple, intuitive, and practical scheme. Keywords Genomics studies Distributed biobanks Genomics cross-exploration Coronavirus 2 SARS-CoV-2 Secondary use 1 Introduction The continuous evolution of sequencing technology prompted a new step in clinical research, in which it is possible to scan the whole-genome of individual DNA samples at an acceptable cost and time [1,2]. This increasing generation of genomics data also led to big data problems which require the reorganization of current biobank policies [3]. In fact, this can be considered an opportunity to build infrastructures that allow the linkage of existing biobanks and bio clinical projects [4]. Biobanking currently represents a new research field that involves international infrastructures and government agencies requiring the creation of policies to provide ethical and legal guidelines for public health [5]. The need for high-quality and clinically annotated biospecimens for personalized medicine and forensic applications is raising new research challenges [6,7]. However, other major problems have followed this growth, namely the evolution of biobanking in a decentralized way, with heterogeneous procedures for data collection and storage, as well as different legal policies for data access [8]. One of the key challenges is to find the right balance between preserving the privacy of the subjects in the study and the data availability for sharing the results through global research networks [2]. Although genomics datasets are not linked to medical records, which preserves subject identity [9], some authors tried to reverse the process only using the DNA present in the datasets. Claes et al. [10] show it is possible to model a 3D face using genomics data obtaining a similar facial appearance. Privacy issues are one of the main obstructions in health research, including in the area of genomics [11,12]. Answers to biomedical questions may currently be hidden in private data repositories that are not explored due to the lack of methodologies to analyse this data [13]. The problem can be addressed at different levels, from biomedical data discovery to multi-repository analysis, i. e., there are gaps in the way biobanks are exposed to the research community and the methodologies currently available are not designed to simplify the exploration of multiple and private repositories. The idea of distributing research questions over distinct organisations and databases has been applied to allow the secondary use of multiple Electronic Health Records (EHR) [14]. However, no such experience has been proposed in the genomics field, which could help in discovering new disease biomarkers, especially in rare diseases. In short, we can identify three main issues that are currently present in genomics studies: • privacy issues: the genomics dataset can contain private information that allows subject identification, which causes access problems; • data interoperability: the metadata of the private repositories are publicly available following heterogeneous description; • data access: since sharing data will violate privacy issues, the exploration of multiple repositories is currently a poorly explored subject. Therefore, for the success of genomics research, it is essential to create solutions that decrease the impact of these issues. In this paper, we present a methodology that allows the exploration of distributed genomics repositories, without giving researchers direct contact with the data. This ensures data privacy and speeds up the process of exploring multiple datasets, which is imperative to increase the impact of new findings. 2 Genomics interoperability Exploring distributed and private biobanks without compromising ethical regulations is a complex subject already studied by the research community. Almost all sequenced genomes are currently stored in protected repositories with strict access rules or securely encrypted [15–18]. However, accessing those datasets allows analysis of a larger number of subjects in order to identify genetic variants that are statistically correlated [19]. This is essential to detect genetic signals expressed with small effect size or in rare variants [20]. The problem with ensuring subjects’ privacy when analyzing their genomics information is crucial. Cho et al. [19] described an approach for genome-wide association studies, using cryptographic techniques for secure data analysis while maintaining the confidentiality of genotypes and phenotypes. Although they ensure a private and secure environment, the data needs to be shared with the participants involved in each study. This solution is very useful, but this scenario requires that all the institutions follow this architecture. Another technique for genomics data sharing is the use of web services that provide only allele-presence information, designated as “beacon”. This was created in the Beacon Project by the Global Alliance for Genomics & Health (GA4GH) 1 1 where users can perform queries about genomics information in the institutional beacons [21]. However, some authors have already studied the risk of using this technique, where users can perform as many queries as they want, based on the limitations of the services. Shringarpure et al. [21] show through simulations that in a beacon with 1 000 subjects, re-identification was possible with 5 000 queries. With the extraordinary advances in genomics, several consortia have collected and made publicly available some of those data after processing, mainly due to the complexity in sharing genomics data and aiming to explore human genes. The Encyclopedia of DNA Elements (ENCODE) project mapped genomics regions of transcription, transcription factor association, chromatin structure and histone modification [22]. This project aims to identify functional elements in the human genomes. The data collected during the project enabled the assignment of biochemical functions for 80% of the genome [22]. In the ENCODE Portal 2 2 the data is accessible to the research community. Another project with the goal of studying the human genome is the 1000 Genomes Project. During the project, the genomes of 2 504 individuals from 26 populations [23] were reconstructed. This allowed the establishment of a detailed catalogue of human genetic variations. Other projects aim at genomics data collection and analysis, but focus on specific goals. In these approaches, the reuse of the collected data is complex and privacy assurance can be questionable. Therefore, the goal of this methodology is to create strategies to study the genomics raw data that have been collected over the last decades without compromising any privacy issue or institutional rule. 3 Methodology The proposed methodology simplifies the process of studying genomics data in distributed and private repositories. This approach streamlines the entire process without removing full control of the data from the Data Owners. The data is kept in the Data Owners’ repositories and the analyses are performed locally. At the end, each Data Owner can share the results of the study without ever exposing the data repositories. In each study there are three different roles for the various participants (Fig. 1 ): • the Data Owner (DO): the entity responsible for managing the genomics repositories of each organisation; • the Researcher (R): the entity interested in analysing the data repositories and in performing the study; • the Study Manager (SM): the entity who coordinates the study and the distinct actors in the study. 3.1 The Genomics Catalogue The main purpose of this platform is to simplify genomics data exposure and sharing, without giving access to the data. Besides, the platform contains study management features, where users can select the repositories of interest and make research questions. Fig. 2 shows the platform with several repositories and the creation of a study request, i.e. a brief description of the research question to be tackled. This platform was built on top of Montra Framework [24] which was designed as an general engine to build data catalogues of biomedical databases. This system has a flexible data skeleton used to characterise data entities, and is easy to customise for different purposes. The tool also ensures access control and data privacy, following rule-based access policies that are essential to control what data is visible for the different access levels. Moreover, the data available in the platform only characterise the repositories’ content, which does not expose any sensitive information. The Montra Framework was previously used in other initiatives of biomedical data sharing, such as EMIF-Catalogue. 3 3 This web platform, developed in the European Medical Information Framework project (EMIF), 4 4 allows data owners to share information about their databases at different levels of access [25]. Similar to the Genomics Catalogue, the goal was to share metadata information about private data repositories in order to help researchers to find datasets of interest. 3.2 The genomics and proteomics toolkit The genomics and proteomics data in each repository usually follow standard formats. Therefore, we used GTO, 5 5 a standalone toolkit to unify pipelines operating both at genomic and proteomic levels. This toolkit is composed of applications that allow the creation of workflows for identification of metagenomic composition in FASTQ reads, detection and visualisation of genomic rearrangements, mapping and visualisation of variation, localisation of low complexity regions, simulation of sequences with specific SNP and structural variant rates, among many other features [26]. GTO is open-source and written in a low-level language (e.g. C language) without external dependencies, built for ultra-fast computations and flexible integrations. The advantage of using open-source tools is the possibility to compile the code locally and have deep knowledge of how the tools are operating over the data. This allows technical teams from the DOs’ institutions to make sure there are no information leaks while maintaining faster computations. The toolkit supports Unix-like pipelines for easy integration of the available tools, which allows the creation of multiple processing workflows to answer research questions. This feature allows using a chain of processes, which can be easily shared between the DOs and executed locally, as shown in the following example: tool_1 < input | tool_2 | tool_3 > output To take full advantage of the GTO tools, end-users need to have basic shell script knowledge in order to build the analytic scripts. Therefore, script construction and validation is the responsibility of the SM, not invalidating the possibility of this being provided by the researcher. Several validated examples of pipelines built using this toolkit are currently available at and have been used in many projects, for example, in a component of the development and integration of TRACESPipe [27]. 3.3 The workflow manager Studies have been simplified through the use of scientific workflow systems, which allow the combination and execution of computational processes, in cascade and over distributed environments [28,29]. One popular system is Taverna, 6 6 an open-source scientific workflow management system used to facilitate computer simulation of repeatable scientific experiments [30]. Galaxy 7 7 is another commonly used scientific workflow management system, with a user-friendly web interface that allows collaborative discussion of results and studies’ replication [31]. However, in our proposal, the study workflow can be coordinated independently of the solution chosen. In previous work, we developed TASKA, 8 8 which is a task/workflow management system designed as a modular web platform to facilitate studies' execution, team coordination, task scheduling and researcher collaboration [32]. In our pipeline, the study's coordination and monitoring were performed through this system, which contains different types of tasks essential for managing genomic studies: • a Simple task, where instructions are provided about what must be done. For instance, running a script attached to the task when there is no direct connection to execute it automatically in the repository. • a Form task, which allows the construction of a simple online questionnaire (text, multiple choices, etc.) which is then completed by assigned users. This task is used in our proposal for the DOs to indicate their availability to provide answers in each study or answer the research question based on the script execution. • a Processing task, designed to execute operations automatically without user interaction. For instance, running a pipeline built using the GTO toolkit over the data, when a direct connection is available (i.e., when the repository has a public API). The SM conducting the study is responsible for task assignment, scheduling management, and results compilation at the end. Fig. 3 shows a simple workflow to run a study aiming to analyse the complexity profile region in genomics data. The script was attached in the first task, and it will be executed by each DO locally in the second task. Then, the results are uploaded in the third task, when the SM will analyse, compile and answer the question in the final task. 3.4 Overview The proposed methodologies centralise the main information in the Repository Catalogue, which is the portal to present and manage the data available. In this platform, researchers can search for data sources of interest, create study requests and monitor the study status. However, other users with more advanced knowledge about the agents involved in the platform are responsible for managing the study (SM), by contacting and forwarding the request to the DO, and answering the researchers. This procedure is conducted with the support of TASKA, which helps during the monitoring of all tasks involved in the process. The researcher starts by formulating the study request, which can be a simple question or the analysis script if they have more advanced technical knowledge. The request is made in the Genomics Catalogue, where the repositories of interest for the study are also selected. Then, the SM analyses the request, evaluates the suitability of the question and the DO's willingness to participate in this study. During this stage, the SM analyses or builds the script using the GTO tools, and shares it with the DO using TASKA. After receiving all the DO's answers, the SM compiles the information and replies to the researcher. The researcher does not need to know who the Dos are or how to process the genomics data technically using the GTO tools. This is one of the SM's responsibilities. In addition, the SM knows the characteristics of the different repositories and who they need to contact to obtain the information. On the other hand, the DO's responsibilities are running the script provided by the SM locally and determining whether the results can be shared. This methodology gives DOs full control of the data, only sharing the results that do not violate the organisational policy. During this process, administrative issues and governance board approvals are also included in the protocol. These tasks are contemplated and managed in TASKA, where all the participants are able to monitor the study status. 4 Results The proposed methodology can be summarised in Fig. 4 , where the three different actors and their responsibilities in the study are well defined. The goal is to show how each entity can proceed in order to collaborate in the study and provide an answer to a research question. Additionally, in this collaboration, the source data is never exposed and the DOs have full control of what they want to share. The following set of steps reproduces the study feasibility using the proposed methodology. The governance and contractual aspects were ignored in this description, although this can be integrated into this workflow at any time. • Step 1: Creating study request - The methodology starts in the Genomics Catalogue, where the researcher creates a study request. - In this procedure, the researcher fills in a form describing the study objective, the datasets of interest and the research question. - These datasets can be discovered using the platform's search and compare features. - An example of a research question can be “Does a patient sequenced (FASTQ) sample contain SARS-CoV-2?” • Step 2: Feasibility and script definition - The SM is notified about the study request and analyses the feasibility based on the research question. - Assuming that the study request is feasible, the SM builds the analytic script using the tools in the GTO toolkit. - The script can also be created by the researcher, although we are presenting a scenario where this entity's technical knowledge is low. On the other hand, the SM needs to know how to work with these tools as well as how to contact the DO of each repository. - If a research study is not feasible, the workflow ends at this step. • Step 3: Defining and starting the study workflow - In TASKA, the SM creates a workflow of different tasks, in which some of these are assigned to the DO associated with the datasets of interest. - This tool manages the study flow and communications between the SM and the DO, in order to simplify the results. - The use of a workflow system in this step facilitates the tracking of studies in progress and keeps all the information in a centralised platform (i.e., message exchange between the SM and DOs, the status of each task, among others). • Step 4: Running script locally - The script is shared with the DO to be manually executed in the local computational resources. - This allows full control of what is happening with the data and assessing whether the result of the script execution can be shared to the study. - Whenever the repository is publicly available, the SM can also configure a different task in TASKA to execute this automatically. However, this is not the main focus of our methodology. • Step 5: Exporting and sending results - Locally in each DO institution, the script produces the results in different formats (i. e., SVG images showing statistical data, tabular information, small sets of reads, among others). - Before sending the results to the SM, the DOs can decide about how to proceed based on the script output. For example, the DOs can filter part of the results before sharing them with the SM. - The results are attached in TASKA, which notifies the SM about the study progress. • Step 6: Aggregating results - After all DOs complete their tasks, the SM can access the results uploaded in TASKA. - The upload results differ based on the research question. Therefore, when the SM defines the study workflow, this task must be designed according to the DOs' outputs. - The SM then compiles the collected data. In particular cases, for example, in the presence of multiple datasets, the GTO tools can again be used to perform this aggregation task. • Step 7: Results evaluation and reporting - Finally, the researcher receives a notification regarding the study results in the Genomics Catalogue. - In this catalogue, the researcher can access the results, and if required, additional information can be requested. The proposed methodology has supporting tools to manage the execution of genomics research studies over multiple and private biobanks. The possibility of isolating the data analysis locally is essential to preserve privacy rules, which allows the analysis of unexplored datasets, increasing the studies’ dimension. 5 Research application The proposed methodology shows the feasibility of performing distributed research studies over private genomics repositories. To develop this methodology, we use open-source tools in all the study steps, where the goal is to show successful outcomes applied to contemporary problems. Nevertheless, the same structure can be established using other tools. For instance, in this case, we use GTO to perform the study analysis, but this toolkit can use other tools that are not currently available to satisfy the study requirements. The same is true with the study manager. Although we used TASKA, another tool can be used for the study coordination between the SM and DOs. Therefore, using the proposed tools, we create a scenario to validate this methodology. 5.1 Study example: SARS-CoV-2 The proposed scenario to evaluate the feasibility of this methodology is based on the impact of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) in 2020. SARS-CoV-2 is a positive-sense single-strand RNA virus with a tracked origin to a food market in Wuhan, China, in December 2019 which can cause COVID-19 [33]. The standard diagnostic method is real-time RT-qPCR (Reverse Transcriptase quantitative Polymerase Chain Reaction) applied to sub-regions of SARS-CoV-2. However, when dealing with a human sample, which potentially may contain SARS-CoV-2, privacy issues need careful consideration. Some issues are related to the presence of human genetic material that may reveal the individual's characteristics. Accordingly, to address two questions such as “Does this human sample contain SARS-CoV-2? If yes, what is the closest sequenced genome?” the diagnostic must be followed in accordance with preservation of the host's medical and forensic peculiarities. The problem escalates when different institutions need to report the results without sharing the samples and compromising individual privacy while maintaining identical characteristics of the analysis [34]. The following research application shows how to solve this problem. According to the methodology presented in this paper, a pipeline for SARS-CoV-2 is built using GTO and shared between institutions. Then, each institution runs the shared pipeline on their private server with the corresponding private data. Then the results are communicated to the original source and aggregated. In this subsection, we show in detail how to address this question, namely a description of all the materials used and the associated characteristics. The pipelines used in this task are freely available at and described as follows: • gto_build_xs_viral_db.sh: Designed to build a viral and human mitogenome dataset (references only). Since this script downloads the FASTA genomes forming the database, the script can be shared with the DO or in more restricted cases, the produced database can be compressed and then sent to the DOs. • gto_create_sars_dataset.sh: Creates a viral dataset considering the following set of ids: “AY386330.1″, “JN561323.2″, “X04370.1″, “MT007544″, “AB041963.1″, “MG921180.1″, “NC_012920.1”. This dataset was used by the SM to validate the processing pipeline, locally. • gto_sars_simulation_detection.sh: Builds a small FASTA database with viral references and human mitogenome. Then, it creates a FASTQ dataset containing SARS-CoV-2 and the human mitogenome (among others) and maps the existence of human mitogenome and viruses. This script has as input the database created with the gto_build_xs_viral_db.sh. • gto_download_sars2.sh: Downloads 93 SARS-CoV-2 genome sequences from the NCBI repositories. In order to avoid multiple calls to NCBI repositories, the result of this script can be compressed and then sent to the DO. • gto_find_best_sars2.sh: It finds the most similar SARS-CoV-2 genome sequences according to the RS (Relative Similarity) in the dataset produced by gto_sars_simulation_detection.sh. The SM with the five described pipelines can evaluate the feasibility of the research question by using synthetic data, and then send the processing pipelines to the DOs. Finally, the DOs run the pipelines in their private repositories and communicate the results. 5.2 Validation The proposed scenario is fully automatic metagenomic identification of viral content using the FASTQ sample that contains a SARS-Cov-2 genome. The pipeline contains filtering, trimming and mapping operations over the reads, and then performs sensitive identification of the most representative genomes followed by a ranking of abundance. This pipeline was validated using a semi-synthetic viral dataset of SARS-CoV-2 genomes with applied degrees of substitutions and block permutations shuffled with noisy synthetic sequences. The intention is to perform a metagenomic analysis on this dataset without informing the program what organisms are contained in the sample since the program needs to infer the results. Then, we compare the results with the ground truth. If the results are similar to the ground truth, then the pipeline is validated and can be shared with the DOs. For this purpose, GTO uses falcon-meta technology [35,36] which relies on assembly-free and alignment-free [37] comparison of each reference according to the whole reads. The dataset was constructed using gto_create_sars_dataset.sh which simulates synthetic reads (uniform distribution) merged with the following genomes and associated modifications: • SARS-CoV-2: one virus without editions (GID: MT007544.1); • MT: one human mitogenome with 1% of random substitutions (GID: NC_012920.1); • B19V: one B19 parvovirus with random permutations of 500 bases (GID: AY386330.1); • HPV: one human papillomavirus with 5% of random substitutions and random permutations of 300 bases (GID: MG921180.1); • HHV3: one human Herpesvirus 3 with permuted blocks of 300 bases (GID: X04370.1); • TTV: one human Torque teno virus with 5% of editions (GID: AB041963.1); • 300,000 bases of pseudo-random DNA simulated with a uniform distribution. After merging all FASTA sequences, ART [38] was used to generate the paired-end FASTQ reads. Meanwhile, another workflow example was used to create the viral and human mitogenome database (gto_build_xs_viral_db.sh) limited to reference sequences. Then, the pipeline (gto_sars_simulation_detection.sh), which uses the previous two pipelines for simulation, performs the metagenomic detection. The results are presented in Table 1 . According to Table 1, the FASTQ sample, among multiple genomes and the human mitogenome, contains a SARS-CoV-2 genome with a similarity match of 95.4% to the reference genome. Notice that the presence of the mitogenome would be a privacy issue that would lead to identification of the individual's forensic and medical characteristics [39]. Nevertheless, since the data is processed in a private machine, this information is not exported to external entities, respecting privacy concerns. The following procedure addresses the question of identifying the closest similar SARS-CoV-2 genome according to the existing sequencing database. Accordingly, we use the gto_download_sars2.sh pipeline to automatically download 93 SARS-CoV-2 genomes. Then, using the same FASTQ dataset (created with gto_create_sars_dataset.sh) and the previous set of 93 SARS-CoV-2 genomes, we perform an analysis of the most similar SARS-CoV-2 genome according to the reads (using gto_find_best_sars2.sh). These results are reported in Table 2 . As can be seen, the highest match is MT007544.1. Despite the well-known similarity between all the SARS-CoV-2 genomes [40], we were able to identify the exact genome from the set, showing the efficiency of this approach. This research application contains a practical and efficient methodology to solve current Covid-19 diagnostic challenges, effectively solving privacy issues that prevent cooperation between many scientific and industrial entities. Although the pipelines presented refer to a specific disease, the purpose of this methodology is easy creation and execution of genomic pipelines in the Dos’ facilities. 6 Discussion The proposed system has the goal of creating a methodology that helps researchers to conduct distributed studies analysing human genomes. The current tools designed for genome analysis typically require direct access to patients’ data, i.e., and researchers need to access data stored in the biobanks. However, this access often raises privacy issues. Our methodology addresses this problem by offering a straightforward workflow that simplifies communication and the exchange of results between all the actors involved in a study. The success of this methodology is due to the combination of several computational tools and algorithms that together provide a collaborative workflow, which can be customised for each research question, while ensuring privacy issues related to the raw data [41]. Therefore, the distributed analysis of private genomics information applied to the proposed scenario can be discussed based on two perspectives: 1) the technical challenges in conducting distributed studies; and 2) methodologies to study the study example (SARS-CoV-2). The idea of performing distributed medical studies is already applied in specific areas or with specific data types. For instance, Observational Health Data Sciences and Informatics (OHDSI) 9 9 is an international organisation aiming to develop methodologies to support large-scale observational studies in health care data. The data used in these studies contains only the patients' structured information present in the Electronic Health Records (EHR) systems after being anonymised and harmonised into a relational common data model (CDM) [42]. OHDSI has an ecosystem of tools that allows preparation of the data in each data custodian as well as tools to define the study and spread it through the network of data owners [14]. The problem is they are only concerned with performing observational studies using the EHRs’ structured information. The European Medical Information Framework project (EMIF) 10 10 also had the goal of performing distributed medical studies. One of the tracks in this project aimed to accelerate the discovery and validation of new biomarkers to diagnose Alzheimer's Disease (AD) in the predementia stage, and to predict the rate of decline [43]. In this case, the distributed analysis follows a procedure similar to the OHDSI in the case of the structured information, but the analysis of biomarkers was performed in controlled environments. The proposed solution does not require the researcher to access the data. The previous methodologies are focused on providing strategies to study patients' information without exposing this information. However, both were designed to work with structured information. When the goal is to study human genomic data without violating patients’ privacy, there are different techniques. Tkachenko et al. [44] created a solution aiming for data privacy preservation for Genome-Wide Association Studies (GWAS) based on Secure Multi-Party Computation (SMPC). This cryptographic technique has the potential to enable secure data privacy, since it provides ways for entities to jointly compute a function using their inputs, without exposing these inputs [45]. This is a different way of addressing the distributed analysis of genomic information. However, the strategies that were based on SMPC require that all the entities adopt and implement a cryptographic system. Although considered secure, data owners usually feel more confident keeping the data undisclosed, which is the main goal of our methodology. The GTO toolkit used in the data analysis was chosen due to the richness of its algorithms, but the methodology is flexible enough to incorporate other tools. Another reason for this choice is that GTO also contains external tools. It is important to have a well-defined set of tools in the framework that can be used by all researchers. This practice will allow replication of the genomic pipelines built using those tools as well as confirmation of the results of each study. More importantly, data owners need to feel confident in installing and using those tools in their local environments. Thus, open-source or certified tools are the appropriate option for this step [46]. We focused on using only open-source tools and toolkits to have the full methodology available to the community. The research application was based on data from the COVID-19 pandemic. However, many other studies can be performed to address other research questions related to different diseases and/or genomes. This disease was chosen to prove that, with this methodology, it is possible to simplify the genomic analysis over multiple entities without sharing or exposing the original data. A great advance of our methodology is that it can be used during a pandemic, to obtain timely results, which are often delayed by ethical and legal issues. The only condition is the willing cooperation of researchers and data custodians in respecting data-access policies that may exist in different countries, institutions and groups. Other authors created computational approaches aiming to detect SARS-CoV-2 infections, potential drugs and therapeutic strategies regarding this disease as well as tracking and studying the evolution of the COVID-19 pandemic. Hufsky et al. [47] presented a review of tools with these goals, and each tool presented is currently free to use and available online. The European Virus Bioinformatics Center curates a list of bioinformatics tools specifically for SARS-CoV-2, 11 11 and some of them were also analysed by Hufsky et al. [47]. Moreover, we identified from that list some tools with a similar goal as the pipelines presented in the research application. PriSeT 12 12 is a tool for computing SARS-CoV-2 specific primers for RT-PCR tests, enabling the detection of SARS-CoV-2 in a sample [48]. VADR 13 13 is a tool designed for validation and annotation of SARS-CoV-2 [48]. The annotation system was built based on the analysis of input nucleotide sequences using models created from curated RefSeqs [49]. The advantage of using a toolkit such as GTO is the possibility of building pipelines capable of performing similar tasks to those the tools presented were designed for. Since the goal of this work is to provide a generic solution, it is important to choose flexible software, without invalidating the possibility of integrating others. The results of the study example allowed validation of the toolkit analysis using public datasets. Researchers can perform a similar task, creating the pipelines in the research analysis. These pipelines can be easily shared with the data owners and the outputs do not expose any patient information. In fact, if the pipelines are a risk to patients’ privacy, the data owners can simply refuse to share the results. Therefore, with this methodology, it is possible to conduct studies without disclosing the data present in each location. 7 Conclusions Many private biobanks lack direct and cross-exploration due to privacy issues. Although many analysis pipelines exist, the critical issue of the multiple dimensionalities and sensitivity of the data remains an obstacle. In this paper, we proposed a new methodology that allows the exploration of private genomic and proteomic repositories without exposing their content. The privacy issues associated with sensitive data are solved with this methodology since the DOs have full control of the data, refusing unauthenticated access to the repositories. This methodology allows the exploration of multiple repositories, aggregating the results, and increasing the strength of scientific findings by analyzing higher volumes of data. The proposed methodology is a step forward in the cross-analysis of biobanks because it allows sharing architectural pipelines while preserving data privacy, through a process that is intuitive, simple and practical. References [1] J. Johnston J.D. Lantos A. Goldenberg F. Chen E. Parens B.A. Koenig N. Ethics P.A. Board Sequencing newborns: a call for nuanced use of genomic technologies Hastings Cent. Rep. 48 2018 S2 S6 J. Johnston, J. D. Lantos, A. Goldenberg, F. Chen, E. Parens, B. A. Koenig, N. Ethics, P. A. Board, Sequencing newborns: a call for nuanced use of genomic technologies, Hastings Center Report 48 (2018) S2-S6. [2] J. Kaye The tension between data sharing and the protection of privacy in genomics research Annu. Rev. Genom. Hum. Genet. 13 2012 415 431 10.1146/annurev-genom-082410-101454 J. Kaye, The tension between data sharing and the protection of privacy in genomics research, Annual review of genomics and human genetics 13 (2012) 415-431. doi:10.1146/annurev-genom-082410-101454. [3] V. Jalili M. Matteucci M. Masseroli S. Ceri Indexing next-generation sequencing data Inf. Sci. 384 2017 90 109 10.1016/j.ins.2016.08.085 V. Jalili, M. Matteucci, M. Masseroli, S. Ceri, Indexing next-generation sequencing data, Information Sciences 384 (2017) 90-109. doi:10.1016/j.ins.2016.08.085. [4] L. Coppola A. Cianflone A.M. Grimaldi M. Incoronato P. Bevilacqua F. Messina S. Baselice A. Soricelli P. Mirabelli M. Salvatore Biobanking in health care: evolution and future directions J. Transl. Med. 17 1 2019 172 10.1186/s12967-019-1922-3 L. Coppola, A. Cianflone, A. M. Grimaldi, M. Incoronato, P. Bevilacqua, F. Messina, S. Baselice, A. Soricelli, P. Mirabelli, M. Salvatore, Biobanking in health care: evolution and future directions, Journal of translational medicine 17 (1) (2019) 172. doi:10.1186/s12967-019-1922-3. [5] J.-E. Litton Launch of an infrastructure for health research: BBMRI-ERIC Biopreserv. Biobanking 16 3 2018 233 241 10.1089/bio.2018.0027 J.-E. Litton, Launch of an infrastructure for health research: BBMRI-ERIC, Biopreservation and biobanking 16 (3) (2018) 233-241. doi:10.1089/bio.2018.0027. [6] A. Liu K. Pollard Biobanking for personalized medicine Biobanking in the 21st Century 2015 Springer 55 68 10.1007/978-3-319-20579-3_5 A. Liu, K. Pollard, Biobanking for personalized medicine, in: Biobanking in the 21st Century, Springer, 2015, pp. 55-68. doi:10.1007/978-3-319-20579-3_5. [7] A. Amorim F. Pereira C. Alves O. García Species assignment in forensics and the challenge of hybrids Forensic Sci. Int.: Genetics 48 2020 102333 10.1016/j.fsigen.2020.102333 A. Amorim, F. Pereira, C. Alves, O. Garcia, Species assignment in forensics and the challenge of hybrids, Forensic Science International: Genetics 48 (2020) 102333. doi:10.1016/j.fsigen.2020.102333. [8] H. Langhof H. Kahrass S. Sievers D. Strech Access policies in biobank research: what criteria do they include and how publicly available are they? A cross-sectional study Eur. J. Hum. Genet. 25 3 2017 293 300 10.1038/ejhg.2016.172 H. Langhof, H. Kahrass, S. Sievers, D. Strech, Access policies in biobank research: what criteria do they include and how publicly available are they? A cross-sectional study, European Journal of Human Genetics 25 (3) (2017) 293-300. doi:10.1038/ejhg.2016.172. [9] J. Kulynych H.T. Greely Clinical genomics, big data, and electronic medical records: reconciling patient rights with research when privacy and science collide J. Law Biosci. 4 1 2017 94 132 10.1093/jlb/lsw061 J. Kulynych, H. T. Greely, Clinical genomics, big data, and electronic medical records: reconciling patient rights with research when privacy and science collide, Journal of Law and the Biosciences 4 (1) (2017) 94-132. doi:10.1093/jlb/lsw061. [10] P. Claes D.K. Liberton K. Daniels K.M. Rosana E.E. Quillen L.N. Pearson B. McEvoy M. Bauchet A.A. Zaidi W. Yao Modeling 3D facial shape from DNA PLoS Genet. 10 3 2014 10.1371/journal.pgen.1004224 P. Claes, D. K. Liberton, K. Daniels, K. M. Rosana, E. E. Quillen, L. N. Pearson, B. McEvoy, M. Bauchet, A. A. Zaidi, W. Yao, et al., Modeling 3D facial shape from DNA, PLoS genetics 10 (3) (2014). doi:10.1371/journal.pgen.1004224. [11] P.J. McLaren J.L. Raisaro M. Aouri M. Rotger E. Ayday I. Bartha M.B. Delgado Y. Vallet H.F. Günthard M. Cavassini Privacy-preserving genomic testing in the clinic: a model using HIV treatment Genet. Med. 18 8 2016 814 822 10.1038/gim.2015.167 P. J. McLaren, J. L. Raisaro, M. Aouri, M. Rotger, E. Ayday, I. Bartha, M. B. Delgado, Y. Vallet, H. F. Gunthard, M. Cavassini, et al., Privacy-preserving genomic testing in the clinic: a model using HIV treatment, Genetics in medicine 18 (8) (2016) 814-822. doi:10.1038/gim.2015.167. [12] D. Grishin K. Obbad G.M. Church Data privacy in the age of personal genomics Nat. Biotechnol. 37 10 2019 1115 1117 10.1038/s41587-019-0271-3 D. Grishin, K. Obbad, G. M. Church, Data privacy in the age of personal genomics, Nature biotechnology 37 (10) (2019) 1115-1117. doi:10.1038/s41587-019-0271-3. [13] M. Masseroli A. Kaitoua P. Pinoli S. Ceri Modeling and interoperability of heterogeneous genomic big data for integrative processing and querying Methods 111 2016 3 11 10.1016/j.ymeth.2016.09.002 M. Masseroli, A. Kaitoua, P. Pinoli, S. Ceri, Modeling and interoperability of heterogeneous genomic big data for integrative processing and querying, Methods 111 (2016) 3-11. doi:10.1016/j.ymeth.2016.09.002. [14] J.R. Almeida O. Fajarda A. Pereira J.L. Oliveira Strategies to access patient clinical data from distributed databases Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 5 HEALTHINF: HEALTHINF 2019 INSTICC, SciTePress 466 473 10.5220/0007576104660473 J. R. Almeida, O. Fajarda, A. Pereira, J. L. Oliveira, Strategies to Access Patient Clinical Data from Distributed Databases, in: Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 5 HEALTHINF: HEALTHINF, INSTICC, SciTePress, 2019, pp. 466-473. doi:10.5220/0007576104660473. [15] J.M. Gaziano J. Concato M. Brophy L. Fiore S. Pyarajan J. Breeling S. Whitbourne J. Deen C. Shannon D. Humphries Million Veteran Program: a mega-biobank to study genetic influences on health and disease J. Clin. Epidemiol. 70 2016 214 223 10.1016/j.jclinepi.2015.09.016 J. M. Gaziano, J. Concato, M. Brophy, L. Fiore, S. Pyarajan, J. Breeling, S. Whitbourne, J. Deen, C. Shannon, D. Humphries, et al., Million Veteran Program: a mega-biobank to study genetic influences on health and disease, Journal of clinical epidemiology 70 (2016) 214-223. doi:10.1016/j.jclinepi.2015.09.016. [16] C. Sudlow J. Gallacher N. Allen V. Beral P. Burton J. Danesh P. Downey P. Elliott J. Green M. Landray UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age PLoS Med. 12 3 2015 10.1371/journal.pmed.1001779 C. Sudlow, J. Gallacher, N. Allen, V. Beral, P. Burton, J. Danesh, P. Downey, P. Elliott, J. Green, M. Landray, et al., UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age, PLoS medicine 12 (3) (2015). doi:10.1371/journal.pmed.1001779. [17] M. Hosseini D. Pratas A.J. Pinho Cryfa: a secure encryption tool for genomic data Bioinformatics 35 1 2019 146 148 10.1093/bioinformatics/bty645 M. Hosseini, D. Pratas, A. J. Pinho, Cryfa: a secure encryption tool for genomic data, Bioinformatics 35 (1) (2019) 146-148. doi:10.1093/bioinformatics/bty645. [18] M. Toppinen D. Pratas E. Väisänen M. Söderlund-Venermo K. Hedman M.F. Perdomo A. Sajantila The landscape of persistent human DNA viruses in femoral bone Forensic Sci. Int.: Genetics 48 2020 102353 10.1016/j.fsigen.2020.102353 M. Toppinen, D. Pratas, E. Vaisanen, M. Soderlund-Venermo, K. Hedman, M. F. Perdomo, A. Sajantila, The landscape of persistent human DNA viruses in femoral bone, Forensic Science International: Genetics 48 (2020) 102353. doi:10.1016/j.fsigen.2020.102353. [19] H. Cho D.J. Wu B. Berger Secure genome-wide association analysis using multiparty computation Nat. Biotechnol. 36 6 2018 547 551 10.1038/nbt.4108 H. Cho, D. J. Wu, B. Berger, Secure genome-wide association analysis using multiparty computation, Nature biotechnology 36 (6) (2018) 547-551. doi:10.1038/nbt.4108. [20] J.P. Ioannidis T.A. Trikalinos M.J. Khoury Implications of small effect sizes of individual genetic variants on the design and interpretation of genetic association studies of complex diseases Am. J. Epidemiol. 164 7 2006 609 614 10.1093/aje/kwj259 J. P. Ioannidis, T. A. Trikalinos, M. J. Khoury, Implications of small effect sizes of individual genetic variants on the design and interpretation of genetic association studies of complex diseases, American journal of epidemiology 164 (7) (2006) 609-614. doi:10.1093/aje/kwj259. [21] S.S. Shringarpure C.D. Bustamante Privacy risks from genomic data-sharing beacons Am. J. Hum. Genet. 97 5 2015 631 646 10.1016/j.ajhg.2015.09.010 S. S. Shringarpure, C. D. Bustamante, Privacy risks from genomic data-sharing beacons, The American Journal of Human Genetics 97 (5) (2015) 631-646. doi:10.1016/j.ajhg.2015.09.010. [22] ENCODE Project Consortium and others An integrated encyclopedia of DNA elements in the human genome Nature 489 2012 57 74 10.1038/nature11247 (7414) ENCODE Project Consortium and others, An integrated encyclopedia of DNA elements in the human genome, Nature 489 (7414) (2012) 57-74. doi:10.1038/nature11247. [23] 1000 Genomes Project Consortium and others, A global reference for human genetic variation Nature 526 2015 68 74 10.1038/nature15393 (7571) 1000 Genomes Project Consortium and others, A global reference for human genetic variation, Nature 526 (7571) (2015) 68-74. doi:10.1038/nature15393. [24] L.B. Silva A. Trifan J.L. Oliveira Montra: an agile architecture for data publishing and discovery Comput. Methods Progr. Biomed. 160 2018 33 42 10.1016/j.cmpb.2018.03.024 L. B. Silva, A. Trifan, J. L. Oliveira, Montra: An agile architecture for data publishing and discovery, Computer methods and programs in biomedicine 160 (2018) 33-42. doi:10.1016/j.cmpb.2018.03.024. [25] J.L. Oliveira A. Trifan L.A.B. Silva EMIF Catalogue: a collaborative platform for sharing and reusing biomedical data Int. J. Med. Inf. 2019 10.1016/j.ijmedinf.2019.02.006 J. L. Oliveira, A. Trifan, L. A. B. Silva, EMIF Catalogue: A collaborative platform for sharing and reusing biomedical data, International Journal of Medical Informatics (2019). doi:10.1016/j.ijmedinf.2019.02.006. [26] J.R. Almeida A.J. Pinho J.L. Oliveira O. Fajarda D. Pratas GTO: a toolkit to unify pipelines in genomic and proteomic research SoftwareX 12 2020 100535 10.1016/j.softx.2020.100535 J. R. Almeida, A. J. Pinho, J. L. Oliveira, O. Fajarda, D. Pratas, GTO: A toolkit to unify pipelines in genomic and proteomic research, SoftwareX 12 (2020) 100535. doi:10.1016/j.softx.2020.100535. [27] D. Pratas M. Toppinen L. Pyöriä K. Hedman A. Sajantila M.F. Perdomo A hybrid pipeline for reconstruction and analysis of viral genomes at multi-organ level GigaScience 9 8 2020 10.1093/gigascience/giaa086 giaa086 D. Pratas, M. Toppinen, L. Pyoria, K. Hedman, A. Sajantila, M. F. Perdomo, A hybrid pipeline for reconstruction and analysis of viral genomes at multi-organ level, GigaScience 9 (8) (2020) giaa086. doi:10.1093/gigascience/giaa086. [28] C.S. Liew M.P. Atkinson M. Galea T.F. Ang P. Martin J.I.V. Hemert Scientific workflows: moving across paradigms ACM Comput. Surv. 49 4 2017 66 10.1145/3012429 C. S. Liew, M. P. Atkinson, M. Galea, T. F. Ang, P. Martin, J. I. V. Hemert, Scientific workflows: moving across paradigms, ACM Computing Surveys (CSUR) 49 (4) (2017) 66. doi:10.1145/3012429. [29] S. Holl O. Zimmermann M. Palmblad Y. Mohammed M. Hofmann-Apitius A new optimization phase for scientific workflow management systems Future Generat. Comput. Syst. 36 2014 352 362 10.1016/j.future.2013.09.005 S. Holl, O. Zimmermann, M. Palmblad, Y. Mohammed, M. Hofmann-Apitius, A new optimization phase for scientific workflow management systems, Future generation computer systems 36 (2014) 352-362. doi:10.1016/j.future.2013.09.005. [30] K. Wolstencroft R. Haines D. Fellows A. Williams D. Withers S. Owen S. Soiland-Reyes I. Dunlop A. Nenadic P. Fisher The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud Nucleic Acids Res. 41 W1 2013 W557 W561 10.1093/nar/gkt328 K. Wolstencroft, R. Haines, D. Fellows, A. Williams, D. Withers, S. Owen, S. Soiland-Reyes, I. Dunlop, A. Nenadic, P. Fisher, et al., The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud, Nucleic acids research 41 (W1) (2013) W557-W561. doi:10.1093/nar/gkt328. [31] J. Goecks A. Nekrutenko J. Taylor Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences Genome Biol. 11 8 2010 R86 10.1186/gb-2010-11-8-r86 J. Goecks, A. Nekrutenko, J. Taylor, Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences, Genome biology 11 (8) (2010) R86. doi:10.1186/gb-2010-11-8-r86. [32] J.R. Almeida R. Gini G. Roberto P. Rijnbeek J.L. Oliveira TASKA: a modular task management system to support health research studies BMC Med. Inf. Decis. Making 19 1 2019 121 10.1186/s12911-019-0844-6 J. R. Almeida, R. Gini, G. Roberto, P. Rijnbeek, J. L. Oliveira, TASKA: A modular task management system to support health research studies, BMC medical informatics and decision making 19 (1) (2019) 121. doi:10.1186/s12911-019-0844-6. [33] F. Wu S. Zhao B. Yu Y.-M. Chen W. Wang Z.-G. Song Y. Hu Z.-W. Tao J.-H. Tian Y.-Y. Pei A new coronavirus associated with human respiratory disease in China Nature 579 2020 265 269 10.1038/s41586-020-2008-3 (7798) F. Wu, S. Zhao, B. Yu, Y.-M. Chen, W. Wang, Z.-G. Song, Y. Hu, Z.-W. Tao, J.-H. Tian, Y.-Y. Pei, et al., A new coronavirus associated with human respiratory disease in China, Nature 579 (7798) (2020) 265-269. doi:10.1038/s41586-020-2008-3. [34] C. Heeney N. Hawkins J. de Vries P. Boddington J. Kaye Assessing the privacy risks of data sharing in genomics Public Health Genom. 14 1 2011 17 25 10.1159/000294150 C. Heeney, N. Hawkins, J. de Vries, P. Boddington, J. Kaye, Assessing the privacy risks of data sharing in genomics, Public health genomics 14 (1) (2011) 17-25. doi:10.1159/000294150. [35] D. Pratas A.J. Pinho Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight 2018 26th European Signal Processing Conference (EUSIPCO) 2018 IEEE 1177 1181 10.23919/EUSIPCO.2018.8553297 D. Pratas, A. J. Pinho, Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight, in: 2018 26th European Signal Processing Conference (EUSIPCO), IEEE, 2018, pp. 1177-1181. doi:10.23919/EUSIPCO.2018.8553297. [36] D. Pratas M. Hosseini G. Grilo A.J. Pinho R.M. Silva T. Caetano J. Carneiro F. Pereira Metagenomic composition analysis of an ancient sequenced polar bear jawbone from svalbard Genes 9 9 2018 445 10.3390/genes9090445 D. Pratas, M. Hosseini, G. Grilo, A. J. Pinho, R. M. Silva, T. Caetano, J. Carneiro, F. Pereira, Metagenomic Composition Analysis of an Ancient Sequenced Polar Bear Jawbone from Svalbard, Genes 9 (9) (2018) 445. doi:10.3390/genes9090445. [37] A. Zielezinski H.Z. Girgis G. Bernard C.-A. Leimeister K. Tang T. Dencker A.K. Lau S. Röhling J.J. Choi M.S. Waterman Benchmarking of alignment-free sequence comparison methods Genome Biol. 20 1 2019 144 10.1186/s13059-019-1755-7 A. Zielezinski, H. Z. Girgis, G. Bernard, C.-A. Leimeister, K. Tang, T. Dencker, A. K. Lau, S. Rohling, J. J. Choi, M. S. Waterman, et al., Benchmarking of alignment-free sequence comparison methods, Genome biology 20 (1) (2019) 144. doi:10.1186/s13059-019-1755-7. [38] W. Huang L. Li J.R. Myers G.T. Marth ART: a next-generation sequencing read simulator Bioinformatics 28 4 2011 593 594 10.1093/bioinformatics/btr708 W. Huang, L. Li, J. R. Myers, G. T. Marth, ART: a next-generation sequencing read simulator, Bioinformatics 28 (4) (2011) 593-594. doi:10.1093/bioinformatics/btr708. [39] Z. Lin A.B. Owen R.B. Altman Genomic research and human subject privacy Science 305 2004 10.1126/science.1095019 (5681) 183–183 Z. Lin, A. B. Owen, R. B. Altman, Genomic Research and Human Subject Privacy, Science 305 (5681) (2004) 183-183. doi:10.1126/science.1095019. [40] D. Pratas J.M. Silva Persistent minimal sequences of SARS-CoV-2 Bioinformatics 07 2020 10.1093/bioinformatics/btaa686 D. Pratas, J. M. Silva, Persistent minimal sequences of SARS-CoV-2, Bioinformatics (07 2020). doi:10.1093/bioinformatics/btaa686. [41] E. Vayena U. Gasser Between openness and privacy in genomics PLoS Med. 13 1 2016 e1001937 10.1371/journal.pmed.1001937 E. Vayena, U. Gasser, Between openness and privacy in genomics, PLoS medicine 13 (1) (2016) e1001937. doi:10.1371/journal.pmed.1001937. [42] G. Hripcsak J.D. Duke N.H. Shah C.G. Reich V. Huser M.J. Schuemie M.A. Suchard R.W. Park I.C.K. Wong P.R. Rijnbeek Observational health data sciences and Informatics (OHDSI): opportunities for observational researchers Stud. Health Technol. Inf. 216 2015 574 G. Hripcsak, J. D. Duke, N. H. Shah, C. G. Reich, V. Huser, M. J. Schuemie, M. A. Suchard, R. W. Park, I. C. K. Wong, P. R. Rijnbeek, et al., Observational Health Data Sciences and Informatics (OHDSI): opportunities for observational researchers, Studies in health technology and informatics 216 (2015) 574. [43] I. Bos S. Vos R. Vandenberghe P. Scheltens S. Engelborghs G. Frisoni J.L. Molinuevo A. Wallin A. Lleó J. Popp The EMIF-AD Multimodal Biomarker Discovery study: design, methods and cohort characteristics Alzheimer's Res. Ther. 10 1 2018 64 10.1186/s13195-018-0396-5 I. Bos, S. Vos, R. Vandenberghe, P. Scheltens, S. Engelborghs, G. Frisoni, J. L. Molinuevo, A. Wallin, A. Lleo, J. Popp, et al., The EMIF-AD Multimodal Biomarker Discovery study: design, methods and cohort characteristics, Alzheimer’s research & therapy 10 (1) (2018) 64. doi:10.1186/s13195-018-0396-5. [44] O. Tkachenko C. Weinert T. Schneider K. Hamacher Large-scale privacy-preserving statistical computations for distributed genome-wide association studies Proceedings of the 2018 on Asia Conference on Computer and Communications Security 2018 ASIACCS ’18, Association for Computing Machinery New York, NY, USA 221 235 10.1145/3196494.3196541 O. Tkachenko, C. Weinert, T. Schneider, K. Hamacher, Large-Scale Privacy-Preserving Statistical Computations for Distributed Genome-Wide Association Studies, in: Proceedings of the 2018 on Asia Conference on Computer and Communications Security, ASIACCS ’18, Association for Computing Machinery, New York, NY, USA, 2018, p. 221-235. doi:10.1145/3196494.3196541. [45] O. Goldreich Secure multi-party computation, Manuscript Preliminary Version 78 1998 O. Goldreich, Secure multi-party computation, Manuscript. Preliminary version 78 (1998). [46] L. Kannan M. Ramos A. Re N. El-Hachem Z. Safikhani D.M. Gendoo S. Davis D. Gomez-Cabrero R. Castelo K.D. Hansen Public data and open source tools for multi-assay genomic investigation of disease Briefings Bioinf. 17 4 2016 603 615 10.1093/bib/bbv080 L. Kannan, M. Ramos, A. Re, N. El-Hachem, Z. Safikhani, D. M. Gendoo, S. Davis, D. Gomez-Cabrero, R. Castelo, K. D. Hansen, et al., Public data and open source tools for multi-assay genomic investigation of disease, Briefings in bioinformatics 17 (4) (2016) 603-615. doi:10.1093/bib/bbv080. [47] F. Hufsky K. Lamkiewicz A. Almeida A. Aouacheria C. Arighi A. Bateman J. Baumbach N. Beerenwinkel C. Brandt M. Cacciabue Computational strategies to combat COVID-19: useful tools to accelerate SARS-CoV-2 and coronavirus research Briefings Bioinf. 11 2020 10.1093/bib/bbaa232 F. Hufsky, K. Lamkiewicz, A. Almeida, A. Aouacheria, C. Arighi, A. Bateman, J. Baumbach, N. Beerenwinkel, C. Brandt, M. Cacciabue, et al., Computational strategies to combat COVID-19: useful tools to accelerate SARS-CoV-2 and coronavirus research, Briefings in Bioinformatics (11 2020). doi:10.1093/bib/bbaa232. [48] M. Hoffmann M.T. Monaghan K. Reinert PriSeT: efficient de novo primer discovery BioRxiv 2020 10.1101/2020.04.06.027961 M. Hoffmann, M. T. Monaghan, K. Reinert, PriSeT: Efficient De Novo Primer Discovery, bioRxiv (2020). doi:10.1101/2020.04.06.027961. [49] A.A. Schäffer E.L. Hatcher L. Yankie L. Shonkwiler J.R. Brister I. Karsch-Mizrachi E.P. Nawrocki VADR: validation and annotation of virus sequence submissions to GenBank BMC Bioinf. 21 2020 1 23 10.1186/s12859-020-3537-3 A. A. Schaffer, E. L. Hatcher, L. Yankie, L. Shonkwiler, J. R. Brister, I. Karsch-Mizrachi, E. P. Nawrocki, VADR: validation and annotation of virus sequence submissions to GenBank, BMC Bioinformatics 21 (2020) 1-23. doi:10.1186/s12859-020-3537-3. "
    },
    {
        "doc_title": "A methodology for cohort harmonisation in multicentre clinical research",
        "doc_scopus_id": "85119961181",
        "doc_doi": "10.1016/j.imu.2021.100760",
        "doc_eid": "2-s2.0-85119961181",
        "doc_date": "2021-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2021 The AuthorsMany clinical trials and scientific studies have been conducted aiming for better understanding of specific medical conditions. However, these studies are often based on a small number of participants due to the difficulty in finding people with similar medical characteristics and available to participate in the studies. This is particularly critical in rare diseases, where the reduced number of subjects hinders reliable findings. To generate more substantial clinical evidence by increasing the power of the analyses, researchers have started to perform data harmonisation and multiple cohort analyses. However, the analysis of heterogeneous data sources implies dealing with different data structures, terminologies, concepts, languages and, most importantly, the knowledge behind the data. In this paper, we present a methodology to harmonise different cohorts into a standard data schema, helping the research community to generate evidence from a wider variety of data sources. Our methodology was inspired by the OHDSI Common Data Model, which aims to harmonise EHR datasets for observational studies, leveraging on knowledge and open source tools to perform multicentric disease-specific studies. This proposal was validated using Alzheimer's Disease cohorts from several countries, combining at the end 6,669 subjects and 172 clinical concepts. The harmonised datasets now enable multi-cohort querying and analysis, helping in the execution of new research. The methodology was implemented in Python language and is available, under the MIT licence, at https://bioinformatics-ua.github.io/CMToolkit/.",
        "available": true,
        "clean_text": "serial JL 312075 291210 291773 291871 291901 291910 31 90 Informatics in Medicine Unlocked INFORMATICSINMEDICINEUNLOCKED 2021-10-19 2021-10-19 2021-10-27 2021-10-27 2022-01-18T19:55:48 S2352-9148(21)00234-3 S2352914821002343 10.1016/j.imu.2021.100760 S300 S300.1 FULL-TEXT 2022-01-18T21:06:40.621223Z 0 0 20210101 20211231 2021 2021-10-19T04:51:18.055141Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst ref 2352-9148 23529148 UNLIMITED FCT true 27 27 C Volume 27 1 100760 100760 100760 2021 2021 2021-01-01 2021-12-31 2021 article fla © 2021 The Authors. Published by Elsevier Ltd. AMETHODOLOGYFORCOHORTHARMONISATIONINMULTICENTRECLINICALRESEARCH ALMEIDA J 1 Introduction 2 Related work 3 Proposal 3.1 Overview 3.2 The OMOP common data model schema 3.3 The ETL OHDSI tools 3.4 Collaborative ontology development 4 Workflow implementation: the CMToolkit 4.1 Data harmonisation 4.2 Customised operations 4.3 Data loading onto OMOP CDM 5 Results 5.1 Ontology 5.2 Cohort harmonisation 5.3 Data analysis environment 5.4 Study example: the prevalence of amyloid pathology 6 Discussion 6.1 Data quality and analysis 6.2 Interoperability and privacy 6.3 Limitations 7 Conclusion References RANGANATHAN 2018 184 P SONG 2010 2234 J LU 2009 691 697 C RANGANATHAN 2019 91 P CARLSON 2009 77 82 M HARRIS 2009 377 381 P BROWN 2013 144 156 C HRIPCSAK 2015 574 G CUSHMAN 2010 S51 S55 R FOX 2020 1015 1029 G MEYSTRE 2017 38 52 S HRIPCSAK 2016 7329 7336 G MURPHY 2010 124 130 S MCMURRY 2013 e55811 A DANIEL 2016 51 C WEEKS 2019 J OVERHAGE 2011 54 60 J MAKADIA 2014 R ALMEIDA 2019 466 473 J HEALTHINF STRATEGIESACCESSPATIENTCLINICALDATADISTRIBUTEDDATABASES BOS 2018 64 I HRIPCSAK 2019 G BOOKOHDSIOBSERVATIONALHEALTHDATASCIENCESINFORMATICSOHDSI DEBNATH 2015 15 24 R PROCEEDINGSACMEIGHTEENTHINTERNATIONALWORKSHOPDATAWAREHOUSINGOLAP TOWARDSAPROGRAMMABLESEMANTICEXTRACTTRANSFORMLOADFRAMEWORKFORSEMANTICDATAWAREHOUSES JAMES 2004 276 280 P TUDORACHE 2013 89 99 T WEBPROTEGEACOLLABORATIVEONTOLOGYEDITORKNOWLEDGEACQUISITIONTOOLFORWEB ALMEIDA 2020 261 264 J 2020IEEE33RDINTERNATIONALSYMPOSIUMCOMPUTERBASEDMEDICALSYSTEMS MULTILANGUAGECONCEPTNORMALISATIONCLINICALCOHORTS VOS 2015 1327 1338 S JANSEN 2015 1924 1938 W 2021 ATHENAOHDSIVOCABULARIESREPOSITORY ALMEIDA 2021 104180 J HONG 2020 1 12 S DELVENNE 2020 e047247 A ALMEIDAX2021X100760 ALMEIDAX2021X100760XJ Full 2021-10-11T20:01:03Z FundingBody Portugal Institutes This is an open access article under the CC BY-NC-ND license. © 2021 The Authors. Published by Elsevier Ltd. 2022-01-18T00:29:52.212Z FCT SFRH/BD/147837/2019 FCT Fundação para a Ciência e a Tecnologia National Science Foundation NSF National Science Foundation J.R. Almeida is funded by the National Science Foundation (FCT) , under the grant SFRH/BD/147837/2019 . 0 item S2352-9148(21)00234-3 S2352914821002343 10.1016/j.imu.2021.100760 312075 2022-01-18T21:06:40.621223Z 2021-01-01 2021-12-31 UNLIMITED FCT true 1262189 MAIN 9 58379 849 656 IMAGE-WEB-PDF 1 gr2 137806 280 659 gr1 107782 232 377 gr4 91733 300 364 gr3 93886 166 512 gr6 101794 244 357 gr5 91893 225 565 gr2 76470 93 219 gr1 78104 134 219 gr4 72826 164 199 gr3 71041 71 219 gr6 78983 150 219 gr5 68808 87 219 gr2 565163 1238 2917 gr1 316187 1025 1669 gr4 213683 1331 1613 gr3 236803 735 2266 gr6 281223 1080 1582 gr5 257361 998 2501 IMU 100760 100760 S2352-9148(21)00234-3 10.1016/j.imu.2021.100760 The Authors Fig. 1 Tables used from OMOP CDM schema in the proposed methodology. The complete data schema is available at [21]. Fig. 2 The migration workflow from raw data to the OMOP CDM structure, using the proposed methodology combined with the ETL OHDSI tools. This workflow is divided into three main stages, having two processes running in parallel (marked in a red dashed line). The first stage extracts cohort information and loads it into the system. The transformation stage performs all the defined operations over the raw data using the mappings mixed with the ontology rules. Finally, the loading stage inserts the data in the database, producing a migration report which indicates all the problems with the original raw data. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 3 Example of cohort raw data (first table) and its structure in the format processed during all the workflow. The blue box represents the key of the key–value structure, and the green box represents the fields that would receive the harmonised concept codes. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 4 Fragment of the ETL workflow focused on the ad hoc modules (represented in blue). Green represents the Cohort Harmoniser, responsible for orchestration of the transforming operations. The datasets in yellow represent the cohort raw data after being transformed to a processing structure (on the left) and the cohort in the same format with the medical concepts mapped to their standard definition (on the right). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 5 Node on the ontology to define a standard concept. The rdfs:label identifies the node in the ontology and for this example, the desired input values from the cohort raw data are positive numbers. The standard code is represented in the rdfs:conceptCode, which belongs to the SNOMED vocabulary with the identifier 45768723 [28]. Fig. 6 Overview of the workflow used to perform a study, showing that the data can be preserved in a private environment or available to be directly queried. Analytical tools and details were omitted in this figure. Table 1 Summary of attributes in both cohorts. The first column contains the sum of all variables in the raw data. The columns discarded and mapped are the number of variables used from the original data, and the composed column is the number of attributes generated from the ontology rules. The final column has the number of attributes forming the migrated cohorts. Variables Discarded Mapped Composed Final Berlin BASE-II 85 26 59 8 67 Maastricht Study 313 200 113 20 133 ☆ J.R. Almeida is funded by the National Science Foundation (FCT) , under the grant SFRH/BD/147837/2019. A methodology for cohort harmonisation in multicentre clinical research João Rafael Almeida a b ⁎ Luís Bastão Silva a c Isabelle Bos d e Pieter Jelle Visser d e José Luís Oliveira a ⁎⁎ a DETI/IEETA, University of Aveiro, Aveiro, Portugal DETI/IEETA, University of Aveiro Aveiro Portugal DETI/IEETA, University of Aveiro, Aveiro, Portugal b Department of Computation, University of A Coruña, A Coruña, Spain Department of Computation, University of A Coruña A Coruña Spain Department of Computation, University of A Corua, A Corua, Spain c BMD Software, Aveiro, Portugal BMD Software Aveiro Portugal BMD Software, Aveiro, Portugal d Alzheimer Centre, Department of Neurology, VU University Medical Centre, Amsterdam, The Netherlands Alzheimer Centre, Department of Neurology, VU University Medical Centre Amsterdam The Netherlands Alzheimer Centre, Department of Neurology, VU University Medical Centre, Amsterdam, The Netherlands e Department of Psychiatry and Neuropsychology, School for Mental Health and Neuroscience (MHeNS), Maastricht University, Maastricht, The Netherlands Department of Psychiatry and Neuropsychology, School for Mental Health and Neuroscience (MHeNS), Maastricht University Maastricht The Netherlands Department of Psychiatry and Neuropsychology, School for Mental Health and Neuroscience (MHeNS), Maastricht University, Maastricht, The Netherlands ⁎ Corresponding author at: DETI/IEETA, University of Aveiro, Aveiro, Portugal. DETI/IEETA, University of Aveiro Aveiro Portugal ⁎⁎ Corresponding author at: DETI/IEETA, University of Aveiro, Aveiro, Portugal. DETI/IEETA, University of Aveiro Aveiro Portugal Many clinical trials and scientific studies have been conducted aiming for better understanding of specific medical conditions. However, these studies are often based on a small number of participants due to the difficulty in finding people with similar medical characteristics and available to participate in the studies. This is particularly critical in rare diseases, where the reduced number of subjects hinders reliable findings. To generate more substantial clinical evidence by increasing the power of the analyses, researchers have started to perform data harmonisation and multiple cohort analyses. However, the analysis of heterogeneous data sources implies dealing with different data structures, terminologies, concepts, languages and, most importantly, the knowledge behind the data. In this paper, we present a methodology to harmonise different cohorts into a standard data schema, helping the research community to generate evidence from a wider variety of data sources. Our methodology was inspired by the OHDSI Common Data Model, which aims to harmonise EHR datasets for observational studies, leveraging on knowledge and open source tools to perform multicentric disease-specific studies. This proposal was validated using Alzheimer’s Disease cohorts from several countries, combining at the end 6,669 subjects and 172 clinical concepts. The harmonised datasets now enable multi-cohort querying and analysis, helping in the execution of new research. The methodology was implemented in Python language and is available, under the MIT licence, at Keywords Clinical studies Observational studies Data harmonisation ETL OMOP CDM 1 Introduction The continuous demand for better health diagnostics and treatment has motivated many clinical research studies, such as observational studies and clinical trials. In clinical trials, patients are commonly divided into two or more groups (e.g. active and placebo), to study the effectiveness of the treatment for a particular clinical condition [1]. In this case, there is direct intervention with the patients, e.g., administration of a drug or therapeutic procedures. However, this approach is not always the most appropriate, e.g. addressing research questions in plastic surgery through randomised controlled trials is often subject to ethical constraints [2]. In addition, in observational studies, researchers do not perform any active intervention with patients, and exposure occurs naturally or through other factors. Here, medical researchers limit themselves to documenting the relationship between the exposure and the outcome in the study [1]. Observational studies can be split in three categories: cohort studies, case-control studies, and cross-sectional studies [3]. A cohort is defined as a group of subjects that share similar characteristics [4]. Cohort studies are based on a set of inclusion and exclusion criteria for the subjects, as well as several features that are identified and observed over time [5]. The data collected during these studies do not follow any standard methodology regarding data storage. Therefore, these studies are usually conducted using a dedicated solution for data gathering, which in some cases can access some of the patient information stored in the institutional Electronic Health Record (EHR) system [6]. While the study can be successfully conducted regardless of the technology adopted, many problems will appear when these data need to be combined with other related cohorts. This is the case of multi-cohort studies that aim to increase the population size, the power of the statistical evidence, and thereby the study’s impact [7]. Integrating multiple cohorts is not just a technological problem. The data content raises other issues because the same medical procedure may have different descriptions and concepts, depending on the country, the institution and even the study. Although there are guidelines to support the design of clinical studies, they do not consider the technological aspects. Therefore, the lack of harmonisation in structure and clinical concepts is one of the main handicaps in health data sharing, a problem that delays or even prevents multi-cohort analysis. The potential impact of these studies has also motivated researchers to seek more robust and reusable solutions to aggregate knowledge from distributed health datasets, leading to the creation of organisations and new methodologies for clinical database exploration [8]. The main obstacle in sharing patient-level data concerns privacy issues, due to legal, ethical and regulatory requirements [9]. Patient data are very sensitive and disruption of this privacy can have dramatic consequences for individuals, healthcare providers and subgroups within society [10]. Besides, the legislation may be different in each country, which makes it difficult to define a protocol that fits in all the institutions involved [11]. Therefore, this is another challenge, which requires finding a solution that allows the analysis of multiple cohorts without exposing the data. Aiming to solve these issues, we propose a methodology to harmonise disease-specific cohorts, by storing data in a standard common model, and mapping clinical concepts to a normalised representation. The data schema is being used to harmonise EHR databases in observational studies at a world-wide scale, enabling the leveraging of previous knowledge and open source tools to perform multi-centric disease-specific studies [12]. 2 Related work Over the last years, researchers have investigated several strategies for performing clinical studies using heterogeneous data from multiple institutions. With this goal in mind, some projects, organisations and tools were created. Informatics for Integrating Biology and the Bedside (i2b2) [13] was one of the first projects aiming to create tools to support clinical researchers in integrating patient data. One of its outcomes was a web application capable of performing cohort estimations and determining study feasibility using anonymised EHR data [14]. A common issue in this approach is the need to have the data centralised and accessible to platform users. However, the centralisation of health data from distinct institutions is complex due to legal, ethical and regulatory policies. The Electronic Health Records for Clinical Research (EHR4CR) was a European project that aimed to improve the design of patient-centric trials [15]. Therefore, during this project, a platform was developed to support researchers in clinical trials’ feasibility assessment and patient recruitment by accessing the existent EHR systems. The platform could perform queries in real-time using multiple clinical data warehouses across Europe containing anonymised patient data. The researcher obtained as output the aggregated results. Although the architecture provides a good solution to access multiple datasets, its success depends completely on health institutions joining the network. HMORN (Health Maintenance Organization Research Network) was another project focusing on creating a large-scale distributed network of health data [16]. PopMedNet 1 1 is an open-source application resulting from this project, designed to simplify the operations over distributed health data queries. However, like the previously described initiatives it focuses on creating strategies to access the data, but not on developing a standard strategy to harmonise and anonymise patient information. The Observational Health Data Sciences and Informatics (OHDSI) 2 2 had a similar goal. This international organisation aims to develop methodologies to support large-scale observational studies in health care data. This organisation was initiated as an outcome of the OMOP project (Observational Medical Outcomes Partnership), to continue the research started on performing observational studies worldwide. Currently, this organisation supports an ecosystem with several open-source solutions to perform medical product safety surveillance using observational databases [8]. An example of such solutions is ATLAS, 3 3 a web-based platform to design cohorts and make a population-level analysis of observational data. A major outcome of the OMOP project was the definition and dissemination of a Common Data Model (CDM), which is a database schema to standardise the content of healthcare databases [17]. The original focus of this model was drug safety surveillance, but it was extended to many other use cases in quality of care, health economics and comparative effectiveness [17,18]. The model accommodates standard definitions for patients’ clinical data, allowing the use of federated queries across databases, enabling multiple and distributed analyses. Although this model is currently used for the data in Electronic Health Record (EHR) databases, we believe that its potential is not limited to this domain. Another outcome of OHDSI was the ETL (Extraction, Transformation and Loading) procedures and tools defined in this context. These tools were specifically designed for EHR data, but they could be adapted for the proposed scenario. Another project inspired by the core principles of OHDSI was the European Medical Information Framework project (EMIF). 4 4 This project aimed to enhance access to patient-level data from distinct health institutions across Europe, and to carry out multi-cohort studies on different diseases [19]. One of its tracks, EMIF-AD, aimed to accelerate the discovery and validation of new biomarkers to diagnose Alzheimer’s Disease (AD) in the pre-dementia stage and to predict the rate of decline [20]. For this purpose, a key aspect to be solved was the combination of distinct cohort datasets into a uniform and consistent model. Therefore, having all these platforms to analyse EHR data, the effort to prepare the cohort data would benefit from these tools up to a certain level. The main issue is the lack of ETL methodologies to handle this kind of data. Although OHDSI already has an ETL pipeline defined for EHR data, the cohort datasets raise different challenges because these do not follow a homogeneous or relational data schema in the raw data. 3 Proposal The proposed methodology reuses as many open-source tools and methodologies as possible, avoiding the development of new ones with similar goals. Therefore, we adopted some of the OHDSI tools and principles in some components of our methodology. Regarding the data schema to store the migrated cohort, we used part of the OMOP CDM without any changes in its structure, as keeping the data schema as it is may increase the interoperability between the databases created from cohorts and the EHR databases, if necessary. This interoperability is ensured because the data schema was not adapted for the cohort scenario. Instead, we tried to fit the information in the existing tables. Therefore, it is possible to use the same analytical tools used for exploring the EHR data migrated to OMOP CDM. We also used some of the ETL (Extraction, Transformation and Loading) supportive tools from OHDSI, which we adapted for the cohort mapping scenario. Although this was a good starting point, we felt the need to use a collaborative platform to manage those mappings through a semantic ontology. This ontology characterises all the elements involved in the vocabularies with extra information and organises them by their relationship with each other. 3.1 Overview The proposed methodology is based on ETL principles. Therefore, in the extraction stage, the selected source data are read by pulling them from one or several data sources. The main goal of this stage is getting the data from the source systems without interfering with their usual performance. In health databases, this is a sensitive task because the EHR cannot be overloaded due to the data extraction procedure. However, in clinical studies, the amount of data is not sufficient to crash the systems during this stage. Furthermore, clinical studies were exported to spreadsheets, which does not require direct interaction with the system used to collect the patient data. The transformation stage is the most complex component in this pipeline. This stage requires the mapping of the source database onto the target schema, as well as harmonisation of the content. For a data source, this procedure requires a full mapping, which is costly. Content harmonisation could have custom operations over the data based on the source of the data. In clinical databases, there is a wide variability of clinical concepts that need to be harmonised using standard vocabularies. Although we were able to automatise parts of this stage, we still require manual validation by a specialised health professional to ensure that all mapped data are correct. Finally, the loading stage inserts the processed data into the target database, which can be then accessed using analytical tools. In clinical databases, these databases are populated with anonymous data, allowing clinical studies to be conducted without violating patients’ privacy rights. Additionally, when the data are migrated to a standard data schema, the original data end up being validated, and inconsistencies can be found in the source database. This is possible due to the quality mechanisms that were created in the pipeline, which are responsible for checking whether loaded data respect the rule attributes for each standard concept. 3.2 The OMOP common data model schema One of the key points in cohort harmonisation is the use of a common data schema for data storage, such as the OMOP CDM. This standard data schema, which is continuously being improved by the OHDSI community and serves as the base for the observational databases in this community, has an excessive number of tables for the identified problem of cohort harmonisation, mainly because this model was designed to extract data from EHR systems. However, clinical studies focused on a disease only need a small part of this schema to store their information. The complete OMOP CDM data schema is detailed at [21]. This data schema was optimised for observational research purposes and the tables and the field of each table were defined by the OHDSI community. However, our approach relies on the set of OMOP CDM tables presented in Fig. 1, without changing their relations and structure. The Person table stores the patient’s personal information, i.e. gender, date of birth, race and ethnicity. The Observation table maintains all the measures made during the study. Each entry in this table contains: (1) a numerical entry for patient identification, which is only used in this database; (2) the standard code for the observation concept, i.e. specific exam conducted during the patient’s visit; (3) the standard code for the observation type concept, which characterises the measure/exam done on the patient when it can be represented using a standard code; (4) the date and value of the observation. This value can be characterised by its type, i.e. it can be numeric, text or a code. The Observation Period table contains the time interval when each patient was under observation. The OMOP CDM has a set of tables belonging to the “Standardized Health System Data”group. Therefore, we also used the Care Site and Location tables to store information about the institution where the clinical study was made. Additionally, we used all the tables from the “Standardised Vocabularies”group to store the standard concepts’ dictionaries. This data schema is created and the database loaded in the third stage of the workflow, namely the loading stage. 3.3 The ETL OHDSI tools The ETL OHDSI tools were an excellent starting point to harmonise the data recorded in the clinical studies, mainly because they were developed to handle clinical data independently of the data format. In the proposed methodology, we used some features of White Rabbit and Usagi for the extraction, harmonisation and mapping of the patients’ clinical data. These tools are part of the ETL pipeline used in the OHDSI migrations, where White Rabbit scans the data source and creates a structured report with all the information about the database content. Usagi is a complementary tool that receives some of the information available in this report to map the concepts with their standard definition. Cohorts follow a spreadsheet structure because this was the export format usually used in the institutional systems, or in some cases the way that the data were recorded. Using White Rabbit in the extraction stage of the methodology workflow, we can have an overview of these datasets, namely the different records made in the clinical study and some statistical representation of their content. The report produced by this tool helps identify some anomalies in the data in a first view. This report is also used as input in some components during the different steps of our methodology. Our adaptation of the Usagi tool plays two different roles in our proposal. One role is concept mapping, which is similar to the original goal of this tool. In this way, we can map the study columns and observations onto the standard vocabularies. The other role is to map the cohort structure onto the OMOP CDM data schema. This tool is a core component of the transformation stage of our workflow. 3.4 Collaborative ontology development The concepts in the database, by being mapped to their standard definition, are easily recognised by medical teams and also allow identification of the same concept in different cohort studies. This procedure refines the data existent in the dataset by discarding unmapped concepts, but the raw cohort data contain more patient information that is not directly present. Depending on the clinical study scenario, i.e. the diseases or health effects in the study, the observations can have additional meanings. Traditional ETL typically extracts the data, converts them into the source target schema and loads the data in a new data schema. This is very efficient when applied to data with a static and well-organised structure [22]. In our scenario, we have additional information that needs to be annotated during the transformation. In a very simple example using two common measurements such as weight and height, we can calculate the patient’s body mass index. As a result, when this value is above 30, the patient is obese which means that the patient has a cardiovascular risk factor that can be classified as a comorbidity [23]. This example, only based on the patient’s height and weight, shows how much information is in the raw data that can improve the efficiency in the patient selection stage in clinical studies. There is more information that could be extracted in this way, but the teams responsible for designing ETL mappings are not able to infer this. To augment the dataset with this semantic information, we rely on WebProtégé [24], to build and keep updated the ontology applied in our harmonisation workflow. This web platform facilitates collaboration between the clinical experts involved in the project, leading to the definition of a disease-specific ontology in the AD domain. At the end, we were able to obtain a structured semantic ontology containing properties to infer knowledge by correlating fields during the migration, and as an additional feature, other properties to validate the input information in each concept. This ontology is used in the transformation stage of the ETL workflow. 4 Workflow implementation: the CMToolkit The proposed methodology was implemented in Python using the adaptations of the previously described tools, and is publicly available, under the MIT licence, at This methodology includes the stages of the ETL operations, i.e. the workflow from the cohort’s raw data into the OMOP CDM database being divided into three stages, as presented in Fig. 2. In our implementation, we segregate these stages so as to allow the user to execute them in isolation. The WhiteRabbit, represented in the extraction stage, provides fingerprinting of the cohort structure. In parallel to this, the cohort reader loads the data into a pre-transformed format. These two outputs are used in the transformation stage, following a parallel flow. Usagi reads the WhiteRabbit outputs and generates the mappings to be used by the Cohort Harmoniser. This main block centralises a set of operations to generate an output file that can be exported to CSV files or to a database using the CDM loader in the loading stage. The implementation of some components was challenging due to the proposed use case. The use of medical data requires deep knowledge of the data source, in order to perform the harmonisation correctly. Another challenging task was the custom operations in each cohort raw data, i.e.. When the data were collected, the entities responsible followed a non-standard strategy, which complicated the migration workflow. Another challenging task was the data loading into the OMOP CDM and quality assurance in the created database. 4.1 Data harmonisation Data harmonisation is the most complex task in this workflow and consequently this stage was split into several steps, which work in parallel. As shown in Fig. 2, there is a pre-processing component to access cohort data stored in the temporary structure and reorganise that information based on the patient follow-ups. This component creates a key–value structure where each measurement is represented with all the patient and time information. This structure includes the patient’s measurements, the date of the exam and the follow-up visit number, which also represents the length of time from the first visit. The key–value would contain as key, (1) the patient identifier, (2) an attribute such as the visit date, and (3) the exam or cohort attribute. The value would be the entry for that attribute and in the next interaction, the standard concept codes for this entry and the attribute. Fig. 3 illustrates an example of the cohort raw data (first table) and its structure in the format processed during all the workflow (table below). The blue box contains the three fields that define the key of the key–value structure. The green box shows two fields that receive the concept codes of the harmonised values. There are situations in which the harmonised value is empty, such as the presented example. However, the harmonised exam needs to be filled, otherwise that entry would be discarded during the loading stage. The cohort owners describe the harmonisation and mapping of concepts using our adapted version of Usagi. The goal of this adaptation was not to improve the metrics obtained from this tool, but instead, to reduce the complexity when dealing with multi-language cohorts, which demanded a significant effort in translating and mapping the concepts manually [25]. The outputs obtained from this procedure are essential to know the cohort variables that are important to migrate, what the standard concepts are for each one, and the mapping of the measurements. In the cohort harmoniser component, the system uses a new structure and adds new attributes. The structure with key–value measurements and the information needed for their characterisation now has more fields identifying the concept type, as well as the standard code for the variable mappings, whereas for the measurement it is possible to have numeric values, strings or concepts. As mentioned in Section 3.4, there is much knowledge stored in raw data that is not directly represented. During harmonisation, the proposed system reads the cohort’s ontology to check and calculate these new variables following the predefined rules. For instance, the same exam in two distinct cohorts can have a different abnormal range of values depending on the technology used to perform the exam, which was easily calculated by specifying in the ontology the normal range of values. This information combined with another patient condition led to a new entry in the database regarding a comorbidity that was not previously defined in the raw data. 4.2 Customised operations The harmoniser component is capable of processing almost all the cohort migration. However, some scenarios are cohort-specific, requiring extra attention. In these cases, we need to develop custom methods, e.g. using python, which will then be called by the harmoniser and process the data with maximum priority over the usual migration. An example of the use of those methods is when there are variables such as “0”and “1”which should represent “no”and “yes”, respectively, but in a specific cohort the “0”can represent the absence of response and the real values for “no”and “yes”are “1”and “2”. Although this example could be solved in Usagi mapping, it can also be solved in this stage of the workflow. These methods are particularly interesting to deal with errors in the cohort data. For example, when the cohort originally stores the patient height in centimetres, but some measurements were recorded in other units, a custom method can easily solve the problem without changing the data source. At the end, this situation is reported to the data owners, so that they can fix the data inconsistency. Another example regards variables that are split in columns or when two variables are in the same column. For both situations, the best solution is to pre-process the data with a custom method that will reorganise it without performing any mapping. In this way, the system will run as was foreseen in normal execution. These operations need to be implemented in Python as modules that the harmoniser will load when executed. Fig. 4 shows a diagram that represents the interaction with these modules in the transforming stage. The ad-hoc modules are represented in green, and these are loaded in the Cohort Harmoniser through a connector. Therefore, the person responsible for handling these special fields only needs to create a module to transform the data mapped to specific standard codes. This module is then injected during the pipeline in the harmoniser. 4.3 Data loading onto OMOP CDM In the final stage of the ETL workflow we can load the data onto the OMOP CDM schema. The system can connect to a new database and perform this loading automatically or return a set of CSV files with the data harmonised and structured. When this methodology is adapted for a new cohort, for further data updates, the pipeline does not need to be changed, and it is ready to append the new data or clean and write a new database. Then, the data can be analysed and validated. At this stage, the system also produces a migration report, which is an execution log with all the errors and warnings that occurred during the procedure. This report helps in validating the migration and identifying data inconsistencies. For instance, when there are measurements with values out of the defined range of values in the ontology or when these values are not from the same types as specified, this will appear as a warning. Additionally, this report shows incorrect dates and missing records, with the latter being detected based on the mappings done by the annotator. If a variable is mapped, this report will contain a warning for each patient with a missing measurement in that variable. 5 Results The proposed methodology enabled the creation of a research ecosystem using multiple cohort data of patients suffering from AD. The main result of this pipeline was the creation of this ecosystem, which harmonised and validated the data. However, during the development of this work, in collaboration with the cohort owners, medical researchers and technical teams, an ontology was created to be used as a base for migrating other AD cohorts. Furthermore, this work also allowed the establishment of a strategy that lets external researchers explore the data of these cohorts without violating data privacy. 5.1 Ontology The ontology was built using the Clinical Data Interchange Standards (CDISC) 5 5 to serve as a guideline, in which we integrated the knowledge of clinical experts in the field and from previous harmonisation efforts related to AD [26,27]. In the ontology, we added the same concepts of the standard vocabularies, reducing the vocabulary size considerably, and simplifying the mapping task. This has two main benefits: it provides an elegant structure to manage the rules to apply over the concepts during the migration process, and it decreases the number of concepts in the Usagi dictionary, which increases the tool’s performance. The ontology created follows a hierarchical structure, subdivided into 12 domains: • Clinical Information: contains sub-domains that describe some clinical information, namely related to alcohol use, smoking, vital signs, comorbidities, clinical visits and follow-ups, and medication use. • Cognitive Screening Tests: contains the concepts for cognitive screening tests, namely cognitive estimation, memory alteration, montreal cognitive assessment and mini-mental state tests. • Demographics: is a small domain for characterising patients at the demographical level. • Harmonised Biomarker Values: is a node for storing meta-information about the possible values of the harmonised biomarkers. • Imaging: contains the standard concepts to map information of CT, MRI and PET exams. • Laboratory Test Results: includes the concepts related to Blood and CSF protocols. • Lifestyle Factors: contains the concepts to map the patient’s information about nutrition, physical activity and sleep. • Neuropsychological Examination: is a node with several layers related to neuropsychological exams, namely visuoconstruction, language, memory, intelligence and attention. • Pharmacogenetics Findings: is mostly related to the apolipoprotein E gene present in the patients. • Rating Scales: defines the rating scales for the different institutions, which is used as a control value, when available. • Subject Characteristics: this node contains information about the patient’s lifestyle and education. • Study Information: contains the cohort raw data metadata. In each of these domains there are several layers of sub-domains, with detailed information that characterises them, for instance, the concept type, range-of-values, a brief description defining the concept, and in specific cases, some additional information relevant for the migration workflow. Fig. 5 shows an ontology entry, which represents how a concept is defined in this ontology. 5.2 Cohort harmonisation The harmonisation workflow was validated in an initial stage using 2 synthetic datasets that were generated from real data. These cohorts had small numbers of patients and a reduced number of concepts. However, we were able to test and validate the efficiency of the automatised components. This initial validation was required to ensure that the system was developed with quality and that the outputs produced were as expected. This validation was made manually with the collaboration of the elements from Alzheimer Centre Amsterdam, in the Netherlands. They received a small sample of the database generated with the methodology and identified possible structural errors, namely in mapping of the concepts in the OMOP CDM schema. With the full pipeline consolidated, we used two heterogeneous cohorts from the EMIF-AD project. Those cohorts are the Berlin Memory Clinic (BMC) cohort related to the Charité University Hospital in Berlin containing 6583 individuals, and a small set of 86 patients from the BioBank Alzheimer Center Limburg (BBACL) cohort related to the memory clinic of the Maastricht University Medical Centre. Both cohorts were mapped following the pipeline described in Section 5. All the attributes were analysed, but we only mapped the variables of interest for AD studies. We used the minimal clinical dataset composed for the EMIF-AD Multimodal Biomarker Discovery study as a guideline for which variables to map [20]. The BMC cohort provided 85 attributes, of which 59 were mapped and 26 discarded. The BBACL cohort contains 313 variables, but only 113 were included in the minimal clinical dataset. From the mapped variables, we further generated new attributes based on the ontology rules: 8 from the Berlin dataset and 20 from the Maastricht cohort. A summary of these variables is presented in Table 1. The variable mapped in this migration is the information considered of interest for future studies. The number of variables discarded represents noise in the data, which would be difficult for the analysis of the cohort if these were not migrated. In the case of Maastricht, a considerable number of variables related to blood analysis was available, which the researchers did not find interest in using for AD studies. The composed variables are the new information that was indirectly present in the cohort, but it was identified and stored in a searchable format. Similarly to the harmonisation procedure and knowledge representation, the migration pipeline also detected incorrect values in the collected data. This analysis allowed the datasets to be cleaned, providing at the end more accurate information. 5.3 Data analysis environment The cohort data in this new format raise new opportunities mainly regarding data analysis. Although the data can be aggregated in a unique database, the goal of this work was to keep each cohort in a unique database. This strategy gives the cohort owners full control over their data when a study is conducted. Fig. 6 shows an overview of the workflow used when a researcher conducts a study, including different levels of privacy. In the first situation (marked in green), the researchers never access the cohort data, but they can send their ATLAS package containing the SQL queries to the Cohort Owners. Then, these entities run the ATLAS package against their databases and analyse the results obtained before sending them to the researcher. If the Cohort Owners agree to share these results, they can respond to the researcher with the package outcome. This strategy is similar to the one presented at [29] and gives Cohort Owners full control of the data. Although this level of privacy may not be required, this strategy can be adopted with the proposed methodology and the data in a common data format. In the second situation (marked in orange), the researcher is allowed to query the cohort database directly. This is a more flexible scenario in which the data can be shared. However, patient privacy is still ensured since one of the principles of the OMOP CDM schema is the anonymisation of all entries. 5.4 Study example: the prevalence of amyloid pathology Let us assume a scenario where researchers want to study the prevalence of amyloid pathology, the first pathological hallmark of AD, in females. One cohort might have measured amyloid pathology via the levels of amyloid-beta 1-42 in cerebrospinal fluid, while another cohort had an amyloid Positron Emission Tomography (PET) scan. From both measures (CSF and Amyloid PET) we can define an abnormal value that allows combining different methods. To answer this research question using the raw data, we would need customised search queries for each cohort, just to obtain the subset desired. This is not straightforward and may require technical support to access and filter the data of interest, which can result in developing specially-designed coding scripts to extract the information. On the other hand, by using the migrated cohorts to answer this question, researchers just need to know how to use ATLAS from OHDSI or the database schema, including which concepts to search. Using ATLAS, the researcher defines the query without the need to know about SQL queries, because this platform already exports a package to be applied in other institutions and work over their databases. For this question, the query will contain the following concept codes for: Female (8532), Baseline observation type (2100000000), Amyloid Beta 1-42 (2000000070), Amyloid Beta 1-42 Abnormal (2000000071), SUVr exams (2000000428) and SUVr Abnormal exams (2000000429). 6 Discussion The results of this work show several benefits in adopting the proposed methodology to migrate cohort data into OMOP CDM databases. Therefore, we discuss the advantages of having the cohort data in this format focusing on data quality and analysis. We also discuss the interoperability of cohorts in the network and how this solves the privacy issues initially presented. Finally, we identify possible limitations of this methodology, which may not affect its use in other scenarios, depending on the study or project requirements. 6.1 Data quality and analysis One advantage of using this workflow is data quality. At the end of the ETL procedure, the system was able to provide a migration report that includes statistical information about the data migrated, including inconsistencies in the source data. This information was helpful for the cohort owners so they could rectify these issues, which were mainly values collected manually during the patient’s follow-up visits. Besides this data quality control and the adoption of a common model, the methodology facilitates data sharing in multiple cohort studies. The research question can be defined in one dataset, where the cohort details are specified, and the resulting query can be shared and executed in the remaining cohorts to assess whether the medical findings are replicable in different populations. This query can be manually defined in the database through SQL language, or by using ATLAS. ATLAS can also be considered a web user-friendly query builder for OMOP CDM databases. Here, the user does not need to know any programming language to work with the data. For instance, considering the following scenario: a researcher wants to study a patient dataset based on several medications and exams, patients’ personal information such as age and gender, and correlating these conditions in a temporal window of events. The answer to this research question using the raw data cohort requires the use of spreadsheet tools in order to filter and obtain the desired subset of patients. If the data are stored in institutional systems, the support of IT teams may be necessary to query the databases, which is time-consuming and not always feasible. Both strategies are currently used in several institutions, but there is a considerable delay associated with data collection. Additionally, neither approach allows data interoperability, which is a main requirement of our methodology. Using ATLAS, this researcher can define this query very quickly. An example of this is presented in Section 5.4, which is a study focused on retrieving the number of females with prevalence of amyloid pathology. Although we used one simple study case, this can easily scale based on the complexity of the study design. For instance, the researcher can define in the web platform the cohort entry events, inclusion and exclusion criteria, the concepts studied and others. 6.2 Interoperability and privacy The advantages of the proposed methodology are not limited to simplification of the data analysis. This also allows use of the same analytical tools in distinct cohorts. The OHDSI community includes specialised tools to show statistical information about the dataset graphically using the ACHILLES, 6 6 tool, which is an R package that performs broad database characterisation. These principles can be applied to the cohorts migrated to the OMOP CDM schema that adopted the proposed migration procedure. Therefore, ATLAS and ACHILLES provide a web environment with analytic features to work with migrated datasets individually, but by being in a homogeneous data schema, these analyses are easily replicated. Additionally, there are other tools, namely the EHDEN NetworkDashboards 7 7 which are focused on comparing OMOP CDM databases, and these features can also be adopted to compare different cohort datasets in order to understand which are feasible as part of a multi-cohort study. Cohorts’ data schema are typically distinct and the integration of multiple cohorts is always an ad-hoc procedure that typically needs to be repeated for each new study. With the proposed methodology, i.e., data harmonisation into a standard schema, we can avoid this problem and speed up research. At the same time, since the data transformation is performed locally by each data team, we ensure the privacy of combined data. Therefore, our methodology can overcome some existent barriers in medical research regarding ethical, legal and social issues. The ethical and legal aspects related to patients’ data privacy and the second use of this information are settled because the OMOP CDM format is compliant with GDPR guidelines. The social issue, namely that researchers usually do not want to share the data, is also solved since we contemplate a scenario in which the data do not need to be shared at all. 6.3 Limitations The methodology was developed to generate OMOP CDM databases using cohort raw data. However, changing the output data schema to completely different from the OMOP CDM may require a restructuring of the loading stage of the proposed pipeline. Small adjustments in this structure are possible with minor effects on the developed system. When we developed the workflow, we kept in mind possible adjustments in the OMOP CDM, because OHDSI is an active community that has improved the OMOP CDM aiming to expand to other medical domains. The methodology was implemented and validated using AD cohorts. We do not consider this methodology limited to this domain. However, applying this migration workflow using cohorts from other diseases may require some adjustments, namely in defining an ontology for this new domain. The methodology is focused on the ETL procedure, which contemplates dataset harmonisation at different levels, and it adopts well-established tools designed to perform EHR observational studies in cohort datasets. Although these cohorts are more disease-specific, the aggregation of results from different institutions has revealed impactful findings [30,31]. 7 Conclusion Multi-cohort studies empower clinical research by extending the research to different populations with similar characteristics. In the study of rare conditions or diseases with a low number of subjects to be studied, the reduced number of participants is normally the most significant drawback to attaining a solid investigation and a higher impact of results. However, using similar cohorts from distinct and independent studies has the potential to increase the research value and validate the findings. To simplify this research scenario, we developed a migration pipeline that relies on a standard data schema (the OMOP CDM), on normalised vocabularies (Unified Medical Language System), and on open-source analytic tools (the OHDSI ecosystem). The result of this work helps collaboration between different clinical institutions studying the same disease, respecting patients’ data privacy. Additionally, this pipeline simplifies data filtering and sharing, necessary to answer specific research questions without making a new clinical trial. Although we presented a fully functional methodology applied to Alzheimer’s Disease, as future work, the methodology can be replicated with other disorders. This may require definition of a new ontology, using the identified tools, or the reuse and extension of an existing one. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. References [1] Ranganathan P. Aggarwal R. Study designs: Part 1–an overview and classification Perspect in Clin Res 9 4 2018 184 10.4103/picr.PICR_124_18 P. Ranganathan, R. Aggarwal, Study designs: Part 1–an overview and classification, Perspectives in clinical research 9 (4) (2018) 184. doi:10.4103/picr.PICR˙124˙18. [2] Song J.W. Chung K.C. Observational studies: cohort and case-control studies Plast Reconstr Surg 126 6 2010 2234 10.1097/PRS.0b013e3181f44abc J. W. Song, K. C. Chung, Observational studies: cohort and case-control studies, Plastic and reconstructive surgery 126 (6) (2010) 2234. doi:10.1097/PRS.0b013e3181f44abc. [3] Lu C.Y. Observational studies: a review of study designs, challenges and strategies to reduce confounding Int J Clin Pract 63 5 2009 691 697 10.1111/j.1742-1241.2009.02056.x C. Y. Lu, Observational studies: a review of study designs, challenges and strategies to reduce confounding, International journal of clinical practice 63 (5) (2009) 691–697. doi:10.1111/j.1742-1241.2009.02056.x. [4] Ranganathan P. Aggarwal R. Study designs: Part 3-analytical observational studies Perspect Clin Res 10 2 2019 91 10.4103/picr.PICR_35_19 P. Ranganathan, R. Aggarwal, Study designs: Part 3-analytical observational studies, Perspectives in clinical research 10 (2) (2019) 91. doi:10.4103/picr.PICR˙35˙19. [5] Carlson M.D. Morrison R.S. Study design, precision, and validity in observational studies J Palliat Med 12 1 2009 77 82 10.1089/jpm.2008.9690 M. D. Carlson, R. S. Morrison, Study design, precision, and validity in observational studies, Journal of palliative medicine 12 (1) (2009) 77–82. doi:10.1089/jpm.2008.9690. [6] Harris P.A. Taylor R. Thielke R. Payne J. Gonzalez N. Conde J.G. Research electronic data capture (REDCap)—a metadata-driven methodology and workflow process for providing translational research informatics support J Biomed Inform 42 2 2009 377 381 10.1016/j.jbi.2008.08.010 P. A. Harris, R. Taylor, R. Thielke, J. Payne, N. Gonzalez, J. G. Conde, Research electronic data capture (REDCap)a metadata-driven methodology and workflow process for providing translational research informatics support, Journal of biomedical informatics 42 (2) (2009) 377–381. doi:10.1016/j.jbi.2008.08.010. [7] Brown C.H. Sloboda Z. Faggiano F. Teasdale B. Keller F. Burkhart G. Methods for synthesizing findings on moderation effects across multiple randomized trials Prev Sci 14 2 2013 144 156 10.1007/s11121-011-0207-8 C. H. Brown, Z. Sloboda, F. Faggiano, B. Teasdale, F. Keller, G. Burkhart, F. Vigna-Taglianti, G. Howe, K. Masyn, W. Wang, et al., Methods for synthesizing findings on moderation effects across multiple randomized trials, Prevention science 14 (2) (2013) 144–156. doi:10.1007/s11121-011-0207-8. [8] Hripcsak G. Duke J.D. Shah N.H. Reich C.G. Huser V. Schuemie M.J. Observational Health Data Sciences and Informatics (OHDSI): opportunities for observational researchers Stud Health Technol Inform 216 2015 574 10.3233/978-1-61499-564-7-574 G. Hripcsak, J. D. Duke, N. H. Shah, C. G. Reich, V. Huser, M. J. Schuemie, M. A. Suchard, R. W. Park, I. C. K. Wong, P. R. Rijnbeek, et al., Observational Health Data Sciences and Informatics (OHDSI): opportunities for observational researchers, Studies in health technology and informatics 216 (2015) 574. doi:10.3233/978-1-61499-564-7-574. [9] Cushman R. Froomkin A.M. Cava A. Abril P. Goodman K.W. Ethical, legal and social issues for personal health records and applications J Biomed Inform 43 5 2010 S51 S55 10.1016/j.jbi.2010.05.003 R. Cushman, A. M. Froomkin, A. Cava, P. Abril, K. W. Goodman, Ethical, legal and social issues for personal health records and applications, Journal of biomedical informatics 43 (5) (2010) S51–S55. doi:10.1016/j.jbi.2010.05.003. [10] Fox G. “To protect my health or to protect my health privacy?” a mixed-methods investigation of the privacy paradox J Assoc Inf Sci Technol 71 9 2020 1015 1029 10.1002/asi.24369 G. Fox, ”to protect my health or to protect my health privacy?” a mixed-methods investigation of the privacy paradox, Journal of the Association for Information Science and Technology 71 (9) (2020) 1015–1029. doi:10.1002/asi.24369. [11] Meystre S.M. Lovis C. Bürkle T. Tognola G. Budrionis A. Lehmann C.U. Clinical data reuse or secondary use: current status and potential future progress Yearb Med Inform 26 01 2017 38 52 10.15265/IY-2017-007 S. M. Meystre, C. Lovis, T. Bürkle, G. Tognola, A. Budrionis, C. U. Lehmann, Clinical data reuse or secondary use: current status and potential future progress, Yearbook of medical informatics 26 (01) (2017) 38–52. doi:10.15265/IY-2017-007. [12] Hripcsak G. Ryan P.B. Duke J.D. Shah N.H. Park R.W. Huser V. Characterizing treatment pathways at scale using the OHDSI network Proc Natl Acad Sci 113 27 2016 7329 7336 10.1073/pnas.1510502113 G. Hripcsak, P. B. Ryan, J. D. Duke, N. H. Shah, R. W. Park, V. Huser, M. A. Suchard, M. J. Schuemie, F. J. DeFalco, A. Perotte, et al., Characterizing treatment pathways at scale using the OHDSI network, Proceedings of the National Academy of Sciences 113 (27) (2016) 7329–7336. doi:10.1073/pnas.1510502113. [13] Murphy S.N. Weber G. Mendis M. Gainer V. Chueh H.C. Churchill S. Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2) J Am Med Inform Assoc 17 2 2010 124 130 10.1136/jamia.2009.000893 S. N. Murphy, G. Weber, M. Mendis, V. Gainer, H. C. Chueh, S. Churchill, I. Kohane, Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2), Journal of the American Medical Informatics Association 17 (2) (2010) 124–130. doi:10.1136/jamia.2009.000893. [14] McMurry A.J. Murphy S.N. MacFadden D. Weber G. Simons W.W. Orechia J. SHRINE: enabling nationally scalable multi-site disease studies PLoS One 8 3 2013 e55811 10.1371/journal.pone.0055811 A. J. McMurry, S. N. Murphy, D. MacFadden, G. Weber, W. W. Simons, J. Orechia, J. Bickel, N. Wattanasin, C. Gilbert, P. Trevvett, et al., SHRINE: enabling nationally scalable multi-site disease studies, PloS one 8 (3) (2013) e55811. doi:10.1371/journal.pone.0055811. [15] Daniel C. Ouagne D. Sadou E. Forsberg K. Mc Gilchrist M. Zapletal E. Cross border semantic interoperability for clinical research: the EHR4CR semantic resources and services AMIA Summits Transl Sci Proc 2016 2016 51 10.1002/lrh2.10014 C. Daniel, D. Ouagne, E. Sadou, K. Forsberg, M. Mc Gilchrist, E. Zapletal, N. Paris, S. Hussain, M.-C. Jaulent, D. Kalra, Cross border semantic interoperability for clinical research: the EHR4CR semantic resources and services, AMIA Summits on Translational Science Proceedings 2016 (2016) 51. doi:10.1002/lrh2.10014. [16] Weeks J. Pardee R. Learning to share health care data: a brief timeline of influential common data models and distributed health data networks in us health care research EGEMs 7 1 2019 10.5334/egems.279 J. Weeks, R. Pardee, Learning to share health care data: a brief timeline of influential common data models and distributed health data networks in us health care research, eGEMs 7 (1) (2019). doi:10.5334/egems.279. [17] Overhage J.M. Ryan P.B. Reich C.G. Hartzema A.G. Stang P.E. Validation of a common data model for active safety surveillance research J Am Med Inform Assoc 19 1 2011 54 60 10.1136/amiajnl-2011-000376 J. M. Overhage, P. B. Ryan, C. G. Reich, A. G. Hartzema, P. E. Stang, Validation of a common data model for active safety surveillance research, Journal of the American Medical Informatics Association 19 (1) (2011) 54–60. doi:10.1136/amiajnl-2011-000376. [18] Makadia R. Ryan P.B. Transforming the premier perspective hospital database into the observational medical outcomes partnership (OMOP) common data model Egems 2 1 2014 10.13063/2327-9214.1110 R. Makadia, P. B. Ryan, Transforming the Premier Perspective Hospital Database into the Observational Medical Outcomes Partnership (OMOP) Common Data Model, Egems 2 (1) (2014). doi:0.13063/2327-9214.1110. [19] Almeida J.R. Fajarda O. Pereira A. Oliveira J.L. Strategies to access patient clinical data from distributed databases HEALTHINF 2019 SciTePress 466 473 10.5220/0007576104660473 J. R. Almeida, O. Fajarda, A. Pereira, J. L. Oliveira, Strategies to Access Patient Clinical Data from Distributed Databases, in: HEALTHINF, SciTePress, 2019, pp. 466–473. doi:10.5220/0007576104660473. [20] Bos I. Vos S. Vandenberghe R. Scheltens P. Engelborghs S. Frisoni G. The EMIF-AD multimodal biomarker discovery study: design, methods and cohort characteristics Alzheimer’s Res Ther 10 1 2018 64 10.1186/s13195-018-0396-5 I. Bos, S. Vos, R. Vandenberghe, P. Scheltens, S. Engelborghs, G. Frisoni, J. L. Molinuevo, A. Wallin, A. Lleó, J. Popp, et al., The EMIF-AD Multimodal Biomarker Discovery study: design, methods and cohort characteristics, Alzheimer’s research & therapy 10 (1) (2018) 64. doi:10.1186/s13195-018-0396-5. [21] Hripcsak G. Ryan P. Madigan D. Kostka K. Schuemie M. DeFalco F. The book of OHDSI: Observational health data sciences and informatics, OHDSI 2019 Hripcsak G, Ryan P, Madigan D, Kostka K, Schuemie M, DeFalco F, et al, The Book of OHDSI: Observational Health Data Sciences and Informatics, OHDSI, 2019. [22] Deb Nath R.P. Hose K. Pedersen T.B. Towards a programmable semantic extract-transform-load framework for semantic data warehouses Proceedings of the ACM eighteenth international workshop on data warehousing and OLAP 2015 ACM 15 24 10.1145/2811222.2811229 R. P. Deb Nath, K. Hose, T. B. Pedersen, Towards a programmable semantic extract-transform-load framework for semantic data warehouses, in: Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP, ACM, 2015, pp. 15–24. doi:10.1145/2811222.2811229. [23] James P.T. Obesity: the worldwide epidemic Clin Dermatol 22 4 2004 276 280 10.1016/j.clindermatol.2004.01.010 P. T. James, Obesity: the worldwide epidemic, Clinics in dermatology 22 (4) (2004) 276–280. doi:10.1016/j.clindermatol.2004.01.010. [24] Tudorache T. Nyulas C. Noy N.F. Musen M.A. Webprotégé: A collaborative ontology editor and knowledge acquisition tool for the web vol. 4 2013 IOS Press 89 99 10.3233/SW-2012-0057 T. Tudorache, C. Nyulas, N. F. Noy, M. A. Musen, Webprotégé: A collaborative ontology editor and knowledge acquisition tool for the web, Vol. 4, IOS Press, 2013, pp. 89–99. doi:10.3233/SW-2012-0057. [25] Almeida J.R. Oliveira J.L. Multi-language concept normalisation of clinical cohorts 2020 IEEE 33rd international symposium on computer-based medical systems 2020 261 264 10.1109/CBMS49503.2020.00056 J. R. Almeida, J. L. Oliveira, Multi-language concept normalisation of clinical cohorts, in: 2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS), 2020, pp. 261–264. doi:10.1109/CBMS49503.2020.00056. [26] Vos S.J. Verhey F. Frölich L. Kornhuber J. Wiltfang J. Maier W. Prevalence and prognosis of alzheimer’s disease at the mild cognitive impairment stage Brain 138 5 2015 1327 1338 10.1093/brain/awv029 S. J. Vos, F. Verhey, L. Frölich, J. Kornhuber, J. Wiltfang, W. Maier, O. Peters, E. Rüther, F. Nobili, S. Morbelli, et al., Prevalence and prognosis of Alzheimers disease at the mild cognitive impairment stage, Brain 138 (5) (2015) 1327–1338. doi:10.1093/brain/awv029. [27] Jansen W.J. Ossenkoppele R. Knol D.L. Tijms B.M. Scheltens P. Verhey F.R. others Prevalence of cerebral amyloid pathology in persons without dementia: a meta-analysis JAMA 313 19 2015 1924 1938 10.1001/jama.2015.4668 W. J. Jansen, R. Ossenkoppele, D. L. Knol, B. M. Tijms, P. Scheltens, F. R. Verhey, P. J. Visser, P. Aalten, D. Aarsland, D. Alcolea, et al., Prevalence of cerebral amyloid pathology in persons without dementia: a meta-analysis, Jama 313 (19) (2015) 1924–1938. doi:10.1001/jama.2015.4668. [28] Athena - OHDSI vocabularies repository 2021 [Accessed 23 July 2021] Athena - OHDSI Vocabularies Repository, accessed: 2021-07-23 (2021). [29] Almeida J.R. Pratas D. Oliveira J.L. A semi-automatic methodology for analysing distributed and private biobanks Comput Biol Med 130 2021 104180 10.1016/j.compbiomed.2020.104180 J. R. Almeida, D. Pratas, J. L. Oliveira, A semi-automatic methodology for analysing distributed and private biobanks, Computers in Biology and Medicine 130 (2021) 104180. doi:10.1016/j.compbiomed.2020.104180. [30] Hong S. Prokopenko D. Dobricic V. Kilpert F. Bos I. Vos S.J. Genome-wide association study of Alzheimer’s disease CSF biomarkers in the EMIF-AD multimodal biomarker discovery dataset Transl Psychiatry 10 1 2020 1 12 10.1038/s41398-020-01074-z S. Hong, D. Prokopenko, V. Dobricic, F. Kilpert, I. Bos, S. J. Vos, B. M. Tijms, U. Andreasson, K. Blennow, R. Vandenberghe, et al., Genome-wide association study of Alzheimers disease CSF biomarkers in the EMIF-AD Multimodal Biomarker Discovery dataset, Translational psychiatry 10 (1) (2020) 1–12. doi:10.1038/s41398-020-01074-z. [31] Delvenne A. Gobom J. Tijms B.M. Bos I. Verhey F.R. Ramakers I.H. others CSF Proteomic profiling of mild cognitive impairment individuals with suspected non-Alzheimer’s disease pathophysiology: Developing topics Alzheimer’s Dementia 16 2020 e047247 10.1002/alz.047247 A. Delvenne, J. Gobom, B. M. Tijms, I. Bos, F. R. Verhey, I. H. Ramakers, P. Scheltens, C. E. Teunissen, R. Vandenberghe, S. Gabel, et al., CSF proteomic profiling of mild cognitive impairment individuals with suspected non-Alzheimers disease pathophysiology: Developing topics, Alzheimer’s & Dementia 16 (2020) e047247. doi:10.1002/alz.047247. "
    },
    {
        "doc_title": "VADER meets BERT: Sentiment analysis for early detection of signs of self-harm through social mining",
        "doc_scopus_id": "85113425769",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85113425769",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Computer engineering",
            "Keyword extraction",
            "Mental disorders",
            "Risk predictions",
            "Social media datum",
            "Supervised machine learning"
        ],
        "doc_abstract": "© 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Computer Engineering of University of Aveiro (BioInfo@UAVR) in the 2nd shared task of CLEF eRisk 2021. eRisk is an “Early Risk Prediction on the Internet” challenge whose tasks consist in analysing social media data and foster research on early detection of mental disorders. This year eRisk had 3 tasks, each focusing on a different disorder. This paper focuses on addressing the 2nd task, whose main objective is the early detection of users at risk of self-harming, based on their Reddit history. We addressed this issue by developing four supervised machine learning models that can classify such users. Our approaches are based on keyword extraction, sentiment analysis and word embeddings.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Leveraging Clinical Notes for Enhancing Decision-Making Systems with Relevant Patient Information",
        "doc_scopus_id": "85107294095",
        "doc_doi": "10.1007/978-3-030-72379-8_26",
        "doc_eid": "2-s2.0-85107294095",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Automatic extraction",
            "Clinical decision support systems",
            "Clinical treatments",
            "Decision-making systems",
            "Electronic health record systems",
            "Patient information",
            "Patient trajectories",
            "Real time evaluation"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.Personalised treatment is usually needed for hospitalised patients afflicted by secondary illnesses that demand daily medication. Even though clinical guidelines were designed to consider those circumstances exist, current decision-support features fail to assimilate detailed relevant patient information. This creates opportunities for the development of systems capable of performing a real-time evaluation of such data against existing knowledge and providing recommendations during clinical treatments. Herein, we describe a proposal for a new feature to be integrated with electronic health record (EHR) systems which can enrich the health treatment process through the automatic extraction of information from patient medical notes and the aggregation of this novel information in clinical protocols. The purpose of this work is to exploit the historical component of the patient trajectory to improve the performance of clinical decision support systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Bilingual emotion analysis on social media throughout the COVID-19 pandemic in Portugal",
        "doc_scopus_id": "85103846666",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85103846666",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Basic emotions",
            "Coronaviruses",
            "Critical periods",
            "Emotion analysis",
            "General population",
            "Proof of concept",
            "Social media",
            "Well being"
        ],
        "doc_abstract": "Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reservedThis paper presents preliminary work on the topic of emotion analysis on Twitter, in the context of the coronavirus pandemic in Portugal. We collected, curated and analyzed covid-related tweets of users in Portugal in order to understand the evolution of the six basic emotions reflected in these tweets. We analyzed tweets written in both English and Portuguese. In this first step of our work we correlate this information with key events of the evolution of the pandemic in Portugal during March, which was the most critical period in Portugal. We do so in an attempt to estimate the online manifestation of the psychological toll that this pandemic has on the overall well-being status of the general population. Our findings show that the sentiment analysis of covid-related tweets is consistent with our hypothesis that negative emotions would intensify as the pandemic progressed. The preliminary results obtained stand as proof of concept that the analysis of real-time tweets or other social media messages through sentiment analysis can be an important tool for behavioural and well-being tracking.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Machine Learning for Depression Screening in Online Communities",
        "doc_scopus_id": "85089220717",
        "doc_doi": "10.1007/978-3-030-54568-0_11",
        "doc_eid": "2-s2.0-85089220717",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Computational approach",
            "Cross evaluation",
            "Mental health",
            "On-line communities",
            "Potential sources",
            "Reproducibilities",
            "Research communities",
            "Social media"
        ],
        "doc_abstract": "© 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.Social media writings have been explored over the last years, in the context of mental health, as a potential source of information for extending the so-called digital phenotyping of a person. In this paper we present a computational approach for the classification of depressed social media users. We conducted a cross evaluation study based on two public datasets, collected from the same social network, in order to understand the impact of transfer learning when the data source is virtually the same. We hope that the results presented here challenge the research community to address more often the issues of reproducibility and interoperability, two key concepts in the era of computational Big Data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring a Siamese Neural Network Architecture for One-Shot Drug Discovery",
        "doc_scopus_id": "85099573915",
        "doc_doi": "10.1109/BIBE50027.2020.00035",
        "doc_eid": "2-s2.0-85099573915",
        "doc_date": "2020-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Accurate prediction",
            "Computational resources",
            "Drug discovery applications",
            "Drug discovery process",
            "Neural network model",
            "Proposed architectures",
            "Similarity functions",
            "Therapeutic activity"
        ],
        "doc_abstract": "© 2020 IEEE.The application of deep neural networks in drug discovery is mainly due to their enormous potential to significantly increase the predictive power when inferring the properties and activities of small-molecules. However, in the traditional drug discovery process, where supervised data is scarce, the lead-optimization step is a low-data problem, making it difficult to find molecules with the desired therapeutic activity and obtain accurate predictions for candidate compounds. One major requirement to ensure the validity of the obtained neural network models is the need for a large number of training examples per class, which is not always feasible in drug discovery applications. This invalidates the use of instances whose classes were not considered in the training phase or in data where the number of classes is high and oscillates dynamically. The main objective of the study is to optimize the discovery of novel compounds based on a reduced set of candidate drugs. We propose a Siamese neural network architecture for one-shot classification, based on Convolutional Neural Networks (CNNs), that learns from a similarity score between two input molecules according to a given similarity function. Using a one-shot learning strategy, few instances per class are needed for training, and a small amount of data and computational resources are required to build an accurate model. The results achieved demonstrate that using a Siamese Deep Neural Network for one-shot classification leads to overall improved performance when compared to other state-of the-art models. The proposed architecture provides an accurate and reliable prediction of novel compounds considering the lack of biological data available for drug discovery tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Merging microarray studies to identify a common gene expression signature to several structural heart diseases",
        "doc_scopus_id": "85087926456",
        "doc_doi": "10.1186/s13040-020-00217-8",
        "doc_eid": "2-s2.0-85087926456",
        "doc_date": "2020-07-08",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 The Author(s).Background: Heart disease is the leading cause of death worldwide. Knowing a gene expression signature in heart disease can lead to the development of more efficient diagnosis and treatments that may prevent premature deaths. A large amount of microarray data is available in public repositories and can be used to identify differentially expressed genes. However, most of the microarray datasets are composed of a reduced number of samples and to obtain more reliable results, several datasets have to be merged, which is a challenging task. The identification of differentially expressed genes is commonly done using statistical methods. Nonetheless, these methods are based on the definition of an arbitrary threshold to select the differentially expressed genes and there is no consensus on the values that should be used. Results: Nine publicly available microarray datasets from studies of different heart diseases were merged to form a dataset composed of 689 samples and 8354 features. Subsequently, the adjusted p-value and fold change were determined and by combining a set of adjusted p-values cutoffs with a list of different fold change thresholds, 12 sets of differentially expressed genes were obtained. To select the set of differentially expressed genes that has the best accuracy in classifying samples from patients with heart diseases and samples from patients with no heart condition, the random forest algorithm was used. A set of 62 differentially expressed genes having a classification accuracy of approximately 95% was identified. Conclusions: We identified a gene expression signature common to different cardiac diseases and supported our findings by showing their involvement in the pathophysiology of the heart. The approach used in this study is suitable for the identification of gene expression signatures, and can be extended to different diseases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A recommender system to help discovering cohorts in rare diseases",
        "doc_scopus_id": "85091187517",
        "doc_doi": "10.1109/CBMS49503.2020.00012",
        "doc_eid": "2-s2.0-85091187517",
        "doc_date": "2020-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Alzheimer's disease",
            "Clinical investigation",
            "Context based retrieval",
            "Health condition",
            "Research groups",
            "Scientific findings",
            "Scientific literature",
            "Statistical power"
        ],
        "doc_abstract": "© 2020 IEEE.Cohort studies have been playing a key role in helping our understanding of diseases, health conditions, and treatments. These cohorts are often composed of a small number of subjects, especially in rare diseases studies, which reduces the statistical power of the results. One solution that can strengthen the scientific findings is to combine distinct studies and perform then multi-cohort analysis. However, even studies conducted for the same purpose in distinct research groups can have different scopes and medical observations, which preclude across-cohort exploration. In this paper, we propose a recommendation system to automatically discover cohorts of interest. This methodology uses context-based retrieval techniques combined with collaborative filtering to find relevant cohorts and scientific literature about a specific clinical investigation. The system was validated in a community focused on the study of Alzheimer's diseases, which includes 62 cohorts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-language concept normalisation of clinical cohorts",
        "doc_scopus_id": "85091140845",
        "doc_doi": "10.1109/CBMS49503.2020.00056",
        "doc_eid": "2-s2.0-85091140845",
        "doc_date": "2020-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Alzheimer's disease",
            "Automatic mapping",
            "Clinical data",
            "Language detection",
            "Multi languages",
            "Normalisation",
            "Research questions",
            "Standard definitions"
        ],
        "doc_abstract": "© 2020 IEEE.The exploration of multiple cohorts allows researchers to answer new research questions using more substantial clinical data. However, this is only possible if the cohorts are interoperable, which implies the migration of the original cohort into a common data schema. The problem of this procedure is the effort necessary to map the original concepts into their standard definitions. While several automatic mapping solutions can help in this task, its complexity increases when dealing with multi-language cohorts, leading to a significant manual effort in translating and mapping. In this paper we propose a system that combines text mining with language detection techniques, aiming to optimise these migration pipelines. This system was designed to be integrated into already existing migration workflows, without the need of adapting them. The system was validated using Alzheimer's diseases cohorts, but it is enough general to be applied in other use cases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GTO: A toolkit to unify pipelines in genomic and proteomic research",
        "doc_scopus_id": "85086630979",
        "doc_doi": "10.1016/j.softx.2020.100535",
        "doc_eid": "2-s2.0-85086630979",
        "doc_date": "2020-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "C language",
            "Life-sciences",
            "Modular architectures",
            "Next-generation sequencing",
            "Proteomic"
        ],
        "doc_abstract": "© 2020 The AuthorsNext-generation sequencing triggered the production of a massive volume of publicly available data and the development of new specialised tools. These tools are dispersed over different frameworks, making the management and analyses of the data a challenging task. Additionally, new targeted tools are needed, given the dynamics and specificities of the field. We present GTO, a comprehensive toolkit designed to unify pipelines in genomic and proteomic research, which combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of the data. This toolkit combines novel tools with a modular architecture, being an excellent platform for experimental scientists, as well as a useful resource for teaching bioinformatics enquiry to students in life sciences. GTO is implemented in C language and is available, under the MIT license, at https://bioinformatics.ua.pt/gto.",
        "available": true,
        "clean_text": "serial JL 312019 291210 291690 291735 291791 291848 31 90 SoftwareX SOFTWAREX 2020-06-20 2020-06-20 2020-06-20 2020-06-20 2021-06-24T16:39:01 S2352-7110(20)30147-3 S2352711020301473 10.1016/j.softx.2020.100535 S300 S300.3 FULL-TEXT 2021-06-24T16:23:59.139745Z 0 0 20200701 20201231 2020 2020-06-20T23:10:09.350353Z articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid fundingbodyid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder sponsoredaccesstype srctitle srctitlenorm srctype ssids alllist content oa subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor grantsponsorid primabst pubtype ref 2352-7110 23527110 UNLIMITED NONE true 12 12 C Volume 12 21 100535 100535 100535 202007 202012 July–December 2020 2020-07-01 2020-12-31 2020 simple-article osp © 2020 The Authors. Published by Elsevier B.V. GTOATOOLKITUNIFYPIPELINESINGENOMICPROTEOMICRESEARCH ALMEIDA J 1 Motivation and significance 2 Software description 2.1 Software architecture 2.2 Software functionalities 2.2.1 Genomics 2.2.2 Proteomics 2.2.3 General purpose 2.2.4 External tools 3 Illustrative examples 3.1 Bi-directional complexity profiles 3.2 Rearrangements map generation 3.3 Viral metagenomic identification 4 Impact 5 Conclusions Acknowledgements References MARDIS 2017 213 E VANDERAUWERA 2013 G KESSNER 2008 2534 2536 D CHEN 2018 i884 i890 S LIU 2019 4560 4567 Y GRABOWSKI 2019 677 678 S MALYSA 2015 3122 3129 G LIU 2019 2066 2074 Y CHANDAK 2019 2674 2676 S PINHO 2013 e79922 A PINHO 2011 2024 2028 A 201119THEUROPEANSIGNALPROCESSINGCONFERENCE SYMBOLICNUMERICALCONVERSIONDNASEQUENCESUSINGFINITECONTEXTMODELS PRATAS 2019 137 145 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS GECO2OPTIMIZEDTOOLFORLOSSLESSCOMPRESSIONANALYSISDNASEQUENCES HOSSEINI 2019 68 76 M CARVALHO 2018 49 55 J HOSSEINI 2016 56 M AGUEROCHAPIN 2006 723 730 G PRATAS 2015 10203 D ZIELEZINSKI 2019 A BENCHMARKINGALIGNMENTFREESEQUENCECOMPARISONMETHODS FORSLUND 2019 469 504 S EVOLUTIONARYGENOMICS EVOLUTIONPROTEINDOMAINARCHITECTURES ROST 1999 85 94 B PRATAS 2018 D FALCONAMETHODINFERMETAGENOMICCOMPOSITIONANCIENTDNA PRATAS 2018 1177 1181 D 201826THEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO METAGENOMICCOMPOSITIONANALYSISSEDIMENTARYANCIENTDNAISLEWIGHT HUANG 2011 593 594 W DROOP 2016 1883 1884 A AFGAN 2018 W537 W544 E SHEN 2016 e0163962 W DEPRISTO 2011 491 M GOECKS 2010 R86 J BLANKENBERG 2010 1783 1785 D LIU 2017 3364 3372 Y OCHOA 2014 626 633 I DEOROWICZ 2015 11565 S PRATAS 2018 105 113 D INTERNATIONALCONFERENCEPRACTICALAPPLICATIONSCOMPUTATIONALBIOLOGYBIOINFORMATICS COMPRESSIONAMINOACIDSEQUENCES PRATAS 2014 40 D ESCALONA 2016 459 M ALMEIDAX2020X100535 ALMEIDAX2020X100535XJ Full 2020-06-03T16:10:38Z Author This is an open access article under the CC BY license. © 2020 The Authors. Published by Elsevier B.V. 2021-06-03T16:43:35.966Z Foundation for Science and Technology SFRH/BD/147837/2019 UIDB/00127/2020 FCT Fundação para a Ciência e a Tecnologia Centro 2020 program NETDIAMOND POCI-01-0145-FEDER-016385 This work has received support from the NETDIAMOND, Portugal project ( POCI-01-0145-FEDER-016385 ), co-funded by Centro 2020 program, Portugal 2020, European Union , and from the FCT - Foundation for Science and Technology, Portugal , in the context of the project UIDB/00127/2020. João Almeida is supported by FCT - Foundation for Science and Technology, Portugal (national funds), grant SFRH/BD/147837/2019 . item S2352-7110(20)30147-3 S2352711020301473 10.1016/j.softx.2020.100535 312019 2021-06-24T16:23:59.139745Z 2020-07-01 2020-12-31 UNLIMITED NONE true 1061501 MAIN 6 51343 849 656 IMAGE-WEB-PDF 1 gr3 21082 116 376 gr1 51221 263 470 gr4 24493 141 376 fx1001 4519 27 817 gr2 16382 83 470 gr3 6347 68 219 gr1 10351 123 219 gr4 7605 82 219 fx1001 777 7 219 gr2 3191 39 219 gr3 155334 516 1668 gr1 436164 1166 2083 gr4 160054 624 1667 fx1001 16238 73 2171 gr2 128419 370 2083 si1 667 SOFTX 100535 100535 S2352-7110(20)30147-3 10.1016/j.softx.2020.100535 The Authors Fig. 1 Bi-directional complexity profiles of four types of human Herpesvirus (HHV2, HHV3, HHV4 and HHV7) generated with GTO using the pipeline: gto_complexity_profile_regions.sh. Complexity values below one are highlighted with blue colour while the others with green. Bps stands for bits per symbol where lower values represent redundancy. The length is in Kb (Kilobases) and all profiles use the same scale. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Bi-directional complexity profiles of human titin protein generated with GTO using the pipeline: gto_proteins_complexity_profile_regions.sh. Complexity values below three are highlighted with a red colour while the others with blue. Bps stands for bits per symbol where lower values represent redundancy. The length is in Ks (Kilosymbols). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 3 Rearrangements map generated with GTO using the pipeline: gto_map_rearrangements.sh. The length of both sequences (A and B) is 5 MB. Wave pattern stands for inverted repeated regions. Fig. 4 Rearrangements map generated with GTO using the pipeline: gto_map_rearrangements_proteins.sh. Table 1 The eight most representative reference sequences according to the RS (Relative Similarity). ID stands for the order of the top output, length for the size of the reference genome, and GID for the sequence global identifier. ID Length RS (%) Reference GID Virus name 1 124884 97.767 X04370.1 HHV3 2 5596 96.603 AY386330.1 B19V 3 172764 94.143 DQ279927.1 HHV4 4 154675 81.400 JN561323.2 HHV2 5 154746 80.153 Z86099.2 HHV2 6 2785 78.300 AB041963.1 TTV 7 7372 71.445 MG921180.1 HPV 8 549 47.591 AY034056.1 PHV3-BALF1-gene Original software publication GTO: A toolkit to unify pipelines in genomic and proteomic research João R. Almeida a b ⁎ Armando J. Pinho a José L. Oliveira a Olga Fajarda a Diogo Pratas a c a Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro Aveiro Portugal Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal b Department of Information and Communications Technologies, University of A Coruña, A Coruña, Spain Department of Information and Communications Technologies, University of A Coruña, A Coruña Spain Department of Information and Communications Technologies, University of A Coruña, A Coruña, Spain c Department of Virology, University of Helsinki, Helsinki, Finland Department of Virology, University of Helsinki Helsinki Finland Department of Virology, University of Helsinki, Helsinki, Finland ⁎ Corresponding author at: Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro, Aveiro, Portugal. Department of Electronics, Telecommunications and Informatics (DETI/IEETA), University of Aveiro Aveiro Portugal Next-generation sequencing triggered the production of a massive volume of publicly available data and the development of new specialised tools. These tools are dispersed over different frameworks, making the management and analyses of the data a challenging task. Additionally, new targeted tools are needed, given the dynamics and specificities of the field. We present GTO, a comprehensive toolkit designed to unify pipelines in genomic and proteomic research, which combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of the data. This toolkit combines novel tools with a modular architecture, being an excellent platform for experimental scientists, as well as a useful resource for teaching bioinformatics enquiry to students in life sciences. GTO is implemented in C language and is available, under the MIT license, at Keywords Genomic Toolkit Proteomic Toolkit Next-generation sequencing Code metadata Current code version v1.5 Permanent link to code/repository used for this code version Legal Code License MIT Code versioning system used GIT Software code languages, tools, and services used C Compilation requirements, operating environments & dependencies GCC and Make If available Link to developer documentation/manual Support email for questions pratas@ua.pt Software metadata Current software version v1.5 Permanent link to executables of this version Legal Software License MIT Computing platforms/Operating Systems Linux and Unix-like Installation requirements & dependencies GCC and Make If available, link to user manual - if formally published include a reference to the publication in the reference list Support email for questions pratas@ua.pt 1 Motivation and significance Next-generation sequencing (NGS) has become an essential tool in genetic and genomic analysis with a substantial impact in the fields of biomedicine and anthropology. The advantages of NGS over traditional methods include its multiplex capability and analytical resolution, making it a time and cost-efficient approach for fast clinical and forensic screening [1]. The development of efficient bioinformatics tools is essential to assess and analyse the large volumes of sequencing data produced by next-generation sequencers. However, more important than that are the computational methods that unify the existing tools, given the notable pace at which these tools become available. Toolkits are sets of tools that combine multiple features in a custom-based manner as some examples show, both in genomics [2] and proteomics [3]. Developing a toolkit requires a specific architecture, namely, taking into account the purpose and technologies, accessibility, compatibility, portability, interoperability, and usability. Moreover, implementation needs to consider efficiency, while maintaining affordable computational resources and the absence of dependencies (standalone use). We contribute with GTO (Genomic Toolkit), a set of tools to unify pipelines operating both at genomic and proteomic levels, with an open licence and free of any dependency. This toolkit includes information theory-based tools for reference-free and reference-based data compression applied to data analysis. Among many applications, this toolkit supports the creation of workflows for identification of metagenomic composition in FASTQ reads, detection and visualisation of genomic rearrangements, mapping and visualisation of variation, localisation of low complexity regions, or simulation of sequences with specific SNP and structural variant rates. The toolkit was designed for Unix/Linux-based systems, built for ultra-fast computations. It supports pipes for easy integration with the sub-programmes as well as external tools. GTO works as LEGOsTM , since it allows the construction of multiple pipelines with many combinations. We support the toolkit with a detailed manual and a website with several examples, including an online manual for fast learning. Due to the variety and distribution of the given tools and their tight interconnection using the command line with pipes, the toolkit is an excellent platform for scientists as well as for empowering students to progress to the scientific aspects of bioinformatics analysis efficiently. Therefore, without the need to install multiple programmes, dependencies, and read different manuals or licences, it is possible to maintain an easy-to-follow connection with all the phases of each pipeline application. 2 Software description GTO is a powerful toolkit composed of more than 75 tools with particular focus on genomics and proteomics, following an integrative and flexible design between the tools. GTO includes tools for information display, randomisation, edition, conversion, extraction, search, calculation, compression, simulation and visualisation. The toolkit can be used in common Linux distributions. We have been using GTO in common personal computers (e.g. a laptop with 8 GB RAM, 128 GB of SSD and an intel-i3 CPU from the 5th generation), but these characteristics can vary according to the data size and the execution requirements. 2.1 Software architecture The tools composing this toolkit aim for key features such as being easy to use, compile and improve and specially designed for work in Unix/Linux command line. These tools can be used in isolation, or combined as one, forming execution workflows. This is technically possible due to the two streams used for the computation, namely the standard input and output. Furthermore, the tools’ aggregation is possible with mechanisms for inter-process communication using message passing, provided by the Unix operating system. This creates a chain of processes in which the output of each process is passed directly as input to the subsequent one, as shown in the following example: In addition to the input/output standard streams, some of the tools accept parameterisation through the definition of arguments when executed. There is also a small set of tools in which the input or output does not make sense to be the standard streams and for those the argument definition is considered. 2.2 Software functionalities The toolkit contains three main groups of tools according to its characteristic: Genomics, Proteomics, and General purpose. The genomics group is subdivided in: FASTQ, FASTA, SEQ (genomic sequences); while the proteomics contains AA (amino acid); the general-purpose tools can be applied to any format sequence. 2.2.1 Genomics The toolkit allows data conversion between different formats namely FASTQ, FASTA and SEQ. It also provides features for filtering and randomising DNA sequences, as well as for analytic purposes followed by simulations of a generation and alteration nature. The SEQ cluster works directly with the DNA sequences without any standard format. These tools allow data extraction, summary, classification and mathematical operations in the field of information theory. Among many examples, which are better described in the supporting website and manual, the toolkit allows preparations of the reads, namely filtering and trimming, the automatic construction of nucleotide reference databases, and comparative genomics. 2.2.2 Proteomics The toolkit has a specific cluster of tools designed to group, compress, and analyse amino acid sequences. These tools allow proteomic analysis based on the amino acids properties, such as electric charge (positive and negative), uncharged side chains, hydrophobic side chains and special cases. The toolkit allows translation of codons into amino acids, permits finding approximate amino acid sequences and performing comparative proteomics analysis. 2.2.3 General purpose This set of tools is complementary to the genomics and proteomics tools, not being designed to work in a specific field, but to assist the pipelines composed of the previously described subsets. These tools provide operations in the symbolical domain, including reversion, segmentation, and permutation; while in the numerical domain they contain tools with low-pass filters (with multiple window types), sum, min and max operations over streams. 2.2.4 External tools External top-performing tools have been integrated in order to increase the variety of functionalities available. The tools integrated are the following: • fastp [4]: enables ultra-fast preprocessing and quality control of FASTQ files. • bfMEM [5]: detects maximal exact matches between a pair of genomes based on bloom filters and rolling hashs. • copMEM [6]: another tool for computing maximal exact matches in a pair of genomes. • qvz [7]: implements a lossy compression algorithm for storing quality scores associated with DNA sequencing. • minicom [8]: a compressor for short reads in FASTQ files that uses large k-minimisers to index the reads. • SPRING [9]: which is reference-free compression tool for FASTQ files. 3 Illustrative examples All the tools in the toolkit were tested with synthetic sequences aiming for individual validation. Therefore, the documented examples are easily replicable with the written tests. Besides applying these tools in controlled environments, the toolkit was also used in several research workflows both as a primary and auxiliary tool. Several complete workflows are available in the repository, under the pipelines folder while an extensive description of the tool can be found in the manual. Next, we include some pipeline examples. 3.1 Bi-directional complexity profiles A workflow example is the computation of bi-directional complexity profiles in any genomic or proteomic sequence [10]. These profiles can localise specific features in the sequences, namely low and high complexity sequences, inverted repeats regions, tandem duplications, among others. The construction of these profiles follows a pipeline formed of many transformations (e.g. reversing, segmenting, inverting) as well as the use of specific low-pass filters after data compression applications [11]. Fig. 1 depicts the complexity profiles of four human Herpesvirus whole genomes using the same scale, where redundant regions are highlighted in blue (below a Bps of one). GTO uses GeCo2 [12] and AC [13] compressors to estimate the local complexity of DNA and amino acid sequences, respectively. However, GTO is not limited to using these data compressors. For example, new models can be tested under this framework, namely with extended alphabets [14]. In general, any data compressor able to output local estimations can be used in the pipeline as an alternative [15]. Analogous to the complexity profiles for DNA sequences, an example using amino acid sequences is given in Fig. 2. This example depicts a bi-directional complexity profile for the largest human protein sequence, titin. Several regions with low complexity are usually associated with specific characteristics, namely loops [16]. 3.2 Rearrangements map generation Another example workflow is in the domain of comparative genomics, namely to map and visualise rearrangements. This workflow is completely automatic from the input of the sequences to the generation of an SVG image, with the associated and transformed regions corresponding to the rearrangements. The pipeline applies smash technology [17,18] for mapping the rearrangements using an alignment-free methodology [19]. To prove the efficiency of the mapping pipeline, we use another pipeline to generate two identical FASTA files with simulated rearrangements between them (gto_simulate_rearragements.sh). After, loading the two FASTA files into the mapping pipeline (gto_map_rearrangements.sh), the output is two files, one with the mapping positions and the other is an SVG image depicting the mapped positions as can be seen in Fig. 3. All the rearrangements have been efficiently mapped with GTO according to the ground truth ( < 1 s of computational time). Analogous to the rearrangements map pipeline, for mapping at proteomic level, we consider the NAV2 HUMAN Neuron navigator 2 and the neuron navigator 2 isoform X15 of Macaca mulatta proteins. Although there are many examples under the proteome evolution [20], these are protein sequences considering identical scale [21]. Additionally, we shuffled the Macaca mulatta proteins using a block size of 300 amino acids. Fig. 4 depicts the proteins map after running the pipeline (gto_map_rearrangements_proteins.sh). Despite a low level of dissimilarity of the sequences with an additional pseudo-random permutation of blocks of 300 symbols, all the regions have been efficiently mapped with GTO ( < 1 s of computational time). 3.3 Viral metagenomic identification A final workflow example is the full automatic metagenomic identification of viral (or any other) content in FASTQ reads. This includes the filtering and trimming of the reads, mapping, and sensitive identification of the most representative genomes, under a ranking of abundance. In this particular example, we generate a semi-synthetic viral dataset containing several real viruses with applied degrees of substitutions and block permutations shuffled with synthetic noisy DNA. This dataset is generated using the gto_create_viral_dataset.sh pipeline. The intention is to perform a metagenomic analysis on this dataset without informing the programme what organisms are contained in the sample since the programme needs to infer the results. Then, we compare the results with the ground truth. If the results are similar to the ground truth, then the methodology is validated. For the purpose, GTO uses falcon-meta technology [22,23] that relies on assembly-free and alignment-free comparison of each reference according to the whole reads. The dataset contains synthetic reads (uniform distribution) merged with the following viruses with the respective modifications: • B19V: two Parvovirus, one with 1% of editions and the other with permuted blocks of 500 bases (GID: AY386330.1); • HHV2: one human Herpesvirus 2 with permuted blocks of size 100 bases (GID: JN561323.2); • HHV3: one human Herpesvirus 3 (GID: X04370.1); • HHV4: two human Herpesvirus 4, one with permuted blocks of 300 bases (GID: DQ279927.1); • TTV: one human Torque teno virus with 5% of editions (GID: AB041963.1); • HPV: one human Papillomavirus with 5% of editions and permuted blocks of 300 bases (GID: MG921180.1). After merging all FASTA sequences, ART [24] was used to generate the paired end FASTQ reads. Meanwhile, another workflow example was used to create the viral database (gto_build_dbs.sh). Then, the pipeline (gto_metagenomics.sh) ran, obtaining the top output presented in Table 1. We can conclude that despite the noise, editions, and permutations applied to real data, all the viruses have been efficiently identified with GTO, including the exact genotype ( < 1.5 min of computational time). 4 Impact Many software application exist to analyse and manipulate sequencing data, namely fqtools [25], GALAXY [26], FASTX-Toolkit [27], SeqKit [28], GATK [29], among others. The fqtools is a suite of tools to view, manipulate and summarise FASTQ data [25]. This software was designed to work specifically with FASTQ files and can be easily integrated into our toolkit. However, the features existent in this software are similar to some of the ones that GTO has in the gto_fastq_* section. Both were written in C which in terms of performance could be similar. GALAXY, is an open and web-based scientific platform for analysing genomic data [30]. This platform integrates several specialised sets of tools, e.g. for manipulating FASTQ files [31]. In this web application, the FASTX-Toolkit was integrated, which is a collection of command-line tools to process FASTA and FASTQ files [27]. The available features in the FASTX-Toolkit are also similar to some of the GTO tools designed to preprocess the FASTA/FASTQ files, which are available in the gto_fastq_* and gto_fasta_* sections. As our goal always was to have an easy to use toolkit written in low-level programming languages and not a web interface, we cannot compare it with GALAXY. However, regarding the FASTX-Toolkit which was also written in C, it is possible to compare and combine it with some of the GTO’s features. The SeqKit is another toolkit used to process FASTA and FASTQ files and it is available for all major operating systems [28]. Comparing the performance and limitations of this toolkit with the fqtools and FASTX-Toolkit is easier than comparing them with GTO, mainly because these three toolkits were designed specifically to manipulate FASTA/FASTQ files. On the other hand, these functionalities are only a fraction of the features that we provide in GTO. The idea never was to create more tools to compete with the ones existing, but instead, aggregate them in order to obtain a more complete toolkit for genomics analysis. This idea of simplifying the development and aggregation of analysis tools for genomic manipulation and analysis is not new. Initially designed as a structured programming framework, the Genome Analysis Toolkit (GATK) is a set of bioinformatics tools for analysing high-throughput sequencing focused on variant discovering and genotyping [2]. The high performance of this toolkit is due to the required infrastructures that a personal computer cannot offer. This is an excellent toolkit that integrates Apache Spark for optimisation, but it is only possible to take advantage of this potential in cloud computing. The efficient performance from some of the presented tools as well as GTO’s tools is due to the use of low-level programming languages (e.g. C language). However, one limitation of this strategy, in which the performance is prioritised, is the lack of a graphical user interface. Moreover, to take full advantage from those tools, the end-users need to have basic shell script knowledge. Nevertheless, GTO combines specialised tools for analysis, simulation, compression, development, visualisation, and transformation of data. Therefore, we would like to highlight some important details that characterise this toolkit: • The toolkit aggregates different tools in order to build research pipelines to deal with very large data sets without losing performance due to its modular architecture. Adoption of standard streams to interconnect the tools improved data processing. Throughout this procedure, the disk read/write operations between tools have been removed by sending the output directly to the input of the next tool. • The toolkit can integrate external tools, besides the ones already available. As such, some specific tools that have already been evaluated and used outside this context were aggregated: – For compression purposes, the toolkit integrates GeCo2 [12], which along with HiRGC [32], iDoComp [33] and GDC2 [34] are considered to have some of the best performance for reference-free DNA compression [35]. Regarding the amino acid sequences, the toolkit uses the AC tool for lossless sequence compression. The performance of AC was compared in [36] to several general-purpose lossless compressors and several protein compressors using different proteomes and AC provides on average the best bit-rates. – Concernings simulation, GTO integrates XS [37] which is a FASTQ read simulation tool. Escalona et al. [38] reviewed 23 NGS simulation tools and XS stands out in relation to the others because it is the only one that does not need a reference sequence. – Additionally, we added a section in the toolkit specially designed for tools from other authors. This way, we simplify their integration and installation using GTO. Those were described in Section 2.2.4. • Finally, as briefly presented in Section 3, the toolkit can answer new genomics questions without the need to create new software. 5 Conclusions We contribute with GTO, a toolkit to unify research pipelines, composed of distinct tools aiming at efficient combinations of them towards specific workflows. GTO’s efficient performance is due to the use of low-level programming languages, which increases the processing speed and decreases the RAM of addressing genomics and proteomics data. The flexibility of this toolkit allows the end-user to quickly create new processing pipelines in the genomic and proteomic field as it was described in the examples provided in this manuscript. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgements This work has received support from the NETDIAMOND, Portugal project (POCI-01-0145-FEDER-016385), co-funded by Centro 2020 program, Portugal 2020, European Union, and from the FCT - Foundation for Science and Technology, Portugal, in the context of the project UIDB/00127/2020. João Almeida is supported by FCT - Foundation for Science and Technology, Portugal (national funds), grant SFRH/BD/147837/2019. References [1] Mardis E.R. DNA sequencing technologies: 2006–2016 Nat Protoc 12 2 2017 213 E. R. Mardis, DNA sequencing technologies: 2006–2016, Nature Protocols 12 (2) (2017) 213. [2] Van der Auwera G.A. Carneiro M.O. Hartl C. Poplin R. Del Angel G. Levy-Moonshine A. From FASTQ data to high-confidence variant calls: the genome analysis toolkit best practices pipeline Curr Protoc Bioinform 43 1 2013 11.10.1-11.10.33 G. A. Van der Auwera, M. O. Carneiro, C. Hartl, R. Poplin, G. Del Angel, A. Levy-Moonshine, T. Jordan, K. Shakir, D. Roazen, J. Thibault, others,From FASTQ data to high-confidence variant calls: the genome analysis toolkit best practices pipeline, Current Protocols in Bioinformatics 43 (1) (2013) 11–10. [3] Kessner D. Chambers M. Burke R. Agus D. Mallick P. ProteoWizard: open source software for rapid proteomics tools development Bioinformatics 24 21 2008 2534 2536 D. Kessner, M. Chambers, R. Burke, D. Agus, P. Mallick, ProteoWizard: open source software for rapid proteomics tools development, Bioinformatics 24 (21) (2008) 2534–2536. [4] Chen S. Zhou Y. Chen Y. Gu J. Fastp: an ultra-fast all-in-one FASTQ preprocessor Bioinformatics 34 17 2018 i884 i890 S. Chen, Y. Zhou, Y. Chen, J. Gu, fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics 34 (17) (2018) i884–i890. [5] Liu Y. Zhang L.Y. Li J. Fast detection of maximal exact matches via fixed sampling of query K-mers and Bloom filtering of index K-mers Bioinformatics 35 22 2019 4560 4567 Y. Liu, L. Y. Zhang, J. Li, Fast detection of maximal exact matches via fixed sampling of query K-mers and Bloom filtering of index K-mers, Bioinformatics 35 (22) (2019) 4560–4567. [6] Grabowski S. Bieniecki W. CopMEM: finding maximal exact matches via sampling both genomes Bioinformatics 35 4 2019 677 678 S. Grabowski, W. Bieniecki, copMEM: finding maximal exact matches via sampling both genomes, Bioinformatics 35 (4) (2019) 677–678. [7] Malysa G. Hernaez M. Ochoa I. Rao M. Ganesan K. Weissman T. QVZ: lossy compression of quality values Bioinformatics 31 19 2015 3122 3129 G. Malysa, M. Hernaez, I. Ochoa, M. Rao, K. Ganesan, T. Weissman, QVZ: lossy compression of quality values, Bioinformatics 31 (19) (2015) 3122–3129. [8] Liu Y. Yu Z. Dinger M.E. Li J. Index suffix–prefix overlaps by (w, k)-minimizer to generate long contigs for reads compression Bioinformatics 35 12 2019 2066 2074 Y. Liu, Z. Yu, M. E. Dinger, J. Li, Index suffix–prefix overlaps by (w, k)-minimizer to generate long contigs for reads compression, Bioinformatics 35 (12) (2019) 2066–2074. [9] Chandak S. Tatwawadi K. Ochoa I. Hernaez M. Weissman T. SPRING: a next-generation compressor for FASTQ data Bioinformatics 35 15 2019 2674 2676 S. Chandak, K. Tatwawadi, I. Ochoa, M. Hernaez, T. Weissman, SPRING: a next-generation compressor for FASTQ data, Bioinformatics 35 (15) (2019) 2674–2676. [10] Pinho A.J. Garcia S.P. Pratas D. Ferreira P.J. DNA sequences at a glance PLoS One 8 11 2013 e79922 A. J. Pinho, S. P. Garcia, D. Pratas, P. J. Ferreira, DNA sequences at a glance, PloS one 8 (11) (2013) e79922. [11] Pinho A.J. Pratas D. Ferreira P.J. Garcia S.P. Symbolic to numerical conversion of DNA sequences using finite-context models 2011 19th European signal processing conference 2011 IEEE 2024 2028 A. J. Pinho, D. Pratas, P. J. Ferreira, S. P. Garcia, Symbolic to numerical conversion of DNA sequences using finite-context models, in: 2011 19th European Signal Processing Conference, IEEE, 2011, 2024–2028. [12] Pratas D. Hosseini M. Pinho A.J. GeCo2: an optimized tool for lossless compression and analysis of DNA sequences International conference on practical applications of computational biology & bioinformatics 2019 Springer 137 145 D. Pratas, M. Hosseini, A. J. Pinho, GeCo2: an optimized tool for lossless compression and analysis of DNA sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2019, 137–145. [13] Hosseini M. Pratas D. Pinho A.J. AC: a compression tool for amino acid sequences Interdiscip Sci: Comput Life Sci 11 1 2019 68 76 M. Hosseini, D. Pratas, A. J. Pinho, AC: a compression tool for amino acid sequences, Interdisciplinary Sciences: Computational Life Sciences 11 (1) (2019) 68–76. [14] Carvalho J.M. Brás S. Pratas D. Ferreira J. Soares S.C. Pinho A.J. Extended-alphabet finite-context models Pattern Recognit Lett 112 2018 49 55 J. M. Carvalho, S. Brás, D. Pratas, J. Ferreira, S. C. Soares, A. J. Pinho, Extended-alphabet finite-context models, Pattern Recognition Letters 112 (2018) 49–55. [15] Hosseini M. Pratas D. Pinho A. A survey on data compression methods for biological sequences Information 7 4 2016 56 M. Hosseini, D. Pratas, A. Pinho, A survey on data compression methods for biological sequences, Information 7 (4) (2016) 56. [16] Agüero-Chapin G. González-Díaz H. Molina R. Varona-Santos J. Uriarte E. González-Díaz Y. Novel 2D maps and coupling numbers for protein sequences. The first QSAR study of polygalacturonases; isolation and prediction of a novel sequence from Psidium guajava L. FEBS Lett 580 3 2006 723 730 G. Agüero-Chapin, H. González-Díaz, R. Molina, J. Varona-Santos, E. Uriarte, Y. González-Díaz, Novel 2D maps and coupling numbers for protein sequences. The first QSAR study of polygalacturonases isolation and prediction of a novel sequence from Psidium guajava L., FEBS Letters 580 (3) (2006) 723–730. [17] Pratas D. Silva R.M. Pinho A.J. Ferreira P.J. An alignment-free method to find and visualise rearrangements between pairs of DNA sequences Sci Rep 5 2015 10203 D. Pratas, R. M. Silva, A. J. Pinho, P. J. Ferreira, An alignment-free method to find and visualise rearrangements between pairs of DNA sequences, Scientific Reports 5 (2015) 10203. [18] Hosseini M, Pratas D, Morgenstern B, Pinho AJ. Smash++: an alignment-free and memory-efficient tool to find genomic rearrangements. GigaScience 9(5). [19] Zielezinski A. Girgis H.Z. Bernard G. Leimeister C.-A. Tang K. Dencker T. Benchmarking of alignment-free sequence comparison methods 2019 BioRxiv A. Zielezinski, H. Z. Girgis, G. Bernard, C.-A. Leimeister, K. Tang, T. Dencker, A. K. Lau, S. Röhling, J. Choi, M. S. Waterman, others,Benchmarking of alignment-free sequence comparison methods, BioRxiv (2019) 611137. [20] Forslund S.K. Kaduk M. Sonnhammer E.L. Evolution of protein domain architectures Evolutionary genomics 2019 Springer 469 504 S. K. Forslund, M. Kaduk, E. L. Sonnhammer, Evolution of protein domain architectures, in: Evolutionary Genomics, Springer, 2019, 469–504. [21] Rost B. Twilight zone of protein sequence alignments Protein Eng 12 2 1999 85 94 B. Rost, Twilight zone of protein sequence alignments, Protein engineering 12 (2) (1999) 85–94. [22] Pratas D. Pinho A.J. Silva R.M. Rodrigues J.M. Hosseini M. Caetano T. FALCON: a method to infer metagenomic composition of ancient DNA 2018 BioRxiv D. Pratas, A. J. Pinho, R. M. Silva, J. M. Rodrigues, M. Hosseini, T. Caetano, P. J. Ferreira, FALCON: a method to infer metagenomic composition of ancient DNA, BioRxiv (2018) 267179. [23] Pratas D. Pinho A.J. Metagenomic composition analysis of sedimentary ancient DNA from the isle of wight 2018 26th european signal processing conference (EUSIPCO) 2018 IEEE 1177 1181 D. Pratas, A. J. Pinho, Metagenomic composition analysis of sedimentary ancient DNA from the Isle of Wight, in: 2018 26th European Signal Processing Conference (EUSIPCO), IEEE, 2018, 1177–1181. [24] Huang W. Li L. Myers J.R. Marth G.T. ART: a next-generation sequencing read simulator Bioinformatics 28 4 2011 593 594 W. Huang, L. Li, J. R. Myers, G. T. Marth, ART: a next-generation sequencing read simulator, Bioinformatics 28 (4) (2011) 593–594. [25] Droop A.P. Fqtools: an efficient software suite for modern FASTQ file manipulation Bioinformatics 32 12 2016 1883 1884 A. P. Droop, fqtools: an efficient software suite for modern FASTQ file manipulation, Bioinformatics 32 (12) (2016) 1883–1884. [26] Afgan E. Baker D. Batut B. Van Den Beek M. Bouvier D. Čech M. The galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update Nucleic Acids Res 46 W1 2018 W537 W544 E. Afgan, D. Baker, B. Batut, M. Van Den Beek, D. Bouvier, M. Čech, J. Chilton, D. Clements, N. Coraor, B. A. Grüning, others,The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update, Nucleic acids research 46 (W1) (2018) W537–W544. [27] Gordon A, Hannon G et al. Fastx-toolkit, FASTQ/A short-reads preprocessing tools Accessed: 2020-06-17. [28] Shen W. Le S. Li Y. Hu F. SeqKit: a cross-platform and ultrafast toolkit for FASTA/Q file manipulation PLoS One 11 10 2016 e0163962 W. Shen, S. Le, Y. Li, F. Hu, SeqKit: a cross-platform and ultrafast toolkit for FASTA/Q file manipulation, PLoS One 11 (10) (2016) e0163962. [29] DePristo M.A. Banks E. Poplin R. Garimella K.V. Maguire J.R. Hartl C. A framework for variation discovery and genotyping using next-generation DNA sequencing data Nature Genet 43 5 2011 491 M. A. DePristo, E. Banks, R. Poplin, K. V. Garimella, J. R. Maguire, C. Hartl, A. A. Philippakis, G. Del Angel, M. A. Rivas, M. Hanna, others,A framework for variation discovery and genotyping using next-generation DNA sequencing data, Nature Genetics 43 (5) (2011) 491. [30] Goecks J. Nekrutenko A. Taylor J. Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences Genome Biol 11 8 2010 R86 J. Goecks, A. Nekrutenko, J. Taylor, Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences, Genome Biology 11 (8) (2010) R86. [31] Blankenberg D. Gordon A. Von Kuster G. Coraor N. Taylor J. Nekrutenko A. Manipulation of FASTQ data with galaxy Bioinformatics 26 14 2010 1783 1785 D. Blankenberg, A. Gordon, G. Von Kuster, N. Coraor, J. Taylor, A. Nekrutenko, G. Team, Manipulation of FASTQ data with Galaxy, Bioinformatics 26 (14) (2010) 1783–1785. [32] Liu Y. Peng H. Wong L. Li J. High-speed and high-ratio referential genome compression Bioinformatics 33 21 2017 3364 3372 Y. Liu, H. Peng, L. Wong, J. Li, High-speed and high-ratio referential genome compression, Bioinformatics 33 (21) (2017) 3364–3372. [33] Ochoa I. Hernaez M. Weissman T. iDoComp: a compression scheme for assembled genomes Bioinformatics 31 5 2014 626 633 I. Ochoa, M. Hernaez, T. Weissman, iDoComp: a compression scheme for assembled genomes, Bioinformatics 31 (5) (2014) 626–633. [34] Deorowicz S. Danek A. Niemiec M. GDC 2: Compression of large collections of genomes Sci Rep 5 2015 11565 S. Deorowicz, A. Danek, M. Niemiec, GDC 2: Compression of large collections of genomes, Scientific Reports 5 (2015) 11565. [35] Hernaez M, Pavlichin D, Weissman T, Ochoa I. Genomic data compression. Annu Rev Biomed Data Sci 2. [36] Pratas D. Hosseini M. Pinho A.J. Compression of amino acid sequences International conference on practical applications of computational biology & bioinformatics 2018 Springer 105 113 D. Pratas, M. Hosseini, A. J. Pinho, Compression of amino acid sequences, in: International Conference on Practical Applications of Computational Biology & Bioinformatics, Springer, 2018, 105–113. [37] Pratas D. Pinho A.J. Rodrigues J.M. XS: a FASTQ read simulator BMC Res Notes 7 1 2014 40 D. Pratas, A. J. Pinho, J. M. Rodrigues, XS: a FASTQ read simulator, BMC Research Notes 7 (1) (2014) 40. [38] Escalona M. Rocha S. Posada D. A comparison of tools for the simulation of genomic next-generation sequencing data Nature Rev Genet 17 8 2016 459 M. Escalona, S. Rocha, D. Posada, A comparison of tools for the simulation of genomic next-generation sequencing data, Nature Reviews Genetics 17 (8) (2016) 459. "
    },
    {
        "doc_title": "Social Media Mining for Postpartum Depression Prediction",
        "doc_scopus_id": "85086928624",
        "doc_doi": "10.3233/SHTI200457",
        "doc_eid": "2-s2.0-85086928624",
        "doc_date": "2020-06-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Artificial intelligence methods",
            "Digital datas",
            "Large amounts",
            "Postpartum depressions",
            "Social media",
            "Social media networks",
            "Data Mining",
            "Depression, Postpartum",
            "Female",
            "Humans",
            "Machine Learning",
            "Social Media",
            "Social Support"
        ],
        "doc_abstract": "© 2020 European Federation for Medical Informatics (EFMI) and IOS Press.This study investigated the feasibility of a postpartum depression predictor based on social media writings. The current broad use of social media networks generates a large amount of digital data, which, when coupled with artificial intelligence methods, have the potential to disclose significant health related insights. In this paper we explore the use of machine learning for prediction of postpartum depression on a corpus created from Reddit posts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A recommender system based on cohorts' similarity",
        "doc_scopus_id": "85086886260",
        "doc_doi": "10.3233/SHTI200353",
        "doc_eid": "2-s2.0-85086886260",
        "doc_date": "2020-06-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Alzheimer's disease",
            "Clinical trial",
            "Context based retrieval",
            "Scientific literature",
            "Scientific studies",
            "Statistical power",
            "Algorithms",
            "Alzheimer Disease",
            "Humans"
        ],
        "doc_abstract": "© 2020 European Federation for Medical Informatics (EFMI) and IOS Press.Aiming to better understand the genetic and environmental associations of Alzheimer's disease, many clinical trials and scientific studies have been conducted. However, these studies are often based on a small number of participants. To address this limitation, there is an increasing demand of multi-cohorts studies, which can provide higher statistical power and clinical evidence. However, this data integration implies dealing with the diversity of cohorts structures and the wide variability of concepts. Moreover, discovering similar cohorts to extend a running study is typically a demanding task. In this paper, we present a recommendation system to allow finding similar cohorts based on profile interests. The method uses collaborative filtering mixed with context-based retrieval techniques to find relevant cohorts on scientific literature about Alzheimer's diseases. The method was validated in a set of 62 cohorts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UA.PT Bioinformatics at ImageCLEF 2020: Lifelog Moment Retrieval Web based Tool",
        "doc_scopus_id": "85121844835",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85121844835",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic approaches",
            "Engineering informatics",
            "ImageCLEF",
            "Images processing",
            "Interactive approach",
            "Life log",
            "Moment retrieval",
            "WEB application",
            "Web applications",
            "Web-based tools"
        ],
        "doc_abstract": "Copyright © 2020 for this paper by its authors.This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the ImageCLEF lifelog task, more specifically in the Lifelog Moment Retrieval (LMRT) sub-task. In our first participation last year we tackled the LMRT challenge with an automatic approach. Following the same steps, we improved our results, while introducing a new interactive approach. For the automatic approach, two submissions were made. We started by processing all images in the lifelog dataset using object detection and scene recognition algorithms. Afterwards, we processed the query topics with Natural Language Processing (NLP) algorithms in order to extract relevant words related to the desired moment. Finally, we compared the visual concepts of the image with the textual concepts of the query topic with the goal of computing a confidence score that relates the image to the topic. For the interactive approach, we developed a web application in order to visualize and provide an interactive tool to the users. The application is divided in three stages. In the first one, the user uploads the images from the dataset, as well the textual data annotations. In the second stage, the user interacts with the application assigning the extracted words to the several topics. Consequently, the application retrieves the image associated to the topic with a certain confidence. In the last stage, we provide a visual environment with two different views, in the form of a image gallery or data tables organized into timestamp clusters. Similarly to our previous participation, the results of the automatic approach are still far from being competitive. We conclude that an automatic approach might not be the best solution for the LMRT task since the currently available state-of-the-art technology is still not able to wield better results. However, our interactive approach with relevance feedback obtained better and competitive results, achieving a F1-measure@10 score of 0.52.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BioInfo@UAVR at eRisk 2020: On the use of psycholinguistics features and machine learning for the classification and quantification of mental diseases",
        "doc_scopus_id": "85121809207",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85121809207",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic Detection",
            "Depression",
            "Early detection",
            "Engineering informatics",
            "Mental disease",
            "Online forums",
            "Previous year",
            "Psycholinguistic pattern",
            "Self-harm",
            "Social mining"
        ],
        "doc_abstract": "Copyright © 2020 for this paper by its authors.This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the shared tasks of CLEF eRisk 20201. The eRisk initiative fosters Natural Language Processing research for the automatic detection of risk situations on the internet. Similar to the previous years, the challenge was organized in two tasks, which aimed the early detection of self-harm (T1) and severity of depression (T2) in online forums. We addressed these tasks both from a standard machine learning perspective and from a behavioural point of view. The results we obtained endorse the use of social monitoring as a possible complement to more traditional public health surveillance and intervention practices.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "File Forgery Detection Using a Weighted Rule-Based System",
        "doc_scopus_id": "85092163560",
        "doc_doi": "10.1007/978-3-030-58219-7_8",
        "doc_eid": "2-s2.0-85092163560",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automated systems",
            "Computational solutions",
            "Digital datas",
            "Forgery detections",
            "Robust methods",
            "Security breaches",
            "Weighted rules"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.The society is becoming increasingly dependent on digital data sources. However, our trust on the sources and its contents is only ensured if we can also rely on robust methods that prevent fraudulent forgery. As digital forensic experts are continually dealing with the detection of forged data, new fraudulent approaches are emerging, making it difficult to use automated systems. This security breach is also a good challenge that motivates researchers to explore computational solutions to efficiently address the problem. This paper describes a weighted rule-based system for file forgery detection. The system was developed and validated in the several tasks of ImageCLEFsecurity 2019 track challenge, where promising results were obtained.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SCALEUS-FD: A FAIR Data Tool for Biomedical Applications",
        "doc_scopus_id": "85090817490",
        "doc_doi": "10.1155/2020/3041498",
        "doc_eid": "2-s2.0-85090817490",
        "doc_date": "2020-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [
            "Big Data",
            "Biological Ontologies",
            "Biological Science Disciplines",
            "Computational Biology",
            "Databases, Factual",
            "Humans",
            "Internet",
            "Metadata",
            "Semantic Web",
            "Semantics",
            "Software"
        ],
        "doc_abstract": "© 2020 Arnaldo Pereira et al.The Semantic Web and Linked Data concepts and technologies have empowered the scientific community with solutions to take full advantage of the increasingly available distributed and heterogeneous data in distinct silos. Additionally, FAIR Data principles established guidelines for data to be Findable, Accessible, Interoperable, and Reusable, and they are gaining traction in data stewardship. However, to explore their full potential, we must be able to transform legacy solutions smoothly into the FAIR Data ecosystem. In this paper, we introduce SCALEUS-FD, a FAIR Data extension of a legacy semantic web tool successfully used for data integration and semantic annotation and enrichment. The core functionalities of the solution follow the Semantic Web and Linked Data principles, offering a FAIR REST API for machine-to-machine operations. We applied a set of metrics to evaluate its \"FAIRness\"and created an application scenario in the rare diseases domain.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a More Reproducible Biomedical Research Environment: Endorsement and Adoption of the FAIR Principles",
        "doc_scopus_id": "85085047024",
        "doc_doi": "10.1007/978-3-030-46970-2_22",
        "doc_eid": "2-s2.0-85085047024",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Biomedical data",
            "Biomedical research",
            "Data management techniques",
            "Data quality",
            "Digital transformation",
            "Guiding principles",
            "Life science research",
            "Scientific data management"
        ],
        "doc_abstract": "© 2020, Springer Nature Switzerland AG.The FAIR guiding Principles for scientific data management and stewardship are a fundamental enabler for digital transformation and transparent research. They were designed with the purpose of improving data quality, by making it Findable, Accessible, Interoperable and Reusable. While these principles have been endorsed by both data owners and regulators as key data management techniques, their translation into practice in quite novel. The recent publication of FAIR metrics that allow for the evaluation of the degree of FAIRness of a data source, platform or system is a further booster towards their adoption and practical implementation. We present in this paper an overview of the adoption and impact of the FAIR principles in the area of biomedical and life-science research. Moreover, we consider the use case of biomedical data discovery platforms and assess the degree of FAIR compatibility of three such platforms. This assessment is guided by the FAIR metrics.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Understanding depression from psycholinguistic patterns in social media texts",
        "doc_scopus_id": "85084177429",
        "doc_doi": "10.1007/978-3-030-45442-5_50",
        "doc_eid": "2-s2.0-85084177429",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic analysis",
            "Mental health",
            "Mental illness",
            "Public forums",
            "Rule based",
            "Social media",
            "Standard machines",
            "World Health Organization"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.The World Health Organization reports that half of all mental illnesses begin by the age of 14. Most of these cases go undetected and untreated. The expanding use of social media has the potential to leverage the early identification of mental health diseases. As data gathered via social media are already digital, they have the ability to power up faster automatic analysis. In this article we evaluate the impact that psycholinguistic patterns can have on a standard machine learning approach for classifying depressed users based on their writings in an online public forum. We combine psycholinguistic features in a rule-based estimator and we evaluate their impact on this classification problem, along with three other standard classifiers. Our results on the Reddit Self-reported Depression Diagnosis dataset outperform some previously reported works on the same dataset. They stand for the importance of extracting psychologically motivated features when processing social media texts with the purpose of studying mental health.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhancing decision-making systems with relevant patient information by leveraging clinical notes",
        "doc_scopus_id": "85083738156",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85083738156",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Clinical decision support systems",
            "Clinical treatments",
            "Decision-making systems",
            "Electronic health record systems",
            "Extracting information",
            "Patient information",
            "Patient trajectories",
            "Real time evaluation"
        ],
        "doc_abstract": "© 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Hospitalised patients suffering from secondary illnesses that require daily medication typically need personalised treatment. Although clinical guidelines were designed considering those circumstances, existing decision-support features fail in assimilating detailed relevant patient information, which opens up opportunities for systems capable of performing a real-time evaluation of such data against existing knowledge and providing recommendations during clinical treatments. In this paper, we present a proposal for a new feature to integrate with electronic health record (EHR) systems that enriches the health treatment process by automatically extracting information from patient medical notes and aggregating it in clinical protocols. Our goal is to leverage the historical component of the patient trajectory to improve clinical decision support systems performance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A computational platform for heart failure cases research",
        "doc_scopus_id": "85083723599",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85083723599",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Computational platforms",
            "Computational system",
            "Different stages",
            "Efficient treatment",
            "Ejection fraction",
            "Genomic information",
            "Health informations",
            "Therapeutic targets"
        ],
        "doc_abstract": "© 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Heart failure is a global health issue that affects millions of people worldwide, and is the main cause of disability and hospitalisation of elderly people. Approximately half of these have heart failure with preserved ejection fraction (HFpEF) and this proportion is increasing as the population ages. There is still no efficient treatment for HFpEF and today's existing therapies only aim at relieving symptoms. With the aim to unravelling the pathophysiology of HFpEF and identify new therapeutic targets, ongoing long-time studies are collecting patient's data, including the genomic information. This procedure is complex and requires electronically-stored health information to keep the patient's information centralised to simplify the following up. In this paper, we present an computational system to support researchers in the different stages of a clinical study, and we describe its use in the management and analyse of HFpEF cohorts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Image selection based on low level properties for lifelog moment retrieval",
        "doc_scopus_id": "85081171286",
        "doc_doi": "10.1117/12.2557073",
        "doc_eid": "2-s2.0-85081171286",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Computational challenges",
            "Computational time",
            "Image selection",
            "Learning curves",
            "Life log",
            "Low-level features",
            "Low-level image features",
            "Wearable devices"
        ],
        "doc_abstract": "© 2020 SPIE.The increasing number of mobile and wearable devices is dramatically changing the way we collect data about person's life. These devices allow recording our daily activities and behavior in several forms, e.g., text, images, bio-signals, or video. However, many times, the collected data includes low quality or irrelevant contents, feeding lifelogging applications with huge amounts of data, and creating computational challenges for patterns' identification. In this paper, we propose a fast image analysis approach to automatically select relevant images from lifelog data. Using images intrinsic information, such as scenes and objects, we have manually curated two datasets, one with relevant content and another with non-relevant information. Then, we applied supervised learning algorithms based on low-level image features, namely blur and focus, to find the binary model that best discriminates between the two classes. The binary models were then compared based on learning curves and f1-scores, achieving a 95.4% of f1-score for the best one. By reducing the amount of images in the lifelog data, we were able to save computational time without losing images with relevant content.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "TASKA: A modular task management system to support health research studies",
        "doc_scopus_id": "85068543763",
        "doc_doi": "10.1186/s12911-019-0844-6",
        "doc_eid": "2-s2.0-85068543763",
        "doc_date": "2019-07-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Policy",
                "area_abbreviation": "MEDI",
                "area_code": "2719"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Databases, Factual",
            "Health Services Research",
            "Humans",
            "Software",
            "Task Performance and Analysis",
            "User-Computer Interface",
            "Workflow"
        ],
        "doc_abstract": "© 2019 The Author(s).Background: Many healthcare databases have been routinely collected over the past decades, to support clinical practice and administrative services. However, their secondary use for research is often hindered by restricted governance rules. Furthermore, health research studies typically involve many participants with complementary roles and responsibilities which require proper process management. Results: From a wide set of requirements collected from European clinical studies, we developed TASKA, a task/workflow management system that helps to cope with the socio-technical issues arising when dealing with multidisciplinary and multi-setting clinical studies. The system is based on a two-layered architecture: 1) the backend engine, which follows a micro-kernel pattern, for extensibility, and RESTful web services, for decoupling from the web clients; 2) and the client, entirely developed in ReactJS, allowing the construction and management of studies through a graphical interface. TASKA is a GNU GPL open source project, accessible at https://github.com/bioinformatics-ua/taska. A demo version is also available at https://bioinformatics.ua.pt/taska. Conclusions: The system is currently used to support feasibility studies across several institutions and countries, in the context of the European Medical Information Framework (EMIF) project. The tool was shown to simplify the set-up of health studies, the management of participants and their roles, as well as the overall governance process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GenericCDSS - A generic clinical decision support system",
        "doc_scopus_id": "85070981385",
        "doc_doi": "10.1109/CBMS.2019.00046",
        "doc_eid": "2-s2.0-85070981385",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Chronic disease",
            "Clinical decision support systems",
            "Clinical protocols",
            "Decision supports",
            "Electronic health record",
            "Medical diagnostics",
            "Open-source solutions",
            "Third-party tools"
        ],
        "doc_abstract": "© 2019 IEEE.Clinical decision support systems (CDSS) are currently essential tools to guide medical diagnostics and patients' treatments, and they are specially important for the better care management of chronic diseases, such as cancer and diabetes. These systems help to decide on the best treatment solution, namely in centres where there is a shortage of medical experts. CDSS tools are often integrated into the Electronic Health Record (EHR) to facilitate the reuse of patient data. However, many times, creating new and intuitive protocols that are disease-specific is still a challenge. In this paper we present an open source solution (GenericCDSS) that can be used to streamline the development of autonomous CDSS, avoiding the dependency on third-party tools to manage patient data and clinical protocols. The software tool provides a modern user interface, supporting multi-platforms such as mobile and desktop devices. GenericCDSS is publicly available at https://github.com/bioinformatics-ua/GenericCDSS, under a GNU GPL license.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "EMIF Catalogue: A collaborative platform for sharing and reusing biomedical data",
        "doc_scopus_id": "85063028413",
        "doc_doi": "10.1016/j.ijmedinf.2019.02.006",
        "doc_eid": "2-s2.0-85063028413",
        "doc_date": "2019-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Biomedical data integration",
            "Data catalogue",
            "Data discovery",
            "Data reuse",
            "Data Sharing",
            "Research studies",
            "Biomedical Research",
            "Cooperative Behavior",
            "Database Management Systems",
            "Humans",
            "Information Dissemination",
            "Knowledge Management",
            "Publishing"
        ],
        "doc_abstract": "© 2019Objective: The collaboration and knowledge exchange between researchers are often hindered by the nonexistence of accurate information about which databases may support research studies. Even though a considerable amount of patient health information does exist, it is usually distributed and hidden in many institutions. The goal of this project is to provide, for any research community, a holistic view of biomedical datasets of interests, from which researchers can explore several distinct levels of granularity. Methods: We developed a community-centered approach to facilitate data sharing while ensuring privacy. A dynamic schema allows exposing any metadata model about existing repositories. The framework was developed following a modular plugin-based architecture that facilitates the integration of internal and external tools. Results: The EMIF Catalogue, a web platform for sharing and reusing biomedical data. Through this system, data custodians can publish and share different levels of information, while the researchers can search for databases that fulfill research requirements. Conclusions: The EMIF Catalogue currently fosters several distinct research communities, with different levels of data governance, combining, for instance, data available in pan-European EHR and Alzheimer cohorts. This portal is publicly available at https://emif-catalogue.eu.",
        "available": true,
        "clean_text": "serial JL 271161 291210 291773 291870 291901 291919 31 International Journal of Medical Informatics INTERNATIONALJOURNALMEDICALINFORMATICS 2019-03-13 2019-03-13 2019-03-19 2019-03-19 2019-04-24T06:55:57 S1386-5056(18)30830-X S138650561830830X 10.1016/j.ijmedinf.2019.02.006 S300 S300.1 FULL-TEXT 2020-05-07T23:09:44.308075Z 0 0 20190601 20190630 2019 2019-03-13T16:30:55.238121Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor primabst ref specialabst 1386-5056 13865056 true 126 126 C Volume 126 9 35 45 35 45 201906 June 2019 2019-06-01 2019-06-30 2019 Regular Papers article fla © 2019 Elsevier B.V. All rights reserved. EMIFCATALOGUEACOLLABORATIVEPLATFORMFORSHARINGREUSINGBIOMEDICALDATA OLIVEIRA J 1 Introduction 2 Background 3 The European Medical Infrastructure Framework Project 4 The EMIF Catalogue 4.1 Building a community 4.2 Describing entities 4.3 Searching for data/Discovering data 4.4 Exploring analytical tools 4.5 Conducting studies - reusing data 4.6 Software technologies 5 Discussion 6 Conclusions Summary Points Acknowledgements References GEISSBUHLER 2013 1 9 A LOUKIDES 2014 46 61 G WANG 2014 42 57 F SAVONNET 2016 1640 1649 M BIANCHI 2009 4654 4657 S INENGINEERINGINMEDICINEBIOLOGYSOCIETY2009 BIOMEDICALDATAINTEGRATIONCAPTURINGSIMILARITIESWHILEPRESERVINGDISPARITIES WEISKOPF 2013 144 151 N GOBLE 2008 687 693 C DANCIU 2014 28 35 I PHAN 2012 74 87 J JOHNSON 2016 69 88 S PEREZRIVEROL 2017 406 Y MURPHY 2006 1040 S INAMIAANNUALSYMPOSIUMPROCEEDINGSVOLUME2006 INTEGRATIONCLINICALGENETICDATAINI2B2ARCHITECTURE BALL 2016 265 268 R HCUPDATABASES 2014 AGENCYFORHEALTHCARERESEARCHQUALITY CIMINO 2014 11 27 J MCMURRY 2015 J REPORTSCALABILITYSEMANTICWEBINTEGRATIONINBIOMEDBRIDGES HARRIS 2009 377 381 P HORVATH 2011 266 276 M CHEN 2018 X SILVA 2018 L MONTRAAGILEARCHITECTUREFORDATAPUBLISHINGDISCOVERY OLIVEIRA 2018 465 472 O IN11THINTERNATIONALCONFERENCEHEALTHINFORMATICS AMETHODOLOGYPERFORMSEMIAUTOMATICDISTRIBUTEDEHRDATABASEQUERIES LIYANAGE 2017 8 12 H VISSER 2015 P120 P121 P LIYANAGE 2018 61 65 H TRIFAN 2018 465 472 A OLIVEIRAX2019X35 OLIVEIRAX2019X35X45 OLIVEIRAX2019X35XJ OLIVEIRAX2019X35X45XJ 2020-03-19T00:00:00.000Z 2020-03-19T00:00:00.000Z © 2019 Elsevier B.V. All rights reserved. item S1386-5056(18)30830-X S138650561830830X 10.1016/j.ijmedinf.2019.02.006 271161 2020-05-07T23:09:44.308075Z 2019-06-01 2019-06-30 true 7300185 MAIN 11 52249 849 656 IMAGE-WEB-PDF 1 ga1 true 7181 115 219 gr1 7136 122 219 gr10 5385 79 219 gr11 9955 135 219 gr12 5039 56 219 gr13 4554 41 219 gr14 7013 105 219 gr15 3781 39 219 gr2 6988 116 219 gr3 7139 80 219 gr4 11245 130 219 gr5 5494 72 219 gr6 6235 163 158 gr7 10122 102 219 gr8 9832 104 219 gr9 8193 110 219 ga1 true 18246 200 380 gr1 24971 262 470 gr10 23296 245 678 gr11 52542 418 678 gr12 28034 182 715 gr13 27963 135 715 gr14 32250 326 678 gr15 28667 118 659 gr2 24503 249 470 gr3 22517 172 470 gr4 56899 402 678 gr5 18687 155 470 gr6 43333 486 470 gr7 64508 317 678 gr8 48167 323 678 gr9 35146 341 678 ga1 true 125171 886 1682 gr1 185736 1160 2083 gr10 197523 1083 3000 gr11 487526 1848 3000 gr12 249937 806 3167 gr13 236532 597 3167 gr14 275010 1441 3000 gr15 202052 521 2917 gr2 176506 1104 2083 gr3 182790 764 2083 gr4 434086 1777 3000 gr5 124269 688 2083 gr6 312252 2152 2083 gr7 564656 1402 3000 gr8 407714 1431 3000 gr9 315137 1507 3000 am 3398675 IJB 3812 S1386-5056(18)30830-X 10.1016/j.ijmedinf.2019.02.006 Elsevier B.V. Fig. 1 A typical search among scattered and heterogeneous data sources. Often clinical researchers have to identify and possibly query these data sources in an isolated manner. Moreover, some data sources might not even be discovered, depending on the search terms criteria and governance rules. Fig. 1 Fig. 2 Conceptual representation of the EMIF Catalogue - a marketplace where data custodians can share their data with different granularity levels; clinical researchers can retrieve and reuse them from an unique entry point. Fig. 2 Fig. 3 A multi-level model for biomedical data exposure. The example depicts a community of databases, exposing homogeneous data, following three distinct paths: a) a general view over metadada; b) an aggregate view, over summary-level data; c) a deeper view, over raw data. Fig. 3 Fig. 4 Landing page of the EMIF Catalogue exposing some of the on-going hosted projects. Fig. 4 Fig. 5 Communities expose in a harmonized manner data coming from scattered data sources. These data sources are characterized based on the same fingerprint template. Fig. 5 Fig. 6 Database fingerprinting - data coming from different health units can be defined and stored based on different data models or structures. A data fingerprint is a collection of meta-data that can describe, in the same way, the information that exists in each of the initially different data entities. Fig. 6 Fig. 7 An example of rules to be followed when defining a schema template spreadsheet. These instructions cover the type of the information - whether it is a question, a category (question set) or a comment - and the type of visual field or input to which it will convert once uploaded in the system. Fig. 7 Fig. 8 Database listing for the EMIF Alzheimer Disease Community. Databases are listed in a tabular form, in which the name of the columns are customizable. On this dashboard, the number of columns, as well as the information displayed, can be any of the fields that characterize the database. Databases can be public, or in draft mode. Drafts are only visible for the data custodian that creates them and for the community manager. Settings in the community allow the process of making a database public to be moderated by the community manager. Fig. 8 Fig. 9 An example of an advanced search boolean query. In this example, an AND boolean query has been constructed, in which the search criteria combines the acronym and the institution name of a database. Fig. 9 Fig. 10 Database comparison within the EMIF Catalogue. The result of the comparison displays in green the information that is common to the databases and in red the information that is distinct. Fig. 10 Fig. 11 Custom View Feature depicting database characteristics and objectives of three different databases belonging to the same community. Fig. 11 Fig. 12 Achilles plugin integrated in the EMIF Catalogue. Several dashboards can be chosen for distinct data visualization. Fig. 12 Fig. 13 EMIF Catalogue integrated external tools. Each of these tools supports different steps of a research study conducted through the EMIF Catalogue. Fig. 13 Fig. 14 Study Request plugin. A researcher can fill in and send a research question regarding one or multiple databases that are present in the Catalogue. This form includes a window chat that supports file uploads. Fig. 14 Fig. 15 The development cycle and continuous growth of the EMIF Catalogue. Each software release of the EMIF Catalogue starts with a development period, followed by thorough tests conducted internally, by a team of 5 to 10 users, both regular and unfamiliarized with the Catalogue. After the release of a new version, feedback from real users of the system is taken into account in the development step. Each new version of the Catalogue leads to an increase in its usage. Fig. 15 EMIF Catalogue: A collaborative platform for sharing and reusing biomedical data José Luís Oliveira a Alina Trifan a Luís A. Bastião Silva b a University of Aveiro, DETI/IEETA, Portugal University of Aveiro, DETI/IEETA Portugal b BMD Software, Aveiro, Portugal BMD Software Aveiro Portugal Graphical abstract Objective The collaboration and knowledge exchange between researchers are often hindered by the nonexistence of accurate information about which databases may support research studies. Even though a considerable amount of patient health information does exist, it is usually distributed and hidden in many institutions. The goal of this project is to provide, for any research community, a holistic view of biomedical datasets of interests, from which researchers can explore several distinct levels of granularity. Methods We developed a community-centered approach to facilitate data sharing while ensuring privacy. A dynamic schema allows exposing any metadata model about existing repositories. The framework was developed following a modular plugin-based architecture that facilitates the integration of internal and external tools. Results The EMIF Catalogue, a web platform for sharing and reusing biomedical data. Through this system, data custodians can publish and share different levels of information, while the researchers can search for databases that fulfill research requirements. Conclusions The EMIF Catalogue currently fosters several distinct research communities, with different levels of data governance, combining, for instance, data available in pan-European EHR and Alzheimer cohorts. This portal is publicly available at Keywords Biomedical data integration Data discovery Data sharing Data catalogue Research study Data reuse 1 Introduction The continuous digitalization of health care services, together with the increasing diversity of personal biomedical data, have led to an exponential increase in the volume of clinical and disease-specific data that is being gathered daily throughout the world [1]. Consequently, the bottleneck of biomedical research is shifting from data generation to data management and analysis. Disseminating these large, complex data has the potential to push forward clinical research advancements and ultimately improve public health, through the secondary reuse of multiple health care databases [2]. However, the fragmentation of data in terms of geographical location, terminology and consistency limits the efficiency of tracking-down and reusing biomedical records [3]. Another issue is the reduced consistency across databases, i.e. they typically rely on different languages, measurement types, coding schema [4]. In addition, relationships among data items are often defined implicitly, in some supplementary documentation or as experts’ tacit knowledge [5]. To reuse these data for clinical research, several methods for quality assessment and data harmonization need to be adopted [6,7]. As a result, it seems that sometimes it is cheaper, easier or more desirable to create a database afresh rather than adapt it to existing resources [8]. All the aforementioned challenges, which are still in the process of being solved, contribute to the difficulties clinical researchers face when trying to identify datasets or candidate subjects for a specific study. We propose in this paper a solution that addresses them, with the ultimate goal of enabling data discovery and reuse: the EMIF Catalogue 1 1 . The aim of the platform is to take full advantage of existing biomedical data, facilitate knowledge discovery, and make scientific discoveries more productive and reproducible. Data can be shared and exposed with different levels of details, from metadata to summarized views. User profiles guarantee that data owners share as much as they want either with any user or with specific ones. Moreover, patient data can be made available in a controlled, remote research environment upon requests that can be made directly from the catalogue. The platform assists users not only in discovering biomedical databases of interests, but more so in assessing their suitability for a specific clinical or research study and the feasibility of the study. The EMIF Catalogue is constantly evolving with the final goal of building a data network to enforce data interoperability and cross-institution reproducibility of clinical and research studies. This paper is divided into five more sections. Section 2 presents an outline of the most recent developments related to biomedical data discovery and its reuse, and highlights the motivation behind this work. We overview the European Medical Infrastructure Framework initiative in Section 3, while Section 4 focuses on the presentation of the EMIF Catalogue. Finally, Section 5 discusses and assesses the impact of our work within the research community and Section 6 concludes the paper. 2 Background Clinical and biomedical information is by nature complex and the exponential growth of gathered biomedical data only contributes to this complexity. The opportunities for the reuse of these data in biomedical research are endless: from the discovery of appropriate subjects in clinical studies, to the reduction of health care costs and procedural errors, and ultimately overall improved and cheaper health care practices [9]. The secondary reuse of citizens’ health data and investigation of the real evidence of therapeutics may lead to the achievement of personalized, predictive and preventive medicine [10]. Currently, researchers need to discover individual repositories and conduct isolated searches among them (Fig. 1 ). These operations are time-consuming and, most importantly, often the user is not able to retrieve the necessary information. A successfully integrated search engine may provide a one-stop shop, where data seekers can quickly access all these resources, improving the community's capacity to utilize existing databases for data query, knowledge dissemination and integrative analysis. Joining together multi-country and multi-center data sources can lead to considerable gains, providing easier access to innovative medicines and improved health outcomes [11]. The field of genomics was a primary driving force behind the generation of what we call today big data in bioinformatics and its consequent harmonization and integration. Among the initiatives that focus on integrating omics data, the Omics Discovery Index [12] aggregates and indexes 90 729 datasets from 15 repositories. The combination of clinical and genomics information of the i2b2 platform [13] permits correlating different data sources and facilitates the design of therapies for patients with genetic diseases. By mining clinical data for cohort discovery and hypothesis generation, i2b2 aims to translate the discoveries of the genomic era into more effective and personalized health care. Among industries, the pharmaceutical industry manages huge repositories of clinical and molecular data. These data are fundamental for drug discovery and require careful assembly, overlay and comparison from various sources. Open PHACTS 2 2 is an initiative aimed at reducing barriers in drug discovery in industry, academia and small businesses. Open PHACTS integrates pharmaceutical data from multiple publicly-available databases into a linked data resource, and provides tools and services to interact with that data. Another US initiative, the Sentinel system, was designed to monitor the safety of FDA-regulated medical products [14]. For this purpose, it uses pre-existing electronic health care data from multiple sources. In population health, disease registries and data from clinical records and studies are being used to measure the impact of health interventions, as well as providing researchers with a complete picture of the expression of a given disease. The RD-Connect platform 3 3 is an integrated platform connecting databases, registries, biobanks and clinical bioinformatics for rare disease research [15]. It supports the development of a suite of clinical bioinformatics tools, including data mining and knowledge discovery tools for analysis and integration of molecular and clinical data to discover new disease genes, pathways and therapeutic targets. The global movement toward increased secondary use of care data opens up new perspectives for researchers and clinicians, and raises new technical and ethical questions. SHARPn, a project designed to explore the secondary use of Electronic Health Records (EHR), receives data in several formats, and generates structured and normalized data, allowing data access through phenotype specifications. The Healthcare Cost and Utilization Project (HCUP) is a family of health care databases and related software tools and products developed through a Federal State industry partnership and sponsored by the Agency for Healthcare Research and Quality [16]. HCUP includes the largest collection of longitudinal hospital care data in the United States. Researchers and policy-makers use HCUP data to identify, track, analyze and compare hospital statistics at the national, regional and state levels. Still in the US, the US National Institutes of Health developed BTRIS [17] with the aim of improving current methods for data access and use. The system accommodates any type of data, including hospital departmental systems and central EHRs and it is optimized for the most likely queries to be performed. The user interface allows users to access data in an independent, “self-service” way. In Europe, BioMedBridges [18] key rational is the establishment of dedicated interactions amongst existing ESFRI (European Strategy Forum on Research Infrastructures) infrastructures. Hence, it aims to create an e-infrastructure to allow interoperability between data and services in the biological, medical, translational and clinical domains and thus strengthen biomedical resources in Europe. Similarly, several other software solutions have been developed with the aim of helping scientists gather, annotate and process health data, in more efficient ways. One such example is REDCap (Research Electronic Data Capture), a web-based system designed to allow researchers to define project-specific data capture and launch protocol data collection [19]. The software supports exporting the data to several formats so that it can be analyzed by any common statistics tool. This is a popular solution in the field, despite not being widely open to users and accessible only on request. Similar to RedCap, SciPort is a web-based biomedical data sharing platform that supports data sharing across distributed organizations [3]. Data can be organized into communities, supported by a generic and customizable meta-data model. To enable data sharing, SciPort provides a central server-based data sharing architecture with one-click data sharing from a local server. Semantic consistency is established by the use of semantic tagging through controlled vocabularies. DEDUCE Guided Query Tool [20] is another web-based platform that supports cohort identification and data extraction. The project uses some business intelligence features to allow investigators to access administrative, financial and clinical information generated during patient care, without needing to understand complex query language or the underlying data models. DataMed, an initiative developed through the National Institutes of Health funded biomedical and healthCAre Data Discovery Index Ecosystem (bioCADDIE) consortium [21], integrates biomedical datasets of interest from heterogeneous sources and makes them searchable through a web-based interface. These initiatives allow collaborative research and represent a joint market place for health care and clinical data. They share a common set of goals and through this evaluation we assessed the lessons learned, challenges encountered and potential future collaborations. By summarizing these state-of-the-art approaches, we identified research gaps that led to the definition of our research goal. These solutions represent isolated software tools or projects designed to support specific research, with a clear purpose, but there are still missing platforms to solve challenges such as the creation of multiple-institution research environments, and the promotion of publications and subscription of specific studies or datasets. In the EMIF ecosystem, not only can the EMIF partners take advantage of this framework, but also new projects can join existing communities or create their own. Its underlying aim is to support scientific biomedical research projects. 3 The European Medical Infrastructure Framework Project The EMIF initiative 4 4 www.emif.eu emerged as an effort to address the aforementioned challenges, by focusing on creating a European Medical Information Framework through which large amounts of European biomedical data can be integrated, exploited and reused with the final goal of providing overall better healthcare (Fig. 2 ). The EMIF Catalogue was designed as a portal for all EMIF provided functionalities and incorporates a web interface that links to other relevant components such as a federated query module and a workflow manager, among others. Its main purpose is to facilitate biomedical data exposure and sharing, on one hand, and data discovery and reuse, on the other. This translated into a one-stop platform, which we denominate as a biomedical marketplace, where data custodians can share their data and researchers can easily discover these data to support their clinical studies. At the same time, our vision targeted a fine-grained access control to these biomedical data. For that, we developed a methodology that tackles data privacy from a different perspective to the one exposed by the literature review. In our approach, real data sources are characterized based on a schema that can be easily defined by any data custodian. Multiple data sources that have the same focus can be clustered into an entity, generically denominated as a community. Harmonization within a community arises with the use of the same schema for meta-data characterization of all databases that form it. A user with a specific research question can either identify a community of interest and further explore its databases, or request a new one for this purpose. In this environment, the data custodian controls right upfront in how much detail s/he wants to expose a data source. Several levels of information can be exposed, from summarized views to raw data, accessible in a controlled and possibly remote environment. The access to raw data is possible upon assessing the suitability and the feasibility of a study based on the metadata and summarized views. A secure research environment can be set up with the patient data, upon agreement between data owners and researchers. Moreover, a role-based access control (RBAC) assures that an access policy can be tailored to combine the access constraints with the needs of biomedical researchers. By conjugating these two orthogonal perspectives, we designed an approach to expose biomedical data sources, at different levels of detail, while preserving data privacy (Fig. 3 ). The lessons learned form the literature review have led to identification of the following functional requirements for the development of a usable data discovery and biomedical research portal: • Data granularity - summarized, aggregated and possibly raw data should be available, under different circumstances and underlying research protocols. • Data privacy - no private data should be exposed, at any of the granularity levels supported. • Flexibility - the framework should include a number of tools and user environments catering for different user levels. • Data diversity - different data sources should be integrated, independently of the data model and physical support. • Usefulness - data put forward by the catalogue should be filterable and extractable. • Usability - in brief, the platform should be usable and intuitive for any user category. 4 The EMIF Catalogue EMIF Catalogue is an online platform designed as a marketplace, in which the biomedical database is the main entity that is characterized, typically EHRs or cohort data. Currently, the EMIF Catalogue supports several distinct projects, combining, for instance, data available in pan-European EHR and Alzheimer cohorts (Fig. 4 ). 4.1 Building a community A collection of databases is denominated as a community within the platform that we present. More specifically, a community is a group of databases that share common features, such as the type of data exposed or their purpose. Access to a community is moderated by the community manager and several community types are supported: open - anyone can join, public - visible to any user and with access granted by request and by invitation only. Within a community, data custodians use the same template for extracting and exposing meta-data about their databases. Each of the rectangles in Fig. 4 represents a community hosted in the EMIF Catalogue. Through a dynamic characterization schema, which we denominate as fingerprint, data custodians are given the opportunity to expose their data in a multi-level visibility approach. While the use of vocabularies or ontologies is not enforced, the integration of data coming from dissimilar sources converges towards conceptual harmonization through the use of the same meta data extraction schema. Different databases, such as EHRs, paper records and cohort data can be exposed in a harmonized manner, without interfering with the underlying data models (Fig. 5 ). Each of these data sources will be characterized by the same fields, which, in a more general way, represent answers to the same questions defined in the fingerprint template. By using a common structure as a fingerprint skeleton, distinct data sources converge to homogeneous exposure of their meta-data. 4.2 Describing entities Data owners can submit their datasets by filling in the fingerprint template, whose structure is designed by the community owner, as simply as filling in a spreadsheet. The fingerprint template consists of question sets regarding the data that each dataset contains. The collection of answers to the questions in each question set provides a general view on the real data, without disclosing any private information contained in it. This approach allows the aggregation and further exposure of meta-data of clinical datasets at a level of detail that is currently not present in any other health care related platform (Fig. 6 ). Flexibility was identified as a requirement of the system and has been put into practice in the way that a fingerprint can be easily extended and updated, using a vast number of standardized fields that can be combined in multiple ways. Each community can implement its own governance rules to which each data custodian will have to agree before sharing any data. Additionally, each public release of a fingerprint can be accompanied by a disclaimer, personalized for the database in question. In the current use of the EMIF Catalogue, community managers - which are usually designated among owners of databases within a community - define a common set of either questions, fields or measurements that define the data within that community. These definitions are created in the form of a spreadsheet, following some simple definition rules. An example of such rules is shown in Fig. 7 . The spreadsheet is uploaded onto the system and the result for the owner end-user is the translation of these definitions into buttons, input fields, publication items and location selects, among others. This eases the process of gathering and exposing meta-data of a given database. The view inside the community is rendered in Figure 8 , with an example of a community that clusters Alzheimer Disease related databases. 4.3 Searching for data/Discovering data EMIF Catalogue yields two distinct search mechanisms. Regular users are able to search over databases, within a community. A simple search will retrieve all databases in which a given search string is identified. To support the search backend, we rely on a Solr 5 5 search engine [22]. A retrieval model is considered for scoring, sorting and improving the quality of results. The following sources of information are integrated: • Text of the questions or categories of the schema template. • Meta-data content. • Medline publications uploaded in the Catalogue and associated to a database. • Other files uploaded in the system and associated to a database. • Attribute-Value Pairs of meta-data uploaded by means of the web-services API. This means that when a search is performed, not only the meta-data is analyzed, but also all its associated information. The same back-office is used to give suggestions in the free text search, a feature supported by another Solr core. The advanced search, depicted in Fig. 9 extends the simple search by providing the possibility of building boolean queries, based on the fields included in the database fingerprint. Advanced Search allows filtering the search for specific fields. For instance, databases can be queried for “a specific location, with a specific codification about pediatrics”. Two Boolean operators are supported, AND and OR, along with a nesting feature that allows tree-like queries. In the Advanced Boolean Search concepts can be dragged within the search area, combined with the two boolean operators or collapsed within other concepts. The Advance Boolean Search is an important tool for the discovery of databases of interest, as it allows a higher granularity and refinement of the search criteria. Moreover, the user has the possibility to save queries within its profile and reuse them or eventually refine them at a later time. Considering that the majority of users are also database submitters, researchers must be able to find databases similar to their own. This led to the definition of relatedness metrics that can retrieve specific property matches, such as location proximity (Fig. 10 ). This feature allows the comparison of up to 3 items simultaneously, side by side. If a bigger number of items are to be compared, they can be pre-selected and switched in the interface. The reference database can be changed at any time as well. For each of the fields defined in the schema template, the comparison feature will highlight similarities and discrepancies, in different colors, for the databases chosen to be compared. Further refinement of databases of interest can be achieved through the custom views feature. This allows users to select specific questions or complete questionsets for specific databases and returns, in a tabular view, the answers to those questions within the chosen databases (Fig. 11 ). 4.4 Exploring analytical tools The platform supports the integration of third-party components, through which its functionalities can be extended without having to deal with its base code. To accomplish this strategy, a microkernel architecture is in place, to which several components can be dynamically added. These third-party components can be added either at a community level or, ultimately, at the Catalogue level, which means they are available within all the communities. Users that are granted the necessary permissions can develop and live-preview their plugin versions during development. Upon submission to the system, these plugins have to be validated either by the community manager or the system administrator. One such component was integrated in the Catalogue due to the wide adoption of the Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) and the need to extract the aggregated data from these data sources. Achilles 6 6 is a module able to extract population characteristics. Although this tool already allowed visualization of the data, it was mainly focused on the information that is stored in a folder. Hence, we developed several contributions to this open source project to permit data representation from any web endpoint. This module was integrated in the Catalogue, as a database plugin (Fig. 12 )). Among other useful plugins, TASKA 7 7 is a workflow management system where users can create workflow templates which can be reused by distinct groups and for different purposes. TASKA was integrated within the EMIF Catalogue as a need to have an integrated tool to manage all the different tasks that are involved in a research study. Another plugin that was integrated derived from the wide adoption by data custodians of the Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) 8 8 . ATLAS 9 9 is the open-source web application used to conduct scientific analyses on standardized observational data converted to the OMOP CDM. This analytic tool allows the generation and execution of scripts with cohort definitions, which significantly simplifies data custodians’ work when asked to query their databases. CodeMapper 10 10 assists in the creation of code sets from case definitions, for several coding systems simultaneously while keeping a record of the complete mapping process. Fig. 13 shows the integration of these tools within the EMIF Catalogue. A methodology based on the use of TASKA and ATLAS for performing semi-automated queries over geographically distributed biomedical databases [23] is in place within the EMIF Catalogue. It relies on the use of user profiles and user-tailored permissions and it allows data custodians to maintain full control of their data. This approach is directly related to data reuse and is presented with more detail next. 4.5 Conducting studies - reusing data We have already described the pipeline that allows a researcher to identify one or several databases of interest. Considering the scenario of conducting research or a clinical study over the identified database(s), the EMIF Catalogue makes available a Study Request form. This is a plugin at the community level through which regular users of the platform can fill in and submit a research question or even a more advanced study request. The form allows the choice of databases to which the study applies or refers to (Fig. 14 ). Given the practical scenario of a study focused on understanding the natural history of Chronic Venous Disease, a first query aimed at assessing the feasibility steps of the study was submitted to one of the communities of the Catalogue. The submitting researcher was interested in finding counts of patients corresponding to ICD10 or READ codes listed in the request. The mechanism for user roles and permissions that is integrated in the Catalogue forwards the study request to users denominated as Study Managers within the system. Study Managers are in charge of orchestrating the workflow that will lead to an answer to this request. In other words, Study Managers handle the requests and closely collaborate with data custodians that own the database(s) of interest in order to provide an answer to the request. These requests, as well as the interactions between data custodians and study managers, are regularly managed through TASKA, the workflow manager directly accessible from the Catalogue. ATLAS comes in handy for the data custodians involved in the study request, who can locally query their databases and export some results. Finally, the researcher can import these results in the ATLAS installation of the Catalogue and interpret them within the purpose of the study. Alternatively, following given protocols, the researcher can be granted access to the raw data in a remote controlled environment [23]. The private remote research environment provides a solid solution for users to manage sensible data in a secure workspace. It consists of a secure cloud server that can be used to support the several uses case of EMIF up to the project completion. 4.6 Software technologies To address the technical and functional requirements that were identified from the analysis of current challenges in the integration of biomedical data, we created an architecture for a marketplace platform that satisfies the research needs of both data custodians and clinical researchers. Within the EMIF Catalogue, communities of databases uncover meta-data as a first level of data exposure. These meta-data are extracted through a fingerprint schema, as previously detailed and presented to the end user through a user-friendly interface. At a second level, aggregated information about these meta-data can be presented through dashboards that are integrated as external components. At a higher level, patient data can be made available for querying in a controlled remote environment, supported by the platform. Furthermore, each of these steps involved in the discovery and reuse of biomedical data can be managed through a workflow management system directly accessible from the catalogue. The EMIF Catalogue was backed by MONTRA [22]. This framework is an Agile architecture for data publishing and discovery, written in Python 11 11 www.python.org , using Django 12 12 , a framework that encourages rapid development and clean programmatic design. To complement this, a considerable part of the development was made in HTML5 13 13 , CSS 14 14 and JavaScript 15 15 , namely the interface and the end-user interaction. 5 Discussion Numerous solutions for discovering, integrating or querying biomedical data sources have already been presented in the literature. However, each of these solutions only addressed particular aspects of the biomedical data integration and reuse problem. Translational research was still in need of a complete framework, to allow discovery and reuse of any type of biomedical data, while preserving data privacy. The EMIF Catalogue came as an attempt to address that need and its continuous use within the biomedical community validates its impact [24–26]. The catalogue has been successfully used for building a european medical information framework for Alzheimer's disease, as well as for the identification and comparison of child health systems across Europe. Other recent European initiatives concerned with extracting insights from clinical big data have started using the EMIF Catalogue in order to assemble data and facilitate their discovery. As such, BigData@Heart 16 16 an EU public-private consortium consisting of patient networks, academia and pharmaceutical companies works toward building a big data-driven translational research platform. The community fostered in the EMIF Catalogue joins clinical metadata of atrial fibrillation, heart failure and acute coronary syndrome. The PIONEER initiative 17 17 brings together private and public stakeholders, in prostate cancer research and clinical care from across 9 countries. This project is another example of recent initiatives that take advantage of the functionalities of the EMIF Catalogue. The newly created PIONEER community within the catalogue aims at combining and analysing patient records of men diagnosed with prostate cancer. Building the EMIF Catalogue was an interactive process, in which both EMIF partners and regular users have been involved (Fig. 15 ). A repetitive cycle of software development followed by internal testing and validation leads to the deployment of stable versions of the catalogue. These versions are regularly updated either with new features or improvements suggested by the users’ community. Regular users of the platform can become involved in the process by submitting feedback directly through the catalogue, using a form specifically designed for this purpose. In order to take advantage of data and enforce data (re)use and interoperability, the FAIR principles have been followed and put into practice [27]. The number of registered users of the Catalogue has continuously grown, since its public release in January 2017. Until then the Catalogue had been used mostly internally, by the EMIF partners. In January 2017 a stronger dissemination strategy was adopted, which led to growth both in number of accesses and users. The EMIF Catalogue fosters, as of July 2018, nine distinct communities, with different levels of disclosure. Throughout these communities, a total number of 410 databases are shared and explored by more than 850 distinct registered users of the platform. 6 Conclusions This paper presented an integrated, efficient Information Framework for consistent re-use and exploitation of available biomedical data, designed to support new research. The EMIF Catalogue supports data discovery, data evaluation and data (re)use and already integrates a set of tools allowing any researcher not only to identify databases of interest, but also to reuse these data for clinical studies. By developing this platform we intended to address, from a different perspective, some of the challenges faced by biomedical data discovery and interoperability. We created a marketplace to attend to the needs of both data custodians and data researchers, while making sure that no data privacy is disclosed at any time during the sharing and reuse of the data. The EMIF Catalogue is being widely used by several European initiative concerned with extracting knowledge from big volumes of biomedical data, as well as impacting translational research and providing overall better healthcare. Summary Points • With the increasing use of EHR data the bottleneck of biomedical research is shifting from data generation to data management and analysis. • The fragmentation of data in terms of geographical location, terminology and consistency limits the efficiency of tracking-down and reusing biomedical records. • Disseminating these large, complex data has the potential to push forward clinical research advancements and ultimately improve public health, through the secondary reuse of multiple health care databases. • The aim of the EMIF Catalogue is to take full advantage of existing biomedical data, facilitate knowledge discovery, and make scientific discoveries more productive and reproducible. • Data can be shared and exposed with different levels of details, from metadata to summarized views. • User profiles guarantee that data owners share as much as they want either with any user or with specific ones. Moreover, patient data can be made available in a controlled, remote research environment upon requests that can be made directly from the catalogue. • The platform assists users not only in discovering biomedical databases of interests, but more so in assessing their suitability for a specific clinical or research study and the feasibility of the study. Acknowledgements This work has received support from the EU/EFPIA Innovative Medicines Initiative Joint Undertaking (EMIF grant n. 115372) and from the Integrated Programme of SR&TD SOCA (Ref. CENTRO-01-0145-FEDER-000010). References [1] Antoine Geissbuhler Charles Safran I. Buchan Riccardo Bellazzi S. Labkoff K. Eilenberg A. Leese C. Richardson J. Mantas Peter Murray Trustworthy reuse of health data: a transnational perspective Int. J. Med. Inf. 82 1 2013 1 9 [2] Grigorios Loukides John Liagouris Aris Gkoulalas-Divanis Manolis Terrovitis Disassociation for electronic health record privacy J. Biomed. Inform. 50 2014 46 61 [3] Fusheng Wang Cristobal Vergara-Niedermayr Peiya Liu Metadata based management and sharing of distributed biomedical data Int. J. Metadata Semantics Ontol. 9 1 2014 42 57 [4] Marinette Savonnet Eric Leclercq Pierre Naubourg eclims: an extensible and dynamic integration framework for biomedical information systems IEEE J. Biomed. Health Inform. 20 6 2016 1640 1649 [5] Stefano Bianchi Anna Burla Costanza Conti Ariel Farkash Carmel Kent Yonatan Maman Amnon Shabo Biomedical data integration-capturing similarities while preserving disparities. In Engineering in Medicine and Biology Society** 2009 EMBC 2009. Annual International Conference of the IEEE. IEEE 2009 4654 4657 [6] Nicole Gray Weiskopf Chunhua Weng Methods and dimensions of electronic health record data quality assessment: enabling reuse for clinical research J. Am. Med. Inform. Assoc. 20 1 2013 144 151 [7] Rosa Gini, Martijn Schuemie, Jeffrey Brown, Patrick Ryan, Edoardo Vacchi, Massimo Coppola, Walter Cazzola, Preciosa Coloma, Roberto Berni, Gayo Diallo, et al. Data extraction and management in networks of observational health care databases for scientific research: a comparison of eu-adr** omop** mini-sentinel and matrice strategies. eGEMs, 4(1), 2016. [8] Carole Goble Robert Stevens State of the nation in data integration for bioinformatics J. Biomed. Inform. 41 5 2008 687 693 [9] Ioana Danciu James D. Cowan Melissa Basford Xiaoming Wang Alexander Saip Susan Osgood Jana Shirey-Rice Jacqueline Kirby Paul A. Harris Secondary use of clinical data: the vanderbilt approach J. Biomed. Inform. 52 2014 28 35 [10] John H. Phan Chang F. Quo Chihwen Cheng May Dongmei Wang Multiscale integration of-omic, imaging, and clinical data in biomedical informatics IEEE Rev. Biomed. Eng. 5 2012 74 87 [11] Steven G. Johnson Stuart Speedie Gyorgy Simon Vipin Kumar Bonnie L. Westra Application of an ontology for characterizing data quality for a secondary use of ehr data Appl. Clin. Inform. 7 1 2016 69 88 [12] Yasset Perez-Riverol Mingze Bai Felipe da Veiga Leprevost Silvano Squizzato Young Mi Park Kenneth Haug Adam J. Carroll Dylan Spalding Justin Paschall Mingxun Wang Discovering and linking public omics data sets using the omics discovery index Nat. Biotechnol. 35 5 2017 406 [13] Shawn N. Murphy Michael E. Mendis David A. Berkowitz Isaac Kohane Henry C. Chueh Integration of Clinical and Genetic Data in the i2b2 Architecture. In AMIA Annual Symposium Proceedings** volume 2006 American Medical Informatics Association 2006 1040 [14] R. Ball M. Robb S.A. Anderson Dal G. Pan The fda's sentinel initiative a comprehensive approach to medical product surveillance Clin. Pharmacol. Therapeut. 99 3 2016 265 268 [15] Lucia Monaco, Marco Crimi, Chiuhui Mary Wang, The challenge for a european network of biobanks for rare diseases taken up by rd-connect. Pathobiology 81(5-6)(2014)231-236. [16] HCU.P. Databases Agency for Healthcare Research and Quality 2014 MD Rockville [17] James J. Cimino Elaine J. Ayres Lyubov Remennik Sachi Rath Robert Freedman Andrea Beri Yang Chen Vojtech Huser The national institutes of healths biomedical translational research information system (btris): design** contents** functionality and experience to date J. Biomed. Inform. 52 2014 11 27 [18] J. McMurry S. Jupp J. Malone T. Burdett Ay Jenkinson H. Parkinson M. Davies M. Brandizi Report on the scalability of semantic web integration in biomedbridges 2015 [19] Paul A. Harris Robert Taylor Robert Thielke Jonathon Payne Nathaniel Gonzalez Jose G. Conde Research Electronic Data Capture (REDCap)-a Metadata-driven Methodology and Workflow Process for Providing Translational Research Informatics Support J. Biomed. Inform. 42 2 2009 377 381 [20] Monica M. Horvath Stephanie Winfield Steve Evans Steve Slopek Howard Shang Jeffrey Ferranti The deduce guided query tool: providing simplified access to clinical data for research and quality improvement J. Biomed. Inform. 44 2 2011 266 276 [21] Xiaoling Chen Anupama E. Gururaj Burak Ozyurt Ruiling Liu Ergin Soysal Trevor Cohen Firat Tiryaki Yueling Li Nansu Zong Min Jiang Datamed-an open source discovery index for finding biomedical datasets J. Am. Med. Inform. Assoc. 2018 [22] Luís Basti ao Silva Alina Trifan José Luís Oliveira Montra: An agile architecture for data publishing and discovery 2018 Computer Methods and Programs in Biomedicine [23] Olga Oliveira Luis Bastiao Silva Peter Rijnbeek Michel Speybroeck Jose Luis Oliveira A methodology to perform semi-automatic distributed EHR database queries. In The 11th International Conference on Health Informatics HEALTHINF 2018 465 472 [24] Harshana Liyanage Uy Hoang Filipa Ferreira Denise Alexander Michael Rigby Mitch Blair Simon De Lusignan Availability of computerised medical record system data to compare models of child health care in primary care across europe Stud. Health Technol. Inform. 244 2017 8 12 [25] Pieter Jelle Visser Johannes Streffer A european medical information framework for alzheimer's disease (emif-ad) Alzheimer's & Dementia: The Journal of the Alzheimer's Association 11 7 2015 P120 P121 [26] H. Liyanage S. Shinneman U. Hoang F. Ferreira D. Alexander M. Rigby M. Blair Profiling databases to facilitate comparison of child health systems across Europe using standardised quality markers Stud. Health Technol. Inform. 247 2018 61 65 [27] Alina Trifan Jose Luis Oliveira A fair marketplace for biomedical data custodians and clinical researchers. In 31st IEEE CBMS International Symposium on Computer-Based Medical Systems. IEEE 2018 465 472 "
    },
    {
        "doc_title": "Patient data discovery platforms as enablers of biomedical and translational research: A systematic review",
        "doc_scopus_id": "85063456029",
        "doc_doi": "10.1016/j.jbi.2019.103154",
        "doc_eid": "2-s2.0-85063456029",
        "doc_date": "2019-05-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Biomedical data",
            "Data interoperability",
            "Discovery platforms",
            "Secondary use",
            "Translational Research",
            "Electronic Health Records",
            "Humans",
            "Software",
            "Translational Medical Research"
        ],
        "doc_abstract": "© 2019Background: The global shift from paper health records to electronic ones has led to an impressive growth of biomedical digital data along the past two decades. Exploring and extracting knowledge from these data has the potential to enhance translational research and lead to positive outcomes for the population's health and healthcare. Obective: The aim of this study was to conduct a systematic review to identify software platforms that enable discovery, secondary use and interoperability of biomedical data. Additionally, we aim evaluating the identified solutions in terms of clinical interest and main healthcare-related outcomes. Methods: A systematic search of the scientific literature published and indexed in Pubmed between January 2014 and September 2018 was performed. Inclusion criteria were as follows: relevance for the topic of biomedical data discovery, English language, and free full text. To increase the recall, we developed a semi-automatic and incremental methodology to retrieve articles that cite one or more of the previous set. Results: A total number of 500 candidate papers were retrieved through this methodology. Of these, 85 were eligible for abstract assessment. Finally, 37 studies qualified for a full-text review, and 20 provided enough information for the study objectives. Conclusions: This study revealed that biomedical discovery platforms are both a current necessity and a significantly innovative agent in the area of healthcare. The outcomes that were identified, in terms of scientific publications, clinical studies and research collaborations stand as evidence.",
        "available": true,
        "clean_text": "serial JL 272371 291210 291682 291870 291901 31 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2019-03-25 2019-03-25 2019-03-28 2019-03-28 2019-06-20T15:55:49 S1532-0464(19)30072-3 S1532046419300723 10.1016/j.jbi.2019.103154 S300 S300.2 FULL-TEXT 2021-10-18T13:04:21.617548Z 0 0 20190501 20190531 2019 2019-03-25T07:24:34.033959Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst orcid primabst ref specialabst 1532-0464 15320464 true 93 93 C Volume 93 22 103154 103154 103154 201905 May 2019 2019-05-01 2019-05-31 2019 Methodological reviews article rev © 2019 Elsevier Inc. PATIENTDATADISCOVERYPLATFORMSENABLERSBIOMEDICALTRANSLATIONALRESEARCHASYSTEMATICREVIEW TRIFAN A 1 Introduction 2 Methods 2.1 Search strategy 2.2 Inclusion and exclusion criteria 2.3 Study selection 2.4 Data collection and analysis 3 Results 3.1 Overview of the studies 3.2 Summary of the studies 3.3 Main findings 4 Discussion 4.1 Limitations 5 Conclusions Conflict of interest Acknowledgements Supplementary material References KRUSE 2016 C PENNINGTON 2014 379 383 J BOTSIS 2010 1 T LINDER 2010 1211 1215 J HUSTON 1996 1697 P RAZICK 2014 S GRO 2016 333 340 A SUBRAMANIAN 2015 27497 S SHEN 2016 e0160005 F SMITH 2007 1251 B RANCE 2015 771 B WANG 2016 47 57 L WILKINSON 2016 M GROTH 2014 12 18 P MAJEED 2018 65 69 R GREEN 2015 464 e20 A DANCIU 2014 28 35 I MAYRHOFER 2016 379 384 M VACCARINO 2018 28 A GAINOTTI 2018 631 S LANCASTER 2015 957 964 O SILVA 2018 33 42 L DAVIES 2016 M BERGERON 2018 e0200926 J WRIGHT 2016 197 201 A NEU 2016 1168 1174 S YAMAMOTO 2018 Y MCQUILTON 2016 P SANSONE 2017 170059 S CHEN 2018 300 308 X DEMOOR 2015 162 173 G WEN 2017 108778 C SAYERS 2009 E MOHER 2015 1 D HIGGINS 2011 d5928 J MARTINSANCHEZ 2014 14 F PHAN 2012 74 87 J DEBACKERE 2018 56 72 F CANUEL 2014 280 290 V TRIFANX2019X103154 TRIFANX2019X103154XA Full 2020-05-01T01:13:16Z OA-Window ElsevierBranded 2020-03-28T00:00:00.000Z 2020-03-28T00:00:00.000Z © 2019 Elsevier Inc. This article is made available under the Elsevier license. 0 item S1532-0464(19)30072-3 S1532046419300723 10.1016/j.jbi.2019.103154 272371 2019-06-20T14:58:52.53392Z 2019-05-01 2019-05-31 true 631514 MAIN 7 51619 849 656 IMAGE-WEB-PDF 1 ga1 true 9827 160 219 gr1 6911 129 219 gr2 5920 163 164 ga1 true 15094 200 274 gr1 44252 318 542 gr2 16560 288 289 ga1 true 98617 886 1214 gr1 295951 1411 2402 gr2 98112 1276 1280 mmc1 false 55428 APPLICATION mmc2 false 82501 APPLICATION YJBIN 103154 103154 S1532-0464(19)30072-3 10.1016/j.jbi.2019.103154 Elsevier Inc. Fig. 1 The systematic search process with inclusion and exclusion criteria. The ellipse shapes represent programmatic searches. Fig. 2 Scope assessment of the identified data discovery solutions. Table 1 Characteristics that were evaluated in the review. Field Description Pubmed ID Pubmed Identifier Title Title of the manuscript Year Publication year Warehouse Indicates whether the platform is a warehouse Open-access Indicates whether the platform is open access Data type What types of data does it leverage? Data size Number of datasets available FAIR Are the FAIR guidelines addressed? If yes, how are they being followed? API Does the platform support interacting with the data through an API? Ontologies/standards Is there any standardization involved? If so, what ontologies or standards are being used? Privacy protection How is data privacy ensured and what are the access rules? Relevant outcomes or use cases What achievements has the platform enabled? To be included Indicates whether the reviewer considers the manuscript fit to the current review Comments Any additional comments that the reviewer might have Table 2 Summarized information about the platforms included in the review. Citation Open Access Data Ontologies/standards Privacy Protection Groth et al. [16] Yes 11 linked datasets of pharmacological and physiochemical data Data from the various data sources are mapped into a single uniform vocabulary Licensing terms and conditions may apply to each dataset Majeed et al. [17] No 17,916 patient registers of lung cancer related data; on average, 26 details are available per patient A common terminology for patient phenotypes and bio specimen was developed and agreed upon within the consortium Data encrypted transfer over the internet; pseudonymization on-the-fly Bergeron et al. [26] Yes More than 180 cojhort epidemiological studies from 14 international networks – 6,240,000 participants Metadata standardization Not addressed Silva et al. [24] Yes 480 datasets of cohort and EHR pan-European data Not enforced, but possible User permissions (Role Based Access Control) and different levels of data exposure. Wright et al. [27] No Multi-scale life-science data No direct mention. There is a CSV template for structuring the data for import (this template could lead to harmonization) Data permissions granted through user groups and role-based access control policies Crawford et al. [28] Yes 479,483 patient records from 44 GAAIN partners containing neuroimaging, demographic, genetic, and biologic data targeting Alzheimer’s disease Partner data in GAAIN is mapped to a single schema Data is deidentified and each custodian has a on/off data switch Yamamoto et al. [29] Yes 65 Sparql endpoints amounting to 22G of multi-scale life-science Linked Data Linked Data – Resource Description Framework (RFD) and SPARQL access protocol Not addressed McQuilton et al. [30] Yes 700 databases of multi-scale life-science Linked Data, including animal samples, proteins, peptides Semantic representations of a topic or field used to catalogue and organise data into ontologies Set of well defined policies in place for accessing data Razick et al. [7] Yes Multi-scale life-scrience data - a demo using the HUNT Biobank is in place Metadata tags based on controlled vocabularies and standardized terms Role based user access control policies Chen et al. [32] Yes 2,336,403 datasets in 75 repositories of protein, phenotype, gene expression, clinical and omics data Yes – Data Tag Suite model (DATS) Not addressed Green et al. [18] Yes Data from 14 cancer clinical trials covering 9,000 subjects Not addressed Data owners are responsible for deidentifying data according to well defined standards Danciu et al. [19] Yes Electronic medical records of 2,2 millions unique individuals Not addressed User access control policies Davies et al. [25] Yes Multi-scale life-science data - four health data networks were implemented using PopMedNet Yes - not specified User access control policies Lancaster et al. [23] Yes Sensitive biomedical data, from genomics to cohort data coming from 15 diagnostic or research labs Bio-ontologies from the National Center for Biomedical Ontology (NCBO) BioPortal library Role based discovery and access control Gainotti et al. [22] Yes 222 registries of biomolecular data and 21 biobanks The Human Phenotype Ontology (HPO) and the Orphanet Rare Disease Ontology (ORDO) Not addressed Pennington et al. [3] No Multi-scale life-science data from 47,300 patients across 24,900 catheterization procedures and 54,000 echocardiogram procedures within the CardioDB installation at The Children’s Hospital of Philadelphia Standard ICD9 and Current Procedural Terminology codes along with CHOP-customized codes Data available only to CHOP partners. De Moor et al. [33] Yes Electronic health records from 11 data provider sites in 5 European countries EHR4CR Common Element Templates (CETs) and Common Data Elements (CDEs) can be considered as the semantic building blocks of the EHR4CR Common Information Model The EHR4CR security architecture also mandates the adoption of a number of information security practices (e.g. regarding the treatment of passwords and personal medical data) as mandated by the EHR4CR non-functional requirements Mayrhofer et al. [20] Yes Human biological samples and associated biomedical and biomolecular data reaching 515 biobanks and standalone collections Yes - not specified Not addressed Wen et al. [34] Yes 1,086 breast cancer patients clinical data and 7 types of omics data of the Cancer Genome Atlas No Not addressed Vaccarino et al. [21] No Clinical, neuroimaging and molecular human and animal brain-related data from 40 institutions, incorporating 17,000 study participants and 1,500 animal subjects Yes - not specified Granular access control Patient data discovery platforms as enablers of biomedical and translational research: A systematic review Alina Trifan ⁎ José Luís Oliveira IEETA/DETI, University of Aveiro, Portugal IEETA/DETI University of Aveiro Portugal ⁎ Corresponding author. Graphical abstract Background The global shift from paper health records to electronic ones has led to an impressive growth of biomedical digital data along the past two decades. Exploring and extracting knowledge from these data has the potential to enhance translational research and lead to positive outcomes for the population’s health and healthcare. Obective The aim of this study was to conduct a systematic review to identify software platforms that enable discovery, secondary use and interoperability of biomedical data. Additionally, we aim evaluating the identified solutions in terms of clinical interest and main healthcare-related outcomes. Methods A systematic search of the scientific literature published and indexed in Pubmed between January 2014 and September 2018 was performed. Inclusion criteria were as follows: relevance for the topic of biomedical data discovery, English language, and free full text. To increase the recall, we developed a semi-automatic and incremental methodology to retrieve articles that cite one or more of the previous set. Results A total number of 500 candidate papers were retrieved through this methodology. Of these, 85 were eligible for abstract assessment. Finally, 37 studies qualified for a full-text review, and 20 provided enough information for the study objectives. Conclusions This study revealed that biomedical discovery platforms are both a current necessity and a significantly innovative agent in the area of healthcare. The outcomes that were identified, in terms of scientific publications, clinical studies and research collaborations stand as evidence. Keywords Biomedical data discovery Discovery platforms Data interoperability Secondary use of data Translational research 1 Introduction The wide adoption of digital records among health institutions across the world has been an important factor in shaping both current medical practices and clinical research. Electronic medical records (EMR), or Electronic Health Records (EHR), have successively replaced paper-based medical records, allowing the storage, retrieval and modification of clinical registers through digital means. They include routinely gathered clinical data, patient demographic data, laboratory results, radiology and pharmaceutical records, statistic and administrative data, as well as patient-centred, such as data coming from self-monitoring devices. With the increasing use of medical digital records, we are witnessing a shift in the complexity of the overall goal of medical institutions and physicians, which is to ultimately provide better healthcare. Nowadays the greatest difficulty is no longer to acquire data, but rather to manage it and extrapolate knowledge from it. The volume, speed and heterogeneity at which medical data is produced expose the so called Big Data era within healthcare [1]. Health Big Data (HBD) covers more than just a very large amount of data or a large number of data sources. It also takes into consideration the complexity, challenges and new opportunities presented by the combined analysis of data and their secondary use [2]. Now more than ever, there is a significant potential for the reuse of HBD for research [3]. Secondary use of health data has the potential to expand knowledge about diseases and their appropriate treatments, generate new understanding about the efficiency of healthcare systems, and fuel new discoveries that can eventually lead to a more personalized healthcare [4–6]. The integration and reuse of these huge amounts of data can impact clinical decisions, pharmaceutical discoveries, disease monitoring and the way the populations healthcare is provided globally. Storing data for future reuse and reference has been a critical factor in the success of modern biomedical sciences [7]. While the storing of electronic health data is done nowadays intrinsically and ubiquitously in more and more healthcare facilities, with many countries already relying fully on digital records, its reuse is still a delicate process and not at all straightforward. In order for data to be reused, first it has to be discovered. Finding a dataset for a study can be burdensome due to the need to search individual repositories, read numerous publications and ultimately contact data owners or publication authors on an individual basis. Electronic health data discovery is raising the interest within the research community, due to the possibility to share and to study large datasets. Data discovery solutions usually focus, on one hand, on providing researchers with an overview of existent and accessible datasets and on the other hand, on giving data owners the possibility to communicate the existence of data, without necessarily fully exposing it, all in the same place. Translational research indicates a need for data discovery platforms that can stage and disseminate data in a readily accessible form [3]. Sharing health digital datasets for secondary use allows carrying out research studies with minor costs by leveraging existing data and achieving a better use of resources invested in research [8]. One important factor that impacts the quality of the data reuse process is its interoperability. Ideally, biomedical data would be universally stored following the same predefined set of rules and protocols. While efforts are being conducted in this direction, through the definition and adoption of ontologies and standard vocabularies [9–14], most of the existing electronic health data is heterogeneous among different healthcare institutions, let alone different countries. For this reason, population-based research is often slowed down by the difficulty in compiling and assessing large amounts of interoperable data. The FAIR principles [15] are a set of guidelines recently published that provide specifications on how to make data more meaningful and useful, by making it Findable, Accessible, Interoperable and Reusable. They put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting their reuse. Their adoption by data discovery platforms may facilitate data reuse and contributes to the advances of translational and clinical research. The objective of this systematic review was to identify projects and software solutions that promote patient electronic health data discovery, as enablers for data reuse and advancement of biomedical and translational research. We have identified 20 such platforms [16–24,3,7,25–34], following a semi-automatic search strategy that we present in the following section. We are interested in understanding how these systems address interoperability, how they impact clinical research and ultimately what the outcomes are in terms of scientific results and contributions to better general healthcare. Moreover, we intend this systematic review to be a support tool for a researcher looking for EMR databases for secondary use, as we examine the existent solutions and aggregate relevant information such as type of data, number of datasets and data privacy concerns. 2 Methods 2.1 Search strategy Our initial approach to identifying original scientific publications show-casing biomedical data discovery platforms was to put together a list of discovery platforms with which we were familiar, given our previous experience in the area. The purpose of this was to identify common terms in the titles of the scientific work behind these platforms, so as to include them in the query of the systematic search protocol. We quickly understood this would be a very challenging task as these studies were very heterogeneous in terms of vocabulary, both in titles and associated keywords. This made it difficult to reach a common ground in what concerns the query terms. Additionally, we avoid the terms “electronic health record” or “electronic medical record” in our queries as there is a the large amount (order of thousands) of manuscripts out of scope that include these terms (such as use cases descriptions, clinical trials experiments, hardware-related). Moreover, queries such as “discovery platform” or “data discovery” on some of the most common scientific databases, such as Scopus, ACM or Pubmed, not only are not able to fully cover the chosen topic, but also lead to thousands of results related to other types of discoveries. In order to cope with these challenges, we adopted a strategy in which we limit the query terms and we explore more the notion of similarity. First, we limited the search to Pubmed database, since it indexes the most extensive collection of health-care related publications. A secondary reason for choosing Pubmed database is that it provides a public Application Programmable Interface (API) that allows programmatic retrieval of information of interest [35]. Based on the previous assessment of the lack of correlation between search concepts and scientific work, and considering the limited number of terms that can be used in querying of online scientific databases, we assumed that no single query would be able to retrieve all, or the great majority, of publications of interest. Therefore, we decided to take a cyclical machine-supported approach for identification of these publications. The programmatic retrieval was done using the Biopython framework. 1 1 We have limited the initial search terms to the following boolean: “data discovery[Title]” OR “discovery platform[Title]”. From here, we constructed a tree search, in which the initial nodes were the Pubmed identifiers of the articles returned by the search query. For each of these ids, we automatically retrieved the first 20 most similar articles. For each of these ids, we automatically retrieved the first 20 most similar articles. We removed the duplicates by means of an automatic Python 2 www.python.org 2 script and we assessed the remaining titles for relevance for the topic. Finally, we repeated the tree search, this time looking only for the 5 most similar articles to the ones previously retrieved (excluding the initial nodes, which had already been considered by the first level of the tree search). Again, the duplicates were removed and the remaining titles were assessed. All searches were limited to the period from January 2014 to September 2018. Additionally, we discarded papers related to the area of molecular biology, since we intended to keep the focus of this paper on patient data. After the title assessment process, we performed a manual assessment of the references cited in this initial batch of articles. The manual search complemented the initial search results by revealing a small number of manuscripts that were considered potentially relevant for the review. 2.2 Inclusion and exclusion criteria The titles, list of authors and publication dates of the manuscripts resulting from the search, both systematic and manual, were joined in a list that was further ordered by author names. Multiple manuscripts belonging to the same author were analyzed in order to identify the most recent or the one that better describe the solution. Having identified one such manuscript per author, the remaining articles belonging to the same authors were discarded, as they would contain similar content and thus add some redundancy to the final results of the review. Other exclusion criteria were, as follows: duplicated entries, relevance for the chosen topic, and the level of detail to which the topic is addressed. Finally, the publication dates were reviewed in order to guarantee that only manuscripts published between January 2014 and September 2018 were included. 2.3 Study selection After applying the inclusion and exclusion criteria, a total of 80 titles were considered as possible candidates for the study. These were complemented by 5 titles that were identified in their bibliography and were manually added to the review process. After abstract evaluation, 37 manuscripts were considered for full text retrieval. Of these, 20 were considered relevant for the study and will be assessed throughout the rest of this manuscript. The results and conclusions drawn are presented in the next sections, following the Preferred Reporting Items for Systematic Review and Meta-Analysis protocols (PRISMA-P) 2015 statement [36]. The complete pipeline of the methodology followed in this review is illustrated in Fig. 1 . 2.4 Data collection and analysis Both authors reviewed the papers to be included in the review. They each recorded independent observations on an individual Excel spreadsheet, focusing on the type of data, data interoperability and outcomes that resulted from use of the platforms for data discovery that were previously identified. Table 1 lists the fields that each of the authors had to track in their individual spreadsheet. Reviewers had to identify possible bias in each paper, based on the Cochrane Collaborations risk-of-bias tool [37]. Finally, observations were combined into one spreadsheet for discussion. No papers were discarded because of bias. 3 Results We identified 20 unique publications that addressed data discovery platforms and architectures. Platforms are mainly web-sites that enable data discovery by exposing different levels of information about electronic health datasets, from meta-data to aggregated data. Architectures are fully-fledged solutions that can be installed in more than just one instance. They are usually detailed with more emphasis on technical aspects and at least one example of a platform based on such an architecture is presented. 3.1 Overview of the studies Biomedical data exists in multiple scales, from molecular to patient data. The integration and reuse of routinely collected clinical data for research purposes has raised growing awareness within the research community. Health systems, genetics and genomics, population and public health are all areas that may benefit from big data integration and its associated technologies [38]. The secondary reuse of citizens’ health data and investigation of the real evidence of therapeutics may lead to the achievement of personalized, predictive and preventive medicine [39]. However, in order for researchers to be able to reuse data and conduct integrative studies, they first have to find the right data for their research. Data discovery platforms and tools are one-stop shops that enable clinical researchers to identify datasets of interest without having to perform individual, extensive searches over distributed, heterogeneous health centers. The manuscripts included in this review describe current data discovery solutions. While they serve the same ultimate purpose, different characteristics among these platforms can be noted. As such, we have identified platforms that integrate more specific information, for example disease-related biomedical data and platforms that showcase a broad range of life-science data. Fig. 2 summarizes our findings in terms of the type of data that the identified solutions showcase. Another relevant aspect in assessment of the identified solutions is the underlying method for exposing and eventually making data available. Discovery systems can expose data at different levels of granularity, from metadata to raw data in some cases. Data catalogs are frequent options in advertising a product and the same concept can be applied to exposing, or making biomedical data discoverable. Depending on the fundamental vision and purpose of each such system, we can distinguish two different methodologies to make data discoverable within the manuscripts selected for this review. On one hand, warehouse solutions (30%), in which all data are gathered at a single central location, and are then accessible from a single access point. On the other hand, most of the manuscripts report on discovery systems in which data remain at the owner’s site, i.e. where it is collected and maintained (70%).Data never leaves the owner’s site, meaning it is kept in its original institution (owner) and no data integration is performed. In this case discovery platforms generally showcase metadata or aggregated views of the original datasets and link to the owner’s site, where data can be made available following given protocols. On the other hand, data warehouses gather data from several repositories and institutions and as such the effort is duplicated as the data warehouse keeps a copy, periodically updated, of the data held at the institution level. 3.2 Summary of the studies We now summarize the studies included in this review, taking into consideration some of the characteristics previously identified. As such, we first overview the warehouse platforms, where data can be discovered and accessed within the platform, as they are integrated at a single location. Next, we shift towards the platforms in which data is kept at the owner’s site, by reviewing disease-oriented solutions. The more mature platforms are presented next, followed by more recent approaches to discoverability enabled by Linked Data. More details of each of these platforms and the type of data they promote are given in Table 2 . Among the warehouse platforms, the Vanderbilt approach [19] is a two-in-one data warehouse, containing both fully de-identified research data and fully identified research that is made available taking into consideration access protocols and governance rules. The Project Data Sphere initiative was built to voluntarily share, integrate, and analyze historical cancer clinical trial data sets with the final goal of advancing cancer research [18]. BBMRI-ERIC, the Biobanking and BioMolecular Resources Research Infrastructure-European Research Infrastructure Consortium, is an umbrella organization for biobanking in Europe. For rare and common diseases alike, it provides fair access to quality-controlled human biological samples and associated biomedical and biomolecular data [20]. Disease oriented platforms, such as The Ontario Brain Institutes (Brain-CODE) [21] are designed with a very explicit, yet not limited, purpose of sup-porting researchers in better understanding a specific disease. Brain-CODE addresses the high dimensionality of clinical, neuroimaging and molecular data related with various brain conditions. The platform provides integrated datasets that can be queried and linked to provincial, national and international databases. Similarly, the breast cancer (B-CAN) platform [34] was designed as a private cancer data center that enables the discovery of cancer-related data and drives research collaborations aimed at better under-standing of this disease. In the rare disease spectrum, RD-Connect [22] links genomic data with patient registries, biobanks, and clinical bioinformatics tools in an attempt to provide a FAIR rare disease complete ecosystem. In-formation coming from inventories, websites, scientific journals and technical reports can be reached from the Biobank finder, which also provides a link to the RD-Connect Sample Catalogue, an inventory of biological samples. The German Centre for Lung Research (DZL) is an association of Germany’s research and medical institutions dedicated to lung research. It provides a data warehouse where all cancer patient related data are combined and made accessible [17]. On the Alzheimer’s spectrum, the Global Alzheimer’s Association Interactive Network (GAAIN) is a network of shared research data, analysis tools, and computational resources to study the causes of Alzheimer’s disease [28]. Among most established initiatives, Cafe Variome [23] provides a general-purpose, web-based, data discovery tool that can be quickly installed by any genotype–phenotype data owner and makes data discoverable. In this aspect, several similarities between Cafe Variome and MONTRA [24], another fully-fledged open-source discovery solution, were revealed. MONTRA is a rapid-application development framework designed to facilitate the integration and discovery of heterogeneous objects. Both solutions rely on a catalogue for data discovery, and include extensive search functionalities and query capabilities. Harvest [3] is another open-source framework of modular components, used for the rapid development and deployment of custom data discovery software applications. eGenVar [7] is a metadata cataloguing system and a software suite for reporting the presence of data from the life sciences domain. Specifically, it allows users to report, track, and share information on data content and provenance. The PopMedNet software platform features distributed querying, customizable workflows, and search capabilities [25]. A cataloguing toolkit is proposed by Maelstrom Research, built upon two main components: a metadata model and a suite of open-source software applications. When combined, the model and software support implementation of study and variable catalogues and provide a powerful search engine to facilitate data discovery. This toolkit already serves several national and international initiatives [26]. REDCap is an electronic data capture soft-ware that allows the easy building of research instruments while providing collaboration capabilities, metadata workflow, security, auditing, and export to common statistical packages. Medical librarians have used REDCap for both research data capture as well as operational databases [27]. These examples are more than just platforms, being fully-fledged architectures that can be installed in more than one site. They include software to expose biomedical datasets and different other bioinformatics tools. We consider these architectures equally relevant for the topic of this review and we assess the platforms that were built based on them. An ambitious initiative from the US National Institute of Health (NIH), DataMed, envisions to be for data what PubMed has been for the scientific literature. Similar to the Journal Article Tag Suite used in PubMed, the DATS model enables submission of metadata on datasets to DataMed [31]. DataMed can efficiently index and search diverse types of biomedical datasets across repositories and consists of 2 main components: the data ingestion pipeline that collects and transforms original metadata information to the DATS unified data model and a search engine that finds relevant datasets based on user-entered queries [32]. The Innovative Medicines Initiative’s project EHR4CR developed a platform for reusing EHR data to support medical research. An initial instance of the platform integrated datasets from eleven participating hospitals and ten pharmaceutical companies located in seven European countries [33]. Linked Data is also explored in discovery platforms, such as YummyData [29] which was designed to improve the findability and reusability of life science datasets provided as Linked Data. It consists of two components, one that periodically polls a curated list of SPARQL endpoints and a second one that monitors them and presents the information measured. BioSharing is a manually curated searchable portal of three linked registries [30] that cover standards, databases and data policies in the life sciences. Similarly, the Open PHACTS Discovery Platform [16] leverages Linked Data to provide integrated access to pharmacology databases. 3.3 Main findings Data discovery solutions should provide intuitive, easy to use functionalities for identification of the right data. Search support is imperative for the identification of appropriate datasets of interest and for assessment of their suitability. Our findings show that all studies include this functionality. Another critical issue when dealing with biomedical data is privacy protection and proper access control; 20% of the studies take into consideration secure data access and implement access control rules, such as user permissions [24,27,19,33,21]; 15% of the platforms address the privacy issue from the point of view of data anonymization and the possibility of data owners removing their datasets from the platform [17,40,18]. With respect to interoperability, 40% of the evaluated solutions rely on the use of ontologies and standards so as to provide a heterogeneous view of the data [16,17,26–28,30,32,23,21]. This evaluation reveals a moderate adoption of the FAIR principles, which accentuate on the discovery of data by machines, through APIs (45%) [17,26,24,29,30,32,25,23,21]. Lastly, we identified a positive trend in supporting open science, as 60% of these platforms and architectures are open access [16,26,24,28–30,7,32,25,3,21,34]. Table 2 presents the general characteristics of the platforms and architectures. For the architectures that we have identified and that can back-up more than one platform, we include those platforms to which the original architecture publication links to. We propose this table as a discovery tool itself as it can guide researchers in finding the right platform for a specific clinical research question. 4 Discussion Having identified the importance of data discovery platforms in the context of data interoperability and reuse, we identify current platforms and architectures that promote digital health data discovery. The majority of the solutions identified support open access and have taken the first steps in following the FAIR principles, which are recent driving forces behind establishing data interoperability. Additionally, the result table included in this review was intended as a discovery tool itself, to enable researchers to identify current platforms and assess the usability of each platform given a scientific question or a clinical research interest. For each solutions discussed, we were interested in understanding the outcome generated, in terms of scientific and clinical contributions and the impact these platforms have on translational research. Apart from the evident contributions of some of the platforms in promoting research collaborations and shared knowledge on specific disease related research questions [28,17,22], some of the most mature discovery architectures, such as [27] led to more than 6000 written scientific contributions. This platform brings together 3000 institutions from 128 countries. Another relevant outcome, reached by means of the architecture presented by Davies et al. [25] led to four health data networks implemented across US: Sentinel, Patient-Centered Clinical Research Network, Massachusetts Department of Public Health, and the NIH Collaboratory Distributed Research Network. A mature pan-European discovery platform based on the architecture proposed by Silva et al. [24], the EMIF Catalogue 3 www.emif-catalogue.eu 3 aggregates metadata of more than 10 millions European subjects in 480 datasets. Finally, the platform identified in [30] is maintained as a community resource closely embedded in and co-sponsored by several infrastructure programs, including the NIH Big Data to Knowledge Initiative’s BioCADDIE 4 4 and CEDAR 5 5 projects. It is being enhanced as part of the Elixir UK node’s contribution to the ELIXIR EXCELERATE program. 6 6 The amount of data collected by some platforms, such as [24,27,2], as well as the extent of the research collaborations that they fostered, are clear evidences that they are important enablers in reusing health data for research. One important aspect to be considered when dealing with real world health data is data protection. Aspects such as patient privacy, informed consent, transparency or legislative frame must be considered. This responsibility lies with the data owner, who has to inform the patient and act accordingly, before making any data available for secondary use. In addition, it is essential that discovery platforms clearly define access rules and standardized permissions protocols. These discovery platforms are quite recent enablers of translational research. They are proof that current healthcare is already impacted by the ability to find the right data source for secondary research. We believe many more positive outcomes can be expected in the near future, not only in terms of generating new knowledge but also with respect to better understanding of modern diseases and providing personalized, overall improved healthcare. 4.1 Limitations One possible limitation of this study is the reduced number of authors that reviewed the scientific contributions included in the review. Selection bias might be present in some studies. In situations where there was no initial consent to include a manuscript, the authors presented all the reasons for the inclusion/exclusion of a given manuscript and a final decision was reached jointly. Another limitation might reside in the publication period of the considered manuscripts. An inclusion criterion was that they had to be published between January 2014 and September 2018. This decision was motivated by two factors. Firstly, we wanted to focus on recent, on-going projects that leverage biomedical data discovery. Secondly, we identified systematic reviews published previous to 2014 that partially covered this topic, such as [41]. 5 Conclusions The above assessment is intended as a structured overview of the existent solutions for data discovery and integration that support data interoperability, an imminent need for biomedical data reuse. Through this study we have come to understand that data discovery platforms and tools enable more qualitative research and have the potential to speed up and reduce the cost of translational research. This review was based on the analysis of the scientific works that detail either the implementation, the use, or both aspects, of the platforms presented. This review revealed that while there is still work to be done in this research field, the results identified are the standing proof that such platforms promote meaningful research collaboration targeted at secondary use of biomedical data. Conflict of interest The auhtors declared that there is no conflict of interest. Acknowledgements This work has received support from the EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 806968 and from the Integrated Programme of SR&TD SOCA (Ref. CENTRO-01-0145-FEDER-000010). Appendix A Supplementary material Supplementary data associated with this article can be found, in the online version, at Supplementary material The following are the Supplementary data to this article: Supplementary data 1 Supplementary data 2 References [1] C.S. Kruse R. Goswamy Y. Raval S. Marawi Challenges and opportunities of big data in health care: a systematic review JMIR Med. Informatics 4 2016 [2] G. Bouzillé, R. Westerlynck, G. Defossez, D. Bouslimi, S. Bayat, C. Riou, Y. Busnel, C. Le Guillou, J.-M. Cauvin, C. Jacquelinet, et al., Sharing health big data for research-a design by use cases: the inshare platform approach, in: The 16th World Congress on Medical and Health Informatics (MedInfo2017), 2017. [3] J.W. Pennington B. Ruth M.J. Italia J. Miller S. Wrazien J.G. Loutrel E.B. Crenshaw P.S. White Harvest: an open platform for developing web-based biomedical data discovery and reporting applications J. Am. Med. Inform. Assoc. 21 2014 379 383 [4] T. Botsis G. Hartvigsen F. Chen C. Weng Secondary use of ehr: data quality issues and informatics opportunities Summit Transl. Bioinform. 2010 2010 1 [5] J.A. Linder J.S. Haas A. Iyer M.A. Labuzetta M. Ibara M. Celeste G. Getty D.W. Bates Secondary use of electronic health record data: spontaneous triggered adverse drug event reporting Pharmacoepidemiol. Drug Saf. 19 2010 1211 1215 [6] P. Huston C.D. Naylor Health services research: reporting on studies using secondary data sources CMAJ: Can. Med. Assoc. J. 155 1996 1697 [7] S. Razick R. Močnik L.F. Thomas E. Ryeng F. Drabløs P. Sætrom The egenvar data management systemcataloguing and sharing sensitive data and metadata for the life sciences Database 2014 2014 [8] L. Hernandez, J. Onieva, G. Fico, J. Cancela, A. Dagliati, M. Bucalo, L. Sacchi, R. Bellazzi, M.T. Arredondo, A proposal of architecture to share patients data out of healthcare settings for research purposes, in: 2014 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI), IEEE, 2014, pp. 789–792. [9] A. Groß C. Pruski E. Rahm Evolution of biomedical ontologies and mappings: overview of recent approaches Comput. Struct. Biotechnol. J. 14 2016 333 340 [10] S.L. Subramanian R.R. Kitchen R. Alexander B.S. Carter K.-H. Cheung L.C. Laurent A. Pico L.R. Roberts M.E. Roth J.S. Rozowsky biomedical ontologies and linked data technologies J. Extracell. Vesicles 4 2015 27497 [11] F. Shen Y. Lee Knowledge discovery from biomedical ontologies in cross domains PloS One 11 2016 e0160005 [12] B. Smith M. Ashburner C. Rosse J. Bard W. Bug W. Ceusters L.J. Goldberg K. Eilbeck A. Ireland C.J. Mungall The obo foundry: coordinated evolution of ontologies to support biomedical data integration Nat. Biotechnol. 25 2007 1251 [13] B. Rance T. Le O. Bodenreider Fingerprinting biomedical terminologies–automatic classification and visualization of biomedical vocabularies through umls semantic group profiles Stud. Health Technol. Informatics 216 2015 771 [14] L. Wang B.E. Bray J. Shi G. Del Fiol P.J. Haug A method for the development of disease-specific reference standards vocabularies from textual biomedical literature resources Artif. Intell. Med. 68 2016 47 57 [15] M.D. Wilkinson M. Dumontier I.J. Aalbersberg G. Appleton M. Axton A. Baak N. Blomberg J.-W. Boiten L.B. da Silva Santos P.E. Bourne The fair guiding principles for scientific data management and stewardship Sci. data 3 2016 [16] P. Groth A. Loizou A.J. Gray C. Goble L. Harland S. Pettifer Api-centric linked data integration: the open phacts discovery platform case study Web Semant.: Sci. Serv. Agents World Wide Web 29 2014 12 18 [17] R.W. Majeed M.R. Stöhr C. Ruppert A. Günther Data discovery for integration of heterogeneous medical datasets in the german center for lung research (dzl) Stud. Health Technol. Informatics 253 2018 65 69 [18] A.K. Green K.E. Reeder-Hayes R.W. Corty E. Basch M.I. Milowsky S.B. Dusetzina A.V. Bennett W.A. Wood The project data sphere initiative: accelerating cancer research by sharing data Oncologist 20 2015 464 e20 [19] I. Danciu J.D. Cowan M. Basford X. Wang A. Saip S. Osgood J. Shirey-Rice J. Kirby P.A. Harris Secondary use of clinical data: the vanderbilt approach J. Biomed. Informatics 52 2014 28 35 [20] M.T. Mayrhofer P. Holub A. Wutte J.-E. Litton Bbmri-eric: the novel gateway to biobanks Bundesgesundheitsblatt-Gesundheitsforschung-Gesundheitsschutz 59 2016 379 384 [21] A.L. Vaccarino M. Dharsee S.C. Strother D. Aldridge S.R. Arnott B. Behan C. Dafnas F. Dong K. Edgecombe R. El-Badrawi sharing and analysis of multi-dimensional neuroscience data Front. Neuroinformatics 12 2018 28 [22] S. Gainotti P. Torreri C.M. Wang R. Reihs H. Mueller E. Heslop M. Roos D.M. Badowska F. Paulis Y. Kodra The rd-connect registry & biobank finder: a tool for sharing aggregated data and metadata among rare disease researchers Eur. J. Hum. Genet. 26 2018 631 [23] O. Lancaster T. Beck D. Atlan M. Swertz D. Thangavelu C. Veal R. Dalgleish A.J. Brookes Cafe variome: general-purpose software for making genotype–phenotype data discoverable in restricted or open access contexts Hum. Mutation 36 2015 957 964 [24] L.B. Silva A. Trifan J.L. Oliveira Montra: an agile architecture for data publishing and discovery Comput. Methods Programs Biomed. 160 2018 33 42 [25] M. Davies K. Erickson Z. Wyner J. Malenfant R. Rosen J. Brown Software-enabled distributed network governance: the popmednet experience eGEMs 4 2016 [26] J. Bergeron D. Doiron Y. Marcon V. Ferretti I. Fortier Fostering population-based cohort data discovery: the maelstrom research cataloguing toolkit PloS One 13 2018 e0200926 [27] A. Wright Redcap: A tool for the electronic capture of research data J. Electron. Resourc. Med. Lib. 13 2016 197 201 [28] S.C. Neu K.L. Crawford A.W. Toga Sharing data in the global alzheimer’s association interactive network Neuroimage 124 2016 1168 1174 [29] Y. Yamamoto A. Yamaguchi A. Splendiani Yummydata: providing high-quality open life science data Database 2018 2018 [30] P. McQuilton A. Gonzalez-Beltran P. Rocca-Serra M. Thurston A. Lister E. Maguire S.-A. Sansone Biosharing: curated and crowd-sourced metadata standards, databases and data policies in the life sciences Database 2016 2016 [31] S.-A. Sansone A. Gonzalez-Beltran P. Rocca-Serra G. Alter J.S. Grethe H. Xu I.M. Fore J. Lyle A.E. Gururaj X. Chen the data tag suite to enable discoverability of datasets Sci. Data 4 2017 170059 [32] X. Chen A.E. Gururaj B. Ozyurt R. Liu E. Soysal T. Cohen F. Tiryaki Y. Li N. Zong M. Jiang Datamed–an open source discovery index for finding biomedical datasets J. Am. Med. Inform. Assoc. 25 2018 300 308 [33] G. De Moor M. Sundgren D. Kalra A. Schmidt M. Dugas B. Claerhout T. Karakoyun C. Ohmann P.-Y. Lastic N. Ammour Using electronic health records for clinical research: the case of the ehr4cr project J. Biomed. Informatics 53 2015 162 173 [34] C.-H. Wen S.-M. Ou X.-B. Guo C.-F. Liu Y.-B. Shen N. You W.-H. Cai W.-J. Shen X.-Q. Wang H.-Z. Tan B-can: a resource sharing platform to improve the operation, visualization and integrated analysis of tcga breast cancer data Oncotarget 8 2017 108778 [35] E. Sayers The e-utilities in-depth: parameters, syntax and more Entrez Programming Utilities Help [Internet] 2009 [36] D. Moher L. Shamseer M. Clarke D. Ghersi A. Liberati M. Petticrew P. Shekelle L.A. Stewart Preferred reporting items for systematic review and meta-analysis protocols (prisma-p) 2015 statement Syst. Rev. 4 2015 1 [37] J.P. Higgins D.G. Altman P.C. Gøtzsche P. Jüni D. Moher A.D. Oxman J. Savović K.F. Schulz L. Weeks J.A. Sterne The cochrane collaborations tool for assessing risk of bias in randomised trials Bmj 343 2011 d5928 [38] F. Martin-Sanchez K. Verspoor Big data in medicine is driving big changes Yearbook Med. Informatics 9 2014 14 [39] J.H. Phan C.F. Quo C. Cheng M.D. Wang Multiscale integration of-omic, imaging, and clinical data in biomedical informatics IEEE Rev. Biomed. Eng. 5 2012 74 87 [40] F. De Backere P. Bonte S. Verstichel F. Ongenae F. De Turck Sharing health data in belgium: a home care case study using the vitalink platform Informatics Health Soc. Care 43 2018 56 72 [41] V. Canuel B. Rance P. Avillach P. Degoulet A. Burgun Translational research platforms integrating clinical and omics data: a review of publicly available solutions Brief. Bioinform. 16 2014 280 290 "
    },
    {
        "doc_title": "Handling Noise in Protein Interaction Networks",
        "doc_scopus_id": "85075179357",
        "doc_doi": "10.1155/2019/8984248",
        "doc_eid": "2-s2.0-85075179357",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [
            "Area Under Curve",
            "Computational Biology",
            "Databases, Protein",
            "Humans",
            "Protein Interaction Mapping",
            "Protein Interaction Maps",
            "Saccharomyces cerevisiae Proteins"
        ],
        "doc_abstract": "© 2019 Fernanda B. Correia et al.Protein-protein interactions (PPIs) can be conveniently represented as networks, allowing the use of graph theory for their study. Network topology studies may reveal patterns associated with specific organisms. Here, we propose a new methodology to denoise PPI networks and predict missing links solely based on the network topology, the organization measurement (OM) method. The OM methodology was applied in the denoising of the PPI networks of two Saccharomyces cerevisiae datasets (Yeast and CS2007) and one Homo sapiens dataset (Human). To evaluate the denoising capabilities of the OM methodology, two strategies were applied. The first strategy compared its application in random networks and in the reference set networks, while the second strategy perturbed the networks with the gradual random addition and removal of edges. The application of the OM methodology to the Yeast and Human reference sets achieved an AUC of 0.95 and 0.87, in Yeast and Human networks, respectively. The random removal of 80% of the Yeast and Human reference set interactions resulted in an AUC of 0.71 and 0.62, whereas the random addition of 80% interactions resulted in an AUC of 0.75 and 0.72, respectively. Applying the OM methodology to the CS2007 dataset yields an AUC of 0.99. We also perturbed the network of the CS2007 dataset by randomly inserting and removing edges in the same proportions previously described. The false positives identified and removed from the network varied from 97%, when inserting 20% more edges, to 89%, when 80% more edges were inserted. The true positives identified and inserted in the network varied from 95%, when removing 20% of the edges, to 40%, after the random deletion of 80% edges. The OM methodology is sensitive to the topological structure of the biological networks. The obtained results suggest that the present approach can efficiently be used to denoise PPI networks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Passive sensing of health outcomes through smartphones: Systematic review of current solutions and possible limitations",
        "doc_scopus_id": "85074280806",
        "doc_doi": "10.2196/12649",
        "doc_eid": "2-s2.0-85074280806",
        "doc_date": "2019-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Humans",
            "Mobile Applications",
            "Outcome Assessment, Health Care",
            "Smartphone",
            "Wearable Electronic Devices"
        ],
        "doc_abstract": "©Alina Trifan, Maryse Oliveira, José Luís Oliveira.Background: Technological advancements, together with the decrease in both price and size of a large variety of sensors, has expanded the role and capabilities of regular mobile phones, turning them into powerful yet ubiquitous monitoring systems. At present, smartphones have the potential to continuously collect information about the users, monitor their activities and behaviors in real time, and provide them with feedback and recommendations. Objective: This systematic review aimed to identify recent scientific studies that explored the passive use of smartphones for generating health- and well-being–related outcomes. In addition, it explores users’ engagement and possible challenges in using such self-monitoring systems. Methods: A systematic review was conducted, following Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines, to identify recent publications that explore the use of smartphones as ubiquitous health monitoring systems. We ran reproducible search queries on PubMed, IEEE Xplore, ACM Digital Library, and Scopus online databases and aimed to find answers to the following questions: (1) What is the study focus of the selected papers? (2) What smartphone sensing technologies and data are used to gather health-related input? (3) How are the developed systems validated? and (4) What are the limitations and challenges when using such sensing systems? Results: Our bibliographic research returned 7404 unique publications. Of these, 118 met the predefined inclusion criteria, which considered publication dates from 2014 onward, English language, and relevance for the topic of this review. The selected papers highlight that smartphones are already being used in multiple health-related scenarios. Of those, physical activity (29.6%; 35/118) and mental health (27.9; 33/118) are 2 of the most studied applications. Accelerometers (57.7%; 67/118) and global positioning systems (GPS; 40.6%; 48/118) are 2 of the most used sensors in smartphones for collecting data from which the health status or well-being of its users can be inferred. Conclusions: One relevant outcome of this systematic review is that although smartphones present many advantages for the passive monitoring of users’ health and well-being, there is a lack of correlation between smartphone-generated outcomes and clinical knowledge. Moreover, user engagement and motivation are not always modeled as prerequisites, which directly affects user adherence and full validation of such systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring the Value of Electronic Health Records from Multiple Datasets",
        "doc_scopus_id": "85071693378",
        "doc_doi": "10.1007/978-3-030-29196-9_19",
        "doc_eid": "2-s2.0-85071693378",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Data interoperability",
            "Electronic health record",
            "Medical information",
            "Multiple data sets",
            "Observational data",
            "Observational study",
            "Privacy and security",
            "Secondary use"
        ],
        "doc_abstract": "© 2019, Springer Nature Switzerland AG.During the last decades, most European countries dedicated huge efforts in collecting and maintaining Electronic Health Records (EHR). With the continuous grow of these datasets, it became obvious that its secondary use for research may lead to new insights about diseases and treatments outcomes. EHR databases can be used to speed up and reduce the cost of health research studies, which are essential for the advance and improvement of health services. However, many times, a single observational data source is not enough for a clinical study, thus data interoperability has a major impact on the exploration of value of EHRs. Despite the recognized benefit of data sharing, database owners remain reluctant in conceding access to the contents of their databases, mainly due to ownership, privacy and security issues. In this paper, we exploit two major international initiatives, the European Medical Information Framework (EMIF) and the Observational Health Data Sciences and Informatics (OHDSI), to provide a methodology through which multiple longitudinal clinical repositories can be queried, without the data leaving its original repository.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Bioinfo@uAVR at ERiSk 2019: Delving into social media texts for the early detection of mental and food disorders",
        "doc_scopus_id": "85070513742",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85070513742",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Anorexia",
            "Automatic Detection",
            "Depression",
            "Engineering informatics",
            "Psycholinguistic patterns",
            "Social media",
            "Social media minings"
        ],
        "doc_abstract": "Copyright © 2019 for this paper by its authors.This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the shared tasks of CLEF eRisk 20191. The objective of the eRisk initiative is to encourage research in the area of information retrieval for the automatic detection of risk situations on the internet. The challenge was organized in three tasks, focused on the early detection of anorexia (T1), self-harm (T2) and severity of depression (T3). We addressed these tasks using a mix approach that combines machine learning with psycholinguistics and behavioural patterns. The results obtained validate the use of such patterns in the context of social media mining and motivate future research into this field.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UA.Pt bioinformatics at ImageClef 2019: Lifelog moment retrieval based on image annotation and natural language processing",
        "doc_scopus_id": "85070513155",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85070513155",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Engineering informatics",
            "Life log",
            "NAtural language processing",
            "Natural languages",
            "Pre-processing step",
            "Processed images",
            "Processing tools",
            "Visual information"
        ],
        "doc_abstract": "© 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).The increasing number of mobile and wearable devices is dramatically changing the way we can collect data about a person's life. These devices allow recording our daily activities and behavior in the form of images, video, biometric data, location and other data. This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the ImageCLEF lifelog task, more specifically in the Lifelog Moment Retrieval sub-task. The approach to solve this sub-task is divided into three stages. The first one is the pre-processing of the lifelog dataset for a selection of the images that contain relevant information in order to reduce the amount of images to be processed and obtain additional visual information and concepts from the ones to be considered. In the second step, the query topics are analyzed using Natural Languages Processing tools to extract relevant words to retrieve the desired moment. This words are compared with the visual concepts words, obtained in the pre-processing step using a pre-trained word2vec model, to compute a confidence score for each processed image. An additional step is used in the last two runs, in order to include the images not processed in the first step and improve the results of our approach. A total of 6 runs were submitted and the results obtained show an evolution with each submission. Although the results are not yet competitive with other teams, this challenge is a good starting point for our research work. We pretend to continue the development of a lifelogging application in the context of a research project, so we expect to participate in the next year in the ImageCLEFlifelog task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A weighted rule-based model for file forgery detection: UA.Pt bioinformatics at ImageCLef 2019",
        "doc_scopus_id": "85070502485",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85070502485",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Computational solutions",
            "Digital technologies",
            "Forgery detections",
            "ImageCLEF",
            "Poor performance",
            "Random forest algorithm",
            "Rule-based models",
            "Weighted rules"
        ],
        "doc_abstract": "Copyright © 2019 for this paper by its authors.With today's digital technology, disparate kinds of data can be easily manipulated. The forgery commonly hides information, by altering files' extensions, files' signatures, or by using steganography. Consequently, digital forensic examiners are faced with new problems in the detection of these forged files. The lack of automatised approaches to discover these infractions encourages researchers to explore new computational solutions that can help its identification. This paper describes the methodologies used in the ImageCLEFsecurity 2019 challenge, which were mainly rule-based models. The rules and all of their underlying mechanisms created for each task are described. For the third task, was used a random forest algorithm due to the poor performance of these rules.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Strategies to access patient clinical data from distributed databases",
        "doc_scopus_id": "85064615496",
        "doc_doi": "10.5220/0007576104660473",
        "doc_eid": "2-s2.0-85064615496",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Common data model",
            "Distributed database",
            "Distributed query",
            "Electronic health record",
            "Electronic health record systems",
            "Health services",
            "Observational study",
            "Privacy and security"
        ],
        "doc_abstract": "© 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Over the last twenty years, the use of electronic health record systems has become widespread worldwide, leading to the creation of an extensive collection of health databases. These databases can be used to speed up and reduce the cost of health research studies, which are essential for the advance of health science and the improvement of health services. However, despite the recognised gain of data sharing, database owners remain reluctant to grant access to the contents of their databases because of privacy and security issues, and because of the lack of a common strategy for data sharing. Two main approaches have been used to perform distributed queries while maintaining all data control in the hands of the data custodians: applying a common data model, or using Semantic Web principles. This paper presents a comparison of these two approaches by evaluating them according to parameters relevant to data integration, such as cost, data quality, interoperability, extendibility, consistency, and efficiency.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "FAIRness in biomedical data discovery",
        "doc_scopus_id": "85064597466",
        "doc_doi": "10.5220/0007576401590166",
        "doc_eid": "2-s2.0-85064597466",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Biomedical data",
            "Data discovery",
            "Data interoperability",
            "Data quality",
            "FAIR Guidelines",
            "FAIR Metrics",
            "Guiding principles",
            "Multiple research"
        ],
        "doc_abstract": "© 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.The FAIR Guiding Principles are a recent, yet powerful set of recommendations for turning data Findable, Accessible, Interoperable and Reusable. They were designed with the purpose of improving data quality and reusability. Over the last couple of years they have been adopted more and more by both data owners and funders as key data management approaches. Despite their increasing popularity and endorsement by multiple research initiatives from some of the most diverse areas, there are still only a few examples on how these principles have been translated into practice. In this work we propose an open evaluation of their adoption by biomedical data discovery platforms. We first overview current biomedical data discovery platforms that introduce the FAIR guiding principles as requirements of their functioning. We then employ the more recent FAIR metrics for evaluating the degree to which these biomedical data discovery platforms follow the FAIR principles. Moreover, we assess their impact on enabling data interoperability and secondary reuse.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Simplifying biomedical data sharing through a web portal generator",
        "doc_scopus_id": "85058347875",
        "doc_doi": "10.1109/HealthCom.2018.8531097",
        "doc_eid": "2-s2.0-85058347875",
        "doc_date": "2018-11-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health (social science)",
                "area_abbreviation": "SOCI",
                "area_code": "3306"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Biomedical data",
            "European research",
            "Model description",
            "Non-technical users",
            "Semi-automated",
            "Web based",
            "Web system"
        ],
        "doc_abstract": "© 2018 IEEE.Despite the huge amount of biomedical data that is currently being collected, they end up many times neglected in internal silos, hindering its discovery and exploration at a large scale. One issue that contributes to this scenario is the complexity in constructing a full-fleshed web-system, to capture and manage these multiple and heterogeneous datasets.To help simplifying this task, we propose an semi-automated solution for the development of web-based data catalogs, targeted for non technical users. Starting from a simple data model description, the system automatically generates user interfaces and services for data management. This engine is being successfully used in several European research communities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automated ICD-9-CM medical coding of diabetic patient's clinical reports",
        "doc_scopus_id": "85058162044",
        "doc_doi": "10.1145/3279996.3280019",
        "doc_eid": "2-s2.0-85058162044",
        "doc_date": "2018-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Automated classification",
            "Convolutional Neural Networks (CNN)",
            "Diabetic patient",
            "Electronic health record",
            "Fully automated",
            "Independent variables",
            "Medical personnel",
            "Unsolved problems"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery. ACMThe assignment of ICD-9-CM codes to patient's clinical reports is a costly and wearing process manually done by medical personnel, estimated to cost about $25 billion per year in the United States. To develop a system that automates this process has been an ambition of researchers but is still an unsolved problem due to the inherent difficulties in processing unstructured clinical text. This problem is here formulated as a multi-label supervised learning one where the independent variable is the report's text and the dependent the several assigned ICD-9-CM labels. Different variations of two neural network based models, the Bag-of-Tricks and the Convolutional Neural Network (CNN) are investigated. The models are trained on the diabetic patient subset of the freely available MIMIC-III dataset. The results show that a CNN with three parallel convolutional layers achieves F1 scores of 44.51% for five digit codes and 51.73% for three digit, rolled up, codes. Although fully automated coding is not achievable, these results suggest that automated classification could be used to aid clinical staff by selecting the most probable codes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Smartphone as data collector in health monitoring",
        "doc_scopus_id": "85058158945",
        "doc_doi": "10.1145/3279996.3280017",
        "doc_eid": "2-s2.0-85058158945",
        "doc_date": "2018-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Current limitation",
            "Data collection method",
            "Embedded sensors",
            "Expensive equipments",
            "Health care information system",
            "Health monitoring",
            "Mobile computing systems",
            "Sensing technology"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery. ACMSensing health and well-being parameters from citizens and patients has been an increasing concern in our society. However, since the traditional data collection methods rely mostly on dedicated and expensive equipments, in the recent years, the potential of smartphones has been largely investigated because of its unobtrusiveness and embedded sensors. In this paper, we evaluate the use of smartphones as data collector in health monitoring, focusing on its sensing technologies and on the data they typically collect. Moreover, we will discuss the users' expectations, the current limitations of available solutions, and the gap between both.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Services Orchestration and Workflow Management in Distributed Medical Imaging Environments",
        "doc_scopus_id": "85050985319",
        "doc_doi": "10.1109/CBMS.2018.00037",
        "doc_eid": "2-s2.0-85050985319",
        "doc_date": "2018-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Distributed systems",
            "Healthcare institutions",
            "Information and communication systems",
            "Operational requirements",
            "Production environments",
            "System installation",
            "Workflow managements"
        ],
        "doc_abstract": "© 2018 IEEE.Medical imaging laboratories are supported by information and communication systems commonly denominated as PACS, that encompasses technology for acquisition, archive, distribution and visualization of digital images in network. Concerning the data and workflow management, traditional solutions used in production provide a limited set of services usually configured at system installation. As result, healthcare institutions are not able to fully explore their infrastructure or adapt it to new operational requirements, either for clinical or research procedures. This article proposes a framework for services orchestration and workflow management in distributed medical imaging environments. It was designed for end-user usage and is accessible through a Web portal that allows to document, repeat and allocate procedures and tasks to correct resources, either from information systems or human interventions. It provides an abstraction layer for integration with distinct data sources through standard services, allows the creation of new services through orchestration of existent ones and the scheduling of tasks. Moreover, it includes a logging and alert mechanism integrated with email service. The solution was validated through the specification of two use cases that were deployed in production environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Simplifying the Digitization of Clinical Protocols for Diabetes Management",
        "doc_scopus_id": "85050980040",
        "doc_doi": "10.1109/CBMS.2018.00038",
        "doc_eid": "2-s2.0-85050980040",
        "doc_date": "2018-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Clinical conditions",
            "Computational solutions",
            "Diabetes management",
            "Glycemic control",
            "Health professionals",
            "Healthcare institutions",
            "Hyperglycemia",
            "Hypoglycemia"
        ],
        "doc_abstract": "© 2018 IEEE.Hyperglycemia is a health condition characterized by abnormally high blood glucose, typically caused by a deficient usage, or lack, of insulin. Due to the metabolic derangements of this clinical condition, its regular monitoring, as well the administration of the most effective treatment, are major concerns for healthcare institutions. In this paper, we present a computational solution to build diabetes management protocols, which helps health professionals providing an adequate treatment for each hyperglycemic inpatient.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A FAIR Marketplace for Biomedical Data Custodians and Clinical Researchers",
        "doc_scopus_id": "85050968259",
        "doc_doi": "10.1109/CBMS.2018.00040",
        "doc_eid": "2-s2.0-85050968259",
        "doc_date": "2018-07-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Biomedical data integration",
            "Biomedical research",
            "Clinical researchers",
            "Data discovery",
            "FAIR principles",
            "Health-care system",
            "Healthcare services",
            "Heterogeneous data sources"
        ],
        "doc_abstract": "© 2018 IEEE.The exponential growth in the volume of biomedical data, induced by the increasing digitalization of health care services, has led to a shift of the bottleneck of biomedical research: from data generation to data management and analysis. When shared, the opportunities of secondary use for these data are end-less, from advancing clinical research to an overall improvement of healthcare systems. The FAIR principles have been designed as a guideline for the development of systems that enhance data reusability. The challenges of biomedical data discovery and reuse can be understood and addressed from two distinct points of view: on one side, data custodians are apprehensive about ethical and social issues when turning data discoverable; on the other one, clinical researchers have to perform intensive searches over geographical scattered, heterogeneous data sources and pursue extensive protocols for reusing these data. In this paper we present a web platform intended to serve as a bridge between data custodians and biomedical researchers. Data custodians are able to publish and share several levels of information about biomedical databases, while researchers can search for databases that fulfil their research requirements.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "MONTRA: An agile architecture for data publishing and discovery",
        "doc_scopus_id": "85044949656",
        "doc_doi": "10.1016/j.cmpb.2018.03.024",
        "doc_eid": "2-s2.0-85044949656",
        "doc_date": "2018-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Automatically generated",
            "Clinical study",
            "Data catalogues",
            "Dynamic composition of services",
            "Heterogeneous object",
            "Patient registries",
            "Presenting informations",
            "Rapid application development",
            "Biomedical Research",
            "Computer Systems",
            "Databases, Factual",
            "Humans",
            "Publishing",
            "Search Engine",
            "Software",
            "Systems Integration"
        ],
        "doc_abstract": "© 2018Background and Objective: Data catalogues are a common form of capturing and presenting information about a specific kind of entity (e.g. products, services, professionals, datasets, etc.). However, the construction of a web-based catalogue for a particular scenario normally implies the development of a specific and dedicated solution. In this paper, we present MONTRA, a rapid-application development framework designed to facilitate the integration and discovery of heterogeneous objects, which may be characterized by distinct data structures. Methods: MONTRA was developed following a plugin-based architecture to allow dynamic composition of services over represented datasets. The core of MONTRA's functionalities resides in a flexible data skeleton used to characterize data entities, and from which a fully-fledged web data catalogue is automatically generated, ensuring access control and data privacy. Results: MONTRA is being successfully used by several European projects to collect and manage biomedical databases. In this paper, we describe three of these applications scenarios. Conclusions: This work was motivated by the plethora of geographically scattered biomedical repositories, and by the role they can play altogether for the understanding of diseases and of the real-world effectiveness of treatments. Using metadata to expose datasets’ characteristics, MONTRA greatly simplifies the task of building data catalogues. The source code is publicly available at https://github.com/bioinformatics-ua/montra.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2018-03-29 2018-03-29 2018-04-04 2018-04-04 2018-05-08T11:00:43 S0169-2607(17)31272-5 S0169260717312725 10.1016/j.cmpb.2018.03.024 S300 S300.1 FULL-TEXT 2018-05-08T10:34:56.682407Z 0 0 20180701 20180731 2018 2018-03-29T03:21:22.070992Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table e-component body acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref 0169-2607 01692607 true 160 160 C Volume 160 11 33 42 33 42 201807 July 2018 2018-07-01 2018-07-31 2018 Section II: Systems and Programs article fla © 2018 Elsevier B.V. All rights reserved. MONTRAAGILEARCHITECTUREFORDATAPUBLISHINGDISCOVERY BASTIAOSILVA L 1 Introduction 2 Background 3 Methods 3.1 System requirements 3.2 Motivational example 4 Results 4.1 System architecture 4.2 Data skeleton 4.3 CRUD operations 4.4 Searching services 4.5 Plugins integration 4.6 Role-Based Access Control 4.7 RESTful API 4.8 Software technologies 5 Discussion 6 Conclusions Acknowledgment Appendix A Supplementary materials References HALEVY 2006 9 16 A PROCEEDINGS32NDINTERNATIONALCONFERENCELARGEDATABASES DATAINTEGRATIONTEENAGEYEARS LAPATAS 2015 9 V LOPES 2015 P CHALLENGESOPPORTUNITIESFOREXPLORINGPATIENTLEVELDATA VANSCHAIK 2014 100 104 T TEODORO 2009 175 179 D LENZERINI 2002 233 246 M PROCEEDINGS21STACMSIGMODSIGACTSIGARTSYMPOSIUMPRINCIPLESDATABASESYSTEMS DATAINTEGRATIONATHEORETICALPERSPECTIVE SMITH 2007 1251 1255 B OVERHAGE 2011 54 60 J HRIPCSAK 2015 574 G MERELLI 2014 I MANAGINGANALYSINGINTEGRATINGBIGDATAINMEDICALBIOINFORMATICSOPENPROBLEMSFUTUREPERSPECTIVES BEYNONDAVIES 1999 211 223 P MASSEROLI 2014 S2 M SUJANSKY 2001 285 298 W MURPHY 2006 1040 S AMIAANNUALSYMPOSIUMPROCEEDINGSVOLUME2006 INTEGRATIONCLINICALGENETICDATAINI2B2ARCHITECTURE BUDINLJOSNE 2014 317 I LANCASTER 2015 957 964 O FORTIER 2017 103 105 I ZARIN 2011 852 860 D BHATTACHARYA 2014 234 239 S PANG 2016 2176 2183 C MUNGALL 2016 D712 D722 C TERRY 2014 375 376 S CROSSWELL 2012 241 242 L ANES 2012 A GUIDEMETHODOLOGICALSTANDARDSINPHARMACOEPIDEMIOLOGY EUROPEANNETWORKCENTRESFORPHARMACOEPIDEMIOLOGYPHARMACOVIGILANCEENCEPP 2013 259 265 HARRIS 2009 377 381 P MARTIN 1991 J RAPIDAPPLICATIONDEVELOPMENT SILVA 2017 L NUNES 2013 1915 1916 T SINGHAL 2001 35 43 A BASTIAOSILVAX2018X33 BASTIAOSILVAX2018X33X42 BASTIAOSILVAX2018X33XL BASTIAOSILVAX2018X33X42XL 2019-04-04T00:00:00.000Z UnderEmbargo © 2018 Elsevier B.V. All rights reserved. item S0169-2607(17)31272-5 S0169260717312725 10.1016/j.cmpb.2018.03.024 271322 2018-05-08T10:34:56.682407Z 2018-07-01 2018-07-31 true 2155517 MAIN 10 54108 849 656 IMAGE-WEB-PDF 1 gr1 8685 163 158 gr2 6036 81 219 gr3 10706 106 219 gr4 6216 47 219 gr5 10018 109 219 gr6 5671 69 219 gr7 6777 106 219 gr8 11092 97 219 gr9 6069 52 219 fx1 6714 164 208 gr1 34587 405 391 gr2 14790 189 508 gr3 71234 391 806 gr4 21984 174 806 gr5 46657 403 806 gr6 18216 179 565 gr7 34717 319 659 gr8 49559 356 806 gr9 16763 169 715 fx1 12249 189 240 gr1 234942 1793 1733 gr2 96517 837 2250 gr3 571202 1730 3567 gr4 163177 770 3567 gr5 356774 1783 3567 gr6 111841 793 2500 gr7 216457 1414 2917 gr8 395969 1577 3567 gr9 141444 749 3167 fx1 63280 835 1062 mmc1 mmc1.zip zip 324 APPLICATION COMM 4664 S0169-2607(17)31272-5 10.1016/j.cmpb.2018.03.024 Elsevier B.V. Fig. 1 The process of building a data skeleton. Data coming from different health units, or, more generally put, different data entities can be defined based on different structures. A data skeleton is a collection of metadata that can describe, in the same way, the information that exists in each of the initially different data entities. By complying to this skeleton, the data entities will expose the same information, which leads to data harmonization. Fig. 1 Fig. 2 MONTRA general view. Fig. 2 Fig. 3 Mapping from the skeleton definition into HTML5 forms. On the left, a view of some fields from the skeleton template (open-text, multiple choice, numeric and location). On the right, their rendering on the web interface. Fig. 3 Fig. 4 An example of a boolean query that can be performed in the advanced search. Fig. 4 Fig. 5 Database comparison. Similarities to the left-side data entity are shown in green, while the differences appear in red. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 5 Fig. 6 Plugin rendering lifecycle. Fig. 6 Fig. 7 Plugin development lifecycle. Fig. 7 Fig. 8 Example of a third-party plugin integrated within a possible MONTRA-based platform. The plugin is a population characteristics dashboard that shows statistic information about the data entity, in this example, EHRs. Fig. 8 Fig. 9 User token and registry id key. Fig. 9 Table 1 Question types and the respective rendering. Table 1 Component Visual render Open Open Answer, single line [input] Open-button Open Answer, single line [input] with a button to validate Open-textfield Open Answer, multi-line [textarea] Choice-yesno Yes/No Choice [radio] Choice-yesnocomment Yes/No Choice with optional comment [radio, input] Choice-yesnodontknow Yes/No/Don’t know Choice [radio] Comment Comment Only Choice Choice [radio] Choice-freeform Choice with a freeform option [radio] Choice-multiple Multiple-Choice, Multiple-Answers [checkbox] Choice-multiple-freeform Multiple-Choice, Multiple-Answers, plus freeform [checkbox, input] Publication Publication Datepicker Date choice MONTRA: An agile architecture for data publishing and discovery Luís Bastião Silva a Alina Trifan b José Luís Oliveira ⁎ b a BMD Software, Aveiro, Portugal BMD Software Aveiro Portugal b University of Aveiro, DETI/IEETA, Portugal University of Aveiro, DETI/IEETA Portugal ⁎ Corresponding author. Background and Objective Data catalogues are a common form of capturing and presenting information about a specific kind of entity (e.g. products, services, professionals, datasets, etc.). However, the construction of a web-based catalogue for a particular scenario normally implies the development of a specific and dedicated solution. In this paper, we present MONTRA, a rapid-application development framework designed to facilitate the integration and discovery of heterogeneous objects, which may be characterized by distinct data structures. Methods MONTRA was developed following a plugin-based architecture to allow dynamic composition of services over represented datasets. The core of MONTRA's functionalities resides in a flexible data skeleton used to characterize data entities, and from which a fully-fledged web data catalogue is automatically generated, ensuring access control and data privacy. Results MONTRA is being successfully used by several European projects to collect and manage biomedical databases. In this paper, we describe three of these applications scenarios. Conclusions This work was motivated by the plethora of geographically scattered biomedical repositories, and by the role they can play altogether for the understanding of diseases and of the real-world effectiveness of treatments. Using metadata to expose datasets’ characteristics, MONTRA greatly simplifies the task of building data catalogues. The source code is publicly available at Keywords Biomedical databases Data catalogues Patient registries Clinical studies 1 Introduction Data integration methodologies have become crucial in many research fields, and particularly in biomedical sciences, by enabling the discovery of knowledge that leads to new advancements in research, as well as by stimulating adoption of the necessary technical mechanisms to make such processes traceable, shareable and integrable. However, integrating biomedical data is a major challenge faced by applications that need to query across multiple autonomous data sources [1,2]. A reliable data integration system must deliver data from a variety of sources and must cope with any limitations that the data may impose [3]. Biomedical research represents a large field that can greatly benefit from data integration tools. Clinical studies, for instance, could be conducted in a faster and more extensive way if data integration systems could provide not only the integrated data of different biomedical datasets, but also the tools needed in order to discover, compare and consult the datasets that can support the study. However, due to the numerous challenges that the integration of biomedical data imposes, data sharing is not the default, but the exception [4]. One of the most common challenges to be overcome is that biomedical data sources are often hard to locate or unavailable outside of the institution that owns them. Moreover, data privacy is an important aspect to be taken into consideration and often a limiting factor when it comes to clinical information sharing [5]. Biomedical data integration systems have to provide solutions for integrating data from multiple sources without having to first load all the data into a central warehouse, since this would not only be impracticable, but would also raise many ownership and privacy issues. Another key challenge resides in finding ways of dealing with the heterogeneity, diversity and complexity of the information found in geographically scattered databases and medical healthcare units [6]. However, the value of any kind of data is greatly enhanced when it exists in a form that allows it to be integrated with other data [7]. Current efforts go towards defining data standards and models [8,9], common ontologies and semantics [10] that can support fluid data integration. In this paper we propose an alternative solution and we intend to tackle the biomedical data integration problem from a different perspective. The architecture we propose provides a different view of biomedical data integration, in which integration and sharing is made possible independently of the structure of the biomedical data entities and the type of data they contain. We introduce a Rapid Application Development system [11], designated MONTRA, which is intended as a sustainable framework, capable of enabling data linkage at a level of detail not currently available in any other systems. The core of MONTRA’s functionalities resides in a dynamic data skeleton used for the characterization of data entities, or in other words, metadata extraction. These metadata can be browsed, compared and queried by users of the system without having any private data exposed. This paper is divided into 5 more sections. In the following section we present an overview of several data catalogue solutions, especially the ones being used in the biomedical area. In Section 3.1 we describe several key requirements that should be fulfilled by a data integration system. MONTRA’s architecture is detailed in Section 4.1, while Section 5 discusses the use of this system within three different scenarios. Finally, in the Conclusions we discuss the main outcomes of this work. 2 Background Based on large and commonly supported research infrastructures, universal computational platforms capable of providing unified solutions for multiple life science needs are emerging [12]. These solutions, complemented by open source and open access policies, have the potential to sustain the development of data integration computational systems. Integrated data fosters knowledge that can support not only advances in biomedical research, but that can also significantly improve patient care, public health and administrative efficiency [13]. Most of the challenges these systems have to overcome are data size, heterogeneity, geographical location and data privacy. In this section we will review some of the current solutions for biomedical data integration. Cohort discovery platforms, as the name suggests, focus on the discovery of cohort data sources, usually related to a specific disease. The Global Alzheimers Association Interactive Network (GAAIN) [14], for instance, aims to accelerate the development of Alzheimers disease prevention, treatments and a cure. The platform fosters cohort discovery, collaboration and sharing. The i2b2 project [15] enables researchers to discover cohorts of patients using data from Electronic Health Record systems. In addition, it supports different types of queries of clinical data, including whether clinical concepts occurred at any point in a patients medical history, during a particular visit, or in a sequence of events. In the area of genomics, the Cohort Discovery [16] and the Genomics Cohort Catalogue [17] are web platforms that connect a wide range of research cohort data with resources and specimens available for further investigation. An extensive range of well phenotyped and catalogued population cohorts representing more than 600,000 subjects and including a number of ethnically homogeneous population sets can be reached through the ENGAGE Catalogue (European Network for Genetic and Genomic Epidemiology) [18]. A more sophisticated search and query data discovery platform is CafeVariome [19]. The platform is built as a shop window interface to support the discovery of genotype-phenotype data and to allow data access under three different models. Another wide topic addressed by these health science catalogues is clinical studies. CLOSER Discovery [20] is a search engine that allows researchers to explore the content of eight leading UK longitudinal studies. A similar tool, the Quebec Study Catalogue [21] is a Maelstrom Research initiative aiming to document and promote the scientific usage of large scale epidemiological studies in Quebec. With a wider impact, Clinicaltrials.gov [22] is a database of privately and publicly funded clinical studies conducted around the world. The ImmPort [23] project focuses on archiving and exchanging research and clinical data for the life science researchers. At its core, ImmPort is a data warehouse containing experimental data and metadata that describe the purpose of a study and the methods of data generation. A more advanced web platform, MOLGENIS [24] was developed from molecular genetics research and has been used in several scientific areas such as biobanking, rare disease research and patient registries. It comprises a suite of web databases for genotype, phenotype and analysis pipelines, as well as a software generator to rapidly build web databases. Among the phenotype and genotype integration platforms, the Monarch Initiative [25] provides a portal for exploration of phenotype-based similarity. For this it integrates and re-distributes cross-species gene, genotype, variant, disease, and phenotype data. With a broader approach, the Catalogue of Activities in eHealth [26] was designed to identify and aggregate global resources about clinical and genomic data. The catalogue enables both researchers and clinicians to find appropriate resources to meet the needs of their data sharing projects. Improving access, facilitating the secondary use of health data and providing technical and governance solutions are among the aims of the EMIF project [27]. To this end, a common information framework (EMIF-Platform) links up and facilitates access to diverse medical and research data sources. Similar to the efforts envisioned by the EMIF Platform, ELIXIR [28] is an intergovernmental organization that brings together life science resources from across Europe, such as databases, software tools and training materials. The organization’s goal is to coordinate them into a single infrastructure that could enable finding and sharing data and expertise. Another collaborative scientific network is the ENCePP (European Network of Centres for Pharmacoepidemiology and Pharmacovigilance), whose goal is to strengthen the post-authorisation monitoring of medicinal products, whilst bringing together relevant research centers, healthcare databases, electronic registries and existing networks across Europe [29]. Throughout the US, two of the best-known initiatives dedicated to the integration of health databases are Bridge-To-Data [30] and the Healthcare Cost and Utilization Project (HCUP) [31]. Bridge-To-Data offers services that allow users to identify key features and compare database profiles, while it also serves as an educational tool for public health research. HCUP includes the largest collection of longitudinal hospital care data in the United States. Researchers and policy-makers use HCUP data to identify, track, analyze and compare hospital statistics at the national, regional and State levels. While being able to collect virtually any type of data. REDCap (Research Electronic Data Capture) [32,33] is specifically geared to support online or offline data capture for research studies and operations. It allows researchers to define project-specific data capture and launch protocol data collection. As previously discussed, different approaches for data capture, integration and analysis of biomedical data have been designed. Some of them focus on a specific disease, health topic, or geographical location of the data sources, while others offer a wider access to distinct resources. To facilitate the construction of these repositories, we propose MONTRA, an out-of-the-box architecture for designing data integration platforms, with emphasis on biomedical data. Such platforms can cover a large spectrum of biomedical data sources, that can eventually be organized by research topic or disease. A system based on MONTRA is able to centralize heterogeneous biomedical data sources under the same web interface, thus making them accessible from a unique entry point. The main advantages of MONTRA resides in the ability of building such web platforms almost on the fly, as well as on the flexibility that this architecture offers and that we will discuss in more detail in the next sections. 3 Methods 3.1 System requirements From the solutions and challenges of the systems presented in the previous section we have found a set of requirements that should be addressed within the architecture that we present. – Flexibility - The system should be flexible enough to integrate information coming from different sources. This information might be stored on diverse physical storage devices, obey distinct data models and concern more than just one specific research topic. Addressing the variety of information that should be managed by such a system is a critical aspect that has already been identified in the literature [10]. – Data privacy - Despite enabling collaborative research and clinical data reuse, no private data can be exposed. The system must be able to perform biomedical data integration while taking into account privacy and confidentiality issues, which are often the main barriers to data sharing. – Dynamic template - In order to achieve data integration without privacy exposure, a data skeleton template has to be built for each data source. The template schema of the skeleton can be defined for each entity or can be reused for various entities, in the case of data aggregation. The most common types of entities used in biomedical sciences are Electronic Health Records (EHR), Electronic Data Capture (EDC), cohorts, medical imaging repositories and observational databases, among others. The architecture of the system should allow uses to easily create their data skeleton or a skeleton template, with the support of daily tools, such as a spreadsheet, for example. – Access control - Data privacy goes hand in hand with giving data owners fine-grained access control over their data. This can be achieved by creating different user profiles. The system should allow system administrators to dynamically modify or update any template schema. Data providers should be allowed to edit the information of their data’s skeleton. – Data discovery - In their first interaction with the system, most users are interested in finding data entities aligned with their research interests. Therefore, a search functionality should be integrated within the system. – Data comparison - Comparing data belonging to different data entities should be enabled in the system. – Data query - After identifying the data entities, querying the data that they present within the system should be possible. While real data cannot flow out of healthcare institutions boundaries, users should be able to query the high-level information that has been integrated within the system. – Data aggregation - Labeling or grouping within the same structure data entities that concern the same topic should be possible. – Data statistics - Extracting statistical measurements from the integrated data is another important requirement. Not only the capability of having a global statistics dashboard should be featured, but also the ability to extract statistical measurements for individual data entities. – User-friendly dashboard - The system should have a web dashboard with global information about the data sources that are integrated. – Security strategy - Last but not least, a security strategy should be applied and Single Sign On (SSO) implemented over the various components of the system. Several other non-functional requirements can also be defined, namely modularity and portability. 3.2 Motivational example The architecture of the system that we propose in this paper was designed for centralizing and sharing public or private data, whatever the data model or its purpose. Due to its architecture and flexibility, MONTRA can be applied to gather any type of data. However, the work that we present here was motivated by the plethora of geographically scattered biomedical datasets and the understanding of how life sciences could evolve if such datasets could be reused. There has been an exponential increase in the volume of clinical and disease-specific data throughout Europe over the last few years, and there is now a need to link these data to provide additional benefits. Even though a considerable amount of relevant patient health information does exist, it is usually contained in a variety of systems stored in different locations, which inhibits efficient access from a central place. It is very important for researchers to have access to related data, and this work presents an effort towards accomplishing the integration of data across multiple heterogeneous data sources. Generically, our research motivation was based on how to gather and integrate knowledge from M different types of registries among N different healthcare units. Our goal was to create a software solution that facilitates the integration and aggregation of biomedical data, enabling data discovery and promoting data sharing without breaking privacy rules. A basic usage scenario of MONTRA shows two different views of the motivation behind it. On one hand, we have a clinical researcher that needs to identify datasets that could support a study that (s)he intends to conduct. Instead of contacting different healthcare units and data custodians in order to find an answer, the researcher can access MONTRA and through its user-friendly interface find and compare a list of data entities that match given search criteria. The second view is that of a data custodian who wants to contribute to the advancement of biomedical sciences and is in need of a way to share the data that he or she is responsible for, without disclosing private or confidential information. MONTRA supports the data custodian by providing a flexible solution for building the skeleton of the data and sharing it without exposing patients’ private data. 4 Results 4.1 System architecture After having identified the existent lacuna in the process of integrating biomedical data sources and having gathered requirements from end-users, we constructed a flexible architecture for centralizing and sharing biomedical data coming from multiple, independent, heterogeneous sources, as well as a user-friendly interface allowing interaction with the data. The data sources that are characterized within the system contain the full data, while the dashboard available through MONTRA provides an integrated view of the skeleton of these underlying sources. The heterogeneous data sources can be of any type, from EHR and EDC records, cohorts or even patient files. The use of a dynamic data skeleton for data characterization and integration represents a layer of abstraction that does not depend on data models or physical data supports. The core of our approach, which we refer to as the skeleton, is represented in Fig. 1 . The skeleton represents a collection of metadata that best encapsulate the real data, which has to remain private. The skeleton definition is a straightforward operation that can be done by any data custodian and can be saved as a spreadsheet file and submitted through the web interface. If data aggregation is intended, the same skeleton template can be reused for the characterization of multiple data sources. Different data entities complying to the same skeleton template will have a common representation within the platform. Each data source, that might belong to different institutions, will be characterized by the same fields, which, in a more general way, represent answers to the same questions defined in the skeleton template. By using a common structure as a metadata skeleton, distinct data sources converge to a homogeneous exposure of their metadata. A simplified overview of MONTRA is summarized in Fig. 2 . The metadata encapsulated in the skeleton of a data entity is also referred to in the system as a database. Users of MONTRA can browse, search, compare and query databases listed in the web interface. MONTRA was planned as a Rapid Application Development (RAD) [34] system, to allow fast deployment for a specific use case, and by following the Agile practices in software development [35]. RAD systems, unlike conventional ones, evolve as the project advances. They do not rely on rigid initial specifications, but are rather continuously adjustable in order to fit new requirements that arise as the project progresses and new knowledge is generated. 4.2 Data skeleton To integrate distinct types of data entities from different sources, we created a system that supports several schemas. Taking into account that the broader concept of skeleton is a set of aggregated data about a data entity, it could be translated into questionnaires or aggregated data that could be imported in CSV files. Thinking of the skeleton as a questionnaire, we have implemented a schema that contains questions and answers about one or more data entities. The key idea behind the skeleton schema is that we do not know what kind of questions or data will be introduced. Thus, our implementation needs to be flexible and not bound to a fixed set of questions. To accomplish this, we developed a dynamic questionnaire with multiple groups of questions, which can have dependencies between them. The questionnaire represents the type of data entity that will be skeletonized. Each questionnaire is composed of groups of questions. A group of questions is denominated a QuestionSet. Then, each QuestionSet is formed of Questions that can be of any type. The implementation of the answers to these questions is very flexible and extensible. Each skeleton schema contains several QuestionSets. Within these, each question can be individually defined, as well as the answer type - Table 1 . The skeleton schema can be developed using a common spreadsheet to easily reach the end-users, usually data owners. If the skeleton is intended as a reusable template, the skeleton does not include the answers to the questions that have been defined in it. The skeleton is uploaded once to create the catalogue template, and the questions are rendered as shown in Fig. 3 . Each data custodian will fill in online the respective database or other entity information. If the skeleton is intended for just one database, the answers can be included and the complete spreadsheet can be uploaded through the web interface. In both situations, the information defined in the skeleton can be edited at any time. An import service is available to automatically load both a reusable skeleton template or a filled-in skeleton. Some of them are generic, open text fields or numeric ones, while others allow the users to restrict the answer to a question to a specific type. Such fields are, for example, the date field, publication field or the location. A date field is visually rendered as a calendar drop-down that eases the user’s task of typing a specific date. Similarly, locations can be chosen from a dynamic drop-down list that includes countries, regions, districts and cities. The Publication question, as named in Table 1 is based on a web widget designed for this purpose. Data custodians can simply add one or a list of Pubmed ids and the widget fetches the information from the scientific publications identified by those ids. It does not only fetch the title and metadata about the publication, but also its abstract. Thereafter, the abstract is annotated using Becas [36], a web application service that provides biomedical concept identification. With this annotation, the user will have a better notion of the concepts identified in the database publication and will be linked to other relevant knowledge resources. We intended MONTRA to be as open and standardless as possible. However, we maintain the possibility for the end-user to restrict the answer to a set of pre-defined values or answer types. 4.3 CRUD operations Within software engineering, the acronym CRUD stands for Create, Read, Update and Delete entities. These operations are the four basic functions that are provided by relational database applications. Within the data catalogue created with a particular schema based on MONTRA, CRUD operations are automatically available for registered users, i.e. they can create, read, update and delete their own registers. A data custodian defines the data skeleton and fills in the respective information in an Excel file, as described above. After uploading the file through the web interface, its registry is displayed, similar to the information shown in Fig. 3. Furthermore, the user interface allows viewing, searching, modifying and deleting information through computer-based forms. With the registry online, the data custodian can decide if the registry is submitted as a draft or as public. Draft registries are only visible to the data custodian. At any time he or she can edit the information that was filled in. A registry can have more than one administrator. By default, the register owner is user submitting it, but ownership and administration privileges can be shared with other users. In addition to these operations, a private link of a registry can be created and shared with non-registered users. The web interface generates a private link which can be sent directly from it to an external user. The non-registered user will only have browsing privileges for that particular registry. 4.4 Searching services Within the system, users can have access to all the catalogue entries, according to their user profile. They are able to browse the registers but also to search for specific free text terms. A second search feature, which we called advanced search, is also available. This feature allows users to specify a more fine-grained search following the skeleton schema. To support the searching backend, all the questions and answers are indexed by Solr. 1 1 A retrieval model was also built to score, sort and improve the quality of results. For instance, in a yes/no question such as “BMI measurements?”, if the answer for a particular register is “yes”, then, if a query by “BMI” is made, this entry should be retrieved. This means that not only the answer should be analyzed, but also the question, according to its type. The developed backend was also used to give suggestions in the free text search. The autocomplete suggestions are supported by another core (index schema) of Solr. For implementation, we rely on a tokenizer with an edge filter n-gram between 1 and 25. This is particularly useful to accelerate the process of giving accurate suggestions while the user is typing the text. Since the MONTRA architecture is data-agnostic, i.e. it does not assume any particular end-user application, the Search functionality was designed as a flexible retrieving approach, to maximize the recall over the precision. Additional complexity can be provided by the advanced search by using boolean logic terms. When this type of search is performed a boolean query is created, e.g. a query that includes a relation between two or more concepts. With these queries, the user can combine multiple search criteria to search for specific terms in each question that comprises the registry skeleton. The search engine understands the combination of AND and OR terms to filter content that will be displayed in the search results (Fig. 4 ). Additionally, the user can save boolean queries within his account and retrieve and reuse them at any time. Apart from retrieving and searching for terms of interest, it is also possible to compare several skeletons, taking into consideration a similarity metric based on the Levenshtein distance [37] for textual information, and on the cosine similarity [38] between records. The comparison feature enables the user to identify metadata similarities across distinct data entities. The results highlight in red the information that varies, while the information that is common appears in green (Fig. 5 ). 4.5 Plugins integration MONTRA supports third-party components and allows users, with adequate permissions, to extend its functionalities without having to deal with its base code. To accomplish this strategy, a microkernel architecture was developed, providing the platform’s core to which several components can be dynamically added, enriching the overall system’s functionality. Two different types of components, or plugins, can be added to the platform by third-party developers: – Global: they provide general services for a MONTRA instance. Once added to the platform, these plugins will be available on the user dashboard and/or in the main menu. – Registry: this type of plugins provides added functionalities for each data record, and as such will be available as new services over the skeleton. These plugins can be further divided into two types: third-party plugins and fully-fledged plugins. Third-party plugins are full web applications that are linked to the system, through the navigation menu. They usually provide a completely different functionality. The main goal of these plugins is to integrate their application features in MONTRA’s environment. Fully-fledged plugins are internal extensions and they provide additional data services within the system. This type of plugin allows the development of decoupled services that can be easily attached to the main core module. To allow its integration in the same web interface, each plugin is supported by a re-rendering method (Fig. 6 ). After its initial representation, in the browser, each time an event occurs the plugin updates its content using self.html() or self.append(). When this is done, self.refresh() has to be executed to update the visualization. The plugin development process follows a specific lifecycle that is managed internally within MONTRA services (Fig. 7 ). The first step is to obtain the administrator’s approval to develop plugins. After that, the user can create any number of plugins. Each plugin can have a series of versions, although users can develop and live-preview their plugin versions during development. Whenever a plugin version is deemed ready for production, it must be submitted for the administrator’s approval to become available. Any further changes to an already approved version will remove the approval status, and the plugin will have to be submitted again. This workflow assures the quality control of components added by external developers. Through this plugin functionality, a MONTRA-based platform can be extended with external components, which may provide, beyond the original metadata, complementary information of the data entities, such as aggregated data or summarized views. Such dashboards can include, for instance, the number of patients per age and per year, the average time of follow-up and many other statistics. Taking the example of EHR data, a population characteristics dashboard can be integrated as a third-party plugin (Fig. 8 ). 4.6 Role-Based Access Control A Role-Based Access Control (RBAC) system is included in MONTRA to guarantee that proper access constraints are in place. Besides the users, groups and roles can be created to define permissions to access data and services. A regular User can register in the system and, once accepted, s/he can browse the list of existing records. This user profile has access to the information available in the catalogue, e.g. can search, query, compare or export data. A data Owner is a regular user who, at some point, added new entries into the catalogue. By entering data, that user is then responsible for the management and access control of those entries (can edit, decide about public/private permissions, share owner responsibilities, private share, ...). The system Administrator is able to create and edit skeleton schemas, validate user registrations, and the overall management of the platform. A Developer is a user who is allowed to create several types of new components, by following the plugins workflow. 4.7 RESTful API MONTRA integrates a RESTful API which provides a set of programmatic endpoints that can be consulted by third party applications. The main idea behind the Web API is that other applications can send data to MONTRA, in the format of key-value pairs, containing extra metadata of the registry. It is a simple mechanism to dynamically add metadata information in each entry. A double key schema allows controlling the access to this API. To use a web service, third-party applications need to know two distinct tokens: the user token and the registry key. The access to the information is granted through the combination of both keys, being then possible to send extra data to the specific registry IDs. These tokens are available within MONTRA’s web interface, in the user workspace. (Fig. 9 ). The web service accepts information using key-value pairs, and associates it to the respective registry. An example of a JSON format that the API can receive: 4.8 Software technologies MONTRA is written in Python 2.7.6, 2 2 using Django 1.4.5, 3 3 a framework that encourages rapid development and clean programmatic design. However, a considerable part of the development was made in HTML5, CSS and JavaScript, namely the interface and the end-user interaction. Furthermore, in order to improve the web design quality, we have adopted the Bootstrap2 4 4 framework, a front-end framework for web development. To assure the system’s performance when dealing with concurrent requests a local cache is maintained, based on memcached3. 5 5 Moreover, several tasks, such as indexing the skeleton in Solr, can take too much time, making users wait for the operation to complete. To manage these tasks, we rely on a queue message system (Rabbit MQ4 6 6 ) and on the Celery 7 7 backend to execute them in the background. 5 Discussion The solution that we propose for integrating a vast range of biomedical data coming from different, disperse institutions resides on using the same skeleton template for the same family of data sources (e.g. EHR repositories, disease-specific cohorts, etc.). A group, or a collection of data sources or, more generically, data entities, that are characterized by the same skeleton template will naturally converge toward a common structure in what concerns the information that it exposes. Multiple similar data sources can be aggregated, or simply put, grouped as an entity that respects the same metadata skeleton and that is managed by one or multiple “skeleton managers”. The manager can accept or reject the publishing of a data source or even request for changes in answers submitted by a given data custodian. The task of designing the skeleton template most certainly involves the collaboration of data custodians, but once having the skeleton decided, the process of filling in the data is straightforward. The effort spent into this task is not too significant compared to the final result of having a data source discoverable, shareable and overall useful for the research community. MONTRA has the potential to simplify the setup of web-based catalogues, for many distinct scenarios and applications. We have been applying this framework to build the EMIF Catalogue, 8 8 [27] an online platform that aims to be a marketplace where data custodians can publish and share different levels of information about their clinical databases, while biomedical researchers can search for databases that fulfill their particular study requirements. In this scenario, the “Database” is the main entity characterized, typically EHR or cohorts. Currently, the EMIF Catalogue supports several distinct projects, combining, for instance, data available in pan-European EHR and Alzheimer cohorts. Currently, the EMIF Platform integrates 374 distinct databases and around 700 users. Another example where MONTRA is being used is in maintaining a catalogue of neuroradiology clinical cases. 9 9 In this portal, which describes patient information, several plugins were added to include also the search and visualization of a Picture Archiving and Communication System (PACS) archive. This repository includes interesting case studies from the Portuguese Society of Neuroradiology. It is an annotated Web-based medical imaging repository used for clinical research and academic purposes. A third scenario, currently being deployed, is to support the gathering and maintenance of Case Report Forms (CRF) together with omics data, for a cohort of patients suffering from heart failure with preserved injection. 6 Conclusions The main motivation for this work was to facilitate the setup of web data catalogues for distinct applications. MONTRA, the presented system, is based on dynamic skeletons which allow describing any kind of data, being automatically used to create the data store and to build the web user interface, without the need to create a single line of code. This framework is being used and validated in several applications, such as the EMIF European project, to allow the presentation, discovery and share of biomedical data sources. Acknowledgment This work has received support from the EU/EFPIA Innovative Medicines Initiative Joint Undertaking (EMIF grant n. 115372), and from the NETDIAMOND project (POCI-01-0145-FEDER-016385). We would like to acknowledge all of the team members that have contributed to the development of this architecture along the time. A list of the main contributors can be found here: Supplementary material Supplementary material associated with this article can be found, in the online version, at 10.1016/j.cmpb.2018.03.024 Appendix A Supplementary materials Supplementary Data S1 Supplementary Raw Research Data. This is open data under the CC BY license Supplementary Data S1 References [1] A. Halevy A. Rajaraman J. Ordille Data integration: The teenage years Proceedings of the 32nd International Conference on Very Large Data Bases 2006 VLDB Endowment 9 16 [2] V. Lapatas M. Stefanidakis R.C. Jimenez A. Via M.V. Schneider Data integration in biological research: an overview J. Biol. Res. Thessaloniki 22 1 2015 9 [3] P. Lopes L.B. Silva J.L. Oliveira Challenges and Opportunities for Exploring Patient-Level Data 2015 BioMed Research International [4] T.A. van Schaik N.V. Kovalevskaya E. Protopapas H. Wahid F.G.G. Nielsen The need to redefine genomic data sharing: A focus on data accessibility Appl. Transl. Genomics 3 4 2014 100 104 Global Sharing of Genomic Knowledge in a Free Market [5] D. Teodoro R. Choquet E. Pasche J. Gobeill C. Daniel Patrick ruch, and christian lovis. biomedical data management: a proposal framework Stud. Health Technol. Inform. 150 2009 175 179 [6] M. Lenzerini Data integration: A theoretical perspective Proceedings of the 21st ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems 2002 ACM New York, NY, USA 233 246 [7] B. Smith M. Ashburner C. Rosse J. Bard W. Bug W. Ceusters L.J. Goldberg K. Eilbeck A. Ireland C.J. Mungall N. Leontis P. Rocca-Serra A. Ruttenberg S.A. Sansone R.H. Scheuermann N. Shah P.L. Whetzel S. Lewis The OBO foundry: Coordinated evolution of ontologies to support biomedical data integration Nat. Biotechnol. 25 11 2007 1251 1255 [8] J.M. Overhage P.B. Ryan C.G. Reich A.G. Hartzema P.E. Stang Validation of a common data model for active safety surveillance research J. Am. Med. Inf. Assoc. 19 1 2011 54 60 [9] G. Hripcsak J.D. Duke N.H. Shah C.G. Reich V. Huser M.J. Schuemie M.A. Suchard R.W. Park I.C.K. Wong P.R. Rijnbeek Observational health data sciences and informatics (OHDSI): opportunities for observational researchers Stud. Health Technol. Inform. 216 2015 574 [10] I. Merelli H. Pérez-Sánchez S. Gesing D. DAgostino Managing, Analysing, and Integrating Big Data in Medical Bioinformatics: Open Problems and Future Perspectives 2014 BioMed Research International [11] P. Beynon-Davies C. Carne H. Mackay D. Tudhope Rapid application development (RAD): an empirical review Eur. J. Inf. Syst. 8 3 1999 211 223 [12] M. Masseroli B. Mons E. Bongcam-Rudloff S. Ceri A. Kel F. Rechenmann F. Lisacek P. Romano Integrated bio-search: Challenges and trends for the integration, search and comprehensive processing of biological information BMC Bioinform. 15 1 2014 S2 [13] W. Sujansky Heterogeneous database integration in biomedicine J. Biomed. Inform. 34 4 2001 285 298 [14] The global alzheimer’s association interactive network (GAAIN). alzheimer’s & dementia, 2015, 11, 7, P121. [15] S.N. Murphy M.E. Mendis D.A. Berkowitz I. Kohane H.C. Chueh Integration of clinical and genetic data in the i2b2 architecture AMIA Annual Symposium Proceedings, volume 2006 2006 American Medical Informatics Association 1040 [16] W. Hersh, S. Wu, IR meets EHR: A patient cohort discovery task. [17] Genomics cohort catalogue, Accessed: 2017-07-26. [18] I. Budin-Ljøsne J. Isaeva B.M. Knoppers A.M. Tassé H.y. Shen M.I. McCarthy J.R. Harris E. Consortium Data sharing in large research consortia: Experiences and recommendations from ENGAGE Eur. J. Hum. Genet. 22 3 2014 317 [19] O. Lancaster T. Beck D. Atlan M. Swertz D. Thangavelu C. Veal R. Dalgleish A.J. Brookes Cafe variome: General-purpose software for making genotype phenotype data discoverable in restricted or open access contexts Hum. Mutat. 36 10 2015 957 964 [20] CLOSER DISCOVERY-a resource for social science researchers using longitudinal data, Accessed: 2017-07-26. [21] I. Fortier P. Raina E.R.V.d. Heuvel L.E. Griffith C. Craig M. Saliba D. Doiron R.P. Stolk B.M. Knoppers V. Ferretti Maelstrom research guidelines for rigorous retrospective data harmonization Int. J. Epidemiol. 46 1 2017 103 105 [22] D.A. Zarin T. Tse R.J. Williams R.M. Califf N.C. Ide The clinicaltrials. gov results database update and key issues N. Engl. J. Med. 364 9 2011 852 860 [23] S. Bhattacharya S. Andorf L. Gomes P. Dunn H. Schaefer J. Pontius P. Berger V. Desborough T. Smith J. Campbell Immport: disseminating data to the public for the future of immunology Immunol. Res. 58 2–3 2014 234 239 [24] C. Pang D. van Enckevort M. de Haan F. Kelpin J. Jetten D. Hendriksen T. de Boer B. Charbon E. Winder K.J. van der Velde Molgenis/connect: a system for semi-automatic integration of heterogeneous phenotype data with applications in biobanks Bioinformatics 32 14 2016 2176 2183 [25] C.J. Mungall J.A. McMurry S. Köhler J.P. Balhoff C. Borromeo M. Brush S. Carbon T. Conlin N. Dunn M. Engelstad The monarch initiative: an integrative data and analytic platform connecting phenotypes to genotypes across species Nucleic Acids Res. 45 D1 2016 D712 D722 [26] S.F. Terry The global alliance for genomics & health Genet. Test. Mol. Biomarkers 18 6 2014 375 376 [27] EMIF - european medical information platform, Accessed: 2017-07-26. [28] L.C. Crosswell J.M. Thornton Elixir: a distributed infrastructure for european biological data Trends Biotechnol. 30 5 2012 241 242 [29] A.M. Anes A. Arana K. Blake J. Bonhoeffer S. Evans A. Fourrier-Réglat J. Hallas U. Kirchmayer V. Kiri O. Klungel The european network of centres for pharmacoepidemiology and pharmacovigilance (ENCePP) Guide on Methodological Standards in Pharmacoepidemiology volume 2014 2012 [30] Bridge to data, Accessed: 2017-06-16. [31] HCUP databases. agency for healthcare research and quality. rockville, MD, 2014. [32] Procurement of shared data instruments for research electronic data capture (REDCap) J. Biomed. Inform. 46 2 2013 259 265 [33] P.A. Harris R. Taylor R. Thielke J. Payne N. Gonzalez J.G. Conde Research electronic data capture (REDCap)-a metadata-driven methodology and workflow process for providing translational research informatics support J. Biomed Inform. 42 2 2009 377 381 [34] J. Martin Rapid Application Development 1991 Macmillan Publishing Co., Inc. Indianapolis, IN, USA [35] L.B. Silva R.C. Jimenez N. Blomberg J.L. Oliveira General guidelines for biomedical software development F1000Research 6 2017 [36] T. Nunes D. Campos S. Matos J.L. Oliveira Becas: biomedical concept recognition services and visualization Bioinformatics 29 15 2013 1915 1916 [37] M. Gilleland, Levenshtein distance, in three flavors, 2009, Merriam Park Software: [38] A. Singhal Modern information retrieval: a brief overview IEEE Data Eng. Bull. 24 4 2001 35 43 "
    },
    {
        "doc_title": "COEUS 2.0: Automated platform to integrate and publish biomedical data as nanopublications",
        "doc_scopus_id": "85045083838",
        "doc_doi": "10.1049/iet-sen.2016.0325",
        "doc_eid": "2-s2.0-85045083838",
        "doc_date": "2018-04-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Biomedical domain",
            "Important features",
            "Open source application",
            "Research communities",
            "Resource description framework",
            "Seamless integration",
            "Second generation",
            "System functionality"
        ],
        "doc_abstract": "© The Institution of Engineering and Technology 2017.Publishing, analysing or properly accessing the abundant information resulting largely from experimental studies in the biomedical domain are current challenges for the research community. Problems with the extraction of relevant information, redundant data, and lack of associations or provenance are good examples of the main concerns. The innovative nanopublication publishing strategy tries to overcome these issues by representing the essential pieces of publishable information on the Semantic Web. However, existing methods to create these Resource Description Framework-based data snippets are based on complex scripting procedures, hindering their use by the community. Therefore, novel and automated strategies are needed to explore the evident value of nanopublications and to enable data attribution mechanisms, an important feature for data owners. To solve these challenges, the authors introduce the second generation of the COEUS open-source application framework (http://bioinformatics.ua.pt/coeus/), an automated platform to integrate heterogeneous scientific outcomes into nanopublications. This results in seamless integration, making data accessible and citable at the same time. No additional scripting methods are needed. A validation of a nanopublishing pipeline is described to demonstrate the system functionalities, integrating and publishing common biomedical achievements into the Semantic Web ecosystem.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A methodology to perform semi-automatic distributed EHR database queries",
        "doc_scopus_id": "85051747869",
        "doc_doi": "10.5220/0006579701270134",
        "doc_eid": "2-s2.0-85051747869",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Clinical data",
            "Database queries",
            "Digital datas",
            "Electronic health",
            "Electronic health record (EHRs)",
            "Pharmaceutical research",
            "Semi-automatics",
            "Social issues"
        ],
        "doc_abstract": "Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.The proliferation of electronic health databases has resulted in the existence of a wide collection of diversified clinical digital data. These data are fragmented over dispersed databases in different clinical silos around the world. The exploration of these electronic health records (EHRs) is essential for clinical and pharmaceutical research and, therefore, solutions for secure sharing of information across different databases are needed. Although several partial solutions have been proposed over the years, data sharing and integration has been hindered by many ethical, legal and social issues. In this paper, we present a methodology to perform semiautomatic queries over longitudinal clinical data repositories, where every data custodian maintains full control of data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A computational pipeline for sepsis patients’ stratification and diagnosis",
        "doc_scopus_id": "85051711226",
        "doc_doi": "10.5220/0006579104080413",
        "doc_eid": "2-s2.0-85051711226",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Acquisition device",
            "Early diagnosis",
            "Mortality rate",
            "Public health issues"
        ],
        "doc_abstract": "Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reservedSepsis is still a little acknowledged public health issue, despite its increasing incidence and the growing mortality rate. In addition, a clear diagnosis can be lengthy and complicated, due to highly variable symptoms and non-specific criteria, causing the disease to be diagnosed and treated too late. This paper presents the HemoSpec platform, a decision support system which, by collecting and automatically processing data from several acquisition devices, can help in the early diagnosis of sepsis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A modular workflow management framework",
        "doc_scopus_id": "85050956922",
        "doc_doi": "10.5220/0006583104140421",
        "doc_eid": "2-s2.0-85050956922",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Complex processing",
            "Computational process",
            "Scientific workflows",
            "Task management system",
            "Task-scheduling",
            "Web based platform",
            "Work-flow systems",
            "Workflow managements"
        ],
        "doc_abstract": "Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reservedTask management systems are crucial tools in modern organizations, by simplifying the coordination of teams and their work. Those tools were developed mainly for task scheduling, assignment, follow-up and accountability. Then again, scientific workflow systems also appeared to help putting together a set of computational processes through the pipeline of inputs and outputs from each, creating in the end a more complex processing workflow. However, there is sometimes a lack of solutions that combine both manually operated tasks with automatic processes, in the same workflow system. In this paper, we present a web-based platform that incorporates some of the best functionalities of both systems, addressing the collaborative needs of a task manager with well-structured computational pipelines. The system is currently being used by a European consortium for the coordination of clinical studies.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Biomedical informatics - How to choose the best tool for each task",
        "doc_scopus_id": "85046548154",
        "doc_doi": "10.3233/978-1-61499-852-5-406",
        "doc_eid": "2-s2.0-85046548154",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Benchmarking methodology",
            "Bioinformatics software",
            "Biomedical informatics",
            "Clinical practices",
            "End users",
            "Performance indicators",
            "Software benchmarking",
            "Benchmarking",
            "Computational Biology",
            "Humans",
            "Software"
        ],
        "doc_abstract": "© 2018 European Federation for Medical Informatics (EFMI) and IOS Press.The ever-increasing number of bioinformatics software tools that are publicly available, is leading to greater expectations about its regular use in clinical practice. However, from the end-users' perspective, they face many time the challenge of choosing the right tool for each task, from a panoply of solutions that have been developed over the years. In this paper, we propose a benchmarking methodology, based on a set of performance indicators, which can be used to identify the best methods and tools for each particular use case, both in research as in clinical practice.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A methodology for fine-grained access control in exposing biomedical data",
        "doc_scopus_id": "85046546740",
        "doc_doi": "10.3233/978-1-61499-852-5-561",
        "doc_eid": "2-s2.0-85046546740",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Biomedical data",
            "Biomedical data integration",
            "Clinical information",
            "Data visibility",
            "Fine grained",
            "Level of detail",
            "Multiple levels",
            "Research plans",
            "Biomedical Research",
            "Humans",
            "Research Design",
            "Statistics as Topic"
        ],
        "doc_abstract": "© 2018 European Federation for Medical Informatics (EFMI) and IOS Press.Biomedical data integration and processing is a very sensitive issue and a main barrier for research, since it normally implies dealing with private clinical information. To overcome this problem, we propose a solution based on multiple levels of data visibility, combined with a fine-grained access control over the shared data. Through our proposal, on one hand, data custodians can decide the level of detail at which they want to share data, in a flexible manner that can be adjusted along the time. On the other hand, adequate permissions are provided to the users that want to access the data, according to their role and research plan.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Fighting fire with fire: Computational prediction of microbial targets for bacteriocins",
        "doc_scopus_id": "85045894842",
        "doc_doi": "10.1007/978-3-319-78723-7_19",
        "doc_eid": "2-s2.0-85045894842",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bacteriocins",
            "Computational predictions",
            "Computer aided methods",
            "Experimental methods",
            "Interactome",
            "Microbiome",
            "Potential drug targets",
            "Protein-protein interactions"
        ],
        "doc_abstract": "© 2018, Springer International Publishing AG, part of Springer Nature.Recently, we have witnessed the emergence of bacterial strains resistant to all known antibacterials. Due to several limitations of existing experimental methods, these events justify the need of computer-aided methods to systematically and rationally identify new antibacterial agents. Here, we propose a methodology for the systematic prediction of interactions between bacteriocins and bacterial protein targets. The protein-bacteriocin interactions are predicted using a mesh of classifiers previously developed by the authors, allowing the identification of the best bacteriocin candidates for antibiotic use and potential drug targets.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Open-source electronic health record systems for low-resource settings: Systematic review",
        "doc_scopus_id": "85049575717",
        "doc_doi": "10.2196/medinform.8131",
        "doc_eid": "2-s2.0-85049575717",
        "doc_date": "2017-10-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2017 JMIR Publications Inc.. All right reserved.Background: Despite the great impact of information and communication technologies on clinical practice and on the quality of health services, this trend has been almost exclusive to developed countries, whereas countries with poor resources suffer from many economic and social issues that have hindered the real benefits of electronic health (eHealth) tools. As a component of eHealth systems, electronic health records (EHRs) play a fundamental role in patient management and effective medical care services. Thus, the adoption of EHRs in regions with a lack of infrastructure, untrained staff, and ill-equipped health care providers is an important task. However, the main barrier to adopting EHR software in low- and middle-income countries is the cost of its purchase and maintenance, which highlights the open-source approach as a good solution for these underserved areas. Objective: The aim of this study was to conduct a systematic review of open-source EHR systems based on the requirements and limitations of low-resource settings. Methods: First, we reviewed existing literature on the comparison of available open-source solutions. In close collaboration with the University of Gondar Hospital, Ethiopia, we identified common limitations in poor resource environments and also the main requirements that EHRs should support. Then, we extensively evaluated the current open-source EHR solutions, discussing their strengths and weaknesses, and their appropriateness to fulfill a predefined set of features relevant for low-resource settings. Results: The evaluation methodology allowed assessment of several key aspects of available solutions that are as follows: (1) integrated applications, (2) configurable reports, (3) custom reports, (4) custom forms, (5) interoperability, (6) coding systems, (7) authentication methods, (8) patient portal, (9) access control model, (10) cryptographic features, (11) flexible data model, (12) offline support, (13) native client, (14) Web client,(15) other clients, (16) code-based language, (17) development activity, (18) modularity, (19) user interface, (20) community support, and (21) customization. The quality of each feature is discussed for each of the evaluated solutions and a final comparison is presented. Conclusions: There is a clear demand for open-source, reliable, and flexible EHR systems in low-resource settings. In this study, we have evaluated and compared five open-source EHR systems following a multidimensional methodology that can provide informed recommendations to other implementers, developers, and health care professionals. We hope that the results of this comparison can guide decision making when needing to adopt, install, and maintain an open-source EHR solution in low-resource settings.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Erratum: Correction: Phenotypic heterogeneity promotes adaptive evolution (PLoS biology (2017) 15 5 (e2000644))",
        "doc_scopus_id": "85063713156",
        "doc_doi": "10.1371/journal.pbio.1002607",
        "doc_eid": "2-s2.0-85063713156",
        "doc_date": "2017-06-01",
        "doc_type": "Erratum",
        "doc_areas": [
            {
                "area_name": "Neuroscience (all)",
                "area_abbreviation": "NEUR",
                "area_code": "2800"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "[This corrects the article DOI: 10.1371/journal.pbio.2000644.].",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Intelligent generator of big data medical imaging repositories",
        "doc_scopus_id": "85022227696",
        "doc_doi": "10.1049/iet-sen.2016.0191",
        "doc_eid": "2-s2.0-85022227696",
        "doc_date": "2017-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Design and implementations",
            "Imaging modality",
            "Performance and scalabilities",
            "Real environments",
            "Research purpose",
            "Test bed environment",
            "Validation process"
        ],
        "doc_abstract": "© The Institution of Engineering and Technology 2017.The production of medical imaging data has grown tremendously in the last decades. Nowadays, even small institutions produce a considerable amount of studies. Furthermore, the general trend in new imaging modalities is to produce more data per examination. As a result, the design and implementation of tomorrow's storage and communication systems must deal with big data issues. The research on technologies to cope with big data issues in large scale medical imaging environments is still in its early stages. This is mostly due to the difficulty of implementing and validating new technological approaches in real environments, without interfering with clinical practice. Therefore, it is crucial to create test bed environments for research purposes. This study proposes a methodology for creating simulated medical imaging repositories, based on the indexing of model datasets, extraction of patterns and modelling of study production. The system creates a model from a real-world repository's representative time window and expands it according to on-going research needs. In addition, the solution provides distinct approaches to reducing the size of the generated datasets. The proposed system has already been used by other research projects in validation processes that aim to assess the performance and scalability of developed systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Phenotypic heterogeneity promotes adaptive evolution",
        "doc_scopus_id": "85020231053",
        "doc_doi": "10.1371/journal.pbio.2000644",
        "doc_eid": "2-s2.0-85020231053",
        "doc_date": "2017-05-09",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Neuroscience (all)",
                "area_abbreviation": "NEUR",
                "area_code": "2800"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            }
        ],
        "doc_keywords": [
            "Adaptation, Biological",
            "Biological Evolution",
            "Drug Resistance, Fungal",
            "Genes, Fungal",
            "Mutation",
            "Phenotype",
            "Saccharomyces cerevisiae"
        ],
        "doc_abstract": "© 2017 Bódi et al.Genetically identical cells frequently display substantial heterogeneity in gene expression, cellular morphology and physiology. It has been suggested that by rapidly generating a subpopulation with novel phenotypic traits, phenotypic heterogeneity (or plasticity) accelerates the rate of adaptive evolution in populations facing extreme environmental challenges. This issue is important as cell-to-cell phenotypic heterogeneity may initiate key steps in microbial evolution of drug resistance and cancer progression. Here, we study how stochastic transitions between cellular states influence evolutionary adaptation to a stressful environment in yeast Saccharomyces cerevisiae. We developed inducible synthetic gene circuits that generate varying degrees of expression stochasticity of an antifungal resistance gene. We initiated laboratory evolutionary experiments with genotypes carrying different versions of the genetic circuit by exposing the corresponding populations to gradually increasing antifungal stress. Phenotypic heterogeneity altered the evolutionary dynamics by transforming the adaptive landscape that relates genotype to fitness. Specifically, it enhanced the adaptive value of beneficial mutations through synergism between cell-to-cell variability and genetic variation. Our work demonstrates that phenotypic heterogeneity is an evolving trait when populations face a chronic selection pressure. It shapes evolutionary trajectories at the genomic level and facilitates evolutionary rescue from a deteriorating environmental stress.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A De-Identification Pipeline for Ultrasound Medical Images in DICOM Format",
        "doc_scopus_id": "85017457466",
        "doc_doi": "10.1007/s10916-017-0736-1",
        "doc_eid": "2-s2.0-85017457466",
        "doc_date": "2017-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Confidentiality",
            "Data Anonymization",
            "Image Processing, Computer-Assisted",
            "Information Dissemination",
            "Privacy",
            "Software",
            "Ultrasonography"
        ],
        "doc_abstract": "© 2017, Springer Science+Business Media New York.Clinical data sharing between healthcare institutions, and between practitioners is often hindered by privacy protection requirements. This problem is critical in collaborative scenarios where data sharing is fundamental for establishing a workflow among parties. The anonymization of patient information burned in DICOM images requires elaborate processes somewhat more complex than simple de-identification of textual information. Usually, before sharing, there is a need for manual removal of specific areas containing sensitive information in the images. In this paper, we present a pipeline for ultrasound medical image de-identification, provided as a free anonymization REST service for medical image applications, and a Software-as-a-Service to streamline automatic de-identification of medical images, which is freely available for end-users. The proposed approach applies image processing functions and machine-learning models to bring about an automatic system to anonymize medical images. To perform character recognition, we evaluated several machine-learning models, being Convolutional Neural Networks (CNN) selected as the best approach. For accessing the system quality, 500 processed images were manually inspected showing an anonymization rate of 89.2%. The tool can be accessed at https://bioinformatics.ua.pt/dicom/anonymizer and it is available with the most recent version of Google Chrome, Mozilla Firefox and Safari. A Docker image containing the proposed service is also publicly available for the community.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SCALEUS: Semantic Web Services Integration for Biomedical Applications",
        "doc_scopus_id": "85013301875",
        "doc_doi": "10.1007/s10916-017-0705-8",
        "doc_eid": "2-s2.0-85013301875",
        "doc_date": "2017-04-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Databases, Factual",
            "Humans",
            "Information Storage and Retrieval",
            "Medical Informatics",
            "Semantics",
            "Systems Integration"
        ],
        "doc_abstract": "© 2017, Springer Science+Business Media New York.In recent years, we have witnessed an explosion of biological data resulting largely from the demands of life science research. The vast majority of these data are freely available via diverse bioinformatics platforms, including relational databases and conventional keyword search applications. This type of approach has achieved great results in the last few years, but proved to be unfeasible when information needs to be combined or shared among different and scattered sources. During recent years, many of these data distribution challenges have been solved with the adoption of semantic web. Despite the evident benefits of this technology, its adoption introduced new challenges related with the migration process, from existent systems to the semantic level. To facilitate this transition, we have developed Scaleus, a semantic web migration tool that can be deployed on top of traditional systems in order to bring knowledge, inference rules, and query federation to the existent data. Targeted at the biomedical domain, this web-based platform offers, in a single package, straightforward data integration and semantic web services that help developers and researchers in the creation process of new semantically enhanced information systems. SCALEUS is available as open source at http://bioinformatics-ua.github.io/scaleus/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automated nanopublications generation from biomedical literature",
        "doc_scopus_id": "85018711529",
        "doc_doi": "10.1109/ENBENG.2017.7889462",
        "doc_eid": "2-s2.0-85018711529",
        "doc_date": "2017-03-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Bioengineering",
                "area_abbreviation": "CENG",
                "area_code": "1502"
            }
        ],
        "doc_keywords": [
            "Automated information",
            "Automated workflow",
            "Biomedical literature",
            "Biomedical research",
            "Information detection",
            "Knowledge distribution",
            "Knowledge exchange",
            "Scientific community"
        ],
        "doc_abstract": "© 2017 IEEE.The continuous growth of unstructured information resulting from biomedical research is a trending challenge for the scientific community. In this way, novel methods for information management are emerging to improve knowledge distribution and access. The concept of nanopublications illustrates one of these recent strategies to implement machine-readable knowledge assertions. It tries to overcome inconsistency, ambiguity and redundancy of traditional publications. The purpose is that they are more suited than traditional papers to represent relationships that exist between research data, providing an efficient mechanism for knowledge exchange. Although the evident benefits of these RDF-based snippets, its applicability stills challenging due to the inexistence of extraction and publications methods. To solve that issue, we propose an automated workflow for nanopublications generation from biomedical literature. The proposed method consists of exploring an automated information extraction tool for relevant information detection from published documents and respective standardization of the mined information through semantic web recommendations, for further exploration.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A semantic-based workflow for biomedical literature annotation",
        "doc_scopus_id": "85050210198",
        "doc_doi": "10.1093/database/bax088",
        "doc_eid": "2-s2.0-85050210198",
        "doc_date": "2017-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            }
        ],
        "doc_keywords": [
            "Data Curation",
            "Data Mining",
            "Databases, Bibliographic",
            "Semantic Web"
        ],
        "doc_abstract": "© The Author(s) 2017. Published by Oxford University Press.Computational annotation of textual information has taken on an important role in knowledge extraction from the biomedical literature, since most of the relevant information from scientific findings is still maintained in text format. In this endeavour, annotation tools can assist in the identification of biomedical concepts and their relationships, providing faster reading and curation processes, with reduced costs. However, the separate usage of distinct annotation systems results in highly heterogeneous data, as it is difficult to efficiently combine and exchange this valuable asset. Moreover, despite the existence of several annotation formats, there is no unified way to integrate miscellaneous annotation outcomes into a reusable, sharable and searchable structure. Taking up this challenge, we present a modular architecture for textual information integration using semantic web features and services. The solution described allows the migration of curation data into a common model, providing a suitable transition process in which multiple annotation data can be integrated and enriched, with the possibility of being shared, compared and reused across semantic knowledge bases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Linked Registries: Connecting Rare Diseases Patient Registries through a Semantic Web Layer",
        "doc_scopus_id": "85042161013",
        "doc_doi": "10.1155/2017/8327980",
        "doc_eid": "2-s2.0-85042161013",
        "doc_date": "2017-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [
            "Computational Biology",
            "Database Management Systems",
            "Databases, Factual",
            "Humans",
            "Information Dissemination",
            "Information Storage and Retrieval",
            "Internet",
            "Rare Diseases",
            "Registries",
            "Semantic Web",
            "Software"
        ],
        "doc_abstract": "© 2017 Pedro Sernadela et al.Patient registries are an essential tool to increase current knowledge regarding rare diseases. Understanding these data is a vital step to improve patient treatments and to create the most adequate tools for personalized medicine. However, the growing number of disease-specific patient registries brings also new technical challenges. Usually, these systems are developed as closed data silos, with independent formats and models, lacking comprehensive mechanisms to enable data sharing. To tackle these challenges, we developed a Semantic Web based solution that allows connecting distributed and heterogeneous registries, enabling the federation of knowledge between multiple independent environments. This semantic layer creates a holistic view over a set of anonymised registries, supporting semantic data representation, integrated access, and querying. The implemented system gave us the opportunity to answer challenging questions across disperse rare disease patient registries. The interconnection between those registries using Semantic Web technologies benefits our final solution in a way that we can query single or multiple instances according to our needs. The outcome is a unique semantic layer, connecting miscellaneous registries and delivering a lightweight holistic perspective over the wealth of knowledge stemming from linked rare disease patient registries.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "General guidelines for biomedical software development",
        "doc_scopus_id": "85028025434",
        "doc_doi": "10.12688/f1000research.10750.2",
        "doc_eid": "2-s2.0-85028025434",
        "doc_date": "2017-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Pharmacology, Toxicology and Pharmaceutics (all)",
                "area_abbreviation": "PHAR",
                "area_code": "3000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2017 Silva LB et al.Most bioinformatics tools available today were not written by professional software developers, but by people that wanted to solve their own problems, using computational solutions and spending the minimum time and effort possible, since these were just the means to an end. Consequently, a vast number of software applications are currently available, hindering the task of identifying the utility and quality of each. At the same time, this situation has hindered regular adoption of these tools in clinical practice. Typically, they are not sufficiently developed to be used by most clinical researchers and practitioners. To address these issues, it is necessary to re-think how biomedical applications are built and adopt new strategies that ensure quality, efficiency, robustness, correctness and reusability of software components. We also need to engage end-users during the development process to ensure that applications fit their needs. In this review, we present a set of guidelines to support biomedical software development, with an explanation of how they can be implemented and what kind of open-source tools can be used for each specific topic.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Four simple recommendations to encourage best practices in research software",
        "doc_scopus_id": "85024481292",
        "doc_doi": "10.12688/f1000research.11407.1",
        "doc_eid": "2-s2.0-85024481292",
        "doc_date": "2017-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Pharmacology, Toxicology and Pharmaceutics (all)",
                "area_abbreviation": "PHAR",
                "area_code": "3000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2017 Jiménez RC et al.Scientific research relies on computer software, yet software is not always developed following practices that ensure its quality and sustainability. This manuscript does not aim to propose new software development best practices, but rather to provide simple recommendations that encourage the adoption of existing best practices. Software development best practices promote better quality software, and better quality software improves the reproducibility and reusability of research. These recommendations are designed around Open Source values, and provide practical suggestions that contribute to making research software and its source code more discoverable, reusable and transparent. This manuscript is aimed at developers, but also at organisations, projects, journals and funders that can increase the quality and sustainability of research software by encouraging the adoption of these recommendations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Computational Discovery of Putative Leads for Drug Repositioning through Drug-Target Interaction Prediction",
        "doc_scopus_id": "84999751697",
        "doc_doi": "10.1371/journal.pcbi.1005219",
        "doc_eid": "2-s2.0-84999751697",
        "doc_date": "2016-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Ecology, Evolution, Behavior and Systematics",
                "area_abbreviation": "AGRI",
                "area_code": "1105"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Ecology",
                "area_abbreviation": "ENVI",
                "area_code": "2303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Cellular and Molecular Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2804"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            }
        ],
        "doc_keywords": [
            "Anti-Bacterial Agents",
            "Bacterial Proteins",
            "Computer Simulation",
            "Drug Discovery",
            "Drug Evaluation, Preclinical",
            "Drug Repositioning",
            "Models, Chemical",
            "Protein Interaction Mapping"
        ],
        "doc_abstract": "© 2016 Coelho et al.De novo experimental drug discovery is an expensive and time-consuming task. It requires the identification of drug-target interactions (DTIs) towards targets of biological interest, either to inhibit or enhance a specific molecular function. Dedicated computational models for protein simulation and DTI prediction are crucial for speed and to reduce the costs associated with DTI identification. In this paper we present a computational pipeline that enables the discovery of putative leads for drug repositioning that can be applied to any microbial proteome, as long as the interactome of interest is at least partially known. Network metrics calculated for the interactome of the bacterial organism of interest were used to identify putative drug-targets. Then, a random forest classification model for DTI prediction was constructed using known DTI data from publicly available databases, resulting in an area under the ROC curve of 0.91 for classification of out-of-sampling data. A drug-target network was created by combining 3,081 unique ligands and the expected ten best drug targets. This network was used to predict new DTIs and to calculate the probability of the positive class, allowing the scoring of the predicted instances. Molecular docking experiments were performed on the best scoring DTI pairs and the results were compared with those of the same ligands with their original targets. The results obtained suggest that the proposed pipeline can be used in the identification of new leads for drug repositioning. The proposed classification model is available at http://bioinformatics.ua.pt/software/dtipred/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ensemble-based methodology for the prediction of drug-target interactions",
        "doc_scopus_id": "84987670069",
        "doc_doi": "10.1109/CBMS.2016.67",
        "doc_eid": "2-s2.0-84987670069",
        "doc_date": "2016-08-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Antimicrobial resistances",
            "Classification performance",
            "Drug repositioning",
            "Drug-target interactions",
            "Ensemble learning",
            "G protein coupled receptors",
            "Methicillin-resistant staphylococcus aureus",
            "Random forests"
        ],
        "doc_abstract": "© 2016 IEEE.Antibacterial resistance has been progressively increasing mostly due to selective antibiotic pressure, forcing pathogens to either adapt or die. The development of antibacterial resistance to last-line antibiotics urges the formulation of alternative strategies for drug discovery. Recently, attention has been devoted to the development of computational methods to predict drug-target interactions (DTIs). Here we present a computational strategy to predict proteome-scale DTIs based on the combination of the drugs' chemical features and substructural fingerprints, and on the structural information and physicochemical properties of the proteins. We propose an ensemble learning combination of Support-Vector Machine and Random Forest to deal with the complexity of DTI classification. Two distinct classification models were developed to ascertain whether taking the type of protein target (i.e., enzymes, g-protein-coupled receptors, ion channels and nuclear receptors) into account improves classification performance. External validation analysis was consistent with internal five-fold cross-validation, with an AUC of 0.87. This strategy was applied to the proteome of methicillin-resistant Staphylococcus aureus COL (MRSA COL, taxonomy id: 93062), a major nosocomial pathogen worldwide whose antimicrobial resistance and incidence rate keeps steadily increasing. Our predictive framework is available at http://bioinformatics.ua.pt/software/dtipred.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Caching and prefetching images in a web-based DICOM viewer",
        "doc_scopus_id": "84987624074",
        "doc_doi": "10.1109/CBMS.2016.68",
        "doc_eid": "2-s2.0-84987624074",
        "doc_date": "2016-08-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Caching and prefetching",
            "DICOM viewer",
            "End-user experience",
            "Imaging applications",
            "Information access",
            "Mammography screening",
            "Prefetching",
            "Web caching"
        ],
        "doc_abstract": "© 2016 IEEE.The general trend of information access anywhere and anytime is also leading to the emergence of innovative medical imaging systems, adapted to this new reality. The HTML5 standard led Web applications to another software level, providing a set of features that allows developing professional Web-based medical imaging applications. However, despite the visualization quality that is already possible in HTML5 browsers, the performance is still an issue, due to the typical size of image studies. In this paper, we present a caching and prefetching solution that enriches the end-user experience in a Web-based DICOM viewer, by reducing data access latency of examinations under revision. We deployed the system in a radiology center for mammography screening, at a national level, and the results show that this technique significantly reduces the average examination access latency, during the reviewing process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Generating Big Data repositories for research in medical imaging",
        "doc_scopus_id": "84982156746",
        "doc_doi": "10.1109/CISTI.2016.7521382",
        "doc_eid": "2-s2.0-84982156746",
        "doc_date": "2016-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Design and implementations",
            "dicom",
            "Digital acquisition",
            "Real environments",
            "repositories",
            "Statistical summary",
            "Test bed environment"
        ],
        "doc_abstract": "© 2016 AISTI.The production of medical imaging data has grown tremendously in the last decades. The proliferation of digital acquisition equipment has enabled even small institutions to produce considerable amounts of studies. Furthermore, the general trend for new imaging modalities is to produce more data per examinations. As a result, the design and implementation of tomorrow's storage and communication systems must deal with Big Data issues. Moreover, new realities such as the outsourcing of medical imaging infrastructures to the Cloud impose additional pressure on these systems. The research on technologies for coping with Big Data issues on large scale medical imaging environments is still in its early stages. This is mostly due to the difficulty of implementing and validating new technological approaches in real environments, without interfering with clinical practice. Therefore, it is crucial to create test bed environments for research purposes. This article proposes a methodology for creating simulated Big Data repositories. The system is able to use a real world repository to collect data from a representative time window and expand it according to research needs. In addition, the solution provides anonymization tools and allows exporting two types of simulated data: A structured file repository containing DICOM objects and a statistical summary of the archive content based on its characteristics, such as the number of produced studies per day, their modality and their associated data volumes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Representation learning for mammography mass lesion classification with convolutional neural networks",
        "doc_scopus_id": "84955570567",
        "doc_doi": "10.1016/j.cmpb.2015.12.014",
        "doc_eid": "2-s2.0-84955570567",
        "doc_date": "2016-07-08",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Area under the ROC curve",
            "Automatic classification",
            "Breast Cancer",
            "Breast cancer diagnosis",
            "Convolutional neural network",
            "Discriminative features",
            "Feature learning",
            "Histogram of oriented gradients (HOG)",
            "Biopsy",
            "Breast Neoplasms",
            "Female",
            "Humans",
            "Machine Learning",
            "Mammography",
            "Neural Networks (Computer)"
        ],
        "doc_abstract": "© 2015 Elsevier Ireland LtdBackground and objective The automatic classification of breast imaging lesions is currently an unsolved problem. This paper describes an innovative representation learning framework for breast cancer diagnosis in mammography that integrates deep learning techniques to automatically learn discriminative features avoiding the design of specific hand-crafted image-based feature detectors. Methods A new biopsy proven benchmarking dataset was built from 344 breast cancer patients’ cases containing a total of 736 film mammography (mediolateral oblique and craniocaudal) views, representative of manually segmented lesions associated with masses: 426 benign lesions and 310 malignant lesions. The developed method comprises two main stages: (i) preprocessing to enhance image details and (ii) supervised training for learning both the features and the breast imaging lesions classifier. In contrast to previous works, we adopt a hybrid approach where convolutional neural networks are used to learn the representation in a supervised way instead of designing particular descriptors to explain the content of mammography images. Results Experimental results using the developed benchmarking breast cancer dataset demonstrated that our method exhibits significant improved performance when compared to state-of-the-art image descriptors, such as histogram of oriented gradients (HOG) and histogram of the gradient divergence (HGD), increasing the performance from 0.787 to 0.822 in terms of the area under the ROC curve (AUC). Interestingly, this model also outperforms a set of hand-crafted features that take advantage of additional information from segmentation by the radiologist. Finally, the combination of both representations, learned and hand-crafted, resulted in the best descriptor for mass lesion classification, obtaining 0.826 in the AUC score. Conclusions A novel deep learning based framework to automatically address classification of breast mass lesions in mammography was developed.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2016-01-07 2016-01-07 2016-03-18 2016-03-18 2016-03-18T13:03:09 S0169-2607(15)30011-0 S0169260715300110 10.1016/j.cmpb.2015.12.014 S300 S300.1 FULL-TEXT 2016-09-02T11:56:12.822172-04:00 0 0 20160401 20160430 2016 2016-01-07T16:39:56.757699Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes highlightsabst primabst ref 0169-2607 01692607 true 127 127 C Volume 127 23 248 257 248 257 201604 April 2016 2016-04-01 2016-04-30 2016 Section II: Systems and Programs article fla Copyright © 2015 Elsevier Ireland Ltd. All rights reserved. REPRESENTATIONLEARNINGFORMAMMOGRAPHYMASSLESIONCLASSIFICATIONCONVOLUTIONALNEURALNETWORKS AREVALO J 1 Introduction 2 Material and methods 2.1 Breast cancer digital repository 2.1.1 Benchmarking dataset 2.1.2 Baseline descriptors 2.2 Proposed method 2.2.1 Preprocessing 2.2.2 Supervised feature learning 2.2.3 Classification 3 Experimental setup 4 Results 4.1 Learned features 4.2 Classification results 5 Conclusions Acknowledgements References TABAR 2011 658 663 L AYER 2010 313 323 T MOURA 2013 561 574 D RAMOSPOLLAN 2012 2259 2269 R RAMOSPOLLAN 2012 2245 2257 R LIU 2014 910 920 X DONG 2015 613 625 M BENGIO 2013 1798 1828 Y SCHMIDHUBER 2015 85 117 J AREVALO 2013 J CRUZROA 2013 403 410 A INMEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTIONMICCAI2013 ADEEPLEARNINGARCHITECTUREFORIMAGEREPRESENTATIONVISUALINTERPRETABILITYAUTOMATEDBASALCELLCARCINOMACANCERDETECTION SUK 2013 583 590 H LECTURENOTESINCOMPUTERSCIENCEINCLUDINGSUBSERIESLECTURENOTESINARTIFICIALINTELLIGENCELECTURENOTESINBIOINFORMATICS DEEPLEARNINGBASEDFEATUREREPRESENTATIONFORADMCICLASSIFICATION SUK 2013 1 19 H SUK 2014 569 582 H LI 2014 240 247 F MACHINELEARNINGINMEDICALIMAGINGVOL8679LECTURENOTESINCOMPUTERSCIENCE ROBUSTDEEPLEARNINGFORIMPROVEDCLASSIFICATIONADMCIPATIENTS PRASOON 2013 246 253 A MEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTIONMICCAI2013VOL8150LECTURENOTESINCOMPUTERSCIENCE DEEPFEATURELEARNINGFORKNEECARTILAGESEGMENTATIONUSINGATRIPLANARCONVOLUTIONALNEURALNETWORK JALALIAN 2013 420 426 A PETERSEN 2012 K INSTMIWORKSHOPMICCAI201215THINTERNATIONALCONFERENCEMEDICALIMAGECOMPUTINGCOMPUTERASSISTEDINTERVENTION BREASTDENSITYSCORINGMULTISCALEDENOISINGAUTOENCODERS PETERSEN 2014 88 94 K BREASTIMAGINGVOL8539LECTURENOTESINCOMPUTERSCIENCE BREASTTISSUESEGMENTATIONMAMMOGRAPHICRISKSCORINGUSINGDEEPLEARNING ZHANG 2014 970287 X GE 2006 2975 2988 J JAMIESON 2012 A BREASTIMAGEFEATURELEARNINGADAPTIVEDECONVOLUTIONALNETWORKS ANDREEA 2011 241 248 G PEREZ 2014 N JARRETT 2009 2146 2153 K INCOMPUTERVISION2009IEEE12THINTERNATIONALCONFERENCE BESTMULTISTAGEARCHITECTUREFOROBJECTRECOGNITION KRIZHEVSKY 2012 1097 1105 A ADVANCESINNEURALINFORMATIONPROCESSINGSYSTEMS25 IMAGENETCLASSIFICATIONDEEPCONVOLUTIONALNEURALNETWORKS PINTO 2008 e27 N LYU 2008 1 8 S IEEECONFERENCECOMPUTERVISIONPATTERNRECOGNITION2008CVPR2008 NONLINEARIMAGEREPRESENTATIONUSINGDIVISIVENORMALIZATION LECUN 2012 496 505 Y COMPUTERVISIONECCVWORKSHOPSDEMONSTRATIONSVOL7583LECTURENOTESINCOMPUTERSCIENCE LEARNINGINVARIANTFEATUREHIERARCHIES KRIZHEVSKY 2009 A LEARNINGMULTIPLELAYERSFEATURESTINYIMAGESTECHREP VELDKAMP 2000 731 738 W KE 2014 4146 4153 Q INCOMPUTERVISIONPATTERNRECOGNITIONCVPR2014IEEECONFERENCE ROTATIONANUISANCEINSHAPERECOGNITION GOODFELLOW 2013 1319 1327 I PROCEEDINGS30THINTERNATIONALCONFERENCEMACHINELEARNINGICML13VOL28 MAXOUTNETWORKS SRIVASTAVA 2014 1929 1958 N BASTIEN 2012 F DEEPLEARNINGUNSUPERVISEDFEATURELEARNINGNIPS2012WORKSHOP THEANONEWFEATURESSPEEDIMPROVEMENTS BERGSTRA 2012 281 305 J DENG 2014 197 387 L RUSSAKOVSKY 2013 O ININTERNATIONALCONFERENCECOMPUTERVISIONICCV DETECTINGAVOCADOSZUCCHINISDONEGOING RUEDAPLATA 2015 275 284 D COMPUTATIONALCOLLECTIVEINTELLIGENCEVOL9329LECTURENOTESINCOMPUTERSCIENCE SUPERVISEDGREEDYLAYERWISETRAININGFORDEEPCONVOLUTIONALNETWORKSSMALLDATASETS AREVALOX2016X248 AREVALOX2016X248X257 AREVALOX2016X248XJ AREVALOX2016X248X257XJ UnderEmbargo 2017-03-18T00:00:00Z item S0169-2607(15)30011-0 S0169260715300110 10.1016/j.cmpb.2015.12.014 271322 2016-03-18T11:30:53.90933-04:00 2016-04-01 2016-04-30 true 1489993 MAIN 10 44569 849 656 IMAGE-WEB-PDF 1 gr1 13934 90 219 gr2 14642 105 219 gr3 21982 156 219 gr4 12137 48 219 gr5 29160 164 164 gr6 7477 164 216 gr7 4681 162 219 gr1 38580 246 602 gr2 50572 288 602 gr3 67633 429 602 gr4 38681 148 678 gr5 55752 300 301 gr6 24552 272 358 gr7 13587 271 367 gr1 138939 655 1600 gr2 258868 1277 2667 gr3 258306 1139 1600 gr4 191734 654 3000 gr5 126835 798 800 gr6 176207 1206 1587 gr7 96191 1202 1625 si1 257 11 61 si10 848 29 221 si11 191 10 41 si12 316 14 77 si13 559 25 98 si14 224 11 47 si15 321 13 85 si16 265 13 65 si2 316 19 86 si3 576 21 136 si4 242 15 54 si5 256 13 62 si6 152 15 12 si7 800 37 182 si8 278 13 64 si9 601 36 118 COMM 4040 S0169-2607(15)30011-0 10.1016/j.cmpb.2015.12.014 Elsevier Ireland Ltd Fig. 1 Samples of lesions presented in the dataset. Malignant lesion in (a) oblique view and (b) craneo-caudal view. Benign lesion in (c) oblique view and (d) craneo-caudal view. Fig. 2 Workflow diagram of the proposed method. Fig. 3 Mammography images after the preprocessing step. Images A and B represent malignant and benign lesions respectively. Images a1 and b1 are the bounding box of the lesions. Images a2 and b2 show the output of global and local contrast normalizations. Images a3 and b3 show outline of the lesions over the normalized images. Fig. 4 Best convolutional neural network evaluated on mass classification. Fig. 5 Filters learned in the first layer of the CNN model. Fig. 6 ROC curve for evaluated representations in test set for the best run. Fig. 7 Boxplots of different runs for each representation method. Table 1 Set of hand-crafted features. For details see [3]. Type Features Intensities Mean, median, maximum, minimum, standard deviation, skewness, kurtosis Shape Area, perimeter, circularity, elongation, y_center_mass, x_center_mass, form Textures Contrast, correlation, entropy Table 2 Summary of results in terms of AUC in the test set. Best results are shown in bold typeface and (*) signals scores with no evidence of differences from the highest (ρ <0.1). Representation Standalone Combined with HCfeats CNN3 0.82 ± 0.03 0.82±0.03 (*) CNN2 0.76±0.05 0.78±0.04 HGD 0.78±0.04 0.83 ± 0.04 HOG 0.77±0.03 0.81±0.03 (*) DeCAF 0.79±0.05 0.82±0.03 (*) HCfeats 0.77±0.02 – Representation learning for mammography mass lesion classification with convolutional neural networks John Arevalo a Fabio A. González a Raúl Ramos-Pollán b Jose L. Oliveira c Miguel Angel Guevara Lopez d ⁎ a Universidad Nacional de Colombia, Bogotá, Colombia Universidad Nacional de Colombia Bogotá Colombia b Universidad Industrial de Santander, Bucaramanga, Colombia Universidad Industrial de Santander Bucaramanga Colombia c DETI-IEETA, Universidade de Aveiro, Portugal DETI-IEETA, Universidade de Aveiro Portugal d CCG, Computer Graphics Center, Portugal CCG, Computer Graphics Center Portugal ⁎ Corresponding author. Tel.: +351 253510580. Background and objective The automatic classification of breast imaging lesions is currently an unsolved problem. This paper describes an innovative representation learning framework for breast cancer diagnosis in mammography that integrates deep learning techniques to automatically learn discriminative features avoiding the design of specific hand-crafted image-based feature detectors. Methods A new biopsy proven benchmarking dataset was built from 344 breast cancer patients’ cases containing a total of 736 film mammography (mediolateral oblique and craniocaudal) views, representative of manually segmented lesions associated with masses: 426 benign lesions and 310 malignant lesions. The developed method comprises two main stages: (i) preprocessing to enhance image details and (ii) supervised training for learning both the features and the breast imaging lesions classifier. In contrast to previous works, we adopt a hybrid approach where convolutional neural networks are used to learn the representation in a supervised way instead of designing particular descriptors to explain the content of mammography images. Results Experimental results using the developed benchmarking breast cancer dataset demonstrated that our method exhibits significant improved performance when compared to state-of-the-art image descriptors, such as histogram of oriented gradients (HOG) and histogram of the gradient divergence (HGD), increasing the performance from 0.787 to 0.822 in terms of the area under the ROC curve (AUC). Interestingly, this model also outperforms a set of hand-crafted features that take advantage of additional information from segmentation by the radiologist. Finally, the combination of both representations, learned and hand-crafted, resulted in the best descriptor for mass lesion classification, obtaining 0.826 in the AUC score. Conclusions A novel deep learning based framework to automatically address classification of breast mass lesions in mammography was developed. Keywords Breast cancer Feature learning Convolutional neural networks Computer-aided diagnosis Mammography 1 Introduction Breast cancer is the most common cancer in women worldwide, with nearly 1.7 million new cases diagnosed in 2012 (second most common cancer overall); this represents about 12% of all new cancer cases and 25% of all cancers in women. 1 1 World Cancer Research Fund International Accessed May 20, 2015 Breast cancer has a known asymptomatic phase that can be detected with mammography, and therefore, mammography is the primary imaging modality for screening. Double-reading (two radiologists independently read the same mammograms) has been advocated to reduce the proportion of missed cancers and it is currently included in most screening programs [1]. However, double-reading incurs in additional workload and costs. Alternatively, computer-aided diagnosis (CADx) systems can assist a single radiologist when reading mammograms providing support for their decisions. These systems can be used as second opinion criteria by radiologists, playing a key role in the early detection of breast cancer and helping to reduce the death rate among women with breast cancer in a cost-effective manner [2]. A successful approach to build CADx systems is to use machine learning classifiers (MLC). MLC are learned from a set of labeled data samples capturing complex relationships in the data [3–5]. In order to train a MLC for breast cancer diagnosis, a set of features describing the image is required. Ideally, features should have high discriminant power that allows inferring whether a given image is from a malignant finding or not. This is, however, a challenging topic that has gathered the focus of research in several sciences, from medicine to computer vision. Thus, several types of features may be used to infer the diagnosis. Many CADx systems use hand-crafted features based on prior knowledge and expert guidance. In particular, strategies based on feature selection [6] and hand-crafted features that characterize geometry and textures [7] has been proposed for mass classifications. As an alternative, the use of machine learning strategies to learn good features directly from the data is a new paradigm that has shown successful results in different computer vision tasks. One such paradigm is deep learning. Deep learning methods have been widely applied in recent years to address several computer perception tasks [8]. Their main advantage lies in avoiding the design of specific feature detectors. In turn, deep learning models look for a set of transformations directly from the data. This approach has had remarkable results, particularly in computer vision problems such as natural scene classification and object detection [9]. Deep learning models have also been adapted to different medical tasks such as tissue classification in histology and histopathology images [10,11], Alzheimer disease diagnosis [12–15], and knee cartilage segmentation [16] among others. However, only few works have explored deep learning methods to address the automatic classification of identified lesions in mammography images [17]. In [18] stacked deep auto-encoders were used to estimate breast density score using multiscale features. Lately, this has been extended by including breast tissue segmentation and scoring of mammographic texture [19] with a convolutional neural network (CNN) model. CNN model is the most successful deep learning strategy applied to image understanding [9]. In [20,21] CNNs are used as representation strategy to characterize microcalcifications. Finally, the most recent work developed in this area was done in [22] which uses Adaptive Deconvolutional Networks to learn the representation in order to classify malign/benign breast lesions. Such strategy was evaluated on 245 lesions in a bootstrap fashion, reporting the area under the ROC curve (AUC) AUC =0.71. In this work, we also use convolutional architectures, however the features are learned in a supervised way during CNN training, taking advantage of expert knowledge represented by previously identified lesions in breast imaging, manually segmented by expert radiologists in both mammographic views (mediolateral oblique and craniocaudal). The remainder of the paper is organized as follows: Section 2 describes the proposed approach to perform classification of identified lesions in mammography images. Section 3 details the experimental setup used to evaluate the proposed approach. Finally, Sections 4 and 5 show results and present the main conclusions of this work. 2 Material and methods 2.1 Breast cancer digital repository The benchmarking dataset used in this study is available on the Breast Cancer Digital Repository (BCDR). 2 2 BCDR is a wide-ranging annotated public repository composed of Breast Cancer patient’ cases in the northern region of Portugal. The BCDR is subdivided in two different repositories: (1) a Film Mammography-based Repository (BCDR-FM) and (2) a Full Field Digital Mammography-based Repository (BCDR-DM). Both repositories were created with anonymous cases from medical archives (complying with current privacy regulations as they are also used to teach regular and postgraduate medical students) supplied by the Faculty of Medicine – Centro Hospitalar São João, at University of Porto (FMUP–HSJ). BCDR provides normal and annotated patient cases of breast cancer including mammography lesions outlines, anomalies observed by radiologists, pre-computed image-based descriptors and related clinical data. The BCDR-FM is composed by 1010 patient cases (998 female and 12 male, with ages between 20 and 90 years old), including 1125 studies, 3703 mediolateral oblique (MLO) and craniocaudal (CC) mammography incidences and 1044 identified lesions clinically described (820 already identified in MLO and/or CC views). With this, 1517 segmentations were manually made and BI-RADS classified by specialized radiologists. MLO and CC images are grey-level digitized mammograms with a resolution of 720 (width) by 1168 (height) pixels and a bit depth of 8 bits per pixel, saved in the TIFF format. The BCDR-DM, still in construction, at the time of writing is composed by 724 Portuguese patient cases (723 female and 1 male, with ages between 27 and 92 years old), including 1042 studies, 3612 MLO and/or CC mammography incidences and 452 lesions clinically described (already identified in MLO and CC views). With this, 818 segmentations were manually made and BI-RADS classified by specialized radiologists. The MLO and CC images are grey-level mammograms with a resolution of 3328 (width) by 4084 (height) or 2560 (width) by 3328 (height) pixels, depending on the compression plate used in the acquisition (according to the breast size of the patient). The bit depth is 14 bits per pixel and the images are saved in TIFF format. As described below, this work is focused on the BCDR-FM Repository. 2.1.1 Benchmarking dataset A new dataset of the BCDR-FM repository has been made publicly available, at for comparison and research reproducibility purposes. The 8-bit resolution “Film Mammography Dataset Number 3” (BCDR-F03) was built as a subset of the BCDR-FM and it is composed of 344 patients with 736 film images containing 426 benign mass lesions and 310 malign mass lesions, including clinical data and image-based descriptors. Such lesions are associated with masses. The motivations to choose 8-bit resolution images over 12-bit or 14-bit are twofold: Firstly, in contrast to the BCDR-DM (currently under construction), almost all lesions in the BCDR-FM repository have a proven biopsy; and secondly, digital mammography (high resolution images) are not as widely available as film mammography images since the former are more expensive to acquire [23]. For all the experimentation clinical data were not included as features. Fig. 1 shows examples of both classes with their respective segmentations. The dataset contains MLO and CC views. 2.1.2 Baseline descriptors Based on the systematic evaluation presented by Moura et al. [3], the histogram of oriented gradients (HOG) and the histogram of gradient divergence (HGD) were selected as descriptors for our baseline since they showed the best performance against other traditional descriptors. Additionally, a set of 17 hand-crafted features extracted from the segmented lesions (representative of shape, texture and intensities of the mammograms) are used for comparative purposes. Hand-crafted features (HCfeats): HCfeats is a set comprising 17 features selected from produced sets of high performance features proposed by Perez et al. [24] that demonstrated a high impact in characterizing lesions corresponding to masses. Table 1 lists the features and their description. HCfeats is composed by intensity descriptors computed directly from the grey-levels of the pixels inside the lesion’ s contour identified by the radiologists; texture descriptors computed from the grey-level co-occurrence matrix related to the bounding box of the lesion’ s contour; and shape descriptors computed from the lesion’ s contour. Notice that computing this set of features requires not only the region of interest (ROI) detection, but also the manual segmentation provided by the expert. Histogram of oriented gradients (HOG): HOG describes images through the distribution of the gradients. Images are divided into a grid of blocks (e.g. 3×3), and each block is described by a histogram of the orientation of the gradient. Each histogram has a predefined number of bins dividing the range of possible orientations (from 0 to 2π radians, or from 0 to π radians), and the value of each bin is calculated by summing the magnitude of the gradient of the pixels which have gradient direction within the limits of the bin. Histogram of gradient divergence (HGD): Gradient divergence in a point i, j is measured as the angle between the vector of the intensity gradient on i, j and a vector pointing to the center of the image with origin in i, j. HGD describes images through the distribution of the gradient divergence. Images are divided into concentric regions, and each region is described by a histogram of the gradient divergence. 2.2 Proposed method Image representation is fundamental for automatic classification of lesions in mammography images. The goal is to describe the content of the image in a compact and discriminative way. Traditional CADx systems represent images with a carefully selected set of mathematical and heuristic features aiming to characterize the lesion. Recent studies have replaced this hand-crafted process with a learning-based approach where a model is trained in an unsupervised way using deep learning, to transform the raw pixels in a set of features that feeds a classifier algorithm [22,19]. In contrast to previous work, we have herewith applied a hybrid approach in which CNNs are used to learn the representation in a supervised way. That is, we used lesions previously classified (labeled as benign or malignant) to guide the feature learning process. Fig. 2 shows an overall view of the proposed method. It comprises two main stages: preprocessing and supervised training. The preprocessing stage aims to prepare the data in better conditions through a set of transformations so that the next stage takes advantage of relevant characteristics. Supervised learning is the second stage that involves two processes: feature learning and classification training. Feature learning is performed by training a CNN. It is noteworthy that feature learning is a supervised stage since the CNN training is guided by the labeled samples. The final stage is the SVM classifier training with the penultimate layer of the CNN as features. 2.2.1 Preprocessing Preprocessing is a common stage in CADx systems. Its main goal is to enhance the characteristics of the image by applying a set of transformations that could help to improve performance in following stages. The first step in this work is to extract the ROI from the image. Secondly, an oversampling strategy is used to both get more samples artificially and help to prevent overfitting during training. Finally, a normalization process is carried out to prepare data for learning algorithms. It is widely known that feature learning and deep learning methods usually perform better when the input data has some properties such as decorrelation and normalization, mainly because such properties help gradient-based optimization techniques to converge [25]. Cropping: CADx systems aim at classifying a previously identified ROI in the whole film image. This ROI can be obtained by a manual segmentation or automatically detected by a computer aided detection system. Because of lesions in BCDR-03 dataset are manually segmented, we fixed the input size to ROIs of r × r pixels. With this, ROIs can be easily extracted by taking the bounding box of the segmented region. Specifically, images were cropped to the bounding box of the lesions and rescaled to r × r pixels preserving the aspect ratio when either width or height of the bounding box are greater than r, otherwise the lesion is centered without scaling and preserving the surrounding region. Data augmentation: The expressiveness of neural network models, and particularly deep ones, comes mainly from the large number of parameters to learn. However, more complex models also increase the chance of overfitting the training data. Data augmentation is a good way to help to prevent this behavior [26]. Data augmentation is the process of artificially create new samples by applying transformations to the original data. In a lesion classification problem, data augmentation makes sense because a lesion can be presented in any particular orientation. Thus, the model also should be able to learn from such transformations. In particular, For each training image, we have artificially generated 7 new label-preserving samples using a combination of flipping and 90,180 and 270 degrees rotation transformations. Global contrast normalization: Due to the digitalization process, the lighting conditions between different film images will be different, and all pixel values of the image are affected by that. A common way to overcome this effect, is to perform a global contrast normalization (GCN) by subtracting the mean of the intensities in the image to each pixel. Notice that the mean is not calculated per pixel, but per image, so it is perfectly fine to subtract it without worrying about whether the current image belongs to train, validation, or test set. Let X ∈ ℝ r × r be the image, the element-wise transformation is (2.1) X ˆ i , j = X i , j − x ¯ with x ¯ ∈ ℝ ; x ¯ = 1 r 2 ∑ i , j X i , j , the mean of the X image intensities, and X i , j ∈ ℝ the intensity in the i, j pixel. Local contrast normalization: Local contrast normalization (LCN) is a transformation inspired by computational neuroscience models [27]. Its main idea is to mimic the behavior the V1 visual cortex. It is implemented by defining a G ∈ ℝ k × k normalized Gaussian window, i.e ∑ p,q G p,q =1. Then, for each pixel in the global contrast normalized image X ˆ , the mean of its k × k neighborhood is removed: (2.2) V i , j = X ˆ i , j − ∑ p , q G p , q · X ˆ i + p , j + q with V ∈ ℝ k × k as the local normalized patch. Then the norm of each k × k neighborhood is scaled to 1 when it is greater than 1: (2.3) X ˜ i , j = V i , j max c , σ i , j where σ i , j ∈ ℝ ; σ i , j = ∑ p , q G p , q · v i + p . j + q 2 is the norm of the k × k neighborhood, and c ∈ ℝ is a tolerance parameter to avoid floating point precision problems. It has been empirically shown that such divisive normalization reduces statistical dependencies [28,25], which in turn accentuates differences between input features and accelerates gradient-based learning [29]. Improvement in both performance and training time when using such normalizations has been reported when the stochastic gradient descent algorithm is used to train deep networks [25]. This has been explained by the fact that, in the same way as whitening and other decorrelation methods, all variables end up with similar variances, making the model more likely to discover non-trivial relationships between spatially near inputs [30]. Also, it has been shown that similar strategies to locally normalize contrast in mammograms have enhanced performance of automatic analysis [31]. Fig. 3 shows an original image and its corresponding output after applying the preprocessing stage. Again, this preprocessing is performed in an image-wise fashion, thus it is not necessary to store parameters in the training procedure. 2.2.2 Supervised feature learning A CNN is a neural network that shares connections between hidden units yielding low computational time and translational invariance properties. CNNs have been successfully applied in shape recognition problems [32] as well as medical diagnosis that involved texture as a discriminant feature [11]. Because mass characterization is highly correlated with shape and texture features [17,3], a CNN model becomes a suitable strategy for mass lesion classification. The main components of the CNN and the applied strategies to train it are detailed below. Architecture: A CNN comprises 3 main components: a convolutional layer, an activation function and a pooling layer. To improve the capability of the model the three components are stacked iteratively so that the output of one component is the input for the next one, and the output of one set of components is the input for the next set, building a deep neural network with many layers. The convolutional layer is composed of several small matrices or “kernels” that are convolved throughout the whole input image working as filters. The output of this convolution is called “feature map”. These feature maps are the input for the activation function which applies a non-linear transformation in an element-wise fashion. Finally, the pooling layer aggregates contiguous values to one scalar with functions like mean or max. The proposed architecture, depicted in Fig. 4 , has 11×11 local kernels and the rectifier linear as activation function in the first convolutional layer followed by a 5×5 pooling layer with stride of 4×4 pixels. The second convolutional layer has 4×4 local kernels with the rectifier linear as activation function, with 4×4 pooling layer without overlapping. Then a fully connected layer with 400 units with maxout activation function is stacked to finally add a softmax classifier. In particular, the maxout activation function h i : ℝ n → ℝ is a defined as: (2.4) h i ( s ) = max j ∈ [ 1 , k ] z i , j where s ∈ ℝ n is the input vector, z i,j = s T W ⋯ij + b ij is the output of the j-th linear transformation of the i-th hidden unit, and W ∈ ℝ d × m × k and b ∈ ℝ m × k are learned parameters. It has been shown that maxout models with just 2 hidden units behave as universal approximators, while are less prone to saturate units [33]. Since it is our intention to measure how the network's depth affects the performance of the model, we first evaluate the architecture with a single convolutional layer with a fully connected layer and called it CNN2 in the experiments. Consequently, the whole architecture, i.e. two convolutional layers plus a fully connected layer, is referenced as CNN3. Regularization: The number of parameters in the model is directly related to capability to overfit the training data. Usually neural networks require different strategies to control this behavior. In this work dropout and max-norm regularization were used. Dropout randomly set to 0 the input of a unit, while max-norm regularization forces the norm of each vector of incoming weights in a unit to a maximum value. In [34] it was empirically shown that these two strategies help prevent co-adaptation between units, e.g., during error back propagation, a unit should not rely on other units to correct its mistakes since there is no certainty about their activations. Optimization: The proposed architecture has approximately 4.6 million of parameters. Training large models has to scale in both, memory requirements and computational time. The strategy used in this work to train the CNN is stochastic gradient descent with momentum. An early stopping strategy monitoring the area under the ROC curve (AUC) on the validation set was chosen as stop criterion. The implementation of the whole framework was carried out with the Pylearn2 framework [35]. This library uses the Theano framework [36], which in turn takes advantage of GPU technology obtaining up to 140times speedup with respect to CPU implementations, making feasible the training of architectures with millions of parameters. 2.2.3 Classification Following the previous work, a linear SVM was selected as classification strategy. Train and validation sets were used to fine-tune the C parameter. To evaluate the CNN as a representation strategy, images are propagated through the network, then the penultimate layer activations are extracted and used as representation. This process is done to reduce processing time because, in terms of computational cost, training a single SVM is cheaper than training the whole CNN network. This stage can be seen as a fine-tuning process of the last layer, where a smaller model is adjusted. 3 Experimental setup The dataset was split in training (50%), validation (10%) and test (40%) sets following a stratified sampling per patient, that is, we make sure all computed instances of a particular patient belongs to only one of the three subsets. This setup warranties that the model is not tested using patients seen during the training stage. In the preprocessing stage, the size of the cropped region was fixed to r =150 according to the distribution of the lesion size and computational capability; and the filter size for LCN is k =11 pixels. Following previous results [3], 5×5 and 3×3 blocks sizes for HOG and 4 and 8 regions for HGD were explored. Histograms for both 8 and 16 bins were evaluated. The best configuration in train-validation setup was used to report test results. The CNN parameter exploration was performed by training 25 models with random hyperparameter initializations and the best was chosen according to validation performance. It has been reported that this strategy is preferable over grid search when training deep models [37]. Exploration was conducted using the CETA-CIEMAT 3 3 accessed on February 17, 2015 Research Center infrastructure. Bigger models that requires more intensive computation were carried out using a NVidia Tesla K40 GPGPU card. Before training the SVM model, a zero-mean unit-variance normalization process is carried out. Train and validation sets were used to fine-tune the C parameter for the SVM classifier. Final performance is reported in terms of AUC in the test set. Comparison of the methods was based on the average AUC of 5 runs using different random seeds for dataset splitting for each run. Experiments were supported by the Wilcoxon signed rank test to determine whether differences have statistical evidence (ρ >0.1). 4 Results 4.1 Learned features Recall that the CNN weights in the first layer are equivalent to local kernels that work as filters over the image. Thus, visualizing them would allow to describe the patterns that the model is looking for. Fig. 5 shows the weights of the best learned model. This image exposes a set of edges in different orientations as well as some texture patterns. It seems the learned filters are affected by noise, probably because it is still few data for this kind of models. We experimentally found that normalization preprocessing was fundamental to obtain good-looking features and ultimately, good performances in the classification. Without normalization the models were not able to surpasses 0.7 of AUC. 4.2 Classification results Fig. 6 shows ROC curves for all the evaluated representations for the best run. The HCfeats set, which uses segmentation information, performs slightly better than HOG-based descriptors. This confirms the importance of shape information for mass characterization. Interestingly, CNN models, which use only the raw pixels, outperform the state-of-the-art features [3]. The training of CNN3 model took 1.4h on the Tesla K40 GPGPU card. It is also worthwhile noting that adding a second hidden layer to the CNN model improves the representation capability producing better results. Such behavior is consistent with theoretical foundations to choose deep architectures over shallow ones [38]. For comparative purposes, we included the evaluation of DeCAF [39], a pre-trained model with the Imagenet dataset [40]. DeCAF is a model with greater complexity than all the other representations evaluated on this work. Thus, it is expected to perform better than using hand-crafted features. However, a smaller CNN model trained with the images of the domain performs the best. This behavior, similarly reported when CNNs are trained with small datasets [41], leads to the two main conclusions of this work: On one hand, CNN models outperform state-of-the-art representations for automatic lesion classification in mammography image analysis. On the other hand, such automatic mammography image analysis is a problem with its own particularities, and thus it is not enough to learn the representation using a large CNN model. The learning process should also be guided by a training set with a wide visual variability to show the model texture and shape features presented in mass lesions. Fig. 7 shows boxplots results in terms of AUC for each representation. According to the Wilcoxon test hypothesis, the CNN3 model performs best as compared to other evaluated representations (ρ <0.1). In order to combine the image-based features with additional information given in the segmentation, HCfeats, described in Section 2.1, were concatenated to each CNN representation and baseline descriptors (HOG, HGD and DeCAF). The resultant vector feature of each image has 417 elements, 400 from the last fully connected layer in the CNN plus 17 features from the HCfeats set. Table 2 shows a summary of these experiments. In general, this combination improves the results. It specially helps to augment the performance of the hand-crafted representations, while CNN models are not very affected. This suggests that CNN models are already capable of capturing shape information, which is consistent with the learned filters depicted in Fig. 5, and thus giving such information explicitly could be redundant. Again, this experimentation was supported by the Wilcoxon test, which showed no significant statistical evidence in the differences between representations combined with HCfeats. However, comparing standalone vs combined with HCfeats, all representations except CNN3, obtained evidence for a statistically significant improvement (ρ <0.05). An open question regarding these results is how this method would perform in high resolution images (12 or 14-bit images). Based on preliminary experimentation, we hypothesize that the model would obtain superior performance using higher resolution images, since the learning model will have more available information. However, we still do not have enough data to report statistically significant results. On the other hand, it is noteworthy that the neural network design would face new challenges such as higher dimensional input, fewer number of examples and different primitive patterns, among others. Thus, we believe new network architectures should be explored to address high resolution images. 5 Conclusions This paper presented a framework to address classification of mass lesions in mammography film images. Instead of designing particular descriptors to explain the content of mammography images, the proposed approach learns them directly from data in a supervised way. CNNs were used as the representation learning strategy. The proposed neural network architecture takes the raw pixels of the image as input, to learn in a hierarchical way a set of nonlinear transformations to represent the visual content of an image. The model is composed of a set of local filters with a rectified linear unit activation function, maxpooling layers, a fully-connected layer with maxout activation function and a softmax layer. Our approach outperformed the state-of-the-art image features, HOG and HGD descriptors [3], increasing the performance from 0.787 to 0.822 in terms of AUC. Interestingly, this model also outperforms a set of hand-crafted features that take advantage of additional information from segmentation by the radiologist. Finally, the combination of both representations, learned and hand-crafted, resulted in the best descriptor for mass lesion classification, obtaining 0.826 in the AUC score. Our future work includes larger architectures as well as the inclusion of other image modalities to enhance the representation. It also would be worth to evaluate the proposed strategy on BCDR-DM images since this suppose a new challenge due to the high resolution images. Acknowledgements This work was mainly supported by the Cloud Thinking project (CENTRO-07-ST24-FEDER-002031), co-funded by QREN, “Mais Centro” program. Also, this work was partially funded by projects “Multimodal Image Retrieval to Support Medical Case-Based Scientific Literature Search”, ID R1212LAC006 by Microsoft Research LACCIR, “Diseño e implementación de un sistema de cómputo sobre recursos heterogéneos para la identificación de estructuras atmosféricas en predicción climatológica” number 1225-569-34920 through Colciencias contract number 0213-2013, “Programa nacional de proyectos para el fortalecimiento de la investigación, la creación y la innovación en posgrados de la Universidad Nacional de Colombia 2013–2015” with proposal number 18722 and the computing facilities of Extremadura Research Centre for Advanced Technologies (CETA-CIEMAT), funded by the European Regional Development Fund (ERDF). CETA-CIEMAT belongs to CIEMAT and the Government of Spain. Arevalo also thanks Colciencias for its support through a doctoral grant in call 617/2013. The authors also thank for K40 Tesla GPU donated by NVIDIA and which was used for some feature learning experiments. References [1] L. Tabár B. Vitak T.H.-H. Chen A.M.-F. Yen A. Cohen T. Tot S.Y.-H. Chiu S.L.-S. Chen J.C.-Y. Fann J. Rosell H. Fohlin R.A. Smith S.W. Duffy Swedish two-county trial: impact of mammographic screening on breast cancer mortality during 3 decades Radiology 260 3 2011 658 663 10.1148/radiol.11110469 [2] T. Ayer M.U. Ayvaci Z.X. Liu O. Alagoz E.S. Burnside Computer-aided diagnostic models in breast cancer screening Imaging Med. 2 3 2010 313 323 [3] D.C. Moura M.A. Guevara López An evaluation of image descriptors combined with clinical data for breast cancer diagnosis Int. J. Comp. Assist. Radiol. Surg. 8 4 2013 561 574 10.1007/s11548-013-0838-2 [4] R. Ramos-Pollán M.A. Guevara-López C. Suárez-Ortega G. Díaz-Herrero J.M. Franco-Valiente M. Rubio-del Solar N. González-de Posada M.A.P. Vaz J. Loureiro I. Ramos Discovering mammography-based machine learning classifiers for breast cancer diagnosis J. Med. Syst. 36 4 2012 2259 2269 10.1007/s10916-011-9693-2 [5] R. Ramos-Pollán M.A. Guevara-López E. Oliveira A software framework for building biomedical machine learning classifiers through grid computing resources J. Med. Syst. 36 4 2012 2245 2257 10.1007/s10916-011-9692-3 [6] X. Liu J. Tang Mass classification in mammograms using selected geometry and texture features, and a new SVM-based feature selection method Syst. J. IEEE 8 3 2014 910 920 10.1109/JSYST.2013.2286539 [7] M. Dong X. Lu Y. Ma Y. Guo Y. Ma K. Wang An efficient approach for automated mass segmentation and classification in mammograms J. Digit. Imaging 28 5 2015 613 625 10.1007/s10278-015-9778-4 [8] Y. Bengio A. Courville P. Vincent Representation learning: a review and new perspectives IEEE Trans. Pattern Anal. Mach. Intell. 35 8 2013 1798 1828 doi:10.1109/TPAMI.2013.50 [9] J. Schmidhuber Deep learning in neural networks: an overview Neural Netw. 61 2015 85 117 10.1016/j.neunet.2014.09.003 [10] J. Arevalo A. Cruz-Roa F.A. González Hybrid image representation learning model with invariant features for basal cell carcinoma detection Proc. SPIE 8922 2013 10.1117/12.2035530 pp. 89220M-89220M-6 [11] A.A. Cruz-Roa J.E.A. Ovalle A. Madabhushi F.A.G. Osorio A deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection in: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013 2013 Springer 403 410 10.1007/978-3-642-40763-5_50 [12] H.I. Suk D. Shen Deep learning-based feature representation for AD/MCI classification Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) Vol. 8150 LNCS 2013 583 590 10.1007/978-3-642-40763-5_72 [13] H.-I. Suk S.-W. Lee D. Shen Latent feature representation with stacked auto-encoder for AD/MCI diagnosis Brain Struct. Funct. 2013 1 19 10.1007/s00429-013-0687-3 [14] H.-I. Suk S.-W. Lee D. Shen Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis Neuroimage 101 0 2014 569 582 10.1016/j.neuroimage.2014.06.077 [15] F. Li L. Tran K.-H. Thung S. Ji D. Shen J. Li Robust deep learning for improved classification of AD/MCI patients G. Wu D. Zhang L. Zhou Machine Learning in Medical Imaging, Vol. 8679 of Lecture Notes in Computer Science 2014 Springer International Publishing 240 247 10.1007/978-3-319-10581-9_30 [16] A. Prasoon K. Petersen C. Igel F. Lauze E. Dam M. Nielsen Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network K. Mori I. Sakuma Y. Sato C. Barillot N. Navab Medical Image Computing and Computer-Assisted Intervention - MICCAI 2013, Vol. 8150 of Lecture Notes in Computer Science 2013 Springer Berlin/Heidelberg 246 253 10.1007/978-3-642-40763-5_31 [17] A. Jalalian S.B. Mashohor H.R. Mahmud M.I.B. Saripan A.R.B. Ramli B. Karasfi Computer-aided detection/diagnosis of breast cancer in mammography and ultrasound: a review Clin. Imaging 37 3 2013 420 426 10.1016/j.clinimag.2012.09.024 [18] K. Petersen K. Chernoff M. Nielsen A.Y. Ng Breast density scoring with multiscale denoising autoencoders in: STMI workshop at MICCAI 2012 (15th International Conference on Medical Image Computing and Computer Assisted Intervention) 2012 [19] K. Petersen M. Nielsen P. Diao N. Karssemeijer M. Lillholm Breast tissue segmentation and mammographic risk scoring using deep learning H. Fujita T. Hara C. Muramatsu Breast Imaging, Vol. 8539 of Lecture Notes in Computer Science 2014 Springer International Publishing 88 94 10.1007/978-3-319-07887-8_13 [20] X.-S. Zhang A new approach for clustered MCs classification with sparse features learning and TWSVM Sci. World J. 2014 970287 10.1155/2014/970287 [21] J. Ge B. Sahiner L.M. Hadjiiski H.-P. Chan J. Wei M.A. Helvie C. Zhou Computer aided detection of clusters of microcalcifications on full field digital mammograms Med. Phys. 33 8 2006 2975 2988 [22] A.R. Jamieson K. Drukker M.L. Giger Breast image feature learning with adaptive deconvolutional networks 2012 10.1117/12.910710 [23] G.I. Andreea R. Pegza L. Lascu S. Bondari Z. Zoia Stoica A. Bondari The role of imaging techniques in diagnosis of breast cancer J. Curr. Health Sci. 37 2 2011 241 248 [24] N.P. Pérez M.A.G. López A. Silva I. Ramos Improving the Mann–Whitney statistical test for feature selection: an approach in breast cancer diagnosis on mammography Artif. Intell. Med. 2014 10.1016/j.artmed.2014.12.004 [25] K. Jarrett K. Kavukcuoglu M. Ranzato Y. LeCun What is the best multi-stage architecture for object recognition? in: Computer Vision, 2009 IEEE 12th International Conference on 2009 2146 2153 10.1109/ICCV.2009.5459469 [26] A. Krizhevsky I. Sutskever G.E. Hinton Imagenet classification with deep convolutional neural networks F. Pereira C. Burges L. Bottou K. Weinberger Advances in Neural Information Processing Systems 25 2012 Curran Associates Inc. 1097 1105 [27] N. Pinto D.D. Cox J.J. DiCarlo Why is real-world visual object recognition hard? PLoS Computat. Biol. 4 1 2008 e27 10.1371/journal.pcbi.0040027 [28] S. Lyu E. Simoncelli Nonlinear image representation using divisive normalization IEEE Conference on Computer Vision and Pattern Recognition, 2008. CVPR 2008 2008 1 8 10.1109/CVPR.2008.4587821 [29] Y. LeCun Learning invariant feature hierarchies A. Fusiello V. Murino R. Cucchiara Computer Vision – ECCV. Workshops and Demonstrations, Vol. 7583 of Lecture Notes in Computer Science 2012 Springer Berlin, Heidelberg, Florence, Italy 496 505 10.1007/978-3-642-33863-2_51 [30] A. Krizhevsky Learning multiple layers of features from tiny images, Tech. rep. 2009 University of Toronto Toronto [31] W.J. Veldkamp N. Karssemeijer Normalization of local contrast in mammograms IEEE Trans. Med. Imaging 19 7 2000 731 738 10.1109/42.875197 [32] Q. Ke Y. Li Is rotation a nuisance in shape recognition? in: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on 2014 4146 4153 10.1109/CVPR.2014.528 [33] I. Goodfellow D. Warde-farley M. Mirza A. Courville Y. Bengio Maxout networks S. Dasgupta D. Mcallester Proceedings of the 30th International Conference on Machine Learning (ICML-13), Vol. 28 JMLR Workshop and Conference Proceedings 2013 1319 1327 [34] N. Srivastava G. Hinton A. Krizhevsky I. Sutskever R. Salakhutdinov Dropout: a simple way to prevent neural networks from overfitting J. Mach. Learn. Res. 15 2014 1929 1958 [35] I.J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra, F. Bastien, Y. Bengio, Pylearn2: a machine learning research library, arXiv preprint arXiv:1308.4214. [36] F. Bastien P. Lamblin R. Pascanu J. Bergstra I.J. Goodfellow A. Bergeron N. Bouchard Y. Bengio Theano: new features and speed improvements Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop 2012 [37] J. Bergstra Y. Bengio Random search for hyper-parameter optimization J. Mach. Learn. Res. 13 2012 281 305 [38] L. Deng D. Yu Deep learning: methods and applications Found. Trends Signal Process. 7 3–4 2014 197 387 [39] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, T. Darrell, Decaf: A deep convolutional activation feature for generic visual recognition, arXiv preprint arXiv:1310.1531. [40] O. Russakovsky J. Deng Z. Huang A.C. Berg L. Fei-Fei Detecting avocados to zucchinis: what have we done, and where are we going? in: International Conference on Computer Vision (ICCV) 2013 [41] D. Rueda-Plata R. Ramos-Pollán F.A. González Supervised greedy layer-wise training for deep convolutional networks with small datasets M. Nú nez N. Nguyen D. Camacho B. Trawiński Computational Collective Intelligence, Vol. 9329 of Lecture Notes in Computer Science 2015 Springer International Publishing 275 284 10.1007/978-3-319-24069-5_26 "
    },
    {
        "doc_title": "Erratum : mRNA secondary structure optimization using a correlated stem-loop prediction (Nucleic Acids Research (2013) 41:6 (e73) DOI: 10.1093/nar/gks1473)",
        "doc_scopus_id": "84976435942",
        "doc_doi": "10.1093/nar/gkw127",
        "doc_eid": "2-s2.0-84976435942",
        "doc_date": "2016-06-20",
        "doc_type": "Erratum",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "I2x: An Automated Real-Time Integration and Interoperability Platform (Short Paper)",
        "doc_scopus_id": "84964852200",
        "doc_doi": "10.1109/SOCA.2015.16",
        "doc_eid": "2-s2.0-84964852200",
        "doc_date": "2016-02-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Event-driven",
            "External resources",
            "Heterogeneous data sources",
            "intelligent ETL",
            "Open source frameworks",
            "Publish/subscribe",
            "Real time",
            "Real time integration"
        ],
        "doc_abstract": "© 2015 IEEE.In the age of cloud computing, the amount of data and services available for researchers continues to grow. As data are easier to generate and services simpler to deploy, researchers continue to face insurmountable integration and interoperability challenges. A major ongoing issue across many fields regards the automated and real-time integration of data and services. Besides aggregating and processing disparate sources, we need an always up-to-date view of our heterogeneous and distributed datasets. Enabling realistic insights over acquired information requires that data are as fresh as possible. This perspective appears in opposition to traditional warehousing strategies, where data are manipulated regularly over large intervals. Our proposal introduces a reactive and event-driven framework to simplify and automate real-time data integration and interoperability. This platform, entitled i2x, streamlines the creation of customizable integration tasks connecting heterogeneous data sources with any kind of services. Integration is poll-based, with intelligent agents monitoring data sources, or push-based, where the platform waits for data submission by external resources. I2X delivers data to services through a comprehensive template engine, where the platform maps data from the original data source to the destination resources. I2X is an open-source framework available online at https://bioinformatics.ua.pt/i2x/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BioCreative V BioC track overview: collaborative biocurator assistant task for BioGRID",
        "doc_scopus_id": "85037580282",
        "doc_doi": "10.1093/database/baw121",
        "doc_eid": "2-s2.0-85037580282",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Automatic Data Processing",
            "Data Curation",
            "Data Mining",
            "Information Dissemination"
        ],
        "doc_abstract": "© 2016 Published by Oxford University Press 2016. This work is written by US Government employees and is in the public domain in the US.BioC is a simple XML format for text, annotations and relations, and was developed to achieve interoperability for biomedical text processing. Following the success of BioC in BioCreative IV, the BioCreative V BioC track addressed a collaborative task to build an assistant system for BioGRID curation. In this paper, we describe the framework of the collaborative BioC task and discuss our findings based on the user survey. This track consisted of eight subtasks including gene/protein/organism named entity recognition, protein-protein/genetic interaction passage identification and annotation visualization. Using BioC as their data-sharing and communication medium, nine teams, world-wide, participated and contributed either new methods or improvements of existing tools to address different subtasks of the BioC track. Results from different teams were shared in BioC and made available to other teams as they addressed different subtasks of the track. In the end, all submitted runs were merged using a machine learning classifier to produce an optimized output. The biocurator assistant system was evaluated by four BioGRID curators in terms of practical usability. The curators' feedback was overall positive and highlighted the user-friendly design and the convenient gene/protein curation tool based on text mining. Database URL: http://www.biocreative.org/tasks/biocreative-v/track-1-bioc/",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mining clinical attributes of genomic variants through assisted literature curation in Egas",
        "doc_scopus_id": "85009124421",
        "doc_doi": "10.1093/database/baw096",
        "doc_eid": "2-s2.0-85009124421",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "Animals",
            "Data Curation",
            "Data Mining",
            "Humans",
            "Internet",
            "User-Computer Interface",
            "Web Browser"
        ],
        "doc_abstract": "The veritable deluge of biological data over recent years has led to the establishment of a considerable number of knowledge resources that compile curated information extracted from the literature and store it in structured form, facilitating its use and exploitation. In this article, we focus on the curation of inherited genetic variants and associated clinical attributes, such as zygosity, penetrance or inheritance mode, and describe the use of Egas for this task. Egas is a web-based platform for text-mining assisted literature curation that focuses on usability through modern design solutions and simple user interactions. Egas offers a flexible and customizable tool that allows defining the concept types and relations of interest for a given annotation task, as well as the ontologies used for normalizing each concept type. Further, annotations may be performed on raw documents or on the results of automated concept identification and relation extraction tools. Users can inspect, correct or remove automatic text-mining results, manually add new annotations, and export the results to standard formats. Egas is compatible with the most recent versions of Google Chrome, Mozilla Firefox, Internet Explorer and Safari and is available for use at https://demo.bmd-software.com/egas/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A curation pipeline and web-services for PDF documents",
        "doc_scopus_id": "84985920440",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84985920440",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical literature",
            "Extract informations",
            "Side by sides",
            "Text extraction",
            "Text-based documents",
            "Textual content",
            "User friendly",
            "Web services platform"
        ],
        "doc_abstract": "The continuous growth of the biomedical literature and the need to efficiently find and extract information from its content led to the development of various text mining tools. More recently, these tools started being integrated in user-friendly applications facilitating their use by expert database curators. However, these tools were mainly designed to extract information from text based documents, in XML and other formats, while today a considerable part of the biomedical literature is published and distributed in PDF format. To address this limitation, we extended the web-based literature curation tool Egas, adding support for direct document curation and annotation over PDF files, with side-by-side visualization of the original PDF document and of the extracted textual content. Egas' PDF document processing and text-mining features are supported by a newly developed web-services platform built over Neji, a highly efficient information extraction framework. These web services allow integrating PDF text extraction and annotation capabilities to other tools and text mining pipelines.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semantic knowledge base construction from radiology reports",
        "doc_scopus_id": "84969263661",
        "doc_doi": "10.5220/0005709503450352",
        "doc_eid": "2-s2.0-84969263661",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Clinical reports",
            "Health care information system",
            "Healthcare institutions",
            "Natural languages",
            "Radiology reports",
            "Semantic knowledge",
            "Text mining"
        ],
        "doc_abstract": "Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.The tremendous quantity of data stored daily in healthcare institutions demands the development of new methods to summarize and reuse available information in clinical practice. In order to leverage modern healthcare information systems, new strategies must be developed that address challenges such as extraction of relevant information, data redundancy, and the lack of associations within the data. This article proposes a pipeline to overcome these challenges in the context of medical imaging reports, by automatically extracting and linking information, and summarizing natural language reports into an ontology model. Using data from the Physionet MIMIC II database, we created a semantic knowledge base with more than 6.5 millions of triples obtained from a collection of 16,000 radiology reports.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Prediction of cancer using network topological features",
        "doc_scopus_id": "84969234015",
        "doc_doi": "10.5220/0005696202070215",
        "doc_eid": "2-s2.0-84969234015",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Biological networks",
            "Cancer prediction",
            "Classification models",
            "Data mining methods",
            "Protein-protein interaction networks",
            "Supervised classification",
            "Topological features",
            "Topological properties"
        ],
        "doc_abstract": "Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.Several data mining methods have been applied to explore biological data and understand the mechanisms that regulate genetic and metabolic diseases. The underlying hypothesis is that the identification of signatures can help the clinical identification of diseased tissues. Under this principle many different methodologies have been tested mostly using unsupervised methods. A common trend consists in combining the information obtained from gene expression and protein-protein interaction networks analyses or, more recently, building series of complex networks to model system dynamics. Despite the positive results that these works present, they typically fail to generalize out of sample datasets. In this paper we describe a supervised classification approach, with a new methodology for extracting the network topology dynamics embedded in a disease system, to improve the capacity of cancer prediction, using exclusively the topological properties of biological networks as features. Four microarrays datasets were used, for testing and validation, three from breast cancer experiments and one from a liver cancer experiment. The obtained results corroborate the potential of the proposed methodology to predict a certain type of cancer and the necessity of applying different classification models to different types of cancer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A cloud architecture for teleradiology-as-a-service",
        "doc_scopus_id": "84968867602",
        "doc_doi": "10.3414/ME14-01-0052",
        "doc_eid": "2-s2.0-84968867602",
        "doc_date": "2016-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Advanced and Specialized Nursing",
                "area_abbreviation": "NURS",
                "area_code": "2902"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Cloud Computing",
            "Internet",
            "Radiology Information Systems",
            "Social Media",
            "Teleradiology",
            "Time Factors"
        ],
        "doc_abstract": "© Schattauer 2016.Background: Telemedicine has been promoted by healthcare professionals as an efficient way to obtain remote assistance from specialised centres, to get a second opinion about complex diagnosis or even to share knowledge among practitioners. The current economic restrictions in many countries are increasing the demand for these solutions even more, in order to optimize processes and reduce costs. However, despite some technological solutions already in place, their adoption has been hindered by the lack of usability, especially in the set-up process. Objectives: In this article we propose a telemedicine platform that relies on a cloud computing infrastructure and social media principles to simplify the creation of dynamic user-based groups, opening up opportunities for the establishment of teleradiology trust domains. Methods: The collaborative platform is provided as a Software-as-a-Service solution, supporting real time and asynchronous collaboration between users. To evaluate the solution, we have deployed the platform in a private cloud infrastructure. The system is made up of three main components - the collaborative framework, the Medical Management Information System (MMIS) and the HTML5 (Hyper Text Markup Language) Web client application - connected by a message-oriented middleware. Results: The solution allows physicians to create easily dynamic network groups for synchronous or asynchronous cooperation. The network created improves dataflow between colleagues and also knowledge sharing and cooperation through social media tools. The platform was implemented and it has already been used in two distinct scenarios: teaching of radiology and tele-reporting. Conclusions: Collaborative systems can simplify the establishment of telemedicine expert groups with tools that enable physicians to improve their clinical practice. Stream - lining the usage of this kind of systems through the adoption of Web technologies that are common in social media will increase the quality of current solutions, facilitating the sharing of clinical information, medical imaging studies and patient diagnostics among collaborators.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Conserved and highly expressed tRNA derived fragments in zebrafish",
        "doc_scopus_id": "84951822589",
        "doc_doi": "10.1186/s12867-015-0050-8",
        "doc_eid": "2-s2.0-84951822589",
        "doc_date": "2015-12-22",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            }
        ],
        "doc_keywords": [
            "Animals",
            "Base Sequence",
            "Cell Line",
            "Colorectal Neoplasms",
            "Conserved Sequence",
            "Gene Expression Regulation",
            "High-Throughput Nucleotide Sequencing",
            "Humans",
            "Mice",
            "NIH 3T3 Cells",
            "Regulatory Sequences, Ribonucleic Acid",
            "Ribonuclease III",
            "RNA Interference",
            "RNA, Small Untranslated",
            "RNA, Transfer",
            "Sequence Analysis, RNA",
            "Zebrafish"
        ],
        "doc_abstract": "© 2015 Soares et al.Background: Small non-coding RNAs (sncRNAs) are a class of transcripts implicated in several eukaryotic regulatory mechanisms, namely gene silencing and chromatin regulation. Despite significant progress in their identification by next generation sequencing (NGS) we are still far from understanding their full diversity and functional repertoire. Results: Here we report the identification of tRNA derived fragments (tRFs) by NGS of the sncRNA fraction of zebrafish. The tRFs identified are 18-30 nt long, are derived from specific 5′ and 3′ processing of mature tRNAs and are differentially expressed during development and in differentiated tissues, suggesting that they are likely produced by specific processing rather than random degradation of tRNAs. We further show that a highly expressed tRF (5′tRF-ProCGG) is cleaved in vitro by Dicer and has silencing ability, indicating that it can enter the RNAi pathway. A computational analysis of zebrafish tRFs shows that they are conserved among vertebrates and mining of publicly available datasets reveals that some 5′tRFs are differentially expressed in disease conditions, namely during infection and colorectal cancer. Conclusions: tRFs constitute a class of conserved regulatory RNAs in vertebrates and may be involved in mechanisms of genome regulation and in some diseases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ann2RDF: Moving annotations to semantic web",
        "doc_scopus_id": "84967145648",
        "doc_doi": "10.1145/2837185.2837253",
        "doc_eid": "2-s2.0-84967145648",
        "doc_date": "2015-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Annotation",
            "Annotation tool",
            "Data transformation",
            "Representation model",
            "Semantic knowledge",
            "Text mining",
            "Transition process"
        ],
        "doc_abstract": "© 2015 ACM.The annotation of concepts and susceptible interactions has been assuming a key role in the extraction of relevant information from published documents. However, distinct annotation tools generate also different formats, creating a barrier to efficiently combine and exchange this information. The migration of curated information into semantic web format and services provides an additional value to share that knowledge, but data transformation represents here an additional challenge. In this manuscript, we present a unified layer between text-mining tools and semantic web services to reduce the effort of combining different formats. The Ann2RDF is focused on reusing existing curated data from external text-mining tools to improve their availability through an open representation model. This result in a more suitable transition process, in which desired annotations are enriched with the possibility to be shared, compared and reused across semantic Knowledge Bases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Erratum: Species-Specific Codon Context Rules Unveil Non-Neutrality Effects of Synonymous Mutations (PLoS ONE (2011) 6:10 (e26817))",
        "doc_scopus_id": "84956899012",
        "doc_doi": "10.1371/journal.pone.0145593",
        "doc_eid": "2-s2.0-84956899012",
        "doc_date": "2015-12-01",
        "doc_type": "Erratum",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Uncovering microbial duality within human microbiomes: A novel algorithm for the analysis of host-pathogen interactions",
        "doc_scopus_id": "84953319377",
        "doc_doi": "10.1109/EMBC.2015.7319086",
        "doc_eid": "2-s2.0-84953319377",
        "doc_date": "2015-11-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Bacteria",
            "Host-Pathogen Interactions",
            "Humans",
            "Microbiota"
        ],
        "doc_abstract": "© 2015 IEEE.Microbial species thrive within human hosts by establishing complex associations between themselves and the host. Even though species diversity can be measured (alpha- and beta-diversity), a methodology to estimate the impact of microorganisms in human pathways is still lacking. In this work we propose a computational approach to estimate which human pathways are targeted the most by microorganisms, while also identifying which microorganisms are prominent in this targeting. Our results were consistent with literature evidence, and thus we propose this methodology as a new prospective approach to be used for screening potentially impacted pathways.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A machine learning methodology for medical imaging anonymization",
        "doc_scopus_id": "84953281406",
        "doc_doi": "10.1109/EMBC.2015.7318626",
        "doc_eid": "2-s2.0-84953281406",
        "doc_date": "2015-11-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Diagnostic Imaging",
            "Machine Learning",
            "Privacy"
        ],
        "doc_abstract": "© 2015 IEEE.Privacy protection is a major requirement for the complete success of EHR systems, becoming even more critical in collaborative scenarios, where data is shared among institutions and practitioners. While textual data can be easily de-identified, patient data in medical images implies a more elaborate approach. In this work we present a solution for sensitive word identification in medical images based on a combination of two machine-learning models, achieving a F1-score of 0.94. Three experts evaluated the system performance. They analyzed the output of the present methodology and categorized the studies in three groups: studies that had their sensitive words removed (true positive), studies with complete patient identity (false negative) and studies with mistakenly removed data (false positive). The experts were unanimous regarding the relevance of the present tool in collaborative medical environments, as it may improve the exchange of anonymized patient data between institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Convolutional neural networks for mammography mass lesion classification",
        "doc_scopus_id": "84953238909",
        "doc_doi": "10.1109/EMBC.2015.7318482",
        "doc_eid": "2-s2.0-84953238909",
        "doc_date": "2015-11-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Mammography",
            "Neural Networks (Computer)",
            "ROC Curve"
        ],
        "doc_abstract": "© 2015 IEEE.Feature extraction is a fundamental step when mammography image analysis is addressed using learning based approaches. Traditionally, problem dependent handcrafted features are used to represent the content of images. An alternative approach successfully applied in other domains is the use of neural networks to automatically discover good features. This work presents an evaluation of convolutional neural networks to learn features for mammography mass lesions before feeding them to a classification stage. Experimental results showed that this approach is a suitable strategy outperforming the state-of-the-art representation from 79.9% to 86% in terms of area under the ROC curve.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An automated real-time integration and interoperability framework for bioinformatics",
        "doc_scopus_id": "84945232213",
        "doc_doi": "10.1186/s12859-015-0761-3",
        "doc_eid": "2-s2.0-84945232213",
        "doc_date": "2015-10-13",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Bioinformatics applications",
            "Event-driven",
            "Extract , transform and loads",
            "Heterogeneous services",
            "Intelligent ETL",
            "Interoperability framework",
            "Publish/subscribe",
            "Workflow",
            "Automation",
            "Cloud Computing",
            "Computational Biology",
            "Humans",
            "Software"
        ],
        "doc_abstract": "© 2015 Lopes and Oliveira.Background: In recent years data integration has become an everyday undertaking for life sciences researchers. Aggregating and processing data from disparate sources, whether through specific developed software or via manual processes, is a common task for scientists. However, the scope and usability of the majority of current integration tools fail to deal with the fast growing and highly dynamic nature of biomedical data. Results: In this work we introduce a reactive and event-driven framework that simplifies real-time data integration and interoperability. This platform facilitates otherwise difficult tasks, such as connecting heterogeneous services, indexing, linking and transferring data from distinct resources, or subscribing to notifications regarding the timeliness of dynamic data. For developers, the framework automates the deployment of integrative and interoperable bioinformatics applications, using atomic data storage for content change detection, and enabling agent-based intelligent extract, transform and load tasks. Conclusions: This work bridges the gap between the growing number of services, accessing specific data sources or algorithms, and the growing number of users, performing simple integration tasks on a recurring basis, through a streamlined workspace available to researchers and developers alike.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Computational methodology for predicting the landscape of the human-microbial interactome region level influence",
        "doc_scopus_id": "84947041842",
        "doc_doi": "10.1142/S0219720015500237",
        "doc_eid": "2-s2.0-84947041842",
        "doc_date": "2015-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Algorithms",
            "Computational Biology",
            "Computing Methodologies",
            "Databases, Protein",
            "Female",
            "Genetic Variation",
            "Host-Pathogen Interactions",
            "Humans",
            "Male",
            "Metagenomics",
            "Microbiota",
            "Organ Specificity",
            "Phylogeny",
            "Protein Interaction Mapping"
        ],
        "doc_abstract": "© 2015 Imperial College Press.Microbial communities thrive in close association among themselves and with the host, establishing protein-protein interactions (PPIs) with the latter, and thus being able to benefit (positively impact) or disturb (negatively impact) biological events in the host. Despite major collaborative efforts to sequence the Human microbiome, there is still a great lack of understanding their impact. We propose a computational methodology to predict the impact of microbial proteins in human biological events, taking into account the abundance of each microbial protein and its relation to all other microbial and human proteins. This alternative methodology is centered on an improved impact estimation algorithm that integrates PPIs between human and microbial proteins with Reactome pathway data. This methodology was applied to study the impact of 24 microbial phyla over different cellular events, within 10 different human microbiomes. The results obtained confirm findings already described in the literature and explore new ones. We believe the Human microbiome can no longer be ignored as not only is there enough evidence correlating microbiome alterations and disease states, but also the return to healthy states once these alterations are reversed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a knowledge federation of linked patient registries",
        "doc_scopus_id": "84943339930",
        "doc_doi": "10.1109/CISTI.2015.7170546",
        "doc_eid": "2-s2.0-84943339930",
        "doc_date": "2015-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Complex Processes",
            "Data translations",
            "Electronic patient record",
            "Engineering tools",
            "Knowledge federations",
            "Knowledge-oriented",
            "Security and privacy",
            "State of the art"
        ],
        "doc_abstract": "© 2015 AISTI.Rare disease patient registries are now an essential tool for all clinical stakeholders. These systems' features aim to improve patient treatments by collecting comprehensive electronic patient records. Understanding these data is a vital step towards personalized medicine. Yet, the growing number of disease-specific patient registries brings new challenges for life sciences developers. These systems are closed data silos, with independent formats and data models. As they were built with security and privacy in mind, available tools lack comprehensive data access mechanisms, thus making data sharing a complex process. However, exchanging knowledge is essential to a better understanding of studied diseases. To tackle these challenges we introduce a semantic web-based architecture to connect distributed and heterogeneous registries. This enables the federation of knowledge between multiple independent environments. The semantic web paradigm enhances the ways we deal with data, optimising how we can create, infer and publish knowledge. Hence, we adopt these modern standards to deploy patient registry add ons. These can extract anonymised data and elevate them to a knowledge-oriented format, common to all registries. The outcome is a unique semantic layer, connecting miscellaneous registries, which we access using federated querying. Ultimately, this strategy empowers an holistic view through connected registries, enabling state-of-the-art semantic data sharing and access.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Challenges and opportunities for exploring patient-level data: Preliminary results",
        "doc_scopus_id": "84943328616",
        "doc_doi": "10.1109/CISTI.2015.7170531",
        "doc_eid": "2-s2.0-84943328616",
        "doc_date": "2015-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Data exploration",
            "Evaluation framework",
            "Large-scale projects"
        ],
        "doc_abstract": "© 2015 AISTI.The exploration of clinical registries is the subject of many large-scale projects. The overarching goal behind existing initiatives is to uncover the full potential behind patient-level data, leading to a better understanding of diseases and treatments effectiveness. In this work we performed a review of well-known large-scale initiatives, strategies and projects where the exploration of patient-level data is an underlying goal. We setup an evaluation framework assessing, for each project, its technological and scientific outcomes, partners' features and other miscellaneous data-related topics. Our aim is to unveil ongoing challenges in this domain, exploring how we can transform them into opportunities for future developments. Only with this evolution we can continue moving forward towards personalised medicine.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integration-as-a-service for bioinformatics",
        "doc_scopus_id": "84943303227",
        "doc_doi": "10.1109/CISTI.2015.7170530",
        "doc_eid": "2-s2.0-84943303227",
        "doc_date": "2015-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Bioinformatics applications",
            "Command line",
            "Common features",
            "Complex applications",
            "Data translations",
            "Essential services",
            "Integration of services",
            "Multiple resources"
        ],
        "doc_abstract": "© 2015 AISTI.Data integration is currently an essential service for bioinformatics researchers and developers. The ability to import, transform and export data from multiple resources is also a common feature for miscellaneous platforms, ranging from complex application suites to simple standalone command-line tools. Despite the quality behind available solutions, data and services' evolution keeps pushing integration demands forward, to cope, for instance, with dynamic data sources and cloud-based technologies. In this work we introduce a cloud-based framework to streamline the integration of services and data in bioinformatics. Our goal is to facilitate the creation of next-generation bioinformatics applications that rely heavily on automated, real-time, integration and interoperability tasks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Normalizing Heterogeneous Medical Imaging Data to Measure the Impact of Radiation Dose",
        "doc_scopus_id": "84946490879",
        "doc_doi": "10.1007/s10278-015-9805-5",
        "doc_eid": "2-s2.0-84946490879",
        "doc_date": "2015-05-27",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "ALARA",
            "Automatic monitoring",
            "Body regions",
            "DICOM",
            "Healthcare institutions",
            "PACS",
            "Radiation Exposure",
            "Set-ups",
            "Humans",
            "Quality Assurance, Health Care",
            "Radiation Dosage",
            "Radiology Information Systems",
            "Tomography, X-Ray Computed"
        ],
        "doc_abstract": "© 2015, Society for Imaging Informatics in Medicine.The production of medical imaging is a continuing trend in healthcare institutions. Quality assurance for planned radiation exposure situations (e.g. X-ray, computer tomography) requires examination-specific set-ups according to several parameters, such as patient’s age and weight, body region and clinical indication. These data are normally stored in several formats and with different nomenclatures, which hinder the continuous and automatic monitoring of these indicators and the comparison between several institutions and equipment. This article proposes a framework that aggregates, normalizes and provides different views over collected indicators. The developed tool can be used to improve the quality of radiologic procedures and also for benchmarking and auditing purposes. Finally, a case study and several experimental results related to radiation exposure and productivity are presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BioinformaticsUA: Machine Learning and Rule-Based Recognition of Disorders and Clinical Attributes from Patient Notes",
        "doc_scopus_id": "85122039614",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85122039614",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            }
        ],
        "doc_keywords": [
            "Entity recognition",
            "F-score",
            "Large amounts",
            "Machine-learning",
            "Normalisation",
            "Rule-based method",
            "Rule-based recognition",
            "SNOMED-CT",
            "Subtask",
            "Text-analysis methods"
        ],
        "doc_abstract": "© 2015 Association for Computational LinguisticsNatural language processing and text analysis methods offer the potential of uncovering hidden associations from large amounts of unprocessed texts. The SemEval-2015 Analysis of Clinical Text task aimed at fostering research on the application of these methods in the clinical domain. The proposed task consisted of disorder identification with normalization to SNOMED-CT concepts, and disorder attribute identification, or template filling. We participated in both sub-tasks, using a combination of machine-learning and rules for recognizing and normalizing disease mentions, and rule-based methods for template filling. We achieved an F-score of 71.2% in the entity recognition and normalization task, and a slot weighted accuracy of 69.5% in the template filling task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Architecture to Summarize Patient-Level Data Across Borders and Countries",
        "doc_scopus_id": "84951971617",
        "doc_doi": "10.3233/978-1-61499-564-7-687",
        "doc_eid": "2-s2.0-84951971617",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Aggregated information",
            "Cohorts",
            "Electronic health record systems",
            "Health care information system",
            "Observational study",
            "Real-world",
            "Translational medicine",
            "Translational Research",
            "Confidentiality",
            "Data Mining",
            "Electronic Health Records",
            "Europe",
            "Information Dissemination",
            "Medical Record Linkage",
            "Models, Organizational",
            "Translational Medical Research"
        ],
        "doc_abstract": "© 2015 IMIA and IOS Press.Translational medicine is becoming fundamental in promoting information flow between basic research and clinical practice, and in improving patients' health. The need for efficient systems to process and share information from multiple sources-including distinct areas of medicine-is a pressing need in biomedical research. Nevertheless, healthcare information systems are fragmented over different databases, medical institutions, and geographical locations. There are already several approaches to tackle this problem based on centralized or distributed solutions. Nonetheless, mainly due to privacy reasons, these models only work for specific silos. In this paper, we present a new ecosystem for connecting database owners and researchers. Our approach consists of gathering, via a common fingerprint, an extensive characterization of dispersed databases. This fingerprint typically contains high-level aggregated information addressing questions at a population level and allowing, for instance, quick identification of databases with data that may help to answer a specific research question. This work is being conducted in the context of EMIF (European Medical Information Framework), an IMI (The Innovative Medicines Initiative) joint undertaking funded project, wherein the Catalogue is being used to collect data from cohorts and Electronic Health Record systems of several European countries.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Overview of Biomolecular Event Extraction from Scientific Documents",
        "doc_scopus_id": "84947447541",
        "doc_doi": "10.1155/2015/571381",
        "doc_eid": "2-s2.0-84947447541",
        "doc_date": "2015-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Automatic extraction",
            "Biological process",
            "Biomedical literature",
            "Future perspectives",
            "Linguistic phenomena",
            "NAtural language processing",
            "Scientific documents",
            "State-of-the-art approach",
            "Animals",
            "Computational Biology",
            "Data Mining",
            "Databases, Factual",
            "Humans",
            "Machine Learning",
            "Natural Language Processing",
            "Systems Biology"
        ],
        "doc_abstract": "© 2015 Jorge A. Vanegas et al.This paper presents a review of state-of-the-art approaches to automatic extraction of biomolecular events from scientific texts. Events involving biomolecules such as genes, transcription factors, or enzymes, for example, have a central role in biological processes and functions and provide valuable information for describing physiological and pathogenesis mechanisms. Event extraction from biomedical literature has a broad range of applications, including support for information retrieval, knowledge summarization, and information extraction and discovery. However, automatic event extraction is a challenging task due to the ambiguity and diversity of natural language and higher-level linguistic phenomena, such as speculations and negations, which occur in biological texts and can lead to misunderstanding or incorrect interpretation. Many strategies have been proposed in the last decade, originating from different research areas such as natural language processing, machine learning, and statistics. This review summarizes the most representative approaches in biomolecular event extraction and presents an analysis of the current state of the art and of commonly used methods, features, and tools. Finally, current research trends and future perspectives are also discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A document processing pipeline for annotating chemical entities in scientific documents",
        "doc_scopus_id": "84946128141",
        "doc_doi": "10.1186/1758-2946-7-S1-S7",
        "doc_eid": "2-s2.0-84946128141",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Physical and Theoretical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1606"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Copyright © 2015 Campos et al.Background: The recognition of drugs and chemical entities in text is a very important task within the field of biomedical information extraction, given the rapid growth in the amount of published texts (scientific papers, patents, patient records) and the relevance of these and other related concepts. If done effectively, this could allow exploiting such textual resources to automatically extract or infer relevant information, such as drug profiles, relations and similarities between drugs, or associations between drugs and potential drug targets. The objective of this work was to develop and validate a document processing and information extraction pipeline for the identification of chemical entity mentions in text. Results: We used the BioCreative IV CHEMDNER task data to train and evaluate a machine-learning based entity recognition system. Using a combination of two conditional random field models, a selected set of features, and a post-processing stage, we achieved F-measure results of 87.48% in the chemical entity mention recognition task and 87.75% in the chemical document indexing task. Conclusions: We present a machine learning-based solution for automatic recognition of chemical and drug names in scientific documents. The proposed approach applies a rich feature set, including linguistic, orthographic, morphological, dictionary matching and local context features. Post-processing modules are also integrated, performing parentheses correction, abbreviation resolution and filtering erroneous mentions using an exclusion list derived from the training data. The developed methods were implemented as a document annotation tool and web service, freely available at http://bioinformatics.ua.pt/becas-chemicals/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Challenges and opportunities for exploring patient-level data",
        "doc_scopus_id": "84945400618",
        "doc_doi": "10.1155/2015/150435",
        "doc_eid": "2-s2.0-84945400618",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [
            "Computer Security",
            "Confidentiality",
            "Data Mining",
            "Electronic Health Records",
            "Health Records, Personal",
            "Information Storage and Retrieval",
            "Patient Participation",
            "Precision Medicine"
        ],
        "doc_abstract": "© 2015 Pedro Lopes et al.The proper exploration of patient-level data will pave the way towards personalised medicine. To better assess the state of the art in this field we identify the challenges and uncover the opportunities for the exploration of patient-level data through the review of well-known initiatives and projects focusing on the exploration of patient-level data. These cover a broad array of topics, from genomics to patient registries up to rare diseases research, among others. For each, we identified basic goals, involved partners, defined strategies and key technological and scientific outcomes, establishing the foundation for our analysis framework with four pillars: control, sustainability, technology, and science. Substantial research outcomes have been produced towards the exploration of patient-level data. The potential behind these data will be essential to realise the personalised medicine premise in upcoming years. Hence, relevant stakeholders continually push forward new developments in this domain, bringing novel opportunities that are ripe for exploration. Despite last decade's translational research advances, personalised medicine is still far from being a reality. Patients' data underlying potential goes beyond daily clinical practice. There are miscellaneous challenges and opportunities open for the exploration of these data by academia and business stakeholders.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "New insights in echocardiography based left-ventricle dynamics assessment",
        "doc_scopus_id": "84944463907",
        "doc_doi": "10.1007/978-3-319-16483-0_30",
        "doc_eid": "2-s2.0-84944463907",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic systems",
            "Cardio-vascular disease",
            "Clinical diagnosis",
            "Echocardiogram",
            "Left ventricles",
            "Network classifiers",
            "Quantitative parameters",
            "Sensitivity and specificity"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Cardiovascular diseases affect a high percentage of people worldwide, being currently a major clinical concern. Echocardiograms are useful exams that allow monitoring the heart dynamics. However, their analysis depends on trained physicians with well-developed skills to recognize pathology from morphological and dynamical cues. Furthermore, these exams are often difficult to interpret due to image quality. Therefore, automatic systems able to analyze echocardiographic quantitative parameters in order to convey useful information will provide a great help in clinical diagnosis. A robust dataset was built, comprising variables associated with left-ventricle dynamics, which were studied in order to build a classifier able to discriminate between pathological and non-pathological records. To accomplish this goal, a network classifier based on decision tree was developed, using as input the left ventricle velocity over a complete cardiac cycle. This classifier revealed both sensitivity and specificity over 90% in discriminating non-pathological records, or pathological records (dilated or hypertrophic).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A semantic layer for unifying and exploring biomedical document curation results",
        "doc_scopus_id": "84944460806",
        "doc_doi": "10.1007/978-3-319-16483-0_2",
        "doc_eid": "2-s2.0-84944460806",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical documents",
            "Biomedical information",
            "Computerized solution",
            "Curation",
            "Multiple interfaces",
            "Scientific publications",
            "Scientific workflows",
            "Text mining"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Tackling the ever-growing amount of specialized literature in the life sciences domain is a paramount challenge. Various scientific workflows depend on using domain knowledge from resources that summarize, in structured form, validated information extracted from scientific publications. Manual curation of these data is a demanding task, and latest strategies use computerized solutions to aid in the analysis, extraction and storage of relevant concepts and their respective attributes and relationships. The outcome of these complex document curation workflows provides valuable insights into the overwhelming amount of biomedical information being produced. Yet, the majority of automated and interactive annotation tools are not open, limiting access to knowledge and reducing the potential scope of the manually curated information. In this manuscript, we propose an interoperable semantic layer to unify document curation results and enable their proper exploration through multiple interfaces geared towards bioinformatics developers and general life sciences researchers. This enables a unique scenario where results from computational annotation tools are harmonized and further integrated into rich semantic knowledge bases, providing a solid foundation for discovering knowledge.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Recommender System for Medical Imaging Diagnostic",
        "doc_scopus_id": "84937510681",
        "doc_doi": "10.3233/978-1-61499-512-8-461",
        "doc_eid": "2-s2.0-84937510681",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "CBIR",
            "Clinical practices",
            "Context based retrieval",
            "Context-based",
            "Diagnostic decisions",
            "Healthcare institutions",
            "Imaging diagnostics",
            "Large volumes",
            "Data Mining",
            "Decision Support Systems, Clinical",
            "Diagnostic Imaging",
            "Expert Systems",
            "Image Interpretation, Computer-Assisted",
            "Natural Language Processing",
            "Radiology Information Systems",
            "Search Engine"
        ],
        "doc_abstract": "© 2015 European Federation for Medical Informatics (EFMI).The large volume of data captured daily in healthcare institutions is opening new and great perspectives about the best ways to use it towards improving clinical practice. In this paper we present a context-based recommender system to support medical imaging diagnostic. The system relies on data mining and context-based retrieval techniques to automatically lookup for relevant information that may help physicians in the diagnostic decision.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "RecRWR: A recursive random walk method for improved identification of diseases",
        "doc_scopus_id": "84927144569",
        "doc_doi": "10.1155/2015/747156",
        "doc_eid": "2-s2.0-84927144569",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            }
        ],
        "doc_keywords": [
            "Databases, Genetic",
            "Gene Expression Profiling",
            "Genetic Diseases, Inborn",
            "High-Throughput Nucleotide Sequencing",
            "Humans",
            "Models, Genetic",
            "Oligonucleotide Array Sequence Analysis"
        ],
        "doc_abstract": "© 2015 Joel Perdiz Arrais and José Luís Oliveira.High-throughput methods such as next-generation sequencing or DNA microarrays lack precision, as they return hundreds of genes for a single disease profile. Several computational methods applied to physical interaction of protein networks have been successfully used in identification of the best disease candidates for each expression profile. An open problem for these methods is the ability to combine and take advantage of the wealth of biomedical data publicly available. We propose an enhanced method to improve selection of the best disease targets for a multilayer biomedical network that integrates PPI data annotated with stable knowledge from OMIM diseases and GO biological processes. We present a comprehensive validation that demonstrates the advantage of the proposed approach, Recursive Random Walk with Restarts (RecRWR). The obtained results outline the superiority of the proposed approach, RecRWR, in identifying disease candidates, especially with high levels of biological noise and benefiting from all data available.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An event-driven architecture for biomedical data integration and interoperability",
        "doc_scopus_id": "84925258148",
        "doc_doi": "10.1007/978-3-319-16480-9_17",
        "doc_eid": "2-s2.0-84925258148",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical applications",
            "Biomedical data integration",
            "Cloud-based architectures",
            "Data translations",
            "Data warehouse and repositories",
            "Event-driven architectures",
            "Real time integration",
            "Software engineering tools"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Connecting data and services is nowadays an essential skill for life sciences researchers. Handling data from unrelated sources using problemspecific software or labor-intensive tools are common tasks. Despite the latest advances, integration and interoperability developments still involve primitive interactions and manual triggers. On the one hand, available tools cover specific niches, ignoring more generic problems. On the other hand, overly complex tools with excessive features dominate the market. In this proposal we introduce a cloud-based architecture to simplify real-time integration and interoperability for biomedical data. We support our strategy in an event-driven service-oriented architecture, where new data are pushed from any content source, through an intelligent proxy, for delivery to heterogeneous endpoints. This enables a passive integration approach, providing developers with a streamlined solution for deploying integrative biomedical applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving the accuracy of recombinant protein production through integration of bioinformatics, statistical and mass spectrometry methodologies",
        "doc_scopus_id": "84923212005",
        "doc_doi": "10.1111/febs.13181",
        "doc_eid": "2-s2.0-84923212005",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Cell Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1307"
            }
        ],
        "doc_keywords": [
            "Biotechnology",
            "Computational Biology",
            "Data Interpretation, Statistical",
            "Escherichia coli",
            "Mass Spectrometry",
            "Plasmodium falciparum",
            "Recombinant Proteins"
        ],
        "doc_abstract": "© 2014 FEBS.Heterologous protein production is a key technology for biotechnological, health sciences and many other research fields. Various approaches have been developed for its optimization, but the research emphasis has been on optimization of protein yield rather than protein quality. In this study, we have established a workflow for synthetic gene optimization for heterologous protein expression that combines bioinformatics, laboratory experiments, mass spectrometry and statistical analysis. Two gene primary structure analysis platforms, Anaconda and EuGene, and multivariate optimization methods were employed to re-design the Plasmodium falciparum lysyl-tRNA synthetase gene for optimal expression in Escherichia coli. Synthetic genes were expressed from common vectors, and amino acid mis-incorporations in the expressed proteins were detected and quantified using mass spectrometry. The association between the identified amino acid mis-incorporations and 23 gene variables was then analysed. The synthetic genes yielded significantly higher levels of protein relative to the wild-type gene, but 71 amino acid mis-incorporation sites were observed along the whole protein and across the synthetic genes that were statistically associated with specific codons and protein secondary structures. The optimization method that led to production of the most accurate protein was based on a multivariate approach that combined variables that are known to influence mRNA translation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semantic search over DICOM Repositories",
        "doc_scopus_id": "84949924636",
        "doc_doi": "10.1109/ICHI.2014.41",
        "doc_eid": "2-s2.0-84949924636",
        "doc_date": "2014-03-02",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "PACS",
            "Semantic data",
            "Semantic representation",
            "Semantic search",
            "Software implementation",
            "Standard architecture",
            "Technical solutions"
        ],
        "doc_abstract": "© 2014 IEEE.The integration of semantic representation in medical information systems has been seen as a key opportunity to achieve full interoperability between distinct technical solutions and to extract knowledge from existing repositories. In this paper, we propose an ontology-based medical imaging archive that provides a generic, dynamic and standard architecture to interrogate the repository. Moreover, the solution flexibility makes possible to improve the search results only by updating the ontologies used, without any changes in the software implementation. This archive is based on a triple store that follows the Rad Lex ontology. To validate the proposed system, the performance of storage and search queries were compared against the results obtained in a traditional PACS archive.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Computational prediction of the human-microbial oral interactome",
        "doc_scopus_id": "84897570043",
        "doc_doi": "10.1186/1752-0509-8-24",
        "doc_eid": "2-s2.0-84897570043",
        "doc_date": "2014-02-27",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: The oral cavity is a complex ecosystem where human chemical compounds coexist with a particular microbiota. However, shifts in the normal composition of this microbiota may result in the onset of oral ailments, such as periodontitis and dental caries. In addition, it is known that the microbial colonization of the oral cavity is mediated by protein-protein interactions (PPIs) between the host and microorganisms. Nevertheless, this kind of PPIs is still largely undisclosed. To elucidate these interactions, we have created a computational prediction method that allows us to obtain a first model of the Human-Microbial oral interactome.Results: We collected high-quality experimental PPIs from five major human databases. The obtained PPIs were used to create our positive dataset and, indirectly, our negative dataset. The positive and negative datasets were merged and used for training and validation of a naïve Bayes classifier. For the final prediction model, we used an ensemble methodology combining five distinct PPI prediction techniques, namely: literature mining, primary protein sequences, orthologous profiles, biological process similarity, and domain interactions. Performance evaluation of our method revealed an area under the ROC-curve (AUC) value greater than 0.926, supporting our primary hypothesis, as no single set of features reached an AUC greater than 0.877. After subjecting our dataset to the prediction model, the classified result was filtered for very high confidence PPIs (probability ≥ 1-10-7), leading to a set of 46,579 PPIs to be further explored.Conclusions: We believe this dataset holds not only important pathways involved in the onset of infectious oral diseases, but also potential drug-targets and biomarkers. The dataset used for training and validation, the predictions obtained and the network final network are available at http://bioinformatics.ua.pt/software/oralint. © 2014 Coelho et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Variobox: Automatic detection and annotation of human genetic variants",
        "doc_scopus_id": "84891943923",
        "doc_doi": "10.1002/humu.22474",
        "doc_eid": "2-s2.0-84891943923",
        "doc_date": "2014-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Genetics (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2716"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Triggered by the sequencing of the human genome, personalized medicine has been one of the fastest growing research areas in the last decade. Multiple software and hardware technologies have been developed by several projects, culminating in the exponential growth of genetic data. Considering the technological developments in this field, it is now fairly easy and inexpensive to obtain genetic profiles for unique individuals, such as those performed by several genetic analysis companies. The availability of computational tools that simplify genetic data analysis and the disclosure of biomedical evidences are of utmost importance. We present Variobox, a desktop tool to annotate, analyze, and compare human genes. Variobox obtains variant annotation data from WAVe, protein metadata annotations from Protein Data Bank, and sequences are obtained from Locus Reference Genomic or RefSeq databases. To explore the data, Variobox provides an advanced sequence visualization that enables agile navigation through genetic regions. DNA sequencing data can be compared with reference sequences retrieved from LRG or RefSeq records, identifying and automatically annotating new potential variants. These features and data, ranging from patient sequences to HGVS-compliant variant descriptions, are combined in an intuitive interface to analyze genes and variants. Variobox is a Java application, available at http://bioinformatics.ua.pt/variobox. © 2013 WILEY PERIODICALS, INC.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GeneCommittee: A web-based tool for extensively testing the discriminatory power of biologically relevant gene sets in microarray data classification",
        "doc_scopus_id": "84893179202",
        "doc_doi": "10.1186/1471-2105-15-31",
        "doc_eid": "2-s2.0-84893179202",
        "doc_date": "2014-01-30",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: The diagnosis and prognosis of several diseases can be shortened through the use of different large-scale genome experiments. In this context, microarrays can generate expression data for a huge set of genes. However, to obtain solid statistical evidence from the resulting data, it is necessary to train and to validate many classification techniques in order to find the best discriminative method. This is a time-consuming process that normally depends on intricate statistical tools.Results: geneCommittee is a web-based interactive tool for routinely evaluating the discriminative classification power of custom hypothesis in the form of biologically relevant gene sets. While the user can work with different gene set collections and several microarray data files to configure specific classification experiments, the tool is able to run several tests in parallel. Provided with a straightforward and intuitive interface, geneCommittee is able to render valuable information for diagnostic analyses and clinical management decisions based on systematically evaluating custom hypothesis over different data sets using complementary classifiers, a key aspect in clinical research.Conclusions: geneCommittee allows the enrichment of microarrays raw data with gene functional annotations, producing integrated datasets that simplify the construction of better discriminative hypothesis, and allows the creation of a set of complementary classifiers. The trained committees can then be used for clinical research and diagnosis. Full documentation including common use cases and guided analysis workflows is freely available at http://sing.ei.uvigo.es/GC/. © 2014 Reboiro-Jato et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Twitter: A good place to detect health conditions",
        "doc_scopus_id": "84900419026",
        "doc_doi": "10.1371/journal.pone.0086191",
        "doc_eid": "2-s2.0-84900419026",
        "doc_date": "2014-01-29",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "With the proliferation of social networks and blogs, the Internet is increasingly being used to disseminate personal health information rather than just as a source of information. In this paper we exploit the wealth of user-generated data, available through the micro-blogging service Twitter, to estimate and track the incidence of health conditions in society. The method is based on two stages: we start by extracting possibly relevant tweets using a set of specially crafted regular expressions, and then classify these initial messages using machine learning methods. Furthermore, we selected relevant features to improve the results and the execution times. To test the method, we considered four health states or conditions, namely flu, depression, pregnancy and eating disorders, and two locations, Portugal and Spain. We present the results obtained and demonstrate that the detection results and the performance of the method are improved after feature selection. The results are promising, with areas under the receiver operating characteristic curve between 0.7 and 0.9, and f-measure values around 0.8 and 0.9. This fact indicates that such approach provides a feasible solution for measuring and tracking the evolution of health states within the society. © 2014 Prieto et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "TrigNER: Automatically optimized biomedical event trigger recognition on scientific documents",
        "doc_scopus_id": "84892661802",
        "doc_doi": "10.1186/1751-0473-9-1",
        "doc_eid": "2-s2.0-84892661802",
        "doc_date": "2014-01-08",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: Cellular events play a central role in the understanding of biological processes and functions, providing insight on both physiological and pathogenesis mechanisms. Automatic extraction of mentions of such events from the literature represents an important contribution to the progress of the biomedical domain, allowing faster updating of existing knowledge. The identification of trigger words indicating an event is a very important step in the event extraction pipeline, since the following task(s) rely on its output. This step presents various complex and unsolved challenges, namely the selection of informative features, the representation of the textual context, and the selection of a specific event type for a trigger word given this context. Results: We propose TrigNER, a machine learning-based solution for biomedical event trigger recognition, which takes advantage of Conditional Random Fields (CRFs) with a high-end feature set, including linguistic-based, orthographic, morphological, local context and dependency parsing features. Additionally, a completely configurable algorithm is used to automatically optimize the feature set and training parameters for each event type. Thus, it automatically selects the features that have a positive contribution and automatically optimizes the CRF model order, n-grams sizes, vertex information and maximum hops for dependency parsing features. The final output consists of various CRF models, each one optimized to the linguistic characteristics of each event type. Conclusions: TrigNER was tested in the BioNLP 2009 shared task corpus, achieving a total F-measure of 62.7 and outperforming existing solutions on various event trigger types, namely gene expression, transcription, protein catabolism, phosphorylation and binding. The proposed solution allows researchers to easily apply complex and optimized techniques in the recognition of biomedical event triggers, making its application a simple routine task. We believe this work is an important contribution to the biomedical text mining community, contributing to improved and faster event recognition on scientific articles, and consequent hypothesis generation and knowledge discovery. This solution is freely available as open source at http://bioinformatics.ua.pt/trigner. © 2014 Campos et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BioinformaticsUA: Concept Recognition in Clinical Narratives Using a Modular and Highly Efficient Text Processing Framework",
        "doc_scopus_id": "85122043120",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85122043120",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            }
        ],
        "doc_keywords": [
            "'current",
            "Amount of information",
            "Concept recognition",
            "Discharge summary",
            "Discharge tests",
            "Modular architectures",
            "Modulars",
            "Normalisation",
            "Test reports",
            "Text-processing"
        ],
        "doc_abstract": "© 8th International Workshop on Semantic Evaluation, SemEval 2014 - co-located with the 25th International Conference on Computational Linguistics, COLING 2014, Proceedings. All rights reserved.Clinical texts, such as discharge summaries or test reports, contain a valuable amount of information that, if efficiently and effectively mined, could be used to infer new knowledge, possibly leading to better diagnosis and therapeutics. With this in mind, the SemEval-2014 Analysis of Clinical Text task aimed at assessing and improving current methods for identification and normalization of concepts occurring in clinical narrative. This paper describes our approach in this task, which was based on a fully modular architecture for text mining. We followed a pure dictionary-based approach, after performing error analysis to refine our dictionaries. We obtained an F-measure of 69.4% in the entity recognition task, achieving the second best precision over all submitted runs (81.3%), with above average recall (60.5%). In the normalization task, we achieved a strict accuracy of 53.1% and a relaxed accuracy of 87.0%.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Current methodologies for biomedical named entity recognition",
        "doc_scopus_id": "85101461012",
        "doc_doi": "10.1002/9781118617151.ch37",
        "doc_eid": "2-s2.0-85101461012",
        "doc_date": "2014-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014 John Wiley & Sons, Inc.The primary goal of text mining is to retrieve knowledge that is hidden in text and to present it in a concise and simple form to the final users. To achieve this objective, two main directions of research can be defined: Information Extraction (IE) and Information Retrieval (IR). Named Entity Recognition (NER) is one of the most important tasks, since the IE steps will be performed using the names provided by it. The chapter presents the steps necessary to implement solutions using dictionaries and machine learning and hybrid approaches, respectively. Some practical examples for each approach are provided. It also describes the existing approaches to develop NER and normalization solutions, presenting and explaining the core techniques accompanied with examples of some existent systems. The chapter presents one practical example for each approach in order to demonstrate how the several steps could be implemented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Survey on Biclustering of gene expression data",
        "doc_scopus_id": "85101433313",
        "doc_doi": "10.1002/9781118617151.ch25",
        "doc_eid": "2-s2.0-85101433313",
        "doc_date": "2014-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014 John Wiley & Sons, Inc.Microarrays allow measuring the expression level of a large number of genes under different experimental samples or environmental conditions. The data generated from them are called gene expression data. Gene expression data are usually represented by a matrix M, where the ith row represents the ith gene, the jth column represents the jth condition, and the cell mij represents the expression level of the th gene under the jth condition. In this chapter, the authors make a survey on biclustering of gene expression data. First, the chapter presents the different types of biclusters and groups of biclusters. Then, it discusses the evaluation functions and systematic and stochastic biclustering algorithms. Finally, the chapter focuses on bicluster validation that can qualitatively evaluate the capacity of an algorithm to extract meaningful biclusters from a biological point of view.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Egas: A collaborative and interactive document curation platform",
        "doc_scopus_id": "84987673679",
        "doc_doi": "10.1093/database/bau048",
        "doc_eid": "2-s2.0-84987673679",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© The Author(s) 2014.With the overwhelming amount of biomedical textual information being produced, several manual curation efforts have been set up to extract and store concepts and their relationships into structured resources. As manual annotation is a demanding and expensive task, computerized solutions were developed to perform such tasks automatically. However, high-end information extraction techniques are still not widely used by biomedical research communities, mainly because of the lack of standards and limitations in usability. Interactive annotation tools intend to fill this gap, taking advantage of automatic techniques and existing knowledge bases to assist expert curators in their daily tasks. This article presents Egas, a web-based platform for biomedical text mining and assisted curation with highly usable interfaces for manual and automatic in-line annotation of concepts and relations. A comprehensive set of de facto standard knowledge bases are integrated and indexed to provide straightforward concept normalization features. Real-time collaboration and conversation functionalities allow discussing details of the annotation task as well as providing instant feedback of curator's interactions. Egas also provides interfaces for on-demand management of the annotation task settings and guidelines, and supports standard formats and literature services to import and export documents. By taking advantage of Egas, we participated in the BioCreative IV interactive annotation task, targeting the assisted identification of protein-protein interactions described in PubMed abstracts related to neuropathological disorders. When evaluated by expert curators, it obtained positive scores in terms of usability, reliability and performance. These results, together with the provided innovative features, place Egas as a state-of-the-art solution for fast and accurate curation of information, facilitating the task of creating and updating knowledge bases and annotated resources.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-provider Architecture for Cloud Outsourcing of Medical Imaging Repositories",
        "doc_scopus_id": "84929511107",
        "doc_doi": "10.3233/978-1-61499-432-9-146",
        "doc_eid": "2-s2.0-84929511107",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Data availability",
            "DICOM",
            "Improving performance",
            "Maintenance cost",
            "Medical community",
            "Medical institutions",
            "Quality healthcare",
            "Resource management",
            "Computer Security",
            "Information Storage and Retrieval",
            "Internet",
            "Medical Record Linkage",
            "Models, Organizational",
            "Outsourced Services",
            "Radiology Information Systems",
            "Systems Integration"
        ],
        "doc_abstract": "© 2014 European Federation for Medical Informatics and IOS Press.Over the last few years, the extended usage of medical imaging procedures has raised the medical community attention towards the optimization of their workflows. More recently, the federation of multiple institutions into a seamless distribution network has brought hope of increased quality healthcare services along with more efficient resource management. As a result, medical institutions are constantly looking for the best infrastructure to deploy their imaging archives. In this scenario, public cloud infrastructures arise as major candidates, as they offer elastic storage space, optimal data availability without great requirements of maintenance costs or IT personnel, in a pay-as-you-go model. However, standard methodologies still do not take full advantage of outsourced archives, namely because their integration with other in-house solutions is troublesome. This document proposes a multi-provider architecture for integration of outsourced archives with in-house PACS resources, taking advantage of foreign providers to store medical imaging studies, without disregarding security. It enables the retrieval of images from multiple archives simultaneously, improving performance, data availability and avoiding the vendor-locking problem. Moreover it enables load balancing and cache techniques.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Screening Radiation Exposure for Quality Assurance",
        "doc_scopus_id": "84929507627",
        "doc_doi": "10.3233/978-1-61499-432-9-622",
        "doc_eid": "2-s2.0-84929507627",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "As low as reasonably achievable",
            "DICOM",
            "Digital radiography",
            "Distributed systems",
            "IHE REM",
            "Medical Exposure Directive",
            "Radiation Exposure",
            "Radiological protection",
            "Environmental Exposure",
            "Europe",
            "Information Storage and Retrieval",
            "Quality Assurance, Health Care",
            "Radiation Dosage",
            "Radiography",
            "Radiometry"
        ],
        "doc_abstract": "© 2014 European Federation for Medical Informatics and IOS Press.Quality assurance for planned radiation exposure situations (e.g. Digital Radiography, Computed Tomography or Radio Fluoroscopic studies) requires the application of examination-specific scans protocols. These are tailored to patient age or size, body region and clinical indication for ensuring that the dose applied to each patient is as low as reasonably achievable for the clinical purpose of the image acquisition (ALARA principle). The European legal framework-2013/59/EURATOM-points that health authorities will be more pervasive on inspecting the dosimetry applied to patients. This paper discusses these legal alterations and presents an interoperable distributed system for dose monitoring, which is compliant with legal procedures and the IHE Radiation Exposure Monitoring profile (REM). The system combines the most representative stakeholders affected and directly interested in the patient radiological protection: patients, radiologists, practitioners, health authorities, and ethics committee. The system is capable of gathering, in real-time, dose information applied to the patient and storing it in a regional or national wide dose registry. The paper addresses which information should such systems hold and which should be accessed, from each stakeholder perspective. Furthermore, the system may detect irregular dose patterns, which could indicate dose abuses, and signal those findings to the appropriate stakeholders.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A cytoscape-based web tool for representing protein networks",
        "doc_scopus_id": "84927658289",
        "doc_doi": "10.1007/978-3-319-03005-0_79",
        "doc_eid": "2-s2.0-84927658289",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Bioengineering",
                "area_abbreviation": "CENG",
                "area_code": "1502"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Biomedical research",
            "Cytoscape",
            "Innovative method",
            "Molecular process",
            "On-line tools",
            "OralCard",
            "Protein network",
            "Technological advances"
        ],
        "doc_abstract": "© 2014, Springer International Publishing Switzerland.The full understanding of the biological and molecular processes that take place in the human oral cavity can only be attained through the identification of the components that participate within each process. Even though technological advances in computer science are promoting innovative methods and tools of great relevance in biomedical research, there is still a lack of efficient web tools to represent the complexity of these systems. In this paper we present an online tool that builds and represents protein networks through the use of Cytoscape Web, providing researchers with a rich set of functionalities that ease the task of extracting biological evidences about these networks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Weighted gene co-expression network analysis applied to head and neck squamous cell carcinoma data",
        "doc_scopus_id": "84927586031",
        "doc_doi": "10.1007/978-3-319-03005-0_76",
        "doc_eid": "2-s2.0-84927586031",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Bioengineering",
                "area_abbreviation": "CENG",
                "area_code": "1502"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Co-expression networks",
            "Dissimilarity measures",
            "Experimental factors",
            "Head-and-neck cancer",
            "Head-and-neck squamous cell carcinoma",
            "Microarray technologies",
            "Molecular mechanism",
            "Simultaneous monitoring"
        ],
        "doc_abstract": "© 2014, Springer International Publishing Switzerland.Microarray technology has made possible the simultaneous monitoring of the expression levels of thousands of genes under multiple disease states. Due to the high complexity of the obtained data the use of computational methods for extracting biological evidences is still a major issue. In this work we address this problem by adjusting the use of gene coexpression networks to analyze a Head and Neck Squamous Cell Carcinoma (HNSCC) dataset. The proposed method applies hierarchic clustering to identify gene modules using the topological overlap dissimilarity measure after, defining a gene co-expression similarity, defining a family of adjacency functions and calculating their parameters. This method calculates the eigengenes of each module to define a network of modules and the correlation between the eigengenes and the risk factors, identifying modules of genes where those are more expressed and associating these concepts to gene ontology functional terms. The preliminary results described in this paper contribute to reveal the molecular mechanisms associated with HNSCC and the contribution of experimental factors types like differentiation, alcohol use, sex, age, tumor site, smoking pack years and race.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A nanopublishing architecture for biomedical data",
        "doc_scopus_id": "84921765548",
        "doc_doi": "10.1007/978-3-319-07581-5_33",
        "doc_eid": "2-s2.0-84921765548",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automated approach",
            "COEUS",
            "Information overloads",
            "Integration frameworks",
            "Massive production",
            "Nanopublications",
            "Scientific results",
            "Semantic web standards"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.The massive production of data in the biomedical domain soon triggered a phenomenon known as information overload. The majority of these data are redundant without linked statements or associations, which hinders research methods. In this work, we describe an innovative and automated approach to integrate scientific results into small RDF-based data snippets called nanopublications. A nanopublication enhances attribution and ownership of specific data elements, representing the smallest unit of publishable information. It is particularly relevant for the scientific domain, where controlled publication, validation and ownership of data are essential. This proposal extends an existing semantic data integration framework by enabling the generation of nanopublications. Furthermore, we explore a streamlined integration and retrieval pipeline, empowered by current Semantic Web standards.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mutation analysis in PARK2 gene uncovers patterns of associated genetic variants",
        "doc_scopus_id": "84921741831",
        "doc_doi": "10.1007/978-3-319-07581-5_18",
        "doc_eid": "2-s2.0-84921741831",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Co-occurrence",
            "Comparative analysis",
            "Elderly populations",
            "Genetic variants",
            "Genetic variation",
            "Health concerns",
            "Mutation analysis",
            "Next-generation sequencing"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.We present a comparative analysis of PARK2 genetic variants based on genotype data from HapMap. We focused our study on the association between missense mutations and all other variations within the same gene to uncover patterns of hidden genetic variation. Alzheimer’s disease (AD) and Parkinson’s disease (PD) are the main neurodegenerative diseases and represent a growing health concern worldwide, with the increase in the elderly population. Mutations in several genes have been associated with either AD or PD, and the number of novel genetic variants characterized is expanding rapidly with the introduction of next generation sequencing technologies. Most of these variants, however, are of unknown consequences as their effect might be mediated through association with additional mutations. Our results show that significant correlation between genetic variants exists and their co-occurrence might contribute to previously unidentified risk increase.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Genome annotation using nanopublications: An approach to interoperability of genetic data",
        "doc_scopus_id": "84920001167",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84920001167",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Annotated datasets",
            "Data interoperability",
            "Functional annotation",
            "Mammalian genomes",
            "Next-generation sequencing",
            "Semantic Web technology",
            "Time-consuming tasks",
            "Transcription start site"
        ],
        "doc_abstract": "With the widespread use of Next Generation Sequencing (NGS) technologies, the primary bottleneck of genetic research has shifted from data production to data analysis. However, annotated datasets produced by different research groups are often in different formats, making genomic comparisons and integration with other datasets challenging and time consuming tasks. Here, we propose a new data interoperability approach that provides unambiguous (machine readable) description of genomic annotations based on a novel method of data publishing called nanopublication. A nanopublication is a schema built on top of existing semantic web technologies that consists of three components: An individual assertion (i.e., the genomic annotation); provenance (containing links to the experimental information and data processing steps); and publication info (information about data ownership and rights, allowing each genomic annotation to be citable and its scientific impact tracked ) [1]. We use nanopublications to demonstrate automatic interoperability between individual genomic annotations from the functional annotation of the mammalian genome 5 (FANTOM5) consortium (transcription start sites) and the Leiden Open Variation Database (genomic variants). The nanopublications can also be integrated with the data of the other semantic web frameworks like COEUS. Exposing legacy information and new NGS data as nanopublications promises tremendous scaling advantages when integrating very large and heterogeneous genomic datasets.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Normalizing medical imaging archives for dose quality assurance and productivity auditing",
        "doc_scopus_id": "84906920699",
        "doc_doi": "10.1109/MeMeA.2014.6860112",
        "doc_eid": "2-s2.0-84906920699",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "As low as reasonably achievable",
            "Continuous monitoring",
            "DICOM",
            "Distinct points",
            "Medical institutions",
            "PACS",
            "Quality indicators",
            "Radiation Exposure"
        ],
        "doc_abstract": "Quality assurance for planned radiation exposure situations (e.g. x-ray, CT) requires the application of examination-specific scans tailored to patient age or size, body region and clinical indication for ensuring that the dose to each patient is as low as reasonably achievable for the clinical purpose of the image acquisition. Nevertheless, assuring quality implies a heavy manual labor to measure and optimize care services flow. There are already several methodologies and protocols that can be used to achieve this objective. However, the continuous monitoring of quality indicators is still not performed in many healthcare centers. The challenge is to find a way to analyze these metrics in an efficient, effective and convenient manner. Moreover, these results are not often shared, due to confidentiality and privacy of patients and medical staff. In this paper, we propose a methodology and a software tool to aggregate and normalize the monitoring data from distinct points, in order to collect indicators, namely about productivity, efficiency and dose usage, factors that are crucial for benchmarking and for improving the quality of protocol procedures. To evaluate the effectiveness of our solution, several results were collected from two medical institutions. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Medical imaging archiving: A comparison between several NoSQL solutions",
        "doc_scopus_id": "84906883440",
        "doc_doi": "10.1109/BHI.2014.6864305",
        "doc_eid": "2-s2.0-84906883440",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Digital medical imaging systems",
            "Healthcare institutions",
            "High availability",
            "Medical decision making",
            "MongoDB",
            "Nosql database",
            "Relational Database"
        ],
        "doc_abstract": "The use of digital medical imaging systems has greatly increased in healthcare institutions and they are currently valuable tools supporting medical decision and treatment procedures. The proliferation of digital modalities led to an explosion on medical images production, increasing the need to have larger repositories to afford all this amount of data with high availability and performance. NoSQL databases have been replacing relational databases in some scenarios, due to their horizontal scalability and to their flexibility to adapt to dynamic requirements. In this paper, we present an implementation of a medical imaging archive supported in both MongoDB and CouchDB. This implementation is compliant with the medical imaging standards and the storage and query/retrieve performance of our different implementations were evaluated. We also discuss the strengths and weakness of the proposed implementations and present several scenarios that take advantage of the proposed solutions. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extracting sentences describing biomolecular events from the biomedical literature",
        "doc_scopus_id": "84906048690",
        "doc_doi": "10.1007/978-3-319-07593-8_48",
        "doc_eid": "2-s2.0-84906048690",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bio-molecular",
            "Biomedical literature",
            "Scientific articles",
            "Scientific literature",
            "Sentence-based",
            "Sources of informations",
            "Specific information",
            "Supervised machine learning"
        ],
        "doc_abstract": "The scientific literature is one of the main sources of information for researchers. However, due to the rapid increase of the number of scientific articles, satisfying a specific information need has become a very demanding task, and researchers often have to scan through a large number of publications in search of a specific nugget of information. In this work we propose the use of supervised machine learning techniques to retrieve and rank sentences describing different types of biomolecular events. The objective is to classify and rank sentences that match any general query according to the likelihood of mentioning events involving one or more biomolecular entities. These ranked results should provide a condensed, or summarized, view of the knowledge present in the literature and related to the user's information need. © Springer International Publishing Switzerland 2014.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "XDS-I outsourcing proxy: Ensuring confidentiality while preserving interoperability",
        "doc_scopus_id": "84904289764",
        "doc_doi": "10.1109/JBHI.2013.2292776",
        "doc_eid": "2-s2.0-84904289764",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Current limitation",
            "Document sharing",
            "Health professionals",
            "Integrating the healthcare enterprise",
            "Search patterns",
            "Searchable encryptions",
            "Security constraint",
            "Computer Security",
            "Confidentiality",
            "Diagnostic Imaging",
            "Electronic Health Records",
            "Humans",
            "Internet",
            "Medical Informatics Computing"
        ],
        "doc_abstract": "The interoperability of services and the sharing of health data have been a continuous goal for health professionals, patients, institutions, and policy makers. However, several issues have been hindering this goal, such as incompatible implementations of standards (e.g., HL7, DICOM), multiple ontologies, and security constraints. Cross-enterprise document sharing (XDS) workflows were proposed by Integrating the Healthcare Enterprise (IHE) to address current limitations in exchanging clinical data among organizations. To ensure data protection, XDS actors must be placed in trustworthy domains, which are normally inside such institutions. However, due to rapidly growing IT requirements, the outsourcing of resources in the Cloud is becoming very appealing. This paper presents a software proxy that enables the outsourcing of XDS architectural parts while preserving the interoperability, confidentiality, and searchability of clinical information. A key component in our architecture is a new searchable encryption (SE) scheme - Posterior Playfair Searchable Encryption (PPSE) - which, besides keeping the same confidentiality levels of the stored data, hides the search patterns to the adversary, bringing improvements when compared to the remaining practical state-of-the-art SE schemes. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The landscape of protein biomarkers proposed for periodontal disease: Markers with functional meaning",
        "doc_scopus_id": "84904108852",
        "doc_doi": "10.1155/2014/569632",
        "doc_eid": "2-s2.0-84904108852",
        "doc_date": "2014-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Immunology and Microbiology (all)",
                "area_abbreviation": "IMMU",
                "area_code": "2400"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [
            "Biofilms",
            "Biological Markers",
            "Bone Resorption",
            "Chronic Disease",
            "Disease Progression",
            "Fibroblasts",
            "Gingiva",
            "Gingivitis",
            "Humans",
            "Inflammation",
            "Macrophages",
            "Monocytes",
            "Neutrophils",
            "Periodontal Diseases"
        ],
        "doc_abstract": "Periodontal disease (PD) is characterized by a deregulated inflammatory response which fails to resolve, activating bone resorption. The identification of the proteomes associated with PD has fuelled biomarker proposals; nevertheless, many questions remain. Biomarker selection should favour molecules representing an event which occurs throughout the disease progress. The analysis of proteome results and the information available for each protein, including its functional role, was accomplished using the OralOme database. The integrated analysis of this information ascertains if the suggested proteins reflect the cell and/or molecular mechanisms underlying the different forms of periodontal disease. The evaluation of the proteins present/absent or with very different concentrations in the proteome of each disease state was used for the identification of the mechanisms shared by different PD variants or specific to such state. The information presented is relevant for the adequate design of biomarker panels for PD. Furthermore, it will open new perspectives and help envisage future studies targeted to unveil the functional role of specific proteins and help clarify the deregulation process in the PD inflammatory response. © 2014 Nuno Rosa et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sensor-based architecture for medical imaging workflow analysis systems-level quality improvement",
        "doc_scopus_id": "84904038647",
        "doc_doi": "10.1007/s10916-014-0063-8",
        "doc_eid": "2-s2.0-84904038647",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Data Mining",
            "Hospital Information Systems",
            "Humans",
            "Radiology Information Systems",
            "Systems Integration",
            "User-Computer Interface",
            "Workflow"
        ],
        "doc_abstract": "The growing use of computer systems in medical institutions has been generating a tremendous quantity of data. While these data have a critical role in assisting physicians in the clinical practice, the information that can be extracted goes far beyond this utilization. This article proposes a platform capable of assembling multiple data sources within a medical imaging laboratory, through a network of intelligent sensors. The proposed integration framework follows a SOA hybrid architecture based on an information sensor network, capable of collecting information from several sources in medical imaging laboratories. Currently, the system supports three types of sensors: DICOM repository meta-data, network workflows and examination reports. Each sensor is responsible for converting unstructured information from data sources into a common format that will then be semantically indexed in the framework engine. The platform was deployed in the Cardiology department of a central hospital, allowing identification of processes' characteristics and users' behaviours that were unknown before the utilization of this solution. © 2014 Springer Science+Business Media New York.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A centralized platform for geo-distributed PACS management",
        "doc_scopus_id": "84897027876",
        "doc_doi": "10.1007/s10278-013-9650-3",
        "doc_eid": "2-s2.0-84897027876",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Geo-Distributed PACS",
            "Healthcare institutions",
            "Operation and management",
            "PACS administrator",
            "Patient care",
            "Picture archive and communication systems",
            "Remote monitoring"
        ],
        "doc_abstract": "Picture Archive and Communication System (PACS) is a globally adopted concept and plays a fundamental role in patient care flow within healthcare institutions. However, the deployment of medical imaging repositories over multiple sites still brings several practical challenges namely related to operation and management (O&M). This paper describes a Web-based centralized console that provides remote monitoring, testing, and management over multiple geo-distributed PACS. The system allows the PACS administrator to define any kind of service or operation, reducing the need for local technicians and providing a 24/7 monitoring solution. © 2013 Society for Imaging Informatics in Medicine.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gathering and exploring scientific knowledge in pharmacovigilance",
        "doc_scopus_id": "84892404109",
        "doc_doi": "10.1371/journal.pone.0083016",
        "doc_eid": "2-s2.0-84892404109",
        "doc_date": "2013-12-11",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Drug Interactions",
            "Female",
            "Humans",
            "Internet",
            "Male",
            "Pharmacovigilance",
            "Software"
        ],
        "doc_abstract": "Pharmacovigilance plays a key role in the healthcare domain through the assessment, monitoring and discovery of interactions amongst drugs and their effects in the human organism. However, technological advances in this field have been slowing down over the last decade due to miscellaneous legal, ethical and methodological constraints. Pharmaceutical companies started to realize that collaborative and integrative approaches boost current drug research and development processes. Hence, new strategies are required to connect researchers, datasets, biomedical knowledge and analysis algorithms, allowing them to fully exploit the true value behind state-of-the-art pharmacovigilance efforts. This manuscript introduces a new platform directed towards pharmacovigilance knowledge providers. This system, based on a service-oriented architecture, adopts a plugin-based approach to solve fundamental pharmacovigilance software challenges. With the wealth of collected clinical and pharmaceutical data, it is now possible to connect knowledge providers' analysis and exploration algorithms with real data. As a result, new strategies allow a faster identification of high-risk interactions between marketed drugs and adverse events, and enable the automated uncovering of scientific evidence behind them. With this architecture, the pharmacovigilance field has a new platform to coordinate large-scale drug evaluation efforts in a unique ecosystem, publicly available at http://bioinformatics.ua.pt/euadr/. © 2013 Lopes et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating echocardiogram reports with medical imaging",
        "doc_scopus_id": "84889070590",
        "doc_doi": "10.1109/CBMS.2013.6627819",
        "doc_eid": "2-s2.0-84889070590",
        "doc_date": "2013-12-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Clinical services",
            "Computational system",
            "Healthcare institutions",
            "Heterogeneous database",
            "Index information",
            "New approaches",
            "Ultrasound medical images"
        ],
        "doc_abstract": "Healthcare institutions are increasingly taking advantage of information and computational systems to enhance the efficiency and quality of their services. These IT systems are normally able to handle huge amounts of digital data, and to extract relevant fingerprints useful to improve the quality of clinical practice. However, building automatized processes to achieve this over multiple and heterogeneous databases is still a challenge. This paper presents a new approach able to collect and index information from distinct medical data sources, allowing us to identify important metrics to evaluate the performance and quality of clinical services. A case study combining information from ultrasound medical images and echocardiogram clinical reports is also presented. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A multi-domain platform for medical imaging",
        "doc_scopus_id": "84888996180",
        "doc_doi": "10.1109/CBMS.2013.6627770",
        "doc_eid": "2-s2.0-84888996180",
        "doc_date": "2013-12-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Cloud services",
            "Data repositories",
            "Health informations",
            "Imaging protocol",
            "Indexing engines",
            "Medical imaging equipment",
            "Peer-to-peer paradigm",
            "Security constraint"
        ],
        "doc_abstract": "The increasing adoption of medical imaging equipment in healthcare has been leading to a huge dispersion of data repositories and institutions. Although the quality of diagnostic and treatment is deeply dependent on the health information that is available for physicians, several legal and technological issues have hindered the integration of these data. One of such problems is because traditional medical imaging protocols do not perform well in inter-institutional scenarios. This paper describes a hybrid network platform for medical imaging systems that provides searching and retrieval over multiple centres. Three key components support the system: an indexing engine, a multicast framework and a cloud service. Using a peer-to-peer paradigm with security constraints, the platform gathers the information of medical imaging repositories hosted inside the institutions, allowing physicians to access data when and where they need it. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring nanopublications integration in pharmacovigilance scenarios",
        "doc_scopus_id": "84894221191",
        "doc_doi": "10.1109/HealthCom.2013.6720773",
        "doc_eid": "2-s2.0-84894221191",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Nowadays, the post-market assessment and monitoring of interactions amongst prescribed drugs and adverse events in the pharmacovigilance domain reveals previously unknown interactions. Involved stakeholders are realising that modern approaches, based on integrative data exploration environments, are vital to improve current drug research and development processes. In this manuscript we introduce a comprehensive pipeline for integrating large collections of annotated data, abstracting it to a Semantic Web environment, modelling it according to the nanopublications standard, and delivering straightforward access to collected knowledge. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A DICOM viewer based on web technology",
        "doc_scopus_id": "84894154830",
        "doc_doi": "10.1109/HealthCom.2013.6720660",
        "doc_eid": "2-s2.0-84894154830",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "During the last decade, medical imaging services have assumed a central role in healthcare institutions, and they are nowadays a decisive factor for the quality of diagnostic and treatments. Health stakeholders and policy makers have been steadily adopting PACS and DICOM standard, simplifying interoperability between distinct equipment and institutions. To assist images' interpretation, several visualization solutions emerged. However, these applications are targeted to specific operating systems, hindering its ubiquitous use, in increasing web-based working environments. In this paper we present a Web-based DICOM viewer that was entirely developed with web technology, namely HTML5 and JavaScript. The result is a visualization station that is already in use in two medical imaging centres and that can be accessed through a common web browser, from any computer, mobile device, or operation system. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Leveraging XDS-I and PIX workflows for validating cross-enterprise patient identity linkage",
        "doc_scopus_id": "84894123103",
        "doc_doi": "10.1109/HealthCom.2013.6720686",
        "doc_eid": "2-s2.0-84894123103",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Document exchange communities set the ground for cross-organization cooperation. They enable the exchange of patient's documents across distinct health organizations. However, there are various challenges that must be overcame before deploying such communities, for instance the construction of the Enterprise Master Patient Index (EMPI) which maps the several patient identifiers of each domain. This paper describes the development of an interoperable distributed system that expedites the exchange of documents by taking care of the patient identities autonomously. The system automatically builds the EMPI leveraging the healthcare workflow (based on PIX and XDS-I) for validating the automatic linkages of the patient identifiers. The human validation is a consequence of user's interaction with cross-domain documents: distributing and attenuating the validation effort. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An agile framework to support distributed medical imaging scenarios",
        "doc_scopus_id": "84893447291",
        "doc_doi": "10.1109/ICHI.2013.48",
        "doc_eid": "2-s2.0-84893447291",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Collaborative workspace",
            "Communications platform",
            "Computational power",
            "Continuous production",
            "DICOM",
            "Healthcare institutions",
            "Imaging applications",
            "PACS"
        ],
        "doc_abstract": "Healthcare institutions are increasingly taking advantage of telemedicine to create collaborative workspaces. However, the continuous production of medical data requires higher performance from infrastructures, to maintain or even enhance the quality of service. On the other hand, a tremendous amount of ubiquitous computational power and an unprecedented number of Internet resources and services are used every day as a normal commodity. This paper presents a Cloud-based telemedicine framework that allows applications to store data and communicate easily, using any Cloud provider. From this framework, two medical imaging applications were developed to provide standard services with high abstraction level: a medical imaging repository and an inter-institutional communications platform. Furthermore, a telemedicine case study is presented, namely a regional repository, where the physicians can review medical studies from anywhere and at any time. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An innovative portal for rare genetic diseases research: The semantic Diseasecard",
        "doc_scopus_id": "84888205095",
        "doc_doi": "10.1016/j.jbi.2013.08.006",
        "doc_eid": "2-s2.0-84888205095",
        "doc_date": "2013-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Hardware and software",
            "Heterogeneous resources",
            "Holistic perspectives",
            "Rare disease",
            "Scientific breakthrough",
            "Semantic Web technology",
            "Services and applications",
            "Whole genome sequencing"
        ],
        "doc_abstract": "Advances in \"omics\" hardware and software technologies are bringing rare diseases research back from the sidelines. Whereas in the past these disorders were seldom considered relevant, in the era of whole genome sequencing the direct connections between rare phenotypes and a reduced set of genes are of vital relevance. This increased interest in rare genetic diseases research is pushing forward investment and effort towards the creation of software in the field, and leveraging the wealth of available life sciences data. Alas, most of these tools target one or more rare diseases, are focused solely on a single type of user, or are limited to the most relevant scientific breakthroughs for a specific niche. Furthermore, despite some high quality efforts, the ever-growing number of resources, databases, services and applications is still a burden to this area. Hence, there is a clear interest in new strategies to deliver a holistic perspective over the entire rare genetic diseases research domain. This is Diseasecard's reasoning, to build a true lightweight knowledge base covering rare genetic diseases. Developed with the latest semantic web technologies, this portal delivers unified access to a comprehensive network for researchers, clinicians, patients and bioinformatics developers. With in-context access covering over 20 distinct heterogeneous resources, Diseasecard's workspace provides access to the most relevant scientific knowledge regarding a given disorder, whether through direct common identifiers or through full-text search over all connected resources. In addition to its user-oriented features, Diseasecard's semantic knowledge base is also available for direct querying, enabling everyone to include rare genetic diseases knowledge in new or existing information systems. Diseasecard is publicly available at http://bioinformatics.ua.pt/diseasecard/. © 2013 Elsevier Inc.",
        "available": true,
        "clean_text": "serial JL 272371 291210 291682 291870 291901 31 Journal of Biomedical Informatics JOURNALBIOMEDICALINFORMATICS 2013-08-21 2013-08-21 2014-10-01T00:19:33 S1532-0464(13)00121-4 S1532046413001214 10.1016/j.jbi.2013.08.006 S300 S300.3 FULL-TEXT 2015-05-15T06:30:58.629321-04:00 0 0 20131201 20131231 2013 2013-08-21T00:00:00Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb vol volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref specialabst 1532-0464 15320464 true 46 46 6 6 Volume 46, Issue 6 17 1108 1115 1108 1115 201312 December 2013 2013-12-01 2013-12-31 2013 Special Section: Social Media Environments Alejandro Rodríguez González Miguel Angel Mayer Jesualdo Tomás Fernández-Breis Regular Research Papers article fla Copyright © 2013 Elsevier Inc. All rights reserved. INNOVATIVEPORTALFORRAREGENETICDISEASESRESEARCHSEMANTICDISEASECARD LOPES P 1 Introduction 2 Methods 2.1 Architecture 2.2 From OMIM’s maps to 2 million triples 2.3 Semantic integration network 2.4 Implementation 3 Results 3.1 Semantic knowledge base 3.2 In-context research 3.2.1 Search 3.2.2 Web workspace 3.3 Semantic interoperability 4 Discussion 4.1 Exploring Huntington’s disease knowledge 4.2 Integrating Diseasecard’s data 4.3 Future perspectives 5 Conclusions Acknowledgments References NABARETTE 2006 41 H ARONSON 2006 243 245 J WASTFELT 2006 1 10 M SEOANEVAZQUEZ 2008 33 E SCHIEPPATI 2008 2039 2041 A COOPER 2010 631 655 D ZHANG 2010 e13857 L ZHANG 2010 e14288 L COLOMA 2011 1 11 P GOWTHAMAN 2007 153 R AARTSMARUS 2006 135 144 A HAMOSH 2005 D514 D517 A AMBERGER 2011 564 567 J OLIVEIRA 2005 161 167 I GARDNER 2005 1001 1007 S PASQUIER 2008 584 594 C LOPES 2012 1 19 P BAIROCH 2005 D154 D159 A MULDER 2007 D224 D228 N SIGRIST 2010 D161 D166 C ROSE 2013 D475 D482 P LIPSCOMB 2000 265 266 C KAHRAMAN 2005 418 420 A MOTTAZ 2008 S3 A MILICICBRANDT 2011 M ARTIFICIALINTELLIGENCEINMEDICINE MAPPINGORPHANETTERMINOLOGYUMLS BELLEAU 2008 706 716 F CHAUDHURI 1997 65 74 S WEIBEL 1997 9 11 S PAN 2003 J SEMANTICWEBISWC2003 RDFSFARDFMTTWOSEMANTICSFORRDFS RUTTENBERG 2007 S2 A BIZER 2009 87 92 C VONSATTEL 1998 57 J DRIVERDUNCKLEY 2007 879 885 E NEUROLOGYCLINICALNEUROSCIENCE HUNTINGTONSDISEASE LOPES 2011 32 P FOKKEMA 2011 557 563 I ORTH 2011 1409 1412 M STAUSBERG 2010 23 29 J STAUSBERG 2011 134 J CAMPOS 2013 54 D ARRAIS 2011 123 130 J ROSA 2012 N NUNES 2013 T OLIVEIRA 2012 J BELLGARD 2012 M LOPESX2013X1108 LOPESX2013X1108X1115 LOPESX2013X1108XP LOPESX2013X1108X1115XP Full 2014-12-01T00:02:24Z OA-Window ElsevierBranded item S1532-0464(13)00121-4 S1532046413001214 10.1016/j.jbi.2013.08.006 272371 2014-10-01T02:41:15.290749-04:00 2013-12-01 2013-12-31 true 1254105 MAIN 8 58450 849 656 IMAGE-WEB-PDF 1 gr3 652859 1647 3504 gr2 235624 652 3445 gr1 298984 1349 3150 fx1 true 310999 864 2213 gr3 82259 372 791 gr2 38227 147 778 gr1 48263 304 711 fx1 true 40292 195 500 gr3 7021 103 219 gr2 3227 41 219 gr1 5985 94 219 fx1 true 7341 86 219 YJBIN 2048 S1532-0464(13)00121-4 10.1016/j.jbi.2013.08.006 Elsevier Inc. Fig. 1 Diseasecard architecture overview. (A) External resources are identified and configured for integration in Diseasecard’s rare genetic diseases semantic network. (B) Diseasecard architecture, highlighting COEUS, with the data integration connectors, the knowledge base, and the interoperability API; the Tomcat server, for web application delivery; the Solr indexing engine, for improved search performance; and the Redis cache engine, for faster disease network access. (C) Any external system can query Diseasecard’s knowledge base and use its data without limitations. Fig. 2 Diseasecard’s integration network overview. (A) The integration process starts with OMIM’s morbid map and constructs the OMIM and HGNC individuals, which will be used to expand other resources. (B) On a second level, the Orphanet’s OrphaData is used to load Orphanet mappings; UniProt is used for disease-gene-protein associations; ClinicalTrials, GWASCentral, WAVe, LSDBs and Ensembl mappings are obtained from their respective APIs using HGNC and OMIM identifiers. (C) The third and final level loads: (1) PharmGKB entries from GeNS for each UniProt entry; (2) MeSH terms are loaded from the results of previous research; (3) GeneCards, InterPro, Prosite, PDB, KEGG, Enzyme, Gene Ontology and STRING entries from direct UniProt queries; and (4) ICD10 mappings to Orphanet entries from OrphaData. Fig. 3 Diseasecard’s workspace for Huntington’s disease. (A) Initial display for Diseasecard’s workspace for Huntington’s disease. Both the left navigation tree and the central hypertree provide direct access to the collected disease network. These tree-based navigation strategies reflect the Entity-Concept-Item structure behind Diseasecard’s configuration and result in a familiar interaction metaphor for the application users. (B) Loading external resources in Diseasecard using LiveView opens the associated location in the workspace, empowering in-context research. LiveView highlights Diseasecard’s connectedness features, as external resources are not hidden or replicated, they are linked. Furthermore, this strategy overcomes traditional drawbacks for original resource creators, maintaining content accreditation and ownership. Table 1 List of resources integrated in Diseasecard’s knowledge base, comprising the entity (for the tree-based navigation interface), the resource name, the origin of the integration mapping and the original resource URL. Entity Resource Origin Resource URL Disease OMIM Morbid map Orphanet OrphaData Drug PharmGKB GeNS Literature Pubmed Morbid map Locus Ensembl Ensembl Entrez Entrez GeneCards UniProt HGNC Gene map Ontology Gene Ontology UniProt ICD10 OrphaData MeSH UniProt2MeSH Pathways KEGG UniProt Enzyme UniProt Protein InterPro UniProt PDB UniProt PROSITE UniProt STRING UniProt UniProt UniProt Study Clinical Trials Clinical Trials GWASCentral GWASCentral Variome LSDB GEN2PHEN WAVe HGNC An innovative portal for rare genetic diseases research: The semantic Diseasecard Pedro Lopes ⁎ José Luís Oliveira DETI/IEETA, Universidade de Aveiro, Portugal DETI/IEETA Universidade de Aveiro Portugal ⁎ Corresponding author. Address: IEETA, Campus Universitario de Santiago, 3810-193 Aveiro, Portugal. Graphical abstract Advances in “omics” hardware and software technologies are bringing rare diseases research back from the sidelines. Whereas in the past these disorders were seldom considered relevant, in the era of whole genome sequencing the direct connections between rare phenotypes and a reduced set of genes are of vital relevance. This increased interest in rare genetic diseases research is pushing forward investment and effort towards the creation of software in the field, and leveraging the wealth of available life sciences data. Alas, most of these tools target one or more rare diseases, are focused solely on a single type of user, or are limited to the most relevant scientific breakthroughs for a specific niche. Furthermore, despite some high quality efforts, the ever-growing number of resources, databases, services and applications is still a burden to this area. Hence, there is a clear interest in new strategies to deliver a holistic perspective over the entire rare genetic diseases research domain. This is Diseasecard’s reasoning, to build a true lightweight knowledge base covering rare genetic diseases. Developed with the latest semantic web technologies, this portal delivers unified access to a comprehensive network for researchers, clinicians, patients and bioinformatics developers. With in-context access covering over 20 distinct heterogeneous resources, Diseasecard’s workspace provides access to the most relevant scientific knowledge regarding a given disorder, whether through direct common identifiers or through full-text search over all connected resources. In addition to its user-oriented features, Diseasecard’s semantic knowledge base is also available for direct querying, enabling everyone to include rare genetic diseases knowledge in new or existing information systems. Diseasecard is publicly available at Keywords Rare diseases Biomedical semantics Data integration Interoperability Semantic web 1 Introduction Rare genetic diseases research is at the forefront of the most modern personalized medicine endeavors. The rare term broadly defines disorders that affect at most 1 in 2000 patients [1] and the European Organization for Rare Diseases (EURORDIS) estimates that there are approximately 6000–8000 rare diseases, affecting about 6–8% of the population [2]. Within these, about 80% are caused by genetic changes, further strengthening the relations between genotypes and phenotypes associated with these particular conditions [3,4]. Some of these chronic diseases hinder the patients’ quality of life and cause serious damage or social disability [5]. Moreover, the low patient number severely obstructs the creation of adequate research cohorts, making it very difficult to coordinate studies capable of generating results in a scientifically-relevant scale [6,7]. In addition to long-term patient care improvements, understanding gene-disease associations is a fundamental goal for bioinformatics research, especially at the rare disease level, where genotype-phenotype connections are typically limited to one or a few more genes. This moves rare diseases research from a relatively minor concern to a major player in a new era of “omics” research [8,9]. Genomics, metabolomics, proteomics or pharmacogenomics, among others, benefit from the focused approach enabled by rare genetic diseases research. A direct consequence of this growing importance is the renewed interest from pharmaceutical companies in this area, which are supporting multiple worldwide initiatives towards improved rare diseases research. On a broader scope, the International Rare Diseases Consortium (IRDiRC) is leveraging several projects on the field. RD-Connect RareConnect EuRenOmics and NeurOmics are some of the highlights from IRDiRC sponsorships. On the European level, GEN2PHEN EU-ADR [10] or EMIF projects are actively investing in setting up state of the art research activities for rare diseases stakeholders. A key feature of these projects is their multidisciplinary approach, leveraging on the natural connections between clinicians, researchers, bioinformaticians and patients, who, for once, are active elements of the proposed strategies. With these miscellaneous initiatives, players and requirements, there is an overwhelming challenge to tackle the wealth of data being made available by next-generation sequencing hardware, omics databases, patient registries, and pharmacovigilance or electronic health records. Multiple new rare diseases research tools are emerging, focusing only a set of particular conditions. Niche databases for the domains of neurological disorders [11] or muscular problems [12], for example, tackle small specific sub-groups. While they provide high quality information and resources, their disease coverage is small and inadequate. Likewise, platforms such as the Online Mendelian Inheritance in Man database (OMIM) [13,14], the National Organization for Rare Disorders (NORD) website or Orphanet, among others, collect and filter available information, with a particular set of users in mind. Where OMIM is more focused on aggregating content for bioinformatics researchers, Orphanet has decade-long pedigree and content far beyond simple research data. It is geared towards clinicians, medical researchers and patients, boasting a large collection of curated clinical information such as patient registries, biobanks and specialized clinician contacts, among others, in multiple languages. Despite these high quality efforts, entropy is a recurring problem in this field with the ever-growing number of resources, databases, services and applications. Therefore, a new approach is desired, one where everyone is able to quickly access the available knowledge regarding a given set of rare disorders. Diseasecard addresses these needs by delivering a lightweight holistic perspective over the rare genetic diseases research field. Stemming from a legacy portal [15,16], a crawler-based system that kick-started a new strategy for in-context research, the new semantic Diseasecard version focuses on three fundamental elements: • The rapid and lightweight access to a comprehensive semantic network of scientifically relevant resources for a given disease, covering multiple domains from proteomics to clinical studies up to medical ontologies. • The innovative in-context browsing allowing for the eased navigation amongst the multitude of connected resources without leaving the initial research focus. • The open interoperability layer, making the semantic knowledge base available for everyone to query and access, and enabling the integration of rich rare diseases data in new or existing information systems. In addition, Diseasecard comprises a rich semantic layer, providing future-proof technologies for inference and reasoning over the created knowledge base. Diseasecard is publicly available online at 2 Methods Semantic data integration is, in itself, a complex data engineering issue [17], and the life sciences field further increases this complexity [18]. To support Diseasecard’s ambitious integration and interoperability features we rely on the COEUS framework [19]. Exploring COEUS flexible integration engine enabled us to simplify the overall platform architecture through the creation of a comprehensive dependency-based resource integration network. Diseasecard’s integration pipeline, including COEUS’ use and the internal resource organization, are described in detail next. 2.1 Architecture To overcome the challenges behind the amount of scattered data for rare diseases, Diseasecard’s underlying objective is to collect, connect and deliver access to a network of the most relevant rare diseases scientific resources. To attain this, Diseasecard’s knowledge base is constructed from an integration network starting with OMIM’s morbid map and spanning through multiple resources, including proteomics data from UniProt [20], InterPro [21], Prosite [22] and Protein Data Bank (PDB) [23] up to ontology data from Medical Subject Headings (MeSH) [24] and International Classification of Diseases (ICD version 10), among many others. These data are obtained from multiple mapping studies [25–27] and genomic name servers, such as GeNS [28] and Bio2RDF [29]. This broad scope results in an extremely rich dataset, where OMIM’s rare disorder list is expanded to more than 2 million triples. To improve its semantic data integration and interoperability features, Diseasecard is built with the COEUS semantic web application framework [30], which heavily influences Diseasecard’s architectural design. COEUS delivers a “Semantic Web in a box” approach, enabling the rapid development of new knowledge management systems adopting semantic web technologies [31,32]. By default, the COEUS framework already includes the necessary components to build and launch a new semantic information system from scratch. The platform comprises the tools to acquire and translate knowledge from miscellaneous data sources, and to deliver access to the constructed knowledge base through various interoperable formats. One of COEUS’ key caveats is the lack of advanced update methods. Despite this, the trade-off between its semantic integration and interoperability capabilities, and the lack of update features drawback is a positive one, especially considering COEUS’ build engine performance. To complete Diseasecard’s architecture we created a dedicated client-side application, to support the agile web workspace; added an indexing engine to improve the efficiency behind the full-text search infrastructure; and added an object-oriented database, to cache data for each rare genetic disease network, to improve workspace access performance. The entire architecture is described in detail in Fig. 1 . 2.2 From OMIM’s maps to 2 million triples Diseasecard adopts a targeted warehousing data integration strategy [33,34]. Accordingly, the data import and translation process gathers all data from external resources in a single centralized knowledge base, in opposition to real-time data gathering strategies [35]. Curating a niche warehouse focused on rare genetic diseases knowledge enables Diseasecard’s future endeavors on advanced inference and reasoning algorithms. Since we were looking at managing semantic information from the start, Diseasecard’s integration process not only collects data per se, but it translates external data into a semantic knowledge base. Creating this new semantic layer leverages a major challenge on how to translate large heterogeneous datasets into a new semantic environment. This problem can be divided in two areas, focusing on the technological challenge and on the logical data modelling challenge. From the technological perspective, Diseasecard relies on the COEUS framework to perform the data abstractions, triplifying data from the miscellaneous external resources into a unified Diseasecard knowledge environment. On the integration modelling side, Diseasecard has a custom ontology to integrate OMIM’s data This simple ontology is used to enhance the translation from OMIM’s morbid map into a semantic environment. Additionally, following Semantic Web’s “reuse instead of rewrite” motto, Diseasecard’s data model reuses existing schemas internally. Using COEUS’ instance configuration and taking advantage of existing ontologies and models for internal use is enough to organize collected data. Diseasecard’s lightweight integration approach means that, for each individual, such as a UniProt protein or an OMIM entry, we only need to store its identifier. Hence, we can reuse the identifier term from the Dublin Core ontology [36]. As such, each individual has a dc:identifier data property, matching a string with the external identifier. Another example is the rdfs:label property, obtained from the Resource Description Format (RDF) schema ontology that is used to label each individual [37]. External LinkedData references are also included, establishing direct connections to external individuals. For instance, UniProt published interfaces are linked through the rdfs:seeAlso property. Despite this over-simplification, new relationships amongst integrated data are autonomously generated. Whereas in a CSV file we have a set of columns with text, with the move to a semantic environment all data are interconnected, generating a richer dataset. The same is true for SQL databases where foreign key relationships and table/column names are mapped to new properties, resulting in more metadata and more relationships. Starting with OMIM’s morbid map, which has around 6300 entries related to a gene map with about 14,200 entries, Diseasecard’s engine expands the integration network, generating new knowledge, and collecting pointers for the resources mentioned in Table 1 , as detailed in the following section. 2.3 Semantic integration network Diseasecard’s knowledge base covers miscellaneous resources within the rare genetic diseases research domain. The knowledge base is obtained from a dependency graph, the semantic integration network, where the acquisition of data from external sources is defined. Starting with OMIM’s morbid map, Diseasecard loads information about rare disorders and HUGO Gene Nomenclature Committee (HGNC) gene symbols. The integration engine then proceeds to expand the list of integrated individuals into new sources. For instance, using the OMIM accession number, Diseasecard obtains the associated UniProt and Orphanet identifiers. Likewise, from the integrated HGNC symbols, Diseasecard obtains identifiers for GWAS Central, Clinical Trials and Ensembl databases. Fig. 2 displays a visual overview over Diseasecard’s complete semantic integration network, highlighting the multiple connections amongst resources and how each is extended to generate more data inputs into the knowledge base. A sample example for semantic integration is the translation of UniProt and PDB identifiers into Diseasecard’s knowledge base. With UniProt becoming a major source for data mappings, it is used in Diseasecard to establish connections among diseases, proteins and external entities. This integration branch starts with the aggregation of UniProt entries associated with each particular OMIM code (using COEUS’ CSV connector to translate UniProt search results). Next, Diseasecard uses COEUS’ XML connector to load XPath query results and generate new triples. For instance, to create new PDB individuals and their respective connections, the “//entry/dbReference[@type=“PDB”]” XPath query is performed on each UniProt entry XML. When Diseasecard retrieves the semantic network for each disease, the association graph between OMIM, UniProt and PDB entries is traversed. 2.4 Implementation Using COEUS imposes some restrictions on the technologies used in the Diseasecard platform. As the framework provides a solid Java-based backend solution, we opted to deploy the Diseasecard client-side web environment also within an Apache Tomcat server Diseasecard’s indexing engine is built on top of Solr This Lucene-based search engine enables indexing the resources connected in Diseasecard’s knowledge base and searching them with a notable performance. Initial tests unravelled a slow response time for the disease network generation tasks. The complexity behind the SPARQL queries retrieving all identifiers associated with a given rare genetic disease reduces the web application usability. Hence, Diseasecard uses an object-oriented database, Redis to store a cached version of the knowledge network for each disease entry. With this, the performance increased ten-fold from an average page loading time of 4.5s to an almost instant 300ms for the rare diseases workspaces. For Diseasecard’s web interface, a combination of JavaScript algorithms with modern CSS and HMTL5 technologies was used. The main JavaScript library used is jQuery with several plugins for cookie management and tree displays. The JavaScript InfoVis Toolkit is used to display the central workspace hypertree. The outcome of these technological implementation choices is a responsive and agile web application, further improving the final user experience. 3 Results Diseasecard’s knowledge base contains around 2 million triples, built from OMIM’s maps and the expanded rare genetic diseases network. These triples establish about 500 thousand connections to more than 100 thousand unique resources, which are entirely indexed by Diseasecard’s engine. From these numbers we can infer that, on average, each unique resource is present in 5 single disease networks. Additionally, each rare disorder has, on average, around 24 connections to external resources. As expected, disease resources from OMIM represent the biggest slice of individuals with around 18 thousand entries for more than 11 thousand HGNC entries. Another interesting result stems from Ontology mappings, as MeSH and ICD terms are the least represented concepts in Diseasecard’s knowledge base. The entirety of these data are stored in Diseasecard’s semantic knowledge base, made available for end-users through an innovative in-context web workspace and to developers through an advanced semantic interoperability layer, which are detailed next. 3.1 Semantic knowledge base One of the main premises behind the creation of a new Diseasecard version lied in the need to better explore the powerful technologies pushed forward by the Semantic Web paradigm. With these, Diseasecard is able to construct a rich and comprehensive semantic knowledge base for rare genetic diseases. Its knowledge infrastructure extends the capabilities of the majority research platforms by making the collected knowledge interoperable and future-proof. With this knowledge network the door is open for inferring new relationships amongst connected resources and for reasoning over gathered data in search for previously uncovered connections. In the future, these methods can be used to enrich Diseasecard’s knowledge base with new annotations, and to federate knowledge discovery through multiple databases with the publicly available SPARQL endpoints. It is equally important to note that the lightweight integrative approach adopted in Diseasecard re-uses existing ontologies to describe data. Consequently, data from the knowledge base is easier to integrate by third parties and to connect using LinkedData technologies. 3.2 In-context research Diseasecard is a unique alternative for exploring biomedical rare diseases information in a centralised web-based workspace. Along with direct access to diseases’ workspaces through any of the integrated resources identifiers, full-text searching enables querying all the web pages for all the resources integrated in Diseasecard’s knowledge base. 3.2.1 Search For a more comprehensive access, Diseasecard has a powerful search feature comprising three components: browsing, identifier search and full-text search. With Diseasecard’s browsing feature, users can browse all entries by their starting letter – This displays the OMIM accession number, the disease name and the number of available connections in the generated network. The identifier search, selected by default, searches through the extended identifier network. This network includes accession numbers for resources listed in Table 1. In addition to search boxes, using the query string to perform searches is also available. For instance, will retrieve all entries where the HTT string identifier is present. At last, full-text search is the most powerful search mechanism. This method, selectable in the home page search button, searches web pages for the connected resources. This results in detailed access to a restricted set of locations, where users filter more specific queries such as author names, publication titles, protein sequences or more complex disease descriptions. 3.2.2 Web workspace As mentioned, Diseasecard’s semantic integration network starts with OMIM’s morbid map. Consequently, disorders can be directly accessed using their unique OMIM identifier. For instance, OMIM’s “Huntington Disease” entry (OMIM #143100) can be explored in Diseasecard at The displayed web workspace has two key features: the navigation tree and map – Fig. 3 (A), and the LiveView browsing – Fig. 3(B). The navigation tree and map are two complementary alternatives for exploring each network. The left sidebar displays the disease navigation tree to quickly access all links with a familiar metaphor. The central area displays a circular navigation map, pointing to all individual identifiers. Both the navigation tree and map trigger the Live View feature. This opens the external resource application within Diseasecard, allowing users to browse the multiple collected connections without leaving the initial context. 3.3 Semantic interoperability With Diseasecard’s knowledge base built, several interoperability services are enabled by default. Hence, Diseasecard’s access API includes two main data access alternatives: a SPARQL endpoint [38] and a LinkedData interface [39]. These two options allow flexible output formats, thus facilitating the data integration from Diseasecard’s platform in external applications. SPARQL is the most advanced query language available and enables distributed reasoning and inference, as well as combining Diseasecard’s data with other federated SPARQL endpoints. The LinkedData interfaces provide quick access to all data for a given resource and the use of these resources, through their URIs, in any external context. The URI for accessing Huntington’s disease data in Diseasecard’s knowledge base is In the discussion section we highlight how these methods can be used to enrich an existing system with Diseasecard’s rare genetic diseases knowledge. 4 Discussion The concept behind Diseasecard is a unique approach towards comprehensive access to rare genetic diseases research knowledge. To demonstrate Diseasecard’s innovative features, we detail in the following sections a couple real-world scenarios regarding Huntington disease. Starting with an end-user oriented scenario, we highlight the variety of relevant questions that can be answered with Diseasecard for clinicians, researchers and patients. In comparison with the aforementioned systems, such as Orphanet, NORD or OMIM, none offers such a broad scope of directly accessible knowledge. On top of end-user features, Diseasecard also provides an interoperability layer. Henceforth, we emphasize how its knowledge can be integrated in external systems through simple methods. 4.1 Exploring Huntington’s disease knowledge Huntington disease is an “autosomal dominant neurodegenerative disorder with midlife onset characterized by psychiatric, cognitive, and motor symptoms” [40]. Once a patient is diagnosed with Huntington’s disease, he has an average 12–15years until death [41]. Along with its genetic profile, Huntington’s disease affects about 5–10 people in 100 thousand [42]. To fully demonstrate Diseasecard’s capabilities we setup three use cases, targeting users with distinct needs, in the context of Huntington’s disease. This study starts when a patient is seeking for diagnostic and starts being monitored by his general practitioner, who is traditionally not familiarized with Huntington’s disease, but suspects this is the patient condition. Consequently, the clinician wants to learn about this disorder, searching for answers for the following questions: 1. What are Huntington’s disease main features? 2. Are there any ICD terms for this disease? 3. What laboratories perform genetic tests for this disease? 4. What are the most relevant Huntington’s disease publications? Once the clinician has a better understanding concerning Huntington’s disease, he proposes his patient for genetic analysis in a nearby institute. However, researchers working with this patient may also be unaware of the deep genotype characteristics of this disorder. To further understand the genomic and proteomic scope of this disease, the researcher starts by exploring answers to multiple questions. Some of these are: 5. What are the underlying genes associated with Huntington’s disease? a. What are the gene names? b. What are known gene mutations? 6. What are the proteins coded by these genes? a. What is the 3D structure of these proteins? 7. In what pathways are the genes for this disease involved? At last we need to consider the patient perspective, of someone who was recently diagnosed with an unknown disease, and that is looking into understanding what is happening in his organism. The patient searches for answers for the following questions to learn more about Huntington’s disease: 8. Where can I get a description of this disease? 9. Are there any clinical trials open for this disease? a. What are the results of previous clinical trials? 10. Are there any patient registries or biobanks for Huntington’s disease? Finding the answers for these questions is not a straightforward process. Without a tool like Diseasecard, clinicians, researchers and patients will loose precious hours browsing Google, Wikipedia, OMIM, Orphanet or UniProt. This is a rather inefficient and ineffective endeavors. Diseasecard’s streamlines this workflow. Starting by typing “Huntington” in Diseasecard’s homepage search box, the entry for Huntington’s disease appears almost immediately on the top results From there, the disorder workspace provides quick access to multiple web resources where users can find the answers to their questions: 1. Huntington’s disease clinical features aptly start with the mention of classic signs of progressive chorea, rigidity and dementia. Clicking the disease name on Huntington’s disease entry loads the disorder OMIM page in LiveView, where this information is highlighted. 2. On Diseasecard’s navigation map and tree (Ontology node), the ICD10 node shows one link to the ICD version 10 “G10” term, entitled “Huntington disease”. 3. Orphanet is the best resource to find genetic testing laboratories and is also linked in Diseasecard (Disease node). Orphanet lists around 180 diagnostic testing laboratories covering almost the entire Europe, from Portugal to Finland. 4. Pubmed is the key resource for relevant scientific publications. Diseasecard links directly to Pubmed’s search engine (Literature node), where this publication list can be retrieved. 5. HGNC genes are loaded from OMIM’s morbid map, thus playing a key role in Diseasecard’s integration network. From the disease workspace, we can access HTT HGNC page (Locus node), Huntington’s disease approved gene. a. From the previous page, we learn that HTT gene is denominated “huntingtin”. b. WAVe is a gene-centric web portal collecting links for multiple locus specific databases [43]. WAVe access is provided in Diseasecard (Variation node) and it lists two locus-specific databases for the HTT gene, where we can find the variants in an LOVD [44] instance curated by Weilleke van Roon-Mom. 6. Huntington’s disease proteins can be inferred from underlying genes of this disease. In Diseasecard’s navigation interface, we learn that UniProt entry P42858 (HD_HUMAN) is associated with Huntington’s disease (Protein node). a. Like UniProt, PDB is available in Diseasecard and this resource includes multiple 2D and 3D models portraying this protein. 7. Using KEGG (Pathway node), Diseasecard delivers easy access to the Huntington’s disease pathway. 8. Like the clinical features, a disorder description is available in Huntington’s disease OMIM entry (Disease node). 9. Clinical Trials detail studies and analysis over a variety of cohorts. The Study node lists two open Clinical Trials (as of early 2013), NCT01597128 at the University of Kentucky, USA; and NCT01065220 at the Medical University of Vienna, Austria. a. The Clinical Trial NCT00491842 has already finished and the collected data is also available in Diseasecard. 10. The previously mentioned Orphanet database also lists available patient registries and biobanks for Huntington’s disease, easing the tasks of accessing and contacting these sites spread throughout Europe. Diseasecard’s focused environment provides a broad amount of connections where the answers to critical clinical and research questions can be answered. In comparison to using multiple applications, Diseasecard’s always in-context browsing environment dynamically improves the end users data exploration workflow. 4.2 Integrating Diseasecard’s data The European Huntington disease network is a large-scale patient registry, with multiple centres spread throughout Europe focused on creating a wide patients community [45]. Taking in account the amount and variety of knowledge that is offered to patients, clinicians and researchers of this database, and, additionally, the scope of knowledge that could be available, the inclusion of external data provided by Diseasecard is a welcome addition. Despite being a closed system, we can assert from the various documentation available that this patient registry could be improved with the introduction of further clinical-oriented information. This information can be in the form of related ICD terms or links to Orphanet database entries, both containing relevant information for clinical practice. For instance, ICD classifications are already widely used in multiple hospital information systems [46–48]. The following SPARQL query can be sent to Diseasecard’s endpoint, at retrieving a unified list of ICD and MeSH terms, for the OMIM entries regarding Huntington’s disease (#143100) and the “huntingtin” gene (*613004). PREFIX coeus: PREFIX diseasecard: PREFIX dc: SELECT DISTINCT ?icd { ?item dc:title ?icd. ?item coeus:hasConcept diseasecard:concept_ICD10. ?item coeus:isAssociatedTo ?orpha. ?orpha coeus:isAssociatedTo ?omim. ?omim coeus:hasConcept diseasecard:concept_OMIM. { ?omim diseasecard:omim “143100” } UNION { ?omim diseasecard:omim “613004” } } ORDER BY ASC(?icd) The results list the ICD10 identifier “G10” as the term matching Huntington’s disease. The query can be tested at While SPARQL may have a steep learning curve, its advanced features make it the most complete query language for accessing any semantic knowledge base. Furthermore, results can be obtained in CSV, XML or JSON, making the use of these data in any programming language very straightforward. While these mappings are also provided through several other services, Diseasecard’s interoperability API offers a bigger variety of identifiers than most common systems. Querying UniProt or PDB identifiers is a similar process to querying the detailed ICD and Orphanet entries. 4.3 Future perspectives Diseasecard can be seen as an initial step towards a comprehensive integrative “omics” suite. This will make it a key player in future large-scale research projects, acting as a channel for delivering a rich set of connections to rare genetic diseases knowledge. A vital enhancement for future Diseasecard developments regards the inclusion of deeper semantic relationships amongst aggregated data. Whereas all individuals are connected through similar predicates in the current version, future iterations will comprise new rich connections between particular individuals, obtained from data mining workflows [49] and new scientific discoveries [50,51]. New metadata will improve Diseasecard’s genomics perspective, with annotations for relationships regarding diseases, genes and proteins interactions [52]; and the clinical perspective, with annotations mined from electronic medical records [53] and specialized patient registries [54]. 5 Conclusions We presented Diseasecard, a portal for rare diseases researchers, clinicians and patients. Diseasecard features a rich semantic knowledge base to deliver a lightweight holistic perspective over the wealth of genetic diseases information stemming from the growing number of “omics” research projects. Diseasecard’s results are significant in at least three major respects. (1) The use of semantic web technologies to collect connections to the most relevant resources regarding rare diseases is a pivotal step in the future of data integration and interoperability. (2) The available in-context research features – full text search and LiveView augmented browsing – enable full access to external resources within each disease workspace. (3) At last, making all data available for further inclusion in other systems, whether through LinkedData or the SPARQL endpoint, empowers other developers to enrich their systems with a myriad of connections to the most relevant rare diseases resources. The new Diseasecard represents a milestone towards semantic interoperable rare diseases knowledge, and is publicly available online at Acknowledgments The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP7/2007–2013) under Grant agreement No. 200754 – the GEN2PHEN project, and under Grant agreement No. 305444 – the RD-Connect project. References [1] H. Nabarette D. Oziel B. Urbero N. Maxime S. Aymé Use of a directory of specialized services and guidance in the healthcare system: the example of the Orphanet database for rare diseases Rev d’épidémiol santé publique 54 2006 41 [2] EURORDIS. What is a rare disease? 2012. [3] J.K. Aronson Rare diseases and orphan drugs Br J Clin Pharmacol 61 2006 243 245 [4] M. Wastfelt B. Fadeel J.I. Henter A journey of hope: lessons learned from studies on rare diseases and orphan drugs J Intern. Med. 260 2006 1 10 [5] E. Seoane-Vazquez R. Rodriguez-Monguio S.L. Szeinbach J. Visaria Incentives for orphan drug research and development in the United States Orphanet J Rare Dis 3 2008 33 [6] A. Schieppati J.I. Henter E. Daina A. Aperia Why rare diseases are an important medical and social issue Lancet 371 2008 2039 2041 [7] D.N. Cooper J.M. Chen E.V. Ball K. Howells M. Mort A.D. Phillips Genes, mutations, and human inherited disease at the dawn of the age of personalized genomics Hum Mutat 31 2010 631 655 [8] L. Zhang Y.-F. Pei J. Li C.J. Papasian H.-W. Deng Improved detection of rare genetic variants for diseases PLoS ONE 5 2010 e13857 [9] L. Zhang Y.-F. Pei J. Li C.J. Papasian H.-W. Deng Efficient utilization of rare variants for detection of disease-related genomic regions PLoS ONE 5 2010 e14288 [10] P.M. Coloma M.J. Schuemie G. Trifirò R. Gini R. Herings J. Hippisley-Cox Combining electronic healthcare databases in Europe to allow for large-scale drug safety monitoring: the EU-ADR Project Pharmacoepidemiol. Drug Saf. 20 2011 1 11 [11] R. Gowthaman N. Gowthaman M.K. Rajangam K. Srinivasan Database of neurodegenerative disorders Bioinformation 2 2007 153 [12] A. Aartsma-Rus J.C. Van Deutekom I.F. Fokkema G.J.B. Van Ommen J.T. Den Dunnen Entries in the Leiden Duchenne muscular dystrophy mutation database: an overview of mutation types and paradoxical cases that confirm the reading-frame rule Muscle Nerve 34 2006 135 144 [13] A. Hamosh A.F. Scott J.S. Amberger C.A. Bocchini V.A. McKusick Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders Nucleic Acids Res 33 2005 D514 D517 [14] J. Amberger C. Bocchini A. Hamosh A new face and new challenges for Online Mendelian Inheritance in Man (OMIM®) Hum Mutat 32 2011 564 567 [15] Oliveira JL, Dias GMS, Oliveira IFC, Rocha PDNSd, Hermosilla I, Vicente J, et al. DiseaseCard: a web-based tool for the collaborative integration of genetic and medical information. In: 5th International symposium, ISBMDA 2004: biological and medical data analysis; 2004. p. 409–17. [16] I. Oliveira J. Oliveira J. Sanchez V. Lopez-Alonso F. Martin-Sanchez V. Maojo Grid requirements for the integration of biomedical information resources for health applications Methods Inf Med 44 2005 161 167 [17] S.P. Gardner Ontologies and semantic data integration Drug Discovery Today 10 2005 1001 1007 [18] C. Pasquier Biological data integration using Semantic Web technologies Biochimie 90 2008 584 594 [19] P. Lopes J.L. Oliveira COEUS: “semantic web in a box” for biomedical applications J Biomed Semantics 3 2012 1 19 [20] A. Bairoch R. Apweiler C.H. Wu W.C. Barker B. Boeckmann S. Ferro The universal protein resource (UniProt) Nucleic Acids Res 33 2005 D154 D159 [21] N.J. Mulder R. Apweiler T.K. Attwood A. Bairoch A. Bateman D. Binns New developments in the InterPro database Nucleic Acids Res 35 2007 D224 D228 [22] C.J.A. Sigrist L. Cerutti E. De Castro P.S. Langendijk-Genevaux V. Bulliard A. Bairoch PROSITE, a protein domain database for functional characterization and annotation Nucleic Acids Res 38 2010 D161 D166 [23] P.W. Rose C. Bi W.F. Bluhm C.H. Christie D. Dimitropoulos S. Dutta The RCSB Protein Data Bank: new resources for research and education Nucleic Acids Res 41 2013 D475 D482 [24] C.E. Lipscomb Medical subject headings (MeSH) Bull Med Libr Assoc 88 2000 265 266 [25] A. Kahraman A. Avramov L.G. Nashev D. Popov R. Ternes H.-D. Pohlenz PhenomicDB: a multi-species genotype/phenotype database for comparative phenomics Bioinformatics 21 2005 418 420 [26] A. Mottaz Y. Yip P. Ruch A. Veuthey Mapping proteins to disease terminologies: from UniProt to MeSH BMC Bioinformatics 9 suppl. 5 2008 S3 [27] M. MiličićBrandt A. Rath A. Devereau S. Aymé Mapping orphanet terminology to UMLS M. Peleg N. Lavrač C. Combi Artificial intelligence in medicine 2011 Springer Berlin Heidelberg p. 194–203. [28] Arrais J, Pereira J, Oliveira JL. GeNS: A biological data integration platform. In: Brojack B, editor. ICBB 2009, international conference on bioinformatics and biomedicine. Venice: WASET, World Academy of Science, Engineering and Technology; 2009. [29] F. Belleau M.-A. Nolin N. Tourigny P. Rigault J. Morissette Bio2RDF: towards a mashup to build bioinformatics knowledge systems J Biomed Inform 41 2008 706 716 [30] Lopes P, Oliveira JL. COEUS: a semantic web application framework. In: Proceedings of the 4th international workshop on semantic web applications and tools for the life sciences: ACM; 2011. p. 66–73. [31] Lopes P, Oliveira JL. A semantic web application framework for health systems interoperability. In: Proceedings of the first international workshop on managing interoperability and complexity in health systems: ACM; 2011. p. 87–90. [32] Lopes P, Oliveira JL. Towards knowledge federation in biomedical applications. In: Proceedings of the 7th international conference on semantic systems: ACM; 2011. p. 87–94. [33] Zhu Y, An L, Liu S. Data updating and query in real-time data warehouse system. In: Computer science and software engineering, 2008 international conference on 2008. p. 1295–7. [34] Reddy SSS, Reddy LSS, Khanaa V, Lavanya A. Advanced techniques for scientific data warehouses. In: International conference on advanced computer control, ICACC2009. p. 576–80. [35] S. Chaudhuri U. Dayal An overview of data warehousing and OLAP technology ACM Sigmod Record 26 1997 65 74 [36] S. Weibel The dublin core: a simple content description model for electronic resources Bull Am Soc Inform Sci Technol 24 1997 9 11 [37] J. Pan I. Horrocks RDFS(FA) and RDF MT: Two Semantics for RDFS D. Fensel K. Sycara J. Mylopoulos The semantic web – ISWC 2003 2003 Springer Berlin (Heidelberg) p. 30–46. [38] A. Ruttenberg T. Clark W. Bug M. Samwald O. Bodenreider H. Chen Advancing translational research with the Semantic Web BMC Bioinformatics 8 2007 S2 [39] C. Bizer The emerging web of linked data Intell Syst IEEE 24 2009 87 92 [40] J.P.G. Vonsattel M. DiFiglia Huntington disease J Neuropathol Exp Neurol 1998 57 [41] Walker FO. Huntington’s disease. The Lancet, vol. 369. p. 218–28. [42] E. Driver-Dunckley J. Caviness Huntington’s disease A.H.V. Schapira Neurology and clinical neuroscience 2007 Mosby Elsevier 879 885 [43] P. Lopes R. Dalgleish J.L. Oliveira WAVe: web analysis of the variome Hum Mutat 2011 32 [44] I.F.A.C. Fokkema P.E.M. Taschner G.C.P. Schaafsma J. Celli J.F.J. Laros J.T. den Dunnen LOVD v. 2.0: the next generation in gene variant databases Hum Mutat 32 2011 557 563 [45] M. Orth Network TEHsD. Observing huntington’s disease: the European Huntington’s disease network’s REGISTRY J Neurol Neurosurg Psychiatry 82 2011 1409 1412 [46] Hougland P, Nebeker J, Pickard S, Van Tuinen M, Masheter C, Elder S, et al. Using ICD-9-CM codes in hospital claims data to detect adverse events in patient safety surveillance. Advances in patient safety: new directions and alternative approaches (vol 1: Assessment); 2008. [47] J. Stausberg J. Hasford Identification of adverse drug events: the use of ICD-10 coded diagnoses in routine hospital data Dtsch Arztebl Int 107 2010 23 29 [48] J. Stausberg J. Hasford Drug-related admissions and hospital-acquired adverse drug events in Germany: a longitudinal analysis from 2003 to 2007 of ICD-10-coded routine data BMC Health Services Res 11 2011 134 [49] D. Campos S. Matos J. Oliveira Gimli: open source and high-performance biomedical name recognition BMC Bioinformatics 14 2013 54 [50] J. Arrais J. Oliveira Using biomedical networks to prioritize gene-disease associations Open Access Bioinf 3 2011 123 130 [51] N. Rosa M.J. Correia J.P. Arrais P. Lopes J. Melo J.L. Oliveira From the salivary proteome to the OralOme: comprehensive molecular oral biology Arch Oral Biol 2012 [52] T. Nunes D. Campos S. Matos J.L. Oliveira BeCAS: biomedical concept recognition services and visualization Bioinformatics 2013 [53] J.L. Oliveira P. Lopes T. Nunes D. Campos S. Boyer E. Ahlberg The EU-ADR Web Platform: delivering advanced pharmacovigilance tools Pharmacoepidemiol Drug Saf 2012 n/a–n/a. [54] M.I. Bellgard A. Macgregor F. Janon A. Harvey P. O’Leary A. Hunter A modular approach to disease registry design: Successful adoption of an internet-based rare disease registry Hum Mutat 2012 "
    },
    {
        "doc_title": "Ontology-based health information search: Application to the neurological disease domain",
        "doc_scopus_id": "84887971150",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887971150",
        "doc_date": "2013-11-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Health informations",
            "Named-entity recognition",
            "Neurological disease",
            "OWL",
            "RDF",
            "Semantic annotations"
        ],
        "doc_abstract": "The amount of information on health subjects on the Web and users' interest in having efficient access to it continues to increase. This information can also be very useful for professionals, researchers and students in this field. But the usual word-based search does not allow full exploration of the information, pointing to the need for more semantic approaches. Aiming to contribute to providing a semantic search for information on Health, available using written Portuguese, the article presents enhanced search platform architecture based on the enterprise search platform (FAST), providing a processing pipeline capable of exploring annotation of entities and relations defined in a domain. A modular architecture was defined in which an ontology-based knowledge base has an essential role. As it is essential to make the user task as simple as possible, the platform also includes ontology navigation in the classes and relations of the entity to guide queries; use of the most frequent domain classes in the documents relating to a search to provide additional ways of navigation; and inclusion in the score of the ontology-guided annotations. As a first proof of concept, the architecture was instantiated in a specific domain, neurological diseases, for which an ontology was created. © 2013 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhanced regional network for medical imaging repositories",
        "doc_scopus_id": "84887947734",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887947734",
        "doc_date": "2013-11-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Healthcare institutions",
            "High level control",
            "Large amounts of data",
            "PACS",
            "Performance issues",
            "Picture archive and communication systems",
            "Web-based technologies"
        ],
        "doc_abstract": "Nowadays PACS (Picture Archive and Communication System) tends to integrate web-based technologies in processes associated to storage, distribution and visualization of medical images, creating the general abstraction of distributed PACS. Although PACS integration with web-based technologies offer significant advantages to healthcare institutions, it is crucial that information technology based issues do not impose constraints in the medical practice workflow. Therefore, distributed PACS have to deal with performance issues in both storing and retrieving of large amounts of data across distinct systems, possibly hosted on different locations. This paper proposes an approach to reduce data transference footprint in a distributed PACS environment with DICOM-Ready applications. The proposed method provides a high level control of transference parameters, such as, number of connections, number of fragments and size of each fragment. The result was a highly tunable data stream that can be used in any distributed PACS environment. © 2013 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving data and knowledge management to better integrate health care and research",
        "doc_scopus_id": "84883742021",
        "doc_doi": "10.1111/joim.12105",
        "doc_eid": "2-s2.0-84883742021",
        "doc_date": "2013-10-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Internal Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2724"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A modular framework for biomedical concept recognition",
        "doc_scopus_id": "84884469671",
        "doc_doi": "10.1186/1471-2105-14-281",
        "doc_eid": "2-s2.0-84884469671",
        "doc_date": "2013-09-24",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Appropriate techniques",
            "Biomedical information extractions",
            "Command-line interfaces",
            "Named entity recognition",
            "NAtural language processing",
            "Normalization methods",
            "Open source frameworks",
            "Part of speech tagging"
        ],
        "doc_abstract": "Background: Concept recognition is an essential task in biomedical information extraction, presenting several complex and unsolved challenges. The development of such solutions is typically performed in an ad-hoc manner or using general information extraction frameworks, which are not optimized for the biomedical domain and normally require the integration of complex external libraries and/or the development of custom tools.Results: This article presents Neji, an open source framework optimized for biomedical concept recognition built around four key characteristics: modularity, scalability, speed, and usability. It integrates modules for biomedical natural language processing, such as sentence splitting, tokenization, lemmatization, part-of-speech tagging, chunking and dependency parsing. Concept recognition is provided through dictionary matching and machine learning with normalization methods. Neji also integrates an innovative concept tree implementation, supporting overlapped concept names and respective disambiguation techniques. The most popular input and output formats, namely Pubmed XML, IeXML, CoNLL and A1, are also supported. On top of the built-in functionalities, developers and researchers can implement new processing modules or pipelines, or use the provided command-line interface tool to build their own solutions, applying the most appropriate techniques to identify heterogeneous biomedical concepts. Neji was evaluated against three gold standard corpora with heterogeneous biomedical concepts (CRAFT, AnEM and NCBI disease corpus), achieving high performance results on named entity recognition (F1-measure for overlap matching: species 95%, cell 92%, cellular components 83%, gene and proteins 76%, chemicals 65%, biological processes and molecular functions 63%, disorders 85%, and anatomical entities 82%) and on entity normalization (F1-measure for overlap name matching and correct identifier included in the returned list of identifiers: species 88%, cell 71%, cellular components 72%, gene and proteins 64%, chemicals 53%, and biological processes and molecular functions 40%). Neji provides fast and multi-threaded data processing, annotating up to 1200 sentences/second when using dictionary-based concept identification.Conclusions: Considering the provided features and underlying characteristics, we believe that Neji is an important contribution to the biomedical community, streamlining the development of complex concept recognition solutions. Neji is freely available at http://bioinformatics.ua.pt/neji. © 2013 Campos et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A common API for delivering services over multi-vendor cloud resources",
        "doc_scopus_id": "84881477740",
        "doc_doi": "10.1016/j.jss.2013.04.037",
        "doc_eid": "2-s2.0-84881477740",
        "doc_date": "2013-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Business computing",
            "Cloud database",
            "Cloud providers",
            "Cloud services",
            "Cloud storages",
            "Computing markets",
            "Redundant services",
            "Secondary operations"
        ],
        "doc_abstract": "The increasing pace of evolution in business computing services leads enterprises to outsource secondary operations that are not part of their core business. The cloud computing market has been growing over the past few years and, consequently, many cloud companies are now offering a rich set of features to their consumers. Unfortunately, those cloud players have created new services with different APIs, which imply that cloud-oriented applications might be instantiated in one single cloud provider. This scenario is not desirable to the IT industry because their applications will become provider-dependent. In this paper we present a platform that allows applications to interoperate with distinct cloud providers' services using a normalized interface. The proposed approach provides a common API that minimizes the present deficit of cloud API standardization and provides secure and redundant services allocation. Moreover, services from different cloud providers can be combined and decorated with additional functionalities like, for instance, redundancy and ciphering on-the-fly. © 2013 Elsevier Inc.",
        "available": true,
        "clean_text": "serial JL 271629 291210 291773 291791 291869 291870 31 Journal of Systems and Software JOURNALSYSTEMSSOFTWARE 2013-04-24 2013-04-24 2014-08-17T12:57:20 S0164-1212(13)00105-2 S0164121213001052 10.1016/j.jss.2013.04.037 S300 S300.2 FULL-TEXT 2015-05-14T04:32:04.646695-04:00 0 0 20130901 20130930 2013 2013-04-24T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings tomb volfirst volissue volumelist yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 0164-1212 01641212 false 86 86 9 9 Volume 86, Issue 9 9 2309 2317 2309 2317 201309 September 2013 2013-09-01 2013-09-30 2013 SI :The Future of Software Engineering For/In the Cloud Edited by Rami Bahsoon, Ivan Mistrík, Nour Ali, TS Mohan & Nenad Medvidovic article fla Copyright © 2013 Elsevier Inc. All rights reserved. ACOMMONAPIFORDELIVERINGSERVICESOVERMULTIVENDORCLOUDRESOURCES BASTIAOSILVA L 1 Introduction 2 Background and related work 2.1 Cloud computing services 2.1.1 Storage-as-a-service 2.1.2 Database-as-a-service 2.1.3 Notification service 2.2 Interoperability and standardization 3 A Service Delivery Cloud Platform 3.1 Entities 3.2 Cloud services 3.2.1 Cloud streams 3.2.2 Columnar data abstraction 3.2.3 Notification abstraction 3.3 Cloud Controller 3.3.1 RESTful API 3.3.2 Dashboard panel 3.4 Cloud Gateway 3.5 SDCP-SDK 3.6 Privacy and confidenciality model 4 Case studies 4.1 Medical imaging repository 4.2 Sharing medical images “anytime and anywhere” 5 Discussion 5.1 Abstractions 5.2 Advantages and drawbacks 6 Conclusion Acknowledgements References BASTIAO 2011 1 10 L BASTIAO 2011 L APACSGATEWAYCLOUD BIEBERSTEIN 2006 N SERVICEORIENTEDARCHITECTURECOMPASSBUSINESSVALUEPLANNINGENTERPRISEROADMAP DIMARTINO 2011 571 578 B BUILDINGAMOSAICCLOUDS DILLON 2010 27 33 T 201024THIEEEINTERNATIONALCONFERENCEADVANCEDINFORMATIONNETWORKINGAPPLICATIONS CLOUDCOMPUTINGISSUESCHALLENGES DILLON 2010 27 33 T HAJJAT 2010 243 254 M KEAHEY 2009 43 51 K KUMBHARE 2012 510 517 A LEAVITT 2009 5 N LEE 2010 451 459 C PROCEEDINGS19THACMINTERNATIONALSYMPOSIUMHIGHPERFORMANCEDISTRIBUTEDCOMPUTINGACM APERSPECTIVESCIENTIFICCLOUDCOMPUTING PARAMESWARAN 2009 19 26 A PETCU 2011 405 411 D SHAN 2012 82 89 C SOMASUNDARAM 2012 162 167 T RECENTTRENDSININFORMATIONTECHNOLOGYICRTIT2012INTERNATIONALCONFERENCEIEEE ARCHITECTURALFRAMEWORKSOLVEINTEROPERABILITYISSUEBETWEENPRIVATECLOUDSUSINGSEMANTICTECHNOLOGY VECCHIOLA 2009 C ANEKAASOFTWAREPLATFORMFORNETBASEDCLOUDCOMPUTING ZENG 2009 1044 1048 W PROCEEDINGS2NDINTERNATIONALCONFERENCEINTERACTIONSCIENCESINFORMATIONTECHNOLOGYCULTUREHUMANACM RESEARCHCLOUDSTORAGEARCHITECTUREKEYTECHNOLOGIES BASTIAOSILVAX2013X2309 BASTIAOSILVAX2013X2309X2317 BASTIAOSILVAX2013X2309XL BASTIAOSILVAX2013X2309X2317XL item S0164-1212(13)00105-2 S0164121213001052 10.1016/j.jss.2013.04.037 271629 2014-08-17T10:09:15.119889-04:00 2013-09-01 2013-09-30 true 766252 MAIN 9 63627 849 656 IMAGE-WEB-PDF 1 gr8 21106 252 452 gr7 17583 149 339 gr6 19888 229 471 gr5 15950 149 271 gr4 16580 168 294 gr3 29588 282 471 gr2 10329 144 322 gr1 41472 525 386 gr8 4549 122 219 gr7 4814 96 219 gr6 3971 107 219 gr5 7116 120 219 gr4 5678 125 219 gr3 5262 131 219 gr2 3928 98 219 gr1 4398 163 120 JSS 9155 S0164-1212(13)00105-2 10.1016/j.jss.2013.04.037 Elsevier Inc. Fig. 1 SDCP general overview. Fig. 2 Entities of Service Delivery Cloud Platform. Fig. 3 Cloud input/output. Fig. 4 Abstraction columnar data. Fig. 5 Publish/subscribe abstraction. Fig. 6 Cloud controller – architecture. Fig. 7 Cloud controller dashboard. Fig. 8 Cloud gateway architecture. A common API for delivering services over multi-vendor cloud resources Luís A. Bastião Silva ⁎ Carlos Costa José Luís Oliveira University of Aveiro, DETI/IEETA, Portugal University of Aveiro, DETI/IEETA Portugal ⁎ Corresponding author. Tel.: +351 916427877. The increasing pace of evolution in business computing services leads enterprises to outsource secondary operations that are not part of their core business. The cloud computing market has been growing over the past few years and, consequently, many cloud companies are now offering a rich set of features to their consumers. Unfortunately, those cloud players have created new services with different APIs, which imply that cloud-oriented applications might be instantiated in one single cloud provider. This scenario is not desirable to the IT industry because their applications will become provider-dependent. In this paper we present a platform that allows applications to interoperate with distinct cloud providers’ services using a normalized interface. The proposed approach provides a common API that minimizes the present deficit of cloud API standardization and provides secure and redundant services allocation. Moreover, services from different cloud providers can be combined and decorated with additional functionalities like, for instance, redundancy and ciphering on-the-fly. Keywords Cloud services Cloud standardization Cloud storage Cloud databases 1 Introduction The increasing pace of evolution in business computing services leads enterprises to renew their way of operating. Business requirements have also changed and outsourcing has been adopted by many industries, allowing the enterprise to focus more on its core business (Bieberstein, 2006). The Cloud computing market has grown significantly over the past few years and, following the natural progress of business models, there is a great interest in the IT industry in migrating services to this kind of infrastructures (Hajjat et al., 2010; Leavitt, 2009). In order to respond to this demand, many cloud companies increasingly offer new features to their consumers–using a rich set of services on the server side, and/or providing a platform where users can more easily deploy their applications. For instance, Amazon Web Services (Amazon, 2011) has released services such as Simple Storage Service (S3), Amazon SQS, SimpleDB and many others. However, despite this evolution, most cloud companies have been creating distinct APIs for their services, which implies that the applications developed for the cloud can only be instantiated in one single provider, i.e. they are locked to each vendor. To achieve portability and interoperability, it is clear that stockholders, i.e. the user community and the marketplace, must adopt a common design (Lee, 2010). Some efforts have been made to create standards, in order to grant interoperability among the various cloud providers, e.g. SNIA (Association, 2011b) and CDMI (Association, 2011a). The desired scenario consists of creating specifications and interfaces that could be used by all cloud service providers. However, these standards are still drafts, with reduced practical impact. Furthermore, several efforts have been made to standardize Infrastructure-as-a-Service (IaaS), while the Platform-as-a-Service (PaaS) still does not have any consistent unified API. To tackle this issue, a notable effort has been made by several communities, such as jclouds (jclouds, 2010), libcloud (libcloud, 2011) and simplecloud (simplecloud, 2011), to provide a unique programmatic API to deal with multiple cloud solutions. Unfortunately, they do support a limited number of providers and the extension to new ones is not simple. Most of them still focus on IaaS, and there is still a gap in the standardization of other services like storage, database and notification services. The transparent combination of multiple cloud resources will allow applications to communicate with any cloud, even with different providers at same time, leading to the Sky computing concept (Keahey et al., 2009). Although this paradigm was initially used just for IaaS, recently several efforts have been made to extend it to PaaS (Petcu et al., 2011; Di Martino et al., 2011). However, Sky computing for PaaS is still not fully developed and only possible architecture has been briefly discussed until now. Standardization may solve many problems in Cloud computing, but there are also some challenges that must be considered during the process. Several usage scenarios imply dealing with critical information, and applications need to manage access to resources to avoid data tampering. Those privacy concerns are a real problem for some corporations as there is information that they intend to keep safeguarded (Kumbhare et al., 2012). Moreover, Amazon released a new service named AWS Storage Gateway, where the main idea is to cipher and decipher safeguarded data over the cloud, i.e. the data is secure encrypted/decrypted in house. However, it is an Amazon service and it does not work with other cloud providers. This paper presents a platform that allows client applications to easily interoperate with distinct cloud providers, combining and decorating services like, for instance, dynamic data storage across multiple and incompatible infrastructures. Throughout the paper, we will describe a set of APIs related to cloud services, focusing on storage, columnar databases and publishing/subscription, i.e. the principal resources consumed by Internet applications. Finally, a case study will be presented and a discussion of the advantages and drawbacks of the solution will be provided. The rest of the manuscript is organized as follows. The next section will give a background of Cloud computing, giving special emphasis to storage, columnar data and publishing/subscribe services. Also, standardization and interoperability will be discussed. Section 3 presents the system architecture and discusses the proposed abstractions. Section 4 presents a case study, i.e. a clinical solution instantiated with this platform. Section 5 presents a discussion of advantages and drawbacks of the proposed solution, as well as a comparative analysis of other solutions. Finally, the main conclusions of the paper will be presented in a summary in Section 6. 2 Background and related work 2.1 Cloud computing services IT solutions have been mostly supported by on-premise software, i.e. hosted on private enterprise datacenters. With the emergence of cloud computing, off-premise software has increased its applicability and nowadays applications are offered as services through cloud providers. Thus, cloud computing has been adapted to customers’ requirements, creating dissimilar development models among cloud providers. Clearly, there are self-services offered by these companies, where resources are available at anytime and anywhere for customers. Moreover, the resources can be distributed over multiple locations, in order to improve reliability. In the following sections, we will focus on the description of three of these services: Storage-as-a-Service, Database-as-a-Service and Notification Service. 2.1.1 Storage-as-a-service Storage-as-a-Service (SaaS) is the ability to offer remote storage in a local virtualized way, for any operating system and application. Nowadays, Cloud providers are offering storage using the Blobstore concept, which, per se, is not new. In the past, these concepts were used in Database Management Systems (DBMS) in the storage and movement of large data blocks. Blobstores are associative memories, i.e. key-value storage providers, where the blob is unstructured data (value) stored in a container and the lookup is performed through a text key. A container is a namespace or domain for the blobs. A blob is always uploaded to a container. The blobstores have a list of containers where the developer can create, remove or rename them. The container holds content, which can be blobs, folders or virtual path. Also, the blobstore in cloud services has an access control list to authorize people to access the data. In practice, blobstore service allows customers to store data in a container under the Cloud. For instance, Amazon S3, Microsoft Azure and OpenStack have their own blobstore APIs. These services are considered PaaS because they allow developers to take advantage of remote storage service to support their application data in a transparent way. There are many examples of SaaS usage, for instance, the Dropbox application which stores customers’ files in Amazon S3 or commercial web portals that store great quantities of pictures in cloud blobstores. 2.1.2 Database-as-a-service Database-as-a-Service (DaaS) is a new paradigm that outsources the burden effort to the cloud provider. Therefore, the database is hosted in a remote datacenter and can be shared between users, in a transparent way. For instance, Amazon AWS, Windows Azure and Rackspace (2011) offer a database as a service where customers pay for what they use. There is a new type of databases, called columnar data, which are organized in a key-value structure. These databases store the information by column, instead of the traditional ones that store the information by row. It has several advantages in computing large amounts of information and cloud providers are now offering these databases. All database operations are supported by these services, for instance, creating tables, loading and accessing data in the tables. Cloud players often supply an API to access the database, and execute operations through a web service API. Furthermore, database maintainers do not need to worry about the server's redundancy, upgrades, back-up plan and recovery from disaster. Nonetheless, some enterprises are concerned about ensuring data privacy. In fact, this is one of the weaknesses of DaaS. Despite the provision of Server Level Agreements (SLA) by Cloud Providers, there are legal issues that need a very high level of privacy and confidentiality and these organizations do not have clear data in any case. Also, store procedures and triggers might not be supported in the overwhelming majority of cloud providers supplying DaaS. Finally, performance might deteriorate because applications will access the data in remote datacenters when located in the public cloud provider. 2.1.3 Notification service Most information systems and computer applications rely heavily on updated and real time information. In addition, many different applications need to exchange information, i.e. the capability to publish a message from an application and immediately deliver it to other applications that subscribe the same channel. Nowadays, many applications provide this functionality supported on a polling strategy, i.e. checking periodically for new messages. They supply these features in a transparent way to the end-user, although there are extra computational resources requirements and in some cases without the expected efficiency. Cloud providers were influenced by this tendency and have also created services to communicate through a message-based model. Notification services refer to the ability to communicate with another remote entity through Cloud Services. There is some variety of communication services, but essentially they use the Publish/Subscribe model. Several companies are working on notification services, i.e. Amazon SQS (2011), PubNub (PubNub, 2011) and Azure Queue (Corporation). All of them have the same concept: a web service that sends notifications to hanging users through an event-driven workflow. 2.2 Interoperability and standardization Nowadays there is great competition in the industry to provide better and more services over the Cloud. However, the services provided by different players are not typically compatible (Petcu et al., 2011; Shan et al., 2012; Dillon et al., 2010b). Undoubtedly, interoperability and portability is required to allow applications to be ported more easily between different cloud providers. Recently, several groups have been formed to create standards and common interfaces that could allow interoperability between distinct cloud solutions (Parameswaran and Chaddha, 2009). For example, Storage Network Industry Association (SNIA) (Association, 2011b) has been working on a storage data standard in the cloud (Association, 2011a). This standard explores the features that vendors are offering and extracts the common domain, aiming for a high quality cloud storage interface in the future. Also, they focus on the financial paradigm of “pay-as-you-go”, considering that such attributes will interest many businesses. Cloud Computing Interoperability Forum is another group that aims to standardize cloud computing. They are an open and vendor-neutral organization, which intends their solutions to be rapid, potentiating successful industry adoption. There are also other committees with the same goals, such as Open Cloud Computing Interface, Open Grid Forum and Open Cloud Consortium. Most of this standardization work has been related to IaaS, while solutions for PaaS are still emerging (Dillon et al., 2010a). Also, little work has been done regarding standardization in multiple vendor clouds for services such as storage, database and communication. Aneka (Vecchiola et al., 2009), for instance, is a computational platform whose main goal is to support multiple programming models by using specific abstractions for virtual machine deployment. Although cloud services have been used, multi-vendor integration still needs unified specifications (Zeng et al., 2009). Nimbus is an open-source project that aims to offer a Sky computing framework. The main goal is to connect several nodes from different providers, allowing communication between them, transparent migration of machines and ubiquitous management. Nimbus raises the concept of IaaS gateway, working like a deployment orchestrator that allows interoperability to be maintained between several clouds, and creating federated cloud computing, known as Sky computing. Nevertheless, this architecture does not have PaaS services such as databases or communication APIs. mOSAIC (mOSAIC, 2011) is another group that intends to propose a standard API for using Sky-computing. They intend to allow interoperability and portability to support IaaS and also PaaS. However, the project has a wide scope and efforts have been made in the IaaS and processing. They are developing an outstanding architecture that aims to support a Sky in the Cloud computing universe. Moreover, they are designing a language and an API for using multi-cloud resources. However, the project is still ongoing and there are as yet no concerns about the privacy of data. Orleans Bykov et al., 2010 is a programming framework that intends to create a model to outsource the processing, storage and a few other services to the cloud. This framework is quite significant, and is a major contribution to PaaS standardization. However, it does not address interoperability and privacy issues. Cloud4SOA is another European project focusing on achieving semantic interoperability between clouds. They are tackling challenges related with the development of Cloud applications, as well as deployment and migration. This project promises to be very relevant for cloud computing interoperability in the coming years. However, it is still a work in progress. There are other proposals focusing on achieving interoperability through semantic technology. Somasundaram et al. (Somasundaram et al., 2012) created a framework that aims to grant interoperability between Eucalyptus and OpenNebula, proposing a resource description, discovery and submission based on a broker that performs this translation. Although several organizations have come together to constitute interoperability groups, services over the cloud are not yet compatible. Clients from one cloud storage service cannot easily migrate their data to others. For instance, an application developed to use Amazon SimpleDB would not work with Azure Table. It is not flexible and may be expensive if the application needs data stored on different cloud computing platforms. In the following section we propose an architecture that solves this restriction and is able to deal with three cloud services. Moreover, the platform model copes well with upcoming standards and provides support to service combination, decoration and orchestration. For instance, secure and redundant services allocation. 3 A Service Delivery Cloud Platform We have developed a common API for delivering services over multi-vendor cloud resources, entitled Service Delivery Cloud Platform (SDCP). This platform has three main goals: (1) grant interoperability between different cloud providers, creating an abstract layer for three cloud services; (2) deliver services using multiple cloud resources, including storage, database management and notification systems; (3) provide service combination, decoration and orchestration. The first goal (1) consists of granting interoperability between cloud players in a transparent way. Basically, an application can work with as many vendors as is desired, taking advantage of existing cloud providers. The SDCP allows creation of cloud provider poll. For instance, it can store data in multiple cloud vendors or cloud free services, creating a federate view of all containers. In addition, it enables the developer to have interoperability with other protocols (2) inside private networks. Cloud services of distinct providers can bundled and decorated with extra functionalities like, for instance, data ciphering on-the-fly (3). Moreover, cache and pre-fetching mechanisms are other examples of value-added SDCP services, extremely important to reduce latency (3). The presented architecture consists of a hybrid infrastructure that allows “Enterprise to the Cloud” and “Enterprise to the Cloud to Enterprise” applications, i.e. communication between two or more different enterprises, using multiple resources from different cloud vendors. The architecture has basically two main components: the Cloud Controller and the Cloud Gateway (Fig. 1 ). The Controller contains sensitive information and must therefore be deployed in a trustable provider. This separation in the architecture was necessary to support critical use cases. For instance, some information should not be held in the public cloud providers, as discussed further in Section 4. Within the SDCP architecture we have also built a SDK (Software Development Toolkit) that simplifies the development of SDCP-based applications. The SDCP was designed to make it easier to develop and load new application modules using a plugin approach or web service API. As it is possible to see in Fig. 1, the applications are on top of SDCP. The platform is able to deliver new services using the cloud facilities: data store, databases and communication using cloud providers. In order to extend the platform to support different providers and services (e.g. Google, Amazon, Azure, Rakespace, etc.), we have built a specific model whose structural design sustains the use of different modules under the same interface. We have considered a distributed architecture to support multiple accesses to this data, from distinct points. We will describe each component of this architecture in the following sub-sections. 3.1 Entities The platform has its own entities that model the system architecture and describe how it is structured. The fundamental entities and associations of the infrastructure are described in Fig. 2 : • Agent – each gateway has to login using an agent account. Basically, agents are the entities situated inside the enterprise that relay the information to the cloud. • Domain – is a group of agents belonging to the same enterprise or the same trustable group/enterprise group. Thus, only agents of the same domain can communicate or access the data belonging to its domain. • Provider – defines a cloud provider and credentials to access them. It can be a storage, database or communication provider. These providers also belong to a domain. • Private Service – external services that can take advantage of Cloud Controller agents and cloud providers. This service will extend the functionality of the Cloud Controller. These entities are actors and concepts of the SDCP. The domain is a very important concept because it characterizes the trustable model, i.e. models the relationships and the manages the control of the resources. 3.2 Cloud services This section describes the implemented Cloud services. We will describe the three implemented cloud services and how the abstraction for these services was applied. 3.2.1 Cloud streams As expressed, the goal of our platform was to use any resources of the cloud without being locked to a specific provider. To implement this feature for storage services, we used an abstraction to write a set of bytes (i.e. blob) into the Cloud storage using typical Input/Output (I/O) streams. The designed abstraction assures provider independency but also makes it easier to extend to other cloud solutions. Two new I/O entities were implemented: CloudInputStream and CloudOutputStream, Fig. 3 . These entities are used to read/write in the storage services as a common Java stream mechanism. An important aspect regarding the writing of a blob is the access policy. By default, we assume that the blob is private, although the user can specify an ACL (Access Control List) to give permission to the blob. A blobstore API has different features implemented in different cloud providers. Although several features are presented in the blobstore API, others are not often presented. Our abstraction will not consider these features, and in that case an extension to the platform will be necessary. Nonetheless, we take into account that several features are just used occasionally, and a trade-off was necessary. At present, most cloud storage solutions do not offer an option to encrypt data when it is uploaded to the cloud. Our platform has an encryption/decryption layer on the client side, i.e. the cipher and decipher operations are executed on-the-fly on the enterprise side, through our abstraction. In that case, it is ciphered with AES (Advance Encryption Standard) algorithm and the key is stored in the Cloud Controller. On the other hand, multiple cloud providers can be supplied with a list of CloudSocket being blobs written in both and read from the first one that is available. The developed Cloud Streams extend the IO Java streams. The Cloud socket contains the identifier of the implementation that will be used to call the most appropriate one for a specific service. JClouds (jclouds, 2010) is an open source framework for cloud development that already provides several cloud players, and as such we decided to build the Cloud Streams as an instance of JClouds blobstores. In addition, we implemented our local storage, following the proposed abstraction. Furthermore, new APIs of different blobstore cloud vendors can be easily implemented using the proposed abstraction. 3.2.2 Columnar data abstraction As in the previous storage service, we have also developed the same generic API upon cloud databases. This aims to create an abstraction to columnar data, for instance, SimpleDB, Azure Table and other cloud databases publicly available. Nowadays there is a new trend to store information in columnar data instead of the traditional relational system. These tables are very dynamic and the developer does not need to pre-define a model, because the structure auto-fits the data. There were several problems regarding scalability, which have to be solved in this abstraction. For instance, Amazon Simple DB uses a mainly horizontal scalability, in opposition to Azure Table, which allows control of the vertical partition. Each partition key represents a different node to have the information. This issue was solved through the Table ID, which identifies the Table name, together with the node label or the location label. The idea was to contain generic features that can be applied in many database services. We implemented two of the available APIs, but it will work for other databases. The Java SDK already uses a high-level abstraction for databases, named JPA (Java Persistence API). Although it is widely used with Object-Oriented databases, we decided to follow this standard for two main reasons: JPA is often used by Java developers to abstract the access to databases, and it is easy to keep compatibility with these applications and the chosen an API that fits the JPA abstraction. Thus, it was decided to use the same JPA methods and also add other methods that are specific to the Cloud databases, such as create/remove tables (Fig. 4 ). Also, for representation of the results, we use a library named Guava (Google Collections), which provides very generic Java collections, e.g. the Table collection. Table is a triple values class <R, C, V> data structure, i.e. Row, Column and Value. This representation is perfect to retrieve the results of queries and also to insert new data into the columnar tables. Fig. 4 shows the architecture of the columnar data abstraction. A Select action is executed quite similarly to the JPA method. It uses a small set of the SQL and just conditions are considered. Complex queries with joins will not be considered in this abstraction, since the columnar tables do not support such a feature. The data columnar abstraction was deployed on Amazon SimpleDB. The API has a different representation from the JPA abstraction. For instance, each row is called Item, and each item has several attributes that are not structured and can change dynamically for each item. That is why it is called columnar data, because it can be different for each row, i.e. each record can contain different fields. Thus, when creating the table, we do not define a structure as in a common database. The SimpleDB uses a REST to supply the programmatic interaction with the developer and the results are retrieved in XML files with the responses. So the first step was to create the XML parsers and client communication with the REST AWS interface, as described in their specification (2011). The abstraction for this service is quite similar to the Cloud Streams, and any specific implementation has to be compliant with interfaces described in Fig. 4. In the SimpleDB case, we implemented all functions documented in the abstraction. The model copes with the common API with minor conversions. 3.2.3 Notification abstraction The notification abstraction aims to dynamically create a message-based communication, based on the Publish/Subscribe mechanism. It is asynchronous, allowing application delivery using this platform to tackle the polling issue often implemented in many applications to simulate an asynchronous system. However, not many Publish/Subscribe public services use only HTTP. We will take the example of PubNub (2011), although a new instance can be implemented, for example using other public services such as Channel API of Google AppEngine, or other protocols like XMPP which support the Publish/Subscribe mechanism. Moreover, the polling approach can also fit the abstraction, and in that case the subscriber has to poll the server until it has a signal message, and then it will call the Receiver callback. Also, for instance, Ajax Push Engine (APE) can be installed in a public cloud provider like Amazon EC2, and the service can be used with quite similar behavior to PubNub. Nevertheless, there are also very similar services based on the Publish/Subscribe model, for instance, Amazon SQS and Azure Queue. In this service abstraction, we used an Observer Pattern, and in the current implementation we created two entities: Publish and Subscribe (see Fig. 5 ). The channel represents the domain of each agent, and it assures that the communication can only be established between agents of the same domain. It is important to mention that PubNub specific implementation is quite analogous to the one proposed. So the abstraction classes will call the implementation of PubNub directly using the adapter pattern, similarly to the other previously presented abstractions. 3.3 Cloud Controller The Cloud Controller is a major component of our architecture responsible for functionalities such as: aggregating providers’ credentials; controlling access to cloud resources; managing authentication processes with Cloud Gateways; and addition of new services. This controller provides an API that can be used by third party applications to access their services. The Controller communicates through HTTP, using RESTful specification; thereby it will be much easier for other entities to access services. The Cloud Controller allows us to store credentials of cloud providers for different services, such as blobstore, database and communication (Fig. 6 ). Also, the ciphered keys used to cipher and decipher the blobs are stored in the Cloud controller, unless the developer explicitly denies the action. Moreover, it also supports addition of external services used by third party applications, extending in this way the Cloud Controller functionality. This platform was instanced with several end-user services associated with Medical Imaging (Repository Data Privacy), particularly the safe storage of medical data in multiple cloud players as described in Section 4. There is critical information in diverse scenarios. In such cases, the developer can create a new service in a private cloud to keep the more restricted access data. Our platform will be compatible with public or private clouds. Moreover, the Cloud Gateway can cipher the data before sending it to the cloud, and store the keys in these private services. 3.3.1 RESTful API The interface to external applications is issued as a RESTful web service that provides several interfaces, starting with an authentication mechanism. User validation is based on username and password and if the login is valid, the web service returns a token that will be used to validate subsequent operations. We created functions to get cloud provider and services information. 3.3.2 Dashboard panel In addition to the web services API, the Cloud Controller also provides a web portal interface (Fig. 7 ), whereby administrators can add or remove new cloud providers (storage, database, services, etc.) and also check the operation's logs. This portal was implemented through GWT (Google Web Toolkit) technologies. Also, they can create new domains, add/remove/ban agents and add new services. This dashboard also allows the user to setup a threshold of cloud provider requests because the actions of gateways cloud interactions are sent to the Cloud Controller. 3.4 Cloud Gateway The Cloud Gateway is a very important component of the architecture. Basically it is an application that loads new services dynamically. It grants authentication from the Cloud Controller and automatically loads the services that are uploaded by the user. Cloud Gateway can run as a daemon. Also, it has an optional external GUI that allows the user to load new plugins/applications or see operation logs. For instance, new adapters for new cloud providers are loaded in the Cloud Gateway. The architecture of Cloud Gateway (Fig. 8 ) also uses the SDCP-SDK. Namely, it has access to the plugin core mechanism to load new plugins. Moreover, the interfaces used in API plugins will be instantiated automatically using the Inversion of Control pattern. The plugins to the Cloud Gateway can be services programmed in Java, directly using the SDCP-SDK, but we offer the possibility of external applications, sending information to the cloud through a web service interface. This raises a question: what is the advantage of using the web service API? Third-party application will be allowed to store, access, and use resources from multiple public clouds, using a normalized interface. Thus, third-party applications do not need to be coupled as a Cloud Gateway Java plugin. Nearly every web application requires an authentication system. The Cloud Gateway is the middleware layer that allows access to cloud resources, and thereby it requires a user validation system. The Cloud Gateway authentication is used through the RESTful web services that access the Cloud Controller web services, previously described. When the gateway application starts, it requires a username and password for the end-user. Next, Cloud Gateway executes authentication and saves the token. 3.5 SDCP-SDK The end users of the SDCP are allowed to develop applications that use the cloud resources, as well as new plugins to new cloud providers. Thus, the applications can also take advantage of other cloud providers that the developer wants to support. To create these new applications, the developer will use the SDCP-SDK. The SDCP-SDK defines contracts and specification of the platform, including the communication between the Cloud Controller and the Cloud Gateway. The platform was developed in Java through a set of interfaces. The main idea is that the developer can take advantage of SDCP-SDK to delegate the authorization process to the platform. Also, the access to the cloud resources is provided by the SDK. The new application will be deployed in Cloud Gateway, the entity responsible for loading the applications. On the other hand, the abstractions of blobstore, columnar data and notification systems are also possible to extend using the SDCP-SDK. For instance, it is possible to the developer write a new plugin for a specific provider based on the SDCP-SDK, only implementing the methods described earlier in Section 3.2. We developed a plugin for notification system based on PubNub in 8h and now all developed applications with SDCP will benefit of this provider. 3.6 Privacy and confidenciality model Undoubtedly, cloud computing has several advantages for enterprises, but two major issues need to be addressed: the cost/benefits of the solution and the privacy and confidentiality of the data stored over the cloud. The first issue depends on the business, and several studies (Armbrust et al., 2009) have been addressing the financial impact of cloud computing. Often associated with data tampering, privacy aspects are still a challenge in these scenarios. Our platform takes those two aspects into consideration, because we can store the information in multiple cloud players and, at the same time, we also tackle the privacy and confidentiality issue. The solution architecture was built taking into account that particular requirement. Our cloud has two main components: Cloud Controller and the cloud players. Thus, for instance, in storage service, we have the opportunity to store the information in a ciphered way. At present, most cloud solutions do not offer an option to encrypt data when it is uploaded to the cloud. Some companies are already offering this service, for instance, AWS Storage Gateway, but we believe this should be a client service to give more confidence in the cloud solution. Our proposed platform has an encryption/decryption layer on the client side, i.e. the cipher and decipher operations are executed on-the-fly on the enterprise side. Moreover, this privacy issue is independent of the cloud vendor and the data can be easily sent and accessed in multiple cloud players at the same time. The end-user can do that more easily when writing, specifying a list of cloud providers they intend to use. Use of a common interface should be adopted by services and this will be a contribution to Cloud resources standardization. Another important issue regarding the architecture is that it can be deployed in a hybrid infrastructure. Nonetheless, the Cloud Controller can be deployed in a public or private cloud. Several applications may want to extend the functionality of the Cloud Controller because it may be relevant to host some information in the public cloud. 4 Case studies 4.1 Medical imaging repository Several use case scenarios that will benefit from the proposed solution can be pointed out. In this article, we will describe one such example: medical imaging repositories. Over the past two decades, the healthcare sector has increasingly adopted ICT to support diagnoses, treatment and patient care. Medical imaging, for instance, produces a huge amount of information and takes advantage of these technologies in daily diagnostic procedures. PACS (Picture Archive Communication System) encompasses several hardware and software technologies for acquisition, distribution, storage and analysis of digital images in distributed environments. The main components are: image acquisition devices, storage archive units, display workstations and databases. The amount of medical images has increased significantly over the last decade as result of the increase in the number and quality of studies. According to some researchers, this trend will continue over the next years. A common PACS archive has two major components: the DICOM (Digital Imaging and Communication in Medicine) object repository and the database system. The object repository typically demands an infrastructure with huge storage capacity to support all DICOM studies. The database module is normally a relational database management system (RDBMS) that supports the DICOM Information Model (DIM) (DICOM-P3, 2001), containing mandatory metadata related to patients, studies, series and images. Medical institutions have to store a large number of medical studies/images, i.e. DICOM object files. Thus, they need to have large datacenters inside the hospital, a major source of problems for systems’ administrators. The Cloud Computing model fits this scenario. Moreover, healthcare is a critical service and the information must always be available. Therefore, the approach described in this paper is crucial to create solutions that can be instanced in more than one cloud provider. Based on the SDCP-SDK presented, a PACS Cloud archive was developed supporting medical image storage and database services. A PACS Cloud Gateway to the Cloud (Bastião et al., 2011) was developed, which provides outsourcing of these two components to the cloud, namely using the new concepts of blobstore and database accessible through web services. Internally, the Gateway is seeded as a common Intranet PACS repository supporting DICOM standard services. A Cloud Gateway plugin was instantiated and also another component named PACS Cloud Controller to keep sensitive medical information. Those PACS modules were both deployed to interact with two distinct cloud providers, namely Amazon S3 (S3) and Google Storage (Bastiao et al., 2011). This solution is used to support a distributed archive over a medical institution with multiple centers. The medical repository is deployed over a private cloud, with two distinct gateways supporting the access to the archive. There are an average of 250 exams daily for this archive. Our solution will also replicate a given percentage of the archive to a public cloud provider, in a ciphered way, in order to achieve higher availability taking benefit of the SDCP features, i.e. writing in two distinct cloud providers using our common API. Due to some technical reasons or natural disasters the private cloud might be offline for a certain amount of time. If the solution is also deployed in a public cloud, medical data will be not lost and querying the repository will still be possible. Moreover, since we are working with huge volume of data the latency is one of the main barriers to the adoption of cloud services in medical imaging environments. The SDCP gateway plugin includes cache and pre-fetching mechanisms increasing considerably the quality of service provided. 4.2 Sharing medical images “anytime and anywhere” Cloud computing is largely used to share files over the Internet, and many examples can be pointed out, such as Dropbox (Dropbox, 2011) and Gmail (Google, 2011). Moreover, Cloud providers offer high availability and scalability of their services. In this case study, we do not outsource the medical repositories to the cloud. Instead, we keep them in the healthcare enterprises, but the goal is to grant external and/or inter-institutional access over organization boundaries. Our DICOM relay architecture takes advantage of the SDCP platform to provide transparent exchange of imagiologic information between several locations. Communication between the components of the digital medical laboratories is mainly carried out through DICOM. An application deployed in the Cloud Gateway was developed to effect a DICOM protocol forwarder via cloud SDCP. This DICOM relay implements the DICOM protocol and waits for new requests. For instance, when it receives storage requests for medical exams, it forwards the messages to another application that is running in another institution. In order to achieve this, the application relies on storage and notification services offered by the SDCP-SDK. Currently, this solution is supporting teleradiology in a medical institution, with two distinct locations. Physicians are reporting from several places, using the same central medical repository and the DICOM Router to access medical imaging. The solution is online 24/7 and is supporting transferring around 50 exams daily. Our Bridge was deployed in a local private cloud. The direct connection is frequently blocked or needs to be set up in the network, unlike our transparent Web 2.0 approaches. 5 Discussion 5.1 Abstractions The SNIA project has developed notable work in cloud storage standardization. A large set of important features are supported in the blobstore, including the methodologies to transfer objects to the cloud provider, the mechanism to request and get information and what kind of metadata can be supported. Furthermore, OpenStack has been adopting the SNIA design principles and companies like HP Cloud Services and Rackspace are using OpenStack to support their API infrastructure. However, there are still other players not offering SNIA compatible, which makes the presented platform and the proposed abstractions worthwhile. “Database-as-a-Service” is also a very valuable service offered by cloud players. However, standardization at this level has not yet been developed. Nonetheless, much effort has been made over the years to make interoperability between databases possible. In turn, implementing several drivers for each provider could solve the problem, using solutions like JPA, which we proposed in SDCP. Although this may be true, we made several contributions to the standardization of these interfaces. However, if organizations like SNIA extend their work to databases, interoperability will become much easier. Regarding the asynchronous systems for notification, there are no standards yet. Similar to database abstraction, we present a contribution toward standardization, as well as creating an abstraction for it. XMPP is a good standard for Publish/Subscribe and many applications use XMPP as a communication service. Nevertheless, the protocol was written to be a chat and not for notification proposes. Moreover, although the XMPP runs over HTTP, the service is not commonly offered as a web service. Thus, a possible approach to standardization in this service will be achieved by using the Publish/Subscribe pattern with multiple channels to communicate. 5.2 Advantages and drawbacks Despite the decoupling of the cloud players, the platform still has some weaknesses. Introduction of an abstraction, as a middleware, does not allow support of all features of all cloud providers. Cloud solution vendors are continuously competing for new and distinct features, so they can build a solid position in this growing market. Despite this competition is quite positive for customers, it also raises several problems mostly related with the lack of interoperability. This lack of normalization between interfaces can be easily overpassed with SDCP, which takes a conservative view of available services–keeping the maximum common set. However, if these services are not enough, and developers want to take advantage of a specific service for on provider, they can use the service directly through the vendor API, or they can create a plugin through SDCP. For instance, common features presented in blob structures, follow a key-value strategy inside a bucket. If a particular provider decided to support blob compression, since the SDCP API do not include that feature, it will be necessary to use the original service API or specific. Our approach explores several APIs and proposes an implemented platform that is able to support many vendors and easier to extend to other new providers. Using cloud computing, the risk of incidents is reduced because cloud providers have the data in multiple locations. However, there is another risk that developers have to consider: what happens if cloud-computing providers stop supplying their services? Such a situation will certainly harm cloud clients. The proposed approach will greatly minimize those risks, because the data can be redundantly stored in multiple cloud providers, without impact on SDCP API client applications. Moreover, it can automatically forward the resource to another provider, if a cloud provider starts failing. SDCP normalized interface over services of distinct providers, is an important advantage but not the unique one. Providers can be selected on-demand according to client predefined rules or quality parameters. The multi-provider services can be combined, orchestrated and decorated with new functionalities. The result is a value-added service with a high abstraction level. 6 Conclusion Despite many cloud service applications having been developed, the interoperability and portability of cloud resources is still a major challenge, and common specifications and standards are needed to enable multi-vendor integration. In this paper we have presented a solution targeting this goal. The proposed Service Delivery Cloud Platform (SDCP) is a cloud middleware infrastructure that provides a rich set of services using resources from multiple cloud providers. The presented architecture allows decoupling the specificities of each cloud provider API into a unique abstraction. Throughout the article we have extensively detailed the system architecture, and discussed the advantages and drawbacks of the presented solution. The platform was validated with private and public clouds, using three common services: blobstore, columnar data and notification service. In addition, the platform was successfully used in several medical image scenarios that exploit storage redundancy using two databases, in private and public clouds, and the notification service to communicate between multiple data access points. Acknowledgements The research leading to these results has received funding from FEDER under the COMPETE programme and by FCT – Fundação para a Ciência e a Tecnologia under grant agreement PTDC/EIA-EIA/104428/2008 and SFRH/BD/79389/2011. References Amazon SimpleDB Documentation, 2011 Amazon SimpleDB Documentation, 2011. Secondary, Secondary Amazon SimpleDB Documentation. Amazon SQS, 2011 Amazon SQS, 2011. Secondary, Secondary Amazon SQS, Amazon, 2011 Amazon, 2011. Amazon webservices (AWS). In: Secondary Amazon (Ed.), Secondary Amazon Webservices (AWS). Armbrust et al., 2009 Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R.H., Konwinski, A., Lee, G., Patterson, D.A., Rabkin, A., Stoica, I., 2009. Above the Clouds: A Berkeley View of Cloud Computing. Technical Report UCB/EECS-2009-28, EECS Department, University of California, Berkeley. Association S.N.I., 2011a Association, S.N.I., 2011a. Cloud data management interface v.1.0.1h, Overview of cloud storage. Association S.N.I., 2011b Association, S.N.I., 2011b. Cloud storage initiative. In: Secondary Association, S.N.I. (Ed.), Secondary Cloud Storage Initative. Bastiao et al., 2011 L. Bastiao C. Costa J.L. Oliveira A PACS archive architecture supported on cloud services International Journal of Computer Assisted Radiology and Surgery 2011 1 10 Bastião et al., 2011 L.A. Bastião C. Costa J.L. Oliveira A PACS Gateway to the Cloud 2011 CISTI Chaves Bieberstein, 2006 N. Bieberstein Service-oriented architecture compass: business value, planning, and enterprise roadmap 2006 Prentice Hall Bykov et al., 2010 Bykov, S.a.G., A., Kliot, G., Larus, J., Pandya, R., Thelin, J., 2010. Orleans: a framework for cloud computing. Corporation, M, Windows Azure Platform. Cloud Computing Interoperability Forum (CCIF), 2011 Cloud Computing Interoperability Forum (CCIF), 2011. In: Secondary (Ed.), Secondary Cloud Computing Interoperability Forum (CCIF), Di Martino et al., 2011 B. Di Martino D. Petcu R. Cossu P. Goncalves T. Máhr M. Loichate Building a Mosaic of Clouds 2011 Springer Ischia, Italy 571 578 DICOM-P3, 2001 DICOM-P3, 2001. Digital Imaging and Communications in Medicine (DICOM), Part 3: Information Object Definitions. National Electrical Manufacturers Association. Dillon et al., 2010a T. Dillon C. Wu E. Chang Cloud computing: issues and challenges 2010 24th IEEE International Conference on Advanced Information Networking and Applications 2010 27 33 Dillon et al., 2010b T. Dillon C. Wu E. Chang Cloud computing: issues and challenges IEEE 2010 27 33 Dropbox, 2011 Dropbox, 2011. Dropbox service, In: Secondary Dropbox (Ed.), Secondary Dropbox Service, Google Storage for Developer, in press Google Storage for Developers, Accessed in April 2013. Google, 2011 Google, 2011. Gmail, In: Secondary Google (Ed.), Secondary Gmail, Hajjat et al., 2010 M. Hajjat X. Sun Y.-W.E. Sung D. Maltz S. Rao K. Sripanidkulchai M. Tawarmalani Cloudward bound: planning for beneficial migration of enterprise applications to the cloud SIGCOMM: Computer Communication Review 40 2010 243 254 jclouds, 2010 jclouds, 2010. jclouds: multi-cloud library, In: Secondary jclouds (Ed.), Secondary jclouds: Multi-Cloud Library, Keahey et al., 2009 K. Keahey M. Tsugawa A. Matsunaga J. Fortes Sky computing Internet Computing, IEEE 13 2009 43 51 Kumbhare et al., 2012 A. Kumbhare Y. Simmhan V. Prasanna Cryptonite: a secure and performant data repository on public clouds, Cloud Computing (CLOUD) 2012 IEEE 5th International Conference on IEEE 2012 510 517 Leavitt, 2009 N. Leavitt Is cloud computing really ready for prime time? Growth 27 2009 5 Lee, 2010 C.A. Lee A perspective on scientific cloud computing Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing. ACM Chicago, IL 2010 451 459 libcloud, 2011 libcloud, 2011. libcloud, In: Secondary libcloud (Ed.), Secondary libcloud, mOSAIC, 2011 mOSAIC, 2011. mOSAIC platform, In: Secondary mOSAIC (Ed.), Secondary mOSAIC Platform, Open Cloud Consortium (OCC), 2011 Open Cloud Consortium (OCC), 2011. In: Secondary, Secondary Open Cloud Consortium (OCC), OGF, 2011 OGF, 2011. Open Grid Forum (OGF), In: Secondary (OGF), O.G.F. (Ed.), Secondary Open Grid Forum (OGF), Parameswaran and Chaddha, 2009 A.V. Parameswaran A. Chaddha Cloud interoperability and standardization SETLabs Briefings 7 2009 19 26 Petcu et al., 2011 D. Petcu C. Craciun M. Neagul I. Lazcanotegui M. Rak Building an interoperability API for sky computing IEEE 2011 405 411 PubNub, 2011 PubNub, 2011. PubNub, In: Secondary PubNub (Ed.), Secondary PubNub, S3, A., Amazon Simple Storage Service. Shan et al., 2012 C. Shan C. Heng Z. Xianjun Inter-cloud operations via NGSON. Communications Magazine IEEE 50 2012 82 89 simplecloud, 2011 simplecloud, 2011. Simple cloud API, In: Secondary simplecloud (Ed.), Secondary Simple Cloud API, Somasundaram et al., 2012 T. Somasundaram K. Govindarajan M. Rajagopalan S. Rao An architectural framework to solve the interoperability issue between private clouds using semantic technology Recent Trends in Information Technology (ICRTIT), 2012 International Conference on IEEE 2012 162 167 Vecchiola et al., 2009 C. Vecchiola X. Chu R. Buyya Aneka: A Software Platform for Net-Based Cloud Computing 2009 arXiv:0907.4622 Zeng et al., 2009 W. Zeng Y. Zhao K. Ou W. Song Research on cloud storage architecture and key technologies Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human ACM 2009 1044 1048 Luís A. Bastião Silva is a PhD student of MAP Doctoral Program in Computer Science of University of Aveiro. He received his Master of Science on Computer and Telematics Engineering in 2011. He worked on medical informatics research with partnerships with several medical institutions. His current research interests include healthcare records, medical imaging repositories, health records data federation, integration and cloud computing. Carlos Manuel Azevedo Costa is Assistant Professor at the Department of Electronics, Telecommunications and Informatics of the Aveiro University. He is also a researcher at the Institute of Electronics and Telematics Engineering of Aveiro (IEETA) and member of the Bioinformatics group at the University of Aveiro. He holds a PhD in Medical Informatics and he is author or co-author of more than 80 publications in this area. His main research activity is in the area of PACS-DICOM (medical imaging systems and networks) and Healthcare Information Systems. He has also interests in other areas of research such as telemedicine, security and access control. José Luis Oliveira is an associate professor at the Electronics, Telecommunications and Informatics Department, University of Aveiro, Portugal. His research interests include text mining, information retrieval, distributed systems and computational methods in biomedical informatics. He was involved in more than 20 international projects, such as InfoGenMed, Daidalos, EuroNGI, InfoBioMed, EU-ADR (FP7), GEN2PHEN (FP7), RD-CONNECT (FP7) and EMIF (IMI). He has more than 250 publications in book chapters, journals and international conferences. "
    },
    {
        "doc_title": "Drug-Induced Acute Myocardial Infarction: Identifying 'Prime Suspects' from Electronic Healthcare Records-Based Surveillance System",
        "doc_scopus_id": "84883144641",
        "doc_doi": "10.1371/journal.pone.0072148",
        "doc_eid": "2-s2.0-84883144641",
        "doc_date": "2013-08-28",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background:Drug-related adverse events remain an important cause of morbidity and mortality and impose huge burden on healthcare costs. Routinely collected electronic healthcare data give a good snapshot of how drugs are being used in 'real-world' settings.Objective:To describe a strategy that identifies potentially drug-induced acute myocardial infarction (AMI) from a large international healthcare data network.Methods:Post-marketing safety surveillance was conducted in seven population-based healthcare databases in three countries (Denmark, Italy, and the Netherlands) using anonymised demographic, clinical, and prescription/dispensing data representing 21,171,291 individuals with 154,474,063 person-years of follow-up in the period 1996-2010. Primary care physicians' medical records and administrative claims containing reimbursements for filled prescriptions, laboratory tests, and hospitalisations were evaluated using a three-tier triage system of detection, filtering, and substantiation that generated a list of drugs potentially associated with AMI. Outcome of interest was statistically significant increased risk of AMI during drug exposure that has not been previously described in current literature and is biologically plausible.Results:Overall, 163 drugs were identified to be associated with increased risk of AMI during preliminary screening. Of these, 124 drugs were eliminated after adjustment for possible bias and confounding. With subsequent application of criteria for novelty and biological plausibility, association with AMI remained for nine drugs ('prime suspects'): azithromycin; erythromycin; roxithromycin; metoclopramide; cisapride; domperidone; betamethasone; fluconazole; and megestrol acetate.Limitations:Although global health status, co-morbidities, and time-invariant factors were adjusted for, residual confounding cannot be ruled out.Conclusion:A strategy to identify potentially drug-induced AMI from electronic healthcare data has been proposed that takes into account not only statistical association, but also public health relevance, novelty, and biological plausibility. Although this strategy needs to be further evaluated using other healthcare data sources, the list of 'prime suspects' makes a good starting point for further clinical, laboratory, and epidemiologic investigation. © 2013 Coloma et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "BeCAS: Biomedical concept recognition services and visualization",
        "doc_scopus_id": "84880552535",
        "doc_doi": "10.1093/bioinformatics/btt317",
        "doc_eid": "2-s2.0-84880552535",
        "doc_date": "2013-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [
            "Data Mining",
            "Databases, Factual",
            "Internet",
            "MEDLINE",
            "Software"
        ],
        "doc_abstract": "The continuous growth of the biomedical scientific literature has been motivating the development of text-mining tools able to efficiently process all this information. Although numerous domain-specific solutions are available, there is no web-based concept-recognition system that combines the ability to select multiple concept types to annotate, to reference external databases and to automatically annotate nested and intercepted concepts. BeCAS, the Biomedical Concept Annotation System, is an API for biomedical concept identification and a web-based tool that addresses these limitations. MEDLINE abstracts or free text can be annotated directly in the web interface, where identified concepts are enriched with links to reference databases. Using its customizable widget, it can also be used to augment external web pages with concept highlighting features. Furthermore, all text-processing and annotation features are made available through an HTTP REST API, allowing integration in any text-processing pipeline.Availability: BeCAS is freely available for non-commercial use at http://bioinformatics.ua.pt/becas.Contacts: or jlo@ua.pt © 2013 The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "OralCard: A bioinformatic tool for the study of oral proteome",
        "doc_scopus_id": "84878216035",
        "doc_doi": "10.1016/j.archoralbio.2012.12.012",
        "doc_eid": "2-s2.0-84878216035",
        "doc_date": "2013-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Otorhinolaryngology",
                "area_abbreviation": "MEDI",
                "area_code": "2733"
            },
            {
                "area_name": "Dentistry (all)",
                "area_abbreviation": "DENT",
                "area_code": "3500"
            },
            {
                "area_name": "Cell Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1307"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Objectives: The molecular complexity of the human oral cavity can only be clarified through identification of components that participate within it. However current proteomic techniques produce high volumes of information that are dispersed over several online databases. Collecting all of this data and using an integrative approach capable of identifying unknown associations is still an unsolved problem. This is the main motivation for this work. Results: We present the online bioinformatic tool OralCard, which comprises results from 55 manually curated articles reflecting the oral molecular ecosystem (OralPhysiOme). It comprises experimental information available from the oral proteome both of human (OralOme) and microbial origin (MicroOralOme) structured in protein, disease and organism. Conclusions: This tool is a key resource for researchers to understand the molecular foundations implicated in biology and disease mechanisms of the oral cavity. The usefulness of this tool is illustrated with the analysis of the oral proteome associated with diabetes melitus type 2. OralCard is available at http://bioinformatics.ua.pt/oralcard. © 2013 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 271218 291210 291722 31 Archives of Oral Biology ARCHIVESORALBIOLOGY 2013-02-08 2013-02-08 2014-08-20T12:17:27 S0003-9969(13)00003-4 S0003996913000034 10.1016/j.archoralbio.2012.12.012 S300 S300.2 FULL-TEXT 2015-05-13T23:31:12.449479-04:00 0 0 20130701 20130731 2013 2013-02-08T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings tomb volfirst volissue volumelist yearnav figure table e-component body affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0003-9969 00039969 false 58 58 7 7 Volume 58, Issue 7 4 762 772 762 772 201307 July 2013 2013-07-01 2013-07-31 2013 Saliva and salivary glands article fla Copyright © 2013 Elsevier Ltd. All rights reserved. ORALCARDABIOINFORMATICTOOLFORSTUDYORALPROTEOME ARRAIS J 1 Introduction 2 Materials and methods 2.1 Manual data assembly and curation 2.2 Automatic data annotation 2.3 Implementation 3 Results 3.1 Protein view 3.2 Disease view 3.3 Organism view 4 Discussion 4.1 Diabetes mellitus type 2 (DMT2) case study Funding Competing interests Ethical approval Authors’ contribution References ROUABHIA 2002 47 51 M KAUFMAN 2002 197 212 E IMAMURA 2003 443 450 T MANDEL 1990 119 125 I LEE 2009 241 248 Y WESTRA 2004 8130 8131 W LAWRENCE 2002 170 174 H SMOOT 2005 6 11 L CHIAPPELLI 2006 331 334 F LIJNEN 2001 511 517 I AMADO 2005 521 539 F HU 2007 3588 3600 S LOO 2010 1016 1023 J CHERRY 2012 D700 D705 J CONSORTIUM 2012 D71 D75 T ROSA 2012 N BERNSTEIN 1977 535 542 F SEAL 2011 D514 D519 R MAGLOTT 2005 D54 D58 D FLICEK 2011 D800 D806 P MCDONAGH 2011 795 806 E SOHNGEN 2011 329 C HAMOSH 2005 D514 D517 A KANEHISA 2012 D109 D114 M ASHBURNER 2000 25 29 M BANDHAKAVI 2011 1052 1061 S BOSTANCI 2010 2191 2199 N BRINKMANN 2011 51 55 O CABRAS 2010 2099 2108 T CASTAGNOLA 2011 M CHI 2009 1453 1474 L CHOI 2011 42 Y COSTA 2010 384 391 P DENNY 2008 1994 2006 P DOWLING 2008 168 175 P FANG 2007 5785 5792 X FLEISSIG 2009 61 68 Y GIUSTI 2007 1634 1643 L GIUSTI 2007 2063 2069 L GONCALVESLDA 2010 1334 1341 R GONZALEZBEGNE 2009 1304 1314 M GONZALEZBEGNE 2011 5031 5046 M GRANT 2010 4732 4744 M HAIGH 2010 241 247 B HARDT 2005 2885 2899 M HARDT 2005 4947 4954 M HE 2004 271 278 Q HJELMERVIK 2009 342 353 T HU 2008 6246 6252 S HU 2005 1714 1728 S HUANG 2004 951 962 C ITO 2009 269 271 K JOU 2010 41 48 Y LARSEN 2007 1778 1787 M LAWLER 2009 R1496 R1502 J LO 2007 101 107 W MESSANA 2004 792 800 I NEGISHI 2009 1605 1611 A PREZA 2009 161 169 D QUINTANA 2009 822 830 M RAMACHANDRAN 2008 80 104 P RAMACHANDRAN 2006 1493 1503 P RAO 2009 239 245 P RYU 2006 1077 1086 O SIQUEIRA 2008 445 450 W STONE 2011 1728 1736 M STRECKFUS 2009 737619 C TURHANI 2006 1417 1423 D VITORINO 2004 570 575 R WALZ 2006 1631 1639 A WILMARTH 2004 1017 1023 P WU 2011 519 2125 Y WU 2009 636 644 Y XIE 2005 1826 1830 H YAN 2009 116 134 W YANG 2006 405 407 L ESSER 2008 25 27 D JAGTAP 2012 992 1001 P XIE 2008 486 498 H KERSEY 2004 1985 1988 P 2012 D71 D75 LAKSCHEVITZ 2011 433 439 F ABIKO 2010 186 191 Y LAMSTER 2008 19S 24S I MAFRICI 2010 467 477 A NAWALE 2006 337 344 R ALTARAWNEH 2011 353 361 S CASTAGNOLA 2012 33 46 M XIAO 2012 63 69 H ARRAISX2013X762 ARRAISX2013X762X772 ARRAISX2013X762XJ ARRAISX2013X762X772XJ item S0003-9969(13)00003-4 S0003996913000034 10.1016/j.archoralbio.2012.12.012 271218 2014-08-20T17:10:45.444168-04:00 2013-07-01 2013-07-31 true 2374065 MAIN 11 53959 849 656 IMAGE-WEB-PDF 1 mmc1 mmc1.xlsx xlsx 15810 APPLICATION gr6 169938 713 2000 gr5 247950 915 2000 gr4 198211 876 2000 gr3 250216 848 2000 gr2 550358 2065 2005 gr1 526898 2255 3008 gr6 36341 268 753 gr5 54443 344 753 gr4 46938 330 753 gr3 48951 319 753 gr2 112150 778 755 gr1 67897 509 679 gr6 6972 78 219 gr5 8069 100 219 gr4 5423 96 219 gr3 8251 93 219 gr2 10437 164 159 gr1 8648 164 219 AOB 2940 S0003-9969(13)00003-4 10.1016/j.archoralbio.2012.12.012 Elsevier Ltd Fig. 1 Workflow for manual curation of proteins and automatic annotation. Fig. 2 Interface of the main views provided by OralCard: (a) protein view; (b) disease view; (c) organism view. Fig. 3 OralCard disease view showing the list of pathways related to the proteins identified in diabetes mellitus, type 2. The list of proteins involved in the complement and coagulation cascades is also showed. Fig. 4 OralCard disease view showing a squematic representation of the complement and coagulation cascades pathway. Fig. 5 OralCard view showing the molecular network around alpha-2-macroglobulin (minimum score=900; results per level=10; depth=2). Fig. 6 OralCard protein view showing structural information (PDB tab in left menu) for the protein alpha-1-antitrypsin (A1AT). Table 1 Summary of the human and bacterial protein identified in the oral cavity. Source Human OralOme Bacterial OralOme #Entries #Proteins %Proteins #Entries #Proteins % Proteins Organisms 1 3.523 100.0% 270 1.207 100.0% UniProt 3.523 – – 1207 – – MeSH 11 1.171 33.2% 3 3 0.2% KEGG pathway 236 231 6.6% 20 18 1.5% Gene ontology 4.621 3.346 95.0% 538 652 54.0% PDB 7.675 1.154 33.0% 122 18 1.49% PubMed 52 3.523 100.0% 5 1207 100.0% OralCard: A bioinformatic tool for the study of oral proteome Joel P. Arrais a ⁎ Nuno Rosa b José Melo a Edgar D. Coelho a Diana Amaral b Maria José Correia b Marlene Barros b c José Luís Oliveira a a Department of Electronics, Telecommunications and Informatics (DETI), Institute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro, Portugal b Department of Health Sciences, Institute of Health Sciences, The Catholic University of Portugal, Viseu, Portugal c Centre for Neurosciences and Cell Biology, University of Coimbra, Portugal ⁎ Corresponding author at: Instituto de Engenharia Electrónica e Telemática de Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal. Tel.: +351 234 370 500; fax: +351 234 370 545. Objectives The molecular complexity of the human oral cavity can only be clarified through identification of components that participate within it. However current proteomic techniques produce high volumes of information that are dispersed over several online databases. Collecting all of this data and using an integrative approach capable of identifying unknown associations is still an unsolved problem. This is the main motivation for this work. Results We present the online bioinformatic tool OralCard, which comprises results from 55 manually curated articles reflecting the oral molecular ecosystem (OralPhysiOme). It comprises experimental information available from the oral proteome both of human (OralOme) and microbial origin (MicroOralOme) structured in protein, disease and organism. Conclusions This tool is a key resource for researchers to understand the molecular foundations implicated in biology and disease mechanisms of the oral cavity. The usefulness of this tool is illustrated with the analysis of the oral proteome associated with diabetes melitus type 2. OralCard is available at Keywords Oral proteome Microbiome Diabetes melitus OralCard 1 Introduction The human oral cavity is a complex ecosystem where human, microbial and environmental factors interact in a dynamic equilibrium. Understanding the biology of the oral cavity and disorders that affect it (or systemic diseases that are reflected in it) depends on the compilation and integration of information generated by high-throughput techniques such as proteomic studies, complemented with targeted studies based on antibodies techniques. This is of particular relevance in the oral cavity since it comprises proteins of endogenous (human) and exogenous (microbiome) origin. Comprehending how these different sets of proteins relate is of the utmost importance in understanding oral biology and also the pathogenesis of oral diseases. 1 Several sub-compartments contribute to the oral protein composition, namely secretions from the major salivary glands: the parotid, submandibular (SM) and sublingual (SL) glands, making up 90% of the total salivary secretion. The remaining 10% are contributions of a collection of the minor salivary glands, the gingival crevicular fluid (GCF), the tongue and the oral mucosa. Plasma proteins can reach the oral cavity by several means, the most common being passive diffusion, ultrafiltration, and as a result of GCF outflow contributing to the oral protein composition. 2 The pool of oral protein is also enriched by molecules originating from the microbiome which colonizes the oral surfaces. These microbial metabolites are present in the oral cavity as secreted and may act on the proteins present in the salivary secretions, altering them. 3 Considering saliva as the fluid that reflects the protein composition resulting from the contribution of the above-mentioned oral sub-compartments and that it is readily accessible in a non-invasive way, it has long been identified as a diagnostic sample fluid for a swiftly growing range of disease and clinical scenarios, as well as a candidate for biomarker identification. 4,5 Over the years there have been many reviews of the genomics and proteomics of saliva. It has been highlighted that saliva could be used in oral cancer diagnosis, 6 in the diagnosis of systemic diseases, 7 in microbiome analysis, 8 in psychobiological medicine, 9 and in forensic dentistry. 10 The proteomic analysis of saliva, which was mostly conducted by 2D electrophoresis/mass spectrometry or 2D liquid chromatography/mass spectrometry, 11 showed that the salivary proteome consisted of approximately 1.000 distinct protein sequences, by the year 2007. 12 By the end of 2010 this number had more than doubled, with identification of 2290 salivary proteins. 13 With few exceptions, the microbial contribution to the oral proteome, although inferred from the presence of a well-characterized oral microbiome (e.g., HOMD d d www.homd.org/. and HMP e e ) has not been the subject of saliva or oral fluids proteomic analysis. Biology databases can be focused on particular organisms, such as Saccharomyces Genome Database (SGD) 14 for Saccharomyces, or integrative databases, for instance, the Universal Protein Resource (UniProt). 15 Although UniProt is used worldwide by the scientific community, its data is very broad, which means researchers on very specific topics will only be interested in a minuscule portion of each item in the database. In addition, online data on specific topics is very sparse, making the researcher's task burdensome and extremely time-consuming. These aspects motivate the development of databases to be used by specific scientific communities sharing the same research interests. In previous work we have compiled the OralOme, 16 a collection of specific protein-related biomolecular data collected from UniProt, Protein Data Bank (PDB), 17 HUGO Gene Nomenclature Committee (HGNC), 18 Entrez Gene, 19 Ensembl, 20 Pharmacogenomics Knowledge Database (PharmGKB), 21 BRaunschweig ENzyme Database (BRENDA), 22 Online Mendelian Inheritance in Man (OMIM), 23 Kyoto Encyclopaedia of Genes and Genomes (KEGG), 24 and Gene Ontology (GO). 25 OralOme comprises experimental data relative to 3397 non-redundant human oral proteins in healthy and diseased states (605 altered in pathological conditions and 51 present only in disease), for instance, GO terms, homologies, pathways involved, and protein structure information. In this paper we present OralCard, a Web Application that allows mining over an integrative database containing manually curated information about the oral cavity proteome with the addition of the experimentally determined oral proteome of microbial origin. OralCard promptly allows a wide range of data associations, for instance, whether a protein is involved in any specific pathological condition, which microbial proteins may be present in the oral cavity, or what the annotated functions of any given protein are and in which pathways it is implicated. Furthermore, for each protein it is possible to explore the structural and functional details. OralCard facilitates the interpretation of proteomic data of the oral cavity and will therefore be a valuable resource for researchers aiming to understand the physiologies of oral cavity in health and disease and probing for potential biomarkers for oral and systemic diseases. 2 Materials and methods The development of the OralCard database was a combination of manual and automatic processes. A thorough manual bibliography review was performed to gather current knowledge about all proteins identified in proteomic studies. The protein information obtained from data curation was automatically acquired from public biomedical databases and integrated in the OralCard database. The OralCard was conceived to allow the end-user to perform queries in a fast and intuitive way. The database and user interface were developed using technologies that are currently state-of-the-art in information systems, such as linked data, web services, service composition and interactions network visualization. These tools helped us build and tune a set of automated processes to capture information from various biological resources, simplifying the construction of OralCard data warehouse. Next, we present a general view of the entire process, from the gathering of biological information to the development of OralCard presentation layer (Fig. 1 ). 2.1 Manual data assembly and curation The assembly of the Human Oralome, i.e. its compilation and annotation, was performed mostly as described in 16 with several improvements and updates. The following PubMed query was made: (“proteomics” OR “proteomic” OR “proteome”) AND (“saliva” [Title/Abstract] OR “oral” [Title/Abstract]). The list of articles retrieved was manually inspected and the identifiers from the proteins found in the various references were collected. These studies 12,26–79 (supplemental Table 1) collected samples from different sources, i.e. parotid glands, SM, SL, minor salivary glands, GCF, tongue mucosa and oral mucosa. From the first publication of oral proteomes, many of the original identified proteins, catalogued as different entries in biological databases, have been merged with others and some were deleted due to misidentification. Therefore, all information concerning the identified proteins was manually curated and updated. The update of the IPI (International Protein Index) entries was carried out with “IPI History search” online tool. 80 All other updates have been made according to UniProt database. 81 Another addition to OralOme was the inclusion of manually curated data on the proteins, samples and techniques used. More specifically, for each protein, the following data were added to the database when available: the up/down regulation and fold change regarding normal samples; post translation modifications; and whether the protein had previously been proposed as a biomarker. Regarding the sample collection, data such as stimulation or non-stimulation of saliva and donor data (health/disease, age, sex and social habits) were also stored. In studies where there were donors with disease, manual annotation of the corresponding MeSH term and OMIM code was also performed. The OMIM code was used to query web services to retrieve specific information, such as disease name and description. The usage of MeSH terms provides a widely used vocabulary for disease annotation including conditions such as caries or gingivitis which are non Mendelian diseases and therefore not covered by the OMIM codes. Finally, all of these data are mapped to the PubMed citation where they were published. Supplementary data associated with this article can be found, in the online version, at A further improvement to the previous OralOme database was the inclusion of microbial proteins experimentally determined in proteomic studies of oral samples. 77–79 Jagtap et al. 78 have made the first deep proteomic analysis of saliva samples specifically targeting proteins of microbial origin. This study presents some methodological approaches which increased microbial protein identification by over one order of magnitude. However, almost half of the microbial proteins identified by Jagtap et al. 78 were automatically annotated from the genomic sequences of the Human Oral MicroBiome Database and were not matched to an UniProt identifier. We did not include these proteins in OralOme since there is still little information and evidence of the actual existence of these proteins. Many of the proteins originally identified in the first proteomic studies were misclassified as different entries in the literature and in the biological databases. Some of these incorrectly allocated proteins were either merged with others or deleted. Many of the original entries in protein studies had an International Protein Index (IPI) 80 identifier, which ceased functions in September of 2011. Therefore, the “IPI History Search” tool was used to update the obsolete IPI entries. From now on, the database information will be updated after the UniProt database. 2.2 Automatic data annotation A key objective of the OralCard project was to collect information on proteins present in the oral cavity and integrate that information, providing associations between proteins, diseases, pathways, gene ontologies and organisms. To achieve this goal, we carried out a first survey of online sources where this information would be available. We begin the information retrieval workflow with a protein. For this particular case, the data source used was UniProt, the most complete resource for protein sequence and functional information. UniProt identifies each protein with an accession code, which we will refer to as the UniProtKBAC (UniProt Knowledgebase Accession Code). The UniProt service provides several methods, which combined with programming tools facilitate the retrieval of essential information related to the protein, e.g., structural information, gene nomenclature data, enzyme functional data, and even the impact of genetic variation on drug response, obtained from PDB, HGNC, BRENDA and PharmGKB, respectively. Along with the UniProtKBAC, information manually annotated, as described in the previous section, was also stored. The procedures described above were used to build the OralOme database. The OralCard interface was developed to automatically retrieve and update the Oralome database with the collected information and present it in a user-friendly way. This is extremely useful as it joins in one single endpoint relevant proteomic data concerning oral proteins, which increases the time–efficiency ratio when searching for a protein, disease or microorganism and allows for systematic approaches in exploration of the oral proteome, as will be shown in the case study. Table 1 summarizes up all extracted relations. 2.3 Implementation The OralCard web interface was developed using Stripes, a web framework that facilitates the development of Java web applications. Stripes enabled full control over URLs, easing the task of accessing an entity by only knowing its id. For instance, there is direct access to the protein P22894 (neutrophil collagenase) by introducing the address OralCard is supported by a MySQL Server Edition 5.1, and access to the data is encapsulated by Hibernate, an object-relational mapping tool for Java. All the services are running under the campus core datacentre assuring fault tolerance, reliability and stability. 3 Results This section includes detailed information about the main features of OralCard, and how they are available to the end-user. OralCard is organized in three distinct views, each with a specific searching method: (1) by protein names or respective UniProt codes, (2) by disease name, OMIM code or MeSH term, (3) and by organism. 3.1 Protein view When searching for a protein name or its UniProt code, OralCard retrieves general information about the given protein (Fig. 2 a). The button in the upper right corner of the page shows some details regarding protein name, gene name and synonyms, and also UniProt, HGNC, Ensembl, Enzyme Commission, and Entrez Gene accession codes. When the information is available, it provides more specific data, found in the menu on the left: gene information (Panther), protein domain information (SMART), pharmacogenomic information (PharmaGKB), enzyme information (BRENDA), protein–protein interaction information (STRING), and structural information (PDB). OralCard also lists the diseases and bibliographic references associated with the given protein and its biological sources, either in healthy or diseased states. 3.2 Disease view The same procedure can be used to query OralCard by disease name, OMIM code or Mesh term. When the query is done, the user obtains the OMIM code for the disease and the option for listing the proteins OralCard associates with the disease. This protein list can be further explored with the tools on the left panel, which includes organism, proteins, pathways, GOs and mesh. With “Organism” the user obtains a list of the organism producing the proteins in the initial list. The link “Proteins” lists all proteins that were identified in the manual curation of the selected literature as being associated in OralOme both from human or microbial origin. Following this link the user can view which proteins are reported to be deregulated during the course of the given disease in proteomic studies, which GO terms are related to these proteins, known biomarkers and the annotated post-translation modifications (Fig. 2b). From the initial disease view the user can also use the link “Pathways” where it is possible to view the pathways in which these proteins participate and the percentage of proteins involved. Finally there are two more links available for the user: one to the GO annotation of the proteins in the list and the other to the MESH description of the disease. 3.3 Organism view Using an organism as search term returns its taxonomy information. OralCard also provides a list of the proteins reported to be present in the oral cavity that are attributed to that organism and the diseases in which these proteins are involved (Fig. 2c). These features demonstrate how the information is integrated in OralCard: the user can perform a query in any of the three views and navigate all data. 4 Discussion 4.1 Diabetes mellitus type 2 (DMT2) case study OralCard web tool is to be used in studies related to oral health, but can also be used in systemic disorders presenting altered proteins which can be detected in saliva. We chose diabetes mellitus type 2 (DMT2) as a case study to demonstrate how the information available on OralCard can be queried, understood and related to the molecular mechanisms of this particular disease. DMT2 is an example of a systemic disease of multifactorial origins, in which different signalling and metabolic pathways are compromised and which also has implications for oral health, especially the risk of periodontitis. 82 One of the most common problems in diabetic patients is impaired healing, closely related to blood coagulation, which has a huge impact on oral medical care. 83,84 Under normal conditions, the coagulation system is characterized by a constant balance between the processes of coagulation and fibrinolysis. Since OralCard stores all the information on proteins found in the oral cavity we can verify if there is molecular evidence of the fibrinolysis/coagulation imbalance in the oral proteome of these patients. This survey began with identification of the proteins present in oral samples from type II diabetic patients, using the disease search feature of the OralCard (search for diabetes and choose “Diabetes Mellitus, Type 2”). This approach led to identification of 445 proteins (Fig. 2b). For type II diabetic patients, there are quantitative data, and therefore it is possible to know which proteins have their expression level altered (down or up regulated) and even find the expression level fold change (Fig. 2b, “Regulation” column). This type of information allows the identification of proteins whose expression is more altered in the disease. Apart from information directly related to the oral proteins, OralCard also stores information related to the samples such as donor (age, gender and social habits), sampling and analysis methods used. After identifying the proteins present in saliva that are altered in DMT2 patients (Fig. 1), we check which pathways these proteins are involved in, allowing verification of the molecular mechanisms compromised in this disease. To accomplish that task, in the disease view, we must select “Pathways” from the Menu options (Fig. 3 ). From the list of pathways identified as being altered in DMT2, we can verify that the pathways with the highest number of modified proteins in DMT2 are those corresponding to metabolism 42 and coagulation/complement cascades (Fig. 3, #Proteins column). After identification of proteins in DMT2 already recognized as altered when compared to healthy donors and that these proteins are involved in the blood coagulation cascade and complement response, it is possible, in a next step, to know exactly which proteins are altered so that their specific role in the pathways can be identified. Twenty-two proteins related to coagulation/complement pathway (Fig. 3, column “Proteins”, selecting “show”) are altered in the oral cavity of DMT2 patients. OralCard can be used to obtain information about the changes in the expression of these proteins in DMT2 (Fig. 2b, “Regulation” column). It is evident that three of them alpha 1-antitrypsin (A1AT) (fold change of 3.2×), alpha-2 macroglobulin (A2MG) (2.2×) and complement component C6 (4.8×) are highly increased. The disease view can be used to show what are the molecular functions of the altered proteins. For instance, in this example we see that alpha 1-antitrypsin and alpha-2 macroglobulin both have “serine-type endopeptidase inhibitor activity” (Fig. 2b, expanding protein information “+”). Considering that blood coagulation is a series of serine protease cascade reactions, A1AT and A2MG are key points of regulation in this pathway (Fig. 4 ). The central role of these two proteins in the coagulation cascade is obvious from analysis of the molecular interaction networks of A2MG and A1AT, for they interact with several key molecules like kallikrein (KLKB1), plasminogen (PLMN) or plasminogen activator inhibitor 1 (PAI1) (Fig. 5 ), that is, proteins involved in fibrinolysis, a mechanism closely related to the regulation of blood coagulation. At this point it might seem odd that using OralCard it was possible to identify an increase in the serine protease inhibitors A1AT and A2MG involved in the regulation of fibrinolysis/coagulation, for it is known that DMT2 patients are characterized by hypercoagulable and hypofibrinolysis states. 85 These facts suggest that these two inhibitors, although in higher quantities in DMT2 patients, may not be functional. Hyperglycaemia in DMT2 causes non-enzymatic glycosylation of proteins, leading to various complications like nephropathy, retinopathy, neuropathy and angiopathy. 86 Thus, it was important to verify if the inhibitors A1AT and A2MG have sequences liable to non-enzymatic glycosylation that may affect their function. Since OralCard allows exploration of structural information relative to oral proteins, it is possible to determine the existence of non-enzymatic glycosylation sites capable of influencing the function of A1AT and A2MG. The structure of the protein complex composed of A1AT and one of the serine proteases which it inhibits (trypsin) was obtained from OralCard (in OralCard protein View for A1AT, choose PDB in left Menu, find the structure corresponding to “Crystal Structure of a Serpin:Protease Complex” and click “Download”) (Fig. 6 ) and analysed with the PyMOL software. 87 We identified several possible sites of non-enzymatic glycosylation on inhibitor amino acids near the point of attachment to the active site of the enzyme. These non-enzymatic glycosylations can prevent serpins inhibitory function, since the binding of the inhibitor to the serine enzymes involved in the coagulation cascade is prevented, allowing the hypercoagulate state characteristic of diabetes, even in the presence of elevated levels of protease inhibitors of the coagulation cascade. OralCard also stores information on proteins from microbial sources. However, experimental studies using oral samples from DMT2 patients have not been able to identify any proteins of microbial origin, which precludes any conclusions as to the role microbial proteins might have in the molecular mechanisms of oral imbalances in DMT2 patients. With DMT2 it was possible to illustrate how OralCard can be used to explore information from different sources, enabling the user, in a few clicks, to get information on the proteomic evidence of alterations in the oral cavity of diabetic patients. In this case, biological processes such as blood coagulation were known to be altered by clinical evidence/studies. OralCard, in a simple manner, allowed verification of the proteomic data available, revealing evidence of which proteins are really expressed in altered quantities and identification of the structural reasons for the lack of function of these specific proteins. This tool allowed the extraction of biological meaning from the published proteomic results by revealing the molecular evidence that can explain the impaired healing of oral tissues in diabetic patients, as well as key molecules in the process. In other cases, the researcher might not have clues as to which processes are altered in pathology, but OralCard can be used in a systematic approach to search for the proteomic evidence of altered pathways in the disease and then to analyse the proteins involved therein. The main limitation of these approaches is that most proteomic studies of oral samples in several diseases report only the presence or absence of a protein without protein level quantification. This fact makes the search for altered targets more time-consuming for there is no indication as to which of the possible altered proteins varies the most. As more quantitative proteomics studies are published, they will be included in OralCard and interpretation of proteomics data from the oral cavity will become progressively easier and more accurate. For all the reasons presented, OralCard is a key tool for the design of experimental work in quantitative proteomics. One other aspect we became aware of during manual curation of the information present in OralCard, is that there is still some variability of the proteomic data generated by the sampling methods and the proteomic techniques used. These aspects have been identified by other authors, 88,89 who reported the need for standardization in saliva and oral tissue sampling, processing and proteomic analysis. Recently, studies have been published on the rational for protocol standardization 90 and it is our expectation that these studies will result in more reproducibility in proteomic experiments performed in different laboratories. Funding This work was supported by the European Community's Seventh Framework Programme (FP7/2007–2013), under grant agreement no. 200754 (GEN2PHEN project), and from Fundação para a Ciência e Tecnologia, FCT, under grant agreement PTDC/EIA-CCO/100541/2008. Joel P. Arrais is funded by FCT grant SFRH/BPD/79044/2011. Competing interests None declared. Ethical approval Not required. Authors’ contribution All authors contributed extensively to the work presented in this paper. NR, DA and MJC were the main responsible for the bibliographical review and testing. JPA and JM contributed to the modelling and development. MB and JLO were responsible for the work supervision. All authors discussed the results and implications and contributed to the manuscript. References 1 M. Rouabhia Interactions between host and oral commensal microorganisms are key events in health and disease status Canadian Journal of Infectious Diseases 13 1 2002 47 51 [Epub 2007/12/27] 2 E. Kaufman I.B. Lamster The diagnostic applications of saliva—a review Critical Reviews in Oral Biology and Medicine 13 2 2002 197 212 [Epub 2002/07/05] 3 T. Imamura J. Travis J. Potempa The biphasic virulence activities of gingipains: activation and inactivation of host proteins Current Protein and Peptide Science 4 6 2003 443 450 [Epub 2003/12/20] 4 I.D. Mandel The diagnostic uses of saliva Journal of Oral Pathology and Medicine 19 3 1990 119 125 [Epub 1990/03/01] 5 Y.H. Lee D.T. Wong Saliva: an emerging biofluid for early detection of diseases American Journal of Dentistry 22 4 2009 241 248 [Epub 2009/10/15] 6 W.H. Westra J. Califano Toward early oral cancer detection using gene expression profiling of saliva: a thoroughfare or dead end? Clinical Cancer Research 10 24 2004 8130 8131 [Epub 2004/12/30] 7 H.P. Lawrence Salivary markers of systemic disease: noninvasive diagnosis of disease and monitoring of general health Journal (Canadian Dental Association). Journal de l Association Dentaire Canadienne 68 3 2002 170 174 [Epub 2002/03/26] 8 L.M. Smoot J.C. Smoot H. Smidt P.A. Noble M. Konneke Z.A. McMurry DNA microarrays as salivary diagnostic tools for characterizing the oral cavity's microbial community Advances in Dental Research 18 1 2005 6 11 [Epub 2005/07/07] 9 F. Chiappelli F.J. Iribarren P. Prolo Salivary biomarkers in psychobiological medicine Bioinformation 1 8 2006 331 334 [Epub 2007/06/29] 10 I. Lijnen G. Willems DNA research in forensic dentistry Methods and Findings in Experimental and Clinical Pharmacology 23 9 2001 511 517 [Epub 2002/03/06] 11 F.M. Amado R.M. Vitorino P.M. Domingues M.J. Lobo J.A. Duarte Analysis of the human saliva proteome Expert Review of Proteomics 2 4 2005 521 539 [Epub 2005/08/16] 12 S. Hu J. Wang J. Meijer S. Ieong Y. Xie T. Yu Salivary proteomic and genomic biomarkers for primary Sjogren's syndrome Arthritis and Rheumatism 56 11 2007 3588 3600 [Epub 2007/10/31] 13 J.A. Loo W. Yan P. Ramachandran D.T. Wong Comparative human salivary and plasma proteomes Journal of Dental Research 89 10 2010 1016 1023 14 J.M. Cherry E.L. Hong C. Amundsen R. Balakrishnan G. Binkley E.T. Chan Saccharomyces genome database: the genomics resource of budding yeast Nucleic Acids Research 40 database issue 2012 D700 D705 [Epub 2011/11/24] 15 T.U. Consortium Reorganizing the protein space at the Universal Protein Resource (UniProt) Nucleic Acids Research 40 D1 2012 D71 D75 16 N. Rosa M.J. Correia J.P. Arrais P. Lopes J. Melo J.L. Oliveira From the salivary proteome to the OralOme: comprehensive molecular oral biology Archives of Oral Biology 2012 [Epub 2012/01/31] 17 F.C. Bernstein T.F. Koetzle G.J. Williams E.F. Meyer Jr. M.D. Brice J.R. Rodgers The Protein Data Bank: a computer-based archival file for macromolecular structures Journal of Molecular Biology 112 3 1977 535 542 [Epub 1977/05/25] 18 R.L. Seal S.M. Gordon M.J. Lush M.W. Wright E.A. Bruford genenames.org: the HGNC resources in 2011 Nucleic Acids Research 39 database issue 2011 D514 D519 [Epub 2010/10/12] 19 D. Maglott J. Ostell K.D. Pruitt T. Tatusova Entrez Gene: gene-centered information at NCBI Nucleic Acids Research 33 database issue 2005 D54 D58 [Epub 2004/12/21] 20 P. Flicek M.R. Amode D. Barrell K. Beal S. Brent Y. Chen Ensembl 2011 Nucleic Acids Research 39 Suppl. 1 2011 D800 D806 21 E.M. McDonagh M. Whirl-Carrillo Y. Garten R.B. Altman T.E. Klein From pharmacogenomic knowledge acquisition to clinical applications: the PharmGKB as a clinical pharmacogenomic biomarker resource Biomarkers in Medicine 5 6 2011 795 806 [Epub 2011/11/23] 22 C. Sohngen A. Chang D. Schomburg Development of a classification scheme for disease-related enzyme information BMC Bioinformatics 12 2011 329 [Epub 2011/08/11] 23 A. Hamosh A.F. Scott J.S. Amberger C.A. Bocchini V.A. McKusick Online Mendelian inheritance in man (OMIM), a knowledgebase of human genes and genetic disorders Nucleic Acids Research 33 database issue 2005 D514 D517 [Epub 2004/12/21] 24 M. Kanehisa S. Goto Y. Sato M. Furumichi M. Tanabe KEGG for integration and interpretation of large-scale molecular data sets Nucleic Acids Research 40 database issue 2012 D109 D114 [Epub 2011/11/15] 25 M. Ashburner C.A. Ball J.A. Blake D. Botstein H. Butler J.M. Cherry Gene ontology: tool for the unification of biology. The Gene Ontology Consortium Nature Genetics 25 1 2000 25 29 [Epub 2000/05/10] 26 S. Bandhakavi S.K. Van Riper P.N. Tawfik M.D. Stone T. Haddad N.L. Rhodus Hexapeptide libraries for enhanced protein PTM identification and relative abundance profiling in whole human saliva Journal of Proteome Research 10 3 2011 1052 1061 [Epub 2010/12/15] 27 N. Bostanci W. Heywood K. Mills M. Parkar L. Nibali N. Donos Application of label-free absolute quantitative proteomics in human gingival crevicular fluid by LC/MS E (gingival exudatome) Journal of Proteome Research 9 5 2010 2191 2199 [Epub 2010/03/09] 28 O. Brinkmann D.A. Kastratovic M.V. Dimitrijevic V.S. Konstantinovic D.B. Jelovac J. Antic Oral squamous cell carcinoma detection by salivary biomarkers in a Serbian population Oral Oncology 47 1 2011 51 55 [Epub 2010/11/27] 29 T. Cabras E. Pisano A. Mastinu G. Denotti P.P. Pusceddu R. Inzitari Alterations of the salivary secretory peptidome profile in children affected by type 1 diabetes Molecular and Cellular Proteomics 9 10 2010 2099 2108 [Epub 2010/06/30] 30 M. Castagnola R. Inzitari C. Fanali F. Iavarone A. Vitali C. Desiderio The surprising composition of the salivary proteome of preterm human newborn Molecular and Cellular Proteomics 10 1 2011 [M110.003467. Epub 2010/10/15] 31 L.M. Chi C.W. Lee K.P. Chang S.P. Hao H.M. Lee Y. Liang Enhanced interferon signaling pathway in oral cancer revealed by quantitative proteome analysis of microdissected specimens using 16O/18O labeling and integrated two-dimensional LC-ESI-MALDI tandem MS Molecular and Cellular Proteomics 8 7 2009 1453 1474 [Epub 2009/03/20] 32 Y.J. Choi S.H. Heo J.M. Lee J.Y. Cho Identification of azurocidin as a potential periodontitis biomarker by a proteomic analysis of gingival crevicular fluid Proteome Science 9 2011 42 [Epub 2011/07/29] 33 P.P. Costa G.L. Trevisan G.O. Macedo D.B. Palioto S.L. Souza M.F. Grisi Salivary interleukin-6, matrix metalloproteinase-8, and osteoprotegerin in patients with periodontitis and diabetes Journal of Periodontology 81 3 2010 384 391 [Epub 2010/03/03] 34 P. Denny F.K. Hagen M. Hardt L. Liao W. Yan M. Arellanno The proteomes of human parotid and submandibular/sublingual gland salivas collected as the ductal secretions Journal of Proteome Research 7 5 2008 1994 2006 [Epub 2008/03/26] 35 P. Dowling R. Wormald P. Meleady M. Henry A. Curran M. Clynes Analysis of the saliva proteome from patients with head and neck squamous cell carcinoma reveals differences in abundance levels of proteins associated with tumour progression and metastasis Journal of Proteomics 71 2 2008 168 175 [Epub 2008/07/12] 36 X. Fang L. Yang W. Wang T. Song C.S. Lee D.L. DeVoe Comparison of electrokinetics-based multidimensional separations coupled with electrospray ionization–tandem mass spectrometry for characterization of human salivary proteins Analytical Chemistry 79 15 2007 5785 5792 [Epub 2007/07/07] 37 Y. Fleissig O. Deutsch E. Reichenberg M. Redlich B. Zaks A. Palmon Different proteomic protein patterns in saliva of Sjogren's syndrome patients Oral Diseases 15 1 2009 61 68 [Epub 2008/10/23] 38 L. Giusti C. Baldini L. Bazzichi F. Ciregia I. Tonazzini G. Mascia Proteome analysis of whole saliva: a new tool for rheumatic diseases—the example of Sjogren's syndrome Proteomics 7 10 2007 1634 1643 [Epub 2007/04/17] 39 L. Giusti L. Bazzichi C. Baldini F. Ciregia G. Mascia G. Giannaccini Specific proteins identified in whole saliva from patients with diffuse systemic sclerosis Journal of Rheumatology 34 10 2007 2063 2069 [Epub 2007/08/28] 40 R. Goncalves Lda M.R. Soares F.C. Nogueira C. Garcia D.R. Camisasca G. Domont Comparative proteomic analysis of whole saliva from chronic periodontitis patients Journal of Proteomics 73 7 2010 1334 1341 [Epub 2010/03/11] 41 M. Gonzalez-Begne B. Lu X. Han F.K. Hagen A.R. Hand J.E. Melvin Proteomic analysis of human parotid gland exosomes by multidimensional protein identification technology (MudPIT) Journal of Proteome Research 8 3 2009 1304 1314 [Epub 2009/02/10] 42 M. Gonzalez-Begne B. Lu L. Liao T. Xu G. Bedi J.E. Melvin Characterization of the human submandibular/sublingual saliva glycoproteome using lectin affinity chromatography coupled to multidimensional protein identification technology Journal of Proteome Research 10 11 2011 5031 5046 [Epub 2011/09/23] 43 M.M. Grant A.J. Creese G. Barr M.R. Ling A.E. Scott J.B. Matthews Proteomic analysis of a noninvasive human model of acute inflammation and its resolution: the twenty-one day gingivitis model Journal of Proteome Research 9 9 2010 4732 4744 [Epub 2010/07/29] 44 B.J. Haigh K.W. Stewart J.R. Whelan M.P. Barnett G.A. Smolenski T.T. Wheeler Alterations in the salivary proteome associated with periodontitis Journal of Clinical Periodontology 37 3 2010 241 247 [Epub 2010/02/13] 45 M. Hardt L.R. Thomas S.E. Dixon G. Newport N. Agabian A. Prakobphol Toward defining the human parotid gland salivary proteome and peptidome: identification and characterization using 2D SDS-PAGE, ultrafiltration, HPLC, and mass spectrometry Biochemistry 44 8 2005 2885 2899 [Epub 2005/02/23] 46 M. Hardt H.E. Witkowska S. Webb L.R. Thomas S.E. Dixon S.C. Hall Assessing the effects of diurnal variation on the composition of human parotid saliva: quantitative analysis of native peptides using iTRAQ reagents Analytical Chemistry 77 15 2005 4947 4954 [Epub 2005/08/02] 47 Q.Y. He J. Chen H.F. Kung A.P. Yuen J.F. Chiu Identification of tumor-associated proteins in oral tongue squamous cell carcinoma by proteomics Proteomics 4 1 2004 271 278 [Epub 2004/01/20] 48 T.O. Hjelmervik R. Jonsson A.I. Bolstad The minor salivary gland proteome in Sjogren's syndrome Oral Diseases 15 5 2009 342 353 [Epub 2009/04/15] 49 S. Hu M. Arellano P. Boontheung J. Wang H. Zhou J. Jiang Salivary proteomics for oral cancer biomarker discovery Clinical Cancer Research 14 19 2008 6246 6252 [Epub 2008/10/03] 50 S. Hu Y. Xie P. Ramachandran R.R. Ogorzalek Loo Y. Li J.A. Loo Large-scale identification of proteins in human salivary proteome by liquid chromatography/mass spectrometry and two-dimensional gel electrophoresis–mass spectrometry Proteomics 5 6 2005 1714 1728 [Epub 2005/04/01] 51 C.M. Huang Comparative proteomic analysis of human whole saliva Archives of Oral Biology 49 12 2004 951 962 [Epub 2004/10/16] 52 K. Ito S. Funayama Y. Hitomi S. Nomura K. Katsura M. Saito Proteome analysis of gelatin-bound salivary proteins in patients with primary Sjogren's syndrome: identification of matrix metalloproteinase-9 Clinica Chimica Acta 403 1–2 2009 269 271 [Epub 2009/03/24] 53 Y.J. Jou C.D. Lin C.H. Lai C.H. Chen J.Y. Kao S.Y. Chen Proteomic identification of salivary transferrin as a biomarker for early detection of oral cancer Analytica Chimica Acta 681 1–2 2010 41 48 [Epub 2010/11/03] 54 M.R. Larsen S.S. Jensen L.A. Jakobsen N.H. Heegaard Exploring the sialiome using titanium dioxide chromatography and mass spectrometry Molecular and Cellular Proteomics 6 10 2007 1778 1787 [Epub 2007/07/12] 55 J.M. Lawler H.B. Kwak J.H. Kim M.H. Suk Exercise training inducibility of MnSOD protein expression and activity is retained while reducing prooxidant signaling in the heart of senescent rats American Journal of Physiology. Regulatory, Integrative and Comparative Physiology 296 5 2009 R1496 R1502 [Epub 2009/03/20] 56 W.Y. Lo M.H. Tsai Y. Tsai C.H. Hua F.J. Tsai S.Y. Huang Identification of over-expressed proteins in oral squamous cell carcinoma (OSCC) patients by clinical proteomic analysis Clinica Chimica Acta 376 1–2 2007 101 107 [Epub 2006/08/08] 57 I. Messana T. Cabras R. Inzitari A. Lupi C. Zuppi C. Olmi Characterization of the human salivary basic proline-rich protein complex by a proteomic approach Journal of Proteome Research 3 4 2004 792 800 [Epub 2004/09/14] 58 A. Negishi M. Masuda M. Ono K. Honda M. Shitashige R. Satow Quantitative proteomics using formalin-fixed paraffin-embedded tissues of oral squamous cell carcinoma Cancer Science 100 9 2009 1605 1611 [Epub 2009/06/16] 59 D. Preza B. Thiede I. Olsen B. Grinde The proteome of the human parotid gland secretion in elderly with and without root caries Acta Odontologica Scandinavica 67 3 2009 161 169 [Epub 2009/03/03] 60 M. Quintana O. Palicki G. Lucchi P. Ducoroy C. Chambon C. Salles Inter-individual variability of protein patterns in saliva of healthy adults Journal of Proteomics 72 5 2009 822 830 [Epub 2009/05/28] 61 P. Ramachandran P. Boontheung E. Pang W. Yan D.T. Wong J.A. Loo Comparison of N-linked glycoproteins in human whole saliva, parotid, submandibular, and sublingual glandular secretions identified using hydrazide chemistry and mass spectrometry Clinical Proteomics 4 3–4 2008 80 104 [Epub 2008/12/01] 62 P. Ramachandran P. Boontheung Y. Xie M. Sondej D.T. Wong J.A. Loo Identification of N-linked glycoproteins in human saliva by glycoprotein capture and mass spectrometry Journal of Proteome Research 5 6 2006 1493 1503 [Epub 2006/06/03] 63 P.V. Rao A.P. Reddy X. Lu S. Dasari A. Krishnaprasad E. Biggs Proteomic identification of salivary biomarkers of type-2 diabetes Journal of Proteome Research 8 1 2009 239 245 [Epub 2009/01/03] 64 O.H. Ryu J.C. Atkinson G.T. Hoehn G.G. Illei T.C. Hart Identification of parotid salivary biomarkers in Sjogren's syndrome by surface-enhanced laser desorption/ionization time-of-flight mass spectrometry and two-dimensional difference gel electrophoresis Rheumatology 45 9 2006 1077 1086 [Epub 2006/03/09] 65 W.L. Siqueira E. Salih D.L. Wan E.J. Helmerhorst F.G. Oppenheim Proteome of human minor salivary gland secretion Journal of Dental Research 87 5 2008 445 450 [Epub 2008/04/25] 66 M.D. Stone X. Chen T. McGowan S. Bandhakavi B. Cheng N.L. Rhodus Large-scale phosphoproteomics analysis of whole saliva reveals a distinct phosphorylation pattern Journal of Proteome Research 10 4 2011 1728 1736 [Epub 2011/02/09] 67 C.F. Streckfus K.A. Storthz L. Bigler W.P. Dubinsky A comparison of the proteomic expression in pooled saliva specimens from individuals diagnosed with ductal carcinoma of the breast with and without lymph node involvement Journla of Oncology 2009 2009 737619 [Epub 2010/01/07] 68 D. Turhani K. Krapfenbauer D. Thurnher H. Langen M. Fountoulakis Identification of differentially expressed, tumor-associated proteins in oral squamous cell carcinoma by proteomic analysis Electrophoresis 27 7 2006 1417 1423 [Epub 2006/03/29] 69 R. Vitorino M.J. Lobo J.A. Duarte A.J. Ferrer-Correia P.M. Domingues F.M. Amado Analysis of salivary peptides using HPLC-electrospray mass spectrometry Biomedical Chromatography 18 8 2004 570 575 [Epub 2004/09/24] 70 A. Walz K. Stuhler A. Wattenberg E. Hawranke H.E. Meyer G. Schmalz Proteome analysis of glandular parotid and submandibular–sublingual saliva in comparison to whole human saliva by two-dimensional gel electrophoresis Proteomics 6 5 2006 1631 1639 [Epub 2006/01/13] 71 P.A. Wilmarth M.A. Riviere D.L. Rustvold J.D. Lauten T.E. Madden L.L. David Two-dimensional liquid chromatography study of the human whole saliva proteome Journal of Proteome Research 3 5 2004 1017 1023 [Epub 2004/10/12] 72 Y. Wu R. Shu H. Liu Comparison of proteomic profiles of whole unstimulated saliva obtained from generalized aggressive periodontitis patients and healthy controls Hua Xi Kou Qiang Yi Xue Za Zhi=Huaxi Kouqiang Yixue Zazhi=West China Journal of Stomatology 29 5 2011 519 2125 [Epub 2011/12/15] 73 Y. Wu R. Shu L.J. Luo L.H. Ge Y.F. Xie Initial comparison of proteomic profiles of whole unstimulated saliva obtained from generalized aggressive periodontitis patients and healthy control subjects Journal of Periodontal Research 44 5 2009 636 644 [Epub 2009/05/21] 74 H. Xie N.L. Rhodus R.J. Griffin J.V. Carlis T.J. Griffin A catalogue of human saliva proteins identified by free flow electrophoresis-based peptide separation and tandem mass spectrometry Molecular and Cellular Proteomics 4 11 2005 1826 1830 [Epub 2005/08/17] 75 W. Yan R. Apweiler B.M. Balgley P. Boontheung J.L. Bundy B.J. Cargile Systematic comparison of the human saliva and plasma proteomes Proteomics: Clinical Applications 3 1 2009 116 134 [Epub 2009/11/10] 76 L.L. Yang X.Q. Liu W. Liu B. Cheng M.T. Li Comparative analysis of whole saliva proteomes for the screening of biomarkers for oral lichen planus Inflammation Research 55 10 2006 405 407 [Epub 2006/07/20] 77 D. Esser G. Alvarez-Llamas M.P. de Vries D. Weening R.J. Vonk H. Roelofsen Sample stability and protein composition of saliva: implications for its use as a diagnostic fluid Biomarker Insights 3 2008 25 27 [Epub 2008/01/01] 78 P. Jagtap T. McGowan S. Bandhakavi Z.J. Tu S. Seymour T.J. Griffin Deep metaproteomic analysis of human salivary supernatant Proteomics 12 7 2012 992 1001 [Epub 2012/04/24] 79 H. Xie G. Onsongo J. Popko E.P. de Jong J. Cao J.V. Carlis Proteomics analysis of cells in whole saliva from oral cancer patients via value-added three-dimensional peptide fractionation and tandem mass spectrometry Molecular and Cellular Proteomics 7 3 2008 486 498 [Epub 2007/11/30] 80 P.J. Kersey J. Duarte A. Williams Y. Karavidopoulou E. Birney R. Apweiler The International Protein Index: an integrated database for proteomics experiments Proteomics 4 7 2004 1985 1988 [Epub 2004/06/29] 81 Reorganizing the protein space at the Universal Protein Resource (UniProt) Nucleic Acids Research 40 database issue 2012 D71 D75 [Epub 2011/11/22] 82 F. Lakschevitz G. Aboodi H. Tenenbaum M. Glogauer Diabetes and periodontal diseases: interplay and links Current Diabetes Reviews 7 6 2011 433 439 [Epub 2011/11/19] 83 Y. Abiko D. Selimovic The mechanism of protracted wound healing on oral mucosa in diabetes. Review Bosnian Journal of Basic Medical Sciences 10 3 2010 186 191 [Epub 2010/09/18] 84 I.B. Lamster E. Lalla W.S. Borgnakke G.W. Taylor The relationship between oral health and diabetes mellitus Journal of the American Dental Association 139 Suppl. 2008 19S 24S [Epub 2008/11/01] 85 A. Mafrici R. Proietti Aterotrombosi e diabete mellito di tipo 2: analisi dei principali meccanismi fisiopatogenetici Atherothrombosis in patients with type 2 diabetes mellitus: an overview of pathophysiology Giornale Italiano di Cardiologia 11 6 2010 467 477 [Epub 2010/10/07] 86 R.B. Nawale V.K. Mourya S.B. Bhise Non-enzymatic glycation of proteins: a cause for complications in diabetes Indian Journal of Biochemistry and Biophysics 43 6 2006 337 344 [Epub 2007/02/09] 87 Schrodinger, LLC. The PyMOL molecular graphics system, version 1.3r1, 2010. 88 S.K. Al-Tarawneh M.B. Border C.F. Dibble S. Bencharit Defining salivary biomarkers using mass spectrometry-based proteomics: a systematic review Omics: A Journal of Integrative Biology 15 6 2011 353 361 [Epub 2011/05/17] 89 M. Castagnola T. Cabras F. Iavarone C. Fanali S. Nemolato G. Peluso The human salivary proteome: a critical overview of the results obtained by different proteomic platforms Expert Review of Proteomics 9 1 2012 33 46 [Epub 2012/02/02] 90 H. Xiao D.T. Wong Method development for proteome stabilization in human saliva Analytica Chimica Acta 722 2012 63 69 [Epub 2012/03/27] "
    },
    {
        "doc_title": "From protein-protein interactions to rational drug design: Are computational methods up to the challenge?",
        "doc_scopus_id": "84878589163",
        "doc_doi": "10.2174/1568026611313050005",
        "doc_eid": "2-s2.0-84878589163",
        "doc_date": "2013-06-10",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Drug Discovery",
                "area_abbreviation": "PHAR",
                "area_code": "3002"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The study of protein-protein interactions (PPIs) has been growing for some years now, mainly as a result of easy access to high-throughput experimental data. Several computational approaches have been presented throughout the years as means to infer PPIs not only within the same species, but also between different species (e.g., host-pathogen interactions). The importance of unveiling the human protein interaction network is undeniable, particularly in the biological, biomedical and pharmacological research areas. Even though protein interaction networks evolve over time and can suffer spontaneous alterations, occasional shifts are often associated with disease conditions. These disorders may be caused by external pathogens, such as bacteria and viruses, or by intrinsic factors, such as auto-immune disorders and neurological impairment. Therefore, having the knowledge of how proteins interact with each other will provide a great opportunity to understand pathogenesis mechanisms, and subsequently support the development of drugs focused on very specific disease pathways and re-targeting already commercialized drugs to new gene products. Computational methods for PPI prediction have been highlighted as an interesting option for interactome mapping. In this paper we review the techniques and strategies used for both experimental identification and computational inference of PPIs. We will then discuss how this knowledge can be used to create protein interaction networks (PINs) and the various methodologies applied to characterize and predict the so-called \"disease genes\" and \"disease networks\". This will be followed by an overview of the strategies employed to predict drug targets. © 2013 Bentham Science Publishers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The EU-ADR Web Platform: Delivering advanced pharmacovigilance tools",
        "doc_scopus_id": "84877591332",
        "doc_doi": "10.1002/pds.3375",
        "doc_eid": "2-s2.0-84877591332",
        "doc_date": "2013-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Epidemiology",
                "area_abbreviation": "MEDI",
                "area_code": "2713"
            },
            {
                "area_name": "Pharmacology (medical)",
                "area_abbreviation": "MEDI",
                "area_code": "2736"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Purpose: Pharmacovigilance methods have advanced greatly during the last decades, making post-market drug assessment an essential drug evaluation component. These methods mainly rely on the use of spontaneous reporting systems and health information databases to collect expertise from huge amounts of real-world reports. The EU-ADR Web Platform was built to further facilitate accessing, monitoring and exploring these data, enabling an in-depth analysis of adverse drug reactions risks. Methods: The EU-ADR Web Platform exploits the wealth of data collected within a large-scale European initiative, the EU-ADR project. Millions of electronic health records, provided by national health agencies, are mined for specific drug events, which are correlated with literature, protein and pathway data, resulting in a rich drug-event dataset. Next, advanced distributed computing methods are tailored to coordinate the execution of data-mining and statistical analysis tasks. This permits obtaining a ranked drug-event list, removing spurious entries and highlighting relationships with high risk potential. Results: The EU-ADR Web Platform is an open workspace for the integrated analysis of pharmacovigilance datasets. Using this software, researchers can access a variety of tools provided by distinct partners in a single centralized environment. Besides performing standalone drug-event assessments, they can also control the pipeline for an improved batch analysis of custom datasets. Drug-event pairs can be substantiated and statistically analysed within the platform's innovative working environment. Conclusions: A pioneering workspace that helps in explaining the biological path of adverse drug reactions was developed within the EU-ADR project consortium. This tool, targeted at the pharmacovigilance community, is available online at https://bioinformatics.ua.pt/euadr/. © 2013 John Wiley & Sons, Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gimli: Open source and high-performance biomedical name recognition",
        "doc_scopus_id": "84873744177",
        "doc_doi": "10.1186/1471-2105-14-54",
        "doc_eid": "2-s2.0-84873744177",
        "doc_date": "2013-02-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Automatic recognition",
            "Biomedical information extractions",
            "Configuration files",
            "Name recognition",
            "Named-entity recognition",
            "Optimized models",
            "System characteristics",
            "Underlying systems"
        ],
        "doc_abstract": "Background: Automatic recognition of biomedical names is an essential task in biomedical information extraction, presenting several complex and unsolved challenges. In recent years, various solutions have been implemented to tackle this problem. However, limitations regarding system characteristics, customization and usability still hinder their wider application outside text mining research.Results: We present Gimli, an open-source, state-of-the-art tool for automatic recognition of biomedical names. Gimli includes an extended set of implemented and user-selectable features, such as orthographic, morphological, linguistic-based, conjunctions and dictionary-based. A simple and fast method to combine different trained models is also provided. Gimli achieves an F-measure of 87.17% on GENETAG and 72.23% on JNLPBA corpus, significantly outperforming existing open-source solutions.Conclusions: Gimli is an off-the-shelf, ready to use tool for named-entity recognition, providing trained and optimized models for recognition of biomedical entities from scientific text. It can be used as a command line tool, offering full functionality, including training of new models and customization of the feature set and model parameters through a configuration file. Advanced users can integrate Gimli in their text mining workflows through the provided library, and extend or adapt its functionalities. Based on the underlying system characteristics and functionality, both for final users and developers, and on the reported performance results, we believe that Gimli is a state-of-the-art solution for biomedical NER, contributing to faster and better research in the field. Gimli is freely available at http://bioinformatics.ua.pt/gimli. © 2013 Campos et al.; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Semantic rule processing for real-time data integration",
        "doc_scopus_id": "84908334479",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84908334479",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Application frameworks",
            "Data validation",
            "Distributed environments",
            "Dynamic environments",
            "Post-integration",
            "Real- time",
            "Service interoperability",
            "Service platforms"
        ],
        "doc_abstract": "Integration-as-a-service platforms arise as a modern strategy to integrate data from distributed environments. Nowadays, service interoperability strategies, such as workflows and static service-oriented architectures, are giving place to more dynamic environments, where the path from the original resource to the integrative destination is triggered autonomously and in real-time. However, these concepts still have not been applied to bioinformatics, in great part due to the complexity underlying the data validation and transformation tasks. In this manuscript we introduce a component to enhance these activities by enabling the execution of complex pre- And post-integration semantic algorithms. By leveraging on comprehensive Semantic Web constructs, these activities are better suited to the life sciences domain.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring nanopublishing with COEUS",
        "doc_scopus_id": "84908326384",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84908326384",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Application frameworks",
            "Nanopublication",
            "Nanopublications",
            "Scientific community",
            "Scientific publications",
            "Semantic knowledge",
            "Semantic web applications",
            "Smallest unit"
        ],
        "doc_abstract": "A nanopublication represents the smallest unit of publishable information. This schema enhances attribution and ownership of specific data elements. With these guidelines for relating atomic data with its authors, accessing and exchanging knowledge becomes a more streamlined process. Nanopublications are particularly relevant in the scientific domain, where scientific publication, validation and ownership of data are essential.The COEUS semantic web application framework delivers, in a single package, all the tools required to rapidly build a new semantic knowledge base from scratch, including multiple data integration algorithms and interoperability services.This work introduces the combination of COEUS' integration and interoperability features with the nanopublications standard. This results in a unique nanopublishing pipeline, where collections of annotated data can be modeled and integrated, stored in a semantic knowledge base, and published through COEUS API. These improvements to the COEUS framework greatly benefit the scientific community, where creating and publishing nanopublications is still cumbersome.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating echocardiogram reports with medical imaging",
        "doc_scopus_id": "84897085332",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84897085332",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Clinical practices",
            "Clinical services",
            "Computational system",
            "Healthcare institutions",
            "Heterogeneous database",
            "Index information",
            "New approaches",
            "Ultrasound medical images"
        ],
        "doc_abstract": "Healthcare institutions are increasingly taking advantage of information and computational systems to enhance the efficiency and quality of their services. These IT systems are normally able to handle huge amounts of digital data, and to extract relevant fingerprints useful to improve the quality of clinical practice. However, building automatized processes to achieve this over multiple and heterogeneous databases is still a challenge. This paper presents a new approach able to collect and index information from distinct medical data sources, allowing us to identify important metrics to evaluate the performance and quality of clinical services. A case study combining information from ultrasound medical images and echocardiogram clinical reports is also presented. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A multi-domain platform for medical imaging",
        "doc_scopus_id": "84897064805",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84897064805",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Cloud services",
            "Data repositories",
            "Health informations",
            "Imaging protocol",
            "Indexing engines",
            "Medical imaging equipment",
            "Peer-to-peer paradigm",
            "Security constraint"
        ],
        "doc_abstract": "The increasing adoption of medical imaging equipment in healthcare has been leading to a huge dispersion of data repositories and institutions. Although the quality of diagnostic and treatment is deeply dependent on the health information that is available for physicians, several legal and technological issues have hindered the integration of these data. One of such problems is because traditional medical imaging protocols do not perform well in inter-institutional scenarios. This paper describes a hybrid network platform for medical imaging systems that provides searching and retrieval over multiple centres. Three key components support the system: an indexing engine, a multicast framework and a cloud service. Using a peer-to-peer paradigm with security constraints, the platform gathers the information of medical imaging repositories hosted inside the institutions, allowing physicians to access data when and where they need it. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enabling outsourcing XDS for imaging on the public cloud",
        "doc_scopus_id": "84894384235",
        "doc_doi": "10.3233/978-1-61499-289-9-33",
        "doc_eid": "2-s2.0-84894384235",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Data confidentiality",
            "Document sharing",
            "Legal concern",
            "Medical information",
            "New approaches",
            "Patient privacies",
            "Picture archiving and communication systems (PACS)",
            "Public clouds",
            "Computer Security",
            "Database Management Systems",
            "Information Dissemination",
            "Information Storage and Retrieval",
            "Internet",
            "Medical Record Linkage",
            "Portugal",
            "Radiology Information Systems"
        ],
        "doc_abstract": "Picture Archiving and Communication System (PACS) has been the main paradigm in supporting medical imaging workflows during the last decades. Despite its consolidation, the appearance of Cross-Enterprise Document Sharing for imaging (XDS-I), within IHE initiative, constitutes a great opportunity to readapt PACS workflow for inter-institutional data exchange. XDS-I provides a centralized discovery of medical imaging and associated reports. However, the centralized XDS-I actors (document registry and repository) must be deployed in a trustworthy node in order to safeguard patient privacy, data confidentiality and integrity. This paper presents XDS for Protected Imaging (XDS-p), a new approach to XDS-I that is capable of being outsourced (e.g. Cloud Computing) while maintaining privacy, confidentiality, integrity and legal concerns about patients' medical information. © 2013 IMIA and IOS Press.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Structuring and exploring the biomedical literature using latent semantics",
        "doc_scopus_id": "84891135595",
        "doc_doi": "10.1007/978-3-319-00551-5_72",
        "doc_eid": "2-s2.0-84891135595",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical fields",
            "Biomedical literature",
            "Knowledge exploration",
            "Latent Semantic Analysis",
            "Latent semantics",
            "Literature search",
            "Visualization technique",
            "Wealth of information"
        ],
        "doc_abstract": "The fast increasing amount of articles published in the biomedical field is creating difficulties in the way this wealth of information can be efficiently exploited by researchers. As a way of overcoming these limitations and potentiating a more efficient use of the literature, we propose an approach for structuring the results of a literature search based on the latent semantic information extracted from a corpus. Moreover, we show how the results of the Latent Semantic Analysis method can be adapted so as to evidence differences between results of different searches. We also propose different visualization techniques that can be applied to explore these results. Used in combination, these techniques could empower users with tools for literature guided knowledge exploration and discovery. © Springer International Publishing Switzerland 2013.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Data Integration Solution for Organ-Specific Studies: An Application for Oral Biology",
        "doc_scopus_id": "84886261384",
        "doc_doi": "10.1007/978-3-642-38256-7_27",
        "doc_eid": "2-s2.0-84886261384",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Biological information",
            "Biomedical science",
            "Complex ecosystems",
            "Molecular levels",
            "Multiple interactions",
            "Oral cavity",
            "Oral healths",
            "Proteomic studies"
        ],
        "doc_abstract": "The human oral cavity is a complex ecosystem where multiple interactions occur and whose comprehension is critical in understanding several disease mechanisms. In order to comprehend the composition of the oral cavity at a molecular level, it is necessary to compile and integrate the biological information resulting from specific techniques, especially from proteomic studies of saliva. The objective of this work was to compile and curate a specific group of proteins related to the oral cavity, providing a tool to conduct further studies of the salivary proteome. In this paper we present a platform that integrates in a single endpoint all available information for proteins associated with the oral cavity. The proposed tool allows researchers in biomedical sciences to explore microorganisms, proteins and diseases, constituting a unique tool to analyse meaningful interactions for oral health. © Springer-Verlag Berlin Heidelberg 2013.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Analysing Relevant Diseases from Iberian Tweets",
        "doc_scopus_id": "84880375780",
        "doc_doi": "10.1007/978-3-319-00578-2_10",
        "doc_eid": "2-s2.0-84880375780",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Binary classification",
            "Eating disorders",
            "Feasible solution",
            "Health condition",
            "Information sources",
            "Micro-blogging services",
            "Personal health informations",
            "Social media"
        ],
        "doc_abstract": "The Internet constitutes a huge source of information that can be exploited by individuals in many different ways. With the increasing use of social networks and blogs, the Internet is now used not only as an information source but also to disseminate personal health information. In this paper we exploit the wealth of user-generated data, available through the micro-blogging service Twitter, to estimate and track the incidence of health conditions in society, specifically in Portugal and Spain. We present results for the acquisition of relevant tweets for a set of four different conditions (flu, depression, pregnancy and eating disorders) and for the binary classification of these tweets as relevant or not for each case. The results obtained, ranging in AUC from 0.7 to 0.87, are very promising and indicate that such approach provides a feasible solution for measuring and tracking the evolution of many health related aspects within the society. © Springer International Publishing Switzerland 2013.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantitative Characterization of Protein Networks of the Oral Cavity",
        "doc_scopus_id": "84880338730",
        "doc_doi": "10.1007/978-3-319-00578-2_9",
        "doc_eid": "2-s2.0-84880338730",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Confidence score",
            "Functional relationship",
            "Prediction methods",
            "Protein interaction",
            "Protein network",
            "Protein-protein interactions",
            "Quantitative characterization",
            "Topological properties"
        ],
        "doc_abstract": "Modeling protein interactions as complex networks allow applying graph theory to help understanding their topology, to validate previous evidences and to uncover new biological associations. Topological properties have been recognized by their contribution for the understanding of the structures, functional relationships and evolution of complex networks, helping in a better comprehension of the diseases mechanisms and in the identification of drug targets. The human interactome, i.e. the network formed by all protein-protein interactions, is a complex and yet unknown system. In this paper we present the results of a study about the topological properties of the oral protein network. We evaluate several confidence scores and prediction methods, in order to compare these networks with random organizations with the same size. © Springer International Publishing Switzerland 2013.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DICOM relay over the cloud",
        "doc_scopus_id": "84878019584",
        "doc_doi": "10.1007/s11548-012-0785-3",
        "doc_eid": "2-s2.0-84878019584",
        "doc_date": "2013-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Purpose: Healthcare institutions worldwide have adopted picture archiving and communication system (PACS) for enterprise access to images, relying on Digital Imaging Communication in Medicine (DICOM) standards for data exchange. However, communication over a wider domain of independent medical institutions is not well standardized. A DICOM-compliant bridge was developed for extending and sharing DICOM services across healthcare institutions without requiring complex network setups or dedicated communication channels. Methods: A set of DICOM routers interconnected through a public cloud infrastructure was implemented to support medical image exchange among institutions. Despite the advantages of cloud computing, new challenges were encountered regarding data privacy, particularly when medical data are transmitted over different domains. To address this issue, a solution was introduced by creating a ciphered data channel between the entities sharing DICOM services. Results: Two main DICOM services were implemented in the bridge: Storage and Query/Retrieve. The performance measures demonstrated it is quite simple to exchange information and processes between several institutions. The solution can be integrated with any currently installed PACS-DICOM infrastructure. This method works transparently with well-known cloud service providers. Conclusions: Cloud computing was introduced to augment enterprise PACS by providing standard medical imaging services across different institutions, offering communication privacy and enabling creation of wider PACS scenarios with suitable technical solutions. © 2012 CARS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "MRNA secondary structure optimization using a correlated stem-loop prediction",
        "doc_scopus_id": "84876044227",
        "doc_doi": "10.1093/nar/gks1473",
        "doc_eid": "2-s2.0-84876044227",
        "doc_date": "2013-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Secondary structure of messenger RNA plays an important role in the bio-synthesis of proteins. Its negative impact on translation can reduce the yield of protein by slowing or blocking the initiation and movement of ribosomes along the mRNA, becoming a major factor in the regulation of gene expression. Several algorithms can predict the formation of secondary structures by calculating the minimum free energy of RNA sequences, or perform the inverse process of obtaining an RNA sequence for a given structure. However, there is still no approach to redesign an mRNA to achieve minimal secondary structure without affecting the amino acid sequence. Here we present the first strategy to optimize mRNA secondary structures, to increase (or decrease) the minimum free energy of a nucleotide sequence, without changing its resulting polypeptide, in a time-efficient manner, through a simplistic approximation to hairpin formation. Our data show that this approach can efficiently increase the minimum free energy by >40%, strongly reducing the strength of secondary structures. Applications of this technique range from multi-objective optimization of genes by controlling minimum free energy together with CAI and other gene expression variables, to optimization of secondary structures at the genomic level. © The Author(s) 2013. Published by Oxford University Press.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "COEUS: \"semantic web in a box\" for biomedical applications",
        "doc_scopus_id": "84904751123",
        "doc_doi": "10.1186/2041-1480-3-11",
        "doc_eid": "2-s2.0-84904751123",
        "doc_date": "2012-12-17",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2012 Lopes and Oliveira; licensee BioMed Central Ltd.Background: As the \"omics\" revolution unfolds, the growth in data quantity and diversity is bringing about the need for pioneering bioinformatics software, capable of significantly improving the research workflow. To cope with these computer science demands, biomedical software engineers are adopting emerging semantic web technologies that better suit the life sciences domain. The latter's complex relationships are easily mapped into semantic web graphs, enabling a superior understanding of collected knowledge. Despite increased awareness of semantic web technologies in bioinformatics, their use is still limited. Results: COEUS is a new semantic web framework, aiming at a streamlined application development cycle and following a \"semantic web in a box\" approach. The framework provides a single package including advanced data integration and triplification tools, base ontologies, a web-oriented engine and a flexible exploration API. Resources can be integrated from heterogeneous sources, including CSV and XML files or SQL and SPARQL query results, and mapped directly to one or more ontologies. Advanced interoperability features include REST services, a SPARQL endpoint and LinkedData publication. These enable the creation of multiple applications for web, desktop or mobile environments, and empower a new knowledge federation layer. Conclusions: The platform, targeted at biomedical application developers, provides a complete skeleton ready for rapid application deployment, enhancing the creation of new semantic information systems. COEUS is available as open source at http://bioinformatics.ua.pt/coeus/.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Clustering of distinct PACS archives using a cooperative peer-to-peer network",
        "doc_scopus_id": "84868521138",
        "doc_doi": "10.1016/j.cmpb.2012.05.013",
        "doc_eid": "2-s2.0-84868521138",
        "doc_date": "2012-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clinical environments",
            "Data loss",
            "DICOM",
            "Distributed data",
            "Exchange rates",
            "Health care professionals",
            "High availability",
            "Horizontal scaling",
            "Image fragments",
            "Medical images",
            "PACS",
            "PACS archive",
            "Peer to peer",
            "Scale-out",
            "Transfer rates"
        ],
        "doc_abstract": "To face the demanding requirements of the clinical environment, PACS archives need to be resilient and reliable, supporting high availability and fault tolerance. Often, to ensure no data loss, PACS archives retain two copies of images on separate physical machines, using distributed data storage facilities. However, PACS do not take advantage of the various replicas to improve the transfer rates of medical images. This happens mostly because the DICOM standard does not comply with distributed fetching of image fragments while performing a store. Inspired by this unexplored opportunity, we designed and implemented a new solution that takes advantage of the distributed image replicas and, at the same time, respects the DICOM standard. Our strategy brought significant improvements in the exchange rates, load balancing and availability of installed PACS archives. Moreover, the adopted strategy forms a cluster of PACS archives that transparently enables horizontal scaling, facilitates the creation of backups, and gives to healthcare professionals a unified view of the distributed repositories. © 2012 Elsevier Ireland Ltd.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2012-06-15 2012-06-15 2014-09-30T23:42:41 S0169-2607(12)00152-6 S0169260712001526 10.1016/j.cmpb.2012.05.013 S300 S300.3 FULL-TEXT 2015-05-14T05:25:39.534626-04:00 0 0 20121201 20121231 2012 2012-06-15T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor primabst ref 0169-2607 01692607 false 108 108 3 3 Volume 108, Issue 3 13 1002 1011 1002 1011 201212 December 2012 2012-12-01 2012-12-31 2012 Section I: Methodology article fla Copyright © 2012 Elsevier Ireland Ltd. All rights reserved. CLUSTERINGDISTINCTPACSARCHIVESUSINGACOOPERATIVEPEERTOPEERNETWORK RIBEIRO L 1 Introduction 2 Background 3 Materials 3.1 DICOM standard 3.2 JavaGroups 3.3 Dicoogle Indexing System 4 Methods 4.1 Architecture 4.2 Stateless and stateful active peers 4.3 Peer-to-peer DICOM-based network 4.3.1 Distributed c-find 4.3.2 Distributed c-store 4.3.3 Distributed c-move 5 Results and discussion 5.1 Availability and load balancing 5.2 Scaling and cost saving 5.3 Unifying view 5.4 Performance 6 Conclusions Acknowledgement References PIANYKH 2008 O DIGITALIMAGINGCOMMUNICATIONSINMEDICINEAPRACTICALINTRODUCTIONSURVIVALGUIDE HUANG 2010 H PACSIMAGINGINFORMATICSBASICPRINCIPLESAPPLICATIONS LANGER 2009 48 52 S KOMMERI 2011 40 50 J BOND 2011 1 10 R LIU 2003 165 174 B MICHAEL 2007 1 9 M IEEEINTERNATIONALPARALLELDISTRIBUTEDPROCESSINGSYMPOSIUM SCALEUPSCALEOUTACASESTUDYUSINGNUTCHLUCENE QIU 2004 367 378 D ZHANG 2011 2064 2067 B INTERNATIONALCONFERENCECOMPUTERSCIENCESERVICESYSTEMCSSS OPTIMIZATIONMODELLOADBALANCINGINPEERPEERP2PNETWORK CAO 2010 63 70 Q INTERNATIONALCONFERENCEP2PPARALLELGRIDCLOUDINTERNETCOMPUTING3PGCIC LOADBALANCINGSCHEMESFORAHIERARCHICALPEERTOPEERFILESEARCHSYSTEM HOWARD 1988 23 26 J WINTER1988USENIXCONFERENCEPROCEEDINGS OVERVIEWANDREWFILESYSTEM TREICHEL 2010 1 9 T ASSOCIATION 2007 N DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART4SERVICECLASSSPECIFICATIONS OOSTERWIJK 2005 H DICOMBASICS TREICHEL 2011 1 7 T MUSTRA 2008 21 29 M ELMAR200850THINTERNATIONALSYMPOSIUMVOL1 OVERVIEWDICOMSTANDARD WARNOCK 2007 125 129 M WARNOCK 2007 125 129 M ASSOCIATION 2003 N DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART3INFORMATIONOBJECTDEFINITIONS COSTA 2009 71 77 C INTERNATIONALJOURNALCOMPUTERASSISTEDRADIOLOGYSURGERYVOL4 INDEXINGRETRIEVINGDICOMDATAINDISPERSEUNSTRUCTUREDARCHIVES COSTA 2010 1 9 C BLANQUER 2009 16 24 I SCHANTZ 2001 1 9 R RIBEIROX2012X1002 RIBEIROX2012X1002X1011 RIBEIROX2012X1002XL RIBEIROX2012X1002X1011XL item S0169-2607(12)00152-6 S0169260712001526 10.1016/j.cmpb.2012.05.013 271322 2014-10-02T02:18:14.257946-04:00 2012-12-01 2012-12-31 true 780522 MAIN 10 49197 849 656 IMAGE-WEB-PDF 1 gr5 90958 770 1667 gr4 401888 1796 2667 gr3 210746 952 2667 gr2 237434 1174 1667 gr1 501950 1991 3000 gr5 17579 174 376 gr4 46042 405 602 gr3 30791 215 602 gr2 30503 265 376 gr1 60319 450 678 gr5 3487 101 219 gr4 5940 147 219 gr3 3075 78 219 gr2 9155 154 219 gr1 6462 145 219 COMM 3401 S0169-2607(12)00152-6 10.1016/j.cmpb.2012.05.013 Elsevier Ireland Ltd Fig. 1 System architecture. The active peers form a distributed middleware layer between the workstations and the PACS archives (passive peers) of the healthcare institution. Besides, they are responsible for coordinating the passive peers and integrating them within the new PACS archive cluster. Fig. 2 Graphical User Interface (GUI) for managing the entire distributed system as if it was a centralized system. The GUI automatically lists the current status of active peers. Fig. 3 Distributed c-move message exchange at study level from two data sources. (1) The workstation sends a c-move command specifying the study UID of the target study to be moved. (2) The called active peer (coordinator) translates the c-move message into an equivalent c-find message and queries the passive peers A and B in parallel. The coordinator waits for the c-find response and finds that both passive peers hold the wanted DICOM object. (3) The active peer creates a queue with the SOP instances to move and their respective locations on the cluster. (4) The coordinator sends a message triggering the distributed c-move to each peer holding the SOP instances. The triggering message contains the next SOP instance in the queue to be moved. When the active peer at the other end receives the message it opens an association with the calling workstation and sends the first SOP Instance. After sending, the active peer sends a message to the coordinator asking for the next SOP Instance to be sent. (5) In the same c-store association the peer injects the new SOP instance assigned to it and asks for the following one. This process is repeated until the coordinator answers the peer request with a blank SOP instance UID. When this happens, the active peer closes the association and its interaction with the distributed c-move ends. Finally, the global distributed c-move ends when the last SOP instance is transferred to the workstation. Fig. 4 Topology at the physical layer of the performed experiments. Fig. 5 Chart containing the transfer time results. Clustering of distinct PACS archives using a cooperative peer-to-peer network Luís S. Ribeiro ⁎ Carlos Costa José Luís Oliveira University of Aveiro, IEETA-DETI, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 234370545; fax: +351 234370525. To face the demanding requirements of the clinical environment, PACS archives need to be resilient and reliable, supporting high availability and fault tolerance. Often, to ensure no data loss, PACS archives retain two copies of images on separate physical machines, using distributed data storage facilities. However, PACS do not take advantage of the various replicas to improve the transfer rates of medical images. This happens mostly because the DICOM standard does not comply with distributed fetching of image fragments while performing a store. Inspired by this unexplored opportunity, we designed and implemented a new solution that takes advantage of the distributed image replicas and, at the same time, respects the DICOM standard. Our strategy brought significant improvements in the exchange rates, load balancing and availability of installed PACS archives. Moreover, the adopted strategy forms a cluster of PACS archives that transparently enables horizontal scaling, facilitates the creation of backups, and gives to healthcare professionals a unified view of the distributed repositories. Keywords DICOM Peer-to-peer Load balancing PACS Scale-out 1 Introduction The Picture Archiving and Communication System (PACS) concept embraces typically dispersed sub-systems, ranging from image archiving, acquisition and distribution to visualization. With the introduction of the Digital Imaging and Communication in Medicine (DICOM) standard, PACS received a positive boost, due to the fact that PACS vendors were no longer able to lock a healthcare institution into proprietary communication protocols, forcing it to purchase only their PACS solutions. Regarding communications, DICOM relies on a set of well-defined services that binds together PACS distributed devices [1]. The PACS archive is the heart of an image center, and it is responsible for safely storing all exams produced within the healthcare institution. Due to the importance of clinical exams, these archives typically rely on data redundancy to prevent data loss and resource redundancy to guarantee continuous availability. In other words, current fault-tolerant PACS archive solutions always keep at least two copies of the same image at two different physical machines (archive server and backup server) [2,3]. The complexity of such PACS archives is high and such reliable solutions are typically associated with high purchase costs for healthcare institutions. Therefore, smaller imaging centers are not able to own a state-of-the-art PACS archive and typically rely on a centralized machine to store exams. We implemented a system that upgrades transparently any DICOM compliant PACS archive regarding availability, aggregation of resources, and scaling at low prices. Furthermore, our system also takes advantage of the image replicas already existent and improves the transfer rates between DICOM devices. This paper describes the implementation of the system that upgrades the PACS archive in several dimensions in a transparent and straightforward manner. 2 Background The PACS archive is one of the key components of the healthcare workflow, and its malfunctioning might jeopardize the delivery of high quality care services [2,4,5]. Therefore, it is desirable that PACS should be an extremely reliable system: fault-tolerant, continuously available and with an acceptable performance to satisfy the daily operations of the healthcare institution [6].The importance of the PACS archive becomes even more vital with institutions following a paperless and filmless approach in serving their patients [2]. The reliability level of a computer system can be measured by the number of single points of failure it incorporates. As the expression suggests, single points of failure increase the risk of system failure or malfunction. The risk increases when critical components to deliver a service are unique. For instance, if the power supply unit of a centralized server fails, the entire server will fail and, as a consequence, the services it provides will be unavailable until the power unit is replaced. Therefore, the conventional centralized storage server is not suitable for the healthcare environment, because if the storage server fails, all the PACS services dependent on it will also fail. To build a reliable computer system, the single points of failure must be eliminated or minimized to the extent possible. Typically, this is achieved by resource redundancy. As a result, if a system resource (physical or virtual) fails, there will be at least one backup resource to fill the gap and the requested service will not be at risk. Therefore, the storage archives serving the PACS workflow tend to be a distributed storage system (or storage network), where several machines cooperate in the storage task and ideally, if one of them fails the other machines will compensate for the failure without jeopardizing the storage service. A distributed system, besides eliminating single points of failure, also reduces bottlenecks in the workflow by distributing the workload among the several machines enabling performance improvements. Furthermore, these distributed architectures are able to scale horizontally, which brings significant economic and planning benefits compared to vertical scaling [7]. Scaling horizontally allows a system to scale progressively and according to needs in an efficient and cost friendly manner, the only drawbacks being its complexity to implement and manage compared to vertical scaling. One good example of scaling-out strategies is peer-to-peer networks (e.g. BitTorrent [8,9]) where a peer is at the same time a server and a client, similar to a DICOM device capable of performing the roles of Service Class User (SCU) and Service Class Provider (SCP). In this kind of distributed system the data is spread among the peers, each peer holds a small portion of the network's available data, and there is data redundancy since different peers may hold replicas of the same file. Peer-to-peer networks achieve good levels of availability and fault-tolerance since if a peer fails or disconnects the data it holds, this will almost certainly be available on another peer. Besides, peer-to-peer networks take advantage of this multiplicity of peers holding the wanted file and instead of downloading the entire file from one peer they download the various fragments composing the file (blocks) from multiple peers, bringing load balancing advantages and also improving the file transfer rates [10,11]. Before peer-to-peer networks, other distributed systems had used this strategy, for instance, Andrew's File System (just on read-only files) [12] and OSF's file system in a Distributed Computing Environment [13]. Of course, a conventional peer-to-peer file sharing network such as BitTorrent would not be appropriate for the medical context because the PACS archive must ensure that all exams are available 24/7 and there is no guarantee that the peers holding some piece of data will always be online. However, we believe that peer-to-peer inspired networks could suit the healthcare situation well. Peers must be more reliable and controlled by the healthcare institution (PACS archives). Moreover, they should cooperate in order to support the clinical workflow, providing reliable, widely available and high-performance services at lower prices. 3 Materials Nowadays, PACS may be seen as a mature technology. They are present in the majority of medical imaging centers around the world. This worldwide acceptance was due in part to the success of the DICOM standard which, among other things, enabled interoperability inside healthcare institutions, allowing the devices having a role in the PACS workflow to communicate transparently [14]. PACS archives are no exception. Typically, an archive is expected to implement at least the DICOM services of Storage, Query and Retrieve. As we mentioned before, our system does not intend to replace the healthcare institution's existent PACS archives. Ultimately it intends to improve the performance, availability, fault-tolerance, management and search capabilities of the PACS distributed archive. Therefore, the core of our peer-to-peer network is based on these three DICOM services which most PACS archives support, i.e. in order for a PACS archive to be extended with our system it must support the DICOM services of: Storage, Query and Retrieve. In this section we will present the technologies and software components that were used to build our system. 3.1 DICOM standard Around the world there are many manufacturers of non-invasive medical imaging equipment (modalities). In the past, manufacturers developed their own proprietary communication and data exchange standards, making the interaction between equipment from different vendors impossible [15–17]. This situation was comfortable for most manufacturers who could lock customers, but at the same time unsustainable for healthcare institutions. Therefore, in 1992 the Digital Imaging and Communication in Medicine (DICOM) standard was established and has since become the standard for exchanging medical images in digital format [1,18]. DICOM facilitates the implementation of PACS solution and enables interoperability in a multi-vendor environment [16]. This non-proprietary standard is very complete and has many dimensions. However, it is best known for the definition of objects (DICOM objects including images, waveforms, studies, etc) and for operations applied to these objects such as storing, moving, finding, creating, and printing. Operations in DICOM are called Service Classes or, simply, DICOM services. DICOM services are the tools that enable online interoperability between entities from different vendors. Typically, DICOM services rely on two entities called application entities (AE). One entity assumes the client role, called service class user (SCU), and the other entity assumes the server role, called the service class provider (SCP) [15]. The service is a two-step process: negotiation and exchange. The negotiation step (association establishment) is critical to guarantee that the two entities understand and interpret the information sent in the exchange step. After the negotiation has been completed successfully, the exchange step takes place where the actual information is exchanged. Formally, the negotiation step is known as the DICOM association establishment and the exchange information step occurs within the DICOM association [1]. The exchanged information (Service-Object-Pair SOP) is defined as a combination of a DICOM Service Element (DIMSE), which is a command, and an object. Examples of DIMSE are c-store, c-get, c-find, c-move, etc. Examples of objects are computed tomography (CT) images, magnetic resonance (MR) images, waveforms, etc. To promote transparent integration within the healthcare institution our system follows the DICOM standard. Every peer of our system implements both roles (SCP and SCU) of the DICOM services verification, storage, query and retrieve. To implement the services we used Dcm4Che toolkit. Dcm4Che toolkit is a popular open source implementation of the DICOM Standard developed in Java programming language [19,20]. Therefore, interoperability is ensured with the DICOM services and portability as well since Java is a multi-platform language. 3.2 JavaGroups Compared to server-centric approaches, distributed systems may bring several benefits, such as: improved fault-tolerance, improved availability, cost-friendly solutions and incremental growth. However, these advantages often come at the cost of more complex software requiring greater effort regarding implementation, management, configuration and security [13,7]. For instance, it is simpler to configure one central server than multiple servers in order for them to cooperate. Complex systems are typically prone to human errors, which in the clinical environment could lead to disastrous consequences. Hence, the drawbacks of complexity in distributed systems must be eliminated or diminished to a complexity level equivalent to the server-centric approach. In order to achieve a distributed system with an acceptable management and configuration effort, we used JavaGroups toolkit as a software component of our system. JavaGroups is an open source platform [21] written in Java that enables reliable group communication. Group communication uses the terms of group and member, where members are part of a group. In other words, a member is a node and a group is a cluster. Nodes can join or leave a group, send messages to all the nodes or to a single node of the group. The system keeps track of the members within the cluster and notifies the nodes when a new node joins and when an existent node leaves the cluster or crashes. This node crash awareness enables the design of robust software systems that readjust in order to compensate for eventual system malfunctions. Moreover, JavaGroups communicates through reliable multicast. Therefore, the messages exchanged between nodes are guaranteed to arrive at their destination and in the right order. At the core of JavaGroups there is low-level abstraction enabling endpoint communication with a channel entity group. When a channel is created, it is given a name and a unique address. Multiple channels with the same name form a group. When a member connects to a channel it can send and receive a message to and from all other members. The channel provides its members with a list of addresses, called a view, of the members connected to the group. A member can select an address from this list and send it a unicast message, or send a multicast message to all members of the current view. This feature enables us to create a network abstraction where the system administrator may monitor, manage and configure every node in a unified view as if it was a centralized system. JavaGroups has a decentralized architecture. However, the first member joining the group will be automatically elected as the group leader. Besides the common node functions, the group leader is responsible for sending the view of the system to the other nodes. As we mentioned before, the view consists of a list of group nodes ordered bydate of entry and the first entry is the group leader. Therefore, if the leader crashes or leaves the group, the system will automatically elect the second entry as the group leader. With JavaGroups every peer in our distributed system is immediately aware of which other peers are online/offline, and when unicast or multicast messages are sent the peer is sure that the messages were properly received by the destination peers, simplifying the coordination process of the active peers. Because of its fault tolerance and reliable communication methods, JavaGroups was chosen as one of our system's software components. 3.3 Dicoogle Indexing System Traditionally, a PACS infrastructure relies in a database engine to manage the DICOM information model [22] or, at least, holds the DICOM objects in a local directory called “dicomdir”, a structured linked list containing information related to patients, studies, series and images. When a modification is targeted to the DICOM objects, an update call is triggered at the database making the information at the dicomdir and the database coherent. Dicoogle's main idea is the replacement or extension of the traditional database by an indexing engine [23,24]. This approach makes the PACS management more flexible, because it is possible to index new text-based fields quickly without the need to create new tables or relations as would be necessary in the database supported approach. Furthermore, Dicoogle automatically refreshes the indexes by monitoring the file system events (create, delete, update) targeted to the DICOM files. As we will explain in the following sections, our system offers two operating modes: stateless and stateful. The stateless mode does not hold any extra memory regarding DICOM objects from metadata fields. On the other hand, the stateful operating mode, besides extending the number of cooperating PACS archives, is a PACS archive in itself. The stateful operating mode follows a similar strategy to Dicoogle, offering a Google-like search experience by indexing, locally, the DICOM objects that it holds. 4 Methods PACS architecture began mainly on an ad hoc basis, serving small subsets, called modules, of the radiology department. Each module functioned as an independent island, unable to communicate with other modules [2]. Later it evolved into a PACS infrastructure solution, integrating the hospital information system and the radiology information system, serving the entire hospital. Although the various PACS archives that serve the institution are typically accessible within the institution, they tend to be independent of each other. This may be useful to create federations of clinical specialties within the institution, but if a workstation needs to access the full view of one patient it has to query archive by archive [25]. Furthermore, when the central PACS archive of the healthcare institution or department depreciates (e.g. not capable of delivering an acceptable QoS) the institution typically replaces it with a more powerful machine (scales up) or adds a new and independent PACS archive. Scaling up may improve the QoS but it is normally associated with high costs and data migration problems [26]. Secondly, by simply adding independent archives to the PACS, it spreads the data repositories and as a consequence makes management and discovery of resources more complex. Our system allows the PACS archive to scale out and at the same time with a centralized abstraction of the distributed system. Besides, retrieving an aggregated view of all the distributed resources, our system goes one step further by turning the independent nodes into a cluster of cooperative nodes. It takes advantage of the various file replicas in the institution and improves the transfer performance of the DICOM objects. We now present the methods applied in our system to upgrade the healthcare institution's current PACS archives without the need to replace software and hardware resources. 4.1 Architecture Our distributed system is a symbiosis between the PACS archive(s), owned by the healthcare institution, and the nodes fully developed by us. The machines of the institution's PACS archive we call passive peers and our nodes active peers. For each passive peer there is one active peer responsible for coordinating it within the new distributed system (Fig. 1 ). The coordination is performed exclusively through standard DICOM services: storage, query, retrieve and verification. In this way the passive-active peer symbiosis, to optimize the PACS services, is achieved transparently. As result, the new cluster of peers performs the PACS archive tasks as if they were from the same vendor and planned to work together from the beginning, transforming independent and centralized PACS archives into a network-centric cooperating PACS archive. Acceptance of the network-centric paradigm is growing, and distinct machines with different quality of service (QoS) are integrated by various forms of communication services [27]. Distributed middleware systems have a major role in combining isolated and independent machines into a swarm of machines working together. Middleware approaches in general bring several benefits while developing a new system [27]: • Reducing software lifecycle costs through using the potential of previous systems, capturing development expertise and key functionalities of the existent system, rather than rebuilding them manually one by one. • Providing a higher-level abstraction oriented to the services required by the application, simplifying the development of distributed embedded systems. • Hiding the complexity of lower abstraction layers (e.g. network), focusing on the core services that generate added value. • Making management, configuration, monitoring and security (which 10 has proven to be necessary) efficient and less demanding from the development and administration point of view. Distributed middleware systems yield significant benefits when the standards (e.g. communications, formats, etc.) are mature. During the past decade there was a consolidation of the DICOM standard within the medical imaging environment and DICOM is the consensual standard when treating medical images. This fact enabled us to build a distributed middleware system for the medical imaging environment that unifies and extends the storage services of the PACS workflow. Each active peer must be installed in the same physical machine as the passive peer it must coordinate. Although the active peer and the passive peer share the same hardware resources, they still communicate through DICOM services and the two software devices are independent. The active peer only requires knowing the file directory (or directories) where the DICOM objects are stored, to map the SOP instance unique identifier (UID) of the DICOM object with its respective file path. For the active peers to interact with the passive peer, the Application Entity (AE) title, the IP address and the port of the PACS archives must be known. However, configuring the various active peers one by one requires great effort and could lead to human errors. Therefore, our system provides a graphical manager (Fig. 2 ) that automatically detects the active peers currently online giving a unified view to the network administrator. The administrator configures the required fields of all active peers, which becomes a XML configuration file of the cluster. When the start button is pressed the XML file is sent to each active peer automatically and the cluster starts operating with the desired configurations. The XML file contains: (i) the SCUs allowed to interact with the DICOM network; (ii) the AE Title, port, and IP address of the PACS archives to support; (iii) the port of each active peer where the DICOM services will be available; (iv) operating mode (stateless or stateful). 4.2 Stateless and stateful active peers The active peers may operate in two distinct modes: stateless and stateful. The difference between them is the amount of metadata that they hold. In the stateless mode, the only metadata that the active peer holds is the map between the SOP Instance UID and the respective location of the DICOM object on the local file system. In the other hand, the stateful mode indexes all the metadata of the DICOM object following a similar approach to that used by Dicoogle [23]. By default, the active peer will be operating in the stateless mode. In the stateful mode, the active peer accumulates the tasks of the PACS archive, rather than being just a relay to the cluster of PACS archives. The purpose of offering two distinct operating modes is to facilitate the integration of our system within the healthcare institution. Typically, PACS archives hold large amounts of data and substituting the software of PACS archive may take significant periods of time – since new software would have to extract and structure the metadata from all DICOM objects once again. Therefore, the stateless mode avoids this massive refactoring of metadata turning the initial integration step less demanding. However at a more advanced stage, the scaling out of the cluster will pass by the addition of new peers without any previous DICOM images. Therefore, we provide the stateful operating mode which combines the functionalities of active and passive peers in the same software device. 4.3 Peer-to-peer DICOM-based network The developed peer-to-peer network based on DICOM enables the active peers to cooperate with each other and at the same time coordinate the passive peers transparently. This peer symbiosis relies on the DIMSE commands: c-store, c-find, c-move and c-echo. Nevertheless, the purpose of the developed network is to hide the distributed nature of the system and deliver the DICOM services as if they were requested from a centralized PACS archive. The communication between active and passive peer is performed exclusively with DIMSE commands. 4.3.1 Distributed c-find The purpose of the distributed c-find is to enable queries over the distributed system rather than over a single machine. The active peer that receives the workstation c-find request forwards it to every existent passive peer. The called active peer waits at the most a timeout period for all the c-find responses, joins the search results and answers the calling workstation with the c-find response message. The c-find response is the concatenation of all distributed results without repeated entries. 4.3.2 Distributed c-store Typically, each department of the healthcare institution has its own independent PACS archive. If having a unified view of all independent archives is a positive feature while searching for a patient's exams, regarding storage it might not be the desirable scenario for the majority of healthcare institutions. For instance, the cardiology department probably prefers to continue storing exams next to the exams produced there, rather than store them next to the radiology department, justified only by a new storage system serving the institution. Therefore, each active peer represents each passive peer individually by initiating a DICOM storage service per passive peer. Furthermore, it is also possible to initiate extra DICOM storage services that store the received objects and more than one peer. In this way, when an active peer receives a c-store from a workstation, it knows which is the respective(s) passive peer(s) and where to store it. Therefore, the active peer forwards the c-store to the passive peer(s). Finally, if a PACS archive is currently offline, the active peers store the DICOM objects targeted to that archive temporarily until it is once again online. Hence, the active peers periodically try to execute the c-store command until the PACS archive successfully stores the DICOM objects. 4.3.3 Distributed c-move The traditional c-move command allows one workstation to order the movement of DICOM objects from one device to another, even if the transfer destination is not the workstation itself but a third party device. Therefore, the workstation that triggers the c-move command needs to specify the destination AE title where the DICOM objects will be moved. The Information Object Definition (IOD) commonly known as DICOM object possesses a hierarchical tree structure organized from top to bottom into: patient, study, series and image. Each level is identified by a unique identifier and the objects are moved based on that identifier. For instance, if a workstation sends a c-move command at the study level (specifying the Study UID) all the images belonging to that specific study will be retrieved. On the other hand, the c-move performed at the image level (SOP Instance UID) will just retrieve that specific image. A DICOM object is composed of several independent images that may be retrieved separately. Due to the fact that our system is aware of the DICOM object replicas and knows their location, it takes advantage of this condition and triggers a distributed c-move. The distributed c-move retrieves the images of the wanted DICOM object from several data sources. The data sources are the passive peers that contain the specific DICOM object. For instance, if the same DICOM object is located (for disaster recovery purposes) at several passive peers and the object is composed of several images, transfer of the DICOM object will be balanced among the various data sources. Fig. 3 illustrates a distributed c-move episode where the DICOM object requested by the workstation is present at two data sources. The workstation sends c-move at the study level but receives the SOP Instances that compose the study from several sources (two sources in this example). The called active peer will be the coordinator of the distributed c-move episode and is responsible for creating a queue with the location of each requested SOP instance on the distributed system. The coordinator, after creating the queue, sends a message triggering the distributed c-move. On receiving that message the peers send the respective DICOM object and request the following object to be sent. This process of requesting/sending the following object to move will balance the workload of the transfer through the cluster nodes evenly, since the idler (or faster) peers will request more images to move. Furthermore, at step four of Fig. 3 a pseudo c-move message (sent via JavaGroups communication channels) is used instead of the standard c-move message in order to make this process more efficient. The distributed c-move has the same purpose as the standard c-move message although it allows the movement of a set of independent SOP Instances (without the need for them to belong to the same branch of the DICOM object). At an early stage, we considered use of the c-move message, sent directly to the respective passive peer, but for each independent SOP instance to be moved, the standard c-move opens one association, adding a lot of overhead to the exchange and diminishing the performance results significantly. Therefore, with the distributed c-move we go a step forward, taking advantage of the various image replicas present within the distributed system to reduce transfer times between the PACS archives and the workstations or whenever a DICOM object needs to be moved. 5 Results and discussion Although the presence of the DICOM standard is a fact in the majority of healthcare institutions, the PACS workflow tends to be heterogeneous from one institution to another. PACS are specially designed to serve the workflow needs of one specific healthcare institution. As we mentioned before, our system complements or extends the PACS archives of the healthcare institution. Therefore, we built a flexible system so that it could adapt itself as far as possible to the institution's situation, and not the reverse. For the reader to understand the potentialities of our system in its several dimensions, we present the categories where it may bring benefits to healthcare institutions. 5.1 Availability and load balancing Besides joining together several PACS archives in a single integrated environment, our system may also be exploited to increase the availability and robustness of these archives. By transforming independent PACS archives of healthcare institutions in a cooperating cluster of PACS archives, it reduces the number of single points of failure and as a consequence increases the availability of the supported DICOM services. There is an improvement in the availability of the DICOM storage service: e.g., if one node crashes, the modalities may send the DICOM objects to any other active peer of the cluster so that whenever the machine is back online the object will be stored in the desired node. Regarding the DICOM query/retrieve services, our system might also improve their availability, depending on the architecture of the healthcare institution's PACS archive. In other words, if one machine holds unique DICOM objects and crashes, those objects cannot be searched or moved. However, if our system is installed in the central PACS archive and in the backup server at the same time (in the stateless mode if the backup server is DICOM compliant or stateful if not) the distributed PACS is able to compensate for it and retrieve the respective object. Therefore, peer crashes will not affect the DICOM services and retrieval is possible as long as there is at least one instance of the requested DICOM object within the cluster of PACS archives. The traditional c-move operation triggers the transfer of DICOM objects regardless if the peer is currently busy with other tasks or if it is completely idle. Instead, our distributed c-move spreads the effort of moving the DICOM objects among all peers holding the object. This load balance is not a blind division of the workload per number of peers capable of carrying the move tasks rather our system delegates the transfer tasks according to the peer's current busyness state (on real time). Hence, when a new transfer episode is triggered a peer with lower hardware capabilities (e.g. link with lower bandwidth) or currently flooded with other parallel tasks will receive less tasks of the transfer episode than an idle peer. By adapting to the current conditions of the network our load balancing algorithm accomplishes a continuous flow of DICOM objects on the receiver's link. Furthermore, our empiric tests showed that the weight of our load balancing algorithm on the network traffic is residual – while comparing with the traditional c-move – there is a slight aggravation of 0.96% in average of data exchanged however the real effort for the network is the actual transmission of the DICOM objects. Either the distributed c-move or the traditional c-move will have a similar effect on the network traffic. The main difference is that our system does not focus the movement to just one source rather it balances (in a weighted manner) to all the peers holding the DICOM object. 5.2 Scaling and cost saving Typically, upgrading the PACS archive means replacement of the old hardware and software with new and more powerful resources (vertical scaling). We do not argue that vertical scaling will improve the QoS of the PACS archive. However, vertical upgrades necessary to reach a certain QoS level come at higher financial costs, compared to horizontal scaling. Our system enables the healthcare institution's current PACS archive to scale horizontally, reusing the previous PACS archives as peers of the new cluster of PACS archives. The stateless operating mode integrates the existent PACS archives of the healthcare institution in the distributed system, while the stateful mode allows the system to grow horizontally without the need for other PACS archive software. Furthermore, the project was implemented with Java, making the system portable and enabling the use of open source operating systems (e.g. Linux). Therefore, our system is capable of upgrading the PACS archive and reducing expenses on hardware and software components. However, scaling horizontally raises the complexity of the system, making its management and configuration a more difficult task. To diminish its complexity, our system automatically scans the network for the active peers, giving a centralized view for managing and configuring the distributed system. 5.3 Unifying view Our system also performs the role of distributed middleware system between workstations and the PACS archive. It is possible to aggregate all the PACS archives dispersed over the various departments of the healthcare institution, offering a unified view of the system. Without reorganizing the location of the DICOM objects or creating new metadata (e.g. new database), our system is capable of performing distributed search queries and, at the same time, delivering the search results in a centralized manner, i.e. as if all exams in the patient's health history in the various clinical specialties were present in one central machine serving the entire healthcare institution. 5.4 Performance With implementation of the distributed c-move, there is a reduction of the server's bottleneck and, as a consequence, improved performance regarding the transfer rate of DICOM objects. To demonstrate this statement we set up an evaluation scenario (Fig. 4 ). The PACS archive(s) were based on the Conquest DICOM server [28], which supports the DICOM services of Storage, Query, Retrieve and Verification. In this test we had chosen the stateless operating mode. Nevertheless, the purpose of these tests was to analyze performance improvements while exchanging one DICOM object with several data sources holding the same DICOM object and compare results against the legacy PACS archive. The experiment workflow starts when the workstation sends a c-move message, at the patient level, and finishes when the last association binding the workstation to one active peer is closed. Seven machines (four with Windows and three with Linux) and two Gigabit switches were used for the experiment. The experiments have involved one to three PACS archives and one to four clients. The DICOM object exchange in the various experiments is composed of 9 studies, more precisely 1117 computer tomography (CT), 13 X-ray angiography (XA), 661 magnetic resonance (MR), 12 nuclear medicine, and 225 positron emission tomography (PET), performing a total of 2028 SOP instances and 1.01Gbytes. The SCU, where the measurements were taken, shares the same switch as the PACS archives and at the other switch the remaining SCUs were plugged in (Fig. 4). Fig. 5 shows the results of the evaluation. There were four main experiment sets: (1) PACS archive standing alone, (2) one peer symbiosis (PACS archive aided by its respective active peer), (3) two peer symbiosis, and (4) three peer symbiosis. Inside each experiment set four different workload scenarios were tested: one SCU, two SCUs, three SCUs and four SCUs, always performing the same c-move request in parallel. The PACS archive was tested on three machines and the final result is the average of the three outcomes. In this way we reduce the noise from the hardware distinct performance. As a reference, we consider the ideal scenario to exchange DICOM objects: one SCU requesting one PACS archive. Therefore, the following evaluations of the obtained results will be relative to this scenario. There were no major differences for all the parallel c-move cases when comparing the PACS archive standing alone with one peer symbiosis, i.e. introduction of the active peer has a very small impact on retrieval latency. The transfer time is slightly aggravated (0.6s on average) due to the fact that the peer symbiosis adds c-find messages in the middle of the c-move process (as long as the c-move request does not target the SOP instance UID directly). Continuing the analysis of Fig. 5, it is visible that the major benefit of the distributed c-move is obtained when the servers have higher workloads (i.e. performing several tasks in parallel). When the same SOP instance is in more than one PACS archive, it is exchanged efficiently taking advantage of all available resources of the cluster at that exact moment. As a result, when the joint hardware resources of the cluster are equal to or above the number of parallel c-move requests, the transfer times tend to stabilize around the times of the ideal scenario. For instance, when three parallel c-move requests (3 SCUs) are querying the cluster, the transfer times are ideal on average. Nevertheless, when the available server's resources are less than the parallel c-move requests, there is degradation of the transfer times. However, because the peers on the cluster work as a whole, the server's bottleneck occurrences are diminished and the workload is spread evenly over the cluster, making the ideal scenario always possible when there are enough available hardware resources in the cluster. 6 Conclusions Clinical data stored on the PACS archive is valuable and requires reliable storage, wide availability and a good performance, which will not impair the healthcare workflow. In this paper we describe a distributed system that may easily improve the QoS of current PACS archives. The presented solution turns existing PACS archive(s) into a network-centric PACS archive by joining available hardware resources and enabling DICOM compliant archives to scale horizontally in a transparent manner. The system grows according to the needs of the users, allowing the institution to cut costs by purchasing new hardware resources gradually and reutilizing previous ones. Its integration inside the institution is almost effortless due to communication between peers being performed through DICOM services, and due to the operating modes that ease consolidation of the cluster in two distinct stages: the stateless mode avoids the substitution of the previous PACS archives and the stateful mode eases the horizontal scaling of the cluster. Furthermore, the middleware strategy simplifies backup routines by replicating objects to previous defined nodes. Finally, we also introduced the distributed image retrieval approach which takes advantages of the existent backup replicas of the DICOM objects, bringing improvements in the movement performance of DICOM objects. Acknowledgement The research leading to these results has received funding from Fundao para a Cincia e Tecnologia (FCT) under grant agreement PTDC/EIA-EIA/104428/2008. References [1] O.S. Pianykh Digital Imaging and Communications in Medicine: A Practical Introduction and Survival Guide 2008 Springer Publishing Company, Incorporated [2] H. Huang PACS and Imaging Informatics: Basic Principles and Applications 2010 John Wiley & Sons [3] S. Langer Issues surrounding PACS archiving to external, third-party DICOM archives Journal of Digital Imaging 22 1 2009 48 52 [4] J. Kommeri M. Niinimki H. Mller Safe storage and multi-modal search for medical images Studies in Health Technology and Informatics 2011 40 50 [5] R.R. Bond D.D. Finlay C.D. Nugent G. Moore A review of ECG storage formats International Journal of Medical Informatics 2011 1 10 [6] B. Liu F. Cao M. Zhou G. Mogel L. Documet Trends in PACS image storage and archive Computerized Medical Imaging and Graphics 27 2–3 2003 165 174 [7] M. Michael J. Moreira D. Shiloach R. Wisniewski Scale-up×scale-out: a case study using nutch/lucene IEEE International Parallel and Distributed Processing Symposium 2007 1 9 [8] D. Qiu R. Srikant Modeling and performance analysis of bittorrent-like peer-to-peer networks SIGCOMM – Computer Communication Review 34 2004 367 378 [9] B. Cohen, The Bittorrent Protocol Specification, Version 11031, 2008. [10] B. Zhang S. Wang An optimization model of load balancing in peer to peer (p2p) network International Conference on Computer Science and Service System (CSSS) 2011 2064 2067 [11] Q. Cao S. Fujita Load balancing schemes for a hierarchical peer-to-peer file search system International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC) 2010 63 70 [12] J.H. Howard An overview of the Andrew file system Winter 1988 USENIX Conference Proceedings 1988 23 26 [13] W.e.a.R.P., A Distributed Computing Environment for Multidisciplinary Design, Tech. Rep., 1994. [14] T. Treichel P. Liebmann O. Burgert M. Gessat Applicability of DICOM structured reporting for the standardized exchange of implantation plans International Journal for Computer Assisted Radiology and Surgery 5 1 2010 1 9 [15] N.E.M. Association Digital Imaging and Communications in Medicine (DICOM). Part 4. Service Class Specifications 2007 [16] H. Oosterwijk P. Gihring DICOM Basics 3rd edition 2005 OTech Inc. [17] T. Treichel M. Gessat T. Prietzel O. Burgert DICOM for implantations – overview and application Journal of Digital Imaging: The Official Journal of The Society for Computer Applications in Radiology 2011 1 7 [18] M. Mustra K. Delac M. Grgic Overview of the DICOM standard ELMAR 2008, 50th International Symposium, vol. 1 2008 21 29 [19] M. Warnock C. Toland D. Evans B. Wallace P. Nagy Benefits of using the Dcm4Che DICOM archive Journal of Digital Imaging 20 2007 125 129 [20] M.J. Warnock C. Toland D. Evans B. Wallace P. Nagy Benefits of using the DCM4CHE DICOM archive Journal of Digital Imaging: The Official Journal of the Society for Computer Applications in Radiology 20 October (Suppl. 1) 2007 125 129 [21] B. Ban, Javagroups – Group Communication Patterns in Java, Tech. Rep., Cornell University, 1998. [22] N.E.M. Association Digital Imaging and Communications in Medicine (DICOM). Part 3. Information Object Definitions 2003 [23] C. Costa A. Silva J.L. Oliveira Indexing and retrieving DICOM data in disperse and unstructured archives International Journal of Computer Assisted Radiology and Surgery, vol. 4 2009 71 77 [24] C. Costa C. Ferreira L. Bastião L. Ribeiro A. Silva J. Oliveira Dicoogle – an open source peer-to-peer PACS Journal of Digital Imaging 1 2010 1 9 [25] A. Tzavaras, N. Kontodimopoulos, E. Monoyiou, I. Kalatzis, N. Piliouras, I. Trapezanidis, D. Cavouras, E. Ventouras, Upgrading Undergraduate Biomedical Engineering Laboratory Training, 27th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, vol. 1, 2005, pp. 353–356. [26] I. Blanquer V. Hernández D. Segrelles E. Torres Enhancing privacy and authorization control scalability in the grid through ontologies IEEE Transactions on Information Technology in Biomedicine 13 1 2009 16 24 [27] R.E. Schantz D.C. Schmidt Middleware for distributed systems – evolving the common structure for network-centric applications Encyclopedia of Software Engineering 1 2001 1 9 [28] M. Oskin, M. van Herk, L. Zijp, Conquest DICOM Software 1.4.16 Released, 2007. "
    },
    {
        "doc_title": "Towards ontology based health information search in Portuguese - A case study in neurologic diseases",
        "doc_scopus_id": "84869022464",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84869022464",
        "doc_date": "2012-11-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "General publics",
            "Health informations",
            "Health professionals",
            "Health related informations",
            "Knowledge base",
            "Neurological disease",
            "Ontology-based",
            "Ontology-based information extraction",
            "Proof of concept",
            "Relevant documents",
            "Search interfaces",
            "Semantic annotations"
        ],
        "doc_abstract": "Health related information is spread across different locations, making difficult gathering, structuring and managing all this data to make it available through search and navigation to health professionals, students, researchers or even general public. In this paper, we present a workflow to support enhanced search, comprising the development of an ontology for the domain, entity annotation, advanced queries to the created knowledge base and a search interface to explore this information. The aim was to build a searching platform that would be able to present the most relevant documents as response to advanced user's queries. A proof of concept was implemented for neurologic diseases domain, demonstrating how the workflow can be used to obtain a functional ontology-based information extraction tool. © 2012 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A study of encoding overhead in network management protocols",
        "doc_scopus_id": "84867892559",
        "doc_doi": "10.1002/nem.1801",
        "doc_eid": "2-s2.0-84867892559",
        "doc_date": "2012-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Common open policy services",
            "Desktop management",
            "In-network management",
            "Internet engineering task forces",
            "Management technologies",
            "Network configuration",
            "Network dimensions",
            "Network management technologies",
            "Network overhead",
            "NGN networks",
            "Performance optimizations",
            "Simple network management protocols",
            "System management",
            "Web-based enterprise"
        ],
        "doc_abstract": "Next-Generation Network (NGN) is a critical scenario in terms of network management because of its network dimension, its number of users and its heterogeneity. Since the introduction of the Simple Network Management Protocol (SNMP) at the beginning of the 1990s, much effort has been devoted to the development of new network management technologies. Both the Desktop Management Task Force (DMTF) and the Internet Engineering Task Force (IETF) have developed different network and system management protocols, such as Common Open Policy Service, Web-Based Enterprise Management, Network Configuration and even adapted other protocols, such as Diameter and Web Services. A network management technology with poor scalability could compromise NGN management and ultimately NGN network behaviour. This paper analyses the network overhead of several management technologies developed by the DMTF and IETF, and goes on to compare their results with the usage of SNMP. Furthermore, some deployment recommendations are proposed for performance optimization in NGNs. Copyright © 2012 John Wiley & Sons, Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dicoogle relay - A cloud communications bridge for medical imaging",
        "doc_scopus_id": "84867301732",
        "doc_doi": "10.1109/CBMS.2012.6266402",
        "doc_eid": "2-s2.0-84867301732",
        "doc_date": "2012-10-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Imaging data",
            "Medical data",
            "Remote access"
        ],
        "doc_abstract": "Over the last decades, information systems for medical imaging sharing are imposing themselves as important tools for the diagnostic and study of pathologies. One of the most important advantages of those systems is to allow widespread sharing and remote access to medical data. Nevertheless, there is no simple solution for imaging data exchange between multiple places due to bureaucratic and technical issues. The paradigm introduced by Dicoogle project potentiates queries over a set of distributed repositories, which are logically indexed as a single federate unit. This paper describes a Cloud-based relay service that acts as a bridge of communications between the different institutions, allowing the community to access, share and discover imaging records. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "EuGene: Maximizing synthetic gene design for heterologous expression",
        "doc_scopus_id": "84870439703",
        "doc_doi": "10.1093/bioinformatics/bts465",
        "doc_eid": "2-s2.0-84870439703",
        "doc_date": "2012-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Numerous software applications exist to deal with synthetic gene design, granting the field of heterologous expression a significant support. However, their dispersion requires the access to different tools and online services in order to complete one single project. Analyzing codon usage, calculating codon adaptation index (CAI), aligning orthologs and optimizing genes are just a few examples. A software application, EuGene, was developed for the optimization of multiple gene synthetic design algorithms. In a seamless automatic form, EuGene calculates or retrieves genome data on codon usage (relative synonymous codon usage and CAI), codon context (CPS and codon pair bias), GC content, hidden stop codons, repetitions, deleterious sites, protein primary, secondary and tertiary structures, gene orthologs, species housekeeping genes, performs alignments and identifies genes and genomes. The main function of EuGene is analyzing and redesigning gene sequences using multi-objective optimization techniques that maximize the coding features of the resulting sequence. © The Author 2012. Published by Oxford University Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Advantages of a Pareto-based genetic algorithm to solve the gene synthetic design problem",
        "doc_scopus_id": "84866669905",
        "doc_doi": "10.2174/157489312802460712",
        "doc_eid": "2-s2.0-84866669905",
        "doc_date": "2012-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Codon usage, codon context, rare codons, nucleotide repetition and mRNA destabilizing sequences are but a few of the many factors that influence the efficiency of protein synthesis. Therefore, gene redesign for heterologous expression is a multi-objective optimization problem and the factors that need to be considered are often conflicting. Evolutionary approaches have already been shown to be able to evolve a sequence under the forces of specific constraints. However, it is unclear what are the advantages of a slower algorithm such as GA when compared with other faster algorithms in the gene redesign context. Here, a solution using genetic algorithms along with a Pareto archive is used for the gene synthetic redesign problem. The different redesign parameters are merged using an adapted genetic algorithm strategy. From the created model, the best possible synonymous gene sequence is generated. This allows tackling the gene redesign problem by exploring the large search space of possible synonymous sequences. It is then shown that genetic algorithms have several advantages over other heuristics in the gene redesign problem. For instance, the ability to return the best solutions constituting the main part of the Pareto front, even in non-convex or non-continuous spaces. This allows a researcher to select synonymous genes among the optimal solutions, to best suit his purpose, instead of accepting a single solution that might represent an unwanted trade-off between the objectives. © 2012 Bentham Science Publishers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Telecardiology through ubiquitous Internet services",
        "doc_scopus_id": "84865106654",
        "doc_doi": "10.1016/j.ijmedinf.2012.05.011",
        "doc_eid": "2-s2.0-84865106654",
        "doc_date": "2012-09-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Electronic mailboxes",
            "Internet browsers",
            "Internet services",
            "IT resources",
            "Medical images",
            "Patient safety",
            "Plug-and-play",
            "Portugal",
            "Quality of care",
            "Real environments",
            "Remote diagnosis",
            "Security model",
            "Server-based",
            "Telecardiology",
            "Work efficiency"
        ],
        "doc_abstract": "Purpose: Implementation of telemedicine in many clinical scenarios improves the quality of care and patient safety. However, its use is hindered by operational, infrastructural and financial limitations. This paper describes the design and deployment of a plug-and-play telemedicine platform for cardiologic applications. Methods: The novelty of this work is that, instead of complex middleware, it uses a common electronic mailbox and its protocols to support the core of the telemedicine information system and associated data (ECG and medical images). A security model was also developed to ensure data privacy and confidentiality. Results: The solution was validated in several real environments, in terms of performance, robustness, scalability and work efficiency. During the past three years it has been used on a daily basis by several small and medium-sized laboratories. Conclusions: The advantage of using an Internet service in opposition to a server-based infrastructure is that it does not require IT resources to set up the telemedicine centre. A doctor can configure and operate the centre with the same simplicity as any other Internet browser application. The solution is currently in use to support remote diagnosis and reports of ECG and Echocardiography in Portugal and Angola. © 2012 Elsevier Ireland Ltd.",
        "available": true,
        "clean_text": "serial JL 271161 291210 291773 291870 291901 291919 31 International Journal of Medical Informatics INTERNATIONALJOURNALMEDICALINFORMATICS 2012-06-17 2012-06-17 2012-08-11T03:36:44 S1386-5056(12)00105-0 S1386505612001050 10.1016/j.ijmedinf.2012.05.011 S300 S300.1 FULL-TEXT 2015-05-15T06:04:43.33604-04:00 0 0 20120901 20120930 2012 2012-06-17T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref specialabst alllist content subj ssids 1386-5056 13865056 false 81 81 9 9 Volume 81, Issue 9 5 612 621 612 621 201209 September 2012 2012-09-01 2012-09-30 2012 Regular Papers article fla Copyright © 2012 Elsevier Ireland Ltd. All rights reserved. TELECARDIOLOGYTHROUGHUBIQUITOUSINTERNETSERVICES COSTA C 1 Introduction 2 Methods 3 Framework proposal 3.1 Workflow 3.2 Security architecture 3.3 Reviewing console 3.4 Management module 3.5 Installation and setup 4 Results 5 Conclusions Authors’ contributions form Competing interests References NIKUS 2009 473 480 K WADE 2010 233 V KHASANSHINA 2006 748 775 E DAUCOURT 2006 287 293 V REINER 2005 420 426 B BRADLEY 2004 244 248 W LARSON 2005 967 970 P WILLIAM 2007 545 549 B CAFFERY 2010 20 34 L DELLAMEA 1999 84 89 V FRASER 2001 815 819 H WEISSER 2006 753 758 G CAFFERY 2008 107 L COSTA 2009 273 282 C COSTA 2002 460 462 C SMITH 2005 286 293 A EKELAND 2010 736 771 A BENJAMIN 2010 3 9 M DONNELLY 2010 2029 2038 L PIANYKH 2008 O HMSBOSTONMAUSADIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMAPRACTICALINTRODUCTIONSURVIVALGUIDE BOND 2011 681 697 R COSTA 2007 79 108 C STUDIESINCOMPUTATIONALINTELLIGENCEADVANCEDCOMPUTATIONALINTELLIGENCEPARADIGMSINHEALTHCARE CURRENTPERSPECTIVESPACSCARDIOLOGYCASESTUDY HUANG 2004 H PACSIMAGINGINFORMATICSBASICPRINCIPLESAPPLICATIONS COSTA 2004 277 287 C COSTAX2012X612 COSTAX2012X612X621 COSTAX2012X612XC COSTAX2012X612X621XC item S1386-5056(12)00105-0 S1386505612001050 10.1016/j.ijmedinf.2012.05.011 271161 2012-08-11T00:31:33.497198-04:00 2012-09-01 2012-09-30 true 1590514 MAIN 10 48707 849 656 IMAGE-WEB-PDF 1 gr1 54461 633 263 gr1 5299 164 68 gr1 404936 2810 1167 gr2 77148 376 565 gr2 12957 146 219 gr2 738022 1662 2500 gr3 88165 380 565 gr3 13410 147 219 gr3 1388809 1681 2500 gr4 56810 355 565 gr4 12112 138 219 gr4 483432 1572 2500 gr5 54902 368 565 gr5 11110 143 219 gr5 436276 1630 2500 gr6 47482 312 744 gr6 4700 92 219 gr6 348908 1380 3293 gr7 25125 331 747 gr7 2860 97 219 gr7 187047 1464 3308 gr8 39573 362 753 gr8 3844 105 219 gr8 306271 1603 3333 IJB 2876 S1386-5056(12)00105-0 10.1016/j.ijmedinf.2012.05.011 Elsevier Ireland Ltd Fig. 1 The telecardiology central workflow. Fig. 2 Reviewing console – incoming area (messages and examinations). Fig. 3 Reviewing console – Tele-ECG module (ECG visual representation, textual report, pending examinations and reports). Fig. 4 Reviewing console – Tele-Echo module. Fig. 5 Central management – repository and accounting modules. Fig. 6 Tele-ECG centre – usage statistics. Fig. 7 Physical distance from reading centres to client sites. Fig. 8 Distribution of service requests throughout the day. Telecardiology through ubiquitous Internet services Carlos Costa ⁎ José Luís Oliveira University of Aveiro, DETI/IEETA, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 234 370500. Highlights ► A secure and efficient Tele-cardiology solution. ► The solution is based upon the cloud computing paradigm. ► Can be easily installed and maintained by end-users. ► Three years of exploitation in Portugal and Angola. Purpose Implementation of telemedicine in many clinical scenarios improves the quality of care and patient safety. However, its use is hindered by operational, infrastructural and financial limitations. This paper describes the design and deployment of a plug-and-play telemedicine platform for cardiologic applications. Methods The novelty of this work is that, instead of complex middleware, it uses a common electronic mailbox and its protocols to support the core of the telemedicine information system and associated data (ECG and medical images). A security model was also developed to ensure data privacy and confidentiality. Results The solution was validated in several real environments, in terms of performance, robustness, scalability and work efficiency. During the past three years it has been used on a daily basis by several small and medium-sized laboratories. Conclusions The advantage of using an Internet service in opposition to a server-based infrastructure is that it does not require IT resources to set up the telemedicine centre. A doctor can configure and operate the centre with the same simplicity as any other Internet browser application. The solution is currently in use to support remote diagnosis and reports of ECG and Echocardiography in Portugal and Angola. Keywords Telemedicine Telecardiology ECG Echocardiography 1 Introduction Telemedicine is the provision of health care services through the use of information and communication technology, in situations where the health care professional and the patient are not in the same location [1]. Telemedicine environments have been developed as a way to improve the efficiency of health resources, to reduce costs [2–4], and to promote wider access to healthcare services [5]. However, despite many technological achievements, the requirements of patients and clinicians are also changing. In the area of cardiology, for instance, the increase in life expectancy results in a higher prevalence of chronic cardiovascular diseases and in an increasing demand for continuous health care services. Existent statistical data and productivity studies show that ECG (electrocardiogram) and medical image data can be generated in practically any healthcare institution, even one with limited human or financial resources [6]. Nevertheless, highly skilled cardiologists are usually concentrated in a reduced number of specialized medical centres. The asymmetric distribution of equipment and service providers across countries leads typically to the need to hire third party cardiology reporting services outside the institutions where the exams were made. Nighthawk radiology is already a common solution in tackling this problem, where a staff of radiologists evaluates examinations transmitted electronically to them from around the world. Using IT resources, properly trained and accredited professionals are able to provide the referring physician with a written formal opinion, based on the performed exam, on the patient's clinical history and, in some cases, other physicians’ opinions [7–9]. Communication between the technicians performing the examinations and the specialized unit responsible for the analysis can be achieved using many different available technologies, ensuring the various requirements imposed by distinct scenarios. In the last decade, several low-cost solutions have been proposed for different telemedicine scenarios [10]. Della Mea [11], for instance, was one of the first authors to suggest the use of electronic mail for store-and-forward telemedicine applications. The TeleMedMail was another experiment based on the transmission of digital camera images through email, and was used to promote telemedicine in developing countries [12]. The lack of standardization for teleradiology connections also led the German Radiology Society to propose the use of the DICOM standard in email attachments, to promote interoperability between distinct manufacturers of PACS equipment [13]. However, commercial off-the-shelf email applications were designed for peer-to-peer communication and may be inefficient in delivering telemedicine. One solution may be to develop purpose-written applications [14]. Previous telecardiology projects with national and international partners [15,16] have consolidated our awareness of the fact that, besides technology, the key factor for the success of telemedicine scenarios is the way they replicate traditional procedures and allow increased productivity. In this manuscript we present a simple, robust and low-cost telecardiology centre. 2 Methods The constant advances in information technologies are raising new challenges in the healthcare sector. Professionals are demanding new Web-based access to information, as a way to avoid local software installation and support Internet-based scenarios for remote diagnostic and cooperative work. It is possible to establish telemedicine services with a distinct technological approach to support diversified clinic requirements, including service workflow and operational modus operandi [17]. Nevertheless, the effectiveness of many reported telemedicine projects is limited and inconsistent [18]. There are two main methods of telemedicine: real-time and “store and forward” sessions. In the former, participants can send and receive information almost instantly, for instance, in Tele-consultation and Tele-monitoring practices. In the latter, data is acquired and “stored” in one physical location. When convenient, the data is transmitted (“forwarded”) to a remote peer where clinical specialists analyse it. Finally, the specialist opinion or report will be transmitted back in an uncertain time period. A very simple example of this modus operandi is the transmission of a digital X-ray image via email to a specialist for diagnostic support. The outcome of the specialist work is a final report delivered to the relevant stakeholder (referring physician, patient, administration). The report must be accurate and delivered promptly [19,20]. Those issues are especially important in the business model of outsourcing reporting, where telematics application to support information acquisition, transmission, visualization and reporting is fundamental to increase reviewer efficiency. Efficiency can be measured as speed plus accuracy in result production and delivery [19]. According to this definition, Tele-radiology based on off-the-shelf email applications is not practical for large scale, multi-centre reviewing processes. They require manual processes to send and receive examinations and reports. The outsourced medical imaging report market demands professional Tele-ECG and Tele-Imaging solutions, typically based on dedicated servers, where applications are developed to run in specific operating systems and hardware configurations. This option also requires installation and continuous maintenance processes. Moreover, the portability to other architectures is extremely restricted. Our approach is based on the idea of cloud computing where Internet resources are seen as a natural infrastructure that includes hardware, operating system, applications and services. Analysing the requirements of telematic platforms to process ECG and echocardiography examinations, three main elements were identified: central storage capacity, normalized communication channel and universal access. An electronic email box supplies all the identified requirements, through three fundamental services: • A way to send messages: an encapsulated message, in SMTP normalized format (Simple Mail Transport Protocol) [21], is passed from the sender computer to the destination message recipient box, across intermediate email servers. • A way to receive messages: there are two main email reading protocols, i.e. POP (Post Office Protocol) and IMAP (Internet Mail Access Protocol) [22]. The main difference between them is that IMAP protocol does not remove messages from the server after they are downloaded, i.e. they will remain indefinitely in the server until users explicitly delete them. • A way to store messages: messages are deposited in the server mailboxes. Each mailbox is organized in folders, sub-folder areas and messages. A message has a header, a body, and several attached files. The proposed telematics solution uses normalized email messages to upload examinations to the centre and return reports from the centre to stakeholders. The mailbox storage is used as the platform database and object repository. Upon this messaging infrastructure, a software application was developed to operate with the centre, presently supporting Tele-ECG and Tele-Echocardiography sessions. When used for this purpose, the mailbox account is blocked for regular email clients (Outlook, Eudora, Thunderbird, Entourage, etc.). The medical informatics scenarios empowered by open Internet services are promising and they can even change the pattern of traditional technology use. The proposed Tele-cardiology system is an example of using ubiquitous cloud services to support remote diagnosis on a daily basis. 3 Framework proposal To create an efficient “store and forward” telemedicine platform that supports the reporting of cardiology examinations, several components were addressed: a communication network, a messaging system and a message repository. Choosing a mail server to support our telemedicine centre revealed many advantages, including reduced operational requirements and natural integration with email communications. Moreover, to access IT resources from outside an institution and explore inter-institutional collaboration, it is necessary to adapt application communications to network firewall requirements. Due to network management policies, firewalls typically inhibit direct connections from outside the healthcare centre. Accordingly, our approach is supported by IMAP [22], a very common protocol that “behaves” well with firewalls. From a management point of view, to set up a new telemedicine community, only an email account with IMAP support is needed. Then all the system's operational information, including requests, images, reports and responses are kept in this mailbox. The email account is customized with several service folders and an overall data structure that manages the centre. Because the email server is a passive intermediary, the communication model is poll-based. The advantage of using a well-known Internet service is that it is not necessary to build new protocols and data repositories, and IT resources are not required to set up the telemedicine centre. Moreover, using this common service, it is possible to give clients transparent interaction with the Telecardiology centre. In addition, using a client software module, the central setup is accessible to everyone. 3.1 Workflow End-users, namely the technicians responsible for the acquisition of data (ECG or Echocardiograms), are provided with a simple application to upload the examinations into the email centre (Fig. 1 ). It is possible to select a directory containing exams previously received from acquisition equipment via DICOM [23,24], USB, FTP, etc. The files are introduced in a compressed container that is ciphered with a session key. The software can work in automatic mode (i.e. service mode) in which a file system monitor intercepts file creation events and, according to the kind of data, triggers specific actions. For efficient storage and reduced transmission times, individual files placed inside each container are compressed. Finally, the container is attached to an email message and sent to the central email address via SMTP protocol (Fig. 1). In the central mailbox, this encapsulated exam is stored in the Incoming folder until processed. For this, the technician can use his own email account, and despite a special client module being available, he can use any email browser (Outlook, Thunderbird, etc.) to send exams to the centre. The telecardiology reviewing software (Fig. 1) consists of a standalone application with various functions. When this application starts, it requests the user's authentication credentials. The verification process includes consultation with the central database, via IMAP protocol. After successful authentication, the software will check the Incoming folder for service requests associated with the session user. Messages will then be moved to a “pending folder” where they are separated according to its reviewer. At the same time, attachments will be downloaded and decrypted with the respective secret key. The examination data are then analysed and reported. A PDF report document will be generated with the physician's digital signature. The report is then encrypted and deposited in the central outbox area. When desired, the reviewer can use the “send all” button to send the reports to the technician's email account, according to the original SMTP “Return-path” field. 3.2 Security architecture Health information is very sensitive in nature and such an implementation can raise concerns about security and privacy. Therefore, special measures have to be taken to allay those concerns. The central security architecture was the subject of several considerations. Concerning communication, the centre uses a secure SMTP connection (with SSL – Secure Sockets Layer) to establish contact with service requesters (i.e. receive exams) and secure IMAP protocol (via SSL) between the reviewing software and the central mailbox (Fig. 1). Moreover, the transmitted data is itself encrypted. The Advanced Encryption Standard (AES) algorithm is used to encrypt each exam file inside the ZIP archive with a 256-bit key encryption key. Those containers are attached to email messages and remain ciphered in the central mailbox. The only device capable of decoding this information is the reviewing software module with the proper software service key. Because a password is easier for humans to remember, we use a key derived function to transform this human secret data into a sequence of bits suitable for cryptographic algorithms. More precisely, our symmetric key is generated using an extension of the PBKDF2 algorithm [25], which concatenates a given human password with a salt to reduce the risk of a dictionary attack. Next, the result is submitted to an iterative pseudorandom function to obtain a strong symmetric key. The central administrator needs to define the password, which will be used by client sender module to cipher the service request messages. As previously described, this software is also protected with an authentication mechanism based on username and password. The central administrator controls all users and privileges, defining labs and physicians. The administration module also includes several important security services, including users’ action monitoring, access to statistical measures and central backup. 3.3 Reviewing console Considering that we are using SMTP and IMAP as the core of our telemedicine communication infrastructure, we needed to create a special application for the reviewing process. This is made up of two main components: the repository manager and the reviewing modules. In the repository manager it is possible to control three main areas: Incoming, Reports and Processed examinations. In the Incoming (i.e. Inbox) area the cardiologist will see the service requests, including the provenance, message timestamp and the associated exams (Fig. 2 ). It is also possible to define rules to prioritize services according to request provenance or message prioritization flags. Moreover, a doctor can forward an incoming review service to another colleague, for instance, to request a second clinical opinion. Selecting other repository areas, he can also search and see previously processed studies and reports. An ECG diagnostic tool was the first reviewing module developed upon this cloud-based framework (Fig. 3 ) and it supports several popular data formats [26], namely ECG Signals (SCP) [27], HL7 aECG [28] and Mortara XML. Each new exam is accomplished with respective automatic textual interpretations (Fig. 3) and there is a translating mechanism that can improve reporting quality and speed. It is possible to convert string sequences to improve legibility or translate reports to another language, according to a pre-configured translation table. The idea is to provide a near-automatic reporting feature to minimize manual corrections or text-free inputs. Final reports are generated in PDF format, where patient information, client lab identification, ECG signals over a millimetre grid, textual report and the reviewer's signature are included. When completed, each reviewed examination is moved to the outbox area and the next request is loaded from the incoming area. The main interface also shows the list of pending reviewing requests (Fig. 3 – Inbox). This area display is optional but very useful to detect a duplicate request of examination concerning the same patient. Those situations are normal due to technical reasons associated with exam acquisition. Concerning the solution's efficiency, measured as speed plus accuracy in result production and delivery, physicians’ contributions were fundamental to tune the application and internal workflows. The software memorizes the reviewer preferences, including graphical object locations, sizes and curves display layout. A second reviewing module was developed for echocardiography (Fig. 4 ). This Tele-Echo application is similar to the Tele-ECG but with a different visualization component. It supports DICOM format [23,24] and is based on Himage [15], a cardiac PACS (Picture Archiving and Communication Systems) [29,30] used in several centres, currently supporting more than 2 million still frames and videos. The huge volumes of cardiac dynamic image modalities (i.e. the videos) are not easy to transfer in a time and cost-effective way. For instance, a typical echo-cardiogram study with 15 captures of 20 frames/each and a RGB 480*640 matrix can produce a volume of 276MB. By default, the platform compresses exams in ZIP format to reduce transmission times and storage volume data. However, the DICOM private transfer syntax that is used in Himage offers higher compression capabilities without compromising the diagnosis quality [31]. An impressive fact arising from this study was that, in a simultaneous and blind display of the original against the compressed cine-loops, 37% of trials selected the compressed sequence as the best image. This suggests that other factors related to viewing conditions are more likely to influence observer performance than the image compression itself. Several Himage components were adapted to create the Tele-Echo module over IMAP, namely the DICOM viewer and the report modules. It is possible to visualize the image sequences of still and cine-loops, and select frames to be sent to the report area. Several features are available, e.g. image manipulation algorithms (contrast/brightness), a tissue measurement tool, printing, and exporting of images into distinct formats (DICOM3.0 default transfer syntax, AVI, BMP, JPEG). It is also possible to copy a specific image to the clipboard and paste it onto some other external application. In the report area, the user can arrange (or delete) the images selected in the viewer location using a drag-and-drop function. Finally, the output images matrix (2×3 or 3×3) plus textual report and patient elements are saved in a PDF file. The user can customize the base template used to generate the report file, and even include an institution logo and report headers. 3.4 Management module Several features were also developed to support management tasks related to users, data repository, reporting and accounting (Fig. 5 ). Three user profiles are available: Administrator, Physician and Administrative. The Administrative can access only accounting reports for billing purposes and the Physician can use the centre to report examinations but does not have access to management modules. The Administrator can do everything, including user creation/deletion and repository backup/deletion. The repository data management module (Fig. 5 – front) allows the Administrator to monitor the amount of data in the centre, and choose a specific percentage to be stored and/or deleted. He can also select the type of data that will be affected by the selected action, i.e. past exams, reports or both. The backup option creates a local hierarchical repository (e.g. /dataType/ClientName/Date/Files). The report accounting module (Fig. 5 – back) allows access to statistics that are important to support billing. The Administrative user can filter the service requests by provenance and by time interval. A list of results will be extracted from the Central information system, including the institution name, email, date and number of examinations per request. For instance, Fig. 5 shows that 2682 service requests were received (12,088 exams) from a specific client. 3.5 Installation and setup The following steps can easily create a new reviewing centre: 1. Create an email account in an institutional server or in any Internet provider. 2. Open the client console in administration mode. 3. Run a setup procedure, providing the email account credentials, the central password (to encode messages) and choosing the data type supported (ECG/Medical Image). 4. Create users’ accounts. After the installation process is concluded, the user will lose access to the email account via usual clients (Outlook, Webmail, etc.) because the email password will be replaced by a service one. This option is crucial for the centre's integrity, avoiding data loss or corruption, motivated for instance by accidental deletion (rename) of the mailbox folder. The fault-tolerance of the entire system is dependent on the email provider's quality of service. For instance, Gmail offers reliability up to 99.9% [32] which is quite good for a store-and-forward service. 4 Results The framework presented was designed to solve telemedicine demand in small to middle-sized environments, without the need for expensive and complex IT solutions. The rise of the cloud paradigm was the inspiration for the conception of a new solution supported by ubiquitous services. The centre usage evaluation has been centred on the ECG data according to project development priority associated with real world needs, i.e. there are numerous and distributed ECG collecting points. In fact, there are two reasons for examination centres proliferation: electrocardiograph machines have become cheaper; and the professional skills required to perform the exam are not particularly demanding. With the Tele-ECG, the first reviewing module deployed, more than 55,000 studies were reviewed in the last three years (Fig. 6 ). During the first year, informal validation was performed and physicians’ later suggestions were incorporated in the platform. Since then, the number of examinations has been successively increasing up to 2600 examinations/month, on average, in the last 6 months (Fig. 6). In Portugal, we have currently three centres installed with different exploration scenarios and more than twenty service requesters (i.e. client). Fig. 7 presents a histogram of physical distance, in a straight line, from reading centres to client sites. The three review centres are geographically located in Oporto and, as expected, the majority of service requests come from the city itself, which has the highest population density in the North of Portugal. However, there are important clients located in rural areas and the most remote place is 85km east of Oporto, representing 1h 15m of travelling time by car (Fig. 7 – 60km of distance/4.8% of exams). Another analysed item was the service request distribution throughout the day (Fig. 8 ). As expected, the vast majority of requests arrive during office hours with two peaks, one in the morning and another in the afternoon. However, we can observe that the ubiquity of the solution also allows it to be used during the night. One of these centres receives, on average, 12.1 examinations per incoming service request and returns 10.8 reports per response. A second centre has been used more for urgent requests, with an average of 3.0 examinations per incoming request and 3.4 reports per response. In Angola, a single reading centre is installed in Luanda, providing reviewing services for two clients. The centre was installed 18 months ago and has reported about 5.000 examinations until now. User feedback has been very positive and in the future, the project will include sixteen more clients distributed throughout the country. Analysing the solution's workflow efficiency, namely the time associated with transmissions and report generation times, some measures were performed. Concerning examination analysis and report creation, we measured the physicians’ operating times and, on average, they are 2.9 ECG/min. Those values are possible because the application workflow and reviewing tools were optimized to improve productivity and in many normal examinations, the cardiologist only needs to observe the signals and validate the textual report suggested with one mouse click (i.e. sign the report). Similar operations have been replicated using a traditional email workflow. Opening the client's Outlook email, downloading the ECG exam to local hard disk, opening the file with an ECG viewer and creating a report took about 3min per exam. The reviewing process using developed Tele-ECG solution is approximately 9 times more efficient than a similar operation using a workflow based on separated components (email clients, viewer and reporting tools). We must also stress that the usage of simple store-and-forward email messages, using commercial-off-the-shelf applications, does not promote optimized reviewing procedures. Concerning the size of messages, compressed ECG files can vary between 30 and 140Kbytes, according to their original format, and the PDF report never exceeds 60Kbytes, including wave representation. With those figures, receiving exams or sending reports is almost instantaneous using current network links. The same could not be stated regarding echocardiography image modality. Nevertheless, using our DICOM private transfer syntax to upload exams to the centre, it is possible to compress the volume data significantly. The average file size is about 290Kbytes, including multi-frame captures. In a telework environment, supported by a 20Mbits ADSL (Asymmetric Digital Subscriber Line), for instance, a study with 10 cine-loops takes typically less then 15s to download, decrypt, decompress and display, including the overhead introduced with an encrypted SSL (Secure Sockets Layer) channel. From the above results and considering “store and forward” scenarios, it is possible to have efficient Tele-ECG and Tele-Echo telemedicine scenarios. 5 Conclusions With normal use of Internet services, physicians now have tools that allow them to remotely access patients’ information, fostering telemedicine, telework and collaborative work environments. However, sometimes, healthcare professionals do not adopt telemedicine or telework platforms if they need to invest in major IT infrastructure and its maintenance. This restriction is especially important for small and medium-sized labs. The proposed telecardiology platform is based on ubiquitous email services. The main idea was to decouple the typical centralized database of telemedicine platforms and replace it with a normalized mailbox. The result is a secure telemedicine platform that is easily installed by end-users due to its extremely reduced operational requirement. The solution's simplicity, associated with its usability and mobility, has been captivating new users. The three years spent using this solution in Portugal and Angola are good indicators of its robustness and of user satisfaction. Authors’ contributions form All authors equally participated in solution development, results analysis and drafting of all sections of the manuscript. All authors have read and agreed to the paper being submitted as it is. Competing interests None declared. Summary points “What was already known on the topic?” • The outsourcing of cardiology procedures reporting is a fast-growing market. • Telecardiology solutions are based on dedicated hardware and dedicated applications, requiring technical skill and IT resources to install and maintain. • The deployment of telemedicine projects, namely involving ECG and Echocardiograms, is many times hindered by financial, operational and technological constraints. • There are missing teleradiology solutions that support reviewing workflows in a simple, efficient and secure manner. “What this study added to our knowledge?” • The successful use of a telecardiology solution that provides highly optimized reviewing processes, using public available Internet services. • This work shows that it is possible to have a low-cost and secure telecardiology solution based on Internet ubiquitous services to support remote diagnosis and reporting, on a daily basis. • The proposed solution fits well in distinct usage scenarios, from small low-cost reviewing centres to more demanding environments. • Internet cloud services can be increasingly used in telemedicine system, if robustness and security aspects are guaranteed. Three years of exploitation in Portugal and Angola demonstrates the effectiveness and the robustness of this solution. References [1] K. Nikus The role of continuous monitoring in a 24/7 telecardiology consultation service – a feasibility study J. Electrocardiol. 42 6 2009 473 480 [2] V. Wade A systematic review of economic analyses of telehealth services using real time video communication BMC Health Serv. Res. 10 2010 233 10.1186/1472-6963-10-233 [3] E.V. Khasanshina M.E. Stachura Socio-economical impact of telemedicine in Russian Federation Int. J. Econ. Dev. 8 3 2006 748 775 [4] V. Daucourt Cost-minimization analysis of a wide-area teleradiology network in a French region Int. J. Qual. Health Care 18 4 2006 287 293 [5] J.L. Monteagudo, O.M. Gil, eHealth for patient empowerment in Europe, EU-Report, 2007. [6] B.I. Reiner Multi-institutional analysis of computed and direct radiography. Part II. Economic analysis Radiology 236 2 2005 420 426 [7] W.G. Bradley Offshore teleradiology J. Am. Coll. Radiol. 1 4 2004 244 248 [8] P.A. Larson M.L. Janower The nighthawk: bird of paradise or albatross? J. Am. Coll. Radiol. 2 12 2005 967 970 [9] B.M. William Nighthawks across a flat world: emergency radiology in the era of globalization Ann. Emerg. Med. 50 5 2007 545 549 [10] L.J. Caffery A.C. Smith A literature review of email-based telemedicine Stud. Health Technol. Inform. 161 2010 20 34 [11] V. Della Mea Internet electronic mail: a tool for low-cost telemedicine J. Telemed. Telecare 5 2 1999 84 89 [12] H.S. Fraser TeleMedMail: free software to facilitate telemedicine in developing countries Stud. Health Technol. Inform. 84 Pt 1 2001 815 819 [13] G. Weisser Standardization of teleradiology using Dicom e-mail: recommendations of the German Radiology Society Eur. Radiol. 16 3 2006 753 758 [14] L. Caffery A.C. Smith P.A. Scuffham An economic analysis of email-based telemedicine: a cost minimisation study of two service models BMC Health Serv. Res. 8 2008 107 [15] C. Costa Design, development, exploitation and assessment of a Cardiology Web PACS Comput. Methods Programs Biomed. 93 3 2009 273 282 [16] C. Costa A transcontinental telemedicine platform for cardiovascular ultrasound Technol. Health Care 10 6 2002 460 462 [17] A.C. Smith Telemedicine and rural health care applications J. Postgrad. Med. 51 4 2005 286 293 [18] A.G. Ekeland A. Bowes S. Flottorp Effectiveness of telemedicine: a systematic review of reviews Int. J. Med. Inform. 79 11 2010 736 771 [19] M. Benjamin Y. Aradi R. Shreiber From shared data to sharing workflow: merging PACS and teleradiology Eur. J. Radiol. 73 1 2010 3 9 [20] L.F. Donnelly Quality initiatives: department scorecard: a tool to help drive imaging care delivery performance Radiographics 30 7 2010 2029 2038 [21] J. Klensin, RFC 5321, SMTP – Simple Mail Transfer Protocol, 2008, IETF. [22] M.R. Crispin, RFC 3501: Internet Message Access Protocol – Version 4 Rev1, 2003, IETF. [23] DICOM, Digital Imaging and Communications in Medicine version 3.0, ACR (the American College of Radiology) and NEMA (the National Electrical Manufacturers Association), [24] O.S. Pianykh H.M.S., Boston, MA, USA Digital Imaging and Communications in Medicine (DICOM) – A Practical Introduction and Survival Guide 2008 Springer [25] B. Kaliski, RFC 2898: PKCS #5: Password-Based Cryptography Specification – Version 2.0, IETF Network Working Group, 2000. [26] R.R. Bond A review of ECG storage formats Int. J. Med. Inform. 80 10 2011 681 697 [27] SCP-ECG, Standard Communications Protocol for Computer-Assisted Electrocardiography, CEN European Pre-Standard ENV 1064, 1993. [28] FDA-ECG, FDA XML Data Format Design Specification – Draft – Revision C, 2002. [29] C. Costa A. Silva J.L. Oliveira Current perspectives on PACS and cardiology case study S. Vaidya L.C. Jain H. Yoshida Studies in Computational Intelligence: Advanced Computational Intelligence Paradigms in Healthcare 2007 Springer DE 79 108 (Chapter 5) [30] H.K. Huang PACS and Imaging Informatics: Basic Principles and Applications 2004 Wiley [31] C. Costa Himage PACS: a new approach to storage, integration and distribution of cardiologic images, PACS and imaging informatics Proc. SPIE 5371 2004 277 287 [32] M. Glotzbach, What we learned from 1 million businesses in the cloud, Product Management Director, Google Enterprise, 2008. "
    },
    {
        "doc_title": "From the salivary proteome to the OralOme: Comprehensive molecular oral biology",
        "doc_scopus_id": "84862872419",
        "doc_doi": "10.1016/j.archoralbio.2011.12.010",
        "doc_eid": "2-s2.0-84862872419",
        "doc_date": "2012-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Otorhinolaryngology",
                "area_abbreviation": "MEDI",
                "area_code": "2733"
            },
            {
                "area_name": "Dentistry (all)",
                "area_abbreviation": "DENT",
                "area_code": "3500"
            },
            {
                "area_name": "Cell Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1307"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Objectives: There have been several efforts to identify the protein components of saliva. Some of these studies were conducted in healthy individuals and other in individuals with different oral and systemic disorders. However, a resource compiling and reviewing all of the proteins identified in proteomic studies is still lacking. The aim of this project is to develop such a resource. Design: The proteins identified by proteomic studies were compiled and all information concerning them was manually curated according to \"IPI History search\" and UniProt. Proteins were classified according to gene ontology using PANTHER. The involvement of each protein in disease was scrutinized using DAVID and a classification into protein disease classes was performed. Results: This survey of proteins in the oral cavity lead to the identification of 3397 non-redundant proteins, 605 altered in pathological conditions and 51 present only in disease. These proteins originate from different sources: 3115 from saliva, 990 from oral mucosa and 1929 from plasma. All protein sources contribute with different numbers and types of proteins to identical functions. However, each source produces specific proteins. Examples of the use of this proteomics database of saliva included the analysis of the changes in the proteome associated with periodontitis and a survey of systemic disease potential biomarkers in saliva. Conclusion: The database generated with this work and the information therein stands as a resource for investigators/clinicians studying the oral biology, searching for molecular disease markers, developing diagnostic and prognostic tests, and contributing to the discovery of new therapeutic agents. © 2012 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 271218 291210 291722 31 Archives of Oral Biology ARCHIVESORALBIOLOGY 2012-01-27 2012-01-27 2014-09-05T11:20:03 S0003-9969(12)00002-7 S0003996912000027 10.1016/j.archoralbio.2011.12.010 S300 S300.3 FULL-TEXT 2015-05-13T23:31:12.449479-04:00 0 0 20120701 20120731 2012 2012-01-27T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure table e-component body affil appendices articletitle auth authfirstini authfull authkeywords authlast primabst ref 0003-9969 00039969 false 57 57 7 7 Volume 57, Issue 7 2 853 864 853 864 201207 July 2012 2012-07-01 2012-07-31 2012 Salivary Biology article fla Copyright © 2012 Elsevier Ltd. All rights reserved. SALIVARYPROTEOMEORALOMECOMPREHENSIVEMOLECULARORALBIOLOGY ROSA N 1 Introduction 2 Materials and methods 2.1 Compilation and curation of human OralOme 2.2 Human saliva proteome cataloguing 2.2.1 Gene ontology (GO) analysis 2.2.2 Disease association 3 Results 3.1 Compilation and curation of human oral proteome 3.2 Characterisation of human oral proteome 3.2.1 Saliva, oral mucosae and salivary plasma proteins comparison 3.2.2 Human saliva protein sources 3.3 Strategies for the analysis of molecular changes in disease 3.4 Molecular evidence of potential salivary biomarkers 4 Discussion 4.1 Compilation and curation of human OralOme 4.2 Saliva, oral mucosae and salivary plasma proteins comparison 4.3 Human saliva protein sources 4.4 Strategies for the analysis of molecular changes in disease 4.5 Molecular evidence of potential salivary biomarkers Funding Competing interests Ethical approval Appendix A Supplementary data References HUQ 2007 547 564 N DEALMEIDA 2008 72 80 P GREABU 2009 124 132 M KAUFMAN 2002 197 212 E DENNY 2008 1994 2006 P FANG 2007 5785 5792 X GONZALEZBEGNE 2009 1304 1314 M HARDT 2005 2885 2899 M HARDT 2005 4947 4954 M HU 2005 1714 1728 S HUANG 2004 951 962 C LOO 2010 1016 1023 J MESSANA 2004 792 800 I QUINTANA 2009 822 830 M SIQUEIRA 2008 445 450 W VITORINO 2004 570 575 R WALZ 2006 1631 1639 A WILMARTH 2004 1017 1023 P XIE 2005 1826 1830 H YAN 2009 116 134 W RAMACHANDRAN 2008 80 104 P RAMACHANDRAN 2006 1493 1503 P LARSEN 2007 1778 1787 M BOSTANCI 2010 2191 2199 N BRINKMANN 2011 51 55 O CHI 2009 1453 1474 L COSTA 2010 384 391 P DOWLING 2008 168 175 P GONCALVES 2010 1334 1341 L HAIGH 2010 241 247 B HE 2004 271 278 Q HU 2008 6246 6252 S JOU 2010 41 48 Y LO 2007 101 107 W NEGISHI 2009 1605 1611 A PREZA 2009 161 169 D TURHANI 2006 1417 1423 D WU 2009 636 644 Y YANG 2006 405 407 L CABRAS 2010 2099 2108 T CASTAGNOLA 2010 M FLEISSIG 2009 61 68 Y GIUSTI 2007 1634 1643 L GIUSTI 2007 2063 2069 L HJELMERVIK 2009 342 353 T HU 2007 3588 3600 S ITO 2009 269 271 K RAO 2009 239 245 P RYU 2006 1077 1086 O STRECKFUS 2009 2009 C FLEMING 2006 441 451 K SAHA 2008 1 12 S NISSUM 2007 4218 4227 M KERSEY 2004 1985 1988 P JAIN 2009 136 E THEUNIPROTCONSORTIUM 2010 D214 D219 MI 2009 D204 D210 H THOMAS 2003 2129 2141 P CHO 2000 409 415 R HUANG 2009 44 57 D HUANG 2009 1 13 D LI 2009 D907 D912 S GREABU 2007 209 213 M DELIMA 2003 55 76 A SORSA 2006 306 321 T VITKOV 2009 664 672 L GRANT 2007 157 172 P ISSAD 2010 753 759 T ROSAX2012X853 ROSAX2012X853X864 ROSAX2012X853XN ROSAX2012X853X864XN item S0003-9969(12)00002-7 S0003996912000027 10.1016/j.archoralbio.2011.12.010 271218 2014-09-06T02:08:31.60902-04:00 2012-07-01 2012-07-31 true 1588480 MAIN 12 49241 849 656 IMAGE-WEB-PDF 1 gr5 526968 1593 2500 gr4 1263992 2223 2750 gr3 1152269 3869 2667 gr2 941031 3406 2500 gr1 284067 1582 2500 gr5 59978 360 565 gr4 128437 502 621 gr3 122524 873 602 gr2 97030 770 565 gr1 40944 358 565 gr5 5482 140 219 gr4 8607 164 203 gr3 3472 164 113 gr2 3550 163 120 gr1 5086 139 219 mmc1 mmc1.zip zip 221609 APPLICATION mmc2 mmc2.xls xls 155648 APPLICATION AOB 2738 S0003-9969(12)00002-7 10.1016/j.archoralbio.2011.12.010 Elsevier Ltd Fig. 1 Venn diagram illustrating human OralOme constitution: (a) OralOme main protein sources; (b) oral mucosa protein sources; (c) saliva protein sources. Numbers in parentheses represent the total proteins obtained from a particular source. Underlined numbers represent proteins unique to a particular source relatively to total OralOme. Numbers inside intersections account for proteins common to intersected groups. Numbers with an asterisk represent proteins obtained from patients with oral or systemic diseases. Fig. 2 Classification of saliva, oral mucosae, salivary plasma proteins and total human proteins in PANTHER database according to (a) cellular component (percentage of gene hit against total # component hits); (b) molecular function (percentage of gene hit against total # function hits). Pie chart represents “antioxidant activity” function in detail. (c) Number of proteins with catalytic and binding activities that participate in other molecular functions. Analysis performed with PANTHER classification tool. Fig. 3 Classification of the proteins obtained from different saliva sources according to: (a) molecular function of total number of proteins obtained from each source (percentage of gene hit against total # function hits); (b) molecular function of proteins obtained exclusively from each source relatively to the total number of proteins obtained from saliva samples (percentage of gene hit against total # function hits); (c) biological process (percentage of gene hit against total # process hits). Analysis performed with PANTHER classification tool. Fig. 4 Classification of the proteins obtained from saliva samples from donors suffering various oral diseases according to: (a) molecular function (percentage of gene hit against total # function hits); (b) biological process (percentage of gene hit against total # process hits). Analysis performed with PANTHER classification tool. “Normal OralOme” is understood as the totality of human proteins identified in the oral cavity, except those identified exclusively in pathological conditions. Fig. 5 Human saliva, blood plasma and salivary plasma protein disease class association, determined by DAVID Functional Annotation Tool. Table 1 List of oral cavity proteomic studies analysed by sample origin. Saliva source References N° of identified proteins Condition Health Disease Salivary glands Parotid Gonzalez-Begne et al., 2009 7 431 Preza et al. 2009 36 69 69 Dental caries Siqueira et al. 2008 15 46 Denny et al. 2008 5 989 Ramachandran et al. 2008 21 33 Ryu et al. 2006 49 10 10 Sjögren's syndrome Walz et al. 2006 17 13 Hardt et al. 2005 9 6 Diurnal variation Hardt et al. 2005 8 16 SM/SL Siqueira et al. 2008 15 46 Denny et al. 2008 5 963 Ramachandran et al. 2008 21 56 Walz et al. 2006 17 14 Minor Hjelmervik et al. 2009 45 365 431 Sjögren's syndrome Siqueira et al. 2008 15 56 Gingival crevicular fluid Bostanci et al. 2010 24 66 76 Chronic periodontitis Aggressive periodontitis Oral mucosae Tongue Negishi et al. 2009 35 5 5 HNSCC He et al. 2004 31 12 12 HNSCC Other mucosae Chi et al. 2009 26 962 80 HNSCC Lo et al. 2007 34 11 HNSCC Turhani et al. 2006 37 20 20 HNSCC Whole saliva Brinkmann et al. 2011 25 7 7 HNSCC Haigh et al. 2010 30 10 10 Chronic periodontitis Cabras et al. 2010 40 15 15 Diabetes type 1 Loo et al. 2010 12 2290 Gonçalves et al. 2010 29 25 25 Chronic periodontitis Jou et al. 2010 33 8 8 HNSCC Costa et al. 2010 27 3 3 Chronic periodontitis diabetes type 2 Castagnola et al. 2010 41 15 Preterm newborn Streckfus et al. 2009 50 158 158 Mammary carcinoma Fleissig et al. 2009 42 16 16 Sjögren's syndrome Wu et al. 2009 38 11 11 Aggressive periodontitis Quintana et al. 2009 14 12 Inter-individual variability Ito et al. 2009 47 1 1 Sjögren's syndrome Rao et al. 2009 48 491 61 Diabetes type 2 Yan et al. 2009 20 140 Dowling et al. 2008 28 6 6 HNSCC Hu et al. 2008 32 39 60 HNSCC Ramachandran et al. 2008 21 60 Fang et al. 2007 6 1479 Giusti et al. 2007 43 18 9 Sjögren's syndrome Hu et al. 2007 46 26 28 Sjögren's syndrome Giusti et al. 2007 44 9 Diffuse systemic sclerosis Larsen et al. 2007 23 45 Ramachandran et al. 2006 22 45 Yang et al. 2006 39 2 2 Lichen Planus Walz et al. 2006 17 29 Xie et al. 2005 19 423 Hu et al. 2005 10 266 Vitorino et al. 2004 16 34 Messana et al. 2004 13 7 Huang 2004 11 25 10 Oral cavity bleeding Wilmarth et al. 2004 18 90 Abbreviations: HNSCC, head and neck squamous cell carcinoma. From the salivary proteome to the OralOme: Comprehensive molecular oral biology Nuno Rosa a ⁎ Maria José Correia a Joel P. Arrais c Pedro Lopes c José Melo c José Luís Oliveira c Marlene Barros a b a Health Sciences Institute, Portuguese Catholic University – Viseu, Portugal b Centre for Neurosciences and Cell Biology, University of Coimbra, Portugal c Department of Electronics, Telecommunications and Informatics (DETI), Institute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro, Portugal ⁎ Corresponding author at: Health Sciences Institute, Catholic Portuguese University, Estrada da Circunvalação, 3504-505 Viseu, Portugal. Tel.: +351 232430200; fax: +351 232428344. Objectives There have been several efforts to identify the protein components of saliva. Some of these studies were conducted in healthy individuals and other in individuals with different oral and systemic disorders. However, a resource compiling and reviewing all of the proteins identified in proteomic studies is still lacking. The aim of this project is to develop such a resource. Design The proteins identified by proteomic studies were compiled and all information concerning them was manually curated according to “IPI History search” and UniProt. Proteins were classified according to gene ontology using PANTHER. The involvement of each protein in disease was scrutinized using DAVID and a classification into protein disease classes was performed. Results This survey of proteins in the oral cavity lead to the identification of 3397 non-redundant proteins, 605 altered in pathological conditions and 51 present only in disease. These proteins originate from different sources: 3115 from saliva, 990 from oral mucosa and 1929 from plasma. All protein sources contribute with different numbers and types of proteins to identical functions. However, each source produces specific proteins. Examples of the use of this proteomics database of saliva included the analysis of the changes in the proteome associated with periodontitis and a survey of systemic disease potential biomarkers in saliva. Conclusion The database generated with this work and the information therein stands as a resource for investigators/clinicians studying the oral biology, searching for molecular disease markers, developing diagnostic and prognostic tests, and contributing to the discovery of new therapeutic agents. Keywords Saliva Salivary proteome Oralome Bioinformatics Oral biology Oral pathology Systems biology 1 Introduction The human oral cavity is a complex ecosystem where host, microbial and external factors combine in a dynamic equilibrium which is reflected in saliva. The understanding of oral biology hinges upon the compilation and integration of all information generated by high-throughput techniques, particularly proteomic studies of saliva. Saliva is an aqueous biological fluid, with many and diverse functions, essential for the maintenance of healthy oral tissues. 1 It consists predominantly of water (99%) but includes a complex mixture of electrolytes (sodium, potassium, calcium, chloride, magnesium, bicarbonate and phosphate), proteins (enzymes, immunoglobulins and other antimicrobial factors, mucosal glycoproteins, albumin and other polypeptides and oligopeptides) and, in even smaller amounts, glucose and nitrogenous products (urea and ammonia). 2 The proteins present in whole saliva are secreted mainly from 3 pairs of major salivary glands: the parotid, the submandibular, and the sublingual, making up approximately 90% of total salivary secretion. The remaining proteins result from the minor salivary glands (located at various oral mucosal sites), 3 the gingival crevicular fluid (GCF) and the oral mucosa (including tongue and other oral mucosae). Saliva also has plasmatic proteins. There are several ways by which plasma proteins can reach saliva. The most common include passive diffusion, ultrafiltration (which occurs through the tight junctions between the cells) and as a result of GCF outflow. 4 The complete role of salivary proteins in oral physiology and as indicators of disease states is still poorly understood. The determination of the salivary proteome enables not only the functional characterisation of saliva, and thus the clarification of its role in oral biology, but also the identification of disease biomarkers. The fact that whole saliva includes plasma derived proteins and exfoliated epithelial cells, has led to the suggestion that it may in the future provide a means for diagnosis of conditions that currently require blood or tissue samples. In the last decade, the development of powerful and discriminating proteomics techniques, allowed an exponential increase in the identification of protein components of saliva. Some studies include exclusively samples from healthy individuals 5–23 and others use samples from individuals with several oral 24–39 and systemic 27,40–50 disorders. Despite the vast amount of data generated, these are still scattered and there is, no database publicly available with a compilation and characterisation of the proteins identified by proteomic studies. Bioinformatics tools have a key role in exploring these data by modelling the inter-relationships between the sequences, structures and functions of proteins, extracting biological meaning from the data generated by these studies. 51 The aim of this work was to compile and curate the proteins reported in proteomic studies of saliva and characterize the saliva proteome (OralOme). An integrated database was created in a local repository and will be publically available to the scientific community shortly. We expect this study and the associated database to be a valuable resource for investigators aiming to clarify the oral biology, identify molecular disease markers, develop diagnostic tests and improve prognostic, as well as providing information for the design of biological pathways setting the ground for the discovery of new therapeutic agents. 2 Materials and methods 2.1 Compilation and curation of human OralOme To create the database the first step was to compile the proteins identified in published proteomic studies of saliva samples. We analysed several bibliographic references of proteomic studies in which a complete lists of proteins was provided. 5–50 These studies used saliva from different sources and are classified in Table 1 regarding sample type. Because proteins present in whole saliva could originate from blood transudates, we checked plasma proteomic studies 52,53 to verify which of the proteins present in salivary proteomic results could have originated from the plasma. To this subset of proteins we called the salivary plasma. In this paper, “saliva” is the set of proteins identified by proteomics techniques, obtained from salivary glands, crevicular fluid and whole saliva samples. “Whole saliva” is understood as saliva whose samples were not collected directly from the gland ducts or the gingival crevice. The proteins identified in the different studies were compared and repeated entries eliminated. All proteins showing expression changes in certain pathologies were stored with an indication of up-regulation or down-regulation under these conditions. Since there are differences in expression of proteins dependent on donor age, whenever possible we also recorded the age group for the sample donors. We registered the existence and type of post translational modifications of salivary proteins due to the crucial role these may have in protein function and consequently in oral physiology. From the first publication of saliva proteomes, many of the original identified proteins, catalogued as different entries in biological databases, have been merged with others and some were deleted due to misidentification. Therefore, all information concerning the identified proteins was manually curated and updated as of February 2011. The update of the IPI (International Protein Index) entries was carried out with “IPI History search” tool 54 All other updates have been made according to UniProt database 55,56 The curated list of proteins identified in this work was stored in the database OralCard—Web Information System for Oral Health developed for this purpose. 2.2 Human saliva proteome cataloguing 2.2.1 Gene ontology (GO) analysis The proteins in the database were classified according to molecular function, biological process and cellular component by using the PANTHER database 57,58 PANTHER is a unique resource that classifies genes and proteins by their functions, using published scientific experimental evidence and evolutionary relationships inferred by curators with the goal of predicting function even in the absence of direct experimental evidence. PANTHER applies both software tools and manual curation to perform these inferences as accurately as possible, and to keep them up-to-date as new experimental results accumulate. To accomplish this task we conducted a Batch ID Search with the PANTHER Classification Tool for each group of proteins, using the AC UniProtKB identifiers (without protein isoforms). We also used the PANTHER gene expression data analysis tool to compare classifications of multiple clusters of lists with a reference list (total number of human proteins in PANTHER database) to statistically determine over- or under-representation of PANTHER classification categories. Each list is compared to the reference list using the binomial test 59 for each GO cellular component, molecular function or biological process term in PANTHER. 2.2.2 Disease association Human saliva and plasma proteins were scrutinized for their involvement in diseases and grouped into protein disease classes by the use of DAVID Bioinformatics Resources 6.7 60,61 DAVID bioinformatics resources consist of an integrated biological knowledgebase and analytic tools aimed at systematically extracting biological meaning from large gene/protein lists. For this task, the lists of proteins derived from the saliva, salivary plasma (plasma proteins found in saliva) and total plasma (all plasma proteins) were subjected to Functional Annotation Tool. The proteins were annotated according to “genetic association database disease class” and presented as a “chart report”. This is an annotation-term-focused view which lists annotation terms and their associated genes under study. To avoid over counting duplicated genes, the Fisher exact statistics is calculated based on corresponding DAVID gene IDs by which all redundancies in original IDs are removed. All results of the Chart Report have to pass the thresholds (by default, Max.Prob.≤0.1 and Min.Count≥2) in the Chart Option section to ensure only statistically significant ones are displayed. 3 Results 3.1 Compilation and curation of human oral proteome This work leads to the documentation of 3397 non-redundant proteins that may be found in the oral cavity. Of these, 3115 were found in saliva, 990 in oral mucosa (17 in tongue mucosa 31,35 and 984 in other oral mucosae 26,34,37 ) and 1929 were plasmatic proteins found in the oral cavity 52,53 (Fig. 1 a). The numbers with asterisks in Fig. 1 represent 707 proteins identified in patients with several oral and systemic conditions. Of these proteins, 637 were identified in samples of saliva (73 36,49 in parotid glands, 431 45 in minor glands, 74 24 in crevicular fluid and 228 25,27–30,32,33,38–44,46–48,50 in whole saliva samples) and 118 in oral mucosa (17 in tongue 31,35 and 106 in other oral mucosa 26,34,37 ). The complete list of the conditions and respective proteomic studies included in the analysis is available in Table 1. Fifty-one of the proteins were identified exclusively in pathological situations. The human parotid 5,7–9,15,17,21,36,49 present 1193 proteins (including 431 from parotid exosome 7 ). Since submandibular/sublingual (SM/SL) 5,15,17,21 glands present 999 proteins, they contribute to total protein number almost in the same proportion, with 701 proteins being shared between them. Salivary minor glands 5,15,45 with 554 proteins and GCF 24 with 100 proteins contribute to salivary proteome to a much lesser extent (Fig. 1c). All sources contribute with unique proteins (Fig. 1—underlined numbers). Note that 1283 proteins (40%) appeared only in saliva, and were absent in blood plasma or oral mucosae samples. Of all the proteins obtained from saliva samples, 470 are not found in the salivary glands, GCF or oral mucosae. The parotid glands secrete a higher percentage (15.3%) of unique proteins compared to SM/SL (8.5%) and minor (5.4%) glands as well as GCF (8%). Of the proteins present in the oral cavity 96 have been identified as glycosylated and 9 of these were only found in studies of post translational modification. 21–23 3.2 Characterisation of human oral proteome The human proteins present in the oral cavity have several origins including the salivary glands, oral mucosa and even blood plasma. The proteins associated to each of these compartments were characterised according to diverse aspects including cellular component, molecular function and biological process in order to understand their functional organization. 3.2.1 Saliva, oral mucosae and salivary plasma proteins comparison When the different sources of oral cavity proteins are compared according to cellular component (GO: 0005575 – “The part of a cell or its extracellular environment in which a gene product is located”) (Fig. 2 a) we found that most of the proteins in the OralOme are intracellular. Most of these intracellular proteins are derived from the oral mucosa (60.2%), which also contributes to the oral proteome with a higher percentage of ribonucleoprotein complexes (12.4%) than saliva (5.9%) and salivary plasma (7.7%). Furthermore, we can see that the oral cavity has a large percentage of protein complexes (almost twice what would be expected relative to total human proteins). A comparison of the different sources of oral cavity proteins regarding molecular function (GO: 0003674 – “Elemental activities, such as catalysis or binding, describing the actions of a gene product at the molecular level”), using PANTHER Classification System is presented in Fig. 2b. In saliva, oral mucosa and salivary plasma, proteins participate in the same sort of functions. However, each source contributes with a different number and type of proteins that participate in common functions, which reflects the specific role of each of these sources to salivary functions. Fig. 2b shows that a large percentage of the proteins present in the oral cavity are involved in the molecular functions “catalytic activity” and “binding”. Because some proteins can have more than one function, we tried to find which of the proteins with catalytic and binding activities are also involved in other molecular functions (Fig. 2c). We confirmed that the molecular functions “catalytic activity” and “binding” are quite broad and redundant since the proteins involved in them also have other activities. Note that many proteins with catalytic activity also have binding activity. Likewise, many of the proteins with binding activity are also structural molecules, enzyme regulators or have catalytic activity, amongst other functions. Looking at the antioxidant function we see that, despite the small percentage of proteins involved in it, all sources contribute with proteins to carry out this important function of saliva. Furthermore, the oral cavity has a higher percentage (almost four times, p ≤0.05) of proteins with antioxidant activity than would be expected relative to total human proteins (results not shown). The contribution of each source of OralOme to the antioxidant activity function is almost the same (salivary plasma 35%, saliva 30% and oral mucosae 30%). More interesting is the fact that amongst the plasma proteins with antioxidant function, almost all appear in the oral cavity (salivary plasma). Only 5% of all plasma proteins with antioxidant function are not present in the oral cavity (Fig. 2b—pie chart). We can also see that some of these proteins with antioxidant function (e.g. glutathione peroxidase and extracellular superoxide dismutase [Cu–Zn]), are produced in all sources of saliva, except in the crevicular fluid (Fig. 3 b). 3.2.2 Human saliva protein sources Most of the proteins present in OralOme come directly from saliva sources (major and minor salivary glands and GCF). Thus, these proteins have a prominent role in saliva functions and consequently in oral biology. In order to evaluate the specific contribution of different saliva sources to the saliva functions, we investigated the molecular functions of the proteins produced from each source and the biological processes (GO: 0008150 – “Any process specifically pertinent to the functioning of integrated living units: cells, tissues, organs, and organisms. A process is a collection of molecular events with a defined beginning and end”) in which they are involved, using PANTHER Classification System (Fig. 3). The proteins produced by the major and minor salivary glands contribute to the same molecular functions. However, the number of proteins dedicated to each molecular function varies slightly amongst the major salivary gland proteins and at a greater degree, between the proteins of all the major salivary glands and the proteins of the minor ones. This variation consists mainly of an increase in the number of proteins involved in catalytic and structural activities and a decrease in the number of proteins involved in enzyme regulatory and receptor activities (Fig. 3a). The gingival crevicular fluid does not contain proteins involved in ion channel and receptor activities neither translation regulator activity. Furthermore, the number of proteins from GCF with structural molecular activity is markedly increased in relation to the proteins produced by the salivary glands. We can also observe a sharp increase in the number of unique proteins in GCF with transcription regulatory activity (Fig. 3b). Regarding biological processes involving proteins obtained from each saliva source, it appears that they are very similar and only the gingival crevicular fluid differs significantly compared to other sources (Fig. 3c). Note that the GCF has an increase of proteins involved in response to stimuli (8.4% versus 5.9% of whole saliva) but has almost the same percentage of proteins engaged in the “immune system process” as other saliva sources. Moreover, we observed that most proteins involved in immune response in saliva, were also present in plasma (results not shown). Another difference is that minor salivary glands produce a higher percentage (25.1% versus 18% in parotid, 19.6% in SM/SL and 14.8% in GCF) of proteins involved in metabolic processes. 3.3 Strategies for the analysis of molecular changes in disease One of the most interesting uses of a database compiling all the proteomic data available for the oral cavity is to enable the analysis of proteomics data. This database is fundamental to detect the possible changes in the quantity and type of proteins present in saliva samples from diseased individuals. This kind of approach allows the identification of functions/biological processes compromised in the pathologies analysed. In Fig. 4 we show the classification of the proteins present in saliva samples from donors with various oral diseases, according to molecular functions and biological processes, and compare it to the normal oral proteome (all proteins of the oral cavity except those identified exclusively in diseases). In each of these oral diseases, we can identify molecular functions and biological processes markedly changed from the normal. For instance, in chronic periodontitis there is a significant change in proteins with structural molecular activity (Fig. 4a) and in proteins involved in cellular component organization and immune system biological processes (Fig. 4b). 3.4 Molecular evidence of potential salivary biomarkers In order to understand how the plasma and salivary proteins reflect the state of human health, and therefore their relative value as a diagnostic medium, the proteins obtained from plasma and saliva were classified according to their involvement in diseases and grouped into classes using the DAVID web tool (Fig. 5 ). Salivary proteins are associated with almost the same classes of diseases as plasma proteins. Moreover, the percentage of proteins associated with each disease class is very similar in saliva and salivary plasma. As expected, we find a higher percentage of proteins associated with cardiovascular diseases in plasma (17%) than in saliva (11%). We can also see that there is a higher percentage of proteins associated with immune system diseases in saliva (13%) than in plasma (7%). More unusual is the presence of a higher percentage of proteins related to neurological diseases in saliva (9.3%) than in plasma (5.8%). In fact, we find, in saliva, an extensive range of proteins involved in Alzheimer's, Parkinson's and Huntington's disease. Our analysis also identified salivary proteins involved in diseases related to vision, which did not appear in plasma, as is the case of ornithine aminotransferase (P04181), which, when defective, is responsible for hyperornithinemia with gyrate atrophy of choroid and retina (HOGA) [MIM:258870]. 4 Discussion 4.1 Compilation and curation of human OralOme Most of the attempts to elucidate the oral proteome, have consisted of very specific studies focused on particular situations 24–50 or more general studies where saliva is collected in well-defined conditions. 7–20,24,25 Some attempts were made at a broader approach. The first of these attempts was by Denny et al. in 2008. 5 They identified 1166 proteins from saliva samples collected directly from major salivary glands, developing the Salivary Proteome Knowledge Base. Loo et al. compiled 2290 proteins in saliva 12 and compared it with proteins identified in plasma. Another workgroup, led by Li, published a body fluid database 62 that includes salivary proteins (Sys-BodyFluid). Despite the innovative attempt to integrate information from the application of proteomics to different body fluids, the data pertaining to saliva was limited to eight studies conducted from 2004 to 2008, identifying 2161 proteins. In the Sys-BodyFluid database the proteins were classified with the IPI code, which is now discontinued and hinders the match of these proteins with those stored in the major protein databases such as UniProtKB. It is known that saliva is more complex than what is secreted by the salivary glands and may have proteins from the oral mucosa and even blood plasma. Our database includes proteins from all of these sources both under physiological and pathological conditions. This database offers more than just a list of proteins including further information on each protein. 4.2 Saliva, oral mucosae and salivary plasma proteins comparison The amount and type of proteins obtained from each of the oral protein sources, elucidates their contribution to the physiology of the oral cavity leading to the interpretation of the OralOme. Saliva, oral mucosa and plasma proteins probably participate in the same sort of functions. However, each source contributes with a different set of proteins. All sources have unique proteins that may indicate specific functions carried out by them and therefore, a particular role in the oral cavity physiology. Since some proteins found in the oral cavity are not found in the salivary glands, GCF or oral mucosae, we set forth the hypothesis that, because the salivary gland acinar complexes are “leaky” epithelia, many of these proteins must come from plasma entering in saliva by diffusion across tight junction complexes. We noted that a large percentage of the proteins that appear in the oral cavity are intracellular and are derived from the oral mucosa, which is not surprising considering that saliva collects a large number of proteins resulting from the constant scaling that occurs in the oral cavity both from oral mucosa and salivary gland ducts. Other evidence that supports this is the fact that our results indicate that the mucosa contributes to the oral proteome with a higher percentage of ribonucleoprotein complexes than saliva and salivary plasma, presumably due to cell lysis. It is well established that ribosomes are abundant in the cell and it is estimated that the majority of eukaryotic cellular RNA is ribosomal RNA. So, there is a great contribution, in terms of the mass, of these proteins to the whole cell content. Moreover, we saw that the oral proteome has a percentage of protein complexes higher than what would be expected relative to total human proteins. We propose that this reflects the supramolecular complexes existing in the complex membrane organelle system and in the rough endoplasmic reticulum and therefore structures which are intracellular. Although most of the proteins present in the oral cavity are involved in “catalytic activity” and “binding” these functions are not necessarily the most representative. These functions are quite broad and redundant since the proteins involved in them also have other activities. This is a good example of the difficulties that still exist in the cataloguing of protein functions, as well as, the definition of the different ontologies, that can lead to the misinterpretation of results. The higher percentage of proteins with antioxidant activity observed in the oral cavity in relation to total human proteins is probably explained by the fact that the oral cavity is particularly exposed to oxidative stress. 63 4.3 Human saliva protein sources The absence of proteins in gingival crevicular fluid involved in ion channel and receptor activities can be explained by the fact they are essentially integral membrane proteins, and therefore cannot be released to the extracellular fluid which composes the GCF. Moreover, the absence of proteins with translation regulator activities from the GCF may be related to the lower percentage of proteins with this function, found in blood plasma, since the GCF is formed when fluid exudes from the vessels of the microcirculation into the sulcus or dental pocket. 64 The GCF exhibits a marked increase in the number of proteins with structural molecular activity when compared to the proteins produced by the salivary glands. This fact is probably due to the remodelling of extracellular matrix occurring in healthy periodontium and to the increased degradation of this matrix observed in oral diseases such as periodontitis. 65 Proteins resulting from this degradation accumulate in the GCF. The observed higher number of unique proteins in GCF with transcription regulatory activity reflects the need for an increased protein synthesis in order to offset the degradation taking place in gingival crevice which is consistent with the increase in cellular processes and cellular component organization processes observed in GCF. The increase of proteins identified in GCF involved in the response to stimuli could be explained by the fact that the gingival sulcus is a “war zone” where the response of the host tissue to microbial challenges is present. 66 On the other hand, it is strange that the GCF has almost the same percentage of proteins engaged in the “immune system process” as other saliva sources. These results suggest that, in the GCF the result of “war” is more noticeable than the “warriors” themselves. In other words, the GCF may be a good medium for studying the degradome resulting from the host's immune response to microbial aggression. This is corroborated by the fact that there is an increased amount of GCF produced in periodontal disease 24 and, in this instance, periodontal pockets comprise not only the microorganisms growing in the sulcus, but also the result of an overly aggressive immune response against these microorganisms. Due to the equal participation of different sources of saliva in the immune response, we can also speculate that the participation in the “immune system process” is shared by the whole saliva (because of its importance) and the proteins involved can be recruited from plasma by infiltration in the epithelium of glands and oral mucosa. The presence in plasma of most of the proteins involved in immune response carried out in saliva supports that idea. With this work, we found that all sources of saliva, especially the minor salivary glands, have a high percentage of proteins involved in metabolism. This reveals the interest in saliva as an object in metabolomics studies and in the understanding of the molecular mechanisms of diseases related to defects in metabolism, such as diabetes. Most of the glycosylated proteins found in oral cavity, are involved in metabolic diseases. This influence is through their role in the complement and coagulation cascades pathways (results not shown). Alterations in these pathways are known to be related with poor wound healing. 67 The characteristic diabetic hyperglycaemic state may favour the glycosylation 68 of the proteins present in saliva and therefore be responsible for the difficulty in healing observe in the oral cavity of diabetic patients. 4.4 Strategies for the analysis of molecular changes in disease The analysis of proteomics data revealing changes in the quantity and type of proteins present in the saliva of diseased patients can lead to the identification of functions/biological processes compromised in these conditions. This analysis is substantially easier and more reliable if there is a methodology that can be reproduced in different situations, eliminating most of the analyser's subjectivity. We presented an example of how proteomics data can be analysed and interpreted. This approach allowed us to identify molecular support for the importance of proteins involved in cellular component organization and immune system biological processes as well as structural molecular functions in the molecular mechanisms of periodontitis. Further analyses may lead to the identification of possible molecular markers and even potential therapeutic targets. 4.5 Molecular evidence of potential salivary biomarkers Salivary proteins are associated with nearly the same classes of diseases as plasma proteins, indicating that molecular markers for all these diseases can be found in saliva, enhancing the possibility of its use for diagnosis of both oral and systemic diseases. The presence, in saliva, of a higher percentage of proteins related to neurological, immune system and ophthalmic diseases may be indicative of a strong potential of saliva as a source of molecular markers essential for designing strategies for non-invasive diagnosis of these diseases. In spite of remarkable advances in bioinformatics techniques used in systems biology, there are still clear gaps in the path between proteomics results and the elucidation of molecular mechanisms involving the identified proteins. This void can only be overcome with studies, making use of bioinformatics techniques, always based on human interpretation in the light of existing literature. Information about the oral cavity is dispersed through different databases focused on more general systems. In addition to being dispersed, the data are not always standardized, which makes their integration and comprehensive study a difficult task. The present work is the largest and most comprehensive survey of proteins of the oral cavity, covering proteins from all salivary sources both under physiological and pathological conditions, organized in an integrated database. Funding None. Competing interests None declared. Ethical approval Not required. Appendix A Supplementary data Supplementary data associated with this article can be found, in the online version, at doi:10.1016/j.archoralbio.2011.12.010. Appendix A Supplementary data A complete spread sheet listing the proteins identified in oral cavity is available in the supplemental material along with a list of procedures carried out in order to manually curate all outdated proteins. References 1 N.L. Huq K.J. Cross M. Ung H. Myroforidis P.D. Veith D. Chen A review of the salivary proteome and peptidome and saliva-derived peptide therapeutics Int J Pept Res Ther 13 September (4) 2007 547 564 2 P.D.V. de Almeida A.M.T. Grégio M.A.N. Machado A.A.S. de Lima L.R. Azevedo Saliva composition and functions: a comprehensive review J Contemp Dent Pract 9 3 2008 72 80 3 M. Greabu M. Battino M. Mohora A. Totan A. Didilescu T. Spinu Saliva—a diagnostic window to the body, both in health and in disease J Med Life 2 June (2) 2009 124 132 4 E. Kaufman I.B. Lamster The diagnostic applications of saliva—a review Crit Rev Oral Biol Med 13 2 2002 197 212 5 P. Denny F.K. Hagen M. Hardt L. Liao W. Yan M. Arellanno The proteomes of human parotid and submandibular/sublingual gland salivas collected as the ductal secretions J Proteome Res 7 May (5) 2008 1994 2006 6 X. Fang L. Yang W. Wang T. Song C.S. Lee D.L. DeVoe Comparison of electrokinetics-based multidimensional separations coupled with electrospray ionization-tandem mass spectrometry for characterization of human salivary proteins Anal Chem 79 August (15) 2007 5785 5792 7 M. Gonzalez-Begne B. Lu X. Han F.K. Hagen A.R. Hand J.E. Melvin Proteomic analysis of human parotid gland exosomes by multidimensional protein identification technology (MudPIT) J Proteome Res 8 March (3) 2009 1304 1314 8 M. Hardt L.R. Thomas S.E. Dixon G. Newport N. Agabian A. Prakobphol Toward defining the human parotid gland salivary proteome and peptidome: identification and characterization using 2D SDS-PAGE, ultrafiltration, HPLC, and mass spectrometry Biochemistry 44 March (8) 2005 2885 2899 9 M. Hardt H.E. Witkowska S. Webb L.R. Thomas S.E. Dixon S.C. Hall Assessing the effects of diurnal variation on the composition of human parotid saliva: quantitative analysis of native peptides using iTRAQ reagents Anal Chem 77 August (15) 2005 4947 4954 10 S. Hu Y. Xie P. Ramachandran R.R. Ogorzalek Loo Y. Li J.A. Loo Large-scale identification of proteins in human salivary proteome by liquid chromatography/mass spectrometry and two-dimensional gel electrophoresis-mass spectrometry Proteomics 5 April (6) 2005 1714 1728 11 C.-M. Huang Comparative proteomic analysis of human whole saliva Arch Oral Biol 49 December (12) 2004 951 962 12 J.A. Loo W. Yan P. Ramachandran D.T. Wong Comparative human salivary and plasma proteomes J Dent Res 89 October (10) 2010 1016 1023 13 I. Messana T. Cabras R. Inzitari A. Lupi C. Zuppi C. Olmi Characterization of the human salivary basic proline-rich protein complex by a proteomic approach J Proteome Res 3 4 2004 792 800 14 M. Quintana O. Palicki G. Lucchi P. Ducoroy C. Chambon C. Salles Inter-individual variability of protein patterns in saliva of healthy adults J Proteom 72 July (5) 2009 822 830 15 W.L. Siqueira E. Salih D.L. Wan E.J. Helmerhorst F.G. Oppenheim Proteome of human minor salivary gland secretion J Dent Res 87 May (5) 2008 445 450 16 R. Vitorino M.J.C. Lobo J.A.R. Duarte A.J. Ferrer-Correia P.M. Domingues F.M.L. Amado Analysis of salivary peptides using HPLC-electrospray mass spectrometry Biomed Chromatogr 18 October (8) 2004 570 575 17 A. Walz K. Stühler A. Wattenberg E. Hawranke H.E. Meyer G. Schmalz Proteome analysis of glandular parotid and submandibular-sublingual saliva in comparison to whole human saliva by two-dimensional gel electrophoresis Proteomics 6 March (5) 2006 1631 1639 18 P.A. Wilmarth M.A. Riviere D.L. Rustvold J.D. Lauten T.E. Madden L.L. David Two-dimensional liquid chromatography study of the human whole saliva proteome J Proteome Res 3 October (5) 2004 1017 1023 19 H. Xie N.L. Rhodus R.J. Griffin J.V. Carlis T.J. Griffin A catalogue of human saliva proteins identified by free flow electrophoresis-based peptide separation and tandem mass spectrometry Mol Cell Proteomics 4 November (11) 2005 1826 1830 20 W. Yan R. Apweiler B.M. Balgley P. Boontheung J.L. Bundy B.J. Cargile Systematic comparison of the human saliva and plasma proteomes Proteom Clin Appl 3 January (1) 2009 116 134 21 P. Ramachandran P. Boontheung E. Pang W. Yan D.T. Wong J.A. Loo Comparison of N-linked glycoproteins in human whole saliva, parotid submandibular, and sublingual glandular secretions identified using hydrazide chemistry and mass spectrometry Clin Proteom 4 December (3–4) 2008 80 104 22 P. Ramachandran P. Boontheung Y. Xie M. Sondej D.T. Wong J.A. Loo Identification of N-linked glycoproteins in human saliva by glycoprotein capture and mass spectrometry J Proteome Res 5 June (6) 2006 1493 1503 23 M.R. Larsen S.S. Jensen L.A. Jakobsen N.H.H. Heegaard Exploring the sialiome using titanium dioxide chromatography and mass spectrometry Mol Cell Proteomics 6 October (10) 2007 1778 1787 24 N. Bostanci W. Heywood K. Mills M. Parkar L. Nibali N. Donos Application of label-free absolute quantitative proteomics in human gingival crevicular fluid by LC/MS E (gingival exudatome) J Proteome Res 9 May (5) 2010 2191 2199 25 O. Brinkmann D.A. Kastratovic M.V. Dimitrijevic V.S. Konstantinovic D.B. Jelovac J. Antic Oral squamous cell carcinoma detection by salivary biomarkers in a Serbian population Oral Oncol 47 January (1) 2011 51 55 26 L.-M. Chi C.-W. Lee K.-P. Chang S.-P. Hao H.-M. Lee Y. Liang Enhanced interferon signaling pathway in oral cancer revealed by quantitative proteome analysis of microdissected specimens using 16o/18o labeling and integrated two-dimensional LC-ESI-MALDI tandem MS Mol Cell Proteomics 8 July (7) 2009 1453 1474 27 P.P. Costa G.L. Trevisan G.O. Macedo D.B. Palioto S.L.S. Souza M.F.M. Grisi Salivary interleukin-6, matrix metalloproteinase-8, and osteoprotegerin in patients with periodontitis and diabetes J Periodontol 81 March (3) 2010 384 391 28 P. Dowling R. Wormald P. Meleady M. Henry A. Curran M. Clynes Analysis of the saliva proteome from patients with head and neck squamous cell carcinoma reveals differences in abundance levels of proteins associated with tumour progression and metastasis J Proteom 71 July (2) 2008 168 175 29 L.D.R. Gonçalves M.R. Soares F.C.S. Nogueira C. Garcia D.R. Camisasca G. Domont Comparative proteomic analysis of whole saliva from chronic periodontitis patients J Proteom 73 May (7) 2010 1334 1341 30 B.J. Haigh K.W. Stewart J.R.K. Whelan M.P.G. Barnett G.A. Smolenski T.T. Wheeler Alterations in the salivary proteome associated with periodontitis J Clin Periodontol 37 March (3) 2010 241 247 31 Q.-Y. He J. Chen H.-F. Kung A.P.-W. Yuen J.-F. Chiu Identification of tumor-associated proteins in oral tongue squamous cell carcinoma by proteomics Proteomics 4 January (1) 2004 271 278 32 S. Hu M. Arellano P. Boontheung J. Wang H. Zhou J. Jiang Salivary proteomics for oral cancer biomarker discovery Clin Cancer Res 14 October (19) 2008 6246 6252 33 Y.-J. Jou C.-D. Lin C.-H. Lai C.-H. Chen J.-Y. Kao S.-Y. Chen Proteomic identification of salivary transferrin as a biomarker for early detection of oral cancer Anal Chim Acta 681 November (1–2) 2010 41 48 34 W.-Y. Lo M.-H. Tsai Y. Tsai C.-H. Hua F.-J. Tsai S.-Y. Huang Identification of over-expressed proteins in oral squamous cell carcinoma (OSCC) patients by clinical proteomic analysis Clin Chim Acta 376 February (1–2) 2007 101 107 35 A. Negishi M. Masuda M. Ono K. Honda M. Shitashige R. Satow Quantitative proteomics using formalin-fixed paraffin-embedded tissues of oral squamous cell carcinoma Cancer Sci 100 September (9) 2009 1605 1611 36 D. Preza B. Thiede I. Olsen B. Grinde The proteome of the human parotid gland secretion in elderly with and without root caries Acta Odontol Scand 67 3 2009 161 169 37 D. Turhani K. Krapfenbauer D. Thurnher H. Langen M. Fountoulakis Identification of differentially expressed, tumor-associated proteins in oral squamous cell carcinoma by proteomic analysis Electrophoresis 27 April (7) 2006 1417 1423 38 Y. Wu R. Shu L.-J. Luo L.-H. Ge Y.-F. Xie Initial comparison of proteomic profiles of whole unstimulated saliva obtained from generalized aggressive periodontitis patients and healthy control subjects J Periodont Res 44 October (5) 2009 636 644 39 L.-L. Yang X.-Q. Liu W. Liu B. Cheng M.-T. Li Comparative analysis of whole saliva proteomes for the screening of biomarkers for oral lichen planus Inflamm res 55 July (10) 2006 405 407 40 T. Cabras E. Pisano A. Mastinu G. Denotti P.P. Pusceddu R. Inzitari Alterations of the salivary secretory peptidome profile in children affected by type 1 diabetes Mol Cell Proteom 9 10 2010 2099 2108 41 M. Castagnola R. Inzitari C. Fanali F. Iavarone A. Vitali C. Desiderio The surprising composition of the salivary proteome of preterm human newborn Mol Cell Proteomics 10 1 2010 M110:003467 42 Y. Fleissig O. Deutsch E. Reichenberg M. Redlich B. Zaks A. Palmon Different proteomic protein patterns in saliva of Sjögren's syndrome patients Oral Dis 15 January (1) 2009 61 68 43 L. Giusti C. Baldini L. Bazzichi F. Ciregia I. Tonazzini G. Mascia Proteome analysis of whole saliva: a new tool for rheumatic diseases—the example of Sjögren's syndrome Proteomics 7 May (10) 2007 1634 1643 44 L. Giusti L. Bazzichi C. Baldini F. Ciregia G. Mascia G. Giannaccini Specific proteins identified in whole saliva from patients with diffuse systemic sclerosis J Rheumatol 34 October (10) 2007 2063 2069 45 T.O.R. Hjelmervik R. Jonsson A.I. Bolstad The minor salivary gland proteome in Sjögren's syndrome Oral Dis 15 July (5) 2009 342 353 46 S. Hu J. Wang J. Meijer S. Ieong Y. Xie T. Yu Salivary proteomic and genomic biomarkers for primary Sjögren's syndrome Arthritis Rheum 56 November (11) 2007 3588 3600 47 K. Ito S. Funayama Y. Hitomi S. Nomura K. Katsura M. Saito Proteome analysis of gelatin-bound salivary proteins in patients with primary Sjögren's syndrome: identification of matrix metalloproteinase-9 Clin Chim Acta 403 May (1–2) 2009 269 271 48 P.V. Rao A.P. Reddy X. Lu S. Dasari A. Krishnaprasad E. Biggs Proteomic identification of salivary biomarkers of type-2 diabetes J Proteome Res 8 January (1) 2009 239 245 49 O.H. Ryu J.C. Atkinson G.T. Hoehn G.G. Illei T.C. Hart Identification of parotid salivary biomarkers in Sjogren's syndrome by surface-enhanced laser desorption/ionization time-of-flight mass spectrometry and two-dimensional difference gel electrophoresis Rheumatology (Oxford) 45 September (9) 2006 1077 1086 50 C.F. Streckfus K.A. Storthz L. Bigler W.P. Dubinsky A comparison of the proteomic expression in pooled saliva specimens from individuals diagnosed with ductal carcinoma of the breast with and without lymph node involvement J Oncol 2009 2009 51 K. Fleming L.A. Kelley S.A. Islam R.M. MacCallum A. Muller F. Pazos The proteome: structure, function and evolution Philos Trans R Soc Lond B: Biol Sci 361 March (1467) 2006 441 451 52 S. Saha S.H. Harrison C. Shen H. Tang P. Radivojac R.J. Arnold HIP2: an online database of human plasma proteins from healthy individuals BMC Med Genomics 1 2008 1 12 53 M. Nissum S. Kuhfuss M. Hauptmann C. Obermaier U. Sukop R. Wildgruber Two-dimensional separation of human plasma proteins using iterative free-flow electrophoresis Proteomics 7 December (23) 2007 4218 4227 54 P.J. Kersey J. Duarte A. Williams Y. Karavidopoulou E. Birney R. Apweiler The International Protein Index: an integrated database for proteomics experiments Proteomics 4 July (7) 2004 1985 1988 55 E. Jain A. Bairoch S. Duvaud I. Phan N. Redaschi B. Suzek Infrastructure for the life sciences: design and implementation of the UniProt website BMC Bioinform 10 1 2009 136 56 The UniProt Consortium Ongoing and future developments at the Universal Protein Resource Nucleic Acids Res 39 November (Database) 2010 D214 D219 57 H. Mi Q. Dong A. Muruganujan P. Gaudet S. Lewis P.D. Thomas PANTHER version 7: improved phylogenetic trees, orthologs and collaboration with the Gene Ontology Consortium Nucleic Acids Res 38 December (Database) 2009 D204 D210 58 P.D. Thomas M.J. Campbell A. Kejariwal H. Mi B. Karlak R. Daverman PANTHER: a library of protein families and subfamilies indexed by function Genome Res 13 9 2003 2129 2141 59 R.J. Cho M.J. Campbell Transcription, genomes, function Trends Genet 16 September (9) 2000 409 415 60 D.W. Huang B.T. Sherman R.A. Lempicki Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources Nat Protoc 4 1 2009 44 57 61 D.W. Huang B.T. Sherman R.A. Lempicki Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists Nucleic Acids Res 37 January (1) 2009 1 13 62 S.-J. Li M. Peng H. Li B.-S. Liu C. Wang J.-R. Wu Sys-BodyFluid: a systematical database for human body fluid proteome research Nucleic Acids Res 37 January (Database issue) 2009 D907 D912 63 M. Greabu M. Battino M. Mohora A. Totan T. Spinu C. Totan Could constitute saliva the first line of defence against oxidative stress? Rom J Intern Med 45 2 2007 209 213 64 A.J. Delima T.E. Van Dyke Origin and function of the cellular components in gingival crevice fluid Periodontology 2000 31 2003 55 76 65 T. Sorsa L. Tjäderhane Y.T. Konttinen A. Lauhio T. Salo H.-M. Lee Matrix metalloproteinases: contribution to pathogenesis, diagnosis and treatment of periodontal inflammation Ann Med 38 5 2006 306 321 66 L. Vitkov M. Klappacher M. Hannig W.D. Krautgartner Extracellular neutrophil traps in periodontitis J Periodont Res 44 October (5) 2009 664 672 67 P.J. Grant Diabetes mellitus as a prothrombotic condition J Intern Med 262 August (2) 2007 157 172 68 T. Issad O-GlcNAc glycosylation and regulation of cell signaling Med Sci (Paris) 26 September (8–9) 2010 753 759 "
    },
    {
        "doc_title": "A secure personal health record repository",
        "doc_scopus_id": "84861985443",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861985443",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Access policies",
            "Competitive markets",
            "EHR",
            "Electronic health record systems",
            "Information exchanges",
            "Legal issues",
            "Personal health record",
            "PHR",
            "Repositories",
            "Security",
            "Storage services"
        ],
        "doc_abstract": "Due to strict regulatory, ethic and legal issues, Electronic Health Record (EHR) systems have been mainly deployed in federated health care scenarios. This situation have been hindering the wide adoption of EHRs, contributing to delaying the establishment of a competitive market where contributions from different providers could take full advantage of information exchange and regular practitioners' collaboration. Moreover, with the increasing awareness of medical subjects, patients are demanding more control over their own personal data - Personal Health Record (PHR). This paper presents a secure PHR repository which access is controlled through the joint use of a Virtual Health Card Service (VHCS) and an access Broker. This solution can be deployed in any public or private storage service since it behaves as a sandbox system which access policy is defined externally. To assure a friendly query-retrieve interaction the whole repository is indexed, and separated clinical events are kept independently to increase the efficiency of cipher and encipher algorithms.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gathering and managing genotype and phenotype information about rare diseases patients",
        "doc_scopus_id": "84861985300",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861985300",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Clinical monitoring",
            "Clinical research",
            "Computing resource",
            "Data quality",
            "Genetic mutations",
            "Health professionals",
            "Knowledge dissemination",
            "LSDB",
            "Number of peoples",
            "Quality of life",
            "Rare disease",
            "Reference centers",
            "Scientific information",
            "Technological advancement",
            "Web information systems"
        ],
        "doc_abstract": "Information technology is increasingly present in medicine, and is emerging as a crucial tool both in clinical monitoring and knowledge dissemination, increasing the success rate in diagnosis and subsequently during treatment. A particular sub-group of diseases are designated rare or orphan due to the small number of people suffering from these illnesses. They are sometimes disabling, reducing the quality of life of patients and affect all relatives around them. The aim of this project was to build a Web information system to support practice and clinical research in rare diseases, and to facilitate the collection of scientific information, diagnosis, treatment and patient support. Through this system, one can optimize specialized medical resources, computing resources and data quality, so that the information is available for consulting and supporting future decisions. This developed system also allows health professionals to share information that will be important for enhancing the quality and technological advancement in this area of public health.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Web information system for the study of oral health",
        "doc_scopus_id": "84861982590",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861982590",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Biological information",
            "Biomedical science",
            "Data integration",
            "Healthy individuals",
            "Molecular levels",
            "Multiple interactions",
            "Oral cavity",
            "Protein components",
            "Proteomic studies",
            "Web information systems"
        ],
        "doc_abstract": "The human oral cavity is a complex ecosystem where multiple interactions occur and whose comprehension is critical to understand several disease mechanisms. In order to comprehend the composition of the oral cavity at a molecular level, it is needed to compile and integrate the biological information resulting from specific techniques, especially from proteomic studies of saliva. The objective for this work was to compile and curate a specific group of proteins related to the oral cavity, providing a tool to conduct further studies over the salivary proteome. Despite previous efforts to identify the protein components of saliva in healthy individuals and in several oral and systemic disorders, a resource compiling and reviewing all of these proteins is still lacking. In this paper we present a platform that integrates in a single endpoint all available information for proteins associated with the oral cavity. The proposed tool allows researchers in the biomedical sciences to explore organisms, proteins and diseases, constituting a unique tool to analyse meaningful interactions for oral health.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An openehr repository based on a native XML database",
        "doc_scopus_id": "84861978859",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861978859",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "EHR",
            "Electronic health record",
            "Native xml database",
            "Open-source solutions",
            "Open-standard specification",
            "OpenEHR",
            "Service layers",
            "XML repositories"
        ],
        "doc_abstract": "openEHR is an open standard specification that describes the management, storage, retrieval and exchange of data in Electronic Health Record (EHR). Despite its growing importance in the field, the lack of open source solutions is hindering a larger visibility. In this paper we present an openEHR-based repository supported by a native XML database, which allows to store and query openEHR records through the DB service layer and a set of REST web services. The obtained results highlight the efficiency of this API and show that it can be used as a persistence component in any openEHR solution.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A DICOM relay service supported on cloud resources",
        "doc_scopus_id": "84861968982",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861968982",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Different equipment",
            "Digital imaging communication in medicines",
            "Healthcare institutions",
            "Medical Devices",
            "Medical institutions",
            "PACS",
            "Picture archive and communication systems",
            "Teleradiology",
            "Work-flows"
        ],
        "doc_abstract": "Over the past decades, healthcare institutions adopted Picture Archive and Communication Systems in their workflows. The exchange and interaction between different equipment is performed with Digital Imaging Communication in Medicine (DICOM), which is a very extensive protocol covering many areas of imaging laboratories. However, the communication of a wide domain composed by several medical institutions is not well supported. This paper presents a solution to share DICOM services across healthcare institutions. The proposed implementation is supported on public cloud resources, creating the opportunity to exchange information between medical devices across several institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Parameter influence in genetic algorithm optimization of support vector machines",
        "doc_scopus_id": "84861217450",
        "doc_doi": "10.1007/978-3-642-28839-5_5",
        "doc_eid": "2-s2.0-84861217450",
        "doc_date": "2012-05-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Choice of parameters",
            "Classification accuracy",
            "Classification methods",
            "Data sets",
            "Feature sets",
            "Genetic-algorithm optimizations",
            "Joint optimization",
            "Kernel",
            "Parameters optimization",
            "RBF kernels",
            "SVM",
            "SVM classification"
        ],
        "doc_abstract": "Support Vector Machines provide a well established and powerful classification method to analyse data and find the minimal-risk separation between different classes. Finding that separation strongly depends on the available feature set. Feature selection and SVM parameters optimization methods improve classification accuracy. This paper studies their joint optimization and attribution improvement. A comparison was made using genetic algorithms to find the best parameters for SVM classification. Results show that using the RBF kernel returns better results on average, though the best optimization for some data sets is highly dependent on the choice of parameters and kernels. We also show that, overall, an average 26% relative improvement with 8% std was obtained. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A rare disease patient manager",
        "doc_scopus_id": "84861212295",
        "doc_doi": "10.1007/978-3-642-28839-5_20",
        "doc_eid": "2-s2.0-84861212295",
        "doc_date": "2012-05-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Digital records",
            "Genetic mutations",
            "Incidence rate",
            "Knowledge base",
            "Life-sciences",
            "LSDB",
            "Medical data",
            "Multiple data",
            "Particular condition",
            "Personal health",
            "Personalized medicines",
            "Rare disease",
            "Strongest relations",
            "Treatment process",
            "Working environment"
        ],
        "doc_abstract": "The personal health implications behind rare diseases are seldom considered in widespread medical care. The low incidence rate and complex treatment process makes rare disease research an underrated field in the life sciences. However, it is in these particular conditions that the strongest relations between genotypes and phenotypes are identified. The rare disease patient manager, detailed in this manuscript, presents an innovative perspective for a patient-centric portal integrating genetic and medical data. With this strategy, patient's digital records are transparently integrated and connected to wet-lab genetics research in a seamless working environment. The resulting knowledge base offers multiple data views, geared towards medical staff, with patient treatment and monitoring data; genetics researchers, through a custom locus-specific database; and patients, who for once play an active role in their treatment and rare diseases research. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A RESTful image gateway for multiple medical image repositories",
        "doc_scopus_id": "84860675556",
        "doc_doi": "10.1109/TITB.2011.2176497",
        "doc_eid": "2-s2.0-84860675556",
        "doc_date": "2012-05-11",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Component based",
            "Decision support tools",
            "DICOM",
            "Image repository",
            "Imaging protocol",
            "Medical images",
            "Mobile Technology",
            "PACS",
            "Picture archiving",
            "Representational state transfer",
            "services broker",
            "Telemedicine systems",
            "Universal access"
        ],
        "doc_abstract": "Mobile technologies are increasingly important components in telemedicine systems and are becoming powerful decision support tools. Universal access to data may already be achieved by resorting to the latest generation of tablet devices and smartphones. However, the protocols employed for communicating with image repositories are not suited to exchange data with mobile devices. In this paper, we present an extensible approach to solving the problem of querying and delivering data in a format that is suitable for the bandwidth and graphic capacities of mobile devices. We describe a three-tiered component-based gateway that acts as an intermediary between medical applications and a number of Picture Archiving and Communication Systems (PACS). The interface with the gateway is accomplished using Hypertext Transfer Protocol (HTTP) requests following a Representational State Transfer (REST) methodology, which relieves developers from dealing with complex medical imaging protocols and allows the processing of data on the server side. © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Harmonization of gene/protein annotations: Towards a gold standard medline",
        "doc_scopus_id": "84860473173",
        "doc_doi": "10.1093/bioinformatics/bts125",
        "doc_eid": "2-s2.0-84860473173",
        "doc_date": "2012-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Statistics and Probability",
                "area_abbreviation": "MATH",
                "area_code": "2613"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computational Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Motivation: The recognition of named entities (NER) is an elementary task in biomedical text mining. A number of NER solutions have been proposed in recent years, taking advantage of available annotated corpora, terminological resources and machine-learning techniques. Currently, the best performing solutions combine the outputs from selected annotation solutions measured against a single corpus. However, little effort has been spent on a systematic analysis of methods harmonizing the annotation results and measuring against a combination of Gold Standard Corpora (GSCs). Results: We present Totum, a machine learning solution that harmonizes gene/protein annotations provided by heterogeneous NER solutions. It has been optimized and measured against a combination of manually curated GSCs. The performed experiments show that our approach improves the F-measure of state-of-the-art solutions by up to 10% (achieving ≈70%) in exact alignment and 22% (achieving ≈82%) in nested alignment. We demonstrate that our solution delivers reliable annotation results across the GSCs and it is an important contribution towards a homogeneous annotation of MEDLINE abstracts. © The Author 2012. Published by Oxford University Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "COEUS: A Semantic Web application framework",
        "doc_scopus_id": "84859920065",
        "doc_doi": "10.1145/2166896.2166915",
        "doc_eid": "2-s2.0-84859920065",
        "doc_date": "2012-04-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Application deployment",
            "Application development",
            "Bioinformatics software",
            "Biomedicine",
            "Data integration",
            "Data quantity",
            "Knowledge federation",
            "Latter complexes",
            "Life-sciences",
            "Open sources",
            "Semantic web applications",
            "Semantic Web technology",
            "Software engineers"
        ],
        "doc_abstract": "As the \"omics\" revolution unfolds, the growth in data quantity and diversity is pushing forward the need for pioneering bioinformatics software, capable of significantly improving the research workflow. To cope with these computer science demands, biomedical software engineers are adopting emerging Semantic Web technologies that better suit the life sciences domain. The latter complex innate relationships are easily mapped into Semantic Web graphs, enabling a superior understanding of collected knowledge. Despite the increased awareness regarding Semantic Web technologies in bioinformatics, its usage is still diminished. With COEUS, we introduce a new Semantic Web framework, aiming at a streamlined application development cycle. COEUS follows a \"Semantic Web in a box\" approach, with a package including advanced data integration and triplification tools, base ontologies, a web-oriented engine and a flexible exploration API. The platform, targeted at life sciences developers, provides a complete application skeleton ready for rapid application deployment, and is available free as open source at http://bioinformatics.ua. pt/coeus/. Copyright © 2011 ACM.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A holistic approach for integrating genomic variation information",
        "doc_scopus_id": "84859340827",
        "doc_doi": "10.1007/978-3-642-28062-7_5",
        "doc_eid": "2-s2.0-84859340827",
        "doc_date": "2012-04-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "data integration",
            "Existing systems",
            "Genetic sequence",
            "Holistic approach",
            "Human genetics",
            "Human traits",
            "human variome",
            "LSDB",
            "Patient care",
            "Personalized medicines",
            "Prototype implementations",
            "Scientific community"
        ],
        "doc_abstract": "Personalized medicine is strongly tied with human variome research: understanding the impact of specific genetic sequence mutations on observable human traits will play a key role in the quest for custom drugs therapies and improved patient care. Recent growth in this particular field leveraged the appearance of locus-specific databases (LSDBs). Although these systems are praised in the scientific community, they lack some features that can promote a more widespread usage. Existing systems are closed, independent and designed solely for gene curators. In this paper we present a new approach based on a holistic perspective of the genomic variation field, envisaging the integration of LSDBs, genes and variants, as well as a broad set of related resources in an innovative workspace. A prototype implementation for this approach is deployed online at http://bioinformatics.ua.pt/WAVe . © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic filtering and substantiation of drug safety signals",
        "doc_scopus_id": "84861122386",
        "doc_doi": "10.1371/journal.pcbi.1002457",
        "doc_eid": "2-s2.0-84861122386",
        "doc_date": "2012-04-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Ecology, Evolution, Behavior and Systematics",
                "area_abbreviation": "AGRI",
                "area_code": "1105"
            },
            {
                "area_name": "Ecology",
                "area_abbreviation": "ENVI",
                "area_code": "2303"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Cellular and Molecular Neuroscience",
                "area_abbreviation": "NEUR",
                "area_code": "2804"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Drug safety issues pose serious health threats to the population and constitute a major cause of mortality worldwide. Due to the prominent implications to both public health and the pharmaceutical industry, it is of great importance to unravel the molecular mechanisms by which an adverse drug reaction can be potentially elicited. These mechanisms can be investigated by placing the pharmaco-epidemiologically detected adverse drug reaction in an information-rich context and by exploiting all currently available biomedical knowledge to substantiate it. We present a computational framework for the biological annotation of potential adverse drug reactions. First, the proposed framework investigates previous evidences on the drug-event association in the context of biomedical literature (signal filtering). Then, it seeks to provide a biological explanation (signal substantiation) by exploring mechanistic connections that might explain why a drug produces a specific adverse reaction. The mechanistic connections include the activity of the drug, related compounds and drug metabolites on protein targets, the association of protein targets to clinical events, and the annotation of proteins (both protein targets and proteins associated with clinical events) to biological pathways. Hence, the workflows for signal filtering and substantiation integrate modules for literature and database mining, in silico drug-target profiling, and analyses based on gene-disease networks and biological pathways. Application examples of these workflows carried out on selected cases of drug safety signals are discussed. The methodology and workflows presented offer a novel approach to explore the molecular mechanisms underlying adverse drug reactions. © 2012 Bauer-Mehren et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the parameter optimization of Support Vector Machines for binary classification.",
        "doc_scopus_id": "84875030336",
        "doc_doi": "10.1515/jib-2012-201",
        "doc_eid": "2-s2.0-84875030336",
        "doc_date": "2012-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [
            "MLCS",
            "MLOWN"
        ],
        "doc_abstract": "Classifying biological data is a common task in the biomedical context. Predicting the class of new, unknown information allows researchers to gain insight and make decisions based on the available data. Also, using classification methods often implies choosing the best parameters to obtain optimal class separation, and the number of parameters might be large in biological datasets. Support Vector Machines provide a well-established and powerful classification method to analyse data and find the minimal-risk separation between different classes. Finding that separation strongly depends on the available feature set and the tuning of hyper-parameters. Techniques for feature selection and SVM parameters optimization are known to improve classification accuracy, and its literature is extensive. In this paper we review the strategies that are used to improve the classification performance of SVMs and perform our own experimentation to study the influence of features and hyper-parameters in the optimization process, using several known kernels.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Concepts for a Personal Health Record",
        "doc_scopus_id": "84872558617",
        "doc_doi": "10.3233/978-1-61499-101-4-636",
        "doc_eid": "2-s2.0-84872558617",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Electronic health record",
            "Longitudinal health records",
            "Medical history",
            "Personal health informations",
            "Personal health record"
        ],
        "doc_abstract": "Healthcare is about information. It is usually assumed that personal health information exists primarily for professional's use but well informed patients motivate better informed professionals. A longitudinal health record containing a patient's medical history has been the holy grail of healthcare. Personal Electronic Health Records (P-EHR) hold the potential to transform healthcare by providing a complete set of patient managed information. We present a portable P-EHR's functionalities from the patient's perspective. © 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhancing the many-to-many relations across IHE document sharing communities",
        "doc_scopus_id": "84872536540",
        "doc_doi": "10.3233/978-1-61499-101-4-641",
        "doc_eid": "2-s2.0-84872536540",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "ATNA",
            "Clinical records",
            "Document sharing",
            "Healthcare institutions",
            "Integrating healthcare enterprise",
            "IT infrastructures",
            "Professional relationships",
            "Work in progress"
        ],
        "doc_abstract": "The Integrating Healthcare Enterprise (IHE) initiative is an ongoing project aiming to enable true inter-site interoperability in the health IT field. IHE is a work in progress and many challenges need to be overcome before the healthcare Institutions may share patient clinical records transparently and effortless. Configuring, deploying and testing an IHE document sharing community requires a significant effort to plan and maintain the supporting IT infrastructure. With the new paradigm of cloud computing is now possible to launch software devices on demand and paying accordantly to the usage. This paper presents a framework designed with purpose of expediting the creation of IHE document sharing communities. It provides semi-ready templates of sharing communities that will be customized according the community needs. The framework is a meeting point of the healthcare institutions, creating a favourable environment that might converge in new inter-institutional professional relationships and eventually the creation of new Affinity Domains. © 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dicoogle Mobile: A medical imaging platform for Android",
        "doc_scopus_id": "84872518920",
        "doc_doi": "10.3233/978-1-61499-101-4-502",
        "doc_eid": "2-s2.0-84872518920",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Android applications",
            "Clinical decision support",
            "DICOM",
            "Health care information system",
            "Mobile computing technology",
            "Mobile platform",
            "Quality of care",
            "Ubiquitous access"
        ],
        "doc_abstract": "Mobile computing technologies are increasingly becoming a valuable asset in healthcare information systems. The adoption of these technologies helps to assist in improving quality of care, increasing productivity and facilitating clinical decision support. They provide practitioners with ubiquitous access to patient records, being actually an important component in telemedicine and telework environments. We have developed Dicoogle Mobile, an Android application that provides remote access to distributed medical imaging data through a cloud relay service. Besides, this application has the capability to store and index local imaging data, so that they can also be searched and visualized. In this paper, we will describe Dicoogle Mobile concept as well the architecture of the whole system that makes it running. © 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An integrative approach for codon repeats evolutionary analyses",
        "doc_scopus_id": "84867019138",
        "doc_doi": "10.1504/IJDMB.2012.049294",
        "doc_eid": "2-s2.0-84867019138",
        "doc_date": "2012-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The relationship between genome characteristics and several human diseases has been a central research goal in genomics. Many studies have shown that specific gene patterns, such as amino acid repetitions, are associated with human diseases. However, several open questions still remain, such as, how these tandem repeats appeared in the evolutionary path or how they have evolved in orthologous genes of related organisms. In this paper, we present a computational solution that facilitates comparative studies of orthologous genes from various organisms. The application uses various web services to gather gene sequence information, local algorithms for tandem repeats identification and similarity measures for gene clustering. Copyright © 2012 Inderscience Enterprises Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A PACS archive architecture supported on cloud services",
        "doc_scopus_id": "84862979352",
        "doc_doi": "10.1007/s11548-011-0625-x",
        "doc_eid": "2-s2.0-84862979352",
        "doc_date": "2012-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Purpose Diagnostic imaging procedures have continuously increased over the last decade and this trend may continue in coming years, creating a great impact on storage and retrieval capabilities of current PACS. Moreover, many smaller centers do not have financial resources or requirements that justify the acquisition of a traditional infrastructure. Alternative solutions, such as cloud computing, may help address this emerging need. Methods A tremendous amount of ubiquitous computational power, such as that provided by Google and Amazon, are used every day as a normal commodity. Taking advantage of this new paradigm, an architecture for a Cloud-based PACS archive that provides data privacy, integrity, and availability is proposed. The solution is independent from the cloud provider and the core modules were successfully instantiated in examples of two cloud computing providers. Operational metrics for several medical imaging modalities were tabulated and compared for Google Storage, Amazon S3, and LAN PACS. Results A PACS-as-a-Service archive that provides storage of medical studies using the Cloud was developed. The results show that the solution is robust and that it is possible to store, query, and retrieve all desired studies in a similar way as in a local PACS approach. Conclusion Cloud computing is an emerging solution that promises high scalability of infrastructures, software, and applications, according to a \"pay-as-you- go\" business model. The presented architecture uses the cloud to setup medical data repositories and can have a significant impact on healthcare institutions by reducing IT infrastructures. © 2011 CARS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A semantic web application framework for health systems interoperability",
        "doc_scopus_id": "83255173885",
        "doc_doi": "10.1145/2064747.2064768",
        "doc_eid": "2-s2.0-83255173885",
        "doc_date": "2011-12-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Decision Sciences (all)",
                "area_abbreviation": "DECI",
                "area_code": "1800"
            },
            {
                "area_name": "Business, Management and Accounting (all)",
                "area_abbreviation": "BUSI",
                "area_code": "1400"
            }
        ],
        "doc_keywords": [
            "Application deployment",
            "Clinical practices",
            "Health systems",
            "Heterogeneous data",
            "Human genomes",
            "Interoperability framework",
            "Medical profession",
            "Patient care",
            "semantic integration",
            "Semantic web applications",
            "Software solution",
            "Software stacks"
        ],
        "doc_abstract": "Relevant biomedical advances happen daily, and the medical profession relies on this evolution to deliver an improved patient care. In addition, the growing magnitude of data generated by biomedical software and hardware since the initial discovery of the human genome is remarkable in size and variety. Hence, best-of-breed software solutions are at best a couple years behind clinical practice demands. In this paper we detail an innovative Semantic Web interoperability framework, which provides developers with a complete software stack for semantic application deployment. Interoperability is the defining feature of this framework. On the one hand, new instances are able to integrate several types of distributed and heterogeneous data. On the other hand, collected data are made available through a public SPARQL endpoint. © 2011 ACM.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An IDE for NETCONF management applications",
        "doc_scopus_id": "84863314057",
        "doc_doi": "10.1109/LANOMS.2011.6102269",
        "doc_eid": "2-s2.0-84863314057",
        "doc_date": "2011-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            }
        ],
        "doc_keywords": [
            "Communication interface",
            "Development solutions",
            "Integrated development",
            "Management applications",
            "Model specifications",
            "NETCONF",
            "SDK",
            "System management software",
            "Time-consuming tasks",
            "YANG"
        ],
        "doc_abstract": "The development of network and system management software typically requires data models definition, the creation of specific applications respecting the data model, and yet the implementation of communication interfaces. Skilled professionals usually perform such tasks in a predefined sequence and using different development solutions, but any error or lacks in the data model frequently force to repeat several time-consuming tasks. In this paper we present an integrated development framework that simplifies the construction of NETCONF management applications, from data model specification to deployment and evaluation. The framework is available at http://atnog.av.it. pt/~ptavares/yangplugin. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Challenges storing and representing biomedical data",
        "doc_scopus_id": "82155197287",
        "doc_doi": "10.1007/978-3-642-25364-5_6",
        "doc_eid": "2-s2.0-82155197287",
        "doc_date": "2011-11-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biomedical data",
            "Computational applications",
            "Computer interaction",
            "Data integration",
            "Data management architecture",
            "Data representations",
            "Data sets",
            "Data storing",
            "Experimental data",
            "Scientific achievements"
        ],
        "doc_abstract": "The scientific achievements coming from molecular biology depend greatly on the capability of computational applications to manage and explore laboratorial results. One component that is commonly underrated is the need for proper user interfaces that allow researchers to visually explore the results and extract biological evidences. In this paper, we review the main challenges of dealing with complex biomedical datasets, namely regarding data storing, communicating, representing and visualizing biomedical experimental data. We emphasize the need for proper human computer interaction paradigms and underlying data management architectures in order to achieve a correct interpretation of the experimental results. © 2011 Springer-Verlag Berlin.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards knowledge federation in biomedical applications",
        "doc_scopus_id": "82055208502",
        "doc_doi": "10.1145/2063518.2063530",
        "doc_eid": "2-s2.0-82055208502",
        "doc_date": "2011-11-29",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Aggregated networks",
            "Biological data",
            "Biomedical applications",
            "biomedicine",
            "Development cycle",
            "Digital disease",
            "High-throughput",
            "Human genomes",
            "knowledge federation",
            "Life-sciences",
            "New applications",
            "New strategy",
            "Open Access",
            "Purely semantic",
            "Semantic Web technology",
            "Software solution",
            "Software stacks",
            "Steep learning curve"
        ],
        "doc_abstract": "Knowledge federation is a particular matter of concern in the life sciences domain. The magnitude of data generated by biomedical software and hardware since the initial discovery of the human genome is tremendous in quantity and diversity. Consequently, state-of-the-art software solutions have always lagged behind researchers' demands, even more so with recent developments in high-throughput sequencing technology and open access to digital disease records. The maturity of Semantic Web technologies brought with it new strategies to tackle life sciences challenges. In spite of the increased awareness regarding these technologies' advantages, its adoption has been slow paced. With a steep learning curve and lacking turnkey solutions, bioinformatics developers are faced with many roadblocks, resulting in a low number of purely semantic solutions. Our research springs new life to knowledge federation in the life sciences with a new application deployment framework, which includes a native federation layer connecting and federating deployed instances by default. This feature is integrated in a complete software stack for creating new applications, easing the troubled development cycle associated with Semantic Web tools. As a result, the door is opened for harnessing more insightful knowledge from the aggregated network of relationships amongst biological data. © 2011 ACM.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Species-specific codon context rules unveil non-neutrality effects of synonymous mutations",
        "doc_scopus_id": "80055031054",
        "doc_doi": "10.1371/journal.pone.0026817",
        "doc_eid": "2-s2.0-80055031054",
        "doc_date": "2011-11-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: Codon pair usage (codon context) is a species specific gene primary structure feature whose evolutionary and functional roles are poorly understood. The data available show that codon-context has direct impact on both translation accuracy and efficiency, but one does not yet understand how it affects these two translation variables or whether context biases shape gene evolution. Methodologies/Principal Findings: Here we study codon-context biases using a set of 72 orthologous highly conserved genes from bacteria, archaea, fungi and high eukaryotes to identify 7 distinct groups of codon context rules. We show that synonymous mutations, i.e., neutral mutations that occur in synonymous codons of codon-pairs, are selected to maintain context biases and that non-synonymous mutations, i.e., non-neutral mutations that alter protein amino acid sequences, are also under selective pressure to preserve codon-context biases. Conclusions: Since in vivo studies provide evidence for a role of codon context on decoding fidelity in E. coli and for decoding efficiency in mammalian cells, our data support the hypothesis that, like codon usage, codon context modulates the evolution of gene primary structure and fine tunes the structure of open reading frames for high genome translational fidelity and efficiency in the 3 domains of life. © 2011 Moura et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dicoogle - An open source peer-to-peer PACS",
        "doc_scopus_id": "84855567679",
        "doc_doi": "10.1007/s10278-010-9347-9",
        "doc_eid": "2-s2.0-84855567679",
        "doc_date": "2011-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computer communication networks",
            "Digital imaging and communications in medicines",
            "Information storage and retrieval",
            "Open sources",
            "PACS",
            "PACS implementation",
            "Peer to peer"
        ],
        "doc_abstract": "Picture Archiving and Communication Systems (PACS) have been widely deployed in healthcare institutions, and they now constitute a normal commodity for practitioners. However, its installation, maintenance, and utilization are still a burden due to their heavy structures, typically supported by centralized computational solutions. In this paper, we present Dicoogle, a PACS archive supported by a document-based indexing system and by peer-to-peer (P2P) protocols. Replacing the traditional database storage (RDBMS) by a documental organization permits gathering and indexing data from file-based repositories, which allows searching the archive through free text queries. As a direct result of this strategy, more information can be extracted from medical imaging repositories, which clearly increases flexibility when compared with current query and retrieval DICOM services. The inclusion of P2P features allows PACS internetworking without the need for a central management framework. Moreover, Dicoogle is easy to install, manage, and use, and it maintains full interoperability with standard DICOM services. © Society for Imaging Informatics in Medicine 2010.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Prioritizing literature search results using a training set of classified documents",
        "doc_scopus_id": "80052949641",
        "doc_doi": "10.1007/978-3-642-19914-1_49",
        "doc_eid": "2-s2.0-80052949641",
        "doc_date": "2011-09-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Area of interest",
            "Biomedical fields",
            "Biomedical literature",
            "Classified documents",
            "Literature search",
            "Protein-protein Interactions",
            "Ranking strategy",
            "Rapid expansion",
            "Scientific literature",
            "Test sets",
            "Training sets"
        ],
        "doc_abstract": "Finding relevant articles is rapidly becoming a demanding task for researchers in the biomedical field, due to the rapid expansion of the scientific literature. We investigate the use of ranking strategies for prioritizing literature search results given an initial topic of interest. Focusing on the topic of protein-protein interactions, we compared ranking strategies based on different classifiers and features. The best result obtained on the BioCreative III PPI test set was an area under the interpolated precision-recall curve of 0,629. We then analyze the use of this method for ranking the result of PubMed queries. The results shown indicate that this strategy can be used by database curators to prioritize articles for extraction of protein-protein interactions, and also by general researchers looking for publications describing protein-protein interactions within a particular area of interest. © 2011 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A PACS gateway to the cloud",
        "doc_scopus_id": "80052507826",
        "doc_doi": null,
        "doc_eid": "2-s2.0-80052507826",
        "doc_date": "2011-09-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Cloud providers",
            "Concept-based",
            "Data availability",
            "Data flow",
            "DICOM",
            "High security levels",
            "Medical images",
            "Outsource",
            "PACS",
            "Pay-as-you-go",
            "Universal access",
            "Work-flows"
        ],
        "doc_abstract": "The amount of medical images has increased significantly over the last decade as result of the increase of number and quality of studies. Following some researchers, this trend will continue over the next years. Cloud computing is a new concept based on a well-know model named \"pay-as-you-go\". There is a new concept dubbed PACS Cloud, which the fundamental idea is to do PACS outsourcing taking advantages of the clouds elasticity and scalability, avoiding hardware obsolescence, providing universal access to the information anywhere, anytime and increase the data availability. This paper presents a module of PACS Cloud architecture to grant interoperability with DICOM devices. PACS Cloud Gateway is a component of PACS Cloud, which focuses mainly on the translation from DICOM commands to non-DICOM and vice-versa. While data outsource to the cloud can relieve users from the burden of local storage and maintenance, it also brings new security concerns. This paper presents a secure PACS Cloud Gateway to access PACS Cloud archive, which provides a high security level and without cloud's provider dependence. The workflows of each process was described carefully, specifying data flows since that Gateway is contacted by DICOM device, until it releases the process. Finally, the platform was instantiated in biggest Internet Cloud providers and the solution's results was analysed. © 2011 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hybrid electronic health records",
        "doc_scopus_id": "79960192049",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960192049",
        "doc_date": "2011-07-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Alternative medicine",
            "EHR",
            "Elderly care",
            "Electronic health record",
            "Health records",
            "Hybrid EHR",
            "New services",
            "Personal health record",
            "PHR",
            "Public sector",
            "Remote patient monitoring"
        ],
        "doc_abstract": "The research related with digital health records has been a hot topic since the last two decades, producing diverse results, particularly in two main types - Electronic Health Records and Personal Health Records. With the current wider citizen mobility, the liberalization of health care providing, as well as alternative medicine, elderly care and remote patient monitoring, new challenges had emerged. These brought more actors to the scene that can belong to different healthcare networks, private or public sector even from different countries. For creating a true patient-centric electronic health record, those actors need to collaborate in the creation and maintenance of the record. In this work, the Hybrid Electronic Health Record (HEHR) is presented, describing how information can be created and used, as well as focusing on how the patient defines the access control. Some new services are also discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of openEHR in a portable PHR",
        "doc_scopus_id": "79960172418",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960172418",
        "doc_date": "2011-07-15",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Active parts",
            "Data population",
            "Electronic health",
            "Electronic health record",
            "Information repositories",
            "Medical information",
            "OpenEHR",
            "Patient data management",
            "Personal health record",
            "Security",
            "Security and privacy"
        ],
        "doc_abstract": "Quality medical acts rely on patient medical information. With paper records, the responsibility of gathering the disparate information and making it available to the caregivers, falls exclusively upon the patient. This still is, to great extent, the case with electronic health documents. The consensus is that the advantages of patient involvement in his own health are numerous. With the advent of recent technologies and their deployment in healthcare, new ways of involving the patient and making him an active part of his own health are possible. Electronic Health Records (EHR) and specially Personal Health Records (PHR) are important tools for patient empowerment but data population and management through non-intuitive structured forms is time consuming, takes a great amount of effort, and can be deterring specially for people that are not very computer-oriented. PHRs can be simple and scalable applications that the patient uses to get started and afterwards evolve towards complexity. In any case, compliance with standards must be accomplished. In this paper we present a PHR simple to use, implemented on a USB Flash pen for mobility, and compliant with the openEHR specification. Our model builds on openEHR and adds security and privacy features, allows patient data management and can work as an information repository.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "WAVe: Web analysis of the variome",
        "doc_scopus_id": "79959758966",
        "doc_doi": "10.1002/humu.21499",
        "doc_eid": "2-s2.0-79959758966",
        "doc_date": "2011-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Genetics (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2716"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "DNA sequence variation is the underlying basis of common human traits and rarer single-gene disorders. Understanding the variome, the variants in an individual's genome, is essential to enable the ultimate goals of personalized medicine. This critical research field has grown dramatically in recent years, mostly due to the spread and development of genotyping technologies. Despite these activities being promoted by the Human Genome Variation Society and projects such as the Human Variome Project or the European GEN2PHEN Project, variome data-integration systems are far from being widely used in the research community workflow. Most of ongoing research is focused on improving locus-specific databases. Although the quality and manual curation of LSDBs adds true value to this domain, they are often narrow, heterogeneous, and independent systems. This hampers data harmonization and interoperability between systems, stifling the aggregation of data from LSDBs and related data sources. A new platform entitled Web Analysis of the Variome, WAVe, is introduced. It offers direct and programmatic access to multiple locus-specific databases, with the integration of genetic variation datasets and enrichment with relevant information. WAVe's agile and innovative Web interface is accessible at http://bioinformatics.ua.pt/WAVe. © 2011 Wiley-Liss, Inc.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving the performance of the iterative signature algorithm for the identification of relevant patterns",
        "doc_scopus_id": "79551712212",
        "doc_doi": "10.1002/sam.10104",
        "doc_eid": "2-s2.0-79551712212",
        "doc_date": "2011-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analysis",
                "area_abbreviation": "MATH",
                "area_code": "2603"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Average",
            "Biclustering",
            "Codon",
            "Iterative signature algorithm",
            "Median"
        ],
        "doc_abstract": "The iterative signature algorithm (ISA) has become very attractive to detect co-regulated genes from microarray data matrices and can be a useful tool for the identification of similar patterns in many other kinds of numerical data matrices. Nevertheless, its algorithmic strategy exhibits some limitations since it is based on statistical behavior of the average and considers averages weighted by scores not necessarily positive. Hence, we propose to take the median instead of the average and to use absolutes scores in ISA's structure. Furthermore, a generalized function is also introduced in the algorithm in order to improve its algorithmic strategy for detecting high value or low value biclusters. The effects of these simple modifications on the performance of the biclustering algorithm are evaluated through an experimental comparative study involving synthetic data sets and real data from the organism Saccharomyces cerevisiae. The experimental results show that the proposed variations of ISA outperform the original version in many situations. Absolute scores in ISA are shown to be essential for the correct interpretation of the biclusters found by the algorithm. The median instead of the average turns the biclustering algorithm more resilient to outliers in the data sets. Copyright © 2011 Wiley Periodicals, Inc.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Classification methods for finding articles describing protein-protein interactions in PubMed.",
        "doc_scopus_id": "84855546021",
        "doc_doi": "10.1515/jib-2011-178",
        "doc_eid": "2-s2.0-84855546021",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "With the rapid expansion in the number of published papers in the biomedical field, finding relevant articles has become a demanding task for researchers. This has led to increasing interest in the use of text mining tools that help search the literature and identify the most relevant documents or information. One specific topic of interest is related to the identification of articles that might be used for extracting protein-protein interactions. Using the BioCreative III Article Classification Task dataset, composed of PubMed abstracts classified as relevant or non-relevant for describing protein-protein interactions, we compare different classification methods with different sets of features. The best results--area under the interpolated precision-recall curve of 0.654--indicate that the proposed classification strategy could be incorporated in the database curation workflows in order to prioritize articles for extraction of protein-protein interactions. Furthermore, we also analysed the use of this method for ranking documents resulting from general PubMed queries, and propose that this approach could be useful for general researchers looking for publications describing protein-protein interactions within a particular topic of interest. Copyright 2011 The Author(s). Published by Journal of Integrative Bioinformatics.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Discussion of \"Biomedical ontologies: Toward scientific debate\"",
        "doc_scopus_id": "80155199311",
        "doc_doi": "10.1055/s-0038-1625243",
        "doc_eid": "2-s2.0-80155199311",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Advanced and Specialized Nursing",
                "area_abbreviation": "NURS",
                "area_code": "2902"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Recommendations for use of marginal donors in heart transplantation: Brazilian association of organs transplantation guideline",
        "doc_scopus_id": "79951792323",
        "doc_doi": "10.1016/j.transproceed.2010.12.047",
        "doc_eid": "2-s2.0-79951792323",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Transplantation",
                "area_abbreviation": "MEDI",
                "area_code": "2747"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The high prevalence of heart failure has increased the candidate list for heart transplantation; however, there is a shortage of viable donated organs, which is responsible for the high mortality of patients a waiting a transplantation. Because the marginal donor presents additional risk factors, it is not considered to be an ideal donor. The use of a marginal donor is only justified in situations when the risk of patient death due to heart disease is greater than that offered by the donor. These recommendations sought to expand the supply of donors, consequently increasing the transplant rate. We selected articles based on robust evidence to provide a substratum to develop recommendations for donors who exceed the traditional acceptance criteria. Recipient survival in the immediate postoperative period is intimately linked to allograft quality. Primary allograft failure is responsible for 38% to 40% of immediate deaths after heart transplantation: therefore; marginal donor selection must be more rigorous to not increase the surgical risk. The main donor risk factors with the respective evidence levels are: cancer in the donor (B), female donor (B), donor death due to hemorrhagic stroke (B), donor age above 50 years (relative risk [RR] = 1.5) (B), weight mismatch between donor and recipient < 0.8 (RR = 1.3) (B), ischemia > 240 minutes (RR = 1.2) (B), left ventricular dysfunction with ejection fraction below 45% (B), and use of high doses of vasoactive drugs (dopamine > 15 mg/kg·min) (B). Factors that impact recipient mortality are: age over 50 years (RR = 1.5); allograft harvest at a distance; adult recipient weighing more than 20% of the donor; high doses of vasoactive drugs (dopamine greater than 15 mg/kg·min) and ischemic time >4 hours. The use of a marginal donor is only justified when it is able to increase life expectancy compared with clinical treatment, albeit the outcomes are interior to those using an ideal donor. © 2011 by Elsevier Inc. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271972 291210 291683 291912 31 Transplantation Proceedings TRANSPLANTATIONPROCEEDINGS 2011-02-16 2011-02-16 2013-06-18T09:57:31 S0041-1345(10)01962-7 S0041134510019627 10.1016/j.transproceed.2010.12.047 S300 S300.1 FULL-TEXT 2015-05-14T02:53:04.26492-04:00 0 0 20110101 20110228 2011 2011-02-16T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue table body affil articletitle auth authfirstini authfull authlast primabst pubtype ref alllist content subj ssids 0041-1345 00411345 false 43 43 1 1 Volume 43, Issue 1 45 211 215 211 215 201101 201102 January–February 2011 2011-01-01 2011-02-28 2011 9th Luso-Brazilian Congress of Transplantation Thoracic Transplantation article fla Copyright © 2011 Elsevier Inc. All rights reserved. RECOMMENDATIONSFORUSEMARGINALDONORSINHEARTTRANSPLANTATIONBRAZILIANASSOCIATIONORGANSTRANSPLANTATIONGUIDELINE FIORELLI A Definition, Classification, and Relevance of the Problem Risk Factors and Stratification Social, Epidemiological History, Serology, and Infection Impact of Marginal Donor in Pediatric Recipients Medical Criteria Versus Ethics References LIETZ 2004 1553 K CHEN 2002 608 J NAGJI 2010 168 A VENETTONI 2007 279 S STEHLIK 2010 1089 J LIMA 2006 127 B WEISS 2009 401 E XU 2010 873 D MANCINI 2010 173 D SINGHAL 2009 3539 A TAMISIER 1996 190 D ALI 2009 290 A CHAN 1991 1230 B SCHULER 1988 326 S MORATH 2005 S164 C JEEVANANDAM 1996 1268 V HOUYEL 1992 1184 L BYRNE 2005 335 P MASSIMELLI 2007 83 M 2010 FEDERALOFFICIALDAILY RESOLUTIONFEDERALCOUNCILMEDICINEBRAZILCFMNO19492010 FELKER 2005 1781 G CHEN 2005 224 J BRUZZONE 2008 1064 P WOOD 2004 2730 K GASINK 2006 1843 L KIRK 2010 1119 R FUKUSHIMA 1994 985 N WEST 2006 455 L SARMENTO 2000 2571 A FREEMAN 1999 1107 R FIORELLIX2011X211 FIORELLIX2011X211X215 FIORELLIX2011X211XA FIORELLIX2011X211X215XA item S0041-1345(10)01962-7 S0041134510019627 10.1016/j.transproceed.2010.12.047 271972 2013-06-18T07:51:39.323121-04:00 2011-01-01 2011-02-28 true 146781 MAIN 5 73943 849 656 IMAGE-WEB-PDF 1 TPS 22594 S0041-1345(10)01962-7 10.1016/j.transproceed.2010.12.047 Elsevier Inc. Table 1 Risk Factors for the Viability of the Allograft Survival of the Recipient or Both • Donor with a history of cancer (B) 4,5 • Receiver male and female donor (B) 5 • Death of the donor due to hemorrhagic strome (B) 5 • Left ventricular dysfunction with ejection fraction below 45% (B) 5 • Donor > 50 years present RR > 1.5 (B) 1,3,5, ⁎ • Excessive use of cocaine prior to death or the cause responsible for it is a risk factor for primary allograft failure, for the deleterious effects of vasoconstriction on the myocardium (C) 17 • Ischemia time < 180 minutes (RR < 1.0); 240 minutes (RR = 1.2); 360 minutes (RR = 1.5), increasing the risk of death (B) 5 , ⁎ • Use of high doses of vasoactive drugs (dopamine greater than 15 mg/kg·min) (B) 4 , ⁎ • The search distance alone is a risk factor and it worsened when ischemia is >4 hours (C) 5 , ⁎ • Relationship between the donor and recipient weight or less than 0.3 (RR = 1.3) (B) 4,13 , ⁎ RR, relative risk. ⁎ An isolated risk factor for death. Table 2 Primary Tumors of the Central Nervous System Suitable for Donation Low risk of transmission and suitable for the organs donation (B) 4,15 Glial Neoplasms Anaplastic astrocytoma Pilocytic astrocytoma Subependymoma Xantoastrocytoma pleomorphic Ependymoma Oligoastrocytoma Myxopapillary ependymoma Choroid plexus papillomas Third ventricular chordoid glioma Oligodendroglioma Subependymal giant cells astrocytoma Neuronal and mixed neuronal-glial neoplasms Gangliocytoma Ganglioglioma Central neurocytoma Dysplastic gangliocytoma of cerebellum Liponeurocytoma of cerebellum Astrocytoma/desmoplastic infantile ganglioglioma Meningiomas Atypical meningioma Clear cell meningioma Choroid meningioma Miscellaneous Pinealocytoma Craniopharyngiomas Acoustic schwannomas Hemangioblastomas ⁎ Mature teratomas High Risk of Transmission—relative Contraindication (B) 4,15 Anaplastic astrocytoma Anaplastic oligodendroglioma Anaplastic oligoastrocytoma Anaplastic ependymoma Choroid plexus tumor Gliomatosis cerebri ⁎ Not associated with the syndrome of von Hippel-Lindau. Table 3 Primary Tumors of the Central Nervous System Considered as Contraindication for Donation 4,13 Glial tumors Multiform gliobastoma Embryonal tumors Pineoblastomas Medulloblastoma Primitive neuroectodermal supratentorial tumor Atypica theratoid/rabdoid tumors Medulloepithelioma Ependymoblastoma Tumors of germinal cells Germinoma Embryonal tumors Tumor of the vitellin sac Choriocarcinoma immature teratomas Teratomas with malignant transformation Other frequently metastatic tumors Malignant meningiomas Hemangiopericytomas Meningeal sarcoma Chordomas Malignant lymphomas Table 4 Risk Factors With Suspicion of Infection in Donor Donor HIV-positive or HTLV is contraindicated (D). 14,24 Donor with acute systemic infection such as measles, rubella, adenovirus, enterovirus, meningoencephalitis herpes, malaria, hepatitis viral activity, or disseminated tuberculosis is contraindicated (D). 14,24 The use of donors with HBV or HCV should be contraindicated; which receptors are priority with high risk of death is questionable (B). 25 Localized infection in the donor is not a contraindication to the use of graft. (D) 24 Systemic infection in donors with positive hemocultures and with specific treatment of more than 48 hours does not counterindicate; the use of allograft should be performed to a specific prophylaxis in the receptor (B) (D). 15,24 The Donor cytomegalovirus infection does not counterindicate the use of allograft; in these cases use routine prophylaxis in the receptor (D). 24 HIV, human immunovirus deficiency; HTLV, HBV, hepatitis B virus; HCV, hepatitis C virus. Table 5 Risk Factors to Pediatric Recipients Donor weight > 300% of the recipient (B) 11,24 Female donor (B) 11,24 Ischemia time > 5–6 h (B) 25,27 ABO incompatibility is contraindicated for neonates; pediatric recipients show good immune tolerance with impact on survival of patients on the waiting list, however, worse results compared with those receiving allograft from compatible donors (B) 26,28 Thoracic transplantation Recommendations for Use of Marginal Donors in Heart Transplantation: Brazilian Association of Organs Transplantation Guideline A.I. Fiorelli ⁎ N.A.G. Stolf P.M. Pego-Fernandes J.L. Oliveira Junior R.H.B. Santos C.A.M. Contreras D.D.L. Filho J.J. Dinkhuysen M.C.V. Moreira J.A.C. Mejia M.C.R. Castro Brazilian Association for Organ Transplantation (ABTO) and Brazilian Medical Association (AMB), Brazil ⁎ Address reprint requests to Alfredo I. Fiorelli, Rua Morgado de Mateus 126/81, Sao Paulo/SP, Brazil. CEP: 0415-050 The high prevalence of heart failure has increased the candidate list for heart transplantation; however, there is a shortage of viable donated organs, which is responsible for the high mortality of patients a waiting a transplantation. Because the marginal donor presents additional risk factors, it is not considered to be an ideal donor. The use of a marginal donor is only justified in situations when the risk of patient death due to heart disease is greater than that offered by the donor. These recommendations sought to expand the supply of donors, consequently increasing the transplant rate. We selected articles based on robust evidence to provide a substratum to develop recommendations for donors who exceed the traditional acceptance criteria. Recipient survival in the immediate postoperative period is intimately linked to allograft quality. Primary allograft failure is responsible for 38% to 40% of immediate deaths after heart transplantation: therefore; marginal donor selection must be more rigorous to not increase the surgical risk. The main donor risk factors with the respective evidence levels are: cancer in the donor (B), female donor (B), donor death due to hemorrhagic stroke (B), donor age above 50 years (relative risk [RR] = 1.5) (B), weight mismatch between donor and recipient < 0.8 (RR = 1.3) (B), ischemia > 240 minutes (RR = 1.2) (B), left ventricular dysfunction with ejection fraction below 45% (B), and use of high doses of vasoactive drugs (dopamine > 15 mg/kg·min) (B). Factors that impact recipient mortality are: age over 50 years (RR = 1.5); allograft harvest at a distance; adult recipient weighing more than 20% of the donor; high doses of vasoactive drugs (dopamine greater than 15 mg/kg·min) and ischemic time >4 hours. The use of a marginal donor is only justified when it is able to increase life expectancy compared with clinical treatment, albeit the outcomes are interior to those using an ideal donor. The Brazilian Association for Organ Transplantation (ABTO) and Brazilian Medical Association (AMB) constituted a task force to define guidelines on the use of donor hearts that are not considered to be ideal. After extensive review of literature on the subject, we selected articles with the highest levels of evidence. The final document was judged at a plenary session including various specialists expert in transplantation. The level of recommendation and strength of evidence derived from the articles were considered to be: (A) experimental or observational studies of high consistency, (B) experimental or observational studies of lower consistency, (C) Case reports (uncontrolled studies), and (D) opinion devoid of critical assessment but based on consensus opinions regarding physiological studies or animal models. The categorical variables were subjected to multivariable analyses that whenever possible were reported together with relative risks (RR): A RR < 1.0 means the factor was associated with a lower and a RR > 1.0, a higher probability of the event. This document sought to assist in decision making regarding the use of borderline donors including complications, ethical issues, and expected results with the group. We had no intention of establishing strict criteria for indications for or contraindications to the use of specific donors. Definition, Classification, and Relevance of the Problem The high prevalence of heart failure has generated an increased number of waiting list candidates for heart transplantation albeit with a decreased availability of viable organs (B). 1 Broadening of donor heart selection criteria within safety limits has been the principal means to expand transplant numbers. This fact is corroborated by the difficulties in the use of mechanical circulatory support in many transplantation centers and the lack of alternatives for priority or high-risk recipients (B). 1 A donor heart is considered to be not ideal when it presents unfavorable of cardiac or systemic clinical features, which were previously regarded to be contraindications for heart transplantation. Logistical issues involving the transplantation may become significant factors in a particular donor who is considered appropriate for some but not other recipients. In recent years, expanded limits on donor acceptance have sought to decrease the organ shortage; however, many times these limits are doubtful or uncertain. This new class of donors has received different unfavorable denominations: “expanded criteria,” “not ideal,” “expanded,” “high risk,” or “marginal,” the name adopted herein (B) (C). 2,3 Risk Factors and Stratification Sometimes, the risk factors analysis does not allow one to distinguish those features that interfere directly with allograft function or promote mortality. Stratification of assumed risk is important in organs selection. 4 Unacceptable risk includes the use of an organ contraindicated for: tumor history with the following possible exceptions of primary central nervous system neopleams, carcinoma in situ, or cutaneous squamous cell carcinoma without metastases: HIV seropositive, HbsAg or Chaga's disease; sepsis with no viable treatment or documented prior disease. Acceptable risk includes the increased risk justified only by urgency or severity of illness when transplantation is the only chance for the patient's survival. Organ utilization may be allowed in this setting despite the presence of pathogens or transmissible disease. In these cases calculated risk includes the presence of a specific pathogen or a serological status of the donor (HBsAg+, or anti-hepatitis C virus-positive or HbcAb+) is compatible with the recipient's illness. Unknown risk includes when the evaluation process does not allow an appropriate risk assessment due to lack of donor information and can be only considered when the evaluation yields negative results. Standard risk is the most frequent condition in the assessment: the evaluation process does not identify an additional risk factor. It must be emphasized that the donor selection must occur in the light of a risk-benefit assessment of the recipient. For allograft allocation it is only necessary to respect ABO blood compatibility; however, a multicenter experience has shown that compatible (but nonidentical) ABO matching and prior blood transfusions were associated with approximately a 20% increase of 1 year mortality risk (RR = 1.21) (B). 5 Unfortunately, the availability of donors is short and does not help in the success of achieving this goal. A female donor, HLA-B or -DR mismatches, cerebrovascular accident as the cause of death, history of hypertension, donor history of cancer, increased donor age, increased allograft ischemic time, donor body mass index (BMI), and cytomegalovirus (CMV) mismatches are significant risk factors that exert greater expressiveness in the early years of transplantation. 6–11 The impact of these factors oscillates over the years with increasing or decreasing significance for morbidity or mortality after heart transplantation. For example, a donor who has a history of cancer or who died due to a cerebrovascular cause are both significant risk factors currently, although they were not deemed significant in the past. 6,9,10 Donor heart ischemia time is one of the risk factors of death that increases directly with recipient age, while the impact of donor heart ischemia time on 1-year survival is not statistically significant in the youngest age group (18–30 years), an ischemia time of 320 minutes in a 31- to 60-year-old recipient confers a similar relative risk of 1-year mortality as a 280-minute donor heart ischemia time in a 61- to 75-year-old recipient. An ischemia time of 360 minutes showed RR values of about 1.25, 1.70, and 2.25, for recipient ages of 18 to 30, 31 to 60, and 61 to 75-year-old, respectively (B). 5 Besides the ischemia time, other risk factors may be potentiated by increased recipient age, such as greater donor age; decreased donor/recipient body surface area ratio decreased donor/recipient BMI ratio; non-ABO-identical, which show RR = 1.23 for recipients 31 to 60 years old and RR = 1.45 for those of 61 to 75 years. Female recipient of male donor versus male recipient of male donor graft showed an RR = 1.75 for recipients 61 to 75 years of age (B). 5 Donor CMV-positive/recipient CMV-negative mismatch appeared to have a marginally significant beneficial effect (RR = 0.87 P = .032). The principle risk factors for mortality within 10 years after heart transplantation are: female recipient/male donor (RR = 1.27); female recipient/female donor (RR = 1.24); male recipient/female donor (RR = 1.1); donor CMV−/recipient CMV+ (RR = 0.87); and increased BMI donor (B). 5 This information suggests that younger versus older donor hearts show greater tolerance to longer ischemia times. Likewise, younger recipients also have more tolerance to aggression, perhaps because they show greater reversibility of hepatic dysfunction and pulmonary arterial hypertension. The use of non–heart-beating donors which had historical aspects relevant to the first heart transplantations, was soon abandoned; however, recent cases reports show success of this practice although there is yet no evidence for its routine use. 12 Weight and height should not be used as the only exclusion criteria when there is extreme discrepancy between donor and recipient. In these situations, it is recommended to perform echocardiographic estimates of the allograft dimensions and function (B) (D). 2,8 Donation should be avoided in cases of severe chest trauma with prover myocardial contusion or echocardiography changes. In the absence of evidence of abnormalities, there is no restriction on the use of an allograft (B). 14 Table 1 shows the main risk factors for allograft viability and recipient survival (B). 2 Malignant tumors generally are contraindicated for use of a heart, except cutaneous neoplasms of low-grade malignancy (B), 4,15 carcinoma in situ of the uterus (B), 4,15 and primary central nervous system tumors (Table 2). Table 3 describes the primary tumors of the central nervous system considered to be contraindications for organ donation. The principle risk factors for developing coronary artery vasculopathy within 8 years after heart transplantation are: donor history of hypertension (RR = 1.39); donor clinical infection (RR = 1.13); number of DR locus mismatches (RR = 1.09); number of class I mismatches (RR = 0.92); decreased donor/recipient height difference; increased donor age and donor age/gender interaction (B). 5 The principle risk factors for developing severe renal dysfunction within 1 year after heart transplantation are: central nervous system tumor cause of death (RR = 1.86) and decreased donor/recipient height difference. The donor/recipient height difference is not entirely understood; perhaps it reflects the use of more undersized donor hearts in critically ill recipients. 5,13 Social, Epidemiological History, Serology, and Infection Patients considered to be at high risk for acquired immunodeficiency syndrome are the drug-addicted, sexual behavior change or both. However, when the HIV and hepatitis serology is negative, the subject can be considered for donation (B). 14 Chronic alcoholism is an important risk factor for allograft dysfunction; therefore, it should be well analyzed before accepting the heart (B). 11,16,17 Table 4 presents the main absolute contraindications and acceptance criteria for heart donation in a patient suspected of infection. Impact of Marginal Donor in Pediatric Recipients The use of anencephalic organs for transplantation gained wide publicity in 1980 due to the group of Loma Linda. However, later authors concluded that medical and legal support were necessary to regulate brain death in these cases. Currently, there is still much controversy in the literature on the subject. 18,19 In 2010, the Brazilian Federal Council of Medicine, the most important official organization that regulates medical practice, overturned its previous decision and banned the use of anencephalic organ donors. 20 The principles that guide donor selection in the pediatric population are similar to those in adult; however, the organ shortage in the former setting is more serious, thus marginal donors assume special prominence. The tolerance for marginal donors in the pediatric age range is greater; the main factors that affect the allograft or patient survival are described in Table 5. Medical Criteria Versus Ethics The use of marginal donors seeks to increase organ availability; however, it should be emphasized that the results are not similar to those obtained with ideal donors (B). 21,22 Therefore, the use of these donors is better indicated for patients listed as high priority, or belonging to a less common blood group (B/AB), or to expand indications in unconventional or controversial situations, such as: (1) elderly recipients (>65 years) with insulin-dependent diabetes mellitus or target organ damage (B); 4,15 (2) chronic retransplantation (B), (3) rheumatoid arthritis multiple sclerosis (B), (4) renal failure (creatinine > 2 mg/dL) (B), and (5) hepatitis C or HIV virus infection (B). 21,22,29,30 One must always guarantee respect to the list of organ recipients. The legal term of the informed consent for a marginal donor organ must be specific about emphasizing the risks of not using an ideal donor while emphasizing the benefits of increased survival relative to the natural evolution of cardiomyopathy (B). 21,22 The concept of a marginal donor is controversial in the literature; however, despite the fact these donors yield inferior results when compared with those considered ideal, they must ensure better results than the conventional treatments. Utilization of marginal donor hearts has varied among transplant centers. The alternate list has been a strategy proposed by some centers to increase the use of the marginal donors in recipients who do not meet the standard criteria for heart transplantation, such as advanced age, followed by diabetes mellitus with end-organ dysfunction; positive hepatitis serologies; peripheral vascular disease; chronic renal insufficiency; and prior stroke with residual deficit. If the use of donors rejected by all transplantation teams produces results similar to those obtained with conventional recipients it is important to review the concepts of marginal donor and the alternate lists of recipients (B). 21–22 In conclusion, acceptance of proximates donors can increase the supply of organs for transplant by 30% to 40%. However, the acceptance of a marginal donor heart is only justified when if is possible to offer greater hope of survival than that achieved with existing clinic treatment (B). 21,23 References 1 K. Lietz R. John D.M. Mancini Outcomes in cardiac transplant recipients using allografts from older donors versus mortality on the transplant waiting list: implications for donor selection criteria J Am Coll Cardiol 43 2004 1553 2 J.M. Chen P. Sinha H.A. Rajasinghe Do donor characteristics really matter? Short- and long-term impact of donor characteristics on recipient survival, 1995–1999 J Heart Lung Transplant 21 2002 608 3 A.S. Nagji T. Hranjec B.R. Swenson Donor age is associated with chronic allograft vasculopathy after adult heart transplantation: implications for donor allocation Ann Thorac Surg 90 2010 168 4 S. Venettoni W. Grigioni P. Grossi Criteria and terms for certified suitability of organ donors: assumptions and operational strategies in Italy Ann Ist Super Sanita 43 2007 279 5 J. Stehlik L.B. Edwards A.Y. Kucheryavaya The Registry of the International Society for Heart and Lung Transplantation: twenty-seventh official adult heart transplant report—2010 J Heart Lung Transplant 29 2010 1089 6 B. Lima K. Rajagopal R.P. Petersen Marginal cardiac allografts do not have increased primary graft dysfunction in alternate list transplantation Circulation 114 1 suppl 2006 127 7 E.S. Weiss J.G. Allen N.D. Patel The impact of donor-recipient sex matching on survival after orthotopic heart transplantation: analysis of 18 000 transplants in the modern era Circ Heart Fail 2 2009 401 8 D.S. Xu D. Hartman K. Ludrosky Impact of donor high-risk social behaviors on recipient survival in cardiac transplantation Transplantation 89 2010 873 9 D. Mancini K. Lietz Selection of cardiac transplantation candidates in 2010 Circulation 122 2010 173 10 A.K. Singhal X. Sheng S.G. Drakos Impact of donor cause of death on transplant outcomes: UNOS registry analysis Transplant Proc 41 2009 3539 11 D. Tamisier P. Vouhé J. Le Bidois Donor-recipient size matching in pediatric heart transplantation: a word of caution about small grafts J Heart Lung Transplant 15 1996 190 12 A. Ali P. White K. Dhital Cardiac recovery in a human non-heart-beating donor after extracorporeal perfusion: source for human heart donation? J Heart Lung Transplant 28 2009 290 13 B.B. Chan K.J. Fleischer J.D. Bergin Weight is not an accurate criterion for adult cardiac transplant size matching Ann Thorac Surg 52 1991 1230 14 S. Schüler R. Parnt H. Warnecke Extended donor criteria for heart transplantation J Heart Transplant 7 1988 326 15 C. Morath V. Schwenger J. Schmidt Transmission of malignancy with solid organ transplants Transplantation 80 1 suppl 2005 S164 16 V. Jeevanandam S. Furukawa T.W. Prendergast Standard criteria for an acceptable donor heart are restricting heart transplantation Ann Thorac Surg 62 1996 1268 17 L. Houyel J. Petit R. Nottin Adult heart transplantation: adverse role of chronic alcoholism in donors on early graft function J Heart Lung Transplant 11 1992 1184 18 P. Byrne L. Arbour C. Fernandez Use of anencephalic newborns as organ donors Paediatr Child Health 10 2005 335 19 M. Massimelli The anencephalic newborn: medical/legal and bioethical issues Panminerva Med 49 2007 83 20 Resolution Federal Council of Medicine (Brazil)—CFM No. 1949/2010 Federal Official Daily June 6, 2010 section I, p 85 21 G.M. Felker C.A. Milano J.E. Yager Outcomes with an alternate list strategy for heart transplantation J Heart Lung Transplant 24 2005 1781 22 J.M. Chen M.J. Russo K.M. Hammond Alternate waiting list strategies for heart transplantation maximize donor organ utilization Ann Thorac Surg 80 2005 224 23 P. Bruzzone Religious aspects of organ transplantation Transplant Proc 40 2008 1064 24 K.E. Wood B.N. Becker J.G. McCartney Care of the potential organ donor N Engl J Med 351 2004 2730 25 L.B. Gasink E.A. Blumberg A.R. Localio Hepatitis C virus seropositivity in organ donors and survival in heart transplant recipients JAMA 296 2006 1843 26 R. Kirk L.B. Edwards A.Y. Kucheryavaya The Registry of the International Society for Heart and Lung Transplantation: thirteenth official pediatric heart transplantation report—2010 J Heart Lung Transplant 29 2010 1119 27 N. Fukushima S.R. Gundry A.J. Razzouk Risk factors for graft failure associated with pulmonary hypertension after pediatric heart transplantation J Thorac Cardiovasc Surg 107 1994 985 28 L.J. West T. Karamlou A.I. Dipchand Impact on outcomes after listing and transplantation, of a strategy to accept ABO blood group-incompatible donor hearts for neonates and infants J Thorac Cardiovasc Surg 131 2006 455 29 A. Sarmento F. Freitas A.P. Tavares Organ donor viral screening and its implications in transplantation: an overview Transplant Proc 32 2000 2571 30 R.B. Freeman I. Giatras M.E. Falagas Outcome of transplantation of organs procured from bacteremic donors Transplantation 68 1999 1107 "
    },
    {
        "doc_title": "Successful endomyocardial biopsy guided by transthoracic two-dimensional echocardiography",
        "doc_scopus_id": "79951783115",
        "doc_doi": "10.1016/j.transproceed.2010.12.049",
        "doc_eid": "2-s2.0-79951783115",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Transplantation",
                "area_abbreviation": "MEDI",
                "area_code": "2747"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Introduction: Two-dimensional (2-D) echocardiography is an excellent alternative method to perform endomyocardial biopsies (EB) in special situations, mainly when the patient is in a critical state and cannot go to the catheterization laboratory or when there are contraindications to the use of fluoroscopy as in the pregnancy. Objective: This single-center experience analyzed the last 25 years use of an EB technique guided by echocardiography realized at the bedside on critical patients. Methods: From 1985 to 2010, we performed 76 EB guided by 2-D echocardiography on 59 patients, among whom 38 (64.4%) were critically ill with examinations at the bedside; among 10 (16.9%) subjects, the procedure was carried out simultaneously with fluoroscopy for safety's sake during the learning period. In addition, 8 (13.6%) were unavailable for fluoroscopy, and 3 (5.1%) required a hybrid method due to an intracardiac tumor. Results: The main adverse effects included local pain (n = 4, 5.6%); difficult out successful puncture due to previous biopsies (n = 4, 5.6%); local hematoma without major consequences (n = 3, 4.2%); failed but ultimately successful puncture on the first try due to previous biopsies or (n = 3, 4.2%); obesity and immediate postoperative period with impossibility to pass the bioptome into the right ventricle; however 2 days later the procedure was repeated successfully by echocardiography (n = 1, 1.4%). All myocardial specimens displayed suitable size. There were no undesirable extraction effects on the tricuspid valve tissue. In this series, there was no case of death, hemopericardium, or other major complication as a direct consequence of the biopsy. Conclusion: 2-D echocardiography is a special feature to guide EB is mainly in critically ill patients because it can be performed at the bedside without additional risk or disadvantages of fluoroscopy. The hybrid method associating 2-D echocardiography and fluoroscopy allows the procedure in different situations such as intracardiac tumor cases. © 2011 by Elsevier Inc. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271972 291210 291683 291912 31 Transplantation Proceedings TRANSPLANTATIONPROCEEDINGS 2011-02-16 2011-02-16 2013-06-18T09:57:31 S0041-1345(10)01964-0 S0041134510019640 10.1016/j.transproceed.2010.12.049 S300 S300.1 FULL-TEXT 2015-05-14T02:53:04.26492-04:00 0 0 20110101 20110228 2011 2011-02-16T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure body affil articletitle auth authfirstini authfull authlast authsuff primabst pubtype ref alllist content subj ssids 0041-1345 00411345 false 43 43 1 1 Volume 43, Issue 1 48 225 228 225 228 201101 201102 January–February 2011 2011-01-01 2011-02-28 2011 9th Luso-Brazilian Congress of Transplantation Thoracic Transplantation article fla Copyright © 2011 Elsevier Inc. All rights reserved. SUCCESSFULENDOMYOCARDIALBIOPSYGUIDEDBYTRANSTHORACICTWODIMENSIONALECHOCARDIOGRAPHY FIORELLI A Methods Results Discussion References BEDANOVA 2004 622 H GRANDE 1997 877 A BALZER 1993 510 D DRURY 1997 469 J PYTLEWSKI 1994 1019 G SAKAKIBARA 1962 537 S FIORELLI 2009 935 A JACKSON 2009 171 C ABRAMOWITZ 2007 e39 Y FIORELLIX2011X225 FIORELLIX2011X225X228 FIORELLIX2011X225XA FIORELLIX2011X225X228XA item S0041-1345(10)01964-0 S0041134510019640 10.1016/j.transproceed.2010.12.049 271972 2013-06-18T07:51:39.322438-04:00 2011-01-01 2011-02-28 true 342997 MAIN 4 72932 849 656 IMAGE-WEB-PDF 1 gr1 206045 761 1456 gr1 36202 286 548 gr1 6230 114 219 TPS 22596 S0041-1345(10)01964-0 10.1016/j.transproceed.2010.12.049 Elsevier Inc. Fig 1 (A) Apical four-chamber view obtained by two-dimensional echocardiography. The bioptome (B) is seen (arrow) in the right atrium (RA) and into right ventricle (RV) the tip of bioptome (B) is in the lower third of the interventricular septum. (B) The jaws are opened (large arrows) in the middle of the interventricular septum. This stop-frame image is the exact moment of excision and a little traction is felt at the time of withdrawn the sample. Thoracic transplantation Successful Endomyocardial Biopsy Guided by Transthoracic Two-Dimensional Echocardiography A.I. Fiorelli ⁎ G.B. Coelho R.H.B. Santos J.L. Oliveira Jr V. Aielo L. Benvenuti A.S. Oliveira M.A.F. Da Silva P.R. Chizzola R. Costa W. Mathias Jr F. Bacal E.A. Bocchi N.A.G. Stolf Heart Institute of Sao Paulo University Medical School, Sao Paulo, Brazil ⁎ Address reprint requests to Alfredo I. Fiorelli, Rua Morgado de Mateus 126/81, Sao Paulo/SP, Brazil CEP: 04015-050 Introduction Two-dimensional (2-D) echocardiography is an excellent alternative method to perform endomyocardial biopsies (EB) in special situations, mainly when the patient is in a critical state and cannot go to the catheterization laboratory or when there are contraindications to the use of fluoroscopy as in the pregnancy. Objective This single-center experience analyzed the last 25 years use of an EB technique guided by echocardiography realized at the bedside on critical patients. Methods From 1985 to 2010, we performed 76 EB guided by 2-D echocardiography on 59 patients, among whom 38 (64.4%) were critically ill with examinations at the bedside; among 10 (16.9%) subjects, the procedure was carried out simultaneously with fluoroscopy for safety's sake during the learning period. In addition, 8 (13.6%) were unavailable for fluoroscopy, and 3 (5.1%) required a hybrid method due to an intracardiac tumor. Results The main adverse effects included local pain (n = 4, 5.6%); difficult out successful puncture due to previous biopsies (n = 4, 5.6%); local hematoma without major consequences (n = 3, 4.2%); failed but ultimately successful puncture on the first try due to previous biopsies or (n = 3, 4.2%); obesity and immediate postoperative period with impossibility to pass the bioptome into the right ventricle; however 2 days later the procedure was repeated successfully by echocardiography (n = 1, 1.4%). All myocardial specimens displayed suitable size. There were no undesirable extraction effects on the tricuspid valve tissue. In this series, there was no case of death, hemopericardium, or other major complication as a direct consequence of the biopsy. Conclusion 2-D echocardiography is a special feature to guide EB is mainly in critically ill patients because it can be performed at the bedside without additional risk or disadvantages of fluoroscopy. The hybrid method associating 2-D echocardiography and fluoroscopy allows the procedure in different situations such as intracardiac tumor cases. The Endomyocardial Biopsy (EB) technique routinely employs fluoroscopy to guide the bioptome with few adverse effects. In most cases, when there is a high risk to transfer the patient to the catheterization room or a contraindication to the use of fluoroscopy as in pregnancy, an echocardiogram is practical. Sometimes transthoracic or transesophageal echocardiography is useful not only to confirm the position of intracardiac tumors but also to guide the bioptome by providing two- or three-dimensional images. 1–3 The greatest advantage of two-dimensional echocardiography to guide an EB is the possibility of accomplish the procedure at the bedside and perhaps reduce the risk of perforation by better anatomic definition of the myocardial sampling site. It permits sampling from various ventricular sites, giving preference to the septum. 4,5 The aim of the present study was to present our experience at a single center during the last 20 years with the use of two-dimensional echocardiography to guide EB examinations with emphasis on critically ill patients, and calling attention to the advantages of fluoroscopy. Methods From 1985 to 2010 (25 years), we performed 76 EB guided by two-dimensional echocardiography in 59 patients, including 38 (64.4%) who were critically ill with the exam realized at the bedside; 10 (16.9%), simultaneous with fluoroscopy for safety's sake during the learning period; 8 (13.6%) unavailable for fluoroscopy; and 3 (5.1%), as a hybrid method due to an intracardiac tumor. The main indicators to EB were in 50 (65.8%) cases, rejection under surveillance, after heart transplantation which were performed in 37 (62.7%) patients; 12 (15.8%) with cardiomyopathy diagnosis before heart transplantation was achieved in 10 (16.9%) patients; 11 (14.5%) with myocarditis diagnosis performed in 9 (15.3%) patients with cardiogenic shock; and 3 (3.9%) with tumor diagnosis into the right chambers in 3 (5.1%) patients. After global echocardiographic evaluation to study cardiac chambers and ejection fraction, the transducer was placed at the subcostal area of the apex for an apical four-chamber view. This position provided better imaging to guide the exam, allowing observation of the movements of the bioptome and inside the heart. It was possible to identify the passage of the bioptome within the superior vena cava, right atrium, and through the tricuspid valve to reach the interventricular septum. An echocardiographic-dense image identified the bioptome and the forceps removing the fragments of the interventricular septum. Contrast saline solution injected into the venous system via the jugular vein improved identification of the cardiac chambers. In each exam, we removed three to six myocardial specimens for histopathologic analysis. We avoided withdrawal of myocardial samples from the free wall of the right ventricle due to the higher risk of perforation. The preferred route of access for percutaneous introduction of the bioptome was the right internal jugular vein using an 8.5-French introducer in 62 (81.6%) biopsies; in other cases, we used the left internal jugular (n = 11, 14.5%) or the femoral vein (n = 3, 3.9%). The catheter guide was useful in special situations such as when the femoral vein was the access route. Patients who needed continuous invasive cardiac monitoring had easy installation of a Swan-Ganz catheter after the biopsy. Hybrid guidance with fluoroscopy and two-dimensional echocardiography was useful to perform bioptome procedures with greater safety during the learning period and in patients with cardiac tumors. Results The main adverse effects were pain at the puncture site (n = 5, 6.6%) patients; puncture difficulty albeit successful due to previous biopsies or obesity (n = 5, 6.6%), local hematoma without major consequences (n = 4, 5.3%); chest pain (n = 3, 3.9%); unsuccessful puncture on the first try due to previous biopsies or obesity (n = 3, 3.9%) and difficulty due to immediate postoperative status but the successful 2 days later (n = 1, 1.3%). All myocardial specimens displayed suitable size, and there were not undesirable extraction events in the tricuspid valve or other tissues. There was no case of death, hemopericardium, hemopneumothorax, or other major complications as a direct consequence of the biopsy. The biatrial or bicaval heart transplantation operative technique was not an impediment to notice the EB with two-dimensional echocardiography guidance. Figure 1 shows the details of an EB image guided by two-dimensional echocardiography after heart transplantation using the classic technique. We were successful in visualizing the bioptome and the sampling site in all cases without a cardiovascular complication. No patient needed conversion from echocardiography to fluoroscopy guidance. A patient who was in the second month after heart transplantation was hospitalized in the intensive care unit with severe heart failure and suspicion of an acute rejection episode probably due to interruption of immunosuppression. The EB guided by two-dimensional echocardiography was indicated due to the impossibility of transportation. The ejection fraction was below 15%. When the patient was placed in the horizontal decubitus, cardiac arrest occurred during the puncture and the exam was immediately interrupted, however this complication was not related to the procedure. Discussion Since 1962 when Sakakibara and Konno developed the biopsy catheter, the percutaneous transvenous EB has become a safer, more convenient procedure for rejection surveillance after heart transplantation as well as for histopathologic diagnosis of cardiomyopathies and tumors. 6 The EB technique is safe in experienced hands; however, it may lead to several complications, the most serious being right ventricular perforation with cardiac tamponade. 2,4,7 Two-dimensional echocardiography has advantages over other imaging modalities, such as equipment mobility, X-ray elimination, and real-time images throughout the procedure. It can be done at the bedside, cardiac catheterization laboratory, cardiovascular intensive care unit, emergency room; indeed, any place that can accommodate a wheeled cart. It provides real-time images showing adequate spatial orientation and anatomic definition. It is helpful in the diagnosis of cardiomyopathy, visualization of intracardiac masses, detection of intracardiac catheters, as well as placement and localization of catheters mainly in the pediatric population. 3,8,9 However, the main disadvantage is the necessity for a specialist operator and the greater expense. In 2004, Bedanova et al reported experience with 1262 biopsies collected under echocardiography guidance in 156 patients. Only in 11 patients was there the need for conversion to X-ray guidance and no case of significant tricuspid regurgitation occurred related due to the biopsy. 1 After our learning period, the procedure duration time with echocardiographic guidance was comparable to fluoroscopy, namely 10 to 15 minutes. The percutaneous right jugular vein approach was employed more often due to its facility, offering few curves during advance of the bioptome. Both models can be used with catheter or sheath guidance. In patients with artificial cardiac pacing, the puncture is performed on the opposite side. In this series, the surgical technique of heart transplantation did not offer additional difficulties to EB by two-dimensional echocardiography, an observation in agreement with other investigations. 4,6 Difficulties in the progression of the bioptome occur frequently when there is severe stenosis at the site of the superior vena caval anastomosis. Recently the use of real-time transthoracic three-dimensional echocardiography in endomyocardial right ventricular biopsies has shown ease and safety both in children and in adults, perhaps with more benefits. Hands experienced to accomplish an EB by fluoroscopy show ease to learn two-dimensional echocardiography as the orientation guide. The view by fluoroscopy in the frontal (or coronal) plane follows the bioptome advance until the transition of the superior or inferior vena cava to the right atrium, and the subsequent entry into the right ventricle across the tricuspid valve. The view by two-dimensional echocardiography is in the transverse plane. The transducer is then located in the subcostal position, and a short-axis plane is visualized, showing the junction between the inferior vena cava and the right atrium. If the femoral vein approach is used, the bioptome can be seen entering the right atrium from the inferior vena cava. When the forceps enters the right atrium, the transducer is rotated to obtain a frontal plane as the subcostal four-chamber view is the more appropriate one (Fig 1). The bioptome advances under echocardiographic control, seen crossing the tricuspid valve and entering the right ventricular cavity. Generally, the primary positioning is not satisfactory, and the catheter must be manipulated using two-dimensional echocardiography to position the tip optimally in front of the interventricular septum. The most appalling complication during EB is perforation of the right or left ventricle with cardiac tamponade. It has a high risk of morbidity or death, especially if an inexperienced team is performing the manipulation. Sometimes it has been observed by fluoroscopy that it is difficult to differentiate the apical portion of the septum and the free wall due to dislocation of the tip of the bioptome until the cardiac apex. These situations of greater risk of ventricular perforation occur mainly in dilated cardiomyopathies. EB guidance by two-dimensional echocardiographic may reduce the incidence of complications by providing a better anatomic view of the adequate site where the specimens must be withdrawn, namely the septum and not the right ventricular free wall. In contrast, two-dimensional echocardiography adequately distinguishes the septum from the right ventricular free wall. It also permits patient follow-up after the procedure with immediate detection of complications, such as pericardial effusion or the appearance of a thrombus. Sometimes it is possible to identify the opening and closing of the bioptome jaw (Fig 1). The local pain, hematoma, and other minor complications may be more frequent but are not severe and are generally related to puncture difficulties. This study was not randomized and was performed in a single center with prospective data collection and retrospective analysis. In conclusion, two-dimensional echocardiography is a special method to guide EB mainly in critically ill patients, because it can be realized at the bedside and offers no additional risk with advantages over fluoroscopy. In special situations, mainly intracardiac tumor cases, the hybrid combination can be useful. References 1 H. Bedanova J. Necas E. Petrikovits Echo-guided endomyocardial biopsy in heart transplant recipients Transpl Int 17 2004 622 2 A.M. Grande G. Minzioni L. Martinelli Echo-controlled endomyocardial biopsy in orthotopic heart transplantation with bicaval anastomosis G Ital Cardiol 27 1997 877 3 D. Balzer S. Moorhead J.E. Saffitz Pediatric endomyocardial biopsy performed solely with echocardiographic guidance J Am Soc Echocardiogr 6 1993 510 4 J.H. Drury A.J. Labovitz L.W. Miller Echocardiographic guidance for endomyocardial biopsy Echocardiography 14 1997 469 5 G. Pytlewski S. Georgeson J. Burke Endomyocardial biopsy under transesophageal echocardiographic guidance can be safely performed in the critically ill cardiac transplant recipient Am J Cardiol 73 1994 1019 6 S. Sakakibara S. Konno Endomyocardial biopsy Jpn Heart J 3 1962 537 7 A.I. Fiorelli G.H. Coelho J.L. Oliveira Jr Endomyocardial biopsy as risk factor in the development of tricuspid insufficiency after heart transplantation Transplant Proc 41 2009 935 8 C.E. Jackson R.S. Gardner D.T. Connelly A novel approach for a novel combination: a transseptal biopsy of left atrial mass in recurrent phyllodes tumour Eur J Echocardiogr 10 2009 171 9 Y. Abramowitz N. Hiller G. Perlman The diagnosis of primary cardiac lymphoma by right heart catheterization and biopsy using fluoroscopic and transthoracic echocardiographic guidance Int J Cardiol 118 2007 e39 "
    },
    {
        "doc_title": "Heart transplantation in 107 cases of Chagas' disease",
        "doc_scopus_id": "79951778856",
        "doc_doi": "10.1016/j.transproceed.2010.12.046",
        "doc_eid": "2-s2.0-79951778856",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Transplantation",
                "area_abbreviation": "MEDI",
                "area_code": "2747"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Introduction: Chagas' disease is endemic in South America. Objective: This research reviewed the experience with cardiac transplantation in Chagas' disease, emphasizing reactivation, immunosuppression, and mortality. Methods: Over 25 years from March 1985 to March 2010, 107/409 (26.2%) patients with Chagas' disease underwent heart transplantation, patients including 74 (71.1%) men and 72 (67.2%), in functional class IV with 33 (30.8%) on vasopressors and 17 (10.7%) on mechanical circulatory support. Results: The diagnosis of disease reactivation was performed by identifying the parasite in the myocardium (n = 23; 71.8%) in the subcutaneous tissue (n = 8; 25.0%), in blood (n = 11; 34.3%), or in central nervous tissue (n = 1; 3.1%). Hospital mortality was 17.7% (n = 19) due to infection (n = 6; 31.5%), graft dysfunction (n = 6; 31.5%), rejection (n = 4; 21.1%), or sudden death (n = 2; 10.5%). Late mortality was 27 (25.2%) cases, which were distributed as: rejection (n = 6; 22.2%), infection (n = 6; 22.2%), (n = lymphoma 4; 14.8%), sarcoma (n = 2; 7.4%), for constrictive pericarditis (n = 2; 7.4%) reactivation of Chagas' disease in the central nervous system (n = 1; 7.1%). Conclusions: Transplantation in Chagas' disease has peculiar problems that differ from other etiologies due to the possibility of disease reactivation and the increased possibility of emergence of cancers. However, transplantation is the only treatment able to modify the natural progression of the disease in its terminal phase. Early diagnosis and rapid introduction of benzonidazole reverses the histological patterns. Immunosuppression, especially steroids, predisposes to the development of cancer and disease reactivation. © 2011 by Elsevier Inc. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271972 291210 291683 291912 31 Transplantation Proceedings TRANSPLANTATIONPROCEEDINGS 2011-02-16 2011-02-16 2013-06-18T09:57:31 S0041-1345(10)01961-5 S0041134510019615 10.1016/j.transproceed.2010.12.046 S300 S300.1 FULL-TEXT 2015-05-14T02:53:04.26492-04:00 0 0 20110101 20110228 2011 2011-02-16T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body affil articletitle auth authfirstini authfull authlast authsuff primabst pubtype ref alllist content subj ssids 0041-1345 00411345 false 43 43 1 1 Volume 43, Issue 1 47 220 224 220 224 201101 201102 January–February 2011 2011-01-01 2011-02-28 2011 9th Luso-Brazilian Congress of Transplantation Thoracic Transplantation article fla Copyright © 2011 Elsevier Inc. All rights reserved. HEARTTRANSPLANTATIONIN107CASESCHAGASDISEASE FIORELLI A Materials and Methods Statistical Analysis Results Discussion References CHAGAS 1911 219 C MILEI 2009 22 J GUERRIGUTTENBERG 2009 5 R DEVELOUX 2010 38 M SARKAR 2010 e836 S FIORELLI 2007 2527 A ALMEIDA 1995 I D VILA 1985 I J FIORELLI 2005 2793 A CARVALHO 1996 1815 V ASSEF 2001 289 M BOCCHI 2001 1833 E MOREIRA 2005 261 L BACAL 2010 E29 F BESTETTI 2010 1312 R FIORELLIX2011X220 FIORELLIX2011X220X224 FIORELLIX2011X220XA FIORELLIX2011X220X224XA item S0041-1345(10)01961-5 S0041134510019615 10.1016/j.transproceed.2010.12.046 271972 2013-06-18T07:51:39.322438-04:00 2011-01-01 2011-02-28 true 771901 MAIN 5 72443 849 656 IMAGE-WEB-PDF 1 gr2 853985 1646 2417 gr1 92336 2896 4833 gr2 83544 372 546 gr1 9474 327 546 gr2 18826 149 219 gr1 2600 131 219 TPS 22593 S0041-1345(10)01961-5 10.1016/j.transproceed.2010.12.046 Elsevier Inc. Fig. 1 Curve for freedom from reactivation of Chagas' disease after heart transplantation with a significant reduction in the second phase of clinical experience. Fig. 2 (A) Cranial magnetic resonance imaging revealed a nodule in the right parietal region, with expansive surrounding tissue compression and effacement of sulcus.The biopsy tissue confirmed the presence of the parasite in the central nervous system. (B) Presence of amastigote forms in myocardium detected in routine endomyocardial biopsy. Table 1 Clinical Characteristics of the Patients Before Heart Transplantation Range Average Follow-up (mo) 30–168 Age (y) 11–62 42.7±15.3 LVEF-echo (%) 13–62 32.4±9.8 PASP (mm Hg) 30–60 48.5±8.9 TPG (mm Hg) 9–32 11.6±7.9 PVR (U Wood) 1.7–5.4 3.6±2.8 Gender Male 74 71.1% Female 33 28.9% NYHA functional class IV 72 67.2% IV-shock 15 14.1% IV-IABP 14 13.1% IV-LVAD 3 2.8% III-arrhythmia 3 2.8% Pacemaker 21 19.6% ICD 8 7.4% Pacemaker + ICD 3 2.8% Resynchronization + steam cells 1 0.9% LVEF-echo, left ventricle ejection fraction by echocardiography; PASP, pulmonary artery systolic pressure; TPG, transpulmonary gradient; PVR, pulmonary vascular resistance; IABP, intra-aortic balloon pumping; LVAD, left ventricle assistance device; ICD, implantable cardioverter defibrillator. Table 2 Distribution of Initial Doses of Immunosuppressive Drugs Used in Heart Transplantation With Chagas' Disease in Different Ages 1985–1992 1992–2001 2001 2001–2010 Cyclosporine (mg/kg/d) 6.8±2.0 2.8±1.0 2.9±1.4 2.9±1.3 Azathioprine (mg/kg/d) 1.4±1.3 2.7±0.8 — 2.9±0.9 MMF (g/d) — — 2.0±0.5 — Steroids (mg/kg/d) 2.1±1.3 0.9±0.8 0.8±0.9 0.8±0.8 P <.05 >.05 MMF, mycophenolate mofetil. Thoracic transplantation Heart Transplantation in 107 Cases of Chagas' Disease A.I. Fiorelli ⁎ R.H.B. Santos J.L. Oliveira Jr D.D. Lourenço-Filho R.R. Dias A.S. Oliveira M.F.A. da Silva F.L. Ayoub F. Bacal G.E.C. Souza E.A. Bocchi N.A.G. Stolf Heart Institute of Sao Paulo University Medical School, Sao Paulo, Brazil ⁎ Address reprint requests to Alfredo I. Fiorelli, Rua Morgado de Mateus 126/81, Sao Paulo/SP, Brazil, CEP: 04015-050 Introduction Chagas' disease is endemic in South America. Objective This research reviewed the experience with cardiac transplantation in Chagas' disease, emphasizing reactivation, immunosuppression, and mortality. Methods Over 25 years from March 1985 to March 2010, 107/409 (26.2%) patients with Chagas' disease underwent heart transplantation, patients including 74 (71.1%) men and 72 (67.2%), in functional class IV with 33 (30.8%) on vasopressors and 17 (10.7%) on mechanical circulatory support. Results The diagnosis of disease reactivation was performed by identifying the parasite in the myocardium (n = 23; 71.8%) in the subcutaneous tissue (n = 8; 25.0%), in blood (n = 11; 34.3%), or in central nervous tissue (n = 1; 3.1%). Hospital mortality was 17.7% (n = 19) due to infection (n = 6; 31.5%), graft dysfunction (n = 6; 31.5%), rejection (n = 4; 21.1%), or sudden death (n = 2; 10.5%). Late mortality was 27 (25.2%) cases, which were distributed as: rejection (n = 6; 22.2%), infection (n = 6; 22.2%), (n = lymphoma 4; 14.8%), sarcoma (n = 2; 7.4%), for constrictive pericarditis (n = 2; 7.4%) reactivation of Chagas' disease in the central nervous system (n = 1; 7.1%). Conclusions Transplantation in Chagas' disease has peculiar problems that differ from other etiologies due to the possibility of disease reactivation and the increased possibility of emergence of cancers. However, transplantation is the only treatment able to modify the natural progression of the disease in its terminal phase. Early diagnosis and rapid introduction of benzonidazole reverses the histological patterns. Immunosuppression, especially steroids, predisposes to the development of cancer and disease reactivation. In 1908, the Brazilian researcher Carlos Chagas described American trypanosomiasis in an unprecedented manner in medicine with the simultaneous identification of the etiologic agent Trypanosoma cruzi and the insect vector that belongs to the subfamily Triatominae. 1 The disease is endemic in Latin America from Mexico, including the southern United States to Argentina, due to the presence of vectors suitable for transmission of the parasite. It is still a public health problem. It has been estimated that there are around 12 to 14 million people who are carriers of the parasite. Isolated cases have been described in the United States and Europe due to globalization increased migration. 2–5 Transplantation for this cardiomyopathy was initially much discussed because the disease is systemic and infectious without any possibility for sterilization of the parasite. However, the initial results confirmed the advantages of the operation as an alternative to rescue a patient with end-stage cardiomyopathy. This article has presented our accumulated experience over the past 25 years with heart transplantation for end-stage chagasic cardiomyopathy. Materials and Methods From March 1985 to March 2010, 25 years, 107 of 409 patients who underwent heart transplantation (26.2%) had Chagas' disease and were the targets of this investigation. This cardiomyopathy was the third most common cause for transplantation being preceded by ischemic and by dilated cardiomyopathy. The Chagas' disease diagnosis was confirmed by epidemiological history, complement fixation reactions, and immunoglobulin G (IgG)-positive immunofluorescence. Patients with megaesophagus or megacolon were excluded from the transplant program. Table 1 shows the main clinical characteristics of the patients before heart transplantation. At the beginning of this clinical experience, we employed the classical technique proposed by Shumway and Lower, later introducing the bicaval technique with a left unitrial anastomosis. A prophylactic tricuspid valve annuloplasty introduced in 2002 which was performed in 32 (29.9%) patients seeking to reduce valvular insufficiency as already reported. 6 The donor ages ranged from 13 to 55 years (27.1 ± 11.8 years), and donor body weight was never be low 20% of the recipient. The basic immunosuppression has always consisted of a calcineurin inhibitor, antiproliferative agent, and steroids. The first 10 (9.3%) patients were treated with prednisone (1 mg/kg/d) with gradual reduction to 0.2 mg/kg/d. Azathioprine was administered initially at 1.5 to 3.5 mg/kg/d and adjusted according to the leukocyte count or the grade of rejection. Cyclosporine was administered intravenously during the operation at a dose of 1 to 2 mg/kg, until the introduction of an oral diet. It was adjusted according to the grade of rejection or the concentration. Since 1992, there have been changes in the immunosuppression. The initial steroid doses were similar, but subsequently underwent earlier, more pronounced reductions. Azathioprine was increased to 3 mg/kg/d and cyclosporine reduced to maintain blood levels between 200 and 300 ng/mL during the immediate postoperative period and between 100 and 150 ng/mL in the late period. Subsequently, from 1992, the initial doses of steroids were maintained in the same proportions, but with early, marked reduction in function of the patient. Since 2001, a group of 15 (14.0%) patients received mycophenolate mofetil (MMF); (2 g) in lieu of azathioprine. Only two (1.8%) patients were prescribed tacrolimus due to cyclosporine intolerance. The disease reactivation diagnosis was performed by identifying the parasite in the myocardium, cutaneous nodules, or blood or by serological tests. The xenodiagnosis or blood culture was not considered to be necessary for the diagnosis of disease reactivation, due to the possibility of false-positive results. Episodes of paracytemia and reactivation of Chagas' disease were treated with benzonidazole (10 mg/kg/d for 60 days). Statistical Analysis Kaplan-Meier analysis curves were constructed for freedom from reactivation of Chagas' disease. The comparative analysis of immunosuppressives utilized Student t test with average values and their standard deviations. The significance level was considered to be 5%. Results Table 2 shows the distribution of immunosuppressant therapies in the immediate postoperative period after transplantation. There was a significant reduction in cyclosporine and steroids with increased azathioprine over the time periods. After 1992, we intensified steroid reduction in the late phases of transplantation; in some patients this drug was completely withdrawn. The hospital mortality was 17.7% (n = 19) and causes of death were infection (n = 6; 31.5%) graft dysfunction (n = 6; 31.5%), rejection (n = 4; 21.1%), sudden cardiac arrest (n = 2; 10.5%) or ABO incompatibility (n = 1; 5.3%). Of the three patients on mechanical circulatory support, two are alive and well, and one succumbed in the immediate postoperative period due to bleeding. Late mortality occurred in 27 (25.2%) patients. The main causes of death were rejection (n = 6; 22.2%), infection (n = 6; 22.2%), lymphoma (n = 4; 14.8%), sarcoma (n = 2; 7.4%), pericarditis (n = 2; 7.4%), and reactivation of Chagas' disease in the central nervous system (n = 1; 7.1%). Figure 1 shows the reactivation-free curve of Chagas' disease comparing the first phase (1985–1992) and second phase (2001–2010) of the study; there was a significant reduction in the second phase. Twenty-two (19.6%) patients showed reactivation of Chagas' disease, totaling 32 episodes (range = 1–8 episodes/patient). In the first phase, the average reactivation was 2.30 episodes per patient; in the second phase, it was 0.28 events per patient. The diagnosis of Chagas' disease reactivation was determined by endomyocardial biopsy in 23 (71.8%) episodes, by xenodiagnosis in 11 (34.3%), by biopsy of subcutaneous nodules in 8 (25.0%), or by central nervous system biops in 1 (3.1%) case. Figure 2 shows the ways to identify the parasite in tissues during disease reactivation. Laboratory evaluation by serial determinations of complement fixation reactions and IgG showed erratic behavior; episodes of disease reactivation did not seem to obey any relation to these determinations, where it would be possible to identify a predictor of early diagnosis of the event. The average time of onset of a lymphoproliferative disease or Kaposi's sarcoma was only 6.5 ± 2.1 months after transplantation. Disease reactivation before occurrence of neoplasm ranged between one and four episodes (mean = 2.3 ± 0.89 per patient). Discussion Brazil has applied remarkable and successful efforts to eliminate the transmission of the disease by its principal vector Triatoma infestans. In the evolving physiopathology of the disease, the parasite promotes an inflammatory reaction that causes progressive death of cardiomyocytes with continual replacement by fibrosis, culminating in the emergence of heart failure. Little more than a century since the description of Chagas' disease, this entity still offers great challenges to researchers with special emphasis on the immune status of the patient. Therefore,there is the possibility of reappearance of parasitemia similar to the acute phase. In heart transplantation, there is always a predisposition to reactivation by immunosuppression and steroid pulse therapy to control an acute rejection episode. Benzonidazole produces temporary elimination of T. cruzi during reactivation; however, it is not possible to completely sterilize the host. Thus, chagasic cardiomyopathy is a chronic process. Protocols that recommend prophylaxis with benzonidazole both pre- and immediately posttransplant, do not have clear advantages according to the clinical experience, although Almeida et al reported favorable experience using allopurinol to control parasitemia. 7 Despite the greater morbidity in relation to other cardiomyopathies for transplantation, this procedure has proven highly effective to modify the natural history of disease in its terminal phase. Currently, Chagas' cardiomyopathy is the third leading transplant indication, representing 15% to 20% of cases in our country. 8–11 In clinical practice, the diagnosis of disease reactivation is established by parasite identification in the myocardium, blood, or other tissue because the laboratory tests are nonspecific. Once diagnosis reactivation is confirmed, benzonidazole must be introduced promptly to avoid sequelae in the myocardium. This therapy is capable of eliminating the circulating parasites at around 2 weeks. Benzonidazole side effects are probably attenuated by Immunosuppression. In the first months, the immunosuppression is more intense due to the higher incidence of rejection episodes, thus predisposing patients to reactivation. The early reduction of immunosuppression, and especially the elimination of steroids, are currently accepted practices to reduce the incidence of reactivation and the appearance of neoplasia (19, 20) as can be seen in the evolution of patients in this series. Multicenter studies of transplantation in patients with chagasic have paradoxically shown a tendency toward their better survival, 12 despite the disease being considered a risk factor for patients on the waiting list. 13 It is believed that the low pulmonary vascular reactivity among these patients and perhaps the greater attention dedicated to them may play roles in the better survival results. Chagas' disease was not a risk factor for the development of allograft vasculopathy. The highest incidence of lymphomas observed in this group of patients may be explained by experimental and clinical studies. Benzonidazole is able to alter immune responses, probably due to its cytotoxic effects on T cells that may act synergistically with cyclosporine. Chagas' disease itself also has immunosuppressive behaviors. The introduction of MMF instead of azathioprine, increased the disease reactivation; however, other groups that used half of the recommended dose did not confirm these observations. There is a need for further investigation. 14,15 Chagas' disease has arrhythmogenic characteristics that increase the use of artificial cardiac pacing with a pacemaker or implantable defibrillator before transplantation. However, after transplantation the need for electrical therapy did not differ from other recipients. The limitation of the study was that it is an observational analysis of a series of cases at a single institution. In conclusion, heart transplantation in chagasic cardiomyopathy presents unique challenges that differ when compared with other etiologies due to the possibility of disease reactivation of and the increased incidence of neoplasms. However, it is the only existing form of therapy able to modify the natural progression of the disease in its terminal phase. Early diagnosis and rapid introduction of benzonidazole are able to restore the myocardial histology without scarring immunosuppression, especially steroids, predisposes to cancer and disease reactivation, thus requiring special attention regarding the use of these drugs. References 1 C. Chagas Nova entidade mórbida do homem Resumo geral de estudos etiológicos e clínicos Mem Inst Oswaldo Cruz 3 1911 219 2 J. Milei R.A. Guerri-Guttenberg D.R. Grana Prognostic impact of Chagas disease in the United States Am Heart J 157 2009 22 3 R.A. Guerri-Guttenberg A. Ciannameo C. Di Girolamo Chagas disease: an emerging public health problem in Italy? Infez Med 17 2009 5 4 M. Develoux F.X. Lescure S. Jaureguiberry Emergence of Chagas' disease in Europe: description of the first cases observed in Latin American immigrants in mainland France Med Trop (Mars) 70 2010 38 5 S. Sarkar S.E. Strutz D.M. Frank Chagas disease risk in Texas PLoS Negl Trop Dis 4 2010 e836 pii 6 A.I. Fiorelli N.A. Stolf C.A. Abreu Filho Prophylactic donor tricuspid annuloplasty in orthotopic bicaval heart transplantation Transplant Proc 39 2007 2527 7 D.R. Almeida A.C. Carvalho J.N. Branco Transplante cardíaco em cardiomiopatia chagásica: experiência com esquema modificado de imunossupressão Arq Bras Cardiol 65 suppl I 1995 I 8 J.H.A. Vila R. Macruz F.A. Sampaio Transplante cardíaco em doenca de Chagas: experiência preliminar Arq Bras Cardiol 45 suppl I 1985 I 9 A.I. Fiorelli N.A. Stolf R. Honorato Later evolution after cardiac transplantation in Chagas' disease Transplant Proc 37 2005 2793 10 V.B. Carvalho E.F.L. Sousa J.H.A. Vila Heart transplantation in Chagas' disease 10 years after the initial experience Circulation 94 1996 1815 11 M.A.S. Assef P.F.M.F. Valbuena M.T. Neves Jr Transplante cardíaco no Instituto Dante Pazzanese de Cardiologia: análise da sobrevida Rev Bras Cir Cardiovasc 16 2001 289 12 E.A. Bocchi A.I. Fiorelli The paradox of survival results after heart transplantation for cardiomyopathy caused by Trypanosoma cruzi Ann Thorac Surg 71 2001 1833 13 L.F.P. Moreira J. Galantier A. Benício Perspectivas da evolução clínica de pacientes com cardiomiopatia chagásica listados em prioridade para o transplante cardíaco Rev Bras Cir Cardiovasc 20 2005 261 14 F. Bacal C.P. Silva P.V. Pires Transplantation for Chagas' disease: an overview of immunosuppression and reactivation in the last two decades Clin Transplant 24 2010 E29 15 R.B. Bestetti T.A. Theodoropoulos M.A. Nakazone Usefulness of sirolimus-based immunosuppression in ameliorating pre-transplant renal dysfunction in patients with Chagas' heart disease J Heart Lung Transplant 29 2010 1312 "
    },
    {
        "doc_title": "Long-term pulmonary vascular reactivity after orthotopic heart transplantation by the biatrial versus the bicaval technique",
        "doc_scopus_id": "79951777869",
        "doc_doi": "10.1016/j.transproceed.2010.12.045",
        "doc_eid": "2-s2.0-79951777869",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Transplantation",
                "area_abbreviation": "MEDI",
                "area_code": "2747"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Introduction: Advantages of the bicaval versus the biatrial technique have been reported, emphasizing atrial electrical stability and less tricuspid regurgitation. Objective: To analyze the impact of the surgical technique on long-term pulmonary pressures, contractility, and graft valvular behavior after heart transplantation. Methods: Among 400 orthotopic heart transplantation recipients from 1985 to 2010, we selected 30 consecutive patients who had survived beyond 3 years. The biatrial versus bicaval surgical technique groups included 15 patients each. Their preoperative clinical characteristics were similar. None of the patients displayed a pulmonary vascular resistance or pulmonary artery pressure over 6U Wood or 60 mm Hg, respectively. We evaluated invasive hemodynamic parameters during routine endomyocardial biopsies. Two-dimensional echocardiographic parameters were obtained from routine examinations. Results: There were no significant differences regarding right atrial pressure, systolic pulmonary artery pressure, pulmonary capillary wedge pressure, pulmonary vascular resistance, cardiac index, systolic blood pressure, left ventricular ejection fraction, and mitral regurgitation (P > .05). Tricuspid regurgitation increased significantly over the 3 years of observation only among the biatrial group (P = .0212). In both groups, the right atrial pressure, pulmonary wedge capillary pressure, transpulmonary gradient, and pulmonary vascular resistance decreased significantly (P < .05) from the pre- to the postoperative examination. In both groups cardiac index and systemic blood pressure increased significantly after transplantation (P < .05). Comparative analysis of the groups only showed significant differences regarding right atrial pressure and degree of tricuspid regurgitation; the bicaval group showing the best performance. Conclusions: Both surgical techniques ensure adequate left ventricular function in the long term; however, the bicaval technique provided better trends in hemodynamic performance, as well as a lower incidence and severity of tricuspid valve dysfunction. © 2011 by Elsevier Inc. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271972 291210 291683 291912 31 Transplantation Proceedings TRANSPLANTATIONPROCEEDINGS 2011-02-16 2011-02-16 2013-06-18T09:57:31 S0041-1345(10)01960-3 S0041134510019603 10.1016/j.transproceed.2010.12.045 S300 S300.1 FULL-TEXT 2015-05-14T02:53:04.26492-04:00 0 0 20110101 20110228 2011 2011-02-16T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue table body affil articletitle auth authfirstini authfull authlast authsuff primabst pubtype ref alllist content subj ssids 0041-1345 00411345 false 43 43 1 1 Volume 43, Issue 1 49 229 232 229 232 201101 201102 January–February 2011 2011-01-01 2011-02-28 2011 9th Luso-Brazilian Congress of Transplantation Thoracic Transplantation article fla Copyright © 2011 Elsevier Inc. All rights reserved. LONGTERMPULMONARYVASCULARREACTIVITYAFTERORTHOTOPICHEARTTRANSPLANTATIONBYBIATRIALVERSUSBICAVALTECHNIQUE FIORELLI A Materials and Methods Statistical Analysis Results Discussion References ELGAMEL 1995 721 A DELEUZE 1995 731 P SIEVERS 1994 708 H ORTIZ 2007 2368 V SCHNOOR 2007 1322 M BOCCHI 1998 279 E MUGGE 1990 884 A DAVIES 2010 700 R AZIZ 1999 115 T JEEVANANDAM 2006 2089 V FIORELLI 2010 E168 A PAHL 2000 268 E FIORELLIX2011X229 FIORELLIX2011X229X232 FIORELLIX2011X229XA FIORELLIX2011X229X232XA item S0041-1345(10)01960-3 S0041134510019603 10.1016/j.transproceed.2010.12.045 271972 2013-06-18T07:51:39.322438-04:00 2011-01-01 2011-02-28 true 112548 MAIN 4 71673 849 656 IMAGE-WEB-PDF 1 TPS 22592 S0041-1345(10)01960-3 10.1016/j.transproceed.2010.12.045 Elsevier Inc. Table 1 Baseline Data of the Patients Transplanted Using Biatrial and Bicaval Techniques Variables Biatrial Group Bicaval Group P Gender M 13(86.7%) 9(60.0%) F 2(13.3%) 6(40.0%) Age (y) 43.8±6.4 39.9±13.7 .9834 Cardiomyopathy Idiopathic 7(46.7%) 8(53.3%) Ischemic 6(40.0%) 3(20.0%) Chagas' disease 2(13.3%) 4(26.7%) FC (NYHA) IV 13(86.7%) 8(53.3%) IV + catecholamine 2(13.3%) 5(33.4%) IV + IABP — 2(13.3%) RAP(mm Hg) 19.9±9.3 18.8±10.2 .6798 SPAP (mm Hg) 50.6±18.4 45.1±22.1 .5876 PWCP (mm Hg) 23.5±9.8 19.3±10.1 .3848 TGP (mm Hg) 10.8±4.9 10.1±4.1 .4456 PVR (U Wood) 3.5±2.1 3.4±2.1 .3258 CI (L/min·m2) 2.0±0.6 2.1±0.4 .9385 SBP (mm Hg) 99±9.9 95±9.7 .8259 LVEF-echo (%) 19.5±7.2 22.5±7.2 .8240 FC, functional class; NYHA, New York Heart Association; IABP, intraaortic balloon pump; RAP, right atrium pressure; SPAP, systolic pulmonary artery pressure; PWCP, pulmonary wedge capillary pressure; TPG, transpulmonary gradient; PVR, pulmonary vascular resistance; CI, cardiac index; SBP, systolic blood pressure; LVEF-echo, left ventricle ejection fraction by echocardiography. Table 2 Baseline Data of the Patients Transplanted Using Biatrial and Bicaval Techniques Biatrial Group Bicaval Group Variables Year 1 Year 2 Year 3 P Year 1 Year 2 Year 3 P RAP (mm Hg) 13.9±3.0 14.8±2.8 16.1±2.3 .0492 6.1±2.5 5.8±2.3 7.8±2.8 .0589 SPAP (mm Hg) 32.6±5.4 30.8±5.3 30.5±4.5 .4503 30.6±5.3 29.7±4.5 29.1±2.3 .4787 PWCP (mm Hg) 10.3±3.6 10.9±3.5 10.3±3.1 .5013 11.5±3.4 9.5±3.7 11.4±5.6 .3987 PVR (U Wood) 1.4±0.6 1.2±0.8 1.2±0.7 .0995 1.3±0.7 1.1±1.0 1.2±0.8 .0989 TGP (mm Hg) 8.9±2.9 8.1±2.1 8.2±2.0 .4586 8.2±2.1 8.1±2.4 8.0±1.9 .4139 CI (L/min·m2) 3.0±.5 3.1±.4 2.9±.7 .3465 3.1±0.7 3.3±0.7 3.0±0.9 .5987 SBP (mm Hg) 165±18.7 167±27.0 155±20.4 .6167 148±16.6 144±21.6 155±18.7 .5678 LVEF-echo (%) 68±6 68±7 66±11 .1492 69±8 70±9 68±7 .1948 TR 1.3±0.4 1.8±0.7 2.1±0.5 .0212 0.3±0.2 0.4±0.5 0.4±0.5 .4867 MR 0.1±0.3 0.2±0.4 0.2±0.4 .8789 0.1±0.3 0.1±0.3 0.2±0.4 .8978 RAP, right atrium pressure; SPAP, systolic pulmonary artery pressure; PWCP, pulmonary wedge capillary; TPG, transpulmonary gradient; PVR, pulmonary vascular resistance; CI, cardiac index; SBP, systolic blood pressure; LVEF-echo, left ventricle ejection fraction by echocardiography; TR, tricuspid regurgitation; MR, mitral regurgitation. Table 3 Comparative Analysis Between Preoperative and 1 Year after Heart Transplantation in Both Groups Biatrial Group Bicaval Group Variables Preoperative Year 1 P Preoperative Year 1 P RAP (mm Hg) 19.9±9.3 13.9±3.0 .0152 18.8±10.2 6.1±2.5 .0045 SPAP (mm Hg) 50.6±18.4 32.6±5.4 .0013 45.1±22.1 30.6±5.3 .0145 PWCP (mm Hg) 23.5±9.8 10.3±3.6 .0056 19.3±10.1 11.5±3.4 .0098 PVR (U Wood) 3.5±2.1 1.4±0.6 .0067 3.4±2.1 1.3±0.7 .0045 TGP (mm Hg) 10.8±4.9 8.9±2.9 .0183 10.1±4.1 8.2±2.1 .0267 CI (L/min·m2) 2.0±0.6 3.0±.5 .0169 2.1±0.4 3.1±0.7 .0278 SBP (mm Hg) 99±9.9 165±18.7 .0008 95±9.7 148±16.6 .0001 RAP, right atrium pressure; SPAP, systolic pulmonary artery pressure; PWCP, pulmonary wedge capillary; TPG, transpulmonary gradient; PVR, pulmonary vascular resistance; CI, cardiac index; SBP, systolic blood pressure. Thoracic transplantation Long-Term Pulmonary Vascular Reactivity After Orthotopic Heart Transplantation by the Biatrial Versus the Bicaval Technique A.I. Fiorelli ⁎ R.H.B. Santos J.L. Oliveira Jr M.A.F. Da Silva V.P. dos Santos Jr F.M.P. Rêgo G.E. Souza F. Bacal E.A. Bocchi N.A.G. Stolf Heart Institute of Sao Paulo University Medical School, Sao Paulo, Brazil ⁎ Address reprint requests to Alfredo I. Fiorelli, Rua Morgado de Mateus 126/81, Sao Paulo/SP, Brazil CEP: 04015-050 Introduction Advantages of the bicaval versus the biatrial technique have been reported, emphasizing atrial electrical stability and less tricuspid regurgitation. Objective To analyze the impact of the surgical technique on long-term pulmonary pressures, contractility, and graft valvular behavior after heart transplantation. Methods Among 400 orthotopic heart transplantation recipients from 1985 to 2010, we selected 30 consecutive patients who had survived beyond 3 years. The biatrial versus bicaval surgical technique groups included 15 patients each. Their preoperative clinical characteristics were similar. None of the patients displayed a pulmonary vascular resistance or pulmonary artery pressure over 6U Wood or 60 mm Hg, respectively. We evaluated invasive hemodynamic parameters during routine endomyocardial biopsies. Two-dimensional echocardiographic parameters were obtained from routine examinations. Results There were no significant differences regarding right atrial pressure, systolic pulmonary artery pressure, pulmonary capillary wedge pressure, pulmonary vascular resistance, cardiac index, systolic blood pressure, left ventricular ejection fraction, and mitral regurgitation (P > .05). Tricuspid regurgitation increased significantly over the 3 years of observation only among the biatrial group (P = .0212). In both groups, the right atrial pressure, pulmonary wedge capillary pressure, transpulmonary gradient, and pulmonary vascular resistance decreased significantly (P < .05) from the pre- to the postoperative examination. In both groups cardiac index and systemic blood pressure increased significantly after transplantation (P < .05). Comparative analysis of the groups only showed significant differences regarding right atrial pressure and degree of tricuspid regurgitation; the bicaval group showing the best performance. Conclusions Both surgical techniques ensure adequate left ventricular function in the long term; however, the bicaval technique provided better trends in hemodynamic performance, as well as a lower incidence and severity of tricuspid valve dysfunction. Heart transplantation is the best treatment to rescue patients with heart failure, restoring their hemodynamic status. Since 1967 when the pioneer operation was performed, various advances have occurred in perioperative management of donors and recipients and in long-term prevention and treatment of rejection processed. In recent years, the bicaval technique has gained adherents as opposed to the standard, biatrial technique for cardiac implantation. 1 The advantage of the alternative technique is maintenance of the normal shape of the atrium to preserve its contractility, sinus node function, and tricuspid valvular competence. 2,3 The hemodynamic superiority of the bicaval technique for the right chambers is well know to be due to less anatomic distortion of the right atrium and better electrical stability by reducing the risk of sinoatrial node injury. Increased pulmonary artery pressure in the recipient is one of the most important predictors of right ventricular dysfunction, with increased peri- and postoperative complication and mortality rates. However, patients with high pulmonary vascular resistance who survive to heart transplantation show prompt decreases in the first postoperative months with hemodynamic stability during clinical follow-up. 3–5 This study was designed to compare the long-term hemodynamic performances in pulmonary vascular reactivity and cardiac performance between orthotopic heart transplantations using the biatrial versus the bicaval technique in two different eras. Materials and Methods From 1985 to January 2010, we performed 400 orthotopic heart transplantations. In 1999, we began to perform the bicaval technique, using a single left atrium anastomosis and pulmonary vein maintenance on the recipient atrial segment. This method was associated with prophylactic donor tricuspid annuloplasty beginning in 2000. We selected 30 patients surviving beyond 3 years for this study as a biatrial group (n = 15) and a bicaval group with prophylactic donor tricuspid annuloplasty (n = 15). After the use of vasodilators, none of the patients displayed a pulmonary vascular resistance or a pulmonary artery pressure over 6U Wood or 60 mm Hg, respectively. Baseline data and preoperative clinical characteristics of both groups were similar (Table 1). Rejection monitoring performed by routine endomyocardial biopsies were graded according to the International Society for Heart & Lung Tansplantation system. Gallium-67 myocardial scintigraphy was also used as a sorting method to ascertain patients at greater rejection risk, allowing us of to reduce the number of biopsies. 6 The endomyocardial biopsies were performed through a venous percutaneous approach under fluoroscopic or two-dimensional echocardiography guidance; the myocardial specimens were withdrawn from the right ventricular septum. Two-dimensional echocardiography was performed routinely during clinical follow-up; also, parameters were obtained by transthoracic Doppler echocardiography and flux evaluation by Doppler. Ventricular function was measured by the ejection fraction of the left ventricle (Simpson method). The tricuspid valve was observed in the parasternal and four-chamber apical view. The degree of tricuspid insufficiency was quantified according to the regurgitation jet area: 0 = absent; 1 = mild; 2 = moderate; and 3 = severe. 7 Right heart catheterization was performed during the routine biopsies via a Swan-Ganz catheter to measure the right intracardiac pressures and the cardiac output. All hemodynamic and echocardiographic parameters were selected in patients who were not experiencing rejection processes. Statistical Analysis For statistical analysis the, continuous variables are reported as averages ± standard deviations using the unpaired Student t test for comparisons. To compare quantitative variables over 3 years, we utilized an analysis of variance. The categorical variables expressed as relative frequencies were examined with chi-square tests. P < .05 was considered significant. Results Table 2 shows the hemodynamic and echocardiographic data for both study groups after heart transplantation. There were no significant differences between the groups regarding right atrial pressure, systolic pulmonary artery pressure, pulmonary capillary wedge pressure, pulmonary vascular resistance, cardiac index, systolic blood pressure, left ventricle ejection fraction, and mitral regurgitation over the entire study period. Over 3 years of observation, only the biatrial group showed significantly increased tricuspid regurgitation after transplantation. In the fifth year of the follow-up, these patients needed tricuspid valve replacement for severe insufficiency. The right atrial pressure showed borderline significance (P = .0492) in the biatrial group accompaning increased tricuspid regurgitation. Table 3 shows a comparative analysis before and in the first year after heart transplantation. In both groups, the right atrial pressure, pulmonary wedge capillary pressure, transpulmonary gradient, and pulmonary vascular resistance decreased significantly from the preoperative examination (P < .05), remaining stable until the third year (P > .05). In contrast, the cardiac index and systemic blood pressure increased significantly to the first year examination (P < .05), remaining stable until the third year (P > .05). Comparative analysis between the groups showed the only significant difference to be right atrial pressure (P = .0256) and degree of tricuspid regurgitation (P = .0238); the bicaval group showed the best performance. The surgical technique did not interfere significantly on decrease the pulmonary pressures or pulmonary vascular resistance (P > .05). Discussion The biatrial approach for orthotopic heart transplantation is attractive due to its simplicity; therefore, it has become widely used as the standard technique. In the 1990s, the bicaval and total heart transplantation techniques were introduced, gaining popularity due to their potential benefits. 3,5 The bicaval technique is more demanding technically; there are slightly increased extracorporeal circulation and ischemic times. In recent years, the United Network for Organ Sharing database has shown greater preference for the use of this technique, namely 1083 bicaval versus 806 biatrial procedures in 2005. Retrospectively analyzing 11,931 patients undergoing first-time adult orthotopic heart transplantation in this database revealed a higher mortality rate of 24% among the biatrial group versus 18% for the bicaval group within 5 years who also required more pacemakers (5.3% vs 2.0%) and longer hospital stays. 8 In a comparative cohort study of 200 patients, Aziz et al showed a significant and persistent survival advantage with the bicaval technique, given, resulting in a 5-year survival of 81% versus 62%. 9 The ischemic time is generally slightly higher for the bicaval technique, because there are more anastomoses but usually this does not increase the surgical risk. The disadvantages are compensated for by several significant advantages, including reduced early atrial pressure, reduction in relative mortality risk, reduction in tricuspid valve regurgitation, and higher odds of sinus rhythm. In both techniques, the pulmonary pressure and the pulmonary vascular resistance decreased equally, however the contra-indication limits of the pulmonary hypertension to orthotopic heart transplantation must be always respected by risk of the graft failure. Preoperative evaluation of the pulmonary vascular reactivity with vasodilators is important to distinguish reversible from fixed hypertension. It is well known that there is significant decrease in the pulmonary artery pressure, pulmonary capillary wedge pressure, and pulmonary vascular resistance in the first months after heart transplantation. We observed similar behavior for both groups, demonstrating that normalization of pulmonary parameters depends much more on the effectiveness of ventricular contractility to decompress left chambers than the surgical technique itself. The low incidence of mitral insufficiency which occurs after heart transplantation in both techniques, biatrial and bicaval, is explained by the maintenance of the left ventricle conic shape and a greater resistance to distortion of the mitral annulus. 3–5 In contrast, right ventricular anatomy is different. The muscle mass, which favors tricuspid annulus distortion, is more intense in heart transplantation by the biatrial than the bicaval technique. Tricuspid annular distortion and more elevated right atrial pressure are more likely in the biatrial technique, associated with an increased right atrial chamber, elevated pulmonary artery pressure, and exacerbated by undesired removal of tricuspid tissue during biopsies. The tricuspid annulus distortion and valvular insufficiency, however, are much less frequent and more benign with the bicaval technique, although some authors advocate prophylactic tricuspid annuloplasty in donor hearts. 10,11 Although a significant difference in right atrial pressure between the groups was seen, its clinical significance is questionable. The same cannot be said for tricuspid insufficiency, which also showed a significant difference, This disturbance must be viewed with more emphasis, because it may have ample clinical repercussion depending upon the degree of valvular regurgitation. However, these apparent advantages have not been observed by all investigators; it is still controversial. Studying the hemodynamic performance of pediatric heart transplant recipients during exercise Pahl et al did not find a significant difference in endurance and peak heart rates when comparing patients with a bicaval or biatrial technique. 12 Nevertheless, other researchers showed that the bicaval technique improved cardiovascular dynamics as well as exercise capacity and duration, reducing tricuspid regurgitation during exercise. No doubt, both the bicaval and total orthotopic technique represent breakthroughs that deserve further investigation to know their real benefits. The limitations of this investigation are that the data were collected prospectively but analyzed retrospectively in two different eras. In addition this single center was small and nonrandomized in design. In conclusion, both surgical techniques ensure adequate left ventricular function in the long term; however, the bicaval technique provided better trends in hemodynamic performances as well as a lower incidence and severity of tricuspid valve dysfunction. References 1 A. El-Gamel N.A. Yonan S. Grant Orthotopic heart transplantation: a comparison between the standard and the bicaval Wythenshawe technique J Thorac Cardiovasc Surg 109 1995 721 2 P.H. Deleuze C. Benvenuti J.P. Mazzuotelli Orthotopic cardiac transplantation with direct caval anastomosis: is it the optimal procedure? J Thorac Cardiovasc Surg 109 1995 731 3 H.-H. Sievers R. Leyh A. Jahank Bicaval versus atrial anastomoses in cardiac transplantation J Thorac Cardiovasc Surg 108 1994 708 4 V. Ortiz L. Martínez-Dolz F. Ten Evolution of right cardiac pressures during the first year after heart transplantation Transplant Proc 39 2007 2368 5 M. Schnoor T. Schäfer D. Lühmann Bicaval versus standard technique in orthotopic heart transplantation: a systematic review and meta-analysis J Thorac Cardiovasc Surg 134 2007 1322 6 E.A. Bocchi R. Kalil F. Bacal Magnetic resonance imaging in chronic Chagas' disease: correlation with endomyocardial biopsy findings and gallium-67 cardiac uptake Echocardiography 15 1998 279 7 A. Mugge W. Daniel G. Herrmann Quantification of tricuspid regurgitation by Doppler color flow mapping after cardiac transplantation Am J Cardiol 66 1990 884 8 R.R. Davies M.J. Russo J.A. Morgan Standard versus bicaval techniques for orthotopic heart transplantation: an analysis of the United Network for Organ Sharing database J Thorac Cardiovasc Surg 140 2010 700 9 T. Aziz M. Burgess R. Khafagy Bicaval and standard techniques in orthotopic heart transplantation: medium-term experience in cardiac performance and survival J Thorac Cardiovasc Surg 118 1999 115 10 V. Jeevanandam H. Russell P. Mather Donor tricuspid annuloplasty during orthotopic heart transplantation: long-term results of a prospective controlled study Ann Thorac Surg 82 2006 2089 11 A.I. Fiorelli J.L. Oliveira R.H. Santos Can tricuspid annuloplasty of the donor heart reduce valve insufficiency following cardiac transplantation with bicaval anastomosis? Heart Surg Forum 13 2010 E168 12 E. Pahl S. Sundararaghavan J.F. Strasburger Impaired exercise parameters in pediatric heart transplant recipients: comparison of biatrial and bicaval techniques Pediatr Transplant 4 2000 268 "
    },
    {
        "doc_title": "On the exploitation of cloud computing in bioinformatics",
        "doc_scopus_id": "79951646075",
        "doc_doi": "10.1109/ITAB.2010.5687653",
        "doc_eid": "2-s2.0-79951646075",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Abstraction layer",
            "Analysis tools",
            "Integrated access",
            "Life-sciences",
            "Virtualizations"
        ],
        "doc_abstract": "Life sciences make heavily use of the web for proving data and analysis tools. There are more than one thousand molecular biology databases characterized by their complexity and higher level of interdependence. Cloud computing has been successfully applied in many fields where managing the data is the major hurdle. It offers an abstraction layer that enables an integrated access to processing, storing and virtualization. In this paper we review the current organization of molecular biology databases and the main technologies for cloud computing. Finally we discuss the main opportunities and challenges of applying bioinformatics to cloud computing. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A tagging system for bioinformatics resources",
        "doc_scopus_id": "79951608006",
        "doc_doi": "10.1109/ITAB.2010.5687781",
        "doc_eid": "2-s2.0-79951608006",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Annotation systems",
            "Automatic tagging",
            "Bioinformatics resources",
            "Biological data",
            "Direct access",
            "External sources",
            "Extracting information",
            "Hard task",
            "Life sciences research",
            "Scientific community",
            "Tagging systems",
            "Technical solutions",
            "Textual data",
            "Textual documents",
            "Web interface",
            "Web-based applications"
        ],
        "doc_abstract": "The evolution of information technologies in the life sciences research field has leveraged fast and widespread availability of biological data. To deal with this huge amount of data, multiple distinct information systems were developed, trying to process, organize, and make the knowledge from this data available for the scientific community. Moreover, the number of articles published online has been increasing dramatically. However, extracting information from all these resources is a very hard task. As such, new technical solutions are mandatory to simplify the human burden of keeping abreast of these achievements. This paper presents a biomedical annotation system that enriches textual data through the automatic tagging of specific concepts. The solution was developed following a software-as-aservice strategy and, relying on augmented browsing, can highlight terms in a web interface allowing direct access to additional information from external sources. With this system, users will be able to analyze, in more detail, any kind of textual document provided by common web-based applications. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gene-disease prioritization through biomedical networks",
        "doc_scopus_id": "79951594865",
        "doc_doi": "10.1109/ITAB.2010.5687629",
        "doc_eid": "2-s2.0-79951594865",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Gene Expression Data",
            "Graph representation",
            "Prioritization",
            "Research challenges"
        ],
        "doc_abstract": "Identifying associations between genes and diseases has been an important research challenge in biomedicine, since this will help to create new diagnostics and treatments. This paper describes a computational method that ranks a list of genes according to their level of association to a given disease. In a first step, it constructs a graph representation of biomedical terms such as genes, pathways, homologies, ontologies, gene expression data and literature. Then the implemented algorithm runs over this network taking into consideration relations between the biomedical terms. The results obtained confirm the advantages of the proposed method when used to prioritize genes from a gene expression study. The solution presented here represents major progress when compared with previous methods, hopefully allowing faster and easier establishment of gene-disease associations. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An extensible platform for variome data integration",
        "doc_scopus_id": "79951594849",
        "doc_doi": "10.1109/ITAB.2010.5687784",
        "doc_eid": "2-s2.0-79951594849",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Data integration",
            "Data integration system",
            "Data type",
            "Distributed resources",
            "Genetic mutations",
            "Genetic sequence",
            "Genetic variation",
            "Genomics",
            "Hair colors",
            "Human traits",
            "Life sciences research",
            "Personalized medicines",
            "Research communities",
            "Transparent access",
            "Underlying cause",
            "Web portal"
        ],
        "doc_abstract": "Genetic mutations are the underlying cause of common human traits like hair color, rare conditions like Mendelian diseases, or response to drugs. Understanding the set of mutations in the genetic sequence, the variome, is extremely valuable to achieve the ultimate goals of personal genomics and personalized medicine. This critical life sciences research field has grown greatly in recent years, mostly due to the appearance of projects such as the Human Variome Project or the European GEN2PHEN Project. Nonetheless, variome data integration systems and included variants are far from being standardized and widely used in the research community workflow. The majority of human variome resources remam disconnected and a single view over all these data is clearly missing. In this paper, an extensible engine for genetic variation data integration is presented. Extensions can be easily added to the platform as new features, related data types or connections to distributed resources. A gene-centric web portal, deployed upon the proposed platform, which offers transparent access to multiple locus-specific databases, to known variants and to complementary gene-related information, is also detailed. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Recognition of gene/protein names using conditional random fields",
        "doc_scopus_id": "78651435591",
        "doc_doi": null,
        "doc_eid": "2-s2.0-78651435591",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Gene/Protein names",
            "Machine-learning",
            "Named entity recognition",
            "NAtural language processing",
            "Text mining"
        ],
        "doc_abstract": "With the overwhelming amount of publicly available data in the biomedical field, traditional tasks performed by expert database annotators rapidly became hard and very expensive. This situation led to the development of computerized systems to extract information in a structured manner. The first step of such systems requires the identification of named entities (e.g. gene/protein names), a task called Named Entity Recognition (NER). Much of the current research to tackle this problem is based on Machine Learning (ML) techniques, which demand careful and sensitive definition of the several used methods. This article presents a NER system using Conditional Random Fields (CRFs) as the machine learning technique, combining the best techniques recently described in the literature. The proposed system uses biomedical knowledge and a large set of orthographic and morphological features. An F-measure of 0,7936 was obtained on the BioCreative II Gene Mention corpus, achieving a significantly better performance than similar baseline systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Gathering and managing diagnostic tests",
        "doc_scopus_id": "77957827925",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77957827925",
        "doc_date": "2010-10-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Diagnostic tests",
            "Ease of use",
            "Health informations",
            "Medical care",
            "Personal health",
            "Portable device",
            "Privacy",
            "Security"
        ],
        "doc_abstract": "Personal health information is constituted in its greatest part by complementary diagnostic tests which are an important medical aid. This information is generated dispersedly because the patient seeks medical care in many different places over his lifetime. Access to a comprehensive set of a patient's health information is a challenge. It revolves around the patient so any managing scheme must be patient-centric. We took a pragmatic approach to this problem and developed a software standalone platform for secure personal health information storage, namely complementary diagnostic tests, on a portable device for mobility. Simplicity and ease of use were main objectives. A special attention was given to the security aspects associated with storing this kind of information.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Email-P2P gateway to distributed medical imaging repositories",
        "doc_scopus_id": "77956355091",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77956355091",
        "doc_date": "2010-09-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Diagnostic tools",
            "DICOM",
            "Hardware and software",
            "Health care professionals",
            "Healthcare institutions",
            "Image communication",
            "Isolated islands",
            "Medical data",
            "Medical images",
            "P2P system",
            "PACS integration",
            "Peer interactions",
            "Peer to peer",
            "Private networks",
            "Research organization",
            "Storage devices",
            "Virtual barrier",
            "Virtual private networks"
        ],
        "doc_abstract": "For the healthcare professionals the importance of the medical imaging as a diagnostic tool is undeniable. For this reason, industry and research organizations increased significantly their interest in the medical imaging area, trying to deliver solutions for creating, storing, exchanging and displaying medical images. The raise of hardware and software solutions drove the community of vendors to gradually decrease the price of his solutions. As consequence, there was a rise of small imaging centres competing with bigger healthcare institutions. The market offers drives the patients to move across a wide range of healthcare institutions to undergo all the necessary exams. Producing a great amount of medical data dispersed over several institutions. This scenario of isolated islands of images repositories unable of interacting with each other is, in our opinion, propitious to a peer-to-peer (P2P) archive solution. Until now, medical exams (images and studies) have been exchanged through analogue films, media storage devices (CD, DVD, etc), virtual private networks or manual email procedures. This paper describes the Dicoogle P2P system, a distributed PAC system where its users may easily store, search and exchange DICOM files. However, potential peers of the Dicoogle system are usually inside private networks, behind NATs and firewalls, disabling the inter-institutional peer interaction. Therefore, we propose an Email-P2P gateway to Dicoogle that offers a way to exchange DICOM files through these virtual barriers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modelling a portable Personal Health Record",
        "doc_scopus_id": "77956310891",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77956310891",
        "doc_date": "2010-09-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Direct control",
            "Electronic health record",
            "Health care professionals",
            "Health services",
            "Patient empowerment",
            "Patient health",
            "Personal health",
            "Personal health record",
            "Portable device",
            "Security and privacy issues",
            "Storage hardware"
        ],
        "doc_abstract": "Active and responsible involvement of patients in their own health is accepted as an important contribution towards an increased quality of health services in general. Management of Personal Health Information by the patient can play an important role in the improvement in quality of the information available to health care professionals and as a means of patient involvement. Electronic Health Records are a means of storing this kind of information but their management usually falls under the responsibility of an institution and not on the patient himself. A Personal Health Record under the direct control and management of the patient is the natural solution for the problem. When implemented in a storage hardware portable device, a PHR, allows for total mobility. Personal Health Information is very sensitive in nature so any implementation has to address security and privacy issues. With this in mind we propose a structure for a secure Patient Health Record stored in a USB pen device under the patient's direct management and responsibility.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards an EHR architecture for mobile citizens",
        "doc_scopus_id": "77956306890",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77956306890",
        "doc_date": "2010-09-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "EHR",
            "EHR systems",
            "Electronic health record",
            "Health care professionals",
            "Health care providers",
            "Integrated access",
            "Mobile citizens",
            "Patient health",
            "Patient mobility",
            "Personal health",
            "Personal health record",
            "PHR",
            "Security",
            "USB drives"
        ],
        "doc_abstract": "Electronic Health Records are typically created and stored in different places, by different healthcare providers, using different formats and technology. This poses an obstacle to patient mobility and contributes to scatter personal health related information. Patients constantly move between healthcare providers, searching for a better service, lower prices or specialists. It is important that healthcare professionals, regardless of technology and location, have access to the complete patient health record. The access to this personal health record can be granted through a network (web-based, for example) or can be carried by the patient, in a usb drive, for example. Either approach has to enforce the patient consent to access his information, cope with different types of EHR systems and formats. This paper is an ongoing research, part of a PhD on Electronic Health Records for Mobile Citizens.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GeneBrowser 2: An application to explore and identify common biological traits in a set of genes",
        "doc_scopus_id": "77954675055",
        "doc_doi": "10.1186/1471-2105-11-389",
        "doc_eid": "2-s2.0-77954675055",
        "doc_date": "2010-07-21",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Biological characteristic",
            "Biological information",
            "Biological traits",
            "Computer assisted",
            "Gene expression studies",
            "Laboratory techniques",
            "Result analysis",
            "Time-consuming tasks"
        ],
        "doc_abstract": "Background: The development of high-throughput laboratory techniques created a demand for computer-assisted result analysis tools. Many of these techniques return lists of genes whose interpretation requires finding relevant biological roles for the problem at hand. The required information is typically available in public databases, and usually, this information must be manually retrieved to complement the analysis. This process is a very time-consuming task that should be automated as much as possible.Results: GeneBrowser is a web-based tool that, for a given list of genes, combines data from several public databases with visualisation and analysis methods to help identify the most relevant and common biological characteristics. The functionalities provided include the following: a central point with the most relevant biological information for each inserted gene; a list of the most related papers in PubMed and gene expression studies in ArrayExpress; and an extended approach to functional analysis applied to Gene Ontology, homologies, gene chromosomal localisation and pathways.Conclusions: GeneBrowser provides a unique entry point to several visualisation and analysis methods, providing fast and easy analysis of a set of genes. GeneBrowser fills the gap between Web portals that analyse one gene at a time and functional analysis tools that are limited in scope and usually desktop-based. © 2010 Arrais et al; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A proxy of DICOM services",
        "doc_scopus_id": "77953435038",
        "doc_doi": "10.1117/12.843572",
        "doc_eid": "2-s2.0-77953435038",
        "doc_date": "2010-06-17",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Diagnostic tools",
            "DICOM services",
            "Different domains",
            "Digital imaging",
            "Digital medical images",
            "Healthcare institutions",
            "Image communication",
            "interinstitutional",
            "Local networks",
            "Medical data",
            "Medical decisions",
            "Medical diagnosis",
            "Network address translations",
            "PACS integration",
            "Picture archiving",
            "Service provider",
            "Transport layer security",
            "Virtual bridges"
        ],
        "doc_abstract": "Diagnostic tools supported by digital medical images have increasingly become an essential aid to medical decisions. However, despite its growing importance, Picture Archiving and Communication Systems (PACS) are typically oriented to support a single healthcare institution, and the sharing of medical data across institutions is still a difficult process. This paper describes a proposal to publish and control Digital Imaging Communications in Medicine (DICOM) services in a wide domain composed of several healthcare institutions. The system creates virtual bridges between intranets enabling the exchange, search and store of the medical data within the wide domain. The service provider publishes the DICOM services following a token-based strategy. The token advertisements are public and known by all system users. However, access to the DICOM service is controlled through a role association between an access key and the service. Furthermore, in medical diagnoses, time is a crucial factor. Therefore, our system is a turnkey solution, capable of exchanging medical data across firewalls and Network Address Translation (NAT), avoiding bureaucratic issues with local network security. Security is also an important concern - in any transmission across different domains, data is encrypted by Transport Layer Security (TLS). © 2010 Copyright SPIE - The International Society for Optical Engineering.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Can tricuspid annuloplasty of the donor heart reduce valve insufficiency following cardiac transplantation with bicaval anastomosis?",
        "doc_scopus_id": "77953594608",
        "doc_doi": "10.1532/HSF98.20091146",
        "doc_eid": "2-s2.0-77953594608",
        "doc_date": "2010-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Cardiology and Cardiovascular Medicine",
                "area_abbreviation": "MEDI",
                "area_code": "2705"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: The aim of this study was to evaluate the degree of tricuspid valve insufficiency after orthotopic cardiac transplantation with bicaval anastomosis and prophylactic donor heart annuloplasty. Methods: At present, our cardiac transplantation experience includes 478 cases. After January 2002, we included 30 consecutive patients in this study who had undergone orthotopic cardiac transplantation and survived >6 months. The patients were divided into 2 groups: group I, 15 patients who underwent transplantation with prophylactic tricuspid annuloplasty on the donor heart with the De Vega technique; and group II, 15 patients who underwent transplantation without this procedure. Their preoperative clinical characteristics were the same. During the late postoperative follow-up, the degree of tricuspid insufficiency was evaluated by transthoracic Doppler echocardiography and assessed according to the Simpson scale: 0, absent; 1, mild; 2, moderate; and 3, severe. Hemodynamic parameters were evaluated invasively by means of a Swan-Ganz catheter during routine endomyocardial biopsies. Results: The mean follow-up time was 26.9 ± 5.4 months (range, 12-36 months). In group I, 1 patient (6.6%) died from infection in the 18th month after the operation; the death was not related to the annuloplasty. In group II, 1 death (6.6%) occurred after 10 months because of rejection (P > .05). After the 24-month follow-up, the mean degree of tricuspid insufficiency was 0.4 ± 0.5 in group I and 1.7 ± 0.9 in group II (P < .05). Similarly, the 2 groups were significantly different with respect to the right atrium pressure, which was higher in group II. Conclusions: Prophylactic tricuspid annuloplasty on the donor heart was able to reduce significantly the degree of valvular insufficiency, even in cardiac transplantation with bicaval anastomosis; however, it did not modify significantly the hemodynamic performance of the allograft during the investigation period. It is very important to extend the observation period and casuistics to verify other benefits that this technique may offer. © 2010 Forum Multimedia Publishing, LLC.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Concept-based query expansion for retrieving gene related publications from MEDLINE",
        "doc_scopus_id": "77951569837",
        "doc_doi": "10.1186/1471-2105-11-212",
        "doc_eid": "2-s2.0-77951569837",
        "doc_date": "2010-04-28",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Structural Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1315"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Alternative solutions",
            "Biological resources",
            "Exponential increase",
            "High-throughput method",
            "Relevant documents",
            "Retrieved documents",
            "Scientific literature",
            "Scientific publications"
        ],
        "doc_abstract": "Background: Advances in biotechnology and in high-throughput methods for gene analysis have contributed to an exponential increase in the number of scientific publications in these fields of study. While much of the data and results described in these articles are entered and annotated in the various existing biomedical databases, the scientific literature is still the major source of information. There is, therefore, a growing need for text mining and information retrieval tools to help researchers find the relevant articles for their study. To tackle this, several tools have been proposed to provide alternative solutions for specific user requests.Results: This paper presents QuExT, a new PubMed-based document retrieval and prioritization tool that, from a given list of genes, searches for the most relevant results from the literature. QuExT follows a concept-oriented query expansion methodology to find documents containing concepts related to the genes in the user input, such as protein and pathway names. The retrieved documents are ranked according to user-definable weights assigned to each concept class. By changing these weights, users can modify the ranking of the results in order to focus on documents dealing with a specific concept. The method's performance was evaluated using data from the 2004 TREC genomics track, producing a mean average precision of 0.425, with an average of 4.8 and 31.3 relevant documents within the top 10 and 100 retrieved abstracts, respectively.Conclusions: QuExT implements a concept-based query expansion scheme that leverages gene-related information available on a variety of biological resources. The main advantage of the system is to give the user control over the ranking of the results by means of a simple weighting scheme. Using this approach, researchers can effortlessly explore the literature regarding a group of genes and focus on the different aspects relating to these genes. © 2010 Matos et al; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Managing QoS in a NGN using a PBM approach",
        "doc_scopus_id": "72349097792",
        "doc_doi": "10.1109/ICSNC.2009.91",
        "doc_eid": "2-s2.0-72349097792",
        "doc_date": "2009-12-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Civil and Structural Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2205"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Common information model",
            "IP multimedia subsystems",
            "Learning curves",
            "Managed elements",
            "Next generation network",
            "NGN networks",
            "Policy based management",
            "Policy servers",
            "Policy specification languages",
            "QOS management",
            "Visual Interface",
            "Web-based enterprise"
        ],
        "doc_abstract": "Next Generation Network (NGN) management represents an enormous challenge due to the large number of managed elements, the variety of roles the managed entities play in the network and the difficulty of orchestrating management actions. Several NGN standardization bodies point toward Policy Based Management as the best approach for NGN network management. This paper describes a management solution for a NGN IP Multimedia Subsystem QoS management scenario based on Web-Based Enterprise Management (WBEM) technology. The proposal is for a WBEM-based policy server, a graphical policy editor application and instrumentation logic for the NGN QoS Management. The graphical policy editor reduces the learning curve imposed by the policy specification language, allowing the specification of policies through a rich and user-friendly visual interface, hiding the Common Information Model syntax complexity but keeping its potential. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Arabella: A directed web crawler",
        "doc_scopus_id": "77955347798",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77955347798",
        "doc_date": "2009-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Complex Processes",
            "Directed crawling",
            "Exponential increase",
            "Heterogeneous information",
            "Multi-thread",
            "Multithreaded",
            "Primary sources",
            "Processing capability",
            "Quality information",
            "Scalable web crawlers",
            "Specific information",
            "Structured document",
            "Web crawlers",
            "Web Crawling"
        ],
        "doc_abstract": "The Internet is becoming the primary source of knowledge. However, its disorganized evolution brought about an exponential increase in the amount of distributed, heterogeneous information. Web crawling engines were the first answer to ease the task of finding the desired information. Nevertheless, when one is searching for quality information related to a certain scientific domain, typical search engines like Google are not enough. This is the problem that directed crawlers try to solve. Arabella is a directed web crawler that navigates through a predefined set of domains searching for specific information. It includes text-processing capabilities that increase the system's flexibility and the number of documents that can be crawled: any structured document or REST web service can be processed. These complex processes do not harm overall system performance due to the multithreaded engine that was implemented, resulting in an efficient and scalable web crawler.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Link integrator: A link-based data integration architecture",
        "doc_scopus_id": "77955344708",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77955344708",
        "doc_date": "2009-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Access points",
            "Bioinformatics data",
            "Data association",
            "Data format",
            "Data integration",
            "Data production",
            "Information integration",
            "Information quality",
            "Integration mechanisms",
            "Public repositories",
            "Service integration",
            "WEB application",
            "Web Crawling"
        ],
        "doc_abstract": "The evolution of the World Wide Web has created a great opportunity for data production and for the construction of public repositories that can be accessed all over the world. However, as our ability to generate new data grows, there is a dramatic increase in the need for its efficient integration and access to all the dispersed data. In specific fields such as biology and biomedicine, data integration challenges are even more complex. The amount of raw data, the possible data associations, the diversity of concepts and data formats, and the demand for information quality assurance are just a few issues that hinder the development of a general proposal and solid solutions. In this article we describe a lightweight information integration architecture that is capable of unifying, in a single access point, several heterogeneous bioinformatics data sources. The model is based on web crawling that automatically collects keywords related with biological concepts that are previously defined in a navigation protocol. This crawling phase allows the construction of a link-based integration mechanism that conducts users to the right source of information, keeping the original interfaces of available information and maintaining the credits of original data providers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DynamicFlow: A client-side workflow management system",
        "doc_scopus_id": "77952575920",
        "doc_doi": "10.1007/978-3-642-02481-8_167",
        "doc_eid": "2-s2.0-77952575920",
        "doc_date": "2009-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biological data",
            "Client-side workflow",
            "Computational biology",
            "Computational solutions",
            "Data integration",
            "Information integration",
            "Service orchestration",
            "Web components",
            "Web semantics"
        ],
        "doc_abstract": "The constant increase on the amount and heterogeneity of biological data sources impose a permanent pressure toward the development of computational solutions that can integrate and process all the data, and help give answers to arising biological questions. Besides the work already developed on information integration in computational biology, the novel Web2.0 and Web Semantic trends leverage the design of next-generation applications sustained by the web-as-a-platform principle. Grounded on this idea, this paper presents a service orchestration framework that, using existing web components, allows the user to create and execute their own research workflow relying simply on a normal web browser. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of policy-based admission control mechanisms in NGN",
        "doc_scopus_id": "77950688577",
        "doc_doi": "10.1109/ICTEL.2009.5158633",
        "doc_eid": "2-s2.0-77950688577",
        "doc_date": "2009-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "3GPP IMS",
            "Admission Control",
            "Bandwidth requirement",
            "Communication process",
            "Environment characteristic",
            "General networks",
            "IP multimedia subsystems"
        ],
        "doc_abstract": "The 3GPP consortium proposed in the release 7 of the IP Multimedia Subsystem (IMS) a Diameter interface for the resource admission communication process replacing the previous COPS solution. Although both academic and industry communities have deeply debate the advantages and disadvantages of each protocol, its impact in NGN may have not been thoroughly quantified. This paper compares both protocols in terms of messages exchanged between network entities, and of bandwidth requirements during the admission control process. Based on general network operator environment characteristics, we present several exploitation scenarios where it is analyzed the scalability and adequacy of each protocol. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A graphical user interface for policy composition in CIM-SPL",
        "doc_scopus_id": "74549142283",
        "doc_doi": "10.1109/ICUMT.2009.5345411",
        "doc_eid": "2-s2.0-74549142283",
        "doc_date": "2009-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Drag and drop",
            "Graphical interface",
            "Language syntax",
            "Management scenarios",
            "Network administrator",
            "Policy creation",
            "Policy specification",
            "Policy specification languages"
        ],
        "doc_abstract": "CIM-SPL is a declarative policy specification language proposed inside DMTF. SPL policies allow the specification of rules to govern the behavior of a system using a PBM approach. However, SPL requires thorough knowledge of the language syntax as well as full understanding of the management scenario and its available management features. This paper describes a graphical CIM-SPL editor application and the supporting policy edition metaphor. A graphical composition process of SPL policies is proposed, based on the use of drag and drop operations of the policy component items in a graphical interface. The editor includes policy creation wizards that guide the user in the policy specification process, in order to alleviate network administrators from the difficulties associated with the intricacies of SPL language. Additionally, a text-based SPL edition tool is provided as a complement for experienced SPL language operators. ©2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An evaluation of network management protocols",
        "doc_scopus_id": "70449344247",
        "doc_doi": "10.1109/INM.2009.5188859",
        "doc_eid": "2-s2.0-70449344247",
        "doc_date": "2009-11-17",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "All-IP networks",
            "Comparative studies",
            "COPS",
            "Diameter",
            "Global networks",
            "Information models",
            "Management protocols",
            "Network management protocols",
            "Round trip delay",
            "SNMP",
            "WBEM"
        ],
        "doc_abstract": "During the last decade several network management olutions have been proposed or extended to cope with the growing complexity of networks, systems and services. Architectures, protocols, and information models have been proposed as a way to better respond to the new and different demands of global networks. However this offer also leads to a growing complexity of management solutions and to an increase in systems' requirements. The current management landscape is populated with a multiplicity of protocols, initially developed as an answer to different requirements. This paper presents a comparative study of currently common management protocols in All-IP networks: SNMP, COPS, Diameter, CIM/XML over HTTP and CIM/XML over SOAP. This assessment was focused on wireless aspect issues, and as such includes measures of bandwidth, packets, round-trip delays, and agents' requirements. We also analyzed the advantages of compression in these protocols. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Syntactic parsing for bio-molecular event detection from scientific literature",
        "doc_scopus_id": "71049149316",
        "doc_doi": "10.1007/978-3-642-04686-5_7",
        "doc_eid": "2-s2.0-71049149316",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bio-molecular events",
            "Biomedical literature",
            "Information extraction",
            "Semantic properties",
            "Syntactic parsing"
        ],
        "doc_abstract": "Rapid advances in science and in laboratorial and computing methods are generating vast amounts of data and scientific literature. In order to keep up-to-date with the expanding knowledge in their field of study, researchers are facing an increasing need for tools that help manage this information. In the genomics field, various databases have been created to save information in a formalized and easily accessible form. However, human curators are not capable of updating these databases at the same rate new studies are published. Advanced and robust text mining tools that automatically extract newly published information from scientific articles are required. This paper presents a methodology, based on syntactic parsing, for identification of gene events from the scientific literature. Evaluation of the proposed approach, based on the BioNLP shared task on event extraction, produced an average F-score of 47.1, for six event types. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GeNS: A biological data integration platform",
        "doc_scopus_id": "79954504232",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79954504232",
        "doc_date": "2009-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Biological data integration",
            "Biological database",
            "Comprehensive analysis",
            "Computational applications",
            "Concrete applications",
            "Data integration",
            "Data sets",
            "Data type",
            "Distributed database",
            "Existing systems",
            "Innovative approaches",
            "Integration strategy",
            "Overlapping data",
            "Public database",
            "Scientific achievements"
        ],
        "doc_abstract": "The scientific achievements coming from molecular biology depend greatly on the capability of computational applications to analyze the laboratorial results. A comprehensive analysis of an experiment requires typically the simultaneous study of the obtained dataset with data that is available in several distinct public databases. Nevertheless, developing a centralized access to these distributed databases rises up a set of challenges such as: what is the best integration strategy, how to solve nomenclature clashes, how to solve database overlapping data and how to deal with huge datasets. In this paper we present GeNS, a system that uses a simple and yet innovative approach to address several biological data integration issues. Compared with existing systems, the main advantages of GeNS are related to its maintenance simplicity and to its coverage and scalability, in terms of number of supported databases and data types. To support our claims we present the current use of GeNS in two concrete applications. GeNS currently contains more than 140 million of biological relations and it can be publicly downloaded or remotely access through SOAP web services.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Parallel DNA pyrosequencing unveils new zebrafish microRNAs",
        "doc_scopus_id": "65949098165",
        "doc_doi": "10.1186/1471-2164-10-195",
        "doc_eid": "2-s2.0-65949098165",
        "doc_date": "2009-04-27",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: MicroRNAs (miRNAs) are a new class of small RNAs of approximately 22 nucleotides in length that control eukaryotic gene expression by fine tuning mRNA translation. They regulate a wide variety of biological processes, namely developmental timing, cell differentiation, cell proliferation, immune response and infection. For this reason, their identification is essential to understand eukaryotic biology. Their small size, low abundance and high instability complicated early identification, however cloning/Sanger sequencing and new generation genome sequencing approaches overcame most technical hurdles and are being used for rapid miRNA identification in many eukaryotes. Results: We have applied 454 DNA pyrosequencing technology to miRNA discovery in zebrafish (Danio rerio). For this, a series of cDNA libraries were prepared from miRNAs isolated at different embryonic time points and from fully developed organs. Each cDNA library was tagged with specific sequences and was sequenced using the Roche FLX genome sequencer. This approach retrieved 90% of the 192 miRNAs previously identified by cloning/Sanger sequencing and bioinformatics. Twenty five novel miRNAs were predicted, 107 miRNA star sequences and also 41 candidate miRNA targets were identified. A miRNA expression profile built on the basis of pyrosequencing read numbers showed high expression of most miRNAs throughout zebrafish development and identified tissue specific miRNAs. Conclusion: This study increases the number of zebrafish miRNAs from 192 to 217 and demonstrates that a single DNA mini-chip pyrosequencing run is effective in miRNA identification in zebrafish. This methodology also produced sufficient information to elucidate miRNA expression patterns during development and in differentiated organs. Moreover, some zebrafish miRNA star sequences were more abundant than their corresponding miRNAs, suggesting a functional role for the former in gene expression control in this vertebrate model organism. © 2009 Soares et al; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design, development, exploitation and assessment of a Cardiology Web PACS",
        "doc_scopus_id": "59149083992",
        "doc_doi": "10.1016/j.cmpb.2008.10.015",
        "doc_eid": "2-s2.0-59149083992",
        "doc_date": "2009-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Cardio-PACS",
            "DICOM",
            "PACS",
            "Telework",
            "Web-PACS"
        ],
        "doc_abstract": "Healthcare institutions are increasingly turning to digital medical imaging systems to promote better diagnosis and treatment of their patients. The implementation of the Picture Archiving and Communication System (PACS) clearly contributes to an increase in the productivity of health professionals. However, despite the amount of research that has been done in the past two decades, there are still several technological hurdles that hinder the wide adoption of PACS in the Web environment. In this paper, we present a Web-enabled PACS that through the inclusion of several DICOM services and compression methods promotes medical image availability and greater accessibility to users. © 2008 Elsevier Ireland Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2008-12-30 2008-12-30 2011-01-20T10:46:36 S0169-2607(08)00260-5 S0169260708002605 10.1016/j.cmpb.2008.10.015 S300 S300.1 FULL-TEXT 2015-05-14T05:25:39.534626-04:00 0 0 20090301 20090331 2009 2008-12-30T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0169-2607 01692607 93 93 3 3 Volume 93, Issue 3 7 273 282 273 282 200903 March 2009 2009-03-01 2009-03-31 2009 Section II. Systems and Programs article fla Copyright © 2008 Elsevier Ireland Ltd. All rights reserved. DESIGNDEVELOPMENTEXPLOITATIONASSESSMENTACARDIOLOGYWEBPACS COSTA C 1 Introduction 2 Methods and materials 2.1 Scenario 2.2 Image compression 2.3 DICOM private transfer syntax 3 Results 3.1 Software engineering 3.2 System workflow 3.3 Integration with a cardiovascular information system 3.4 Web-enabled interface 3.5 Exploitation 4 Discussion 5 Conclusions References HUANG 2004 H PACSIMAGINGINFORMATICSBASICPRINCIPLESAPPLICATIONS COSTA 2007 S322 S323 C CARS2007INTERNATIONALCONGRESSEXHIBITIONCOMPUTERASSISTEDRADIOLOGYSURGERY2 ENHANCEDPACSSUPPORTDEMANDINGTELEMEDICINETELEWORKSCENARIOS POLONIA 2008 D PROCEEDINGSSPIEVOLUME6919691906MEDICALIMAGING2008 BROKERAGEMECHANISMPROPOSALFORTELERADIOLOGYSTUDIESDISTRIBUTION OTERO 2008 834 841 H DALLESSIO 2007 34 35 K KALDOUDIA 2006 117 127 E SILVA 1998 A PROCEEDINGSSPIEMEDICALIMAGING ACARDIOLOGYORIENTEDPACS COSTA 2004 C PACSIMAGINGINFORMATICSPROCEEDINGSSPIE HIMAGEPACSANEWAPPROACHSTORAGEINTEGRATIONDISTRIBUTIONCARDIOLOGICIMAGES BRENNECKE 2000 1388 1397 R KERENSKY 2000 1370 1379 R TUINENBURG 2000 1380 1387 J UMEDA 2004 1297 1303 A SEGAR 1999 714 719 D KARSON 1996 769 778 T DICOMP 2004 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART5DATASTRUCTURESENCODING ZHANGA 2003 J COMPUTERIZEDMEDICALIMAGINGGRAPHICS PACSWEBBASEDIMAGEDISTRIBUTIONDISPLAY MATSOPOULOS 2004 53 71 G MARCOS 2007 255 269 E DICOMP 2007 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMPART4SERVICECLASSSPECIFICATIONS BOSWORTH 2003 A XMLSOAPBINARYDATAVERSION10 LEPANTO 2006 92 97 L REINER 2002 22 26 B MACKINNON 2008 796 804 A DICOMSUPL 2004 DIGITALIMAGINGCOMMUNICATIONSINMEDICINEDICOMSUPPLEMENT42MPEG2TRANSFERSYNTAX COSTAX2009X273 COSTAX2009X273X282 COSTAX2009X273XC COSTAX2009X273X282XC item S0169-2607(08)00260-5 S0169260708002605 10.1016/j.cmpb.2008.10.015 271322 2011-02-03T13:14:20.981783-05:00 2009-03-01 2009-03-31 true 2647107 MAIN 10 48258 849 656 IMAGE-WEB-PDF 1 gr1 104685 459 620 gr1 11739 162 219 gr2 32220 134 565 gr2 2373 52 219 gr3 67879 406 650 gr3 6824 137 219 gr4 27833 157 317 gr4 8334 109 219 gr5 95323 645 654 gr5 8754 164 166 gr6 75285 323 654 gr6 10920 108 219 gr7 107903 486 562 gr7 13899 163 189 gr8 108388 436 715 gr8 14170 134 219 gr9 103797 550 558 gr9 11491 164 166 gr10 117533 453 753 gr10 14321 132 219 COMM 2858 S0169-2607(08)00260-5 10.1016/j.cmpb.2008.10.015 Elsevier Ireland Ltd Fig. 1 A multimedia container that allows the introduction of a specific codec (MPEG4 in this case) inside the DICOM private transfer syntax. Fig. 2 An example of a DICOM dump: DICOM “default transfer syntax” vs. “private syntax”. Fig. 3 HIMAGE services and processes workflow. Fig. 4 HIMAGE Web components architecture. Fig. 5 HIMAGE main infra-structure inside the CHVNG cardiology department. Fig. 6 Himage Integration Engine (CIS+PACS). Fig. 7 Main interface of the Web HIMAGE. Fig. 8 Web HIMAGE–viewer window (Normal/Compare Mode). Fig. 9 Web HIMAGE–reporting module. Fig. 10 HIMAGE standalone DICOM viewer. Table 1 CHVNG cardiology department—US statistics. Number of procedures (exams) 36,418 Number of images (still+cine-loops) 856,215 Total size 230,984,532,254bytes (215GB) Average procedure size 6.05MB Average file size 263.5kB Average files/procedure 23.5 (majority of cine-loops) Design, development, exploitation and assessment of a Cardiology Web PACS Carlos Costa a ⁎ José L. Oliveira a Augusto Silva a Vasco Gama Ribeiro b José Ribeiro b a University of Aveiro – DETI/IEETA, 3810-193 Aveiro, Portugal b Centro Hospitalar de Vila Nova de Gaia, Portugal ⁎ Corresponding author. Tel.: +351 234370500. Healthcare institutions are increasingly turning to digital medical imaging systems to promote better diagnosis and treatment of their patients. The implementation of the Picture Archiving and Communication System (PACS) clearly contributes to an increase in the productivity of health professionals. However, despite the amount of research that has been done in the past two decades, there are still several technological hurdles that hinder the wide adoption of PACS in the Web environment. In this paper, we present a Web-enabled PACS that through the inclusion of several DICOM services and compression methods promotes medical image availability and greater accessibility to users. Keywords PACS DICOM Cardio-PACS Web-PACS Telemedicine Telework 1 Introduction A Picture Archiving and Communication System (PACS) is one of the most valuable tools supporting the medical profession both in decision-making and during treatment procedures. It encompasses several technologies that are used in the acquisition, archiving, distribution, and visualization of digital medical images [1]. The Digital Imaging and Communications in Medicine standard (DICOM) [2] was a major contribution to the facilitated exchange of structured medical imaging data, and is now a key component in PACS's success. Currently, almost all medical imaging equipment manufacturers provide DICOM digital output in their products. The deployment of PACS has enabled faster and broader access to medical image data. The movement from film-based processes to digital processes, allied to faster and more robust network infrastructure, reduced the costs associated with the storage and management of images, simplified data portability, and has paved the way for the development of new applications and working scenarios. A significant benefit of digital medical imaging is its availability, both within and between health institutions. Together with a new suite of Internet Protocol (IP)-based applications, such as IP phone and teleconference, PACS presents a tremendous opportunity for the introduction of telemedicine, telework, and collaborative teams, which will help to bridge frontiers and overcome the medical resource asymmetries found in several regions [3,4]. However, several problematic issues still remain, most of them related to the huge volume of data, and to the lack of interoperability between distinct PACS products. For instance, dynamic image modalities (films) such as cardiac ultrasound (US) and X-ray angiography (XA) typically generate hundreds of MB of data for each study. To keep these exams permanently available to practitioners it is necessary to create robust and efficient storage and communication infrastructures. There are no significant technical differences between the PACS used in cardiology and in radiology areas. However, they are often separate systems that tackle distinct user needs, namely related to visualization and reporting protocols [5]. Since almost all systems are DICOM compliant, it is possible from a radiology workstation to query and retrieve images located in a cardiology storage server, and vice-versa. The handicap is that an integrated system can lead to a more efficient workflow and improved access to images and data [6,7]. This paper presents a Web-enabled PACS framework, entitled HIMAGE, which is specially designed to support demanding cardiac imaging laboratories. HIMAGE enables the acquisition, storage, transmission, and visualization of DICOM cardiovascular sequences, providing a cost-efficient digital archive. The core of our approach is the implementation of a private DICOM transfer syntax that supports any video encoder that best suits the specificities of a particular imaging modality or working scenario. The major advantage of the proposed system stems from the high compression rate achieved by video encoding, while still maintaining diagnostic quality in the medical image sequences. This approach ensures full online availability of the studies and simplifies the transmission of medical data over the Internet 2 Methods and materials 2.1 Scenario HIMAGE was developed in the CHVNG cardiology department (Centro Hospitalar de Vila Nova de Gaia – Hospital of Gaia), which is supported by two imaging laboratories. The first implementation of HIMAGE was based on JPEG Baseline (Joint Photographic Experts Group) [8]. With this system, an echocardiography (US) study typically generates data between 20 and 30MB and an angiographic (XA) study between 40 and 60MB, depending on the technical characteristics of the equipment, on the operator expertise, and on the procedure type [9]. Although this volume can be easily transferred inside an institution Intranet or through the Internet, the latency, especially in the latter case, may be a drawback for some scenarios. One possible solution can be the exploitation of lossy compression techniques that still preserve the diagnostic quality of the existent JPEG solutions. Over the years, several studies have investigated the impact of various compression ratios on image quality, and on the observer performance for several diagnostic tasks. The general trend shown by the results of these studies is that optimal compression ratios for a modality are rarely achieved, as the observer's performance is tightly coupled with each particular diagnostic task. Currently, guidelines are published by international scientific and professional organizations, recommending suitable algorithms and compression ratios for a broad range of diagnostic tasks (see [10–12] for XA and [13–15] for US). 2.2 Image compression In cardiology, the majority of the data produced in the XA and US procedures is related to cine-loop sequences, which have a significant time-space redundancy, especially in ultrasound. Because the JPEG-based DICOM compression algorithms only explore the intra-frame image redundancy (space) [16], we decided to investigate the utilization of a more powerful video encoder that could also contemplate the inter-frame redundancy (time). In general, distinct digital video codecs produce significantly dissimilar results and medical image sequence types (modalities, cine/still, color/grayscale, etc.). This occurs because distinct sequences contain distinct kinds of information (motion, noise, etc.). As a result, it is not possible to select the very best among several codecs, to use for compression of all types of image/video. In the particular case of US cardiovascular images, after several trials and result evaluations with dynamic encoders [9], we realized that Moving Picture Experts Group (MPEG4) was the coding standard that provided the best tradeoff between image quality and storage requirements. The emergence of MPEG4 [17] as a coding standard for multimedia data with object-based and other enhanced encoding facilities appears to be a good alternative for the cost-effective storage and transmission of cardiac digital cine-loop sequences. With this new codec, a typical US file rarely exceeds 200–300kB [9]. At this order of magnitude, it is already feasible to envisage a pure online archive solution capable of handling all the registered procedures whatever their clinical or epidemiological life-cycle may be. Another immediate consequence of our encoding approach is that the reduced transmission times, either in Intranet mode or in Internet mode, may now be considered, in the worst cases, minor drawbacks in the overall imaging workflow. 2.3 DICOM private transfer syntax Since MPEG4 is not a DICOM native coding schema, subsequent image transmission, decoding and reviewing is accomplished through a specifically designed DICOM private transfer syntax. Moreover, to ensure that HIMAGE has the flexibility to support other modalities/encoders, it was decided not to insert the MPEG4 directly in the Tag Length Value (TLV) of the DICOM data structure [18]. Instead, we developed a multimedia container that dynamically supports several encoders (Fig. 1 ). The container includes a field that stores the encoder ID code, which is similar to the field used by the Audio Video Interleave (AVI) RIFF headers [19]. This approach represents the best modular software solution, as we simply need to change a single parameter in the HIMAGE conversion engine if a more efficient codec is developed. In Fig. 2 it is possible to compare parts of the DICOM representation of the “default transfer syntax” against our “private syntax,” in the specific case of a US 25-frame image sequence, with a RGB 576*768 matrix. Two important aspects are noticeable. First, the DICOM transfer syntax identifier is changed (from DefaultTransferSyntaxUID to PrivateTransferSyntaxUID). Second, the “PixelData” field size is reduced 120 times (from 33177600 to 275968). 3 Results 3.1 Software engineering In the initial versions of HIMAGE, the main objective was to reduce data volume through the implementation of a private transfer syntax that was capable of supporting multiple video encoders. In the current version herein described, another attained goal is the provisioning of a Web-based PACS that can be accessed easily and securely through a common Web browser with no further need for complex local installation of software. Web solutions can be created using one of the current available architectures [20–22]. Our approach is based on the .NET framework, which allows for smooth integration with existing code (in C, C++ and Visual Basic). All of the core functions used to manipulate the images and the DICOM structures were written in C++ and packed as Dynamic-link Library (DLL) components. We have developed a DICOM C++ SDK (Software Development Kit) that allows manipulation of the DICOM persistent object (i.e. structured files) and implements several HIMAGE (DICOM) network services such as Storage, Query/Retrieve and Modality Worklist (Fig. 3 ) [23]. The graphical components supporting all image-related tasks (visualization, reporting, etc.) were developed as Visual Basic plugins (ActiveX) and are directly embedded in the ASPX .NET pages (Fig. 4 ). The ActiveX viewer allows the direct integration of private DICOM files with the Web content. The communication between the dynamic HTML contents and the ActiveX binary is performed by JavaScript code. To be visualized or processed, the study images must be downloaded from the server to the client platform. All DICOM persistent object transfers (retrieves) are supported by a Web Service developed in C#. Though the Web Services are actually using XML-SOAP (Extensible Markup Language-Simple Object Access Protocol) [24] to transfer data, the XML [25] does not easily handle embedded binary data, which can create problems for DICOM file transfer. There are several issues, such as memory size and computation costs, that are associated with the conversion of binary data into base64 format (which is treated as a string). This encoding schema (base64) typically increases the original object size by 33% and also increases the processing time relative to binary-text-binary conversion [26]. To bypass this XML-SOAP drawback, it was necessary to develop a parallel gateway channel (HTTP encapsulated) to transfer DICOM files in a more efficient way. 3.2 System workflow Our clinical facility (CHVNG) is equipped with nine echocardiography machines distributed over three geographically dispersed hospital units. They have standard DICOM3.0 output interfaces and a configuration that ensures an automatic DICOM SCU (client) storage service at the end of each medical procedure. Daily output to the network typically reaches approximately 1200 DICOM files (both still and cine-loops). The ultrasound image data is sent to the Acquisition Processing Unit (APU) in DICOM default transfer syntax, i.e. uncompressed format (Fig. 5 ). The received procedures are analyzed (the alphanumeric data is extracted from DICOM headers) to detect eventual exam/patient ID errors and, if format conditions are verified, the exam is added to HIMAGE with the image data compressed and embedded in a new DICOM private syntax file. The result is then passed into the “Storage Server” that makes it permanently available to the PACS users. The original raw images are also saved, but they are only kept online for 6 months. The developed client application consists of a Web DICOM viewer (Fig. 7) that handles medical images and films available in the HIMAGE database, formatted in standard DICOM or in DICOM extended with our private syntax. Since the HIMAGE client solution is completely developed in Web technology, the access to the exams is controlled by appropriate security rules. Authentication is performed through a username/password pair and communication security is assured by an HTTPS connection. 3.3 Integration with a cardiovascular information system Implementation of an integrated healthcare access interface to patient clinical data represents the core element to accomplish new healthcare services with improved quality and efficiency. After the emergence of PACS as a fundamental infrastructure of any digital imaging department, the focus turned to the possibilities of integrating the medical image with other sources of information. Cardiologists need structured reports including images and related patient information. In our CHVNG scenario, a commercial Cardiovascular Information System (CIS) handles this additional information. Integration of the PACS with the existing CIS is an important feature for the healthcare professional. Besides the interface conformance, on network and application protocols, it is crucial to ensure data consistency among several systems. To implement this requirement, HIMAGE provides a DICOM Modality Worklist SCP service that is connected to the CIS database. It enables modalities (using C-FIND command) to query the server for patient and study-related information that will be thereafter inserted in the DICOM header. This process ensures consistency between both information systems and also that all PACS studies can be correctly referenced from the CIS. Concerning integrated access to information, the HIMAGE provides smooth integration of patient image data in the CIS environment. The inclusion of PACS images in other environments is easy due to DICOM private transfer syntax container portability. A “Himage Integration Engine” module (Fig. 6 ) was developed with two integration options: a. Multimedia AVI container: the engine can extract the MPEG4 encoded image data from the DICOM and encapsulate it in an AVI file to be, for instance, inserted in the CIS Web document; b. Plug-in module: an ActiveX application viewer is available, allowing the direct integration of the HIMAGE DICOM files in both Winforms and Web environments. This plug-in downloads the image data from HIMAGE archive and displays it. 3.4 Web-enabled interface During the last 5 years, the ubiquity of Web interfaces has pushed practically all PACS suppliers to develop client applications in which clinical practitioners can receive and analyze medical images using conventional personal computers and Web browsers [20,21]. Because of security and performance issues, use of these software packages has mostly been restricted to Intranets. Paradoxically, one of the most important advantages of digital imaging systems was to enable the widespread sharing and remote access to medical data between healthcare institutions. The HIMAGE Web version is fully operational, provides all the necessary functionality, and has the same performance and flexibility as the previous desktop version [9]. The application setup is very simple. It is downloaded from the Web server and is automatically installed when given explicit authorization by the user. Graphically, the HIMAGE main window includes a grid box with a list of patients and a movie preview of three sequences for the current selection (Fig. 7 ). The user can search all procedures using criteria such as patient name, patient ID, procedure ID/type/date, source institutions and equipment. A second graphical application layer provides interfaces to the system modules: communications to telecardiology, DICOM viewer (Fig. 8 ), report (Fig. 9 ) and export. In the DICOM viewer window (Fig. 8), it is possible to visualize the image sequences (still and cine-loops) and to select frames from various sequences that are then sent to the report area. Other traditional functions have been included such as image manipulation tools (contrast/brightness), printing capability, and the export of images in distinct formats (DICOM3.0 default transfer syntax, AVI, BMP, JPEG). It is also possible to copy a specific image to the clipboard and paste it onto some other external application. In the compare mode, the viewer window can work in either of two ways: automatic or manual. The automatic mode is used for a specific type of procedure—the “Stress eco” (Fig. 8). In this case, the visualization technique is based on the information stored in the DICOM file headers. HIMAGE provides a simultaneous and synchronized display of the several “Stages” of a heart “View.” The term “Stage” is defined as a phase in the stress echo exam protocol. The “View” is the result of a particular combination of the transducer position and orientation at the time of the image acquisition. Manual mode allows the user to select among distinct images of the same and/or different procedures that can then be displayed in a defined window matrix. In the report module (Fig. 9), the user can arrange the location of the image(s) or delete some frames using a drag-and-drop functionality. Finally, the output image matrix (2×3 or 3×3) is bundled with the clinical report to generate an Rich Text Format (RTF) file that is compatible with common text editors. The user can customize the base template used to generate the report file. For example, one could include the institution logo and report headers. The export module allows the procedure to be saved in a CDROM or DVD-ROM, using the uncompressed DICOM default transfer syntax format or the AVI format. A standalone fully compliant DICOM viewer application is also stored (Fig. 10 ), which starts automatically whenever the disk is used. 3.5 Exploitation The HIMAGE is installed in two central hospitals and three small diagnostic centers. Our main installation and research laboratory is the CHVNG cardiology department with approximately 65,000 patient records. In Table 1 it is possible to observe some general statistics relative to the CHVNG US data. Concerning transmission times, the Web HIMAGE images are downloaded in packs of 3 sequences (due to preview mode) based on the user selection. In a telework environment, supported by a 4Mbits ADSL (Asymmetric Digital Subscriber Line), a complete pack of 3 cine-loops takes typically less then 10s to download, decompress and display, including the overhead introduced with an encrypted SSL (Secure Sockets Layer) channel. 4 Discussion We have described a Web-enabled PACS software solution that is specially oriented to support demanding cardiac imaging laboratories. It provides a cost-efficient digital archive, ensures full online availability of studies and simplifies the transmission of medical data over the Internet. The benefits obtained from the availability of all historical image data in a simple, fast and integrated Web interface are unquestionable to practitioners and to patients, and are likely to induce a significant improvement in the overall quality of healthcare services. In our main installation, two years after the introduction of HIMAGE in the echocardiography laboratory, and maintaining the human resources, one realize an increase of procedures/year from 4000 to 7000, which is a good indication considering other reported results of PACS productivity improvements [27–29]. Although this result can be explained by several factors, to an increase on patient demand and on optimized workflow process, an informal and continuous assessment performed near the physicians suggest that HIMAGE was a major driving to this change: (a) the dead times and the information handling mistakes were considerably reduced; (b) since HIMAGE is ubiquitously available, the remote diagnostic can be easily performed without moving the patient or his records between several buildings; (c) the patient history (CIS reports and PACS images) is permanently available, which avoids the need for CD or other media storage manual handling. One main HIMAGE advantage is associated with its transfer rate efficiency. Healthcare professionals do not adopt telemedicine or telework platforms if they need to wait, for instance, 2–3h to receive/download a clinical image study with diagnostic quality. The advantage of the proposed system stems from the implementation of a private DICOM transfer syntax that supports any video encoder. In the cardiac US we are using a MPEG4 codec that provides high compression and maintains diagnostic quality. Qualitative assessments have been made of the previous HIMAGE Desktop version (no Web interface) [9]. In a simultaneous and blind display of the original against the compressed cine-loops, 37% of trials have selected the compressed sequence as the best image. This suggests that other factors related to visualization conditions are more likely to influence observer accuracy than the image compression itself. The interoperability of HIMAGE with other DICOM PACS implies the decompression of private MPEG4 images to an uncompressed normalized format like, for instance, the DICOM Default Transfer Syntax. However, we believe that, in the future, MPEG4 will be adopted by the DICOM standard that currently only supports MPEG-2 [30]. Finally, a major constraint of the presented solution is its dependency on Windows and on Internet Explorer browser. This has to do with the richness of the interface that requires a powerful platform to work (like ActiveX or Flash). The result will not be possible using pure web2.0 technologies. This decision provides a Web interface supporting all the requirements of a DICOM viewer. In the future, it is planned to update the HIMAGE platform to Microsoft .NET Silverlight technology, which is multi operating system compliant. 5 Conclusions In this paper, we have presented the Web HIMAGE software, a DICOM conformant Web PACS. Reducing the size of exam images, while preserving the diagnostic quality, is a major novel feature of this HIMAGE application. This is especially important for ensuring time-effective transmission through Internet connections. We successfully demonstrated the utility of HIMAGE (started with the HIMAGE desktop version) in a telemedicine project established between CHVNG (Portugal) and the Central Hospital of Maputo (Mozambique). The Web version of HIMAGE is currently being used in clinical environments with different requirements, from small private laboratories to public central hospitals. In the main installation, (CHVNG), we have made more than 36,000 US procedures permanently available through this system (850,000 cardiovascular digital sequences). References [1] H.K. Huang PACS and Imaging Informatics: Basic Principles and Applications 2004 Wiley [2] DICOM, Digital Imaging and Communications in Medicine version 3.0, ACR (the American College of Radiology) and NEMA (the National Electrical Manufacturers Association), [3] C. Costa J.L. Oliveira A. Silva V. Gama J. Ribeiro Enhanced PACS to support demanding telemedicine and telework scenarios CARS 2007-International Congress and Exhibition: Computer Assisted Radiology and Surgery 2 2007 S322 S323 [4] D. Polónia A. Silva C. Costa J.L. Oliveira Brokerage mechanism proposal for teleradiology studies distribution P.A. Katherine M.S. Khan Proceedings of SPIE-Volume 6919, 691906. Medical Imaging 2008 2008 PACS and Imaging Informatics San Diego, USA [5] H.J. Otero L. Nallamshetty F.J. Rybicki Interdepartmental conflict management and negotiation in cardiovascular imaging Journal of the American College of Radiology 5 2008 834 841 [6] K.M. Dallessio Integrating cardiology and radiology PACS Applied Radiology 36 2007 34 35 [7] E. Kaldoudia D. Karaiskakisb A service based approach for medical image distribution in healthcare Intranets Computer Methods and Programs in Biomedicine 81 2006 117 127 [8] A. Silva C. Costa P. Abrantes V. Gama A. Boer A cardiology oriented PACS Proceedings of SPIE: Medical Imaging San Diego, USA 1998 [9] C. Costa A. Silva J.L. Oliveira V. Ribeiro J. Ribeiro Himage PACS: a new approach to storage, integration and distribution of cardiologic images PACS and Imaging Informatics-Proceedings of SPIE San Diego, CA, USA 2004 [10] R.U. Brennecke U. Burgel R.U. Simon G. Rippin H.P. Fritsch T. Becker S.E. Nissen American College of Cardiology/European Society of Cardiology international study of angiographic data compression phase III: measurement of image quality differences at varying levels of data compression Journal of the American College of Cardiology 35 2000 1388 1397 [11] R.A. Kerensky J.T. Cusma P. Kubilis R.U. Simon T.M. Bashore J.W. Hirshfeld Jr. D.R. Holmes Jr. C.J. Pepine S.E. Nissen American College of Cardiology/European Society of Cardiology international study of angiographic data compression phase I: the effects of lossy data compression on recognition of diagnostic features in digital coronary angiography Journal of the American College of Cardiology 35 2000 1370 1379 [12] J.C. Tuinenburg G. Koning E. Hekking A.H. Zwinderman T. Becker R.U. Simon J.H.C. Reiber American College of Cardiology/European Society of Cardiology international study of angiographic data compression phase II: the effects of varying JPEG data compression levels on the quantitative assessment of the degree of stenosis in digital coronary angiography Journal of the American College of Cardiology 35 2000 1380 1387 [13] A. Umeda Y. Iwata Y. Okada M. Shimada A. Baba Y. Minatogawa T. Yamada M. Chino T. Watanabe M. Akaishi A low-cost digital filing system for echocardiography data with MPEG4 compression and its application to remote diagnosis Journal of the American Society of Echocardiography 17 2004 1297 1303 [14] D.S. Segar D. Skolnick S.G. Sawada G. Fitch D. Wagner D. Adams H. Feigenbaum A comparison of the interpretation of digitized and videotape recorded echocardiogram Journal of the American Society of Echocardiography 12 1999 714 719 [15] T.H. Karson R.C. Zepp S. Chandra A. Morchead J.D. Thomas Digital storage of echocardiograms offers superior image quality to analog storage, even with 20:1 digital compression: results of the digital echo record access study Journal of the American Society of Echocardiography 9 1996 769 778 [16] Joint Photographic Experts Group, JPEG standard (ITU-T T.81 | ISO/IEC 10918-1), 1994. [17] ISO/IEC Moving Picture Experts Group, MPEG-4 standard (ISO/IEC 14496), 1999. [18] DICOM-P5 Digital Imaging and Communications in Medicine (DICOM), Part 5: Data Structures and Encoding 2004 National Electrical Manufacturers Association [19] IBM Corporation and Microsoft Corporation, Multimedia Programming Interface and Data Specifications 1.0, 2001 (cited, available from: [20] J. Zhanga J. Suna J.N. Stahl PACS and Web-based image distribution and display Computerized Medical Imaging and Graphics vol. 27 2003 Elsevier pp. 197–206 [21] G.K. Matsopoulos V. Kouloulias P. Asvestas N. Mouravliansky K. Delibasis D. Demetriades MITIS: a WWW-based medical system for managing and processing gynecological–obstetrical–radiological data Computer Methods and Programs in Biomedicine 76 2004 53 71 [22] E. Marcos C.J. Acuña B. Vela J.M. Cavero J.A. Hernández A database for medical image management Computer Methods and Programs in Biomedicine 86 2007 255 269 [23] DICOM-P4 Digital Imaging and Communications in Medicine (DICOM), Part 4: Service Class Specifications 2007 National Electrical Manufacturers Association [24] World Wide Web Consortium (W3C), Web Services Architecture. 2004, W3C Working Group Note 11, 2004 (available from [25] World Wide Web Consortium (W3C), Extensible Markup Language (XML) 1.0, 4th ed., 2006, W3C Recommendation (available from [26] A. Bosworth D. Box M. Gudgin M. Nottingham D. Orchard J. Schlimmer XML, SOAP and Binary Data—Version 1.0 2003 BEA Systems, Microsoft Corporation [27] L. Lepanto G. Pare D. Aubry P. Robillard J. Lesage Impact of PACS on dictation turnaround time and productivity Journal of Digital Imaging 19 2006 92 97 [28] B. Reiner E. Siegel M. Scanlon Changes in technologist productivity with implementation of an enterprisewide PACS Journal of Digital Imaging 15 2002 22 26 [29] A.D. Mackinnon R.A. Billington E.J. Adam D.D. Dundas U. Patel Picture archiving and communication systems lead to sustained improvements in reporting times and productivity: results of a 5-year audit Clinical Radiology 63 2008 796 804 [30] DICOM-SUPL42 Digital Imaging and Communications in Medicine (DICOM), Supplement 42: MPEG2 Transfer Syntax 2004 National Electrical Manufacturers Association "
    },
    {
        "doc_title": "Improving literature searches in gene expression studies",
        "doc_scopus_id": "58149141893",
        "doc_doi": "10.1007/978-3-540-85861-4_10",
        "doc_eid": "2-s2.0-58149141893",
        "doc_date": "2009-01-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (miscellaneous)",
                "area_abbreviation": "COMP",
                "area_code": "1701"
            },
            {
                "area_name": "Computational Mechanics",
                "area_abbreviation": "ENGI",
                "area_code": "2206"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "MEDLINE is the premier literature database in the biomedical and life sciences fields, containing over 17 million references to journal articles. Searching in this database can be performed through PubMed, a web interface designed to provide a rapid and comprehensive retrieval of articles matching a specific criteria. However, considering the complexity of biological systems and of the genotype to phenotype relations, the results retrieved from PubMed can be only a short view of the relevant information that is available. In this paper we present a new approach for expanding the terminology used in each query to enrich the set of documents that are retrieved. We have developed a paper prioritization methodology that, for a given list of genes, expands the search in several biological domains using a mesh of co-related terms, extracts the most relevant results from the literature, and organize them according to domain weighted factors. © 2009 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Analysing the evolution of repetitive strands in genomes",
        "doc_scopus_id": "77952571423",
        "doc_doi": "10.1007/978-3-642-02481-8_159",
        "doc_eid": "2-s2.0-77952571423",
        "doc_date": "2009-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Codon repetitions",
            "Different stages",
            "Eukaryotic organisms",
            "Motif analysis",
            "Orthologous genes",
            "Primary structures",
            "Species evolution"
        ],
        "doc_abstract": "The analysis of genomes and proteomes of the various organisms allow us to observe its behaviour in the evolution of species. In this study, we focus our attention on a particular aspect of this analysis: the conservation of specific codon and amino acid repetitions in orthologous genes belonging to eukaryotic organisms that are representative of different stages of species evolution. Since it is known that these repeats in humans are the cause of various neurodegenerative diseases, among others, this study help explaining if there is conservation or repression of such repetitions in the specialization process, and if there is any relationship between these repetitions and diseases in advanced live beings. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Endomyocardial Biopsy as Risk Factor in the Development of Tricuspid Insufficiency After Heart Transplantation",
        "doc_scopus_id": "64349115757",
        "doc_doi": "10.1016/j.transproceed.2009.02.011",
        "doc_eid": "2-s2.0-64349115757",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Transplantation",
                "area_abbreviation": "MEDI",
                "area_code": "2747"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Objective: Endomyocardial biopsy (EMB), which is used to monitor for rejection, may cause tricuspid regurgitation (TR) after orthotopic heart transplantation (OHT). The purpose of this investigation was to examine the occurrence of tricuspid valve tissue in myocardial specimens obtained by routine EMB performed after OHT. Patients and Methods: From January 2000 to July 2008, 125 of the patients who underwent OHT survived more than 1 month. Their follow-up varied from 1 month to 8.5 years (mean, 5.1 ± 3.7 years). EMB was the gold standard examination and myocardial scintigraphy with gallium served as a screen to routinely monitor rejection. Results: Each of 428 EMB including 4 to 7 fragments, totaling 1715 fragments, were reviewed for this study. The number of EMB per patient varied from 3 to 8 (mean, 4.6 ± 3.5). Histopathological analysis of these fragments showed tricuspid tissue in 4 patients (3.2%), among whom only 1 showed aggravation of TR. Conclusions: EMB remains the standard method to diagnose rejection after OLT. It can be performed with low risk. Reducing the number of EMB using gallium myocardial scintigraphy or other alternative methods as well as adoption of special care during the biopsy can significantly minimize trauma to the tricuspid valve. © 2009.",
        "available": true,
        "clean_text": "serial JL 271972 291210 291683 291912 31 Transplantation Proceedings TRANSPLANTATIONPROCEEDINGS 2009-04-17 2009-04-17 2010-04-18T20:09:46 S0041-1345(09)00148-1 S0041134509001481 10.1016/j.transproceed.2009.02.011 S300 S300.2 FULL-TEXT 2015-05-14T02:53:04.26492-04:00 0 0 20090401 20090430 2009 2009-04-17T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue table body affil articletitle auth authfirstini authfull authlast authsuff primabst pubtype ref alllist content subj ssids 0041-1345 00411345 41 41 3 3 Volume 41, Issue 3 43 935 937 935 937 200904 April 2009 2009-04-01 2009-04-30 2009 7th Luso-Brazilian Transplantation Congress Thoracic Transplantation article fla Copyright © 2009 Published by Elsevier Inc. ENDOMYOCARDIALBIOPSYRISKFACTORINDEVELOPMENTTRICUSPIDINSUFFICIENCYAFTERHEARTTRANSPLANTATION FIORELLI A Patients and Methods Results Discussion References NGUYEN 2005 S227 V MIELNICZUK 2005 1586 L SAHAR 1997 2675 G MUGGE 1990 884 A JEEVANANDAM 2006 2089 V ANDERSON 2004 1635 C FIORELLI 2007 2527 A MENEGUETTI 1987 171 J CAMARGO 1990 293 P FIORELLIX2009X935 FIORELLIX2009X935X937 FIORELLIX2009X935XA FIORELLIX2009X935X937XA item S0041-1345(09)00148-1 S0041134509001481 10.1016/j.transproceed.2009.02.011 271972 2010-09-22T10:16:15.243795-04:00 2009-04-01 2009-04-30 true 96739 MAIN 3 73586 849 656 IMAGE-WEB-PDF 1 TPS 20562 S0041-1345(09)00148-1 10.1016/j.transproceed.2009.02.011 Table 1 Echocardiographic Changes Before and After Tricuspid Chordal Tissue Disruption From the Routine EMB Case 1 Case 2 Case 3 Case 4 Before After Before After Before After Before After No. of EMB Tricuspid 2 3 1 1 1 1 0 0 Mitral 2 3 0 0 1 1 0 0 Pulmonary 0 2 0 0 0 0 0 0 Aortic 0 1 0 0 0 0 0 0 RV SH SH Normal Normal DH DH Normal Normal LV Normal Normal Normal Normal Normal Normal Normal Normal EF LV (%) 56 56 65 64 67 50 61 74 PAPs (mm Hg) 36 25 36 35 29 31 35 34 RV, right ventricle; LV, left ventricle; EF LV, ejection fraction left ventricle; PAPs, pulmonary arterial systolic pressure; SH, severe hypocontractility; DH, discrete hypocontractility. Valvar insufficiency degree was quantified as: 0 = absent; 1 = mild; 2 = moderate; and 3 = severe. Thoracic transplantation Cardiac Endomyocardial Biopsy as Risk Factor in the Development of Tricuspid Insufficiency After Heart Transplantation A.I. Fiorelli ⁎ G.H.B. Coelho J.L. Oliveira Jr V.D. Aiello L.A. Benvenuti A. Santos A. Chi A. Tallans M.L. Igushi F. Bacal E.A. Bocchi N.A.G. Stolf Heart Institute, São Paulo University School of Medicine, São Paulo, Brazil ⁎ Address reprint requests to Alfredo I. Fiorelli, Rua Morgado de Mateus 126/81, São Paulo/SP, Brazil, CEP: 0415-050 Objective Endomyocardial biopsy (EMB), which is used to monitor for rejection, may cause tricuspid regurgitation (TR) after orthotopic heart transplantation (OHT). The purpose of this investigation was to examine the occurrence of tricuspid valve tissue in myocardial specimens obtained by routine EMB performed after OHT. Patients and Methods From January 2000 to July 2008, 125 of the patients who underwent OHT survived more than 1 month. Their follow-up varied from 1 month to 8.5 years (mean, 5.1 ± 3.7 years). EMB was the gold standard examination and myocardial scintigraphy with gallium served as a screen to routinely monitor rejection. Results Each of 428 EMB including 4 to 7 fragments, totaling 1715 fragments, were reviewed for this study. The number of EMB per patient varied from 3 to 8 (mean, 4.6 ± 3.5). Histopathological analysis of these fragments showed tricuspid tissue in 4 patients (3.2%), among whom only 1 showed aggravation of TR. Conclusions EMB remains the standard method to diagnose rejection after OLT. It can be performed with low risk. Reducing the number of EMB using gallium myocardial scintigraphy or other alternative methods as well as adoption of special care during the biopsy can significantly minimize trauma to the tricuspid valve. Tricuspid regurgitation (TR) is a frequent complication after orthotopic heart transplantation (OHT). The etiology is multifactorial; however, a biopsy-induced flail leaflet is one of the most important mechanisms. Endomyocardial biopsy (EMB) is used to monitor for rejection. It may be one of the causes of TR. The presence of chordal tissue in EMB from heart transplant cases shows an intimate association with TR; it may lead to serious hemodynamic abnormalities. Various studies have retrospectively analyzed the occurrence of tricuspid tissue in EMB using echocardiographic findings of severe or mild TR. 1–3 The purpose of this study was to investigate the occurrence of tricuspid valve tissue in myocardial specimens obtained by routine EMB performed after OHT for evaluation of rejection. Patients and Methods From January 2000 to July 2008, the 125 patients who underwent OHT and survived for more than 1 month were selected for this investigation. Their follow-up varied from 1 month to 8.5 years (mean, 5.1 ± 3.7 years). All transplantations were performed by 2 principal surgeons; we retrospectively analyzed information regarding the surgical technique. EMBs were performed seeking to follow a predefined classic schedule; however, after the patients acquired clinical stability we routinly used gallium scintigraphy as the screening method. Only 3 cardiac surgeons performed all EMBs via the percutaneous right internal jugular vein approach with a short sheath of 9 French × 12 cm in length into the right atrium. A long sheath passing through the tricuspid valve was less often used. The bioptome was threaded and directed under fluoroscopy toward the right interventricular septum to blind take 4 to 6 fragments at each procedure. When the EBM was performed in the intensive care unit, the bioptome was guided by echocardiography. For this study 2 observers who worked independently reviewed all EMBs using optical microscopy; uncertain cases were evaluated by a third professional. For cases that found tricuspid tissue, we reviewed the 2-dimensional and color Doppler echocardiographic studies before and after EMB to evaluate tricuspid valve function. The TR degree was quantified as proposed by Mügge et al 4 in 1990. The data analysis was not able to stratify the cases, because the number of biopsies with the lesion was small. Continuous data are presented as mean values ± standard deviations. Results During the investigation, 125 patients underwent OHT, including 22 patients (17.6%) with an atrial anastomosis and a bicaval anastomosis with prophylactic donor heart tricuspid annuloplasty 86 (68.8%) versus without annuloplasty in 17 (13.6%). The technique was determined by surgeon preference and the transplantation era. There were 428 EBMs performed for rejection control, each one including 4 to 7 fragments, totaling 1715 fragments, all of which were reviewed for this study. The number of EMB per patient varied from 3 to 8 (mean, 4.6 ± 3.5). Meticulous histopathological analysis of these fragments identified tricuspid tissue in 4 patients (3.2%), among whom only 1 experienced aggravation of TR (case 1). Table 1 shows the echocardiographic findings. Discussion Severe intraoperative TR predicts poor late survival following OHT. Fortunately, severe valvar regurgitation does not occur in many cases. Several groups have applied prophylactic tricuspid annuloplasty of the donor heart, because severe late complications and reduced survival may arise from TR development. 5–7 TR is the most common valvular abnormality after OHT with an extremely variable prevalence between 25% and 45%. 1,5 Multiple factors play important roles in this process such as regurgitation classification, operative technique, follow-up time, pulmonary artery pressure, allograft behavior, right ventricular function, frequency/severity of rejection episodes, female donor, donor age, weight mismatch, ischemia time, and, not the least important, the number and technique of the EMB. 1,3,5 EMB remains the gold standard to monitor and diagnose allograft rejection after OHT. It may have a direct impact on the development of TR, although the strength of this association is controversial. 1–3 Most patients with evidence of significant TR after chordal tissue biopsy continue clinically asymptomatic, showing no significant change in hemodynamics. Moreover, there are many reports demonstrating a strong relationship between the number of EMB performed per patient and the development of flail tricuspid leaflets or chordal rupture. Nguyen et al 1 in 2005 strongly suggested a cutoff of <31 EMB to reduce the risk for severe TR. Mielniczuk et al 2 in 2005 reported 19/98 (19.4%) OHT patients displayed significant TR by echocardiogram during a 6-year follow-up. Among 205 biopsies in 19 patients (mean 10.7 ± 4.2), chordal tissue was found in 9 (9.2%). Similar results have been noted by other authors. 1–3 Probably if all the pieces of the biopsy were analyzed, the incidence would have been higher, because many patients remain asymptomatic. In the present study, the incidence of traumatic injury to the tricuspid valve secondary to a biopsy was small. Perhaps some factors significantly contributed to it: the biopsies by a surgeon; the reliance on myocardial scintigraphy with gallium as a screening method to reduce the number of EMB; and the use of a long sheath that could reach the right ventricle through the tricuspid valve. This approach has been applied at our institution for several years because it has reducted the routine EMB by >50%. 7–9 At the beginning of our experience in 1985, findings of tricuspid tissue in biopsies were more frequent, because we were passing through the learning curve. Furthermore, these observations confirmed the involvement of other risk factors in the genesis of TR; they were equally or more important than EMB alone. Only 1 patient (1/385; 0.3%) who was not part of this research developed severe tricuspid insufficiency requiring a valve replacement at 4 years after OHT. In conclusion, EMB remains the standard method to diagnose rejection after heart transplantation. It can be performed with low risk. A reduced number of EMB using screening with gallium myocardial scintigraphy or other alternative methods as well the adoption of special care during the biopsy can significantly minimize trauma to the tricuspid valve. References 1 V. Nguyen M. Cantarovich R. Cecere Tricuspid regurgitation after cardiac transplantation: how many biopsies are too many? J Heart Lung Transplant 24 7 suppl 2005 S227 2 L. Mielniczuk H. Haddad R.A. Davies Tricuspid Valve chordal tissue in endomyocardial biopsy specimens of patients with significant tricuspid regurgitation J Heart Lung Transplant 24 2005 1586 3 G. Sahar A. Stamler E. Erez Etiological factors influencing the development of atrioventricular valve incompetence after heart transplantation Transplant Proc 29 1997 2675 4 A. Mügge W.G. Daniel G. Herrmann Quantification of tricuspid regurgitation by Doppler color flow mapping after cardiac transplantation Am J Cardiol 66 1990 884 5 V. Jeevanandam H. Russell P. Mather Donor tricuspid annuloplasty during orthotopic heart transplantation: long-term results of a prospective controlled study Ann Thorac Surg 82 2006 2089 6 C.A. Anderson S.K. Shernan M. Leacche Severity of intraoperative tricuspid regurgitation predicts poor late survival following cardiac transplantation Ann Thorac Surg 78 2004 1635 7 A.I. Fiorelli N.A. Stolf C.A. Abreu Filho Prophylactic donor tricuspid annuloplasty in orthotopic bicaval heart transplantation Transplant Proc 39 2007 2527 8 J.C. Meneguetti E.E. Camargo J. Soares Jr Gallium-67 imaging in human heart transplantation: correlation with endomyocardial biopsy J Heart Transplant 6 1987 171 9 P.R. Camargo R. Mazzieri R. Snitcowsky Correlation between gallium-67 imaging and endomyocardial biopsy in children with severe dilated cardiomyopathy Int J Cardiol 28 1990 293 "
    },
    {
        "doc_title": "Heart Transplantation in Arrhythmogenic Right Ventricular Dysplasia: Case Reports",
        "doc_scopus_id": "64349100837",
        "doc_doi": "10.1016/j.transproceed.2009.02.010",
        "doc_eid": "2-s2.0-64349100837",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Transplantation",
                "area_abbreviation": "MEDI",
                "area_code": "2747"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Objective: Arrhythmogenic right ventricular dysplasia (ARVD) is a myocardial disease of familiar, origin where the myocardium is replaced by fibrofatty tissue predominantly in the right ventricle. Herein we have presented the clinical courses of 4 patients with ARVD who underwent orthotopic heart transplantation. Patients and Methods: Among 358 adult patients undergoing heart transplantation, 4 (1.1%) displayed ARVD. The main indication for transplantation was the progression to heart failure associated with arrhythmias. All 4 patients displayed rapid, severe courses leading to heart failure with left ventricular involvement and uncontrolled arrhythmias. Results: In all cases the transplantation was performed using a bicaval technique with prophylactic tricuspid valve annuloplasty. One patient developed hyperacute rejection and infection, leading to death on the 7th day after surgery. The other 3 cases showed a good evolution with clinical remission of the symptoms. Pathological study of the explanted hearts confirmed the presence of the disease. Conclusions: ARVD is a serious cardiomyopathy that can develop malignant arrhythmias, severe ventricular dysfunction with right ventricular predominance, and sudden cardiac death. Orthotopic heart transplantation must always be considered in advanced cases of ARVD with malignant arrhythmias or refractory congestive heart failure with or without uncontrolled arrhythmias, because it is the only way to remit the symptoms and the disease. © 2009.",
        "available": true,
        "clean_text": "serial JL 271972 291210 291683 291912 31 Transplantation Proceedings TRANSPLANTATIONPROCEEDINGS 2009-04-17 2009-04-17 2010-04-18T20:09:46 S0041-1345(09)00147-X S004113450900147X 10.1016/j.transproceed.2009.02.010 S300 S300.2 FULL-TEXT 2015-05-14T02:53:04.26492-04:00 0 0 20090401 20090430 2009 2009-04-17T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue table body affil articletitle auth authfirstini authfull authlast authsuff primabst pubtype ref alllist content subj ssids 0041-1345 00411345 41 41 3 3 Volume 41, Issue 3 50 962 964 962 964 200904 April 2009 2009-04-01 2009-04-30 2009 7th Luso-Brazilian Transplantation Congress Case Reports article fla Copyright © 2009 Published by Elsevier Inc. HEARTTRANSPLANTATIONINARRHYTHMOGENICRIGHTVENTRICULARDYSPLASIACASEREPORTS FIORELLI A Case Reports Discussion References MORICJANISZEWSKA 2007 259 E NIJVELDT 2007 1819 R PEZAWAS 2006 360 T YODA 2005 2358 M MARCUS 1995 1298 F ATIK 2005 68 E MCKENNA 1994 215 W FIORELLIX2009X962 FIORELLIX2009X962X964 FIORELLIX2009X962XA FIORELLIX2009X962X964XA item S0041-1345(09)00147-X S004113450900147X 10.1016/j.transproceed.2009.02.010 271972 2010-09-22T10:16:15.244659-04:00 2009-04-01 2009-04-30 true 91594 MAIN 3 70419 849 656 IMAGE-WEB-PDF 1 TPS 20561 S0041-1345(09)00147-X 10.1016/j.transproceed.2009.02.010 Table 1 Characteristics of Patients With ARVD Who Underwent Orthotopic Heart Transplantation Case 1 Case 2 Case 3 Case 4 Average Gender F F M M 1:1 Age (y) 17 20 21 15 18.7 ± 2.5 Familiars with ARVD + − + + 75% Familiars with ARVD and sudden death + − + + 75% Symptomatic during the diagnosis − − + + 50% Kind of symptoms HF HF HF HF 100% Arrhythmia Arrhythmia Arrhythmia Arrhythmia 100% Syncope Syncope Syncope 75% Time of appearance (mos) 5 4 4 5 4.5 ± 0.6 Functional class (NYHA) III IV IV IV III, 25%; IV, 75% T-wave inversion and epsilon waves + − + − 50% EF LV (%) 35 43 20 20 29.5 ICD before heart transplantation + + + − 75% Cardiogenic shock/priority − + − + 50% Biventricular dysfunction + + + + 100% Waiting list (mos) 6 6 4 2 4.5 ± 1.9 Initial immunosuppression Aza Aza Aza MMF Aza, 75%; CsA, 100%; CsA CsA CsA CsA St St St St St, 100%; MMF, 25% Plas Immediate postoperative period − − HR; death on day 7 Pericardial effusion Alive, 3; death, 1 Follow-up (y) 8 6 1.8 1.1 4.42 ± 3.3 F, female; M, male; EF LV, ejection fraction left ventricle; HF, heart failure; HR, hyperacute rejection; NYHA, New York Heart. Association; Aza, azathioprine; CsA, cyclosporine; St, steroid; MMF, mycophenolate mofetil; Plas, plasmapheresis. Case report Heart Transplantation in Arrhythmogenic Right Ventricular Dysplasia: Case Reports A.I. Fiorelli ⁎ G.H.B. Coelho J.L. Oliveira Jr C.N.G. Nascimento L.B. Vilas Boas C.F.C. Napolitano F. Bacal E.A. Bochi N.A.G. Stolf Heart Institute, São Paulo University School of Medicine, São Paulo, Brazil ⁎ Address reprint requests to Alfredo I. Fiorelli, Rua Morgado de Mateus 126/81, São Paulo/SP, Brazil, CEP: 0415-050 Objective Arrhythmogenic right ventricular dysplasia (ARVD) is a myocardial disease of familiar, origin where the myocardium is replaced by fibrofatty tissue predominantly in the right ventricle. Herein we have presented the clinical courses of 4 patients with ARVD who underwent orthotopic heart transplantation. Patients and Methods Among 358 adult patients undergoing heart transplantation, 4 (1.1%) displayed ARVD. The main indication for transplantation was the progression to heart failure associated with arrhythmias. All 4 patients displayed rapid, severe courses leading to heart failure with left ventricular involvement and uncontrolled arrhythmias. Results In all cases the transplantation was performed using a bicaval technique with prophylactic tricuspid valve annuloplasty. One patient developed hyperacute rejection and infection, leading to death on the 7th day after surgery. The other 3 cases showed a good evolution with clinical remission of the symptoms. Pathological study of the explanted hearts confirmed the presence of the disease. Conclusions ARVD is a serious cardiomyopathy that can develop malignant arrhythmias, severe ventricular dysfunction with right ventricular predominance, and sudden cardiac death. Orthotopic heart transplantation must always be considered in advanced cases of ARVD with malignant arrhythmias or refractory congestive heart failure with or without uncontrolled arrhythmias, because it is the only way to remit the symptoms and the disease. Arrhythmogenic right ventricular dysplasia (ARVD) is a degenerative cardiomyopathy that replaces the myocardium with fibrofatty tissue. The myocardium becomes thin and shows dyskinetic delay in sinus stimulation conduction predisposing to development of arrhythmia reentry. The disease has a familiar character in 50% to 80% of cases with an autosomal dominant pattern of inheritance orbeit with incomplete and variable penetrance. 1 The right ventricular dysfunction generally precedes the left ventricular dysfunction with symptoms accompanying the disease evolution. The use of an implantable cardioverter defibrillator (ICD) is indicated in cases of serious arrhythmias refractory to conventional drug treatment. The natural evolution to long-term mortality is associated with the onset of congestive heart failure or complex arrhythmias. 2,3 In such cases, heart transplantation is the standard therapeutic method. 4 This study sought to present the clinical courses of 4 patients with ARVD who underwent cardiac transplantation for disease control. Case Reports In the period from 1985 to 2008, 358 heart transplantations were performed including 4 (1.1%) patients who had ARVD as the main feature (Table 1). Only 1 patient developed hyperacute rejection in the immediate postoperative period. He received steroid pulse therapy and plasmapheresis, but due to an infection he succumbed on the 7th day after surgery. The other 3 cases showed good evolutions with remission of symptoms. Pathological study of the explanted hearts confirmed the presence of the disease with a right ventricle massively dilated and a thin wall measuring 1 mm thickness. Some areas of the anterior right ventricular wall were replaced by fibrofatty tissue and devoid of muscle fibers. The subendocardial or transmural areas of both the ventricles and the septum were replaced by massive fatty and fibrous tissue infiltration. Discussion ARVD, first described in 1977 by Fontaine et al, 5 is a cardiomyopathy that primarily affects the right ventricle in a diffuse or spotty process with fibrofatty replacement of the myocardium most commonly between the anterior infundibulum, apex, and the outflow tract. This process leads to premature death; histological findings confirm the diagnosis of the definitive disease. The 2 greatest causes of death among patients with ARVC are severe ventricular arrhythmias and progressive heart failure. The disease frequently occurs in young people. Often the coronary angiography is normal. ARVC has been shown to be a genetic disorder in 30% to 50% of cases being transmitted as a dominant disorder, although the involved genes and the molecular basis are still unknown. Genes on 7 chromosomes have been identified as responsible for the disease. Most people are able to live normal lives with occasional episodes of abnormal heart rhythms. 1,6 The most common electrocardiographic abnormalities consist of T-wave inversion, slight ST segment elevation, and epsilon waves that can be detected in >30% of cases. Malignant arrhythmias refractory to conventional pharmacological treatment can appear with disease progression. They represent a risk factor for death and ICD must be considered. In this study, ICD occurred before heart transplantation in 75% of the patients, ARVD in 75% of the next-of-kin and sudden death in 50% indicating the gravity of the disease. 7 The biventricular dysfunction was present in all patients. Heart failure can occur due to progressive substitution of contractile tissue by fibrotic and fatty tissue. In end-stage disease, heart transplantation is the only way to change the natural evolution of and remit ARVD. 6,7 In conclusion, ARVD is a serious cardiomyopathy that may produce malignant arrhythmias, severe ventricular dysfunction with right ventricle predominance, and sudden cardiac death. Orthotopic heart transplantation must always be considered in cases of refractory congestive heart failure with or without uncontrolled arrhythmias; it is the only treatment that produces remission of the symptoms and the disease. References 1 E. Moric-Janiszewska G. Markiewicz-Loskot Review on the genetics of arrhythmogenic right ventricular dysplasia Europace 9 2007 259 2 R. Nijveldt A.M. Beek T. Germans Arrhythmogenic right ventricular cardiomyopathy with evidence of biventricular involvement CMAJ 176 2007 1819 3 T. Pezawas G. Stix J. Kastner Ventricular tachycardia in arrhythmogenic right ventricular dysplasia/cardiomyopathy: clinical presentation, risk stratification and results of long-term follow-up Int J Cardiol 107 2006 360 4 M. Yoda K. Minami D. Fritzsche Three cases of orthotopic heart transplantation for arrhythmogenic right ventricular cardiomyopathy Ann Thorac Surg 80 2005 2358 5 F.I. Marcus G. Fontaine Arrhythmogenic right ventricular dysplasia/cardiomyopathy: a review Pacing Clin Electrophysiol 18 1995 1298 6 E. Atik C.E. Rochitte L.F. Avila Prevailing right ventricular myocardiopathy for previous myocarditis or arrhythmogenic dysplasia? Arq Bras Cardiol 85 2005 68 7 W.J. McKenna G. Thiene A. Nava Diagnosis of arrhythmogenic right ventricular dysplasia/cardiomyopathy Task Force of the Working Group Myocardial and Pericardial Diseases of the European Society of Cardiology and of the Scientific Council on Cardiomyopathies of the International Society and Federation of Cardiology Br Heart J 71 1994 215 "
    },
    {
        "doc_title": "Indexing and retrieving DICOM data in disperse and unstructured archives",
        "doc_scopus_id": "63149146090",
        "doc_doi": "10.1007/s11548-008-0269-7",
        "doc_eid": "2-s2.0-63149146090",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Surgery",
                "area_abbreviation": "MEDI",
                "area_code": "2746"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Objective: This paper proposes an indexing and retrieval solution to gather information from distributed DICOM documents by allowing searches and access to the virtual data repository using a Google-like process. Methods and materials: The medical imaging modalities are becoming more powerful and less expensive. The result is the proliferation of equipment acquisition by imaging centers, including the small ones. With this dispersion of data, it is not easy to take advantage of all the information that can be retrieved from these studies. Furthermore, many of these small centers do not have large enough requirements to justify the acquisition of a traditional PACS. Results: A peer-to-peer PACS platform to index and query DICOM files over a set of distributed repositories that are logically viewed as a single federated unit. The solution is based on a public domain document-indexing engine and extends traditional PACS query and retrieval mechanisms. Conclusion: This proposal deals well with complex searching requirements, from a single desktop environment to distributed scenarios. The solution performance and robustness were demonstrated in trials. The characteristics of presented PACS platform make it particularly important for small institutions, including educational and research groups. © CARS 2008.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "GeneSplit-uma aplicação para o estudo de associações de codões e de aminoácidos em ORFeomas",
        "doc_scopus_id": "84893081260",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84893081260",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dynamic Service integration using web-based workflows",
        "doc_scopus_id": "70349114124",
        "doc_doi": "10.1145/1497308.1497426",
        "doc_eid": "2-s2.0-70349114124",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "AJAX",
            "Data integration",
            "Services composition",
            "SOA",
            "Workflow management"
        ],
        "doc_abstract": "Web services have been the main leverage to the development of Service Oriented Architecture (SOA), essentially a collection of interacting software agents with a loosely coupling organization. Despite several composition solutions already exist, the integration of services in a seamless and user-friendly way is not yet a complete solved problem. Most of the times, this integration is performed in hard-coded monolithic desktop applications. This paper presents a web-based application, supported on web2.0 principles, that was designed to integrate and coordinate services and data sources, simplifying knowledge extraction in diverse scientific areas. The system architecture follows an agile solution based on workflows, which may be dynamically designed by the user in a common browser. The framework allows building specific protocols for information retrieval enabling the user to access distributed resources in a single centralized interface. © 2008 ACM.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Alternative lossless compression algorithms in X-ray cardiac images",
        "doc_scopus_id": "60749130789",
        "doc_doi": null,
        "doc_eid": "2-s2.0-60749130789",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Cardiac images",
            "Digital imaging and communications in medicines",
            "Digital medical images",
            "Digital medical imaging systems",
            "Healthcare institutions",
            "Image compression algorithms",
            "Image datum",
            "Lossless",
            "Lossless compression algorithms",
            "Lossless compressions",
            "Medical datum",
            "Medical professions",
            "Medical studies",
            "Picture archiving",
            "Remote access",
            "Storage costs",
            "Storage efficiencies",
            "Time redundancies",
            "Transmission performance",
            "Transmission speed",
            "Workgroups"
        ],
        "doc_abstract": "Over the last decade, the use of digital medical imaging systems has increased greatly in healthcare institutions. Today, Picture Archiving and Communication System (PACS) is one of the most valuable tools supporting medical profession in both decision making and treatment procedures. It reduced the costs associated with the storage and management of image data and also increased both the intra and inter-institutional portability of data. One of the most important benefits of the digital medical image is that it allows the widespread sharing and remote access to medical data by outside institutions. PACS presents an opportunity to improve cooperative workgroups taking place either within or with other healthcare institutions. Storage and transmissions costs are continuously decreasing, but, as individual digital medical studies become significantly larger, further improvements on transmission performance and on storage efficiency are critical. Image compression algorithms offer the means to reduce storage cost and to increase transmission speed. Following previous methodologies [1], this paper provides a comparison about the application of DICOM (Digital Imaging and Communications in Medicine) lossless compression standards on Angiography cine acquisition images. A new lossless compression approach that exploits time redundancy between successive frames is also presented. Finally, the standards codecs values are compared with results obtained with the proposed method and with video lossless codecs. © 2008 Taylor & Francis Group,.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluating severe noise interference in IEEE 802.15.4 based location systems",
        "doc_scopus_id": "56349091855",
        "doc_doi": "10.1109/ETFA.2008.4638502",
        "doc_eid": "2-s2.0-56349091855",
        "doc_date": "2008-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Harsh environments",
            "IEEE 802.15.4",
            "Location systems",
            "Noise immunities",
            "Noise interferences",
            "Physical mechanisms",
            "Significant impacts"
        ],
        "doc_abstract": "This paper studies the impact of severe ISM noise interference in IEEE 802.15.4 based location systems, in particular the LOPES project. The architecture and operation of the location system are presented as well as the underneath physical mechanisms supporting the IEEE 802.15.4 noise immunity. A test-bed is proposed and the assessment scenario is described by focusing on the trial parameters. Results show that this technology is able to operate in extreme harsh environments. However, this has a significant impact in the location system's performance. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A microarray information database",
        "doc_scopus_id": "52449127769",
        "doc_doi": "10.1109/BIOTECHNO.2008.22",
        "doc_eid": "2-s2.0-52449127769",
        "doc_date": "2008-09-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Biocomputation",
            "Biomedical technologies",
            "Computational challenges",
            "Data-intensive",
            "Genomics",
            "International conferences",
            "LIMS",
            "Microarray experiments",
            "Microarray information",
            "Microarray technologies",
            "Microarrays technology",
            "Multi-step",
            "Open sources",
            "Processing methods",
            "Speed ups",
            "Web-based information systems"
        ],
        "doc_abstract": "The development of microarray technology has been phenomenal, during the past years, and it is becoming a daily tool in many genomics research laboratories. However, the multi-step and data-intensive nature of this technology has created an unprecedented computational challenge. In fact, the full power of microarrays technology can only be achieved if researchers are able to efficiently store, analyse and share their results. In this article we describe a flexible and extensible platform that can be used to gather the natural workflow of microarray experiments. It is an open-source web-based information system that also integrates a set of statistical and processing methods to analyse and visualize the data. By doing this integration in a single system, we hope to speed up the research and discovery processes. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploiting codon-triplets association for genome primary structure analysis",
        "doc_scopus_id": "52249109635",
        "doc_doi": "10.1109/BIOTECHNO.2008.21",
        "doc_eid": "2-s2.0-52249109635",
        "doc_date": "2008-09-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Biocomputation",
            "Biomedical technologies",
            "Comparative genomics",
            "Data mining",
            "Fine tuning",
            "Genetic coding",
            "International conferences",
            "Large-scale analysis",
            "Open reading frames",
            "Primary structure analysis",
            "Software applications",
            "tRNA binding"
        ],
        "doc_abstract": "The way evolution shapes the arrangement of synonymous codons within open reading frames (ORF) for fine tuning mRNA decoding efficiency is not yet understood. Since the ribosome has 3 tRNA binding sites, the context of triplets should be relevant to decoding fidelity and efficiency. We have developed a software application for large-scale analysis of codon-triplet associations to shed new light into this problem. The developed algorithms identify codon-triplets context biases, allowing for large scale comparative codon-triplet analysis and for identification of alterations to the standard genetic code. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design of microarray probes for detection of mutations",
        "doc_scopus_id": "52249107591",
        "doc_doi": "10.1109/BIOTECHNO.2008.39",
        "doc_eid": "2-s2.0-52249107591",
        "doc_date": "2008-09-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Biocomputation",
            "Biomedical technologies",
            "DNA hybridizations",
            "Fast methods",
            "Genetic mutations",
            "International conferences",
            "Low costs",
            "Mutation detection",
            "Probe design",
            "Single-nucleotide polymorphisms",
            "Time consuming"
        ],
        "doc_abstract": "As we gather knowledge about the relationship between genetic mutations and the predisposition to certain diseases, it becomes of importance to screen for single nucleotide polymorphisms (SNP) in entire populations, using low cost and fast methods. Microarrays are a recent technology characterized by the miniaturization of DNA hybridization sampling and the scalability of the monitored DNA sequences, making available the detection of genomic polymorphisms in a large scale. A draw back in this technology, however, is the large time consuming work associated with the design of probes for such propose, due to the several properties that all probes must have in common and, at same time, the uniqueness of each one. In this paper we present a computational solution to automate the process of probe design for the purpose of mutations detection, reducing the overall workflow time and increasing the accuracy of the final probes. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Studying the evolution of codon context in conserved gene sequences",
        "doc_scopus_id": "52249094482",
        "doc_doi": "10.1109/BIOTECHNO.2008.18",
        "doc_eid": "2-s2.0-52249094482",
        "doc_date": "2008-09-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Biocomputation",
            "Biomedical technologies",
            "CLUSTALW",
            "Codon context",
            "Codon usages",
            "Comparative genomics",
            "Context patterns",
            "Gene sequencing",
            "International conferences",
            "Multi-alignment",
            "Open reading frames",
            "Primary structures",
            "Public databases"
        ],
        "doc_abstract": "Several studies demonstrate that the codon context is an important characteristic in gene primary structure that modulates the translation of mRNA. To better understand these features we developed a software package that uses sequences of Open Reading Frames (ORFs) available in public databases and applies several statistic and visualization methodologies to unveil codon-context patterns, codon usage and others features. In this paper we describe a new method to study the evolution of codon contexts in conserved sequences between species. For this, we associate the codon-context statistic significance with BLASTP and ClustalW metrics. With these results it is possible to reveal important rules in the evolution of codon-pair context. © 2008 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Brokerage mechanism proposal for teleradiology studies distribution",
        "doc_scopus_id": "42949109165",
        "doc_doi": "10.1117/12.770753",
        "doc_eid": "2-s2.0-42949109165",
        "doc_date": "2008-05-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Teleradiology",
            "Workflow optimization"
        ],
        "doc_abstract": "The asymmetric distribution of PACS equipment and service providers across countries leads typically to the need to hire third party service professionals outside the institutions where the exams were made. In this paper we present a brokerage mechanism that puts customers and remote providers together in a seamless way. The proposed solution, asserted with a case study for the Portuguese national health system, addresses the problems that now impair the optimal provision of those services, enabling a more agile relationship between buyers and sellers, optimizing administrative work and complying with clinical and legal requirements under discussion in the European Union for the free movement of patients and professional health workers. In this document, the detailed process and technical description of the broker functioning is made, and the main benefits for the participants are also evaluated from a technical and economical point of view. Finally, in the discussion chapter, an assessment of the creation of a spot market for imaging studies is made and the integration with other similar markets is discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A model to optimize the use of imaging equipment and human skills scattered in very large geographical areas",
        "doc_scopus_id": "67650240159",
        "doc_doi": null,
        "doc_eid": "2-s2.0-67650240159",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "CSCW",
            "Health information systems",
            "Radiology information systems",
            "Teleradiology",
            "Ubiquitous systems"
        ],
        "doc_abstract": "Recent studies have shown that the good geographical coverage of Imagiologic Information Systems and equipment such as Picture Archiving and Communication Systems (PACS) is not matched by similar coverage levels of radiologists, especially in rural and academic health institutions. In this paper, we address this problem proposing a solution that is twofold, with the first one being process based, through the optimization of work assignment within pools of human resources according to Service Providers availability, and the second part being technology based, through the interconnection of all the health institutions PACS equipment and radiologists geographically dispersed. After describing the high level solution, we present some of the results of the implementation of this concept and some of the technical challenges still to overcome. Finally, the conclusion chapter presents the impact of the system in the involved institutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Codon-triplet context unveils unique features of the Candida albicans protein coding genome",
        "doc_scopus_id": "39149103111",
        "doc_doi": "10.1186/1471-2164-8-444",
        "doc_eid": "2-s2.0-39149103111",
        "doc_date": "2007-11-29",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biotechnology",
                "area_abbreviation": "BIOC",
                "area_code": "1305"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background: The evolutionary forces that determine the arrangement of synonymous codons within open reading frames and fine tune mRNA translation efficiency are not yet understood. In order to tackle this question we have carried out a large scale study of codon-triplet contexts in 11 fungal species to unravel associations or relationships between codons present at the ribosome A-, P- and E-sites during each decoding cycle. Results: Our analysis unveiled high bias within the context of codon-triplets, in particular strong preference for triplets of identical codons. We have also identified a surprisingly large number of codon-triplet combinations that vanished from fungal ORFeomes. Candida albicans exacerbated these features, showed an unbalanced tRNA population for decoding its pool of codons and used near-cognate decoding for a large set of codons, suggesting that unique evolutionary forces shaped the evolution of its ORFeome. Conclusion: We have developed bioinformatics tools for large-scale analysis of codon-triplet contexts. These algorithms identified codon-triplets context biases, allowed for large scale comparative codon-triplet analysis, and identified rules governing codon-triplet context. They could also detect alterations to the standard genetic code. © 2007 Moura et al; licensee BioMed Central Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Large scale comparative codon-pair context analysis unveils general rules that fine-tune evolution of mRNA primary structure",
        "doc_scopus_id": "39449087192",
        "doc_doi": "10.1371/journal.pone.0000847",
        "doc_eid": "2-s2.0-39449087192",
        "doc_date": "2007-09-05",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Background. Codon usage and codon-pair context are important gene primary structure features that influence mRNA decoding fidelity. In order to identify general rules that shape codon-pair context and minimize mRNA decoding error, we have carried out a large scale comparative codon-pair context analysis of 119 fully sequenced genomes. Methodologies/Principal Findings. We have developed mathematical and software tools for large scale comparative codon-pair context analysis. These methodologies unveiled general and species specific codon-pair context rules that govern evolution of mRNAs in the 3 domains of life. We show that evolution of bacterial and archeal mRNA primary structure is mainly dependent on constraints imposed by the translational machinery, while in eukaryotes DNA methylation and tri-nucleotide repeats impose strong biases on codon-pair context. Conclusions. The data highlight fundamental differences between prokaryotic and eukaryotic mRNA decoding rules, which are partially independent of codon usage. © 2007 Moura et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Current perspectives on PACS and a cardiology case study",
        "doc_scopus_id": "34250214449",
        "doc_doi": "10.1007/978-3-540-72375-2_5",
        "doc_eid": "2-s2.0-34250214449",
        "doc_date": "2007-06-18",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Since the first experiments on digital medical imaging, Pictures Archiving and Communication Systems (PACS) have been gaining acceptance along healthcare practitioners. PACS based infrastructures are currently being driven by powerful medical applications that rely completely on the seamless access to images' databases and related metadata. New and demanding applications such as study co-registration and content based retrieval are already driving PACS into new prominent roles. In this chapter we will revise the major key factors that have promoted this technology. We will then present our own solution for a Web-based PACS and the results achieved by its use on a Cardiology Department. We will finally consider future applications that are pushing developmental research in this field. © 2007 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Computational and statistical methodologies for ORFeome primary structure analysis",
        "doc_scopus_id": "36549060956",
        "doc_doi": "10.1385/1-59745-514-8:449",
        "doc_eid": "2-s2.0-36549060956",
        "doc_date": "2007-01-11",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Molecular Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1312"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Codon usage and context are biased in open reading frames (ORFs) of most genomes. Codon usage is largely influenced by biased genome G+C pressure, in particular in prokaryotes, but the general rules that govern the evolution of codon context remain largely elusive. To shed new light into this question, we have developed computational, statistical, and graphical tools for analysis of codon context on an ORFeome wide scale. Here, we describe these methodologies in detail and show how they can be used for analysis of ORFs of any genome sequenced. © Humana Press Inc.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A demanding Web-based PACS supported by Web Services technology",
        "doc_scopus_id": "33745410012",
        "doc_doi": "10.1117/12.653935",
        "doc_eid": "2-s2.0-33745410012",
        "doc_date": "2006-06-30",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "DICOM",
            "Encoding syntax",
            "PACS",
            "Web Services technology"
        ],
        "doc_abstract": "During the last years, the ubiquity of web interfaces have pushed practically all PACS suppliers to develop client applications in which clinical practitioners can receive and analyze medical images, using conventional personal computers and Web browsers. However, due to security and performance issues, the utilization of these software packages has been restricted to Intranets. Paradigmatically, one of the most important advantages of digital image systems is to simplify the widespread sharing and remote access of medical data between healthcare institutions. This paper analyses the traditional PACS drawbacks that contribute to their reduced usage in the Internet and describes a PACS based on Web Services technology that supports a customized DICOM encoding syntax and a specific compression scheme providing all historical patient data in a unique Web interface.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating medical and genomic data: A sucessful example for rare diseases",
        "doc_scopus_id": "39049181687",
        "doc_doi": null,
        "doc_eid": "2-s2.0-39049181687",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Bioinformatics database",
            "Genomic data",
            "Genomics",
            "Heterogeneous information",
            "Primary care",
            "Proteomics research",
            "Rare disease"
        ],
        "doc_abstract": "The recent advances on genomics and proteomics research bring up a significant grow on the information that is publicly available. However, navigating through genetic and bioinformatics databases can be a too complex and unproductive task for a primary care physician. In this paper we present diseasecard, a web portal for rare disease that provides transparently to the user a virtually integration of distributed and heterogeneous information. © 2006 Organizing Committee of MIE 2006.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A prospective study on the integration of microarray data in HIS/EPR",
        "doc_scopus_id": "34547470092",
        "doc_doi": "10.1007/11946465_21",
        "doc_eid": "2-s2.0-34547470092",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Clinical practice",
            "Genetic diagnosis field",
            "Health Information Systems (HIS)",
            "Human Genome Project"
        ],
        "doc_abstract": "The successful completion of the Human Genome Project promised an increase on our knowledge about the way our organism works and therefore would have a major impact in medicine. DNA microarray is one of the techniques that appeared in this \"-omic\" era and that will certainly change the way diagnosis and disease treatment are made. However, despite the successive scientific breakthroughs the integration of microarrays in clinical practice will face yet the lack of proper information systems and communication standards inside the Health Information Systems (HIS) scenarios. We hereby review current information systems for microarrays' laboratories and for healthcare institutions and also the latest integration efforts, assessing the shortcomings and structural difficulties derived from integrating two distinct fields. We also present the expected difficulties that may arise from the developments in the genetic diagnosis field and its interactions with other diagnostic areas such as imaging and/or radiology. From this prospective analysis we propose a model where the laboratorial microarray data can be integrated with other diagnostic systems in clinical environments, performing structured diagnostic workflows and integrating information from multiple diagnostic sources onto the HIS. © Springer-Verlag Berlin Heidelberg 2006.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Statistical, computational and visualization methodologies to unveil gene primary structure features",
        "doc_scopus_id": "33646827361",
        "doc_doi": "10.1055/s-0038-1634061",
        "doc_eid": "2-s2.0-33646827361",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Advanced and Specialized Nursing",
                "area_abbreviation": "NURS",
                "area_code": "2902"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Objectives: Gene sequence features such as codon bias, codon context, and codon expansion (e.g. trinucleotide repeats) can be better understood at the genomic scale level by combining statistical methodologies with advanced computer algorithms and data visualization through sophisticated graphical interfaces. This paper presents the ANACONDA system, a bioinformatics application for gene primary structure analysis. Methods: Codon usage tables using absolute metric and software for multivariate analysis of codon and amino acid usage are available in public databases. However, they do not provide easy computational and statistical tools to carry out detailed gene primary structure analysis on a genomic scale. We propose the usage of several statistical methods - contingency table analysis, residual analysis, multivariate analysis (cluster analysis) - to analyze the codon bias under various aspects (degree of association, contexts and clustering). Results: The developed solution is a software application that provides a user-guided analysis of codon sequences considering several contexts and codon usage on a genomic scale. The utilization of this tool in our molecular biology laboratory is focused on particular genomes, especially those from Sacchoromyces cerevisiae, Candida albicans Escherichia coli. In order to illustrate the applicability and output layouts of the software these species are herein used as examples. Conclusions: The statistical tools incorporated in the system are allowing to obtain global views of important sequence features. It is expected that the results obtained will permit identification of general rules that govern codon context and codon usage in any genome. Additionally, identification of genes containing expanded codons that arise as a consequence of erroneous DNA replication events will permit uncovering new genes associated with human disease. © 2006 Schattauer GmbH.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A quality of service approach for managing tele-medicine multimedia applications requirements",
        "doc_scopus_id": "28844464043",
        "doc_doi": null,
        "doc_eid": "2-s2.0-28844464043",
        "doc_date": "2005-12-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Central hospitals",
            "Design principles",
            "DiffServ",
            "Multimedia"
        ],
        "doc_abstract": "Research reported in this paper presents the design principles, the services and the proposed architecture for a QoS provisioning and management framework. An experimental implementation and the measurements evaluating the QoS provided using different DiffServ mechanisms are also discussed. The research reported here relates to InfraVIDA1 development scenario, a tele-medicine system that intends to allow remotely located health professionals to make tele-diagnosis and get second medical opinion from consultants on central hospitals. ©2004 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A WBEM based solution for a 4G network integrated management",
        "doc_scopus_id": "33845321730",
        "doc_doi": "10.1109/ICAS-ICNS.2005.14",
        "doc_eid": "2-s2.0-33845321730",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Network management",
            "Seamless integration",
            "Web Based Enterprise Management (WBEM)"
        ],
        "doc_abstract": "Next generation networks will put a new set of challenges related to operation and management, due to the increased complexity arising by the seamless integration of different kinds of technologies, services and terminals, and with the expected offered bandwidth. In this paper we present a Policy-based management system that is being developed inside the Daidalos IST project for such environments. This system uses Policy Based Management concepts associated with Web-Based Enterprise Management to control QoS aspects in this complex network. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Policy-based network management in an integrated mobile network",
        "doc_scopus_id": "33750972038",
        "doc_doi": "10.1109/AICT.2005.74",
        "doc_eid": "2-s2.0-33750972038",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Daidalos IST project",
            "Next generation networks",
            "Terminals"
        ],
        "doc_abstract": "Through the seamless integration of different kinds of technologies, services and terminals, and with the expected offered bandwidth, the next generation networks will put a new set of challenges related to operation and management. In this paper we present a Policy-based Network Management System that is being developed inside the Daidalos IST project. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integration of genetic and medical information through a web crawler system",
        "doc_scopus_id": "33745303774",
        "doc_doi": "10.1007/11573067_9",
        "doc_eid": "2-s2.0-33745303774",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bioinformatics",
            "Bioinformatics databases",
            "Clinical practice",
            "Medical information"
        ],
        "doc_abstract": "The huge amount of information coming from genomics and proteomics research is expected to give rise to a new clinical practice, where diagnosis and treatments will be supported by information at the molecular level. However, navigating through bioinformatics databases can be a too complex and unproductive task. In this paper we present an information retrieval engine that is being used to gather and join information about rare diseases, from the phenotype to the geno-type, in a public web portal - diseasecard.org. © Springer-Verlag Berlin Heidelberg 2005.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface",
        "doc_scopus_id": "33745290245",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33745290245",
        "doc_date": "2005-12-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Executable graphics for PBNM",
        "doc_scopus_id": "33744787101",
        "doc_doi": "10.1007/11567486_12",
        "doc_eid": "2-s2.0-33744787101",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Network management policy",
            "Policy language",
            "Visual language"
        ],
        "doc_abstract": "The specification of a policy is performed in a policy language, usually following a textual representation. However, humans process images faster than text and they are prepared to process information presented in two or more dimensions: sometimes it is easier to explain things using figures and their graphical relations than writing textual representations. This paper describes a visual language, in the form of graphics that are executed in a networking environment, to define a network management policy. This approach allows to map visual tokens and corresponding arrangements into other languages to which a mapping is defined. © Springer-Verlag Berlin Heidelberg 2005.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Architecture evaluation for the implementation of a regional integrated electronic health record",
        "doc_scopus_id": "84886915633",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84886915633",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Architecture evaluation",
            "Communication infrastructure",
            "Health care information system",
            "Health information systems",
            "Health planning",
            "Health systems",
            "Medical record",
            "Organizational perspectives",
            "Communication",
            "Electronic Health Records",
            "Health Information Systems",
            "Medical Records Systems, Computerized",
            "Systems Integration"
        ],
        "doc_abstract": "The interconnection between different healthcare information systems is not yet a trivial task. Solid communication infrastructures do exist, solid solutions for Health Information Systems (HIS) are installed, but, unfortunately, too many different solutions hinder the integration of HIS at a regional, national or European level. In this paper we propose a solution and an implementation of an Integrated Electronic Health Record that is a composition of two distinct integration models - centralized and distributed. We exploit this solution against a set of predefined users and institutional requirements, at a regional level in two Portuguese regions. As a conclusion, an evaluation of the cost and inherent benefits is made involving the clinical, economical and organizational perspectives.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Data management and visualization issues in a fully digital echocardiography laboratory",
        "doc_scopus_id": "33745324967",
        "doc_doi": "10.1007/11573067_2",
        "doc_eid": "2-s2.0-33745324967",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Digital echocardiography laboratory",
            "Imaging modality",
            "Ultrasound sequences",
            "Visualization issues"
        ],
        "doc_abstract": "This paper presents a PACS solution for echocardiography laboratories, denominated as Himage, that provides a cost-efficient digital archive, and enables the acquisition, storage, transmission and visualization of DICOM cardiovascular ultrasound sequences. The core of our approach is the implementation of a DICOM private transfer syntax designed to support any video encoder installed on the operating system. This structure provides great flexibility concerning the selection of an encoder that best suits the specifics of a particular imaging modality or working scenario. The major advantage of the proposed system stems from the high compression rate achieved by video encoding the ultrasound sequences at a proven diagnostic quality. This highly efficient encoding process ensures full online availability of the ultrasound studies and, at the same time, enables medical data transmission over low-bandwidth channels that are often encountered in long range telemedicine sessions. We herein propose an imaging solution that embeds a Web framework with a set of DICOM services for image visualization and manipulation, which, so far, have been traditionally restricted to intranet environments. © Springer-Verlag Berlin Heidelberg 2005.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparative context analysis of codon pairs on an ORFeome scale.",
        "doc_scopus_id": "33644889434",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33644889434",
        "doc_date": "2005-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Ecology, Evolution, Behavior and Systematics",
                "area_abbreviation": "AGRI",
                "area_code": "1105"
            },
            {
                "area_name": "Genetics",
                "area_abbreviation": "BIOC",
                "area_code": "1311"
            },
            {
                "area_name": "Cell Biology",
                "area_abbreviation": "BIOC",
                "area_code": "1307"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Codon context is an important feature of gene primary structure that modulates mRNA decoding accuracy. We have developed an analytical software package and a graphical interface for comparative codon context analysis of all the open reading frames in a genome (the ORFeome). Using the complete ORFeome sequences of Saccharomyces cerevisiae, Schizosaccharomyces pombe, Candida albicans and Escherichia coli, we show that this methodology permits large-scale codon context comparisons and provides new insight on the rules that govern the evolution of codon-pair context.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Active traffic monitoring for heterogeneous environments",
        "doc_scopus_id": "24644456982",
        "doc_doi": "10.1007/978-3-540-31956-6_71",
        "doc_eid": "2-s2.0-24644456982",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "IP networks",
            "Network traffic management",
            "One-way active measurement protocol (OWAMP)",
            "Wireless LAN"
        ],
        "doc_abstract": "The traffic management of IP networks faces increasing challenges, due to the occurrence of sudden and deep traffic variations in the network, which can be mainly attributed to the large diversity of supported applications and services, to the drastic differences in user behaviors, and to the complexity of traffic generation and control mechanisms. In this context, active traffic measurements are particularly important since they allow characterizing essential aspects of network operations, namely the quality of service measured in terms of packet delays and losses. The main goal of the work presented in this paper is the performance characterization of operational networks consisting in heterogeneous environments including both wired and wireless LANs, using active measurements. We propose a measurement methodology and its corresponding measurement platform. The measurement methodology is based on the One-Way Active Measurement Protocol (OWAMP), a recent proposal from the Intemet2 and IETF IPPM groups for measuring delays and losses in a single direction. The measurement platform was implemented, tested and conveniently validated in different network scenarios. © Springer-Verlag Berlin Heidelberg 2005.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Grid requirements for the integration of biomedical information resources for health applications",
        "doc_scopus_id": "20344383766",
        "doc_doi": "10.1055/s-0038-1633938",
        "doc_eid": "2-s2.0-20344383766",
        "doc_date": "2005-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Advanced and Specialized Nursing",
                "area_abbreviation": "NURS",
                "area_code": "2902"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Objectives: The goal of this paper is to identify how Grid technology can be applied for the development and deployment of integration systems, bringing together distributed and heterogeneous biomedical information sources for medical applications. Methods: The integration of new genetic and medicol knowledge in clinical workflows requires the development of new paradigms for information management in which the ability to access and relote disparate data sources is essential. We adopt a requirements perspective based on the user needs we have identified in the development of the INFOGENMED system to assess current Grid technology against those requirements. Results: The gap between Grid features and distributed biomedicol information integration needs is characterized. Results from prospective studies are also reported. Conclusions: Grid infrastructures offer advanced features for the deployment of collaborative computational environments across virtual organizations. New Grid developments are in line with the problem of multiple site information integration. From the INFOGENMED point of view, Grid infrastructures need to evolve to implement structured data access services and semantic content description and discovery. © 2005 Schattauer GmbH.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A quality of service framework for tele-medicine applications",
        "doc_scopus_id": "15844412226",
        "doc_doi": "10.1109/WEBMED.2004.1348140",
        "doc_eid": "2-s2.0-15844412226",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "DiffServ mechanisms",
            "Frameworks",
            "Packet loss",
            "Tele-medicine applications"
        ],
        "doc_abstract": "Research reported in this paper presents the design principles, the services and the proposed architecture for a QoS provisioning and management framework. An experimental implementation and the measurements evaluating the QoS provided using different DiffServ mechanisms are also discussed. The research reported here relates to InfraVIDA1 development scenario, a tele-medicine system that intends to allow remotely located health professionals to make tele-diagnosis and get second medical opinion from consultants on central hospitals. © 2004 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Himage PACS: A new approach to storage, integration and distribution of cardiologie images",
        "doc_scopus_id": "12144281711",
        "doc_doi": "10.1117/12.534135",
        "doc_eid": "2-s2.0-12144281711",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Cardiology images",
            "DICOM",
            "HIS",
            "PACS",
            "Patient data integration"
        ],
        "doc_abstract": "This paper presents a Cardiology oriented information system that provides permanent availability of all clinical history, including alphanumeric and image data, with time and cost-effective transmission (reduced download time), without loss of image diagnosis quality and based on a Web Multimedia Integrated Access Interface. This implies the integration of HIS and PACS in a unique access interface, providing on-line and fast access to authorized healthcare professionals. The benefits obtained from the HIS-PACS integration and from the availability of all historical patient data are unquestionable to practitioners but also to the patients. Moreover, the system includes a telematic platform capable of establishing cooperative telemedicine sessions where our most impressive utilization is a transcontinental work platform for cardiovascular ultrasound. The key point of our approach starts with the construction of a DICOM private transfer syntax that is prepared to support any video encoder installed on a Windows-based station. With this structure it is possible to select the best encoder to a specific modality and work scenario. Good trade-off between compression ratio and diagnostic quality, low network traffic load, backup facilities and data portability are other achievements of this system.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Solving transactions control in current management frameworks",
        "doc_scopus_id": "8444246335",
        "doc_doi": null,
        "doc_eid": "2-s2.0-8444246335",
        "doc_date": "2004-11-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Network management",
            "Policies",
            "Policy based management",
            "Transactional control"
        ],
        "doc_abstract": "Policy Based Network Management has been presented as a paradigm for efficient and customisable management systems. The IETF has provided a framework to describe the concept but some aspects still open like transactional control. In fact transactional control mechanisms are receiving today great attention in the scope of network management. In here, we identify the lacks of current management paradigms concerning transactional control and we propose a policy-based network management system that allows specify operations over aggregations of agents and that provides high-level atomic transactions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "E-services in mission-critical organizations: Identification enforcement",
        "doc_scopus_id": "8444230475",
        "doc_doi": null,
        "doc_eid": "2-s2.0-8444230475",
        "doc_date": "2004-11-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Authentication and identification",
            "Digital credentials",
            "E-sevices security",
            "Intranet scenarios"
        ],
        "doc_abstract": "The increasing dependency of enterprise on IT has rise up major concerns on security technology and procedures. Access control mechanisms, which are the core of most security policies, are mostly based on PIN and, some times, in Public Key Cryptography (PKC). Despite these techniques can be already broadly disseminated, the storage and retrieval of security secrets is yet a sensitive and open issue for organization and users. One possible solution can be provided by the utilization of smart cards to store digital certificates and private keys. However, there are special organizations where even this solution does not solve the security problems. When users deal with sensible data and it is mandatory to prevent the delegation of access privileges to third persons new solutions must be provided. In this case the access to the secrets can be enforced by a three-factor scheme: the possession of the token, the knowledge of a PIN code and the fingerprint validation. This paper presents a Professional Information Card system that dynamically combines biometrics with PKC technology to assure a stronger authentication that can be used indistinctly in Internet and Intranet scenarios. The system was designed to fulfill current mission-critical enterprises access control requirements, and was deployed, as a proof of concept, in a Healthcare Information System of a major Portuguese Hospital.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A business process model for public health information systems: A governmental perspective",
        "doc_scopus_id": "8444222741",
        "doc_doi": null,
        "doc_eid": "2-s2.0-8444222741",
        "doc_date": "2004-11-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Information systems",
            "Integration tools"
        ],
        "doc_abstract": "The business process models available for the telecom industry have, in the recent past, made significant developments and reached leading-edge maturity levels. The 1997-2000 technology bubble has injected significant amounts of cash in the market, which has allowed a quick maturing of both the process models and its supporting software applications and integration tools. In its turn, the health industry, in what concerns to technology and associate processes has been maturing more slowly, with lower levels of integration and process models more \"institution oriented\" than \"client oriented\". In this paper it is proposed a process model for the health industry, derived from the enhanced Telecom Operations Model (eTOM), from which is derived a functional architecture that intends to support applications that respond to the current technological, political and economical challenges of the Portuguese national health service.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Diseasecard: A web-based tool for the collaborative integration of genetic and medical information",
        "doc_scopus_id": "35048821032",
        "doc_doi": "10.1007/978-3-540-30547-7_41",
        "doc_eid": "2-s2.0-35048821032",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bioinformatics database",
            "Biomedicine",
            "Collaborative integration",
            "Collaborative services",
            "Genetic disease",
            "Medical information",
            "Proteomics research",
            "User friendly interface"
        ],
        "doc_abstract": "The recent advances on genomics and proteomics research bring up a significant grow on the information that is publicly available. However, navigating through genetic and bioinformatics databases can be a too complex and unproductive task for a primary care physician. Moreover, considering the rare genetic diseases field, we verify that the knowledge about a specific disease is commonly disseminated over a small group of experts. The capture, maintenance and sharing of this knowledge over user-friendly interfaces will introduce new insights in the understanding of some rare genetic diseases. In this paper we present DiseaseCard, a web available collaborative service that aims to integrate and disseminate genetic and medical information on rare genetic diseases. © Springer-Verlag Berlin Heidelberg 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "NeoScreen: A software application for MS/MS newborn screening analysis",
        "doc_scopus_id": "35048821027",
        "doc_doi": "10.1007/978-3-540-30547-7_45",
        "doc_eid": "2-s2.0-35048821027",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Amount of information",
            "Dried blood spots",
            "Metabolic disorders",
            "MS/MS",
            "Newborn screening",
            "Software applications",
            "Software systems",
            "Tandem mass spectrometry"
        ],
        "doc_abstract": "The introduction of the Tandem Mass Spectrometry (MS/MS), in neonatal screening laboratories, has opened the doors to innovative newborn screening analysis. With this technology the number of metabolic disorders, that can be detected, from dried blood-spot species, increases significantly. However, the amount of information obtained with this technique and the pressure for quick and accurate diagnostics raises serious difficulties in the daily data analysis. To face this challenge we developed a software system, Neo-Screen, which simplifies and allow speeding up newborn screening diagnostics.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An integrated access interface to multimedia EPR",
        "doc_scopus_id": "79952568682",
        "doc_doi": "10.1016/S0531-5131(03)00258-9",
        "doc_eid": "2-s2.0-79952568682",
        "doc_date": "2003-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This paper describes a multi-platform interface system that provides integrated access to a distributed Electronic Patient Record (EPR). Information associated with this kind of EPR may be stored either at the wide level of the Healthcare Information Systems (HIS) or at a more specific instance such as a departmental PACS. The system is supported by strong authentication and access control mechanisms, ensuring privacy, confidentiality and no-repudiation of clinical acts, providing the adequate privileges in the patient data handling. © 2003, Elsevier Science B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272802 291210 291682 291735 291857 291889 291929 31 International Congress Series INTERNATIONALCONGRESSSERIES 2003-05-28 2003-05-28 2010-11-13T17:30:03 S0531-5131(03)00258-9 S0531513103002589 10.1016/S0531-5131(03)00258-9 S300 S300.2 FULL-TEXT 2015-05-14T08:23:58.562302-04:00 0 0 20030601 20030630 2003 2003-05-28T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confdate confeditor confloc contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings suppl vol volfirst volissue figure body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0531-5131 05315131 1256 1256 C Volume 1256 145 880 886 880 886 200306 June 2003 2003-06-01 2003-06-30 2003 CARS 2003. Computer Assisted Radiology and Surgery. Proceedings of the 17th International Congress and Exhibition London, UK 25–28 June 2003 20030625 20030628 H. U. Lemke, M.W. Vannier, K. Inamura, A.G. Farman, K. Doi and J. H.C. Reiber converted-article fla Copyright © 2003 Published by Elsevier B.V. INTEGRATEDACCESSINTERFACEMULTIMEDIAEPR COSTA C 1 Introduction 2 Material and methods 2.1 Scenario 2.2 Departmental PACS 2.3 HIS and PACS integration 2.4 Security enforcement 3 Results 4 Discussion References TSIKNAKIS 2002 3 26 M COSTA 2003 C MIE2003PROCEEDING ANEWCONCEPTFORINTEGRATEDHEALTHCAREACCESSMODEL SILVA 1998 A SOBLE 1997 192 197 J DIGITALCARDIACIMAGINGIN21STCENTURYAPRIMER MPEGDIGITALVIDEOCOMPRESSION MENEZES 1996 A HANDBOOKAPPLIEDCRYPTOGRAPHY COSTA 2002 460 462 C TECHNOLOGYHEALTHCARE ATRANSCONTINENTALTELEMEDICINEPLATFORMFORCARDIOVASCULARULTRASOUND COSTA 2002 462 464 C TECHNOLOGYHEALTHCARE AMULTISERVICEPATIENTDATACARD CIMINO 2002 113 127 J COSTAX2003X880 COSTAX2003X880X886 COSTAX2003X880XC COSTAX2003X880X886XC item S0531-5131(03)00258-9 S0531513103002589 10.1016/S0531-5131(03)00258-9 272802 2010-12-19T22:39:48.374939-05:00 2003-06-01 2003-06-30 true 175018 MAIN 7 47550 849 656 IMAGE-WEB-PDF 1 gr1 83408 532 831 gr1 7044 140 219 ICS 2036 S0531-5131(03)00258-9 10.1016/S0531-5131(03)00258-9 Fig. 1 HIS and PACS integration to provide a multimedia EPR. An integrated access interface to multimedia EPR Carlos Costa * a José L Oliveira a Augusto Silva a Vasco Gama b a DET/IEETA, Aveiro University, 3810-193 Aveiro, Portugal b Centro Hospitalar de Vila Nova de Gaia, 4434-502 V.N. Gaia, Portugal * Corresponding author. Tel.: +351-234370500; fax: +351-234370545. This paper describes a multi-platform interface system that provides integrated access to a distributed Electronic Patient Record (EPR). Information associated with this kind of EPR may be stored either at the wide level of the Healthcare Information Systems (HIS) or at a more specific instance such as a departmental PACS. The system is supported by strong authentication and access control mechanisms, ensuring privacy, confidentiality and no-repudiation of clinical acts, providing the adequate privileges in the patient data handling. Keywords EPR HIS PACS Security Access control 1 Introduction A macroscopic analysis of a clinical process reveals that patient data is often generated, manipulated and stored throughout several institutions where the patients are treated or followed. These health institutions can be public or private, national or international, small or wide scope (hospitals, health centers, doctor's offices). In this heterogeneous and complex scenario, the sharing and the remote access to patient information is of capital importance for nowadays-healthcare best practices. The first step towards this sharing was already done by the worldwide trend for the adoption of digital processes and of the Electronic Patient Record (EPR) concept. EPR is a means to manage and integrate all types of clinical data. The information is collected, archived and distributed, introducing automated methods for traditional medical evidence recording. Above all, EPR represents the core element to accomplish new healthcare services with improved quality and efficiency. One of the major problems related with this concept is that we are often dealing with highly heterogeneous, autonomous and distributed medical systems, making very difficult the interoperability task. In this scenario we believe that on a near future healthcare will rapidly take full advantage of Web technology and Internet to allow controlled widespread sharing of medical data between healthcare intervenient. 2 Material and methods World globalization is increasingly promoting peoples' mobility. This fact, associated to the actual healthcare services trend to the creation of specialized diagnostic and therapeutic centers are creating a higher dispersion of patient clinical records. The above reality is forcing even more the healthcare providers to implement telematic infrastructures in order to promote the sharing and remote access to patient clinical data. Moreover, it is not only the dispersion of data that is growing up, but also the number of diagnostic and routine appointments (with production of information) between citizen and practitioners. The result is that one citizen, during his lifetime, is likely to produce a tremendous quantity of clinical information, stored in distinct institutions often supported by heterogeneous information systems. Within such scenario, the need for a unique and integrated access to all distributed patient data is demanded and fundamental for supporting continuous care and improved clinical services [1]. The herein described Integrated Multimedia EPR (IM-EPR) combines different media provided from specialized sub-systems and infrastructures. This integration inside institutions' domains will drive more easily a second level of integration at a national or international scale. Besides, we defend that the integration within a wide scope scenario must contemplate other integration elements as well as a new conceptual model [2]. The political and administrative awareness of the necessity of infrastructures that can provide all the types of clinical data in an integrated way are growing up and are identified as a fundamental key in care services for the upcoming years. We claim that a greater objective, like a truly uniform interface that could provide a regional/national access to all the historical patient data in an integrated way, must start with small and solid steps over the institution islands domains and progressively implement and aggregate new items over the time. Our first step, concerning this objective, was the integration of Healthcare Information System (HIS) and PACS in a unique Web interface, providing on-line and real time access to the authorized healthcare professional and enabling the delivery of time-efficient services with enhanced quality. To address the described motivations and objectives, several functional requirements were identified: – General usage of Web-based interfaces. – Quality of Service (QoS) handling for real time retrieval of remote information without compromising service reliability. – Transparent access to all the historic clinical data of a patient. – Select and retrieve facilities. – Reducing or elimination, of possible, redundant exams and procedures, with a direct impact on service costs and some times life risk. – Implementation of strong and efficient authentication mechanisms and the enforcement of identification procedures by practitioners. – Promoting collaborative research, tele-work and telemedicine. 2.1 Scenario The functional model that is described in this paper is being developed upon the CHVNG Cardiology Department information systems (HIS and PACS). This unity gathers 45,000 clinical patient records and two digital laboratories: a Cardiac Catherization Laboratory (cathlab) with 2500 procedures/year, and a Cardiovascular Laboratory (echolab) with 5000 procedures/year, both supported by a mature PACS system. Inside the departmental network it is possible to access, in a modular but integrated way, the conventional HIS alphanumeric information and also the patients images (still/cine-loops) available in DICOM format. The first PACS system [3] was implemented in 1997 initially covering the cathlab unity. In the beginning of 2001, it was extended to support the echolab through the creation and implementation of a DICOM private syntax (MPEG4 based). This approach enables the storage and online visualization of all the released exams. 2.2 Departmental PACS The production of medical images has been blowing up in the healthcare institutions, representing at this moment an important element to support the medical decision and imposing new difficulties concerning the storage volume, their management and the network infrastructure to support their distribution. The two heavily loaded departmental digital laboratories are producing medical image modalities (XA and US) that demand a huge storage capacity as well communications channel bandwidth. The data rate and volume associated with typical studies pose several problems in the design and deployment of systems contemplating the acquisition, archiving, processing, visualization and transmission functionalities. Our echolab work-reality tell us that, for example, an optimised time-acquisition and uncompressed echocardiogram study can typically produce a volume of 110 Mbytes of information when considering the storage of 10 colour cine-loop (RGB-24 bits/pixel) with 15 frame (≈1 cardiac cycle), and a sampling matrix (480×512). Concerning the cathlab the average study size, for greyscale images (8 bits/pixel) with a 512×512 matrix, is about 280 Mbytes in their uncompressed form. In the above scenario, the digital video compression is a crucial technology when considering key issues like storage and transmission times. Angiographies studies are directly sent to the DICOM storage server by the cathlab equipment. Here these raw data images are compressed and stored in a JPEG (quality factor 95) DICOM format and the PACS system database is updated with this new exam. The echolab is mainly based on multi-vendor echocardiography machines with standard DICOM output interface, storage-processing server unities and review stations. Given the significant time–space redundancy that characterizes this type of cardiovascular video signal there are important gains by choosing a compression methodology that copes well with both intra-frame and inter-frame redundancies [4]. The novelty of our approach starts by embedding each storage server with highly efficient MPEG4 encoding software. Since MPEG4 is not a native DICOM coding standard, subsequent image transmission, decoding and reviewing is accomplished through a DICOM private storage and transfer syntax mechanisms enabled between storage servers and review clients, making use of in house developed software (EcoHimage). In order to achieve maximum compliance with the standard, all the other DICOM information elements were kept unchanged. The original raw data ultrasounds are saved and kept online during 6 months and the DICOM private syntax sequences are made available permanently. 2.3 HIS and PACS integration The implementation of the proposed multi-platform IM-EPR (Fig. 1) starts with the development of a Web-based interface module to the department HIS. It is based on XML/XSL technology for dynamic content creation and formatting, according to the user terminal and to the access privileges of different user profiles. The implemented model was designed to collaborate with the existing HIS system (no Web-based) and to be easily adjustable to different types of applications and client platforms. The interface usability was a major functionality in the analysis and implementation phases, it was necessary to conjugate the high accuracy of clinical information representation and the capacities of visualization and interaction of the different types of terminals (mobile phones, PDA, desktops) since we aim to create a flexible interface to distinct access proveniences (indoor/outdoor) and client interfaces. The developed multi-platform interface integrates in run time the patient information retrieved from the HIS system with PACS system images, making these alphanumeric and multimedia data available in an Internet browser. More concretely when, for example, clinical information is retrieved from the HIS database, the eventual existence of associated DICOM images is checked in the department PACS. Since we have a substantial part of PACS images in DICOM-MPEG4 private syntax format (the ultrasounds), a particular software engine extracts the MPEG4 encoded image data from the DICOM and encapsulates it in an AVI file format. If the PACS image modality is not on this format the engine processes the standard DICOM file (RAW, JPEG) and return the AVI encoded file. The option by the Audio/Video Interleaved (AVI) video format was based in the following aspects: – It is the most used multimedia format in the Internet space. – It supports distinct information encoders, since uncompressed information until optimised MPEG4 encoded information. In this way, it is possible to make distinct compression adjustments to specific medical images modalities. – Our echolab is actually supporting a specially constructed DICOM private transfer syntax, based on MPEG4, easily exportable to an encapsulated AVI file format. – The small image file-size reduces the waiting time to download and display of images. – It is simple to integrate with other information to be displayed in the Web environment. – It does not need any specific client viewer, including mobile equipments browsers' case. 2.4 Security enforcement Security and access control mechanisms are fundamental aspects for the development and wide use of healthcare information services. In most of the current commercial transactions, the security assurance is typically provider-oriented, i.e. the service provider just imposes its security police to the user that have to assure that its own secrets are preserved. Although for most of the scenarios this solution might by acceptable, since the disruption of secrets only affects the owner–provider relation, there are situations where the loss of privacy and data integrity involves third persons—the patient in our scenario. The communication privacy can be resolved with the adoption of protocols like the HTTPS to encrypt the data transferred between server and clients (we are using SSL, 128 bits, in our server). However, concerning access control the problem is a bit more complex. To cope with this we have developed an innovative Healthcare Professional Card (HPC) that integrates strong user authentication based in smart cards and Public Key Cryptography (PKC) technology. The confidence on security issues depends strongly on the trust we have on digital certificates, on private key storage and how it is verified that the correct person is the owner of the private key [5]. Contemplating these demands, the HPC is hosted by a PKC smart card that includes card owner verification procedures. The first identity proof is related with the user card possession. However, the local and remote authentication is made through the professional private key that signs a host-side challenge and proof the user identity. The user private key is unique and securely stored on the card, protected by a PIN or biometric device. The implemented access policy comprises the type of source (indoor/outdoor) as well the client interface and the type of authentication and identification provided, often limited by the technological client platform constraints. The HPC model, oriented to the authentication, was delineated to work in a dual mode scenario. Inside the institution, the user must provide the token password to gain access to the private key (Fig. 1). In outside access, it is additionally demanded the presence of a biometric recognition element. This decision was based on the observation that inside the institution domain the identity control of users is associated to their physical presence that simplifies the authentication procedure. In outdoor access, we assume that the physical control factor should be replaced or at least attenuated by the inclusion of biometric mechanisms. This approach imposes the idea of not allowing delegation of access permission to third persons and it results on a strongest identification method, making possible not only the users indoor access to HIS and PACS but also the department professionals' remote access. 3 Results The use of MPEG4 encoding represents a cost-effective solution to store and transmit a patient study; in special it provides excellent results with ultrasounds image modality. With more than 3000 studies performed in 9 months in the echolab, we observe that file sizes of 15 to 30 frames cine-loops with Doppler colour coded information, rarely exceed 200 kbytes. Moreover, an excellent diagnostic quality was achieved as a clinical validation has been successfully carried out. The usage of MPEG4 as coding standard for multimedia data appears to be a good alternative for cost-effective storage and transmission of digital sequences in the intranet PACS system but also permits the presented integration with HIS information in a Web-based interface. On other hand it reduces the transfer time over the network, which is a very critical issue when we are dealing with costly or low bandwidth connections [6]. Moreover, the presented model is compliant with a previous proposed Multi-Service Patient Data Card [7] developed to provide a set of functionalities that allow the management of structured links to distributed EPR stored in distinct healthcare providers. 4 Discussion The benefits obtained with the availability of all historical patient data in a simple, fast and integrated interface are unquestionable to practitioners and to patients, and is likely to induce a significant improvement in the overall healthcare services quality. The development of this integrating interface also creates conditions to, in a near future, citizens gain access to their clinical data aiming to improve the knowledge of their own health conditions [8]. The presented multi-platform system represents a cost-efficient solution that provides a unique access portal to healthcare clinical information and implements a flexible access model over open and heterogeneous environments as the Internet. Strong securities enforcements and user transparent utilization are other achievements of the proposal. References [1] M. Tsiknakis D.G. Katehakis S.C. Orphanoudakis An open, component-based information infrastructure for integrated health information networks International Journal of Medical Informatics 68 1–3 2002 3 26 [2] C. Costa A new concept for an integrated healthcare access model MIE 2003 Proceeding 2003 IOS Press St. Malo, France [3] A. Silva A cardiology oriented PACS SPIE Proceedings 1998 (San Diego, USA) [4] J.S. Soble MPEG digital video compression Digital Cardiac Imaging in the 21st Century: A Primer 1997 192 197 [5] A.J. Menezes P.C. Oorschot S.A. Vanstone Handbook of Applied Cryptography 1996 CRC Press [6] C. Costa A transcontinental telemedicine platform for cardiovascular ultrasound Technology and Health Care vol. 10(6) 2002 IOS Press 460 462 [7] C. Costa A multi-service patient data card Technology and Health Care vol. 10(6) 2002 IOS Press 462 464 [8] J.J. Cimino V.L. Patel A.W. Kushniruk The patient clinical information system (PatCIS): technical solutions for and experience with giving patients access to their electronic medical records International Journal of Medical Informatics 68 2–3 2002 113 127 "
    },
    {
        "doc_title": "Application scenarios for distributed management using SNMP expressions",
        "doc_scopus_id": "84910090329",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84910090329",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Application scenario",
            "DISMAN",
            "Distributed management",
            "Expression MIB",
            "Management information",
            "SNMP",
            "Solid works",
            "Workgroups"
        ],
        "doc_abstract": "Management distribution is an, we can say, old topic in terms of the number of proposed solutions and publications. Recently, the DISMAN workgroup suggested a set of MIB modules to address this matter in the context of SNMP. One of the DISMAN modules has the capability of using expressions to perform decentralized processing of management information - the Expression MIB. Although existing for some time now, its capabilities are not very well known. In fact, other DISMAN MIBs, such as the Schedule MIB and the Script MIB already got some attention in several papers and are target of very solid work. There are hardly any papers describing the Expression MIB and its functionality. This paper contributes to eliminate this absence by describing our implementation effort around it as well as some real world applications for it.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Policies composition through graphical components",
        "doc_scopus_id": "84910089998",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84910089998",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Business Process",
            "Component-oriented",
            "Graphical interface",
            "Internetworking",
            "Management functionality",
            "Policy based management",
            "Policy rules"
        ],
        "doc_abstract": "Policy based management have gained a crescent importance in the two last years. New demands on internetworking, on services specification, on QoS achievement and generically on network management functionality, have driven this paradigm to a very important level. The main idea is to provide services that allow specifying management and operational rules in the same way people do business. Despite the main focus of this technology has been associated with network management solutions, its generality allows to extend these principles to any business process inside an organization. In this paper we discuss the main proposals in the field, namely the IETF/DMTF model, and we present a proposal that allows the specification of policy rules through a user-friendly and component-oriented graphical interface.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Agentapi: An API for the development of Managed agents",
        "doc_scopus_id": "84910074428",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84910074428",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Access methods",
            "Automatic codes",
            "Complex task",
            "Management applications",
            "Management information",
            "Multiprotocols",
            "Programming experience",
            "SNMP"
        ],
        "doc_abstract": "Managed agents, namely SNMP agents, costs too much to develop, test and maintain. Although assuming simplicity since its origins, the SNMP model has several intrinsic aspects that make the development of management applications a complex task. However, there are tools available which intend to simplify this process by generating automatic code based on the management information definition. Unfortunately, these tools are usually complicated to use and require a strong background of programming experience and network management knowledge. This paper describes an API for managed agent development which also provides multiprotocol capabilities. Without changing the code, the resulting agent can be managed by SNMP, web browsers, wap browsers, CORBA or any other access method either simultaneously or individually.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new user-oriented model to manage multiple digital credentials",
        "doc_scopus_id": "84908869217",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84908869217",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Digital certificates",
            "Digital credentials",
            "Electronic credentials",
            "New approaches",
            "Secure transactions",
            "Security",
            "User oriented",
            "Web-based service"
        ],
        "doc_abstract": "E-Commerce and Services are become a major commodity reality. Aspects like electronic identification, authentication and trust are core elements in referred web market areas. The use of electronic credentials and the adoption of a unique worldwide-accepted digital certificate stored in a smart card will provide a higher level of security while allowing total mobility with secure transactions over the web. While this adoption does not take place, the widespread use of digital credentials will inevitably lead to each service client having to be in possession of different electronic credentials needed for all the services he uses. We present a new approach that provides a user-oriented model to manage multiple electronic credential, based in utilization of only one smart card per user as a basis for secure management of web-based services, thus contributing for a more generalized use of the technology.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A New Concept for an IntegratedHealthcare access model",
        "doc_scopus_id": "8444232153",
        "doc_doi": "10.3233/978-1-60750-939-4-101",
        "doc_eid": "2-s2.0-8444232153",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            },
            {
                "area_name": "Health Information Management",
                "area_abbreviation": "HEAL",
                "area_code": "3605"
            }
        ],
        "doc_keywords": [
            "Distributed HIS",
            "Electronic patient record",
            "Health care information system",
            "Healthcare information systems (HIS)",
            "Identification of persons",
            "Patient data",
            "Patient information",
            "Web-based interface"
        ],
        "doc_abstract": "The increase of population mobility has been promoting a crescent dispersion of patient clinical records in Healthcare Information Systems. In this scenario, it is mandatory that new services will be available for healthcare practitioners, namely web-based interfaces with strong control access mechanisms providing effective authentication and identification of persons, and the establishment of new access models to the disperse patient information. This paper proposes and describes a Healthcare Access Model that integrates a new set of functionalities coping with patient mobility and implements an innovative concept of a virtual unique Electronic Patient Record - EPR.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Delegation of expressions for distributed SNMP information processing",
        "doc_scopus_id": "67650379664",
        "doc_doi": "10.1007/978-0-387-35674-7",
        "doc_eid": "2-s2.0-67650379664",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "DISMAN",
            "Expression MIB",
            "ITS applications",
            "Management information",
            "Scalability problems",
            "SNMP",
            "Workgroups"
        ],
        "doc_abstract": "Due to the scalability problems of SNMP, management distribution has been an important topic during the last years. The DISMAN workgroup propose a set of MIB modules to address this matter. One of the DISMAN modules has the capability of using expressions to perform decentralized processing of management information - the Expression MIB. Although it is essential to network management, the Expression MIB is not as well known as other DISMAN modules, such as the Script MIB, and not as available in terms of implementations. This paper focuses on the Expression MIB features, its implementation details and it also discusses, from a critical point of view, its functionality. It also proposes minor changes which can boost its application range and importance. © 2003 by Springer Science+Business Media Dordrecht.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Electronic patient record virtually unique based on a crypto Smart Card",
        "doc_scopus_id": "35248872485",
        "doc_doi": "10.1007/3-540-45068-8_102",
        "doc_eid": "2-s2.0-35248872485",
        "doc_date": "2003-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Electronic patient record",
            "Multi-services",
            "Patient data",
            "Patient mobility",
            "Web technologies"
        ],
        "doc_abstract": "This paper presents a Multi-Service Patient Data Card (MS-PDC) based on a crypto smart card and Web technology that integrates a new set of functionalities that allow handling well patient mobility. © Springer-Verlag Berlin Heidelberg 2003.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Critical information systems authentication based on PKC and biometrics",
        "doc_scopus_id": "35248824578",
        "doc_doi": "10.1007/3-540-45068-8_101",
        "doc_eid": "2-s2.0-35248824578",
        "doc_date": "2003-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Access control models",
            "Authentication mechanisms",
            "Health care professionals"
        ],
        "doc_abstract": "This paper presents an access control model that dynamically combines biometrics with PKC technology to assure a stronger authentication mechanism to healthcare professional that can be used indistinctly in Internet and Intranets access scenario. © Springer-Verlag Berlin Heidelberg 2003.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A mobile agent manager",
        "doc_scopus_id": "0142187742",
        "doc_doi": "10.1007/978-3-540-39646-8_21",
        "doc_eid": "2-s2.0-0142187742",
        "doc_date": "2003-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Agent research",
            "User requirements"
        ],
        "doc_abstract": "Friendliness on user interfaces has been mostly forgot in mobile agent research. In fact the multitude of problems that have to be solved have left this more \"simple\" piece of the package out of target. However, the success of technology comes also from this important layer in the product development. This paper describes an application that integrates monitoring data from the mobile agents environment, process and presents information in different ways according to different user requirements and allows the user to remotely control mobile agents entities. © Springer-Verlag Berlin Heidelberg 2003.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A multi-protocol architecture for SNMP entities",
        "doc_scopus_id": "84964345627",
        "doc_doi": "10.1109/IPOM.2002.1045759",
        "doc_eid": "2-s2.0-84964345627",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "In networks",
            "Internetworking",
            "Management protocols",
            "Middleware architecture",
            "Multiprotocols",
            "SNMP",
            "Time-constrained service"
        ],
        "doc_abstract": "© 2002 IEEE.Growing requirement in the network management area, driven by the successive increase in network size, internetworking complexity and time-constrained services deployment, impose the omnipresence of technicians and tools. The universality of interfaces is essential to support these requirements, from the side of both the manager and the agent. The paper presents a middleware architecture for the development of agents that can be transparently mapped into different management protocol stacks.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A uniform resource identifier scheme for SNMP",
        "doc_scopus_id": "84964330558",
        "doc_doi": "10.1109/IPOM.2002.1045761",
        "doc_eid": "2-s2.0-84964330558",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            }
        ],
        "doc_keywords": [
            "Different services",
            "Internet resources",
            "Management applications",
            "SNMP",
            "Text string",
            "Uniform resource identifiers"
        ],
        "doc_abstract": "© 2002 IEEE.One of the World Wide Web characteristics, besides its omnipresence in computer systems, is the adoption of a universal user interface that is used to access several different services that were previously accessed individually by independent applications. Internet resources started to be identified by URI (uniform resource identifier) schemes, a text string with specific syntax and grammar. Although existing for several services such as http, ftp, gopher and news, these identifiers are not used to identify SNMP resources. This paper proposes a URI scheme for identifying SNMP resources and presents some practical scenarios where the existence of such a compact and complete identifying mechanism increases flexibility and functionality of network management applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-management schemes for MAF platforms",
        "doc_scopus_id": "84958746788",
        "doc_doi": "10.1007/3-540-36086-7_2",
        "doc_eid": "2-s2.0-84958746788",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Cooperative model",
            "GUI tools",
            "Management scheme",
            "Management tasks",
            "Monitor and control",
            "Networks and systems",
            "Recent researches"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 2002.Due to the crescent complexity of networks and systems, network management solutions are being pushed towards more distributed and cooperative models. Several specifications promoted by the IETF DISMAN charter already allow strong distribution of management tasks. Unfortunately, they are not adequate to achieve cooperative models. According to recent research, mobile agents provide a good platform to back cooperative models but several lacks are still identified – interoperability between different platforms and SNMP integration. Based on Mobile Agent Facility specification from the OMG, we propose a MIB to better integrate mobile agent models into SNMP frameworks and a GUI tool to monitor and control mobile agents platforms.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A trusted brokering service for PKI interoperability and thin-clients integration",
        "doc_scopus_id": "84973866184",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84973866184",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Current problems",
            "Essential features",
            "Information protection",
            "Network infrastructure",
            "Security",
            "Security issues",
            "Security services",
            "WEB security"
        ],
        "doc_abstract": "E-commerce seems to be one of most promising business areas for the upcoming years. However, for its plain fulfillment, security issues have to be judiciously managed. Information protection and digital signatures are essential features for documents that represent commitments. While several solutions already exist on the market, current problems are mainly related with the lack of interoperability. On this paper we present a security broker that uses XML to provide the end-user with a set of security services and tools that are independent of the client hardware, operating system, PKI solutions and network infrastructure.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new mechanism for distributed managers persistence",
        "doc_scopus_id": "84973863957",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84973863957",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Agent interaction",
            "Application scenario",
            "Complex information",
            "Distribution architecture",
            "Management information",
            "Network management frameworks",
            "New mechanisms",
            "SNMP"
        ],
        "doc_abstract": "SNMP is currently a worldwide used network management framework. This primacy is based on simple characteristics - it limits itself to describe the structure of management information and the procedures to access data, i.e. low-level operations. Recent work inside IETF propose a management distribution architecture (DISMAN), which allows to build agents that can cope with rather complex information structures. The model still lacks to define persistence mechanisms for the management delegates. In this paper we present an XML-based data model that can provide persistence for distributed managers configuration. Moreover, several application scenarios will be discussed such as, the definition of high-level macros to group together elementary SNMP operations, and the specification of mobile agent policies for SNMP agent interaction.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "QOS negotiation based on management delegates",
        "doc_scopus_id": "84973862454",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84973862454",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Control mechanism",
            "Distributed management",
            "Implementation complexity",
            "Management flexibility",
            "Network operator",
            "Service control point",
            "Service management",
            "Two stage approach"
        ],
        "doc_abstract": "The introduction of Quality of Service in the Internet will bring increased needs for efficient service management in the network. Current approaches to this problem rely on distributed management protocols with centralized service control points. This paper proposes the usage of software agents for this task, developed in a two-stage approach due to its implementation complexity. The usage of agents, supported by appropriate platforms, would naturally provide delegation and control mechanisms, improve management flexibility and even transparently support the existence of multiple management paradigms across network operators.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SNMP management of MASIF platforms",
        "doc_scopus_id": "84952322936",
        "doc_doi": "10.1109/INM.2001.918047",
        "doc_eid": "2-s2.0-84952322936",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Commercial networks",
            "Mobile agent system",
            "Mobile agent technology",
            "Power system management",
            "Software standards",
            "Technology managements",
            "Usability"
        ],
        "doc_abstract": "© 2001 IEEE.In this paper we describe the architecture, the development and the assessment results of an SNMP agent that allows MASIF (mobile agent system interoperability facilities) compliant platforms to be managed through SNMP. The major outcome of this work is a simple integration of mobile agent technology with any well-established commercial network management system.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of mobility in distributed network management",
        "doc_scopus_id": "85094170877",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85094170877",
        "doc_date": "2000-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Agent paradigm",
            "Distributed network management",
            "Information management systems",
            "Management environments",
            "Mobility capabilities",
            "Working groups"
        ],
        "doc_abstract": "© 2000 IEEEInformation Technology has been under unprecedented transformations and it is dramatically changing the way of work inside organizations. Information management systems must be adequate to cope with the profound effects of this evolution, which expectations includes the introduction into the networks of enormous quantities of different elements. Mobile agent paradigm seems to be, for many researchers, the right solution to deal with the pressures of these new demands. This paper discuss the issues around mobility of code on network management environments and presents ongoing work that provides mobility capability to distributed managers upon recent work of IETF's Disman working group.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Network and desktop management convergence",
        "doc_scopus_id": "85094158640",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85094158640",
        "doc_date": "2000-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Desktop management",
            "Large-scale distributed applications",
            "Management frameworks",
            "Massive quantities",
            "Resource-limited devices",
            "Tools and technologies"
        ],
        "doc_abstract": "© 2000 IEEENetwork management and desktop management frequently employ different tools and technologies. This divorce may be even emphasized as massive quantities of very diverse elements ranging from resource-limited devices to large-scale distributed applications are expected in the Internet. The convergence of these different kinds of management approaches into a common framework is critical for the future development of the Internet and intranets - it would provide the ability to manage these increasingly spread and dissimilar elements, in order to permit management of the services that they collectively provide. This paper discusses the advantages, disadvantages and points of convergence of management frameworks, and presents an integration solution based on emerging standards (CIM/XML) that allows, beyond that convergence, an easy migration to the recent worldwide user interface (the Web Browser).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of mobility in distributed network management",
        "doc_scopus_id": "0033900575",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0033900575",
        "doc_date": "2000-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Distributed network management",
            "Mobility"
        ],
        "doc_abstract": "Information Technology has been under unprecedented transformations and it is dramatically changing the way of work inside organizations. Information management systems must be adequate to cope with the profound effects of this evolution, which expectations includes the introduction into the networks of enormous quantities of different elements. Mobile agent paradigm seems to be, for many researchers, the right solution to deal with the pressures of these new demands. This paper discuss the issues around mobility of code on network management environments and presents ongoing work that provides mobility capability to distributed managers upon recent work of IETF's Disman working group.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Network and desktop management convergence",
        "doc_scopus_id": "0033884194",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0033884194",
        "doc_date": "2000-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Desktop management",
            "Network management",
            "Worldwide user interface"
        ],
        "doc_abstract": "Network management and desktop management frequently employ different tools and technologies. This divorce may be even emphasized as massive quantities of very diverse elements ranging from resource-limited devices to large-scale distributed applications are expected in the Internet. The convergence of these different kinds of management approaches into a common framework is critical for the future development of the Internet and intranets - it would provide the ability to manage these increasingly spread and dissimilar elements, in order to permit management of the services that they collectively provide. This paper discusses the advantages, disadvantages and points of convergence of management frameworks, and presents an integration solution based on emerging standards (CIM/XML) that allows, beyond that convergence, an easy migration to the recent worldwide user interface (the Web Browser).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "New services on an advanced internet",
        "doc_scopus_id": "84947742776",
        "doc_doi": "10.1007/3-540-48757-3_15",
        "doc_eid": "2-s2.0-84947742776",
        "doc_date": "1999-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Internet engineering task forces",
            "Interoperations",
            "Management issues",
            "Multimedia applications",
            "Network operator",
            "New services",
            "Public switched telephone network"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 1999.This document presents some new network operator services possible to deploy on an advanced Internet. These new services are placed in the context of current standardization activities under development in the IETF (Internet Engineering Task Force). In particular, both quality of service and PSTN (Public Switched Telephone Network) interoperation are discussed, and emphasis is placed on multimedia applications. Final comments present some operational and management issues in this new environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enabling computer applications in residential environments",
        "doc_scopus_id": "0030420403",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030420403",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Computer interactions",
            "Internet",
            "Remote information access",
            "Voice mail"
        ],
        "doc_abstract": "This paper present some work that has been done by the Portuguese partners of the IBCoBN European project, that aims to deliver telematic applications to contribute to a better quality of life of elderly and disabled people, and to improve social integration, providing leisure activities and therapeutic programmes at distance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A management architecture based on network topology information",
        "doc_scopus_id": "0028697709",
        "doc_doi": "10.1007/BF02283189",
        "doc_eid": "2-s2.0-0028697709",
        "doc_date": "1994-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Strategy and Management",
                "area_abbreviation": "BUSI",
                "area_code": "1408"
            }
        ],
        "doc_keywords": [
            "Management application programming interface",
            "Management domains",
            "Network topology information",
            "Topology discovery",
            "Topology modeling"
        ],
        "doc_abstract": "This paper describes the development of a formalism to represent Network Topology Information that provides hierarchical views of the network elements distribution, usable with advantage by management applications. It reviews some topology discovery tools and emphasizes the lack of a common methodology to represent the network cabling and organize the agent distribution information. The formalism, consisting on a descriptive language and on an object-oriented topology base, is described and some application areas, where this information can be used, are pointed. Finally it presents a Management API, as part of the overall architecture, that helps to show the usability of the Topology Information Base. © 1994, Plenum Publishing Corporation. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Methodology to represent logical network topology information",
        "doc_scopus_id": "0028016969",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0028016969",
        "doc_date": "1994-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Logical network topology",
            "NMS project"
        ],
        "doc_abstract": "This paper presents a methodology for the representation of network topologies. It will briefly discuss how some state-of-the-art management architectures fails in order to provide information about network topologies and what is the importance of organising such information to achieve a more complete management. The methodology, that follows an object-oriented style language, will be described with emphasis on the reusability in specific management environments. The integration in a NMS project will be studied emphasising the role of this work in the construction of management applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "PC workstation for Ethernet monitoring and management",
        "doc_scopus_id": "0026403147",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0026403147",
        "doc_date": "1991-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Ethernet Monitoring",
            "Network Management",
            "PC Workstation"
        ],
        "doc_abstract": "The authors present a monitoring and management workstation for IEEE 802.3 networks (Ethernet) based on a PC-AT. The monitor performs traffic analysis, detects active machines in the network, maintains a network database with information regarding network topology and machines, filters specific packets or protocols, and can also work as a traffic generator. The system has a friendly user interface simplifying the interaction with the network manager.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrated management system for interconnected local area networks",
        "doc_scopus_id": "0026373979",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0026373979",
        "doc_date": "1991-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "central management workstations",
            "Communication protocols",
            "Data exchanges",
            "Integrated management",
            "Interconnected LAN"
        ],
        "doc_abstract": "In order to provide communication facilities for data exchange among workstations located in a modern organization it is necessary to interconnect different local area networks (LANs). Normally, these interconnected LANs have different access media and topologies (Ethernet, token-ring, FDDI, ...), communications protocols (TCP/IP, OSI, ...) and services. A proposal is presented for an integrated network management system, suitable for such a heterogeneous scenario, that will hide the differences of the underlying LANs. This management system provides a central management workstation with remote access and display, and monitoring agents.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "High speed backbone concentrator",
        "doc_scopus_id": "0026371851",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0026371851",
        "doc_date": "1991-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Ethernet stations",
            "High-speed data communication",
            "High-speed servers",
            "Network concentrators"
        ],
        "doc_abstract": "The authors present the concept of using a high-speed data communication switch as a backbone network concentrator, with low packet delay, in order to accommodate internet and high-demand traffic. Also presented is the front-end processor concept, namely the implementation of an Ethernet front-end processor to connect Ethernet stations and LANs. Another front-end processor is currently being developed for the Fiber Distributed Data Interface (FDDI), because most of the FDDI applications will be LAN interconnection and direct connection of mainframes, high-speed servers and workstations.",
        "available": false,
        "clean_text": ""
    }
]