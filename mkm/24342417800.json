[
    {
        "doc_title": "Data-Driven Analysis of European Portuguese Nasal Vowel Dynamics in Bilabial Contexts",
        "doc_scopus_id": "85130029911",
        "doc_doi": "10.3390/app12094601",
        "doc_eid": "2-s2.0-85130029911",
        "doc_date": "2022-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Process Chemistry and Technology",
                "area_abbreviation": "CENG",
                "area_code": "1508"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Fluid Flow and Transfer Processes",
                "area_abbreviation": "CENG",
                "area_code": "1507"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.European Portuguese (EP) is characterized by a large number of nasals encompassing five phonemic nasal vowels. One notable characteristic of these sounds is their dynamic nature, involving both oral and nasal gestures, which makes their study and characterization challenging. The study of nasal vowels, in particular, has been addressed using a wide range of technologies: early descriptions were based on acoustics and nasalance, later expanded with articulatory data obtained from EMA and real-time magnetic resonance (RT-MRI). While providing important results, these studies were limited by the discrete nature of the EMA-pellets, providing only a small grasp of the vocal tract; by the small time resolution of the MRI data; and by the small number of speakers. To tackle these limitations, and to take advantage of recent advances in RT-MRI allowing 50 fps, novel articulatory data has been acquired for 11 EP speakers. The work presented here explores the capabilities of recently proposed data-driven approaches to model articulatory data extracted from RT-MRI to assess their suitability for investigating the dynamic characteristics of nasal vowels. To this end, we explore vocal tract configurations over time, along with the coordination of velum and lip aperture in oral and nasal bilabial contexts for nasal vowels and oral congeners. Overall, the results show that both generalized additive mixed models (GAMMs) and functional linear mixed models (FLMMs) provide an elegant approach to tackle the data from multiple speakers. More specifically, we found oro-pharyngeal differences in the tongue configurations for low and mid nasal vowels: vowel track aperture was larger in the pharyngeal and smaller in the palatal region for the three non-high nasal vowels, providing evidence of a raised and more advanced tongue position of the nasal vowels. Even though this work is aimed at exploring the applicability of the methods, the outcomes already highlight interesting data for the dynamic characterization of EP nasal vowels.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring the Age Effects on European Portuguese Vowel Production: An Ultrasound Study",
        "doc_scopus_id": "85123516451",
        "doc_doi": "10.3390/app12031396",
        "doc_eid": "2-s2.0-85123516451",
        "doc_date": "2022-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Process Chemistry and Technology",
                "area_abbreviation": "CENG",
                "area_code": "1508"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Fluid Flow and Transfer Processes",
                "area_abbreviation": "CENG",
                "area_code": "1507"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.For aging speech, there is limited knowledge regarding the articulatory adjustments underlying the acoustic findings observed in previous studies. In order to investigate the age-related articulatory differences in European Portuguese (EP) vowels, the present study analyzes the tongue configuration of the nine EP oral vowels (isolated context and pseudoword context) produced by 10 female speakers of two different age groups (young and old). From the tongue contours automatically segmented from the US images and manually revised, the parameters (tongue height and tongue advancement) were extracted. The results suggest that the tongue tends to be higher and more advanced for the older females compared to the younger ones for almost all vowels. Thus, the vowel articulatory space tends to be higher, advanced, and bigger with age. For older females, unlike younger females that presented a sharp reduction in the articulatory vowel space in disyllabic sequences, the vowel space tends to be more advanced for isolated vowels compared with vowels produced in disyllabic sequences. This study extends our pilot research by reporting articulatory data from more speakers based on an improved automatic method of tongue contours tracing, and it performs an inter-speaker comparison through the application of a novel normalization procedure.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A vision for contextualized evaluation of remote collaboration supported by AR",
        "doc_scopus_id": "85118343204",
        "doc_doi": "10.1016/j.cag.2021.10.009",
        "doc_eid": "2-s2.0-85118343204",
        "doc_date": "2022-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Characterization collaborative process",
            "Collaboration efforts",
            "Collaborative process",
            "Distributed teams",
            "Evaluation toolkit",
            "Guideline",
            "Knowledge retention",
            "Remote collaboration",
            "Team members",
            "User study"
        ],
        "doc_abstract": "© 2021 Elsevier LtdRemote collaboration using Augmented Reality (AR) has potential to support physically distributed team-members that need to achieve a common goal by increasing knowledge retention, improving understanding and awareness of the problem and its context. In this vein, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Thus, characterization and evaluation of the collaborative process is paramount, but a particularly challenging endeavor, due to the multitude of aspects that define the collaboration effort. In this context, the work presented here contributes with a critical analysis, discussing current evaluation efforts, identifying limitations and opportunities. Then, we outline a conceptual framework to support researchers in conducting evaluations in a more structured manner. To instrument this vision, an evaluation toolkit is proposed to support contextual data collection and analysis in such scenarios and obtain an additional perspective on selected dimensions of collaboration. We illustrate the usefulness and versatility of the toolkit through a case study on remote maintenance, comparing two distinct methods: sharing of video and AR-based annotations. Last, we discuss the results obtained, showing the proposed vision allows to have an additional level of insights to better understand what happened, eliciting a more complete characterization of the work effort.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2021-10-13 2021-10-13 2022-02-25 2022-02-25 2022-06-01T12:50:43 S0097-8493(21)00220-X S009784932100220X 10.1016/j.cag.2021.10.009 S300 S300.2 FULL-TEXT 2022-06-09T18:22:28.119019Z 0 0 20220201 20220228 2022 2021-10-13T16:42:28.006423Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantnumber grantsponsor highlightsabst orcid primabst pubtype ref specialabst 0097-8493 00978493 true 102 102 C Volume 102 22 413 425 413 425 202202 February 2022 2022-02-01 2022-02-28 2022 Special Section on Adv Graphics+Interaction; Edited by Nuno Rodrigues, Daniel Mendes, Luís Paulo Santos, Kadi Bouatouch article fla © 2021 Elsevier Ltd. All rights reserved. AVISIONFORCONTEXTUALIZEDEVALUATIONREMOTECOLLABORATIONSUPPORTEDBYAR MARQUES B 1 Introduction 2 Background and challenges for evaluation of AR-based remote collaboration 3 A vision for contextualized collaborative AR evaluation 3.1 Evaluation purpose 3.2 Team and collaborative tasks 3.3 Experimental setup and design 3.4 Contextualized data gathering 3.5 Analysis and report 4 Toolkit for distributed evaluations using AR 5 User study on a remote maintenance scenario 5.1 Experimental setup 5.1.1 Video chat tool 5.1.2 AR-based annotation tool 5.2 Experimental design 5.3 Tasks 5.4 Measurements 5.5 Procedure 5.6 Participants 6 Results and discussion 6.1 Overall total time and task time 6.2 Overview of the collaborative process 6.3 Participants preferences and opinion 6.4 Final remarks 7 Conclusions and future work CRediT authorship contribution statement Acknowledgments References KIM 2020 529 538 K CONFERENCEVIRTUALREALITY3DUSERINTERFACES REDUCINGTASKLOADEMBODIEDINTELLIGENTVIRTUALASSISTANTFORIMPROVEDPERFORMANCEINCOLLABORATIVEDECISIONMAKING KIM 2018 2947 2962 K KIM 2018 569 607 S GERVASI 2020 841 865 R LUKOSCH 2015 515 525 S BILLINGHURST 2015 73 272 M ENS 2019 81 98 B WANG 2021 102071 P BOTTANI 2019 284 310 E WANG 2016 1 22 X LEE 2020 343 352 G IEEECONFERENCEVIRTUALREALITY3DUSERINTERFACES AUSERSTUDYVIEWSHARINGTECHNIQUESFORONETOMANYMIXEDREALITYCOLLABORATIONS LUDWIG 2021 119 167 T GUREVICH 2015 527 562 P KIM 2018 6034 6056 S KIM 2020 321 335 S NEALE 2004 112 121 D PROCEEDINGS2004ACMCONFERENCECOMPUTERSUPPORTEDCOOPERATIVEWORK EVALUATINGCOMPUTERSUPPORTEDCOOPERATIVEWORKMODELSFRAMEWORKS HAMADACHE 2009 206 221 K GROUPWAREDESIGNIMPLEMENTATIONUSE STRATEGIESTAXONOMYTAILORINGYOURCSCWEVALUATION ANTUNES 2014 146 169 P BELEN 2019 181 R MARQUES 2021 1 17 B BAI 2012 450 460 Z DEY 2018 37 A ANTON 2018 77 88 D PIUMSOMBOON 2018 2974 2982 T ARAUJO 2004 139 150 R MARQUES 2021 1 18 B MARQUES 2021 1 6 B INFORMATIONVISUALIZATIONIV VISUALLYEXPLORINGACOLLABORATIVEAUGMENTEDREALITYTAXONOMY IZARD 2007 260 280 C BARNUM 2010 C USABILITYTESTINGESSENTIALSREADYSETTEST GUTWIN 1999 243 281 C KIM 2018 6034 6056 S HUANG 2019 428 438 W PATEL 2012 1 26 H PIUMSOMBOON 2019 T MADEIRA 2021 83 89 T HUMANSYSTEMSENGINEERINGDESIGNIII EXPLORINGANNOTATIONSHANDTRACKINGINAUGMENTEDREALITYFORREMOTECOLLABORATION MARQUESX2022X413 MARQUESX2022X413X425 MARQUESX2022X413XB MARQUESX2022X413X425XB 2024-02-25T00:00:00.000Z 2024-02-25T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 0 2022-06-06T22:43:45.902Z University of Aveiro UA Universidade de Aveiro FCT SFRH/BD/143276/2019 UID/CEC/00127/2019 FCT Fundação para a Ciência e a Tecnologia European Regional Development Fund ERDF European Regional Development Fund We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [UID/CEC/00127/2019]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [ UID/CEC/00127/2019 ]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. item S0097-8493(21)00220-X S009784932100220X 10.1016/j.cag.2021.10.009 271576 2022-06-01T12:26:47.516259Z 2022-02-01 2022-02-28 true 3678202 MAIN 13 63615 849 656 IMAGE-WEB-PDF 1 gr3 43612 433 376 gr9 52635 306 376 gr1 28469 346 376 gr14 28871 413 376 gr12 42537 501 376 gr11 27866 474 376 gr7 39516 303 376 gr4 24720 434 376 gr15 11763 198 373 gr6 62725 542 376 ga1 true 16462 245 267 gr5 43801 320 376 gr2 78591 583 376 gr13 87515 441 376 gr8 45540 303 376 gr10 16187 255 369 gr3 6255 163 142 gr9 28164 164 201 gr1 4691 164 178 gr14 4530 164 149 gr12 4835 164 123 gr11 4118 164 130 gr7 20550 164 204 gr4 4480 164 142 gr15 3721 117 219 gr6 7321 163 113 ga1 true 5480 164 179 gr5 9004 164 192 gr2 9859 163 105 gr13 10706 163 139 gr8 22695 164 203 gr10 5254 151 219 gr3 341412 1919 1667 gr9 491740 1357 1667 gr1 192280 1532 1667 gr14 182688 1833 1667 gr12 307513 2220 1667 gr11 187668 2101 1667 gr7 367677 1342 1667 gr4 189805 1924 1667 gr15 78304 880 1654 gr6 482554 2402 1667 ga1 true 110211 1085 1183 gr5 291127 1420 1667 gr2 654708 2584 1667 gr13 594548 1957 1667 gr8 432457 1343 1667 gr10 110543 1129 1635 CAG 3442 S0097-8493(21)00220-X 10.1016/j.cag.2021.10.009 Elsevier Ltd Fig. 1 Conceptual framework for helping researchers evaluate of AR-remote collaboration in a more structured manner. Fig. 2 Scenario of remote collaboration using an AR-based tool instrumented with the CAPTURE toolkit: 1 — On-site technician requiring assistance; 2- Expert using AR to provide remote guidance; 3 - Researcher(s) following the evaluation process; 4- Distributed multi-user data gathering; 5- Contextual data collection based on existing dimensions of collaboration [37]; 6- Evaluation data storage; 7- Visualization dashboard for analysis of the collaborative process. Fig. 3 CAPTURE toolkit — example of pre-defined scenes associated with post-task measurements. Top — questionnaire regarding the collaboration process; Bottom — questionnaire regarding the collaborative tool. Fig. 4 CAPTURE toolkit — example of pre-defined scenes associated with selected dimensions of collaboration. Top — characteristics of the Team; Bottom — characteristics of the Task. Fig. 5 CAPTURE architecture. The toolkit can be integrated into a collaborative tool via visual editor. All data collected during collaboration is stored in a central server, which can be analyzed during post-task analysis through the visualization dashboard. Fig. 6 Overview of the CAPTURE toolkit assets: ready to use scene prefabs and editable scripts, which researchers may modify according to the aspects of collaboration being considered for the evaluation. Fig. 7 Video Chat tool for remote collaboration. Fig. 8 AR-based Annotation tool for remote collaboration. Fig. 9 Illustration of some of the completion stages associated with the maintenance tasks used in the study: 1- replace interconnected components; 2- plug and unplug some energy modules; 3- remove a specific sensor; 4- integrate new components into the equipment. Fig. 10 Total time and task time with the two conditions (in minutes). C1: video chat tool; C2: AR-based annotation tool. Fig. 11 Overview of the collaborative process outcomes for all teams during a scenario of remote maintenance, including all the selected measures collected: easy to share ideas properly, as well as communicate, level of attentional allocation, information understanding, mental effort, enjoyment, spatial presence. Top — C1: video chat tool; Bottom — C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1 — Low; 7 — High. Fig. 12 Collaborative process for the same team during remote maintenance using the two tools: Top — C1: video chat tool; Bottom — C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1 — Low; 7 — High. Fig. 13 Participants total reaction cards regarding the collaborative tools. C1: video chat tool; C2: AR-based annotation tool. A larger font size means that the word was selected by more participants (higher frequency). Red — negative meaning; gray — neutral meaning; green — positive meaning [41]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 14 Participants emotional state before (top) and after (bottom) the tasks for each condition. C1: video chat tool; C2: AR-based annotation tool. Fig. 15 Participants satisfaction towards the tools. C1: video chat tool; C2: AR-based annotation tool. Data displayed using a Likert-type scale: 1- Low; 7- High. Special Section on Adv Graphics+Interaction A vision for contextualized evaluation of remote collaboration supported by AR Bernardo Marques Conceptualization Methodology Software Formal analysis Investigation Resources Writing – original draft Writing – review & editing Visualization Project administration Funding acquisition ⁎ Samuel Silva Conceptualization Methodology Writing – original draft Writing – review & editing Visualization António Teixeira Conceptualization Methodology Writing – original draft Writing – review & editing Visualization Paulo Dias Conceptualization Resources Writing – original draft Writing – review & editing Visualization Supervision Funding acquisition Beatriz Sousa Santos Conceptualization Writing – original draft Writing – review & editing Visualization Supervision Funding acquisition IEETA, DETI, Universidade de Aveiro, Portugal IEETA, DETI, Universidade de Aveiro Portugal DETI, IEETA, Universidade de Aveiro ⁎ Corresponding author. Remote collaboration using Augmented Reality (AR) has potential to support physically distributed team-members that need to achieve a common goal by increasing knowledge retention, improving understanding and awareness of the problem and its context. In this vein, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Thus, characterization and evaluation of the collaborative process is paramount, but a particularly challenging endeavor, due to the multitude of aspects that define the collaboration effort. In this context, the work presented here contributes with a critical analysis, discussing current evaluation efforts, identifying limitations and opportunities. Then, we outline a conceptual framework to support researchers in conducting evaluations in a more structured manner. To instrument this vision, an evaluation toolkit is proposed to support contextual data collection and analysis in such scenarios and obtain an additional perspective on selected dimensions of collaboration. We illustrate the usefulness and versatility of the toolkit through a case study on remote maintenance, comparing two distinct methods: sharing of video and AR-based annotations. Last, we discuss the results obtained, showing the proposed vision allows to have an additional level of insights to better understand what happened, eliciting a more complete characterization of the work effort. Graphical abstract Keywords Remote collaboration Augmented Reality Characterization collaborative process Guidelines Evaluation toolkit User study 1 Introduction Collaboration has the potential to achieve more effective solutions for challenging problems [1]. It has evolved from simple co-located situations to more complex remote scenarios, encompassing several team members with different experiences, expertise’s and multidisciplinary backgrounds. Remote collaboration can be described as the process of joint and interdependent activities between physically distributed collaborators performed to achieve a common goal [2–4]. This activity has become essential in many situations, as is the case of industrial, medical, and educational domains, among others [5,6]. To address such activities, remote solutions have been growing in terms of scale, complexity, and interdisciplinarity, entailing not only the mastery of multiple domains of knowledge, but also a strong level of proficiency in each [5,6]. Scenarios of remote collaboration imply that collaborators establish a joint effort to align and integrate their activities in a seamless manner. To address this, and overcome the fact team-members do not share a common space/world, there is an increasing interest in using Augmented Reality (AR) in this context [7–10]. Remote collaboration mediated by AR combines the advantages of virtual environments and the seamless integration with the real-world objects and other collaborators by overlying responsive computer-generated information on top of the real-world environment [2,11,12], allowing to establish a common ground, analogous to their understanding of the physical space, i.e., serve as a basis for situation mapping, allowing identification of issues, and making assumptions and beliefs visible [13–16]. These solutions can be used to empower workers that require knowledge from professionals unavailable on-site [17]. Remote experts can provide guidance, highlight specific areas of interest or share real-time spatial information [9,10,14,18] in the form of visual communication cues, e.g., pointers, annotations, hand gestures, among others [3,9,17,19–22]. These solutions can better support analysis, discussion and resolution of complex problems and situations, given their ability to enhance alertness, awareness, and understanding of the situation [23]. In the past decade, the community has been particularly active in this domain, concentrating efforts on creating the enabling technology to support the design and creation of an AR-based shared understanding. As the field matures and with the growing number of prototypes, the path to achieve usable, realistic and impactful solutions must entail an explicit understanding regarding how remote collaboration occurs through AR and how it may help contribute to a more effective work effort. Therefore, evaluating such scenarios becomes an essential, but difficult endeavor [23–26], given the lack of methods and frameworks to guide the characterization of the collaborative process [9,27–29]. This is substantiated by Bai et al. reporting that ”it can be hard to isolate the factors that are specifically relevant to collaboration” [30]. In fact, this is further evident in remote scenarios, since the logistics associated with carrying out evaluations in these multifaceted contexts is even more demanding due to a significant number of variables that may affect the way teams collaborate [9,27]. Ratcliffe et al. report that ”remote settings introduce additional uncontrolled variables that need to be considered by researchers, such as potential unknown distractions, (...) participants and their motivation, and issues with remote environmental spaces” [31]. Also, Dey et al. suggest the existence of ”opportunities for increased user studies in collaboration” and the need for ”a wider range of evaluation methods” [32]. In this vein, Ens et al. emphasize that ” frameworks for describing groupware and MR systems are not sufficient to characterize how collaboration occurs through this new medium” [9]. Additionally, Ratcliffe et al. suggest that ”the infrastructure for collecting and storing this (mass) of XR data remotely is currently not fully implemented, and we are not aware of any end-to-end standardized framework” [31]. As such, conducting thorough evaluations is paramount to retrieve the necessary data for more comprehensive analysis that help provide a better perspective on the different factors of collaboration supported by AR. Hence, integration of proper characterization and evaluation methods, covering different contexts of use and tasks are of utmost importance. In this paper, we analyze existing evaluation efforts on remote collaboration using AR to provide a high-level overview. Motivated by the challenges reported, we present a conceptual framework for supporting researchers in obtaining an additional perspective on several dimensions of collaboration. Then, we propose the CAPTURE toolkit, a first instantiation towards the vision proposed, aiming to provide a strategy that monitors data concerning the level of collaboration, behavior and performance of each intervening party, individual and as a team, as well as contextual data. To illustrate the advantages of the framework, the toolkit usefulness and versatility are demonstrated through a case study in a remote maintenance scenario, comparing two distinct methods: sharing of video and AR-based annotations. Then, the results obtained are discussed, showing that the proposed vision allows having an additional level of insights to better understand what happened, eliciting a more complete characterization of the collaborative work effort. The remainder of this paper is organized as follows. Section 2 overviews existing evaluation efforts on remote collaboration mediated by AR. Section 3 proposes our conceptual framework for essential aspects that must be addressed. Section 4 describes the CAPTURE toolkit and Section 5 applies it through a user study on a remote maintenance scenario. Section 6 presents and discusses the main results. Finally, concluding remarks and future research opportunities are drawn in Section 7. 2 Background and challenges for evaluation of AR-based remote collaboration This section reports existing evaluation efforts addressing collaborative AR user studies. The goal is to understand how evaluation has been conducted in such scenarios, provide a high-level overview, and identify existing challenges and gaps. According to Merino et al. ”as MR/AR technologies become more mature, questions that involve human aspects will gain focus in MR/AR research. Consequently, we expect that future MR/AR papers will elaborate on human-centered evaluations that involve not only the analysis of user performance and user experience, but also the analysis of other scenarios, like understanding the role of MR/AR in working places and in communication and collaboration” [26]. However, there is no standard methodology for characterization and evaluation, specifically tailored to assess how remote collaboration occurs through AR. The literature shows that studies that evaluate their solutions rely on single-user methods, mainly focused on the comparison of technological aspects or interaction mechanisms, which are not the most adequate for multifaceted solutions that aim to support distributed team collaboration [2,9,25,27,29,32]. Also, most studies focus exclusively on the performance of one collaborator, i.e., on-site, or remote. This means evaluation usually does not consider interaction, and communication among team-members, and is not conducted in distributed scenarios, as should be the case to establish experimental conditions closer to real scenarios, Likewise, focus is given to the technological aspects of the solution being used, as well as to quantifying the effectiveness in completing the tasks, which mostly lack difficulty, diversity and ecological validity [2,3,7,29,30,32]. Moreover, the majority of studies are formal, conducted in laboratories, collecting objective and subjective data at the end of the tasks through standard practices with fixed answers like scale-based questionnaires (e.g., System Usability Scale (SUS), NASA Task Load Index (TLX), among others) or direct observation [7,9,19,27,32–34]. Adding to these data, only a reduced set of studies include measurements collected during the collaborative process (e.g., task duration and error/accuracy), as well as explicit communication (e.g., spoken messages or gestural cues), ease of collaboration and others [26,30]. While this is the case, the collection of more contextual and behavioral data is often not considered or hindered due to the complexity it entails regarding acquisition, processing and analysis, and more important, the lack of guidelines to inform researchers on what dimensions of collaboration should be collected and how. Therefore, current frameworks are not tailored to characterize how collaboration mediated by AR occurs [9,25,28,31], falling short to retrieve the necessary amount of data for more comprehensive analysis. As a consequence, without the appropriate methods, the research community does not accumulate enough experience to improve the work effort [3,9,25,26,28–30,32,35]. Thus, as the field of remote collaboration using AR matures, evaluation needs to move beyond a simple assessment of how the technology works, as it becomes essential to understand different aspects of collaboration itself, including how teams work together, how communication happens, how AR is used to create a common ground, among others. This should provide a richer output of the evaluation stage, balancing the design against requirements and leading to a more informed refinement of the context of use and system features, e.g., in line with the life-cycle for Human-Centered Design (HCD) described in the principles and activities associated with the [ISO 9241-210]. 1 1 iso.org/standard/77520.html. Given the challenges and constraints involved in evaluating the way collaboration occurs through AR, we argue it is paramount to address a set of important topics, namely: 1- conduct more collaborative-centric evaluations, i.e., move beyond usability testing, which fails to obtain a more comprehensive understanding of the work effort. Equally important, 2- develop evaluation strategies including contextual data collection and visualization, i.e., collect a richer data set to better understand how AR contributes to the collaborative process, in order to shape more effective collaboration. 3 A vision for contextualized collaborative AR evaluation The area being addressed in this work is part of a complex phenomenon. To allow answering existing problems, it is necessary to systematize knowledge and perspectives, so that it can be applied transversely. For this, it is necessary the creation of evaluation frameworks, i.e., capitalize on the hierarchies and dimensions of collaboration from ontologies and taxonomies, as well as the development of tools that allow contextualizing the use of collaborative solutions. Taking into account the challenges and needs identified in the previous section, Fig. 1 structurally presents an evaluation framework of the collaborative process when using a given tool, with a proposal of several levels of information that must be considered for contextualization, derived through a HCD methodology. In this effort, we argue that the evaluation process must be addressed by the research community, namely the definition of the evaluation purpose, as well as the team characteristics and the details of the collaborative tasks. Also, carefully establish the experimental setup and design. Equally important, explore contextualized data gathering and analysis, which requires the creation of novel tools. This last, being the aspect this work further contributes. Next, we elaborate on these with more detail. 3.1 Evaluation purpose To begin, the scope must be defined, taking into account existing dimensions of collaboration to clarify what will be evaluated, so that relevant research questions are formulated in the design phase and answered in the evaluation analysis [24]. 3.2 Team and collaborative tasks Also important, determine the team-members’ characteristics, i.e., role structure, coupling level, life-span, technology literacy and multidisciplinarity. In this context, participants with different ages, perspectives, motivations, and multidisciplinary background should be considered, which might lead to more relevant insights. Moreover, understanding of VR/AR, as well as remote tools is a benefit for the adaptation, thus removing the ’wow factor’ that makes participants feel excitement or admiration towards such technologies. Besides, participants should only perform one role, i.e., on-site or remote, so that they are only exposed to a set of tasks, concerns and responsibilities. Furthermore, the collaborative tasks goals must be clearly established including which team-members will be accountable for achieving each completion stage. It is also important to consider if the tasks are performed indoor, outdoor, or mixed between the two; A balance must be kept between task complexity and duration. Tasks must be complex and long enough to encourage interaction through AR. However, longer tasks may cause fatigue or boredom, affecting the evaluation outcomes. Equally relevant, tasks can introduce deliberated drawbacks, i.e., incorrect, contradictory, vague or missing information, to force more complex situations and elicit collaboration. 3.3 Experimental setup and design Establish the experimental setup and design are equally key. When considering prototypes, evaluation under laboratory settings should be used. Afterwards, when considering more mature solutions, evaluation should be made in the field, with real stakeholders and domain experts, moving beyond typical laboratory settings to increase the ecological validity of the evaluations. Regarding the environment, two separated rooms in the same/ different building(s) should be used. Otherwise, participants must be separated by some kind of physical barrier when in the same room. Furthermore, an adaptation period must be provided so that participants can explore the technology possibilities before the tasks, individually and as a team. Besides, a proper amount of time must be defined for other aspects, e.g., presentation of the study, pre- and post-task questionnaires, team interview, and others. 3.4 Contextualized data gathering As well observed by Merino et al. [26], future works on Mixed and Augmented Reality (MR/AR) will elaborate on human-centered evaluations involving not only the analysis of user experience and performance, but also understanding the role of such technologies in working places, in communication and in collaboration. In this scope, contextual information helps inform the conditions in which the collaboration took place. It can also be used for understanding interaction and communication changes, namely if the surroundings affected the way teams collaborate, in such a way that they needed to adapt it. Also, it helps portrait the conditions in which team-members performed a given action, received information or requested assistance, which can be used to assess uncommon situations or identify patterns that can lead to new understanding of a given artifact, as well as identify new research opportunities. Without comprehending contextual information, it becomes difficult to assess important variables related to the collaborative process, which means the findings reported may be misleading or of limited value. Hence, these aspects have an important impact on how the studies must be prepared and how they were conducted, influencing situation understanding, team-members communication, performance, and usage of AR. Literature shows that a better evaluation process can be supported by improved data collection and data visualization tools [35,36]. In particular, the following factors are crucial and must be taken in account to better understand the real impact of each aspect in the collaborative effort: team, tasks, context and AR-based tool [28]. Through these, a wide range of information is provided when performing judgment over the results and establishing conclusions. Therefore, data collection while team-members collaborate, considering different forms of measurement according to the evaluation goals is paramount and should include: • pre-task measures like demographic questionnaires (e.g., age, gender, occupation, years of experience, etc.), information on participants background: if they knew each other, previous experience with VR/AR technologies and remote tools, among other aspects; • runtime measures may comprise: – performance metrics including overall duration of specific events, number and type of errors; number and type of interactions; frequency of using each feature of the tool; screenshots of the enhanced content; – behavior metrics including conversational analysis (e.g., frequency of conversational turns, number of questions or interruptions, and dialog length, duration of overlapping speech); physical movement around the environment; number of hand gestures; physiological variables and emotions; eye gaze; – collaboration metrics including the level of effectiveness; perception; interest; engagement; awareness; togetherness; mental stress; – researchers may collect audio (or video) and register interesting events including the type (e.g., guide, request, express, propose) and frequency of communication (e.g., never, sometimes, often, continuously), if the goals were accomplished, difficulties detected, if the participants requested assistance and how many types, among other relevant aspects. • post-task measures can encompass: – register usability towards the tools(s) used; – record collaboration metrics including the level of effectiveness; perception; interest; engagement; awareness; togetherness; mental stress, etc; – collect participants reactions, opinions and preferences through semi-structured interviews. 3.5 Analysis and report The use of more contextualized approaches will provide ground to improve how research is analyzed and reported. Hence, increasing the awareness of researchers about the different dimensions of collaboration and the need to improve how the nuances associated to the collaborative effort are described. In turn, a more systematic characterization can lead to a community setting that enables easier communication, understanding, reflection, comparison and refining, building on existing research while fostering harmonization of perspectives for the field. In this context, some noticeable recommendations are: • researchers can profit from the outcomes generated to improve the level of detail provided in their reports; • the collaborative context needs to be widely described, allowing the creation of a better understanding of the surrounding conditions, including relations between individuals, their interconnection as a team, how AR was used, the characteristics of the environment, and others; • the outcomes can help identify limitations and promising functionalities regarding AR, providing opportunities for future work in a technical level; • the insights obtained may also lead to improvements in individual behavior and team collaboration in specific procedures and tasks over longer periods of time. 4 Toolkit for distributed evaluations using AR Given the challenges in evaluating the way remote collaboration occurs, the absence of frameworks and tools, this section describes CAPTURE - Contextual dAta Platform for remoTe aUgmented Reality Evaluation, a first instantiation towards addressing the vision previously described, in particular the need to include more contextual data in the evaluation of the collaborative process (Fig. 2), following the conceptual model [28]. To inform the conceptualization/development, we conducted brainstorm sessions with domain experts (academics, including faculty members and researchers) sharing several years of expertise in HCI, VR/AR, Visualization and remote collaboration, who co-authored multiple publications, and projects on these subjects. Hence, the toolkit must support: • data gathering at distributed locations in synchronous and asynchronous manner; • explicit input on different dimensions of collaboration, following a taxonomy for Collaborative AR [38,39] and an evaluation ontology for remote scenarios [37]; • data collection regarding team interaction, custom logging and registration of interesting events according to the selected scenarios of remote collaboration; • easy instrumentation into remote tools by providing ready to use scripts and prefabs for non-experts in programming, i.e., each process can be configured via visual editors; • modularity to ensure adaptation to different goals; • data storage and aggregation via a centralized server; • post-task analysis through a visualization dashboard. To elaborate, for team-members, the CAPTURE toolkit provides native off-the-shelf modules to support explicit input and data gathering regarding (Figs. 2–4): • individual and team profile: demographic data, knowledge of other collaborators, participants background, emotional state [40], experience with AR and remote tools; • collaborative context: details on the task and the environment, like the number of completion stages, resources available or the amount of persons, movement and noise in the surrounding space; • list of events: task duration, augmented content shared and received, and other relevant occurrences; • pre-defined measures: characteristics associated to the collaborative process, including, but not limited to, easy to communicate or express ideas and the level of spatial presence, enjoyment, mental effort, information understanding, attention allocation or others (Fig. 3 - top). Also, the Microsoft reaction card methodology [41] to have a grasp on team-members reaction towards the tool used for shared understanding (Fig. 3 - bottom); • interaction with the collaborative tool: duration of the collaborative process and specific events, e.g., when creation of content is started or completed, number and type of interactions, frequency of using each feature, as well as captures of the augmented instructions being shared. Regarding pre-defined measures, the aspects of collaboration proposed are the result of carefully survey existing literature to create a list of important topics facing the lack of methodologies and frameworks. This list was presented to the experts, who had an important role in selecting, analyzing and filtering said topics of collaboration by voting about the ones they considered being more relevant. To elaborate, we took inspiration from the questionnaires used by [3,42–46], as well as the works by [11,19,21,22,47–50]. Nevertheless, other aspects of collaboration can be considered according to the evaluation scope due to the inherent flexibility provided by the CAPTURE toolkit implementation, as described below. As for the researcher(s), the toolkit provides native off-the-shelf modules to support explicit input regarding (Fig. 2 - 5): • Study: area of application, research context and study type; • Time: synchronicity, duration and predictability; • Team: distribution, role structure, size, life-span, turnover, multidisciplinarity, technology usage, homogeneity of abilities, and knowledge of others (Fig. 4 - top); • Task: scope and type of task, interdependence, amount of information and movement required to fulfill the task, number of completion stages, resources necessary to achieve the goal (Fig. 4 - bottom); • User Actuation: capacity to passive-view, interact/explore, share/create, as well as level of symmetry; • Communication: structure, mode, intent, frequency and duration; • Environment: amount of noise, level of brightness, number of persons in the environment, weather conditions and resources available; • Notes: interesting events, notes, comments or difficulties, as well as if the goals were achieved and the amount of physical movement conducted by the team-members. At the system level, CAPTURE consists of a Unity Package that can easily be added to existing collaborative solutions in Unity. All data gathered from the different team-members and researcher(s) during collaboration sessions is stored in a central server for post-evaluation analysis through a visualization dashboard (Fig. 5), which allows reviewing the work effort of a particular team or set of teams, as well as compare different tools, if that is the evaluation scope [51]. The modules of the proposed toolkit can be integrated into existing remote tools via visual editors, i.e., with minimal need for programming skills (Fig. 6). It is possible to drag and drop ready to use prefabs and editable scripts into Unity 3D projects, which can be modified according to the evaluation scope in the inspector module. Fig. 6 illustrates the example of the collaborative process script, which researchers can manually edit (set the number of elements, add relevant aspects of collaboration to be assessed, etc.) according to the evaluation scope. This dynamic approach allows researchers to re-use scripts over different evaluation sessions according to the collaborative effort being considered. For development, Unity 3D was used based on C# scripts. Communication between each instance is performed over Wi-Fi through calls to a PHP server. In short, the field needs to have more contextualized evaluation strategies, allowing to learn more regarding how technology address the collaborative process. All of this can support an effort towards systematized data, which may support the proposal of guidelines in the future, resulting from the experience and knowledge accumulated through the analysis from multiple research teams and different technology approaches with contextualized information. This effort will allow to use these recommendations to jump-start the quality of current and novel solutions right from the very beginning of its conceptualization, which have already been proven useful in remote scenarios. 5 User study on a remote maintenance scenario A user study was conducted to compare the collaborative process of distributed teams using two distinct tools when instrumented with CAPTURE: Video Chat and AR-based Annotations. These were proposed following a user-centered approach with partners from the industry sector to probe how AR could provide solutions to support their collaborative needs. 5.1 Experimental setup To create a common ground between distributed team- members, two distinct methods were provided: a video chat tool and an AR-based annotation tool. Next, a brief description of the main features of each tool is provided. To clarify, the hardware used was the same for both methods, only the characteristics of the tool changed. Also, both tools were developed using the Unity 3D game engine, based on C# scripts. Communication was provided over Wi-Fi through WebRTC calls to a dedicated server. To place the augmented content in the real-world environment, we used the Vuforia library. 5.1.1 Video chat tool The first method uses video chat features to provide support (Fig. 7). On-site participants can point a handheld device to the situation context, which is shared though live video stream with the remote expert. In this context, the face of the expert is visible at all times, while the on-site participant may change between showing the task context or his face using the back and front cameras of the device. Besides, team-members can share text messages using the chat to ensure important messages are kept visible. Using these features, team-members may communicate and discuss the content being captured to express the main difficulties, identify areas of interest or the remote expert to inform where to act and what to do. 5.1.2 AR-based annotation tool The second method uses AR-based annotations as additional layers of information (Fig. 8). On-site participants can point a handheld device to capture the situation context. Using audio communication and annotation features like drawing, placing pre-defined shapes or notes, as well as sorting annotations, the participant can edit the capture to illustrate difficulties, identify specific areas of interest or indicate questions. Then, the capture is sent to the remote expert to suggest instructions accordingly i.e., inform where to act, and what to do, using similar annotation features. Afterwards, the on-site participant receives the annotations. The handheld device can be placed on top of a surface to follow the instructions in a hands-free setting. At any time, it can be picked up to perform an augmentation of the annotations, by re-aligning with the real world. 5.2 Experimental design A within-group experimental design was used. The null hypothesis (H0) considered was that the two experimental conditions are equally usable and acceptable to conduct the selected maintenance tasks. The independent variable was the information display method provided during the collaborative process, with two levels corresponding to the experimental conditions: C1 — Video Chat and C2 — AR-based Annotations. For both experimental conditions, the tools used provided a similar level of user actuation for both team-members, having identical features to view (C1 and C2), create, share and interact with augmented content (C2). Performance measures and participants’ opinion were the dependent variables. Participants’ demographic data, as well as previous experience with AR and collaborative tools were registered as secondary variables. 5.3 Tasks We focused on a case study where an on-site participant using a handheld device had to perform a maintenance procedure while being assisted from a remote expert using a computer. The tasks require accomplishing the following steps (Fig. 9): 1- replace interconnected components, 2- plug and unplug some energy modules, 3- remove a specific sensor, as well as 4- integrate new components into the equipment. For each condition, different tasks were used to minimize bias, i.e., learning effect. Nevertheless, we defined these tasks based on feedback from our industry partners regarding their usual work activities and needs, while ensuring a similar level of difficulty and resources. Each task was a defined-problem with 4 completion stages, forcing team-members to communicate in a continuous way while acting alternately (reciprocal interdependence) in an indoor environment with controlled illumination conditions and reduced noise. Besides the participants and researchers, no other individuals were present. The on-site participant needed to use different hand tools to perform the procedures, although low physical movement was required. 5.4 Measurements All data was collected through the CAPTURE toolkit for all conditions, including standard measures found in literature like task performance based on the overall total time, i.e., time needed to complete the tasks, answer to questionnaires and participation in a brief interview, as well as task time, i.e., time required for successfully fulfill the task in a collaborative manner. Besides, novel measures, taking advantage of the toolkit off-the-shelf modules, i.e., information on selected dimensions of collaboration (e.g., time, team; task; user actuation, communication, environment); the overview of the collaborative process (e.g., easy to communicate or express ideas, level of spatial presence, enjoyment, mental effort, information understanding and attention allocation) at the end of the tasks; participants emotional state, before and after the task fulfillment; participants preferences and opinion, also at the end. Hence, the toolkit was integrated into an existing video chat tool, as well as an AR-based tool [52] using stabilized annotations, following prior work with partners from the Industry sector. 5.5 Procedure Participants were instructed on the experimental setup, the tasks and gave their informed consent. Then, they were introduced to both tools and a time for adaptation was provided. Participants would act as on-site technicians with condition C1 and then C2, always in this order, while a researcher was the remote counterpart to ensure the instructions were correctly transmitted. We used this approach to facilitate collaboration, as having participants also act as the remote counterpart would add an additional level of complexity, which we believe was not necessary. Since this role was ensured by one of the researchers, we recognize that it is not the same as having a participant, but still allows to have a granular view of the work effort, since not all collaborative processes are created equal. Hence, the researcher also followed the same procedure during the evaluation. We argue that the data collected from this role convey a variability in the way collaboration occurred and in what works or not, depending on the team-members, which demonstrates the ability of the measures used to have some granularity in the evaluation of how the collaborative process took place. Participants started with a demographic questionnaire. In the next stage, they completed the maintenance tasks while observed by a researcher who assisted them if necessary, and registered any relevant event. Immediately after completing the tasks using the conditions, participants answered a post-study questionnaire regarding the collaborative process, as well as their preferences towards the tool used. Then, a small interview was conducted to understand participants’ opinion regarding their collaboration with each condition. The data collection was conducted under the guidelines of the Declaration of Helsinki. Also, all measures were followed to ensure a COVID-19 safe environment during each session of the user study. 5.6 Participants We recruited 26 participants (9 female - 34.7%), whose ages ranged from 20 to 63 years old (M = 33.1, SD = 11.7). Participants had various professions, e.g., Master and Ph.D. students, Researchers and Faculty members from different fields, as well as Software Engineers, Front-End Developers and an Assembly Line Operator. With respect to individual and team profile, 14 participants had prior experience with AR and 24 with collaborative tools. With the exception of 1 team, all collaborators had knowledge of each other prior to the study. 6 Results and discussion This section presents and discusses the main results obtained from the analysis of the data collected through CAPTURE. 6.1 Overall total time and task time As for the total duration, sessions lasted 32 min on average (SD = 3.10) using condition C1 and 28 min on average (SD = 3.03) using condition C2 (Fig. 10). Regarding task duration, it lasted 16 min on average (SD = 2.68) using condition C1 and 12 min on average (SD = 2.66) using condition C2. Therefore, participants were quicker on average to perform the tasks when using condition C2, despite having a higher data variability when compared to condition C1. 6.2 Overview of the collaborative process Regarding condition C1, participants rated the collaborative process (Likert-type scale: 1 — Low; 7 — High) as following (Fig. 11 — top): express ideas (median = 4.5), attentional allocation (median = 4), information understanding (median = 5), mental effort (median = 5), enjoyment (median = 4), communication (median = 5), spatial presence (median = 5.5). As for condition C2, participants rated the collaborative process as following (Fig. 11 - bottom): express ideas (median = 6), attentional allocation (median = 7), information understanding (median = 7), mental effort (median = 2), enjoyment (median = 6), communication (median = 6), spatial presence (median = 5). Hence, it is possible to understand that for the majority of aspects of collaboration, i.e., easy to share ideas properly, level of attention allocation, level of information understanding, level of enjoyment and easy to communicate, condition C2 was rated higher by the participants. Regarding the level of mental effort, participants rated higher condition C1, possibly due to the diminished level of attentional allocation this condition had, which lead to some communication arguing in order to understand where to perform some activities. Therefore, these results suggest that the AR-based annotation tool was better in such aspects of collaboration when compared to the video alternative. In contrast, for condition C1 the level of spatial presence was higher. This might be associated to the fact that this condition supported live video sharing between team-members, which may have an impact on participants feeling of togetherness with their collaborative counterparts, since it was possible to see the remote expert at all times during the task duration. On the other side, condition C2 provided stabilized AR-based annotations on top of captures/images of the task context. This condition did not allow to see the remote expert during the task procedures, which may have affected participants reaction towards the level of spatial presence, although not with any major difference. In this context, a smaller data variability can also be observed for easy to share ideas properly, level of information understanding, level of mental effort, easy to communicate and level of spatial presence, when analyzing the box plots of condition C1 and C2, as illustrated by Fig. 11. Through the visualization dashboard of the CAPTURE toolkit, it is possible to analyze the collaborative process at the end of an evaluation session for a specific team, or set of different teams. In particular, it is possible to analyze the aspects of collaboration obtained from the use of different tools for the elements of the same team, as explored in this study, which is illustrated in Fig. 12, through a random selection. Naturally, following the results presented above, when using condition C2, the team had a better collaborative performance when compared to the results of condition C1. Nevertheless, by analyzing the elements of each team individually, such type of visualization allows to identify aspects of collaboration that could be useful to improve over time, or that may be relevant to update in the collaborative tool being used. For example, when using condition C2, the on-site participant rated the level of spatial presence lower. This fact may suggest that in order to improve the feeling of togetherness, the AR-based annotation tool might benefit from including video sharing in its features. 6.3 Participants preferences and opinion With respect to participants experience with the tools, 44 reaction cards were selected to characterize condition C1, including 5 neutral, 9 negative and 30 with positive meaning. Likewise, 46 were selected to characterize condition C2, including 3 neutral, 1 negative and 40 with positive meaning (Fig. 13) . The following top 10 reaction cards represent participants most selected expressions to characterize each condition: C1 — accessible, collaborative, helpful, flexible, simplistic, familiar, usable, unrefined, expected and time-consuming; C2 — helpful, empowering, collaborative, appealing, easy-to-use, engaging, flexible, novel, innovative and advanced. However, when analyzing participants emotional state, collected before and after the tasks, a clearer perspective is attained. To elaborate, regarding condition C1, participants emotional state before the study varied among joy (11 out of 26), surprise (3 out of 26), excitement (8 out of 26) and contempt (4 out of 26) (Fig. 14 - top). Then, after the study, it varied among joy (7 out of 26), surprise (1 out of 26), excitement (1 out of 26) and contempt (17 out of 26) (Fig. 14 - top). As for condition C2, participants emotional state before the study varied among joy (12 out of 26), surprise (3 out of 26), excitement (7 out of 26) and contempt (4 out of 26) (Fig. 14 - bottom). Then, after the study, it varied among joy (6 out of 26), surprise (4 out of 26) and excitement (6 out of 26) (Fig. 14 - bottom). Hence, it is possible to verify that for condition C1, there was a decrease in the number of participants feeling joy, surprise and excitement at the end of the study, which lead to a significant rise associated to the emotional state of contempt. Contrarily, regarding condition C2, there were no occurrences of contempt, while joy and surprise had higher number of participants expressing those feelings. As for excitement, although the number of participants that reported such feeling is lower, it is very close to the values reported at the beginning of the study. As such, condition C2 presents significant higher values for emotions correlated with positive connotation, e.g., joy, surprise and excitement when compared to condition C1, which only presents a higher value for contempt (neutral connotation). In addition, Fig. 15 presents participants satisfaction regarding the collaborative tools used through a box plot representation, which illustrates clearly that condition C2 was preferred when compared to condition C1, following the analysis statement of participants emotional state. The interviews conducted at the end of the study also emphasize that the majority of participants preferred condition C2, since it enabled seeing non-verbal cues aligned with the task context, which they mentioned contributed to express themselves better through the augmented features, while also having a greater perception of where to perform a given action. Next, some comments by the participants are presented to provide additional context to the statement previously made: • regarding the level of attentional allocation and information understanding with condition C1, one participant emphasized the following: ”although the video tool is more familiar and quicker to start collaborate, when I needed to express myself about the equipment components or the tools I should use, that’s when I started noticing the lack of support. This lead me to repeat the same ideas in different ways to properly explain the desired goal, and the same also happened to my colleague”; • as for the level of mental effort with condition C1, another participant outlined that ”besides the use of voice, the absence of support to highlight an area of interest or express myself when using the video tool makes me prefer the use of AR-based annotations, in particular for more complex procedures, even though it was a novelty to me and I needed to learn and adapt to it”; • concerning easy to share ideas properly with condition C2, a different participant reported that “the use of AR-based annotations allowed me to interact more naturally, while also better comprehend where to perform a given action”. In regards to attentional allocation with condition C2, the same participant commented that “having the handheld device displaying the annotations near the equipment, allowed me to perform the maintenance tasks easily when compared to the video, since in this last there was no content besides the text chat I could use to remember what to do, or to confirm my actions”; • with respect to the level of mental effort and spatial presence, an additional participant mentioned the following regarding condition C2: “since I’m familiarized with remote video tools in my daily activities, I was expecting that the absence of video would affect my collaboration with the remote expert. Nevertheless, since the AR-based tool focused more on the task itself, I was engaged in such a way, that not viewing the expert did not affect me at all”. Regarding additional comments/suggestions, some participants (7 out of 26) emphasize condition C2 could help create documentation for scenarios where identical tasks may occur. Actually, the AR-based tool already supports revisiting existing annotations, a feature identified as useful by industry partners during the design of a remote maintenance support platform [52]. Nevertheless, for the case study reported, such feature was not made available, since the tasks used did not imply repeating particular activities. Another topic raised by some participants (18 out of 26) was the possible inclusion of Head Mounted Displays (HMDs), which they consider may further enhance their performance, since it supports a hands-free setting. Likewise, the AR-based tool used already supports HMD, as described in prior work [53]. Since our goal was not to compare different set-ups, we decided not to include such type of device at this moment. In addition, 4 out of 26 participants referred to possible limitations regarding the use of mobile devices as means to answer a questionnaire, since they were used to doing so on computers. They reported that for questions using drop-down menus and multiple-choice options it was easy to select the desired answer. As for the ones requiring text entry, the process could be slower and tiring. Yet, they understood the usefulness/relevance due to the fact of monitoring real-life scenarios. For example, CAPTURE is ready to be used in industry contexts, in which most technicians may find themselves without a computer. Furthermore, having these target users answering relevant questions after the tasks provides more useful insights than having them filling the questionnaires at the end of a workday on a more suitable device for writing. In this vein, we argue a compromise was required and that the solution provided takes these constraints into consideration. Nevertheless, this also opens new opportunities to propose novel forms of providing input in such scenarios. Furthermore, following the possible inclusion of HMDs in such scenarios and their similar (or even worst) capacity to answer questionnaires, this is also an open topic. Although it is possible to create text with such devices, e.g., hand interaction, literature shows it may not be the best approach. An alternative may be to use a keyboard linked to the HMD device just for answering the existing questionnaires, or perhaps, support voice/sound data collection, and later convert that into text, either via automatic or semi-automatic means. Nevertheless, more than ease of filling out questionnaires, what really matters is evaluating and monitoring collaboration in the best way possible. Therefore, as mentioned, on-the-fly feedback is essential. The choice of the most adequate input form for collecting information from the questionnaires may depend on the hardware available/being used, or on the person designing the study. Overall, the idea is that the toolkit is flexible enough to support all these options. Last, a reduced number of participants (5 out of 26) suggested viewing the remote expert, not as a basic feature, but as an option for specific cases which may help increase empathy and trust during the collaboration process. 6.4 Final remarks To summarize the added value of our proposal, and how it compares to existing approaches, the conceptual framework instantiated through the CAPTURE toolkit allows to retrieve additional amounts of contextual data, as well as selected aspects of collaboration according to the evaluation scope, (usually ignored in existing evaluations found in literature), for more comprehensive analysis using the visualization dashboard. Another aspect that must be emphasized, is the capacity to adapt to the available data collection instruments. Although self-report was used to gather the emotional response, CAPTURE can adapt to support the inclusion of external sensors (e.g., biomedical devices), if necessary for different scenarios. With all things considered, it is possible to better understand the phenomenon, i.e., recognize when selected aspects of collaboration affect the work effort. By having these insights, it is possible to more easily identify key issues that need to be tackled to ensure a proper shared understanding is attained by distributed team-members in future sections of remote collaboration. By doing so, the research community can evolve from simple evaluations on how technology works, to more complex evaluations aimed to capture a better perspective on the different factors of collaboration supported by AR, which may lead to a more effective collaborative process over time. Hence, we have shown that a better characterization of the collaborative process can be successfully used to provide an additional perspective on the nuances of remote collaboration mediated by AR, which without contextual data would not be possible. Altogether, due to the flexibility and range of the proposed conceptual model, the instrumentation through the CAPTURE toolkit establishes itself as a general-purpose evaluation approach, providing data that otherwise would be difficult to obtain and analyze. While we must be prudent with generalizing our findings, we expect our insights to be valuable for future reproduction in other domains beside maintenance context. To finish, the continuous observation of contextual data in other tools and with other users may allow, in the future, to create guidelines, supported by experimental data, which can guide the initial development of novel collaborative solutions. 7 Conclusions and future work As a contribution, a critical analysis on collaborative user studies mediated by AR is presented, showing that most studies rely on single-user methods, not adapted to collaborative scenarios and that existing frameworks are not well suited to characterize how collaboration occurs. Motivated by these, we presented a conceptual framework to support researchers in designing better evaluations based on retrieving contextualized data for more comprehensive analysis of the collaborative process. To instantiate this framework, the CAPTURE toolkit was proposed to assist with more user centric evaluations, allowing to easily analyze the collaborative process of a particular team or comparison between a set of teams or different tools. During the analysis of the results obtained, it was possible to realize that the contextual data allowed us to understand participants ease to communicate and to share ideas, and the level of attention allocation, spatial presence or others. Also, measure emotional state, and reaction towards the tools used. In this vein, participants felt AR supports more natural interaction, which contributes to increase empathy, interest and collaboration. By having a grasp on these aspects, typically not reported in the literature, but which are very informative/valuable to understand where the focus of the work, it is possible to better define how research should progress and how the tools can evolve. Hence, conduct comparative analysis of distributed teams may benefit researchers in better understanding the collaborative phenomenon, when compared to how its being currently reported, designing novel methods and improve the collaborative effort. This reinforces, once again, the need to evolve and make these experiences more contextualized and better reported, so that the research community can move into a phase of producing guidelines for remote scenarios supported by AR. Later, we intend to support data/voice collection, both during the collaborative process among the remote team members, and as an additional data input during the post-task assessment. We envision it may be relevant for researchers having metrics that can be automatically calculated and brought for analysis through an updated version of the visualization dashboard, e.g., characteristics of the dialog, during synchronous collaboration (e.g., number of questions, interruptions, occurrences of specific words). One possible way being considered is supporting some form of synchronization so that all user-related events are synchronized with video/voice streams captured in the study. Furthermore, we plan to share the toolkit with the research community, which may elicit newer data gathering/visualization requirements. Also, conduct field studies with experts from the industry sector to demonstrate the framework use in real scenarios. Last, pursue the creation of guidelines to elicit more complete evaluations in such scenarios. CRediT authorship contribution statement Bernardo Marques: Conceptualization, Methodology, Software, Formal analysis, Investigation, Resources, Writing – original draft, Writing – review & editing, Visualization, Project administration, Funding acquisition. Samuel Silva: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Visualization. António Teixeira: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Visualization. Paulo Dias: Conceptualization, Resources, Writing – original draft, Writing – review & editing, Visualization, Supervision, Funding acquisition. Beatriz Sousa Santos: Conceptualization, Writing – original draft, Writing – review & editing, Visualization, Supervision, Funding acquisition. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise, in particular Pedro Cruz and António Rocha, as well as Andreia Santos, Francisco Marques and Júlia Santos. This research was developed in the scope of the PhD grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT, in the context of the project [UID/CEC/00127/2019]. The user study of this research was possible due to an industrial collaboration under the Smart Green Homes Project [POCI-01-0247-FEDER-007678], a co-promotion between Bosch Termotecnologia S.A. and the University of Aveiro. It is financed by Portugal 2020 under COMPETE 2020, and by the European Regional Development Fund. All authors have read and agreed to the published version of the manuscript. References [1] Kim K. de Melo C.M. Norouzi N. Bruder G. Welch G.F. Reducing task load with an embodied intelligent virtual assistant for improved performance in collaborative decision making Conference on Virtual Reality and 3D User Interfaces 2020 IEEE VR 529 538 Kim [ K], de Melo [ CM], Norouzi [ N], Bruder [ G], Welch [ GF]. Reducing task load with an embodied intelligent virtual assistant for improved performance in collaborative decision making. Conference on Virtual Reality and 3D User Interfaces, IEEE VR 2020:529–538. [2] Kim K. Billinghurst M. Bruder G. Duh H.B. Welch G.F. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008–2017) IEEE Trans Vis Comput Graphics 24 2018 2947 2962 Kim [ K], Billinghurst [ M], Bruder [ G], Duh [ HB], Welch [ GF]. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017). IEEE Transactions on Visualization and Computer Graphics 2018a;24:2947–2962. [3] Kim S. Billinghurst M. Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration Comput Support Coop Work CSCW 27 3–6 2018 569 607 Kim [ S], Billinghurst [ M], Lee [ G]. The effect of collaboration styles and view independence on video-mediated remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal 2018b;27(3-6):569–607. [4] Gervasi R. Mastrogiacomo L. Franceschini F. A conceptual framework to evaluate human-robot collaboration Int J Adv Manuf Technol 108 3 2020 841 865 Gervasi [ R], Mastrogiacomo [ L], Franceschini [ F]. A conceptual framework to evaluate human-robot collaboration. The International Journal of Advanced Manufacturing Technology 2020;108(3):841–865. [5] Lukosch S. Billinghurst M. Alem L. Kiyokawa K. Collaboration in augmented reality Comput Support Coop Work CSCW 24 2015 515 525 Lukosch [ S], Billinghurst [ M], Alem [ L], Kiyokawa [ K]. Collaboration in Augmented Reality. In: Computer Supported Cooperative Work, CSCW 2015; vol. 24. 2015, p. 515–525. [6] Schneider M, Rambach J, Stricker D. Augmented reality based on edge computing using the example of remote live support, In: 2017 IEEE International Conference on Industrial Technology, 2017, pp. 1277–1282. [7] Billinghurst M. Clark A. Lee G. A survey of augmented reality Found Trends Human Computer Interact 8 2015 73 272 Billinghurst [ M], Clark [ A], Lee [ G]. A Survey of Augmented Reality. Foundations and Trends in HumanComputer Interaction 2015;8:73–272. [8] Jalo H, Pirkkalainen H, Torro O, Kärkkäinen H, Puhto J, Kankaanpää T. How Can Collaborative Augmented Reality Support Operative Work in the Facility Management Industry? In: Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, 2018, pp. 41–51. [9] Ens B. Lanir J. Tang A. Bateman S. Lee G. Piumsomboon T. Billinghurst M. Revisiting collaboration through mixed reality: The evolution of groupware Int J Human Computer Stud 131 2019 81 98 Ens [ B], Lanir [ J], Tang [ A], Bateman [ S], Lee [ G], Piumsomboon [ T], et al. Revisiting Collaboration through Mixed Reality: The Evolution of Groupware. International Journal of Human-Computer Studies 2019;131:81–98. [10] Wang P. Bai X. Billinghurst M. Zhang S. Zhang X. Wang S. He W. Yan Y. Ji H. Ar/MR remote collaboration on physical tasks: A review Robot Comput-Integr Manuf 72 2021 102071 Wang [ P], Bai [ X], Billinghurst [ M], Zhang [ S], Zhang [ X], Wang [ S], et al. Ar/mr remote collaboration on physical tasks: A review. Robotics and Computer-Integrated Manufacturing 2021;72:102071. [11] Aschenbrenner D, Rojkov M, Leutert F, Verlinden J, Lukosch S, Latoschik M, Schilling K. Comparing Different Augmented Reality Support Applications for Cooperative Repair of an Industrial Robot, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2018, pp. 69–74. [12] Bottani E. Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade IISE Trans 51 3 2019 284 310 Bottani [ E], Vignali [ G]. Augmented reality technology in the manufacturing industry: A review of the last decade. IISE Transactions 2019;51(3):284–310. [13] Wang X. Ong S. Nee A. A comprehensive survey of augmented reality assembly research Adv Manuf 4 1 2016 1 22 Wang [ X], Ong [ SK], Nee [ AYC]. A comprehensive survey of augmented reality assembly research. Advances in Manufacturing 2016;4(1):1–22. [14] Hall M, McMahon C, Bermell-Garcia P, Johansson A, Ravindranath R. Capturing synchronous collaborative design activities: A state-of-the-art technology review, In: Proceedings of International Design Conference, DESIGN 2018, 2018, pp. 347–358. [15] Lee G. Kang H. Lee J. Han J. A user study on view-sharing techniques for one-to-many mixed reality collaborations IEEE Conference on Virtual Reality and 3D User Interfaces 2020 IEEE VR 343 352 Lee [ G], Kang [ H], Lee [ J], Han [ J]. A user study on view-sharing techniques for one-to-many mixed reality collaborations. IEEE Conference on Virtual Reality and 3D User Interfaces, IEEE VR 2020;:343–352. [16] Ludwig T. Stickel O. Tolmie P. Sellmer M. Share-IT: Ad hoc remote troubleshooting through augmented reality Comput Support Coop Work (CSCW) 30 1 2021 119 167 Ludwig [ T], Stickel [ O], Tolmie [ P], Sellmer [ M]. shARe-IT: Ad hoc Remote Troubleshooting through Augmented Reality. Computer Supported Cooperative Work (CSCW) 2021;30(1):119–167. [17] Gurevich P. Lanir J. Cohen B. Design and implementation of TeleAdvisor: a projection-based augmented reality system for remote collaboration Comput Support Coop Work (CSCW) 24 6 2015 527 562 Gurevich [ P], Lanir [ J], Cohen [ B]. Design and implementation of teleadvisor: a projection-based augmented reality system for remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal 2015;24(6):527–562. [18] Zigart T, Schlund S. Evaluation of Augmented Reality Technologies in Manufacturing - A Literature Review, In: Proceedings of the AHFE 2020 International Conference on Human Factors and Ergonomics, 2020, pp. 75–82. [19] Fakourfar O, Ta K, Tang R, Bateman S, Tang A. Stabilized Annotations for Mobile Remote Assistance, In: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 2016, pp. 1548–1560. [20] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration KSII Trans Int Inf Syst 12 12 2018 6034 6056 Kim [ S], Billinghurst [ M], Lee [ C], Lee [ G]. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet and Information Systems 2018c;12(12):6034–6056. [21] Kim S, Lee G, Huang W, Kim H, Woo W, Billinghurst M. Evaluating the Combination of Visual Communication Cues for HMD-Based Mixed Reality Remote Collaboration, In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 2019, pp. 1–13. [22] Kim S. Lee G. Billinghurst M. Huang W. The combination of visual communication cues in mixed reality remote collaboration J Multimod User Interfaces 14 4 2020 321 335 Kim [ S], Lee [ G], Billinghurst [ M], Huang [ W]. The combination of visual communication cues in mixed reality remote collaboration. Journal on Multimodal User Interfaces 2020;14(4):321–335. [23] Neale D.C. Carroll J.M. Rosson M.B. Evaluating computer-supported cooperative work: Models and frameworks Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work CSCW ’04 2004 112 121 Neale [ DC], Carroll [ JM], Rosson [ MB]. Evaluating computer-supported cooperative work: Models and frameworks. In: Proceedings of the 2004 ACM Conference on Computer Supported Cooperative Work. CSCW 04; 2004, p. 112121. [24] Hamadache K. Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation Groupware: Design, Implementation, and Use 2009 206 221 Hamadache [ K], Lancieri [ L]. Strategies and taxonomy, tailoring your CSCW evaluation. In: Groupware: Design, Implementation, and Use. 2009, p. 206–221. [25] Antunes P. Herskovic V. Ochoa S.F. Pino J.A. Reviewing the quality of awareness support in collaborative applications J Syst Softw 89 2014 146 169 Antunes [ P], Herskovic [ V], Ochoa [ SF], Pino [ JA]. Reviewing the quality of awareness support in collaborative applications. Journal of Systems and Software 2014;89:146–169. [26] Merino L, Schwarzl M, Kraus M, Sedlmair M, Schmalstieg D, Weiskopf D. Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009–2019), In: IEEE International Symposium on Mixed and Augmented Reality, ISMAR, 2020. [27] Belen R.A.J. Nguyen H. Filonik D. Favero D.D. Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018 AIMS Electron Electr Eng 3 2019 181 Belen [ RAJ], Nguyen [ H], Filonik [ D], Favero [ DD], Bednarz [ T]. A systematic review of the current state of collaborative mixed reality technologies: 20132018. AIMS Electronics and Electrical Engineering 2019;3:181. [28] Marques B, Teixeira A, Silva S, Alves Ja, Dias P, Santos BS. A Conceptual Model for Data Collection and Analysis for AR-based Remote Collaboration Evaluation, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2020, pp. 1–2. [29] Marques B. Teixeira A. Silva S. ao Alves J. Dias P. Santos B.S. A critical analysis on remote collaboration mediated by augmented reality: Making a case for improved characterization and evaluation of the collaborative process Comput Graph 2021 1 17 Marques [ B], Teixeira [ A], Silva [ S], Alves [ J], Dias [ P], Santos [ BS]. A critical analysis on remote collaboration mediated by augmented reality: Making a case for improved characterization and evaluation of the collaborative process. Computers & Graphics 2021a;:1–17. [30] Bai Z. Blackwell A.F. Analytic review of usability evaluation in ISMAR Interact Comput 24 6 2012 450 460 Bai [ Z], Blackwell [ AF]. Analytic review of usability evaluation in ISMAR. Interacting with Computers 2012;24(6):450 – 460. [31] Ratcliffe J, Soave F, Bryan-Kinns N, Tokarchuk L, Farkhatdinov I. Extended Reality (XR) Remote Research: a Survey of Drawbacks and Opportunities, In: CHI Conference on Human Factors in Computing Systems, 2021, pp. 1–13. [32] Dey A. Billinghurst M. Lindeman R.W. Swan J.E. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014 Front Robotics AI 5 2018 37 Dey [ A], Billinghurst [ M], Lindeman [ RW], Swan [ JE]. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014. Frontiers in Robotics and AI 2018;5:37. [33] Anton D. Kurillo G. Bajcsy R. User experience and interaction performance in 2D/3D telecollaboration Future Gener Comput Syst 82 2018 77 88 Anton [ D], Kurillo [ G], Bajcsy [ R]. User experience and interaction performance in 2d/3d telecollaboration. Future Generation Computer Systems 2018;82:77–88. [34] Piumsomboon T. Lee G. Ens B. Thomas B. Billinghurst M. Superman vs giant: A study on spatial perception for a multi-scale mixed reality flying telepresence interface IEEE Trans Vis Comput Graphics 24 11 2018 2974 2982 Piumsomboon [ T], Lee [ G], Ens [ B], Thomas [ B], Billinghurst [ M]. Superman vs giant: A study on spatial perception for a multi-scale mixed reality flying telepresence interface. IEEE Transactions on Visualization and Computer Graphics 2018;24(11):2974–2982. [35] Araujo R.M. Santoro F.M. Borges M.R. A conceptual framework for designing and conducting groupware evaluations Int J Comput Appl Technol 19 3 2004 139 150 Araujo [ RM], Santoro [ FM], Borges [ MRS]. A conceptual framework for designing and conducting groupware evaluations. International Journal of Computer Applications in Technology 2004;19(3):139150. [36] Araujo R, Santoro F, Borges M. The CSCW lab ontology for groupware evaluation, In: 8th International Conference on Computer Supported Cooperative Work in Design, 2, 2004, pp. 148–153. [37] Marques B, Silva S, Dias P, Santos BS. An Ontology for Evaluation of Remote Collaboration using Augmented Reality, In: European Conference on Computer-Supported Cooperative Work, ECSCW: the International Venue on Practice-Centred Computing on the Design of Cooperation Technologies - Posters & Demos, Reports of the European Society for Socially Embedded Technologies, 2021, pp. 1–8. [38] Marques B. Silva S. Alves J. Araujo T. Dias P. Santos B.S. A conceptual model and taxonomy for collaborative augmented reality IEEE Trans Vis Comput Graphics 2021 1 18 Marques [ B], Silva [ SS], Alves [ J], Araujo [ T], Dias [ P], Santos [ BS]. A conceptual model and taxonomy for collaborative augmented reality. IEEE Transactions on Visualization & Computer Graphics 2021c;:1–18. [39] Marques B. Ara T. Silva S. Dias P. Visually exploring a collaborative augmented reality taxonomy Information Visualization, IV 2021 1 6 Marques [ B], Ara [ T], Silva [ S], Dias [ P]. Visually exploring a Collaborative Augmented Reality Taxonomy. In: Information Visualization, IV. 2021d, p. 1–6. [40] Izard C.E. Basic emotions, natural kinds, emotion schemas, and a new paradigm Perspect Psychol Sci 2 3 2007 260 280 Izard [ CE]. Basic Emotions, Natural Kinds, Emotion Schemas, and a New Paradigm. Perspectives on Psychological Science 2007;2(3):260–280. [41] Barnum C.M. Usability Testing Essentials: Ready, Set...Test! first ed. 2010 Morgan Kaufmann Publishers Inc. San Francisco, CA, USA Barnum [ CM]. Usability Testing Essentials: Ready, Set...Test! 1st ed.; San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.; 2010. [42] Gutwin C. Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware ACM Trans on Computer Human Int 6 3 1999 243 281 Gutwin [ C], Greenberg [ S]. The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Transactions on Computer-Human Interaction 1999;6(3):243281. [43] Nilsson S, Johansson B, Jonsson A. Using AR to support cross-organisational collaboration in dynamic tasks, In: IEEE International Symposium on Mixed and Augmented Reality, ISMAR, 2009, pp. 3–12. [44] Kim S, Lee G, Sakata N, Billinghurst M. Improving co-presence with augmented visual communication cues for sharing experience through video conference, In: ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings, 2014, pp. 83–92. [45] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Trans Int Inf Syst 12 2018 6034 6056 Kim [ S], Billinghurst [ M], Lee [ C], Lee [ G]. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet & Information Systems 2018d;12:6034–6056. [46] Huang W. Kim S. Billinghurst M. Alem L. Sharing hand gesture and sketch cues in remote collaboration J Vis Commun Image Represent 58 2019 428 438 Huang [ W], Kim [ S], Billinghurst [ M], Alem [ L]. Sharing hand gesture and sketch cues in remote collaboration. Journal of Visual Communication and Image Representation 2019;58:428–438. [47] Patel H. Pettitt M. Wilson J.R. Factors of collaborative working: A framework for a collaboration model Applied Ergon 43 1 2012 1 26 Patel [ H], Pettitt [ M], Wilson [ JR]. Factors of collaborative working: A framework for a collaboration model. Applied ergonomics 2012;43(1):1–26. [48] Johnson S, Gibson M, Mutlu B. Handheld or Handsfree? Remote Collaboration via Lightweight Head-Mounted Displays and Handheld Devices, In: Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, 2015, pp. 1825–1836. [49] Teo T, Lee G, Billinghurst M, Adcock M. Investigating the use of different visual cues to improve social presence within a 360 mixed reality remote collaboration, In: ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry, 2019. [50] Piumsomboon T. Dey A. Ens B. Lee G. Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality Front Robotics AI 6 2019 Piumsomboon [ T], Dey [ A], Ens [ B], Lee [ G], Billinghurst [ M]. The effects of sharing awareness cues in collaborative mixed reality. Frontiers Robotics AI 2019;6. [51] Marques B, Silva S, Dias P, Santos BS. A Toolkit to Facilitate Evaluation and Characterization of the Collaborative Process in Scenarios of Remote Assistance Supported by AR, In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct), 2021, pp. 336–337. [52] Marques B, Silva S, Rocha A, Dias P, Santos BS. Remote Asynchronous Collaboration in Maintenance scenarios using Augmented Reality and Annotations, In: IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), IEEE VR 2021, 2021, pp. 567–568. [53] Madeira T. Marques B. Alves J.a. Dias P. Santos B.S. Exploring annotations and hand tracking in augmented reality for remote collaboration Human Systems Engineering and Design III 2021 Springer International Publishing 83 89 Madeira [ T], Marques [ B], Alves [ J], Dias [ P], Santos [ BS]. Exploring annotations and hand tracking in augmented reality for remote collaboration. In: Human Systems Engineering and Design III. Springer International Publishing; 2021, p. 83–89. "
    },
    {
        "doc_title": "A critical analysis on remote collaboration mediated by Augmented Reality: Making a case for improved characterization and evaluation of the collaborative process",
        "doc_scopus_id": "85113835900",
        "doc_doi": "10.1016/j.cag.2021.08.006",
        "doc_eid": "2-s2.0-85113835900",
        "doc_date": "2022-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Collaboration process",
            "Collaborative process",
            "Comprehensive evaluation",
            "Design and Development",
            "Enabling technologies",
            "In-depth knowledge",
            "Mixed and augmented realities",
            "Remote collaboration"
        ],
        "doc_abstract": "© 2021 Elsevier LtdRemote Collaboration mediated by Mixed and Augmented Reality (MR/AR) shows great potential in scenarios where physically distributed collaborators need to establish a common ground to achieve a shared goal. So far, most research efforts have been devoted to creating the enabling technology, overcoming engineering hurdles and proposing methods to support its design and development. To contribute to more in-depth knowledge on how remote collaboration occurs through these technologies, it is paramount to understand where the field stands and how characterization and evaluation have been conducted. In this vein, this work reports the results of a literature review which shows that evaluation is frequently performed in ad-hoc manners, i.e., disregarding adapting the evaluation methods to collaborative AR. Most studies rely on single-user methods, which are not suitable for collaborative solutions, falling short of retrieving the necessary amount of contextualized data for more comprehensive evaluations. This suggests minimal support of existing frameworks and a lack of theories and guidelines to guide the characterization of the collaborative process using AR. Then, a critical analysis is presented in which we discuss the maturity of the field and a roadmap of important research actions is proposed, that may help address how to improve the characterization and evaluation of the collaboration process moving forward and, in consequence, improve MR/AR based remote collaboration.",
        "available": true,
        "clean_text": "serial JL 271576 291210 291699 291715 291868 31 Computers & Graphics COMPUTERSGRAPHICS 2021-08-20 2021-08-20 2022-02-25 2022-02-25 2022-06-01T12:50:43 S0097-8493(21)00170-9 S0097849321001709 10.1016/j.cag.2021.08.006 S300 S300.2 FULL-TEXT 2022-06-01T12:26:47.504333Z 0 0 20220201 20220228 2022 2021-08-20T03:46:47.260845Z absattachment articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor highlightsabst orcid primabst pubtype ref specialabst 0097-8493 00978493 true 102 102 C Volume 102 61 619 633 619 633 202202 February 2022 2022-02-01 2022-02-28 2022 Technical Section article fla © 2021 Elsevier Ltd. All rights reserved. ACRITICALANALYSISREMOTECOLLABORATIONMEDIATEDBYAUGMENTEDREALITYMAKINGACASEFORIMPROVEDCHARACTERIZATIONEVALUATIONCOLLABORATIVEPROCESS MARQUES B 1 Introduction 2 Related work on user evaluation in collaborative AR 2.1 Previous surveys including user evaluation information 2.2 Summary 3 Method and overview of recent literature 3.1 Augmented reality vs mixed reality 3.2 Search process 3.3 Analysis process 3.4 Validity limitations 3.5 Results 3.5.1 User studies categorization 3.5.2 Study design 3.5.3 Study type 3.5.4 Task type 3.5.5 Evaluation methods and data type 3.5.6 Participants 3.6 Summary 4 Critical analysis 4.1 Main limitations Limitation 1: partial evaluation Limitation 2: lack of contextual information Limitation 3: failure situations are not contemplated Limitation 4: lack of theories and guidelines Limitation 5: minimal support in existing frameworks Limitation 6: limited reporting of outcomes 4.2 Maturity of the field 5 Charting out a roadmap for the characterization and evaluation of the collaborative process 5.1 Definition of dimensions of collaboration 5.2 Systematization of perspectives for the field 5.3 Creation of new paradigms, architectures and frameworks 5.4 Development of tools for improved data gathering 5.5 New and better outcomes to support the assessment 6 Conclusions and future work CRediT authorship contribution statement Acknowledgments References ALEM 2011 135 148 L RECENTTRENDSMOBILECOLLABORATIVEAUGMENTEDREALITYSYSTEMS HANDSONVIDEOTOWARDSAGESTUREBASEDMOBILEARSYSTEMFORREMOTECOLLABORATION LUKOSCH 2015 515 525 S COMPUTERSUPPORTEDCOOPERATIVEWORKCSCW2015 COLLABORATIONINAUGMENTEDREALITY THOMAS 1996 P CSCWREQUIREMENTSEVALUATION KIM 2018 2947 2962 K KIM 2018 569 607 S KIM 2020 321 335 S KIM 2020 313 319 S ARIAS 2000 84 113 E GRUDIN 2013 J ENCYCLOPEDIAHUMANCOMPUTERINTERACTION BILLINGHURST 2015 73 272 M ENS 2019 81 98 B BRUNO 2019 875 887 F ONG 2008 2707 2742 S WANG 2016 1 22 X PALMARINI 2018 215 228 R BILLINGHURST 2021 1 4 M ALTUG 2016 23 41 Y CHOI 2018 51 66 S BOTTANI 2019 284 310 E VANLOPIK 2020 K BELEN 2019 181 R ELVEZIO 2017 1 2 C BAI 2012 450 460 Z DESOUZACARDOSO 2020 106159 L ROLTGEN 2020 93 100 D FERNANDEZDELAMO 2018 148 155 I JETTER 2018 18 33 J HAMADACHE 2009 206 221 K GROUPWAREDESIGNIMPLEMENTATIONUSE STRATEGIESTAXONOMYTAILORINGYOURCSCWEVALUATION ANTUNES 2014 146 169 P PATEL 2012 1 26 H DUENSER 2008 A SIGGRAPH2008 ASURVEYEVALUATIONTECHNIQUESUSEDINAUGMENTEDREALITYSTUDIES DEY 2018 37 A GUTWIN 1999 243 281 C SPEICHER 2019 1 15 M PROCEEDINGS2019CHICONFERENCEHUMANFACTORSINCOMPUTINGSYSTEMS MIXEDREALITY MILGRAM 1994 282 292 P MILGRAM 1994 1321 1329 P ROKHSARITALEMI 2020 S BAI 2020 1 13 H PIUMSOMBOON 2019 T WANG 2020 P RHEE 2020 T TEO 2019 T PIUMSOMBOON 2019 T WANG 2019 P WALDOW 2019 246 262 K TEO 2018 406 410 T KIM 2018 6034 6056 S GUNTHER 2018 339 344 S PIUMSOMBOON 2018 T RYSKELDIEV 2018 177 185 B HOPPE 2018 328 337 A AKKIL 2018 524 542 D LEE 2017 G SIGGRAPHASIA2017MOBILEGRAPHICSINTERACTIVEAPPLICATIONS MIXEDREALITYCOLLABORATIONTHROUGHSHARINGALIVEPANORAMA KOMIYAMA 2017 R GUREVICH 2015 527 562 P TAIT 2015 563 589 M KIM 2015 1669 1674 S HUANG 2013 70 77 W PECE 2013 1319 1328 F BANNAI 2006 143 154 Y REGENBRECHT 2004 338 354 H ARAUJO 2004 139 150 R RATCLIFFE 2021 1 13 J CHICONFERENCEHUMANFACTORSINCOMPUTINGSYSTEMS EXTENDEDREALITYXRREMOTERESEARCHASURVEYDRAWBACKSOPPORTUNITIES GAINES 1991 3 22 B LALANNE 2009 153 160 D PROCEEDINGS2009INTERNATIONALCONFERENCEMULTIMODALINTERFACES FUSIONENGINESFORMULTIMODALINPUTASURVEY TEIXEIRA 2014 29 A SPEECHAUTOMATAINHEALTHCAREVOICECONTROLLEDMEDICALSURGICALROBOTSCHAPTER1 ACRITICALANALYSISSPEECHBASEDINTERACTIONINHEALTHCAREROBOTSMAKINGACASEFORINCREASEDUSESPEECHINMEDICALASSISTIVEROBOTS BILLINGHURST 2003 M SERENO 2020 1 20 M COLLAZOS 2019 4789 4818 C MEYER 2019 87 97 M AUGSTEIN 2019 27 58 M NICKERSON 2013 336 359 R TERUEL 2017 e1858 M ZOLLMANN 2020 1 20 S CHANDRASEKARAN 1999 20 26 B NOY 2001 1 25 N HERSKOVIC 2007 328 336 V GROUPWAREDESIGNIMPLEMENTATIONUSE EVALUATIONMETHODSFORGROUPWARESYSTEMS PEREIRA 2015 146 157 C PEREIRA 2016 1 196 C DYNAMICEVALUATIONFORREACTIVESCENARIOS MARQUESX2022X619 MARQUESX2022X619X633 MARQUESX2022X619XB MARQUESX2022X619X633XB 2024-02-25T00:00:00.000Z 2024-02-25T00:00:00.000Z © 2021 Elsevier Ltd. All rights reserved. 0 item S0097-8493(21)00170-9 S0097849321001709 10.1016/j.cag.2021.08.006 271576 2022-06-01T12:26:47.504333Z 2022-02-01 2022-02-28 true 1219570 MAIN 15 65990 849 656 IMAGE-WEB-PDF 1 gr3 31653 220 376 gr1 115848 553 567 ga1 true 21692 176 301 gr2 34838 257 376 gr3 7538 128 219 gr1 9119 164 168 ga1 true 8112 128 219 gr2 9782 149 219 gr3 214178 975 1666 gr1 774364 2446 2509 ga1 true 186053 780 1333 gr2 259932 1137 1666 CAG 3400 S0097-8493(21)00170-9 10.1016/j.cag.2021.08.006 Elsevier Ltd Fig. 1 Overview of the main results from the recent literature review on evaluation and AR-supported Remote Collaboration. In the first level are the categories considered for the systematic review, raging among the participants, application areas, collaboration details, study characteristics, task details, adaptation period and evaluation methods. Then, in the outer ring, the detailed topics of interest for each category are presented, respectively. For each, the number of publications covering it is illustrated, following the literature review analysis. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 2 Positioning of Remote Collaboration mediated by AR between the Replication and Empiricism phases of the BRETAM model. Inspired by [95]. Fig. 3 Roadmap overview of the main topics that should be addressed regarding remote collaboration mediated by AR to make the field achieve the Theory, Automation and Maturity phases of the BRETAM model. Inspired by [97]. Table 1 Summary of evaluation surveys addressing Collaborative Augmented Reality (2008–2019). Year & Authors Pub. # Pubs. analyzed Aspects of Collaboration Main outcomes 2008 - Duenser et al. [41] 10 n/a Studies that evaluated collaboration between users using AR were quite underrepresented. Only 10 papers were reported, which were divided according to the study type in formal and informal studies. 2008 - Zhou et al. [42] not specified n/a A small number of examples of collaborative AR prototypes were starting to emerge, but few had been evaluated in formal user studies. 2012 - Bai et al. [29] 9 Communication, Awareness An increase in measurements of particular interest in AR collaborative systems included explicit communication (e.g., spoken and gestural messages), ease of collaboration and information gathering (basic awareness, eye gaze). 2015 - Billinghurst et al. [12] not specified Communication Besides the standard subjective measures, process measures may be more important than quantitative outcome measures. Process measures are typically gathered by transcribing interaction between users, like speech or gestures and performing a conversational analysis. In this context, very few studies have examined communication process measures. 2018 - Kim et al. [6] not specified n/a A reduce but increasing number of publications explicitly focused on ways to improve collaboration using AR. A mixture of qualitative and quantitative experimental measures were used, such as performance time and accuracy (quantitative), and subjective questionnaires (qualitative). 2018 - Dey et al. [43] 12 n/a Need to conduct more user studies regarding collaboration using AR, more use of field studies, and the use of a wider range of evaluation methods. There is an urge to improve the reporting quality of user studies, and education of researchers on how to conduct good AR user studies. 2019 - Ens et al. [14] 110 Time, Space, Symmetry, Artificiality, Focus, Scenario Review of the history of collaborative MR systems, and investigation on how common taxonomies and frameworks in CSCW and MR research could be applied to such systems. The authors emphasize that MR systems have been facing significant engineering hurdles and have only recently started to mature to focused on the nuances of supporting collaboration. 2019 - Belen et al. [25] 259 Task, Awareness, Presence, Social factors A total of 112 papers studied how MR affects the sense of presence and the perception of social awareness, situational awareness and task awareness during collaboration. A considerable amount of research studied how collaboration reduces cognitive workload through MR environments. 55 papers were categorized under user perception and cognition studies. Table 2 Summary of User studies on Remote Collaboration using AR or MR — Part 1. Legend: S — Subjective; O — Objective; HHD — Handheld Device; HMD — Head Mounted Display. ID Pub. Year Application area Collaboration details Task type Devices used (On-site User) Devices used (Remote User) Study type Data type Study design 1 [53] 2020 Assembly Hierarchy - Synchronous Lego Brick Assembly Projector, External Camera HMD, Hand Tracker Formal S Within-subjects 2 [54] 2020 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Controllers, 360°camera HMD, Controllers Formal S Between-subjects 3 [55] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers, Hand Tracker Formal O + S – 4 [56] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Puzzle Assembly See-through HMD, 360°camera HMD, Controllers Formal O + S – 5 [57] 2019 Assembly Hierarchy - Synchronous Lego Brick Assembly See-through HMD, Depth Sensors HMD, Hand Tracker Formal O + S Within-subjects 6 [58] 2019 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation See-through HMD See-through HMD, Formal S Within-subjects 7 [59] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Lego Brick Assembly See-through HMD, 360°camera HMD, Hand Tracker Formal O + S – 8 [60] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers Formal O + S – 9 [61] 2019 Social Presence Parallel - Synchronous Puzzle Assembly See-through HMD HMD, Controllers Formal S Within-subjects 10 [62] 2019 Assembly Hierarchy - Synchronous Lego Brick Assembly Projector, Camera HMD, Hand Tracker Formal O + S Within-subjects 11 [63] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Hand Tracker Formal O + S Within-subjects 12 [52] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker HMD, Hand Tracker Formal O + S Between-subjects 13 [64] 2019 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker See-through HMD, Hand Tracker Formal O + S Within-subjects 14 [6] 2018 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Within-subjects 15 [65] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Hand Tracker Formal O + S Within-subjects 16 [66] 2018 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 17 [67] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HMD, Hand Tracker HMD, Hand Tracker Informal, Formal S Between-subjects 18 [22] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer, Mouse and Keyboard Formal O + S Between-subjects 19 [68] 2018 Assembly Hierarchy - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Hand Tracker Formal O + S Between-subjects 20 [69] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 21 [70] 2018 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD HMD, Controllers Formal O + S Within-subjects 22 [71] 2018 Assistance Parallel - Synchronous Navigation, Object Selection and Manipulation HHD HHD Formal O + S – 23 [72] 2018 Assistance Parallel - Synchronous Navigation, Object Selection and Manipulation HMD, Controllers, Hand Tracker HMD, Controllers, Hand Tracker Formal O + S – 24 [73] 2018 Assembly Parallel - Synchronous Puzzle Assembly Projector, External Camera Computer, Gaze Tracker Informal, Formal O + S Within-subjects 25 [74] 2017 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, 360°camera HMD, Controllers, Hand Tracker Formal O + S – 26 [75] 2017 Assembly Hierarchy - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Gaze Tracker Formal S Between-subjects 27 [76] 2017 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera, Body Tracker Projector, Optitrack Capture Tracker Informal S Within-subjects 28 [77] 2016 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD, Hand Tracker HMD, Controllers Informal O + S Between-subjects 29 [78] 2015 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation, Lego Brick Assembly Projector, External Camera Computer, Mouse and Keyboard Informal, Formal O + S Within-subjects 30 [79] 2015 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD Computer, Mouse and Keyboard – O + S Between-subjects 31 [80] 2015 Assembly Parallel - Synchronous Puzzle Assembly See-through HMD, External Camera Computer, Mouse and Keyboard Formal O + S Between-subjects 32 [81] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation See-through HMD Computer, Mouse and Keyboard – O + S Between-subjects 33 [82] 2014 Assembly Parallel - Synchronous Puzzle Assembly HHD or See-through HMD Computer, Mouse and Keyboard Formal O + S Between-subjects 34 [83] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer with Touch screen Informal, Field S – 35 [84] 2014 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HHD Computer, Mouse and Keyboard Informal, Field O + S Within-subjects 36 [85] 2013 Assembly Hierarchy - Synchronous Puzzle Assembly Monitor, External Camera HMD Formal O + S – 37 [86] 2013 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation HHD HHD Formal O + S – 38 [87] 2012 Co-Design Parallel - Synchronous Navigation, Object Selection and Manipulation See-through HMD, External Camera See-through HMD, external camera Informal S Between-subjects 39 [88] 2012 Assistance Hierarchy - Synchronous Airplane Cockpit HHD Computer, Mouse and Keyboard Formal O + S Within-subjects 40 [89] 2007 Education Parallel - Asynchronous Navigation, Object Selection and Manipulation Computer, External Camera Computer, External Camera, Gaze Tracker Informal S Between-subjects 41 [90] 2006 Assistance Hierarchy - Synchronous Navigation, Object Selection and Manipulation HMD, External Tracker HMD, External Tracker Formal O – 42 [91] 2004 Social Presence Hierarchy - Synchronous Navigation, Object Selection and Manipulation Computer, Mouse and Keyboard Computer, Mouse and Keyboard Formal S Within-subjects Table 3 Summary of User studies in Remote Collaboration using AR or MR — Part 2. ID Pub. Evaluation methods # Participants (# Females) Participant role Participants knew each other Previous experience with AR/VR/MR Description experimental context Adaptation period Duration (min) Recording audio and video 1 [53] Questionnaires, Interview 34 (11) On-site or Remote Yes and No – – Yes 55 – 2 [54] Questionnaires, User Preference 40 (-) On-site or Remote – Yes Yes Yes 40 – 3 [55] Task Performance, Questionnaires, User Preference 32 (8) On-site or Remote – Yes – – – – 4 [56] Task Performance, Questionnaires, User Preference 10 (1) On-site or Remote – Yes Yes Yes – – 5 [57] Task Performance, Questionnaires, User Preference 10 (4) On-site or Remote – – – – – – 6 [58] Questionnaires, User Preference 8 (4) On-site or Remote – – Yes – 30 – 7 [59] Task Performance, Questionnaires, User Preference 14 (-) On-site – – Yes – 70 Yes 8 [60] Task Performance, Questionnaires, User Preference 24 (5) On-site – Yes Yes – 90 – 9 [61] Questionnaires, Interview 48 (24) On-site Yes Yes and No Yes – – – 10 [62] Task Performance, Questionnaires, User Preference 13 (5) On-site or Remote – No – Yes – – 11 [63] Task Performance, Questionnaires, User Preference 12 (3) On-site and Remote Yes Yes Yes – – – 12 [52] Task Performance, Questionnaires, Interview 32 (9) On-site and Remote Yes Yes and No Yes Yes 120 – 13 [64] Task Performance, Questionnaires 20 (5) On-site or Remote No – Yes Yes 35 – 14 [6] Task Performance, Questionnaires, User Preference 24 (7) On-site or Remote Yes – – – – Yes 15 [65] Task Performance, Questionnaires, User Preference 8 (2) On-site – Yes Yes Yes – – 16 [66] Questionnaires, Interview 24 (4) On-site or Remote Yes – Yes Yes – – 17 [67] Questionnaires, User Preference 38 (23) On-site or Remote – – Yes Yes 30 – 18 [22] Task Performance, Questionnaires 30 (4) On-site – – – – – 19 [68] Task Performance, Questionnaires, User Preference 10 (0) On-site – – – – – – 20 [69] Task Performance, Questionnaires, User Preference 8 (4) On-site or Remote – – Yes – 60 Yes 21 [70] Task Performance, Questionnaires, User Preference 16 (5) On-site or Remote – Yes – – – – 22 [71] Task Performance, Questionnaires, User Preference 40 (-) On-site or Remote – – Yes – – – 23 [72] Task Performance, Questionnaires, User Preference 28 (-) On-site and Remote – Yes – – – – 24 [73] Task Performance, Questionnaires, User Preference 24 (16) On-site or Remote – – Yes Yes – – 25 [74] Task Performance, Questionnaires, User Preference 8 (-) On-site and Remote – – – – – – 26 [75] Questionnaires, User Preference 8 (2) On-site or Remote Yes – – – – – 27 [76] Questionnaires, User Preference 8 (-) Remote – – – – – – 28 [77] Task Performance, Questionnaires 10 (-) On-site – – – – – – 29 [78] Task Performance, Questionnaires 13 (-) + 24 (-) On-site or Remote – – – – – – 30 [79] Task Performance, Questionnaires, User Preference 36 (15) On-site or Remote – – – – – Yes 31 [80] Task Performance, Questionnaires, Interview 24 (7) Remote – – Yes Yes 70 – 32 [81] Task Performance, Questionnaires, User Preference – Remote – – – – – Yes 33 [82] Task Performance, Questionnaires, User Preference 24 (7) On-site or Remote Yes – Yes Yes 90 – 34 [83] Questionnaires, User Preference 25 (-) + 11 (5) – – – – – 50 – 35 [84] Task Performance, Questionnaires, User Preference 20 (-) + 60 (29) On-site – – – – – – 36 [85] Task Performance, Questionnaires, User Preference 14 (-) On-site and Remote – – Yes – – – 37 [86] Task Performance, Questionnaires, User Preference 36 (-) On-site or Remote – – Yes – – – 38 [87] Interview 5 (-) On-site – – – – – Yes 39 [88] Task Performance, Questionnaires 48 (21) On-site Yes and No – Yes Yes – – 40 [89] Questionnaires, Interview 9 (3) On-site – – – Yes 20 – 41 [90] Task Performance 12 (2) On-site or Remote – – – – – – 42 [91] Questionnaires, User Preference 27 (8) On-site or Remote – – Yes – – – Table 4 Overview of common approaches and what is missing regarding the evaluation process of remote collaboration mediated by AR. Common: •synchronous hierarchy collaboration •within-subjects design •formal user studies •navigation, selection, manipulation and assembly tasks •focus on technological aspects or interaction mechanisms of the collaborative AR solution •subjective and objective data collection •use of single-user questionnaires, task performance and user preferences assessment •young participants from universities •participants act as on-site or remote team-members Missing: •conduct outdoor and field studies •explore complex/adequate tasks •contemplate failure situations •provide an adaptation period •address participants relationships, knowledge and motivations •better description of the collaborative process supported by AR •reporting of study average duration •data collection on dialog turns, interaction types, main features and visual complexity •contextualized information on the team, task, environment and collaborative tool •improve existing frameworks •use of video, audio recordings, and post-task interviews ☆ This article was recommended for publication by C Sandor. Technical Section A critical analysis on remote collaboration mediated by Augmented Reality: Making a case for improved characterization and evaluation of the collaborative process Bernardo Marques Conceptualization Methodology Validation Formal analysis Investigation Data curation Writing – original draft Writing – review & editing Visualization Project administration Funding acquisition ⁎ António Teixeira Conceptualization Methodology Writing – original draft Visualization Samuel Silva Methodology Investigation Writing – original draft Writing – review & editing João Alves Validation Formal analysis Investigation Data curation Writing – original draft Paulo Dias Conceptualization Investigation Data curation Writing – original draft Writing – review & editing Supervision Funding acquisition Beatriz Sousa Santos Conceptualization Methodology Investigation Writing – original draft Writing – review & editing IEETA, DETI, University of Aveiro, Aveiro, 3810-193, Portugal IEETA, DETI, University of Aveiro Aveiro 3810-193 Portugal IEETA, DETI, University of Aveiro, Aveiro, 3810-193, Portugal ⁎ Corresponding author. Remote Collaboration mediated by Mixed and Augmented Reality (MR/AR) shows great potential in scenarios where physically distributed collaborators need to establish a common ground to achieve a shared goal. So far, most research efforts have been devoted to creating the enabling technology, overcoming engineering hurdles and proposing methods to support its design and development. To contribute to more in-depth knowledge on how remote collaboration occurs through these technologies, it is paramount to understand where the field stands and how characterization and evaluation have been conducted. In this vein, this work reports the results of a literature review which shows that evaluation is frequently performed in ad-hoc manners, i.e., disregarding adapting the evaluation methods to collaborative AR. Most studies rely on single-user methods, which are not suitable for collaborative solutions, falling short of retrieving the necessary amount of contextualized data for more comprehensive evaluations. This suggests minimal support of existing frameworks and a lack of theories and guidelines to guide the characterization of the collaborative process using AR. Then, a critical analysis is presented in which we discuss the maturity of the field and a roadmap of important research actions is proposed, that may help address how to improve the characterization and evaluation of the collaboration process moving forward and, in consequence, improve MR/AR based remote collaboration. Graphical abstract Keywords Remote collaboration Augmented Reality Mixed Reality Evaluation and characterization collaborative process Critical analysis Roadmap proposal 1 Introduction Collaboration is essential in many situations, as is the case of industrial, medical, and educational domains, among others [1–4] and can be described as the process of joint and interdependent activities between co-located or remote collaborators performed to achieve a common goal [5–9]. Collaboration scenarios have evolved from simple co-located scenarios to more complex remote collaboration, encompassing several team members with different experiences, expertise’s and multidisciplinary backgrounds distributed by different geographic locations around the world. Therefore, the methods required to address such activities have been growing in terms of scale, complexity, and interdisciplinarity, entailing not only the mastery of multiple domains of knowledge, but also a strong level of proficiency in each [3,4,10]. Remote collaboration, implies that collaborators establish a joint effort to align and integrate their activities in a seamless manner. Technological support for remote collaboration has been addressed among other fields by Computer-Supported Cooperative Work (CSCW), focusing on conceptualizing, designing, and prototyping solutions for communication, cooperation, assistance, training, learning as well as knowledge sharing between distributed collaborators. One major issue of remote collaboration is the fact that collaborators do not share a common space/world, reason for the interest in using Augmented Reality (AR) in this context [11–15]. Collaboration using AR helps distributed collaborators establish a common ground, analogous to their understanding of the physical space, allowing to inform where to act, and what to do, e.g., making assumptions and beliefs visible by providing real-time spatial information, highlighting specific areas of interest, or sharing situated information associated with relevant objects in the on-site physical environment [16–20]. Remote AR-based solutions are well suited for overlaying responsive computer-generated information on top of the real-world environment, resulting in the creation of solutions that combine the advantages of virtual environments and the possibility for seamless interaction with the real-world objects and other collaborators [6,15,17,19,21–24]. A number of studies have focused on the use of virtual annotations to augment the shared understanding, using drawings, pointers, gaze, hand gestures and others on 2D images or live video streams [3,6,9,14,22,25]. As an alternative, recent studies started to explore the use of virtual replicas [26–28], as well as reconstructions of the physical environment [29,30], although these required the existence of 3D models and additional hardware, which may limit their adoption in some scenarios of application. Using such approaches to enhance the common ground can improve efficiency and accuracy of the performed tasks by enhancing the perception of the shared understanding [6,22,24,31,32], as well as collaboration times, knowledge retention, increased problem context and awareness [16,17,33–35]. While creating the means to support collaboration clearly motivated early research, advances in AR have been limited by new technical developments, which means most of the research efforts, so far, have been focused on creating the enabling technology and propose novel methods to support its design and development [14,36,37]. On the other hand, with the growing development of CSCW, the evaluation of these solutions during the collaborative effort become an essential, but difficult endeavor [31,38,39], given the novelty of the field and the lack of methods and theories [14,25] to guide the characterization of the collaborative process, i.e., describe the contributions of AR to the collaborative work effort. In addition, scenarios of remote collaboration are multifaceted [40], which means many aspects may affect the way teams collaborate, making it difficult to identify all variables related to the collaborative process. Therefore, the integration of proper characterization and evaluation methods and guidelines is of paramount importance. In this paper, we analyze the subject of remote collaboration supported by AR through a systematic review and investigate how characterization and evaluation of the collaborative process has been conducted during user studies to better understand their specificities, rather than focusing on the development of technology itself. In this context, we analyzed existing surveys that addressed collaborative user studies and evaluation in their reviews. Plus, we performed a literature review from 2000 to 2020 to provide a high-level overview of the field, allowing identification of strengths and weaknesses of existing methods. Based on the analyzes, we describe the challenges involved with evaluating these solutions and critically analyze the state of the field. As a result, a possible roadmap is proposed to facilitate and elicit characterization of the collaboration process using AR-based solutions, so that research and development can move forward and focus on the nuances of supporting collaboration, i.e., focus squarely on the human concerns that underlie collaboration, rather than creating the enabling technology that makes remote collaboration mediated by AR possible. The rest of the paper is organized as follows: Section 2 presents an overview based on survey papers to update, complement and fill gaps on the current information on the state-of-the-art. Next, Section 3 details the methodology adopted to conduct our literature review and describes a high-level overview of the reviewed papers. Then, Section 4 provides a critical analysis in which we discuss the challenges associated to the characterization and evaluation of the collaborative process. Afterwards, Section 5 propose a roadmap to address these challenges. Finally, Section 6 concludes by summarizing the main outcomes. 2 Related work on user evaluation in collaborative AR This section identifies and analyzes existing survey papers that cover relevant evaluation details, which are summarized in Table 1. Our goal was to understand how evaluation has been conducted in collaborative scenarios, allowing to compare and contrast different methods, as well as identify opportunities and limitations associated to the characterization of the collaborative process. From the list of prior surveys, the first six entries are rather general in scope, although the review of collaborative AR papers is also mentioned, despite being only a portion of the results reported [6,12,29,41–43]. While this is the case, the two last entries of the list [14,25] focus entirely on the subject of Collaborative AR and MR, including co-located and remote examples. Although these surveys primarily focused on the development of collaborative AR technology itself, some important outcomes regarding evaluation are also reported, as described below in detail. Besides, another publication [7] was considered in this analysis, that even though not strictly a survey, includes important information regarding evaluation of collaborative work in the context being addressed. 2.1 Previous surveys including user evaluation information Duenser et al. (2008) reported on user evaluation techniques used in AR research. Then, studies that evaluate collaboration between users using AR were quite underrepresented: from a total of 161 publications included in the survey, only 10 addressed collaborative AR. Besides reporting that 8 papers were formal and 2 informal user evaluation, the survey does not present further detail on the collaborative studies [41]. In addition, Zhou et al. (2008) presented one of the first overviews of the research conducted until that moment at the ISMAR conference and its predecessors. Although the research focus was on AR technologies, it also pointed out the significance of usability evaluation. The authors reported that a small number of collaborative AR prototypes were starting to emerge, but few had been evaluated in formal user studies. The authors also highlighted how the role of different displays would affect collaboration in the future and how the location of the task affected user behaviors in terms of verbal and non-verbal communication. Since collaboration and evaluation were not one of the focus of the survey, no further detail was provided [42]. In the same way, Bai et al. (2012) conducted an analytic review on usability evaluation at ISMAR. The authors suggested that while the design of usable systems were the main focus of collaborative AR research to that point, an increase in evaluation research was emerging. They also stated that measurements of particular interest in collaborative AR systems may include explicit communication (e.g., spoken and gestural messages), ease of collaboration and information gathering (e.g., basic awareness, eye gaze). The authors also reported that subjective answers may be collected via questionnaire and that direct observation was used to extract objective results. Moreover, signs of discomfort and enjoyment during collaboration were also taken into account by researchers [29]. Billinghurst et al. (2015) published a survey on AR, in which almost 50 years of research and development in the field were summarized. The authors state that in Collaborative AR studies, besides the standard subjective measures, process measures may be more important than quantitative outcome measures. Process measures are typically gathered by transcribing interaction between users, like speech or gestures and performing a conversational analysis. Measures that have been found to be significantly relevant include: frequency of conversational turns, duration of overlapping speech, number of questions, number of interruptions, turn completions and dialog length, among others. Besides, gesture and non-verbal behaviors can also be analyzed for characteristic features. The survey acknowledges that there have been very few user studies with collaborative AR environments and almost none that examined communication process measures [12]. Then again, Kim et al. (2018) revisited the trends presented at ISMAR conferences. According to their review, user evaluation and feedback has become one of the main categories for research presented at ISMAR, with 16.4% of publications reporting evaluation being conducted, showing a significant increase when compared to Zhou et al. 5.8% [42]. The authors extended Zhou et al. list of emerging research, including interactive collaborative systems for multiple remote or co-located users. A mixture of qualitative and quantitative experimental measures were used in studies that addressed collaboration, such as performance time and accuracy (quantitative), and subjective questionnaires (qualitative) [6]. Dey et al. (2018) conducted a systematic review of AR usability studies. A total of 291 papers have been reviewed. Among other things, over the years, there were few collaborative user studies, mostly directed towards remote collaboration. The authors reported 12 papers, in a total of 15 studies associated to the collaboration application area. One noticeable feature was the fact that there were no pilot studies reported, which is an area for potential improvement. Also, a reduced number (3 out of 15) of field studies was reported and all except one were performed indoors. Furthermore, a within-subjects design was used by 14 out of 15 studies, since these require fewer participants to achieve adequate statistical significance, with only 12 participants being recruited per study. Besides, roughly one-third of the participants were females in all studies. Hence, participant populations are dominated by mostly young, educated, male participants, suggesting that the field could benefit from more diversity. A majority of the studies, 8 out of 15 collected both objective (quantitative) and subjective (qualitative) data, while 5 studies were only based on subjective data, and 2 studies were based on only objective data. Aside from subjective feedback or ratings, task completion time and error/accuracy were also extensively used. Curiously, the NASA TLX was only used by one study. This analysis suggest the need of more user studies regarding collaboration using AR, particularly more field studies, and the use of a wider range of evaluation methods [43]. Although not strictly a survey, Kim et al. (2018) proposed a questionnaire including aspects regarding overall collaboration, namely the level of enjoyment and mental stress in communication with the partner, and whether collaboration was effective or not [7]. Moreover, the questionnaire included questions about who (presence of others — users’ feeling of togetherness with the collaborating partner), what (users’ activities — effectiveness in sending and receiving messages) and where (location of activities — whether seeing work space properly and asking the level of having a same focus with a partner). The questionnaire was based on previous work by Gutwin and Greenberg (1999), which suggested three types of experimental measurements are necessary to assess collaboration: product, process, and satisfaction. Product measures focus on assessing collaboration outcomes in terms of efficiency (e.g., task completion time) or quality (e.g., accuracy). Process measures assess user communication and patterns of collaboration and can be obtained by system log data, observation, and video/audio analysis. Satisfaction measures are adequate to assess participants’ subjective opinions on the quality of their collaboration and can be collected through interviews and questionnaires [44]. More recently, Ens et al. (2019) revisited collaboration through MR. A total of 110 papers employing MR technology and motivated by challenges in collaborative scenarios was reviewed, showing a rise in the number of papers published from 2012 and onward. The authors emphasize that MR systems have been facing significant engineering hurdles, being limited by the contemporary capabilities of technology, and have only recently started to mature to the point where researchers can focus squarely on the human concerns that underlie communication and collaboration, instead of focusing on creating the enabling technology. The vast majority of papers analyzed (106, or 95%) focused on synchronous collaboration. Moreover, 30 papers (27%) worked on a co-located setting, while 75 papers (68%) worked on a remote setting, and 6 papers (5%) support both settings. In the early years (up to 2005), most research addressed co-located work. Then, the paradigm changed, and from 2006 forward most work tackled remote collaboration. In addition, 45 papers (41%) focus on symmetric collaboration, while 63 (57%) on asymmetric, and 2 (2%) supported both types. The review states that existing methods are not sufficient to characterize how collaboration occurs. Finally, it also emphasizes the need to deepen the understanding of collaborative work through more user studies [14]. Finally, Belen et al. (2019) performed a systematic review of the current state of collaborative MR technologies published from 2013 to 2018. A total of 259 papers have been classified based on their application areas, types of display devices used, collaboration setups, user interaction and experience aspects. Regarding the collaboration setups used, 129 papers (50%) report works that used a remote setup, 103 papers (40%) used a collocated setup, and 27 (10%) used both settings. The type of user interaction and user experience were categorized, resulting in 55 papers categorized under user perception and cognition studies, which aim to lessen cognitive workload for task understanding and completion time and increase users’ perceptual (e.g., situational, social, and task) awareness and presence. Besides, a total of 112 papers studied how MR affects the sense of presence and the perception of social awareness, situational awareness and task awareness during collaboration. There was also a considerable amount of research on how collaboration reduces cognitive workload through MR environments. This review also showed that user interaction in a collaborative MR environment is an essential topic that requires further investigation [25]. 2.2 Summary Research is evolving from solving technical issues using AR and MR, towards more meaningful studies on collaboration. We were able to understand that evaluation is frequently done using single-user methods, which are not always applicable to groupware collaborative solutions. To clarify, by single-user methods, we are referring to the methodologies used in the collaborative studies. For example, focusing more on technological aspects of the solution being used than in the collaborative process; including tasks with low complexity that do not elicit real collaboration among participants; using only performance measures like task completion time and error/accuracy data, while other important dimensions are ignored; collecting participant data based only on standard practices with fixed answers, applying scales, questionnaires (e.g., System Usability Scale (SUS), NASA Task Load Index (TLX), among others), which are not thought for collaborative scenarios, thus ignoring detail on crucial aspects of collaboration. The majority of papers mentioned in the surveys informed on the tasks, types of devices used (although not specific to on-site or remote users), evaluation design, evaluation methods and number of participants, but lack detail on the participants’ role, if participants knew each other previous to the study, their previous experience with Virtual Reality (VR), AR or MR solutions, description on the experimental context, among other factors of collaboration. However, our review highlights some limitations included in previous surveys, namely the absence of information regarding specific characteristics of the collaborative context. These characteristics are important since collaboration may occur at many levels and depends on several factors that may impact directly the collaborative outcomes [40]. Contextual information helps inform the conditions in which the collaborative effort took place. Without comprehending the contextual information, it becomes difficult to assess the important variables related to the collaborative process, which means the results and findings reported may be misleading or of limited value in these scenarios, thus being an important subject to improve the characterization of the collaborative process. Hence, these aspects have an important impact on how the studies must be prepared and how they were conducted, influencing situation understanding, team-members communication, task performance, and even how AR-based tools were used among team-members, among others. Therefore, it is important to conduct thorough collaborative studies, allowing to retrieve the necessary amount of data for more comprehensive analysis that helps provide a perspective on the different factors of collaboration supported by AR. To sum up, the use of AR-based multi-site solutions creates challenges to the contextualization of the actions of each user and the problems/barriers they may face. Therefore, having a grasp of those aspects is paramount to ensure characterization is genuine. By doing so, researchers may be able to better assess a wide range of information, namely individual and team personalities, motivations, performances, behaviors, who completed the tasks and who provided instructions, how was the communication process, details of the surrounding environments, as well as duration and type of interactions with the collaborative technology, among other aspects when analyzing data and establishing conclusions. 3 Method and overview of recent literature To understand to what extent user evaluation is currently being reported covering collaborative AR and Mixed Reality (MR) research, we conducted an analysis of existing works through a systematic review. This section presents the research methods employed to carry out the review process, which was divided into: the search, i.e., describing how the collection of publications was performed and the review, i.e., explaining the process employed to ensure that the papers follow our review criterion. What differentiates our review from other surveys described in the previous section is the fact that we focus exclusively on evaluation and user studies in remote scenarios mediated by AR/MR to comprehend how the collaborative process has been captured and reported, rather than addressing the technology that made collaboration possible, which was the nucleus of the two only surveys that dedicated their efforts to the subject of Collaborative AR/MR, while the remaining ones are rather general in application scenario, although also addressing more technological aspects of AR/MR. Besides, by identifying relevant aspects that are missing from existing surveys regarding evaluation and user studies, we are able to include them in our analysis, leading to a discussion in which we critically analyze the field in light of the BRETAM model, thus providing a clearer understanding of how the characterization of the collaborative process has been achieved, which lead to the proposal of a roadmap of relevant research topics, aiming to help the community move the field forward. 3.1 Augmented reality vs mixed reality While older papers used the term remote collaboration supported by AR, more recent efforts described in literature are beginning to replace the term AR by MR. Next, we elaborate on the meaning of MR, and why this sudden change has started to emerge. Many researchers see MR as a synonym for AR [45]. Some consider MR a superset of AR, i.e., a real-world object can interact with a virtual one in real-time to assist individuals in practical scenarios [46–48]. Yet, others consider MR distinct from AR in the sense that MR enables walking into, and manipulating a scene, whereas AR does not, i.e., there is a separation of the real and virtual world content, which may lead to lower user immersion [49]. Although MR is increasingly gaining in popularity and relevance, the research community is still far from a shared understanding of what MR actually constitutes. Speicher et al. (2019) highlights that currently, there is no single definition for MR, since this concept can be considered different things for different individuals. In their survey, six partially competing notions were identified based on literature analysis and experts’ responses. Nevertheless, there is no universally agreed on, one-size-fits-all definition of MR. Moreover, the authors state that it is highly unrealistic to expect one single definition may appear in the future, which means discussions about MR become increasingly difficult. Therefore, it is extremely important to be clear and consistent in terminology while communicating one’s understanding of MR in order to avoid confusion and ensure constructive discussion [45]. Among the most important applications of MR are collaborative solutions, that may be used as decision-making tools for daily life problems [14,49]. In this context, Speicher et al. (2019) suggested that MR can be considered as a type of collaboration that describes the interaction between physically separated users exploring AR and VR [45]. This definition includes mapping of the environment of an on-site AR collaborator, i.e., capturing more dimensional information about the local scene, which is reconstructed in VR for the remote collaborator [45,50] and so provides unique capabilities to achieve a common goal, e.g., improved communication cues for more efficient and easier collaboration [8,46,51,52]. Given the aforementioned panorama, we decided to include both terms in our analysis. 3.2 Search process Our review was made as inclusive as possible. We collected papers from the Scopus database (since it covers most top journals and conferences on Collaborative AR) using the search terms: (“Augmented Reality” OR “Mixed Reality”) AND (“Remote Collaboration” OR “Remote Cooperation” OR “Remote Assistance” OR “Remote Guidance” OR “Distributed Collaboration”) AND (“User Evaluation” OR “User Study” OR “User Experiment”) The search for the terms was made in the Title, Abstract, and Keywords fields. All search results published in conferences and journals between 2000 and 2020 were taken into consideration. Only publications in the English language were considered as this is the current ’lingua franca’ of the academic research. 3.3 Analysis process We obtained a total of 64 publications. The search results were analyzed individually to identify whether or not it supported evaluation of remote scenarios supported by solutions using MR or AR. Only 42 publications satisfied the defined criteria. We started by filtering the initial collection of publications to meet our objectives. We removed articles that were incorrectly selected in the search process (false positives) and identified only those articles that included user evaluation. The reviews of each paper focused on the following attributes (Tables 2 and 3): application areas and keywords; type of collaboration; type of task; types of devices used (regarding on-site and remote users); type of study; type of data collected; evaluation design; evaluation methods; number of participants (number of female participants); participant role; participants’ familiarity with each other; previous experience with AR/VR/MR; experimental context description; adaptation period provided; study average duration (min); recording of audio and video. 3.4 Validity limitations A considerable amount of effort was invested on the selection and review process. Although the Scopus bibliographic database has been used to cover a wide range of publication venues and topics, there may be limitations with the described method. The search terms used might be limiting, as other papers could have used different keywords to describe “Remote Collaboration”, “Augmented Reality”, “Mixed Reality” or “Evaluation”. Therefore, it remains likely that there are papers which may have not been included in this review. 3.5 Results Next, a high-level overview of the reviewed papers is provided (Tables 2 and 3), following a similar structure as the one used by Dey et al. (2018) in their systematic review [43], which is extended to include relevant aspects missing from the surveys analyzed in the previous section, such as collaboration details, task type, study type, data type, study design, evaluation methods, participants characteristics, experimental context, adaptation period, and duration. 3.5.1 User studies categorization The papers (Tables 2 and 3) have the following distribution by application areas: assistance (25 papers, 59.5%); assembly (11 papers, 26.2%); co-design (3 papers, 7.1%); social presence (2 paper, 4.8%); education (1 paper, 2.4%), as presented in the orange bubbles in Fig. 1. Regarding the collaboration details, 30 papers (71.4%) explored collaboration using a synchronous hierarchy approach, i.e., each member has a specific function or expertise and all team members are present and could act in real-time, while 11 papers (26.2%) studied synchronous parallel approach, where all elements have the same level of expertise and could act in real time and only 1 paper (2.4%) studied asynchronous parallel approach, i.e., all elements have the same level of expertise in which collaboration would take place at different times, as shown in the dark blue bubbles in Fig. 1. 3.5.2 Study design As shown in Table 2, 16 papers (38.7%) used a within-subjects design, while 15 papers (35.7%) used a between-subjects design. There were no mentions of a mixed-factorial design. In addition, 11 (26.2%) papers did not mention the method used, as illustrated in the green bubbles in Fig. 1. 3.5.3 Study type We found that most papers (33, 78.6%) were formal user studies. On the opposite, 7 papers (16.6%) reported conducting informal studies. Only 2 papers (4.8%) conducted user studies in the field, which shows a lack of experimentation in real-world conditions, as exhibit in the green bubbles in Fig. 1. 3.5.4 Task type As expected, most papers (26 out of 42, 61.9%) explored navigation, object selection and manipulation, forcing participants to communicate and use collaborative tools to provide indications to achieve a concrete goal. Additionally, 12 papers (28.6%) focused on assembly tasks using Lego bricks, or puzzles like tangram, pentominoes, origami, among others. Only 1 paper (2.4%) reported the use of an airplane cockpit as case study, as presented in the red bubbles in Fig. 1. This shows that there is an opportunity for conducting more user studies exploring different, more complex case studies, or even combinations of different types. Moreover, just 14 papers (33.33%) claim to have provided an adaptation period before the performance of the tasks, as shown in the purple bubbles in Fig. 1. Finally, the bulk of the user studies were conducted in an indoor environment, but only 21 papers (50%) described the experimental context, although no clear pattern emerged. 3.5.5 Evaluation methods and data type In terms of data type, 30 papers (71.4%) collected subjective and objective data, 11 papers (26.2%) collected only subjective data, and just 1 (2.4%) only objective data. Concerning the evaluation methods, we found that the most popular method is filling out questionnaires (40 papers, 95.2%), followed by assessing task performance (31 papers, 73.8%) with error/accuracy measures and task completion time. Then, user preference (28 papers, 66.7%) and finally interviews (5 papers, 11.9%), as illustrated in the light blue bubbles in Fig. 1. Note that many papers used more than one evaluation method, so the percentages sum to more than 100%. Another essential point: only 13 papers (31%) mentioned the average duration of the user study (58.5 min). Some papers mentioned the duration of the task, but no clear information on the collaboration process is provided, like dialog length, frequency of conversational turns, among others. Besides, none of the papers report to have conducted gesture or non-verbal behaviors analysis. This is supported by the lack of audio or video recording, since only 6 papers (14.3%) acknowledge to store this type of data. 3.5.6 Participants Our review of the participants shows that the number of participants involved in the analyzed studies ranged from 5 to 48, with an average of 21. Also, a total of 31 out of 42 papers (73.4%) reported involving female participants in their experiments, with the ratio of female participants to male participants being 47.6% of total participants in those 31 papers. Hence, most of the studies were run with young participants, mostly university students, rather than a more representative cross section of the population. Equally important, 23 papers (54.8%) stated that participants would perform the role of the on-site or remote user during the studies. Moreover, in 5 papers (11.9%) the participants would perform the on-site and remote role. 11 papers (26.2%) only allowed the participants to perform the on-site user, while 3 papers (7.1%) only allowed to perform the remote role. In these cases, the counterpart would be performed by a monitor, as presented in the brown bubbles in Fig. 1. Most papers, 32 out of 42 (76.2%) made no mention if participants knew each other, with only 9 clearly stating that information. Likewise, the same percentage did not mention any type of previous experience the participants might have with AR or MR systems. 3.6 Summary Our review (Table 4 and Fig. 1) shows that the dominant type of collaboration is based on the hierarchy approach focused on synchronous communication between participants. Also, that assistance and assembly are the main areas of application, exploring navigation, selection and manipulation tasks in indoor environments, during approximately one hour. On average, studies involved 21 participants, mostly young university students. Moreover, ruffly half of the papers reported that the participants would perform the role of the on-site or remote user during the studies. Besides, most papers lack information regarding if participants knew each other prior to the study and if they had previous experience with MR systems. The majority of the studies conducted are formal studies, collecting objective and subjective data using evaluation methods like questionnaires, task performance and user preferences in that order respectively. As for collaborative measures, most works focus on effectiveness, only checking if participants were able to accomplish a given task collaboratively. Moreover, the evaluation design is distributed between within-subjects and between-subjects. Besides, interviews are not used often, as is also the case of recording audio and video during the studies. In addition, half of the times the experimental context is not described and only one third of the times studies referred the existence of an adaptation period. It is important to report this last fact, as it can affect the way the collaboration process was performed between collaborators, i.e., those that had an opportunity to use, adapt and comprehend the technology that helped create a shared understanding prior to the tasks will easily interact better with their respective counterpart, when compared to the ones that have only done the adjustment process during the task itself. Another observation is that single-user evaluation methods are applied to collaborative tasks, which mainly focus on the comparison of technological aspects or interaction mechanisms based on rather simpler procedures. We argue that collaborative tasks must be difficult and long enough to encourage interaction between collaborators and for the AR-based solution being used to provide enough contribution. In general, tasks can benefit from deliberate drawbacks, and constraints, i.e., incorrect, contradictory, vague or missing information, to force more complicated situations and elicit collaboration. For example, suggest the use of an object which does not exist in the environment of the other collaborator or suggest removing a red cable, which is green in the other collaborator context. Such situations help introduce different levels of complexity, which go beyond the standard approaches used, and elicit more realistic real-life situations where the surroundings are not always perfect. Likewise, multiple procedures may be applied to an evaluation, while also exploring different levels of complexity, contextual changes in the surroundings environments, as well as stress conditions. 4 Critical analysis This section describes the main limitations hindering a better understanding regarding how AR supports collaborative work in remote scenarios. Analysis was mostly based in the results from the literature review process, complemented by meetings with domain experts, and authors’ own experience creating and conducting evaluation studies in this domain [Refs omitted for review purposes]. The contributions presented in this paper were conducted in the scope of a larger multidisciplinary research line, with a total of nine individuals with several years of expertise (minimal of 6 years, and a maximum of 40 years of experience) in the areas of Human–Computer Interaction (HCI), Virtual and Augmented Reality (VR/AR), Information Visualization (IV), Multimodal Interaction (MMI), as well as remote collaboration in several scenarios of application. To this effect, face-to-face and remote meetings were conducted, as well as focus group and brainstorm sessions (sometimes with different combinations of experts according to their availability) over several months. To conclude the section, a global assessment of the field maturity is attempted, followed by a critical analysis on how that may affect the road ahead. 4.1 Main limitations As was aforementioned, the characterization and evaluation of the collaborative process in remote scenarios using AR-based solutions have been reported mainly using single-user methods focusing on technological aspects, thus lacking information and focus on the important dimensions of collaboration. As a result the following main limitations can be identified. Limitation 1: partial evaluation According to Merino et al. “designing appropriate evaluations that examine MR/AR is challenging, and suitable guidance to design and conduct evaluations of MR/AR are largely missing” [37]. This fact is further evident in scenarios of remote collaboration, since the logistics associated with carrying out evaluations is even more demanding due to a significant number of variables that must be considered. The existence of two or more collaborators makes it more difficult to evaluate the solution as a whole, given that it requires to perform multiple evaluations at the same time and that validation from all users is required [40]. As a consequence, there is a clear lack in addressing crucial aspects of collaboration like how was the relation and communication of the collaborators during the tasks (only 10 out of 42 papers reported such information and just 6 recorded audio or video during the studies), whether they had previous experience with AR/VR/MR technologies and were able to use the available solutions to their full potential (a topic just mentioned by 11 out of 42 papers), how the available information was used to support the accomplishment of the tasks, among other aspects. In this context, trying to apply conventional evaluation techniques to collaborative settings, without adapting them can lead to an incomplete vision of the process of collaboration and in turn to dubious results, falling short to retrieve the necessary amount of data for more comprehensive evaluations and characterizations of the collaborative process which may lead to an incomplete vision of the process of collaboration. Given the complex environments and situations collaborators may encounter, such methods alone provide insufficient information and rarely are good indicators for improving distributed solutions [31,38,92,93]. Limitation 2: lack of contextual information Remote collaboration represents high levels of data by involving different types of distributed collaborators, tasks and in encompassing dynamical environments with contextual data. Dey et al. revealed that “work needs to be done towards making AR-based remote collaboration akin to the real world with not only shared understanding of the task but also shared understanding of the other collaborators emotional and physiological states” [43]. Moreover, Ratcliffe et al. suggested that “remote settings introduce additional uncontrolled variables that need to be considered by researchers, such as potential unknown distractions, trust in participants and their motivation, and issues with remote environmental spaces” [94]. However, our analysis shows that half of the papers analyzed (21 out of 42) did not described the experimental context of collaborators, and that 76.2% (32 out of 42) did not report participants knowledge of each other. The same percentage of papers did not mention previous experience with AR or MR technologies, as illustrated in Table 3. By doing this, evaluation scenarios disregard information such as contextual or user related data, obtaining only superficial results. Limitation 3: failure situations are not contemplated Bai et al. stated that: ”as deeper insight is obtained into the affordances of AR collaboration, more complex activities should be supported” [29]. This is also corroborated by Ens et al. which highlighted that “as new capabilities emerge, (...) we expect to see this trend continue, with an initial focus on perfecting the systems, followed by deeper explorations of collaboration” [14]. Furthermore, this is also supported by our analysis from the selected data set, which shows that failure situations were not taken into account by any study. For example, in the case of failure to achieve the intended goals of the collaborative process, how can we understand what went wrong? Was it caused by problems in participants communication, by too much augmented information being displayed, by the actions of a particular collaborator that did not followed correctly some indications, or was it caused by an error in the AR-based solution being used? Limitation 4: lack of theories and guidelines Literature shows an absence of rules, guidelines and theories to guide the characterization of the collaborative process using solutions mediated by AR. For example, Dey et al. suggests that “opportunities for increased user studies in collaboration, more use of field studies, and a wider range of evaluation methods” [43]. Moreover, Ens et al. reported that “MR systems faced significant engineering hurdles, and have only recently started catching up to provide new theories and lessons for collaboration” [14]. A better evaluation strategy is required by researchers and developers to obtain a comprehensive description, given the challenges involved in evaluating many aspects that may influence the way collaboration occurs, e.g., relations between individuals, their interconnection as a team and how the use of AR/MR technologies affected the accomplishment of the tasks in relation to the collaborative effort. Limitation 5: minimal support in existing frameworks The constraints and challenges identified may change according to the maturity of the solution being used, the goal of the evaluation, the participants individual and group characteristics, among other parameters. In this context, existing frameworks are not sufficiently well suited to describe how collaboration mediated by AR/MR technologies happens, thus ignoring detail on crucial aspects of collaboration [7,14,29,36,39,43]. For example, Bai et al. emphasized that “it can be hard to isolate the factors that are specifically relevant to collaboration” [29]. Likewise, Ens et al. outlined that “frameworks for describing groupware and MR systems are not sufficient to characterize how collaboration occurs through this new medium” [14]. In addition, Ratcliffe et al. communicate that “the infrastructure for collecting and storing this (mass) of XR data remotely is currently not fully implemented, and we are not aware of any end-to-end standardized framework” [94]. Therefore, integration of proper characterization and evaluation methods and guidelines, covering different contexts of use and tasks, running in its intended (real or simulated) environment are of paramount importance. Limitation 6: limited reporting of outcomes There is now an opportunity to convince researchers to better document their work, and help improve evaluations and characterizations that are, in our view, a bottleneck in this research area. Currently, researchers struggle to analyze the state of the art, since much information on existing publications lack detail on the collaborative process as previously demonstrated. This may happen since most of the research efforts have been devoted on creating the enabling technology. 4.2 Maturity of the field To put in perspective the evolution of the field, as well as consider current limitations, this section concludes with the analysis of the status of the area according to the BRETAM model (Fig. 2) [95]. This model has been considered useful for the introduction of new knowledge, technology or products and adopted in several scenarios, including for example, in a multimodal interaction review [96]. According to the current panorama reported in this publication, we argue that it is possible to situate the field of remote collaboration mediated by AR between the Replication and Empiricism phases of the BRETAM model as illustrated in Fig. 2. We argue that remote collaboration mediated by AR has already passed the Breakthrough phase, which means research institutions worldwide can replicate the basic concepts, as demonstrated by the last few decades of research [25]. The Replication and Empiricism phases on the other hand imply increased ideas to generate enough experience, leading to empirical design rules. As such, these phases seem adequate to the overall panorama described in this publication, reinforcing the need to deepen the understanding and characterization of the collaborative process through methods, frameworks, guidelines and various user studies. In our view, remote collaboration mediated by AR has still not reached the Theory phase as it requires enough empirical experience to model the basis of success and failure, which cannot be performed without proper methods for the characterization and evaluation of the collaborative process [14,25]. Likewise, the Automation phase was also rejected, which implies automation of the scientific data-gathering and analysis, since existing systems are still limited by the contextual and multi-user data they are able to collect, thus not being sufficient to characterize how collaboration occurs [14]. As such, without fulfilling the previous phases, the field cannot be positioned into the Maturity phase, i.e., turn to cost reduction and quality improvements in what describes a mature technology [95]. 5 Charting out a roadmap for the characterization and evaluation of the collaborative process According to what was said in the critical analysis, it is important to address the main limitations to make the field achieve the Theory, Automation and Maturity phases of the BRETAM model [95]. Aiming at contributing to that, in this section we propose a first roadmap, to deal with the most pressing issues (Fig. 3), composed by five key topics: • definition of dimensions of collaboration to face the partial characterization of the collaborative process; • systematization of perspectives based on the acquired knowledge of the field, facing the lack of theories and guidelines; • creation of new paradigms, architectures and frameworks to answer the limited support to development and evaluation of existing ones; • enhanced support for data gathering, leading to better design, development and evaluation with distributed users supported by AR; • new and better outcomes from the evaluation to support the assessment, leading to the creation of new theories, as well as improve the lack of contextual information. 5.1 Definition of dimensions of collaboration First, it is important to identify dimensions that need to be taken into consideration when performing the characterization of the collaborative process. In practical terms, given a concrete application context and a problem, the research community is still not able to provide an overall definition of the collaborative AR system that addresses it. Although there are works that have presented some dimensions of collaboration, existing efforts are mostly oriented towards technology. As the field matures, it is normal new proposals emerge to address new aspects related to collaboration. A comprehensive set of dimensions must be defined to more thoroughly classify and discuss the contributions of the collaborative work effort, not only addressing the technological features being used, but also encompassing the characteristics of the context. For example, Ens et al. stated the following: “While somewhat useful, the dimensions we used are fairly technical, and focus mainly on mechanical aspects of the system or properties of the underlying technologies. (...) Perhaps additional dimensions with a greater focus on user experience would better allow for capturing the essence of collaborative scenarios“ [14]. Therefore, some of the existing dimensions might still not reflect the full scope of some categories by encompassing all possibilities. Therefore, this effort cannot be intended as a closed work, but should, instead, be taken as the grounds that might enable the community to elaborate, expand, and refine the field. This may be achieved by analyzing the literature regarding collaborative work supported by AR, in particular, existing categorization efforts [13,14,25,45,98–101]. Another possibility is to adopt a conceptual-to-empirical methodology by using a participatory design process, i.e., actively involving stakeholders in focus group and brainstorming sections. This entails going beyond Collaborative AR literature, considering other domains (e.g., CSCW, Groupware, Telerehabilitation, Remote Medicine, among others) that may be relevant to characterize the collaborative effort, to identify which dimensions should be taken into account when we move from asking what existing systems can do, to understanding what they would be able to do in particular contexts, i.e., the value of AR to the collaborative process. 5.2 Systematization of perspectives for the field Ens et al. report that when considering if it is possible to clearly describe distinct categories of collaborative MR research based on the existing dimensions, the answer is “to some extent, yes, however the result is not wholly satisfying (...) these dimensions do not suffice to describe all scenarios” [14]. Therefore, another area of research that needs to be addressed given the lack of theories and guidelines [14,43], is the need to bring new dimensions forward into conceptual models, guidelines, taxonomies and ontologies, that might foster harmonization of perspectives for the field, thus creating a common ground for systematization and discussion [100,102]. Through these, it would be possible to structure the characterization of the collaboration process, which can form the basis for analysis and comparison, fostering a more detailed understanding of the field, and in turn ensure that the research adds to the body of knowledge and provides enough context and evidence to enable a transparent account [103] and transferability [104]. These can also work as a knowledge repository for evaluation, allowing researchers to observe and compare a variety of results inside the same domain and make considerations and conclusions about specific nuances of collaboration. For example, the proposal of human-centered approaches, i.e., focusing on collaboration, instead of the technology, might bring forward a perspective that is not rapidly deprecated with the advancements of technology [105]. To create conceptual models and taxonomies, it is important to ensure the dimensions of collaboration contain categories and characteristics that are mutually exclusive and collectively exhaustive [106,107]; Moreover, a detailed explanation of these objects of interest must be included, following, for example, a similar approach to the one used by Zollmann et al. [108]. It is also relevant to include discussion and refinement over several iterations with domain experts, to verify if the established dimensions, categories and characteristics are well defined, need to be merged, or if new ones can be identified [106]. Regarding ontologies, literature shows that its design is considered a creative process and no two ontologies by different individuals would be the same, since the applications of the ontology and the designer’s understanding of the domain will undoubtedly affect the ontology design choices [109,110]. As such, one possible strategy is to adapt existing ontologies when they exist, or as an alternative when this is not the case, define and populate a new ontology considering relevant dimensions of collaboration as the core classes and establish their relations with each other based on the targeted application of the ontology. 5.3 Creation of new paradigms, architectures and frameworks According to Merino et al. “as MR/AR technologies become more mature, questions that involve human aspects will gain focus in MR/AR research. Consequently, we expect that future MR/AR papers will elaborate on human-centered evaluations that involve not only the analysis of user performance and user experience, but also the analysis of other scenarios, like understanding the role of MR/AR in working places and in communication and collaboration” [37]. However, there is no standard methodology for characterization and evaluation, specifically tailored to assess how remote collaboration occurs through AR/MR technologies. In this vein, without the appropriate paradigms, methods and mechanisms, the research community does not accumulate enough experience to improve collaboration between distributed collaborators [7,14,29,36,37,39,43,92]. Currently, there is too much focus on post-task evaluation. New paradigms must also consider continuous assessment, i.e., giving proper relevance to evaluation conducted during the accomplishment of open challenges, instead of pre-defined tasks, which fail short to mimic real scenarios of remote collaboration. As such, architectures and frameworks capable of supporting the new paradigm(s) must be created, to assist researchers conducting future user studies, while eliciting more characterization of the collaboration process in remote scenarios. Such frameworks must include support for: • defining the evaluation scope for individual and collective assessment by properly identifying which dimensions of collaboration will be evaluated; • detailing collaborative challenges to be performed, including specification of the users minimum level of knowledge, definition of each collaborator activity, as well as definition of the procedures; • defining the experimental setup and design, ensuring each dimension is defined in terms of the necessary variables and how they should be measured according to specific techniques; • conducting data gathering through the use of a distributed evaluation tools focusing in the dimensions proposed specifically for remote collaboration; • performing data analysis, including inspection of what happened during the tasks, to understand how the collaboration process occurred over time. 5.4 Development of tools for improved data gathering According to Marques et al. “it is essential the existence of holistic evaluation strategies that monitor the use and performance of the proposed solutions regarding the team, its members, and the technology, allowing adequate characterization and report of collaborative processes” [36]. To achieve this, the operationalization of data gathering should also deserve its own line of work due to its importance. It is paramount to conduct thorough collaborative user studies to provide new perspectives [14,36–38,111]. A better evaluation process can be supported by improved data collection and data visualization tools [92,112]. In this context, it is necessary to collect, process and analyze a multiplicity of data, e.g., context, history, user related information like actions, emotional state, as well as the results of processing the various components of the data gathering tools, aiming at obtaining a more comprehensive understanding. To accomplish this, tools must be designed and developed to allow researchers to run multiple evaluations at different locations simultaneously, following a distributed paradigm [36]. In this process, researchers should be able define measures, custom logging and register interesting events they detect, which can be later reviewed in post-task analysis, adapting and extending, for example the works by Pereira et al. [93,113,114]. Likewise, the following factors are crucial and must be taken in account to better understand the real impact of each aspect in the collaborative effort: team, collaborative tasks, context and AR-based solution [36]. These factors can help portrait the conditions in which collaborators performed a given action, received information or requested assistance. In addition, they can be used to assert uncommon situations or identify patterns that can lead to new understanding of a given artifact, as well as identify new research questions. Therefore, such tools are essential to help researchers when performing judgment over evaluation results. 5.5 New and better outcomes to support the assessment A better characterization of the collaborative process coupled with improved and specific evaluation tools and methods will provide ground to improve how research is reported. Thus, increasing the awareness of researchers about the different dimensions of collaboration and elicit better reporting, as researchers understand the need to improve how they describe the nuances associated to the collaborative effort of their work. Currently, in most cases, data relevant to characterize the collaborative context is not reported. To elaborate, most works focus only on individual performance, on the technological aspects of the AR-based solution or in quantifying effectiveness of tasks. It is important to consider a wide range of information, namely individual and team personalities, motivations, performances, behaviors, who completed the tasks and who provided instructions, how was the communication process, as well as duration and type of interactions with the collaborative AR-based technology, among other aspects when analyzing data and establishing conclusions. The reporting process must also integrate the context in which the collaborative effort took place, thus allowing the creation of a better understanding of the surrounding conditions, while contributing to support replication of such context, if they are relevant to other researchers, in future studies. Moreover, a complete definition of the data used to substantiate the usefulness of the results reported must be included, as well as the measures used, how was the data computed, based on what criteria, etc. This is essential to move towards replication and interpretability across contributions in the field. A more systematic reporting can, in turn, lead to a community setting that enables easier communication, understanding, reflection, comparison, refining, as well as building on existing research and foster harmonization of perspectives for the field. Furthermore, researchers can also compare their outcomes, as this is also a good opportunity for reflecting and refining. It is important to use what is learned during the studies and identify aspects which did not go according to what was expected or select additional ones which may improve on existing guidelines for future user studies. 6 Conclusions and future work Collaborative AR solutions can be powerful tools for analysis, discussion and support of complex problems and situations in remote scenarios. By bringing different and sometimes opposing points of view together, such solutions can lead to new insights, innovative ideas, and interesting artifacts. However, most research efforts have been devoted to creating the enabling technology for supporting the design and development of such solutions. Hence, the characterization and evaluation of the collaborative process is an essential, but a very difficult endeavor nowadays. This paper describes a critical analysis supported by surveys that addressed evaluation and user studies in scenarios of remote collaboration mediated by AR. In addition, a literature review on works ranging from 2000 to 2020 is also presented. Based on the limitations and challenges identified, we argue that remote collaboration mediated by AR is currently between the Replication and Empiricism phases of the BRETAM model. To contribute to an advance to Theory, Automation and Maturity phases, based in the critical analysis, we propose a roadmap for important research actions that need to be addressed to facilitate and elicit more characterization of the collaboration process using AR-based solutions in the future. Work is being continued through the creation of a conceptual model and taxonomy, as well as an initial architecture and framework aligned with the proposed roadmap. These can form the basis for a common ground, as well as the development of a framework for researchers who want to follow best practices in designing their own collaborative AR user studies in remote scenarios. CRediT authorship contribution statement Bernardo Marques: Conceptualization, Methodology, Validation, Formal analysis, Investigation, Data curation, Writing – original draft, Writing – review & editing, Visualization, Project administration, Funding acquisition. António Teixeira: Conceptualization, Methodology, Writing – original draft, Writing – review, Visualization. Samuel Silva: Methodology, Investigation, Writing – original draft, Writing – review & editing. João Alves: Validation, Formal analysis, Investigation, Data curation, Writing – original draft. Paulo Dias: Conceptualization, Investigation, Data curation, Writing – original draft, Writing – review & editing, Supervision, Funding acquisition. Beatriz Sousa Santos: Conceptualization, Methodology, Investigation, Writing – original draft, Writing – review & editing, Supervision Funding acquisition. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments We would like to thank the reviewers for their thoughtful comments and suggestions towards improving on an earlier version of this manuscript. To everyone involved in case studies, and discussion groups, thanks for your time and expertise. This research was developed in the scope of the Ph.D. grant [SFRH/BD/143276/2019], funded by FCT - Foundation for Science and Technology. It was also supported by IEETA - Institute of Electronics and Informatics Engineering of Aveiro, funded by National Funds through FCT , in the context of the project [UID/CEC/00127/2019]. References [1] Alem L. Tecchia F. Huang W. HandsOnVideo: Towards a gesture based mobile AR system for remote collaboration Recent Trends of Mobile Collaborative Augmented Reality Systems 2011 135 148 Alem L, Tecchia F, Huang W. Handsonvideo: Towards a gesture based mobile ar system for remote collaboration. In: Alem L, Huang W, editors. Recent Trends of Mobile Collaborative Augmented Reality Systems.2011, p. 135–148,. [2] Johnson S, Gibson M, Mutlu B. Handheld or handsfree? Remote collaboration via lightweight head-mounted displays and handheld devices. In: Proceedings of the 18th ACM conference on computer supported cooperative work & social computing; 2015, p. 1825–36. [3] Lukosch S. Billinghurst M. Alem L. Kiyokawa K. Collaboration in augmented reality Computer Supported Cooperative Work, CSCW 2015 2015 515 525 Lukosch S, Billinghurst M, Alem L, Kiyokawa K. Collaboration in Augmented Reality. In: Computer Supported Cooperative Work, CSCW 2015 vol. 24.2015, p. 515–525,. [4] Schneider M, Rambach J, Stricker D. Augmented reality based on edge computing using the example of remote live support. In: 2017 IEEE international conference on industrial technology; 2017, p. 1277–82. [5] Thomas P.J. CSCW Requirements and Evaluation 1996 Springer-Verlag Berlin, Heidelberg Thomas PJ. CSCW Requirements and Evaluation. Berlin, Heidelberg: Springer-Verlag;1996. [6] Kim K. Billinghurst M. Bruder G. Duh H.B. Welch G.F. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017) IEEE Trans Vis Comput Graphics 24 2018 2947 2962 Kim K, Billinghurst M, Bruder G, Duh HB, Welch GF. Revisiting trends in augmented reality research: A review of the 2nd decade of ISMAR (2008-2017). IEEE Transactions on Visualization and Computer Graphics2018a;24:2947–2962,. [7] Kim S. Billinghurst M. Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration Comput Support Coop Work: CSCW: An Int J 27 3–6 2018 569 607 Kim S, Billinghurst M, Lee G. The effect of collaboration styles and view independence on video-mediated remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal2018b;27(3-6):569–607,. [8] Kim S. Lee G. Billinghurst M. Huang W. The combination of visual communication cues in mixed reality remote collaboration J Multimodal User Interf 14 4 2020 321 335 Kim S, Lee G, Billinghurst M, Huang W. The combination of visual communication cues in mixed reality remote collaboration. Journal on Multimodal User Interfaces2020a;14(4):321–335,. [9] Kim S. Billinghurst M. Kim K. Multimodal interfaces and communication cues for remote collaboration J Multimodal User Interf 14 4 2020 313 319 Kim S, Billinghurst M, Kim K. Multimodal interfaces and communication cues for remote collaboration. Journal on Multimodal User Interfaces2020b;14(4):313–319,. [10] Arias E. Eden H. Fischer G. Gorman A. Scharff E. Transcending the individual human mind-creating shared understanding through collaborative design ACM Trans Comput-Human Inter 7 2000 84 113 Arias E, Eden H, Fischer G, Gorman A, Scharff E. Transcending the Individual Human Mind-Creating Shared Understanding through Collaborative Design. ACM Transactions on Computer-Human Interaction2000;7:84–113,. [11] Grudin J. Poltrock S. The Encyclopedia of Human-Computer Interaction 2013 The Interaction Design Foundation Grudin J, Poltrock S. The Encyclopedia of Human-Computer Interaction. The Interaction Design Foundation;2013. [12] Billinghurst M. Clark A. Lee G. A survey of augmented reality Found Trends Human–Comput Interact 8 2015 73 272 Billinghurst M, Clark A, Lee G. A Survey of Augmented Reality. Foundations and Trends in Human–Computer Interaction2015;8:73–272,. [13] Jalo H, Pirkkalainen H, Torro O, Kärkkäinen H, Puhto J, Kankaanpää T. How can collaborative augmented reality support operative work in the facility management industry?. In: Proceedings of the 10th international joint conference on knowledge discovery, knowledge engineering and knowledge management; 2018. p. 41–51. [14] Ens B. Lanir J. Tang A. Bateman S. Lee G. Piumsomboon T. Billinghurst M. Revisiting collaboration through mixed reality: The evolution of groupware Int J Human-Comput Stud 131 2019 81 98 Ens B, Lanir J, Tang A, Bateman S, Lee G, Piumsomboon T,, others,Revisiting Collaboration through Mixed Reality: The Evolution of Groupware. International Journal of Human-Computer Studies2019;131:81–98,. [15] Bruno F. Barbieri L. Marino E. Muzzupappa M. D’Oriano L. Colacino B. An augmented reality tool to detect and annotate design variations in an industry 4.0 approach Int J Adv Manuf Technol 105 1 2019 875 887 Bruno F, Barbieri L, Marino E, Muzzupappa M, D’Oriano L, Colacino B. An augmented reality tool to detect and annotate design variations in an Industry 4.0 approach. The International Journal of Advanced Manufacturing Technology2019;105(1):875–887,. [16] Ong S.K. Yuan M.L. Nee A.Y.C. Augmented reality applications in manufacturing: A survey Int J Prod Res 46 10 2008 2707 2742 Ong SK, Yuan ML, Nee AYC. Augmented reality applications in manufacturing: A survey. International Journal of Production Research2008;46(10):2707–2742,. [17] Wang X. Ong S.K. Nee A.Y.C. A comprehensive survey of augmented reality assembly research Adv Manuf 4 1 2016 1 22 Wang X, Ong SK, Nee AYC. A comprehensive survey of augmented reality assembly research. Advances in Manufacturing2016;4(1):1–22,. [18] Palmarini R. Erkoyuncu J.A. Roy R. Torabmostaedi H. A systematic review of augmented reality applications in maintenance Robot Comput-Integr Manuf 49 2018 215 228 Palmarini R, Erkoyuncu JA, Roy R, Torabmostaedi H. A systematic review of augmented reality applications in maintenance. Robotics and Computer-Integrated Manufacturing2018;49:215–228,. [19] Hall M, McMahon CA, Bermell-Garcia P, Johansson A, Ravindranath R. Capturing synchronous collaborative design activities: A state-of-the-art technology review. In: Proceedings of international design conference, DESIGN 2018; 2018. p. 347–58. [20] Billinghurst M. Grand challenges for augmented reality Front Virtual Reality 2 2021 1 4 Billinghurst M. Grand Challenges for Augmented Reality. Frontiers in Virtual Reality2021;2:1–4,. [21] Altug Y. Mahdy A.M. A perspective on distributed and collaborative augmented reality Int J Recent Trends Human Comput Interact (IJHCI) 7 2016 23 41 Altug Y, Mahdy AM. A Perspective on Distributed and Collaborative Augmented Reality. International Journal of Recent Trends in Human Computer Interaction (IJHCI)2016;7:23–41,. [22] Choi S. Kim M. Lee J. Situation-dependent remote AR collaborations: Image-based collaboration using a 3D perspective map and live video-based collaboration with a synchronized VR mode Comput Ind 101 2018 51 66 Choi S, Kim M, Lee J. Situation-dependent remote AR collaborations: Image-based collaboration using a 3D perspective map and live video-based collaboration with a synchronized VR mode. Computers in Industry2018;101:51–66,. [23] Bottani E. Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade IISE Trans 51 3 2019 284 310 Bottani E, Vignali G. Augmented reality technology in the manufacturing industry: A review of the last decade. IISE Transactions2019;51(3):284–310,. [24] van Lopik K. Sinclair M. Sharpe R. Conway P. West A. Developing augmented reality capabilities for industry 4.0 small enterprises: Lessons learnt from a content authoring case study Comput Ind 117 2020 van Lopik K, Sinclair M, Sharpe R, Conway P, West A. Developing augmented reality capabilities for industry 4.0 small enterprises: Lessons learnt from a content authoring case study. Computers in Industry2020;117. [25] Belen R.A.J. Nguyen H. Filonik D. Favero D.D. Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018 AIMS Electron Electr Eng 3 2019 181 Belen RAJ, Nguyen H, Filonik D, Favero DD, Bednarz T. A systematic review of the current state of collaborative mixed reality technologies: 2013–2018. AIMS Electronics and Electrical Engineering2019;3:181,. [26] Oda O, Elvezio C, Sukan M, Feiner S, Tversky B. Virtual replicas for remote assistance in virtual and augmented reality. In: Proceedings of the 28th annual acm symposium on user interface software & technology - UIST ’15; 2015. p. 405–15. [27] Elvezio C. Sukan M. Oda O. Feiner S. Tversky B. Remote collaboration in AR and VR using virtual replicas ACM SIGGRAPH 2017 2017 1 2 Elvezio C, Sukan M, Oda O, Feiner S, Tversky B. Remote collaboration in AR and VR using virtual replicas. ACM SIGGRAPH 20172017;:1–2,. [28] Barroso Ja, Fonseca L, Marques B, Dias P, Sousa BS. Remote collaboration using mixed reality: Exploring a shared model approach through different interaction methods. In: Proceedings of european conference on computer-supported cooperative work, ecscw 2020 posters; 2020. p. 1–6. [29] Bai Z. Blackwell A.F. Analytic review of usability evaluation in ISMAR Interact Comput 24 6 2012 450 460 Bai Z, Blackwell AF. Analytic review of usability evaluation in ISMAR. Interacting with Computers2012;24(6):450 – 460,. [30] Zillner J, Mendez E, Wagner D. Augmented reality remote collaboration with dense reconstruction. In: Adjunct proceedings - 2018 IEEE international symposium on mixed and augmented reality, ismar-adjunct 2018; 2018. p. 38–9. [31] Neale DC, Carroll JM, Rosson MB. Evaluating computer-supported cooperative work: Models and frameworks. In: Proceedings of the 2004 ACM conference on computer supported cooperative work, CSCW ’04; 2004. p. 112–21. [32] de Souza Cardoso L.F. Mariano F.C.M.Q. Zorzal E.R. A survey of industrial augmented reality Comput Ind Eng 139 2020 106159 de Souza Cardoso LF, Mariano FCMQ, Zorzal ER. A survey of industrial augmented reality. Computers & Industrial Engineering2020;139:106159,. [33] Röltgen D. Dumitrescu R. Classification of industrial augmented reality use cases Procedia CIRP 91 2020 93 100 Röltgen D, Dumitrescu R. Classification of industrial augmented reality use cases. Procedia CIRP2020;91:93 – 100,. [34] Fernández del Amo I. Erkoyuncu J.A. Roy R. Wilding S. Augmented reality in maintenance: An information-centred design framework Proc Manuf 19 2018 148 155 Fernández del Amo I, Erkoyuncu JA, Roy R, Wilding S. Augmented reality in maintenance: An information-centred design framework. Procedia Manufacturing2018;19:148 – 155,. [35] Jetter J. Eimecke J. Rese A. Augmented reality tools for industrial applications: What are potential key performance indicators and who benefits? Comput Hum Behav 87 2018 18 33 Jetter J, Eimecke J, Rese A. Augmented reality tools for industrial applications: What are potential key performance indicators and who benefits? Computers in Human Behavior2018;87:18–33,. [36] Marques B, Teixeira A, Silva S, Alves Ja, Dias P, Santos BS. A conceptual model for data collection and analysis for AR-based remote collaboration evaluation. In: IEEE international symposium on mixed and augmented reality, ISMAR; 2020. [37] Merino L, Schwarzl M, Kraus M, Sedlmair M, Schmalstieg D, Weiskopf D. Evaluating mixed and augmented reality: A systematic literature review (2009–2019). In: IEEE international symposium on mixed and augmented reality, ISMAR; 2020. p. 438–51. [38] Hamadache K. Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation Groupware: Design, Implementation, and Use 2009 206 221 Hamadache K, Lancieri L. Strategies and taxonomy, tailoring your CSCW evaluation. In: Groupware: Design, Implementation, and Use.2009, p. 206–221,. [39] Antunes P. Herskovic V. Ochoa S.F. Pino J.A. Reviewing the quality of awareness support in collaborative applications J Syst Softw 89 2014 146 169 Antunes P, Herskovic V, Ochoa SF, Pino JA. Reviewing the quality of awareness support in collaborative applications. Journal of Systems and Software2014;89:146–169,. [40] Patel H. Pettitt M. Wilson J.R. Factors of collaborative working: A framework for a collaboration model Applied Ergon 43 1 2012 1 26 Patel H, Pettitt M, Wilson JR. Factors of collaborative working: A framework for a collaboration model. Applied ergonomics2012;43(1):1–26,. [41] Duenser A. Grasset R. Billinghurst M. A survey of evaluation techniques used in augmented reality studies SIGGRAPH 2008 2008 Duenser A, Grasset R, Billinghurst M. A survey of evaluation techniques used in augmented reality studies. In: SIGGRAPH 2008.2008,. [42] Feng Zhou, Duh HB, Billinghurst M. Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR. In: International symposium on mixed and augmented reality; 2008. p. 193–202. [43] Dey A. Billinghurst M. Lindeman R.W. Swan J.E. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014 Front Robot AI 5 2018 37 Dey A, Billinghurst M, Lindeman RW, Swan JE. A systematic review of 10 years of augmented reality usability studies: 2005 to 2014. Frontiers in Robotics and AI2018;5:37,. [44] Gutwin C. Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware ACM Trans Comput-Hum Interact 6 3 1999 243 281 Gutwin C, Greenberg S. The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Trans Comput-Hum Interact1999;6(3):243–281,. [45] Speicher M. Hall B.D. Nebeling M. What is mixed reality? Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems 2019 1 15 Speicher M, Hall BD, Nebeling M. What is mixed reality? In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. CHI ’19;2019, p. 1–15,. [46] Kato H, Billinghurst M. Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings 2nd IEEE and ACM international workshop on augmented reality (IWAR’99); 1999. p. 85–94. [47] Milgram P. Takemura H. Utsumi A. Kishino F. Augmented reality: A class of displays on the reality-virtuality continuum Photon Indust Appl 1994 282 292 Milgram P, Takemura H, Utsumi A, Kishino F. Augmented reality: A class of displays on the reality-virtuality continuum. Photonics for industrial applications1994;:282–292,. [48] Milgram P. Kishino F. A taxonomy of mixed reality visual displays IEICE Trans Inform Syst 0916-8532 77 12 1994 1321 1329 Milgram P, Kishino F. A taxonomy of mixed reality visual displays. IEICE TRANSACTIONS on Information and Systems1994;77(12):1321–1329,. [49] Rokhsaritalemi S. Sadeghi-Niaraki A. Choi S.-M. A review on mixed reality: Current trends, challenges and prospects Appl Sci 10 2 2020 Rokhsaritalemi S, Sadeghi-Niaraki A, Choi SM. A review on mixed reality: Current trends, challenges and prospects. Applied Sciences2020;10(2). [50] Bai H. Sasikumar P. Yang J. Billinghurst M. A user study on mixed reality remote collaboration with eye gaze and hand gesture sharing Conf Human Factors Comput Syst - Proc 2020 1 13 Bai H, Sasikumar P, Yang J, Billinghurst M. A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing. Conference on Human Factors in Computing Systems - Proceedings2020;:1–13,. [51] Masai K, Kunze K, Sugimoto M, Billinghurst M. Empathy glasses. In: Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems, CHI EA ’16; 2016. p. 1257–63. [52] Piumsomboon T. Dey A. Ens B. Lee G. Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality Front Robot AI 6 2019 Piumsomboon T, Dey A, Ens B, Lee G, Billinghurst M. The effects of sharing awareness cues in collaborative mixed reality. Frontiers Robotics AI2019a;6. [53] Wang P. Bai X. Billinghurst M. Zhang S. Han D. Sun M. Wang Z. Lv H. Han S. Haptic feedback helps me? A VR-SAR remote collaborative system with tangible interaction Int J Human-Comput Interact 2020 Wang P, Bai X, Billinghurst M, Zhang S, Han D, Sun M,, others,Haptic feedback helps me? a VR-SAR remote collaborative system with tangible interaction. International Journal of Human-Computer Interaction2020;. [54] Rhee T. Thompson S. Medeiros D. Dos Anjos R. Chalmers A. Augmented virtual teleportation for high-fidelity telecollaboration IEEE Trans Vis Comput Graphics 2020 Rhee T, Thompson S, Medeiros D, Dos Anjos R, Chalmers A. Augmented virtual teleportation for high-fidelity telecollaboration. IEEE Transactions on Visualization and Computer Graphics2020;. [55] Teo T, Lee G, Billinghurst M, Adcock M. Investigating the use of different visual cues to improve social presence within a 360 mixed reality remote collaboration. In: Proceedings - VRCAI 2019: 17th ACM siggraph international conference on virtual-reality continuum and its applications in industry; 2019. [56] Teo T, Hayati A, Lee G, Billinghurst M, Adcock M. A technique for mixed reality remote collaboration using 360 panoramas in 3D reconstructed scenes. In: Proceedings of the ACM symposium on virtual reality software and technology, VRST; 2019. [57] Sasikumar P, Gao L, Bai H, Billinghurst M. Wearable remotefusion: A mixed reality remote collaboration system with local eye gaze and remote hand gesture sharing. In: Adjunct Proceedings of the 2019 IEEE international symposium on mixed and augmented reality, ismar-adjunct 2019; 2019. p. 393–4. [58] Mahmood T, Fulmer W, Mungoli N, Huang J, Lu A. Improving information sharing and collaborative analysis for remote geospatial visualization using mixed reality. In: Proceedings - 2019 IEEE international symposium on mixed and augmented reality, ISMAR 2019; 2019. p. 236–47. [59] Teo T. Lawrence L. Lee G. Billinghurst M. Adcock M. Mixed reality remote collaboration combining 360 video and 3D reconstruction Conf Human Factors Comput Syst 2019 Teo T, Lawrence L, Lee G, Billinghurst M, Adcock M. Mixed reality remote collaboration combining 360 video and 3D reconstruction. Conference on Human Factors in Computing Systems2019c;. [60] Piumsomboon T. Lee G. Irlitti A. Ens B. Thomas B. Billinghurst M. On the shoulder of the giant: A multi-scale mixed reality collaboration with 360 video sharing and tangible interaction Conf Human Factors Comput Syst 2019 Piumsomboon T, Lee G, Irlitti A, Ens B, Thomas B, Billinghurst M. On the shoulder of the giant: A multi-scale mixed reality collaboration with 360 video sharing and tangible interaction. Conference on Human Factors in Computing Systems2019b;. [61] Yoon B, Kim H-I, Lee G, Billinqhurst M, Woo W. The effect of avatar appearance on social presence in an augmented reality remote collaboration. In: 26th IEEE conference on virtual reality and 3d user interfaces, vr 2019 - proceedings; 2019. p. 547–56. [62] Wang P. Zhang S. Billinghurst M. Bai X. He W. Wang S. Sun M. Zhang X. A comprehensive survey of AR/MR-based co-design in manufacturing Eng Comput 2019 Wang P, Zhang S, Billinghurst M, Bai X, He W, Wang S,, others,A comprehensive survey of AR/MR-based co-design in manufacturing. Engineering with Computers2019;. [63] Lee G, Teo T, Kim S, Billinghurst M. A user study on MR remote collaboration using live 360 video. In: Proceedings of the 2018 ieee international symposium on mixed and augmented reality, ISMAR 2018; 2019. p. 153–64. [64] Waldow K. Fuhrmann A. Grünvogel S. Investigating the effect of embodied visualization in remote collaborative augmented reality Lecture Notes in Comput Sci 11883 LNCS 2019 246 262 Waldow K, Fuhrmann A, Grünvogel S. Investigating the effect of embodied visualization in remote collaborative augmented reality. Lecture Notes in Computer Science2019;11883 LNCS:246–262,. [65] Teo T. Lee G. Billinghurst M. Adcock M. Hand gestures and visual annotation in live 360 panorama-based mixed reality remote collaboration ACM Int Conf Proc Ser 2018 406 410 Teo T, Lee G, Billinghurst M, Adcock M. Hand gestures and visual annotation in live 360 panorama-based mixed reality remote collaboration. ACM International Conference Proceeding Series2018;:406–410,. [66] Kim S. Billinghurst M. Lee C. Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration KSII Trans Internet Inform Syst 12 12 2018 6034 6056 Kim S, Billinghurst M, Lee C, Lee G. Using freeze frame and visual notifications in an annotation drawing interface for remote collaboration. KSII Transactions on Internet and Information Systems2018c;12(12):6034–6056,. [67] Congdon B, Wang T, Steed A. Merging environments for shared spaces in mixed reality. In: Proceedings of the ACM symposium on virtual reality software and technology, VRST; 2018. [68] Yamada S, Chandrasiri N. Evaluation of hand gesture annotation in remote collaboration using augmented reality. In: 25th IEEE conference on virtual reality and 3d user interfaces, vr 2018 - proceedings; 2018. p. 727–8. [69] Günther S. Avrahami D. Kratz S. Mühlhäuser M. Exploring audio, visual, and tactile cues for synchronous remote assistance ACM Int Conf Proc Ser 2018 339 344 Günther S, Avrahami D, Kratz S, Mühlhäuser M. Exploring audio, visual, and tactile cues for synchronous remote assistance. ACM International Conference Proceeding Series2018;:339–344,. [70] Piumsomboon T. Lee G. Hart J. Ens B. Lindeman R. Thomas B. Billinghurst M. Mini-me: An adaptive avatar for mixed reality remote collaboration Conf Human Factors Comput Syst 2018-April 2018 Piumsomboon T, Lee G, Hart J, Ens B, Lindeman R, Thomas B,, others,Mini-me: An adaptive avatar for mixed reality remote collaboration. Conference on Human Factors in Computing Systems2018;2018-April. [71] Ryskeldiev B. Cohen M. Herder J. Stream space: Pervasive mixed reality telepresence for remote collaboration on mobile devices J Inform Proc 26 2018 177 185 Ryskeldiev B, Cohen M, Herder J. Stream space: Pervasive mixed reality telepresence for remote collaboration on mobile devices. Journal of Information Processing2018;26:177–185,. [72] Hoppe A. Reeb R. van de Camp F. Stiefelhagen R. Interaction of distant and local users in a collaborative virtual environment Lecture Notes in Comput Sci 10909 LNCS 2018 328 337 Hoppe A, Reeb R, van de Camp F, Stiefelhagen R. Interaction of distant and local users in a collaborative virtual environment. Lecture Notes in Computer Science2018;10909 LNCS:328–337,. [73] Akkil D. Isokoski P. Comparison of gaze and mouse pointers for video-based collaborative physical task Interact Comput 30 6 2018 524 542 Akkil D, Isokoski P. Comparison of gaze and mouse pointers for video-based collaborative physical task. Interacting with Computers2018;30(6):524–542,. [74] Lee G. Teo T. Kim S. Billinghurst M. Mixed reality collaboration through sharing a live panorama SIGGRAPH Asia 2017 Mobile Graphics and Interactive Applications 2017 Lee G, Teo T, Kim S, Billinghurst M. Mixed reality collaboration through sharing a live panorama. SIGGRAPH Asia 2017 Mobile Graphics and Interactive Applications2017a;. [75] Lee G, Kim S, Lee Y, Dey A, Piumsomboon T, Norman M, Billinghurst M. Mutually shared gaze in augmented video conference. In: Adjunct proceedings of the 2017 ieee international symposium on mixed and augmented reality, ismar-adjunct 2017; 2017. p. 79–80. [76] Komiyama R. Miyaki T. Rekimoto J. Jackin space: Designing a seamless transition between first and third person view for effective telepresence collaborations ACM Int Conf Proc Ser 2017 Komiyama R, Miyaki T, Rekimoto J. Jackin space: Designing a seamless transition between first and third person view for effective telepresence collaborations. ACM International Conference Proceeding Series2017;. [77] Chenechal M, Duval T, Gouranton V, Royan J, Arnaldi B. Vishnu: Virtual immersive support for HelpiNg users an interaction paradigm for collaborative remote guiding in mixed reality. In: 2016 IEEE 3rd vr international workshop on collaborative virtual environments, 3dcve 2016; 2016. p. 9–12. [78] Gurevich P. Lanir J. Cohen B. Design and implementation of TeleAdvisor: a projection-based augmented reality system for remote collaboration Comput Support Coop Work: CSCW: An Int J 24 6 2015 527 562 Gurevich P, Lanir J, Cohen B. Design and implementation of teleadvisor: a projection-based augmented reality system for remote collaboration. Computer Supported Cooperative Work: CSCW: An International Journal2015;24(6):527–562,. [79] Tait M. Billinghurst M. The effect of view independence in a collaborative AR system Comput Support Coop Work: CSCW: An Int J 24 6 2015 563 589 Tait M, Billinghurst M. The effect of view independence in a collaborative AR system. Computer Supported Cooperative Work: CSCW: An International Journal2015;24(6):563–589,. [80] Kim S. Lee G. Ha S. Sakata N. Billinghurst M. Automatically freezing live video for annotation during remote collaboration Conf Human Factors Comput Syst 18 2015 1669 1674 Kim S, Lee G, Ha S, Sakata N, Billinghurst M. Automatically freezing live video for annotation during remote collaboration. Conference on Human Factors in Computing Systems2015;18:1669–1674,. [81] Tait M, Billinghurst M. View independence in remote collaboration using AR. In: ISMAR 2014 - IEEE international symposium on mixed and augmented reality - science and technology 2014, proceedings; 2014. p. 309–10. [82] Kim S, Lee G, Sakata N, Billinghurst M. Improving co-presence with augmented visual communication cues for sharing experience through video conference. In: ISMAR 2014 - IEEE international symposium on mixed and augmented reality - science and technology 2014, proceedings; 2014. p. 83–92. [83] Gauglitz S, Nuernberger B, Turk M, Höllerer T. In touch with the remote world: Remote collaboration with augmented reality drawings and virtual navigation. In: Proceedings of the ACM symposium on virtual reality software and technology, vrst; 2014. p. 197–205. [84] Gauglitz S, Nuernberger B, Turk M, Höllerer T. World-stabilized annotations and virtual scene navigation for remote collaboration. In: UIST 2014 - Proceedings of the 27th annual acm symposium on user interface software and technology; 2014. p. 449–60. [85] Huang W. Alem L. Tecchia F. HandsIn3D: Supporting remote guidance with immersive virtual environments Lecture Notes in Comput Sci 8117 LNCS PART 1 2013 70 77 Huang W, Alem L, Tecchia F. Handsin3d: Supporting remote guidance with immersive virtual environments. Lecture Notes in Computer Science2013;8117 LNCS(PART 1):70–77,. [86] Pece F. Steptoe W. Wanner F. Julier S. Weyrich T. Kautz J. Steed A. PanoInserts: Mobile spatial teleconferencing Conf Human Factors Comput Syst 2013 1319 1328 Pece F, Steptoe W, Wanner F, Julier S, Weyrich T, Kautz J,, others,Panoinserts: Mobile spatial teleconferencing. Conference on Human Factors in Computing Systems2013;:1319–1328,. [87] Poppe E, Brown R, Johnson D, Recker J. Preliminary evaluation of an augmented reality collaborative process modelling system. In: Proceedings of the 2012 international conference on cyberworlds; 2012. p. 77–84. [88] Gauglitz S, Lee C, Turk M, Höllerer T. Integrating the physical environment into mobile remote collaboration. In: MobileHCI’12 - proceedings of the 14th international conference on human computer interaction with mobile devices and services; 2012. p. 241–50. [89] Barakonyi I, Prendinger H, Schmalstieg D, Ishizuka M. Cascading hand and eye movement for augmented reality videoconferencing. In: IEEE symposium on 3D user interfaces, 3dui 2007; 2007. p. 71–78. [90] Bannai Y. Tamaki H. Suzuki Y. Shigeno H. Okada K. A tangible user interface for remote collaboration system using mixed reality Lecture Notes in Comput Sci 4282 2006 143 154 Bannai Y, Tamaki H, Suzuki Y, Shigeno H, Okada K. A tangible user interface for remote collaboration system using mixed reality. Lecture Notes in Computer Science2006;4282:143–154,. [91] Regenbrecht H. Lum T. Kohler P. Ott C. Wagner M. Wilke W. Mueller E. Using augmented virtuality for remote collaboration Presence: Teleoperat Virtual Environ 13 3 2004 338 354 Regenbrecht H, Lum T, Kohler P, Ott C, Wagner M, Wilke W,, others,Using augmented virtuality for remote collaboration. Presence: Teleoperators and Virtual Environments2004;13(3):338–354,. [92] Araujo R.M. Santoro F.M. Borges M.R.S. A conceptual framework for designing and conducting groupware evaluations Int J Comput Appl Technol 19 3 2004 139 150 Araujo RM, Santoro FM, Borges MRS. A conceptual framework for designing and conducting groupware evaluations. International Journal of Computer Applications in Technology2004;19(3):139–150,. [93] Pereira C, Teixeira A, e Silva MO. Live evaluation within ambient assisted living scenarios. In: Proceedings of the 7th international conference on pervasive technologies related to assistive environments, PETRA 2014; 2014. [94] Ratcliffe J. Soave F. Bryan-Kinns N. Tokarchuk L. Farkhatdinov I. Extended reality (XR) remote research: a survey of drawbacks and opportunities CHI Conference on Human Factors in Computing Systems 2021 1 13 Ratcliffe J, Soave F, Bryan-Kinns N, Tokarchuk L, Farkhatdinov I. Extended Reality (XR) Remote Research: a Survey of Drawbacks and Opportunities. In: CHI Conference on Human Factors in Computing Systems.2021, p. 1–13,. [95] Gaines B.R. Modeling and forecasting the information sciences Inform Sci 57–58 1991 3 22 Information Sciences-Past, Present, and Future Gaines BR. Modeling and forecasting the information sciences. Information Sciences1991;57-58:3 – 22,. Information Sciences-Past, Present, and Future. [96] Lalanne D. Nigay L. Palanque p. Robinson P. Vanderdonckt J. Ladry J.-F.c. Fusion engines for multimodal input: A survey Proceedings of the 2009 International Conference on Multimodal Interfaces ICMI-MLMI ’09 2009 Association for Computing Machinery New York, NY, USA 153 160 Lalanne D, Nigay L, Palanque p, Robinson P, Vanderdonckt J, Ladry JF. Fusion engines for multimodal input: A survey. In: Proceedings of the 2009 International Conference on Multimodal Interfaces. ICMI-MLMI ’09; New York, NY, USA: Association for Computing Machinery;2009, p. 153–160,. [97] Teixeira A. A critical analysis of speech-based interaction in healthcare robots: Making a case for the increased use of speech in medical and assistive robots Speech and Automata in Healthcare : Voice-Controlled Medical and Surgical Robots - Chapter 1 2014 29 Teixeira A. A critical analysis of speech-based interaction in healthcare robots: Making a case for the increased use of speech in medical and assistive robots. In: Speech and Automata in Healthcare : Voice-Controlled Medical and Surgical Robots - Chapter 1.2014, p. 29,. [98] Billinghurst M. Kato H. Collaborative augmented reality Commun ACM 45 2003 Billinghurst M, Kato H. Collaborative augmented reality. Communications of the ACM2003;45. [99] Wang X, Dunston PS. Groupware concepts for augmented reality mediated human-to-human collaboration. In: Joint international conference on computing and decision making in civil and building engineering; 2006. p. 1836–42. [100] Brockmann T, Krueger N, Stieglitz S, Bohlsen I. A framework for collaborative augmented reality applications. In: Proceedings of the nineteenth americas conference on information systems; 2013, p. 1–10. [101] Sereno M. Wang X. Besancon L. Mcguffin M.J. Isenberg T. Collaborative work in augmented reality: A survey IEEE Trans Vis Comput Graphics 2020 1 20 Sereno M, Wang X, Besancon L, Mcguffin MJ, Isenberg T. Collaborative work in augmented reality: A survey. IEEE Transactions on Visualization and Computer Graphics2020;:1–20,. [102] Collazos C.A. Gutiérrez F.L. Gallardo J. Ortega M. Fardoun H.M. Molina A.I. Descriptive theory of awareness for groupware development J Ambient Intell Humaniz Comput 10 12 2019 4789 4818 Collazos CA, Gutiérrez FL, Gallardo J, Ortega M, Fardoun HM, Molina AI. Descriptive theory of awareness for groupware development. Journal of Ambient Intelligence and Humanized Computing2019;10(12):4789–4818,. [103] Talkad Sukumar P, Avellino I, Remy C, DeVito MA, Dillahunt TR, McGrenere J, Wilson ML. Transparency in qualitative research: Increasing fairness in the chi review process. In: Extended abstracts of the 2020 chi conference on human factors in computing systems; 2020. p. 1–6. [104] Meyer M. Dykes J. Criteria for rigor in visualization design study IEEE Trans Vis Comput Graphics 26 1 2019 87 97 Meyer M, Dykes J. Criteria for rigor in visualization design study. IEEE transactions on visualization and computer graphics2019;26(1):87–97,. [105] Augstein M. Neumayr T. A human-centered taxonomy of interaction modalities and devices Interact Comput 31 2019 27 58 Augstein M, Neumayr T. A human-centered taxonomy of interaction modalities and devices. Interacting with Computers2019;31:27–58,. [106] Nickerson R.C. Varshney U. Muntermann J. A method for taxonomy development and its application in information systems Eur J Inform Syst 22 2013 336 359 Nickerson RC, Varshney U, Muntermann J. A method for taxonomy development and its application in information systems. European Journal of Information Systems2013;22:336–359,. [107] Teruel M.A. Navarro E. López-Jaquero V. Montero F. González P. A comprehensive framework for modeling requirements of CSCW systems J Softw: Evolu Process 29 5 2017 e1858 Teruel MA, Navarro E, López-Jaquero V, Montero F, González P. A comprehensive framework for modeling requirements of cscw systems. Journal of Software: Evolution and Process2017;29(5):e1858,. [108] Zollmann S. Grasset R. Langlotz T. Lo W.H. Mori S. Regenbrecht H. Visualization techniques in augmented reality: A taxonomy, methods and patterns IEEE Trans Vis Comput Graphics 2020 1 20 Zollmann S, Grasset R, Langlotz T, Lo WH, Mori S, Regenbrecht H. Visualization techniques in augmented reality: A taxonomy, methods and patterns. IEEE Transactions on Visualization and Computer Graphics2020;:1–20,. [109] Chandrasekaran B. Josephson J.R. Benjamins V.R. What are ontologies, and why do we need them? IEEE Intell Syst Appl 14 1 1999 20 26 Chandrasekaran B, Josephson JR, Benjamins VR. What are ontologies, and why do we need them? IEEE Intelligent Systems and Their Applications1999;14(1):20–26,. [110] Noy N.F. McGuinness D.L. Ontology development 101: A guide to creating your first ontology Stanford Knowl Syst Lab Tech Rep 15 2 2001 1 25 Noy NF, McGuinness DL. Ontology Development 101: A Guide to Creating Your First Ontology. Stanford Knowledge Systems Laboratory Technical Report2001;15(2):1–25,. [111] Herskovic V. Pino J.A. Ochoa S.F. Antunes P. Evaluation methods for groupware systems Haake J.M. Ochoa S.F. Cechich A. Groupware: Design, Implementation, and Use 2007 Springer, Berlin, Heidelberg 328 336 Herskovic V, Pino JA, Ochoa SF, Antunes P. Evaluation methods for groupware systems. In: Haake JM, Ochoa SF, Cechich A, editors. Groupware: Design, Implementation, and Use. Springer, Berlin, Heidelberg;2007, p. 328–336,. [112] de Araujo RM, Santoro FM, Borges MRS. The CSCW lab ontology for groupware evaluation. In: 8th international conference on computer supported cooperative work in design, Vol. 2; 200. p. 148–53. [113] Pereira C. Almeida N. Martins A.I. Silva S. Rosa A.F. Oliveira e Silva M. Teixeira A. Evaluation of complex distributed multimodal applications: Evaluating a TeleRehabilitation system when it really matters Lecture Notes in Comput Sci (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 2015 146 157 Pereira C, Almeida N, Martins AI, Silva S, Rosa AF, Oliveira e Silva M,, others,Evaluation of complex distributed multimodal applications: Evaluating a TeleRehabilitation system when it really matters. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)2015;:146–157,. [114] Pereira C. Teixeira A. Oliveira e Silva M. Dynamic evaluation for reactive scenarios (Ph.D. thesis) 2016 University of Aveiro (MAPi) 1 196 Pereira C, Teixeira A, Oliveira e Silva M. Dynamic Evaluation for Reactive Scenarios, Ph.D. Dissertation. University of Aveiro (MAPi). Ph.D. thesis;2016. "
    },
    {
        "doc_title": "Named Entity Extractors for New Domains by Transfer Learning with Automatically Annotated Data",
        "doc_scopus_id": "85127118131",
        "doc_doi": "10.1007/978-3-030-98305-5_27",
        "doc_eid": "2-s2.0-85127118131",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic annotation",
            "Human errors",
            "Labeled data",
            "Named entities",
            "Named entity extractors",
            "Named entity recognition",
            "Natural languages",
            "New applications",
            "Transfer learning"
        ],
        "doc_abstract": "© 2022, Springer Nature Switzerland AG.Named entity recognition (NER) tasks imply token-level labels. Annotating documents can be time-consuming, costly, and prone to human error. In many real-life scenarios, the lack of labeled data has become the biggest bottleneck preventing NER being effectively used in some domains and with some natural languages, with negative impacts on the quality of some tasks. To overcome the barrier of the lack of annotated data for new application domains in some natural languages, we propose a method that uses the output of an ensemble of NER’s to automatically annotate the data needed to train a Bidirectional Encoder Representations from Transformers (BERT) based NER for Portuguese. The performance was assessed using MiniHAREM dataset with promising results. For domain relevant classes such as LOCAL, F1, Precision and Recall above 50% were obtained when training only with automatically annotated data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Radar-Based Gesture Recognition Towards Supporting Communication in Aphasia: The Bedroom Scenario",
        "doc_scopus_id": "85125218898",
        "doc_doi": "10.1007/978-3-030-94822-1_30",
        "doc_eid": "2-s2.0-85125218898",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Aphasia",
            "Assistive technology",
            "Communication disorders",
            "Daily lives",
            "FMCW radar",
            "Gesture",
            "Gestures recognition",
            "In-bed scenario",
            "Input modalities",
            "Smart environment"
        ],
        "doc_abstract": "© 2022, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.Aphasia and other communication disorders affect a person’s daily life, leading to isolation and lack of self-confidence, affecting independence, and hindering the ability to express themselves easily, including asking for help. Even though assistive technology for these disorders already exists, solutions rely mostly on a graphical output and touch, gaze, or brain-activated input modalities, which do not provide all the necessary features to cover all periods of the day (e.g., night-time). In the scope of the AAL APH-ALARM project, we aim at providing communication support to users with speech difficulties (mainly aphasics), while lying in bed. Towards this end, we propose a system based on gesture recognition using a radar deployed, for example, in a wall of the bedroom. A first prototype was implemented and used to evaluate gesture recognition, relying on radar data and transfer learning. The initial results are encouraging, indicating that using a radar can be a viable option to enhance the communication of people with speech difficulties, in the in-bed scenario.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring Silent Speech Interfaces Based on Frequency-Modulated Continuous-Wave Radar",
        "doc_scopus_id": "85122850592",
        "doc_doi": "10.3390/s22020649",
        "doc_eid": "2-s2.0-85122850592",
        "doc_date": "2022-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Acoustic signals",
            "Ambient noise",
            "Contact less",
            "European portuguese",
            "Frequency-modulated-continuous-wave radars",
            "Health condition",
            "Lighting conditions",
            "Privacy concerns",
            "Silent speech",
            "Silent speech interfaces",
            "Algorithms",
            "Electromyography",
            "Noise",
            "Radar",
            "Speech"
        ],
        "doc_abstract": "© 2022 by the authors. Licensee MDPI, Basel, Switzerland.Speech is our most natural and efficient form of communication and offers a strong potential to improve how we interact with machines. However, speech communication can sometimes be limited by environmental (e.g., ambient noise), contextual (e.g., need for privacy), or health conditions (e.g., laryngectomy), preventing the consideration of audible speech. In this regard, silent speech interfaces (SSI) have been proposed as an alternative, considering technologies that do not require the production of acoustic signals (e.g., electromyography and video). Unfortunately, despite their plentitude, many still face limitations regarding their everyday use, e.g., being intrusive, non-portable, or raising technical (e.g., lighting conditions for video) or privacy concerns. In line with this necessity, this article explores the consideration of contactless continuous-wave radar to assess its potential for SSI development. A corpus of 13 European Portuguese words was acquired for four speakers and three of them enrolled in a second acquisition session, three months later. Regarding the speaker-dependent models, trained and tested with data from each speaker while using 5-fold cross-validation, average accuracies of 84.50% and 88.00% were respectively obtained from Bagging (BAG) and Linear Regression (LR) classifiers, respectively. Additionally, recognition accuracies of 81.79% and 81.80% were also, respectively, achieved for the session and speaker-independent experiments, establishing promising grounds for further exploring this technology towards silent speech recognition.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhanced Communication Support for Aphasia Using Gesture Recognition: The Bedroom Scenario",
        "doc_scopus_id": "85118133283",
        "doc_doi": "10.1109/ISC253183.2021.9562810",
        "doc_eid": "2-s2.0-85118133283",
        "doc_date": "2021-09-07",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Energy Engineering and Power Technology",
                "area_abbreviation": "ENER",
                "area_code": "2102"
            },
            {
                "area_name": "Renewable Energy, Sustainability and the Environment",
                "area_abbreviation": "ENER",
                "area_code": "2105"
            },
            {
                "area_name": "Transportation",
                "area_abbreviation": "SOCI",
                "area_code": "3313"
            },
            {
                "area_name": "Urban Studies",
                "area_abbreviation": "SOCI",
                "area_code": "3322"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Aphasia",
            "Assistive technology",
            "Communication support",
            "Gesture",
            "Gestures recognition",
            "In-bed scenario",
            "Language disorders",
            "Sensor",
            "Smart environment",
            "Speech disorders"
        ],
        "doc_abstract": "© 2021 IEEE.Citizens with speech and language disorders, such as Aphasia, often experience difficulties in expressing their needs. Assistive technologies for these disorders rely mostly on graphical interfaces activated by touch or gaze, which do not effectively cover all communication contexts throughout the day and may raise privacy concerns. In the scope of the AAL APH-ALARM project, our main aim is to extend communication support for users with speech and language difficulties (mainly aphasics) in the bedroom environment. We propose a system for supporting communication based on gesture recognition using non-invasive compact sensors worn by the user or deployed in the environment (e.g., bed). A first prototype was implemented using wrist-worn sensors and machine learning to recognize a small set of gestures. Initial results suggest that gesture recognition to enhance communication for people with speech and language impairments is viable, even when in bed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An accessible smart home based on integrated multimodal interaction",
        "doc_scopus_id": "85112347183",
        "doc_doi": "10.3390/s21165464",
        "doc_eid": "2-s2.0-85112347183",
        "doc_date": "2021-08-02",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Adaptive interaction",
            "Environmentally friendly homes",
            "Green homes",
            "Information and communications technology",
            "Multi-modal",
            "Multi-Modal Interactions",
            "Multiple devices",
            "Suitable solutions",
            "Attitude",
            "Ecosystem",
            "Humans"
        ],
        "doc_abstract": "© 2021 by the authors. Licensee MDPI, Basel, Switzerland. aa.Our homes are becoming increasingly sensorized and smarter. However, they are also becoming increasingly complex, making accessing them and their advantages difficult. Assistants have the potential for improving the accessibility of smart homes, by providing everyone with an integrated, natural, and multimodal way of interacting with the home’s ecosystem. To demonstrate this potential and contribute to more environmentally friendly homes, in the scope of the project Smart Green Homes, a home assistant highly integrated with an ICT (Information and communications technology) home infrastructure was developed, deployed in a demonstrator, and evaluated by seventy users. The users’ global impression of our home assistant is in general positive, with 61% of the participants rating it as good or excellent overall and 51% being likely or very likely to recommend it to others. Moreover, most think that the assistant enhances interaction with the smart home’s multiple devices and is easy to use by everyone. These results show that a home assistant providing an integrated view of a smart home, through natural, multimodal, and adaptive interaction, is a suitable solution for enhancing the accessibility of smart homes and thus contributing to a better living ambient for all of their inhabitants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards automatic creation of annotations to foster development of named entity recognizers",
        "doc_scopus_id": "85115878581",
        "doc_doi": "10.4230/OASIcs.SLATE.2021.11",
        "doc_eid": "2-s2.0-85115878581",
        "doc_date": "2021-08-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Automatic annotation",
            "Automatic creations",
            "Complex task",
            "Gazetteer",
            "Learning techniques",
            "Named entities",
            "Named entity recognition",
            "Portuguese",
            "Recognition systems"
        ],
        "doc_abstract": "© 2021 Schloss Dagstuhl- Leibniz-Zentrum fur Informatik GmbH, Dagstuhl Publishing. All rights reserved.Named Entity Recognition (NER) is an essential step for many natural language processing tasks, including Information Extraction. Despite recent advances, particularly using deep learning techniques, the creation of accurate named entity recognizers continues a complex task, highly dependent on annotated data availability. To foster existence of NER systems for new domains it is crucial to obtain the required large volumes of annotated data with low or no manual labor. In this paper it is proposed a system to create the annotated data automatically, by resorting to a set of existing NERs and information sources (DBpedia). The approach was tested with documents of the Tourism domain. Distinct methods were applied for deciding the final named entities and respective tags. The results show that this approach can increase the confidence on annotations and/or augment the number of categories possible to annotate. This paper also presents examples of new NERs that can be rapidly created with the obtained annotated data. The annotated data, combined with the possibility to apply both the ensemble of NER systems and the new Gazetteer-based NERs to large corpora, create the necessary conditions to explore the recent neural deep learning state-of-art approaches to NER (ex: BERT) in domains with scarce or nonexistent data for training.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Association between acoustic speech features and non-severe levels of anxiety and depression symptoms across lifespan",
        "doc_scopus_id": "85104151098",
        "doc_doi": "10.1371/journal.pone.0248842",
        "doc_eid": "2-s2.0-85104151098",
        "doc_date": "2021-04-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Aged",
            "Aged, 80 and over",
            "Aging",
            "Anxiety",
            "Cross-Sectional Studies",
            "Depression",
            "Female",
            "Humans",
            "Male",
            "Middle Aged",
            "Portugal",
            "Speech Acoustics"
        ],
        "doc_abstract": "© 2021 Albuquerque et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Background Several studies have investigated the acoustic effects of diagnosed anxiety and depression. Anxiety and depression are not characteristics of the typical aging process, but minimal or mild symptoms can appear and evolve with age. However, the knowledge about the association between speech and anxiety or depression is scarce for minimal/mild symptoms, typical of healthy aging. As longevity and aging are still a new phenomenon worldwide, posing also several clinical challenges, it is important to improve our understanding of non-severe mood symptoms' impact on acoustic features across lifetime. The purpose of this study was to determine if variations in acoustic measures of voice are associated with non-severe anxiety or depression symptoms in adult population across lifetime. Methods Two different speech tasks (reading vowels in disyllabic words and describing a picture) were produced by 112 individuals aged 35-97. To assess anxiety and depression symptoms, the Hospital Anxiety Depression Scale (HADS) was used. The association between the segmental and suprasegmental acoustic parameters and HADS scores were analyzed using the linear multiple regression technique. Results The number of participants with presence of anxiety or depression symptoms is low (>7: 26.8% and 10.7%, respectively) and non-severe (HADS-A: 5.4 ± 2.9 and HADS-D: 4.2 ± 2.7, respectively). Adults with higher anxiety symptoms did not present significant relationships associated with the acoustic parameters studied. Adults with increased depressive symptoms presented higher vowel duration, longer total pause duration and short total speech duration. Finally, age presented a positive and significant effect only for depressive symptoms, showing that older participants tend to have more depressive symptoms. Conclusions Non-severe depression symptoms can be related to some acoustic parameters and age. Depression symptoms can be explained by acoustic parameters even among individuals without severe symptom levels.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Rassper: Radar-based silent speech recognition",
        "doc_scopus_id": "85119301325",
        "doc_doi": "10.21437/Interspeech.2021-1413",
        "doc_eid": "2-s2.0-85119301325",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Ambient noise",
            "European portuguese",
            "Health condition",
            "Lighting conditions",
            "Privacy concerns",
            "Public places",
            "Silent speech",
            "Silent speech interfaces",
            "Silent speech recognition",
            "Ultrasound probes"
        ],
        "doc_abstract": "Copyright © 2021 ISCA.Speech is our most natural and efficient way of communication and offers a strong potential to improve how we interact with machines. However, speech communication can sometimes be limited by environmental (e.g., ambient noise), contextual (e.g., need for privacy in a public place), or health conditions (e.g., laryngectomy), hindering the consideration of audible speech. In this regard, silent speech interfaces (SSI) have been proposed (e.g., considering video, electromyography), however, many technologies still face limitations regarding their everyday use, e.g., the need to place equipment in contact with the speaker (e.g., electrodes/ultrasound probe), and raise technical (e.g., lighting conditions for video) or privacy concerns. In this context, the consideration of technologies that can help tackle these issues, e.g, by being contactless and/or placed in the environment, can foster the widespread use of SSI. In this article, continuous-wave radar is explored to assess its potential for SSI. To this end, a corpus of 13 words was acquired, for 3 speakers, and different classifiers were tested on the resulting data. The best results, obtained using Bagging classifier, trained for each speaker, with 5-fold cross-validation, yielded an average accuracy of 0.826, an encouraging result that establishes promising grounds for further exploration of this technology for silent speech recognition.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Prosodic Changes with Age: A Longitudinal Study on a Famous European Portuguese Native Speaker",
        "doc_scopus_id": "85116396250",
        "doc_doi": "10.1007/978-3-030-87802-3_65",
        "doc_eid": "2-s2.0-85116396250",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Different ages",
            "Human communications",
            "Longitudinal analysis",
            "Longitudinal study",
            "Pilot studies",
            "Prosodic features",
            "Prosodics",
            "Prosody",
            "Public figure",
            "Vocal aging"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.The understanding of human communication development throughout the lifetime involves the characterization of both segmental and suprasegmental parameters. This pilot study intends to analyse suprasegmental (i.e., prosodic) features in conversational longitudinal speech samples in uncontrolled environments. The ProsodyDescriptor Extractor was used to extract 17 prosodic features (intonation, intensity and rhythm measures) in a set of 90 speech intervals of 3 s to 6 s selected from three interviews collected in different ages of the same male public figure. Group mean comparison tests revealed that 14 prosodic features presented statistically significant differences between the three ages. In general, in comparison with his younger age, the speaker got a higher F0 mean level, more F0 variability, higher F0 peaks, more variable F0 peak values, less variable F0 falls, higher F0 min, less steeper F0 rises, less steeper F0 falls, less variable F0 rises, more energy in high frequencies, slower speech and articulation rate, less vocal effort and less variable global intensity. The longitudinal study of age-related changes in speech rhythm and intonation could contribute to the normal ageing process’ characterization, being a reference for clinical assessment and intervention.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Designing and Deploying an Interaction Modality for Articulatory-Based Audiovisual Speech Synthesis",
        "doc_scopus_id": "85116375726",
        "doc_doi": "10.1007/978-3-030-87802-3_4",
        "doc_eid": "2-s2.0-85116375726",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Articulatory",
            "Audio-visual speech",
            "Audiovisual speech synthesis",
            "Face contacts",
            "Face to face",
            "Interaction modality",
            "Multi-modal",
            "Multimodal Interaction",
            "Remote communication",
            "Speech interaction"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.Humans communicate with each other in a multimodal way. Even with several technologies mediating remote communication, face-to-face contact is still our main and most natural way to exchange information. Despite continuous advances in interaction modalities, such as speech interaction, much can be done to improve its naturalness and efficiency, particularly by considering the visual cues transmitted by facial expressions through audiovisual speech synthesis (AVS). To this effect, several approaches have been proposed, in the literature, mostly based in data-driven methods. These, while presenting very good results, rely on models that work as black boxes without a direct relation with the actual process of producing speech and, hence, do not contribute much to our understanding of the underpinnings of the synergies between the audio and visual outputs. In this context, the authors proposed a first proof of concept for an articulatory-based approach to AVS, supported on the articulatory phonology framework, and argued that this research needs to be challenged and informed by fast methods to translate it to interactive applications. In this article, we describe further evolutions of the pronunciation module of the AVS core system along with the proposal of a set of interaction modalities to enable its integration in applications to enable a faster translation into real scenarios. The proposed modalities are designed in line with the W3C recommendations for multimodal interaction architectures making it easy to integrate with any applications that consider it.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Assessing Velar Gestures Timing in European Portuguese Nasal Vowels with RT-MRI Data",
        "doc_scopus_id": "85116328665",
        "doc_doi": "10.1007/978-3-030-87802-3_3",
        "doc_eid": "2-s2.0-85116328665",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Articulatory data",
            "Articulatory synthesis",
            "Dynamic nature",
            "European portuguese",
            "Gestural timing",
            "Nasal vowels",
            "Real- time",
            "Real-time magnetic resonance",
            "Resonance imaging data",
            "Vowel production"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.European Portuguese (EP) nasal vowels are characterised by their dynamic nature entailing a gradual variation from an oral into a nasal configuration. The analysis of velar dynamics assumes a particular relevance for improving our understanding of nasal vowel production with an impact, e.g., on articulatory synthesis. Following on previous work, considering EMA and real-time magnetic resonance imaging (RT-MRI), at 14 fps, this study revisits the work regarding the characterisation of EP nasal vowels by analysing gesture timings considering articulatory data obtained from RT-MRI of the vocal tract at a higher frame rate (50 fps) and a larger number of speakers. The analysis, considering eleven EP speakers, characterises the duration of opening and closing velar gestures and explores synchronisation with the previous oral gesture (start-to-release lag) and the potential influence of vowel height in this regard.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Multidisciplinary User-Centered Approach to Designing an Information Platform for Accessible Tourism: Understanding User Needs and Motivations",
        "doc_scopus_id": "85112160466",
        "doc_doi": "10.1007/978-3-030-78092-0_9",
        "doc_eid": "2-s2.0-85112160466",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Development process",
            "Focus groups",
            "Information platform",
            "People with disabilities",
            "Potential users",
            "Technological solution",
            "Tourism activities",
            "User-centered approach"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.The present work aims to expand knowledge on user needs and the motivations of people with disabilities (PwD) in order to create a technological solution for addressing barriers in the accessible tourism market. For this, a user-centered design was followed, putting PwD at the center of the development process. This was obtained by identifying personas and scenarios contributing to a better depiction of the potential users, clearly describing their requirements and accessibility needs. For obtaining the characterization of personas, a comprehensive study in the area of accessible tourism, involving various tourism stakeholders was performed. The methods applied to collect the data were questionnaires, interviews and two focus groups with PwD. Two personas are presented in this article, illustrating the needs and motivations of two groups of PwD. In addition, two scenarios concerning the personas were also elaborated, showing how a technological solution can help the integration in tourism activities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Promoting Social Skills in Technology-Mediated Communication Contexts: First Results on Adopting the Social Compass Curriculum",
        "doc_scopus_id": "85112134284",
        "doc_doi": "10.1007/978-3-030-80091-8_55",
        "doc_eid": "2-s2.0-85112134284",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.Communication is a vital part of our daily lives. It allows us to express ourselves, our needs and feelings, enabling the establishment of relationships with others. Unfortunately, the ability to communicate can often be partially or, even, fully hindered due to a wide range of conditions. Since communication acts as a crucial step for integration, difficulties associated with communication can act as obstacles to those that experience them, making them struggle to feel included in society. Taking in mind the importance of being able to communicate and the multiple facets of technology, but also other dimensions such as social interaction, and building on previous work regarding assistive communication technologies for school kids, this work addresses the promotion of social skills in technology-mediated contexts. To this end, a tool designed to motivate and promote face to face interaction as well as the employment of the user’s social skills during them, is proposed as an instrument to support and potentially teach and improve the child’s social skills. The proposed approach has already been submitted to first evaluations by end-users and professionals in the field of speech therapy with very promising results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Eppur si muove: Formant dynamics is relevant for the study of speech aging effects",
        "doc_scopus_id": "85103844634",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85103844634",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Age-related changes",
            "Automatic speech recognition system",
            "Dynamic information",
            "Formant dynamics",
            "Position papers",
            "Speech production",
            "Static approach",
            "Vowel perception"
        ],
        "doc_abstract": "Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reservedThe evidence have shown that speech change with age and the automatic speech recognition systems needs adaptation to older voices. Most of the acoustic studies about the age effects on speech production have focused on static approaches to obtain the vowel formants. However, vowel formant dynamics may also be important to characterize vowel quality and the age related changes. In this position paper the authors argue for the need to increase the use of dynamic information in acoustic studies. Among the main arguments, we can state that: speech is inherently dynamic; dynamic vowel formants improve the classification of vowels and dialects and play an important role in vowel perception; nowadays better tools allow to go beyond analysis of snapshots.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Smart Home for All Supported by User and Context Adaptation",
        "doc_scopus_id": "85108058459",
        "doc_doi": "10.1145/3439231.3439259",
        "doc_eid": "2-s2.0-85108058459",
        "doc_date": "2020-12-02",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Low light",
            "Smart appliances",
            "Smart homes",
            "User and context adaptation"
        ],
        "doc_abstract": "© 2020 Owner/Author.Homes are becoming increasingly smarter, enabling us to control their smart appliances and devices, as well as obtain relevant information on the home. However, accessibility in smart homes for all is still a challenge, with information being presented in the same way to the different users and in distinct contexts. If the interaction is not adapted to the user, certain citizens (e.g., children, and older/impaired people) can be excluded from exploiting the full potential of smart homes. Furthermore, without adaptation to the context, interaction becomes more difficult in some situations (e.g., noisy or low-light environments). With the aim of enhancing smart home accessibility, we propose a solution for adapting the information presented during interaction with the home to the user's characteristics, capabilities and preferences, as well as to the context, namely the environment's noise and luminosity, and user distance.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Conceptual Model for Data Collection and Analysis for AR-based Remote Collaboration Evaluation",
        "doc_scopus_id": "85099568572",
        "doc_doi": "10.1109/ISMAR-Adjunct51615.2020.00016",
        "doc_eid": "2-s2.0-85099568572",
        "doc_date": "2020-11-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Collaborative process",
            "Conceptual model",
            "Data collection",
            "Enabling technologies",
            "Holistic evaluations",
            "Novel methods",
            "Remote collaboration",
            "Team performance"
        ],
        "doc_abstract": "© 2020 IEEE.A significant effort has been devoted to the creation of the enabling technology and in the proposal of novel methods to support remote collaboration using Augmented Reality (AR), given the novelty of the field. As the field progresses to focus on the nuances of supporting collaboration and with the growing number of prototypes mediated by AR, the characterization and evaluation of the collaborative process becomes an essential, but difficult endeavor. Evaluation is particularly challenging in this multifaceted context involving many aspects that may influence the way collaboration occurs. Therefore, it is essential the existence of holistic evaluation strategies that monitor the use and performance of the proposed solutions regarding the team, its members, and the technology, allowing adequate characterization and report of collaborative processes. As a contribute, we propose a conceptual model for multi-user data collection and analysis that monitors several collaboration aspects: individual and team performance, behaviour and level of collaboration, as well as contextual data in scenarios of remote collaboration using AR-based solutions.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Data-driven critical tract variable determination for European Portuguese",
        "doc_scopus_id": "85094103354",
        "doc_doi": "10.3390/info11100491",
        "doc_eid": "2-s2.0-85094103354",
        "doc_date": "2020-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Critical variables",
            "Data driven",
            "Dynamic aspects",
            "Imaging data",
            "Tract variables",
            "Unsupervised data",
            "Visual analysis",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2020 by the authors. Licensee MDPI, Basel, Switzerland.Technologies, such as real-time magnetic resonance (RT-MRI), can provide valuable information to evolve our understanding of the static and dynamic aspects of speech by contributing to the determination of which articulators are essential (critical) in producing specific sounds and how (gestures). While a visual analysis and comparison of imaging data or vocal tract profiles can already provide relevant findings, the sheer amount of available data demands and can strongly profit from unsupervised data-driven approaches. Recent work, in this regard, has asserted the possibility of determining critical articulators from RT-MRI data by considering a representation of vocal tract configurations based on landmarks placed on the tongue, lips, and velum, yielding meaningful results for European Portuguese (EP). Advancing this previous work to obtain a characterization of EP sounds grounded on Articulatory Phonology, important to explore critical gestures and advance, for example, articulatory speech synthesis, entails the consideration of a novel set of tract variables. To this end, this article explores critical variable determination considering a vocal tract representation aligned with Articulatory Phonology and the Task Dynamics framework. The overall results, obtained considering data for three EP speakers, show the applicability of this approach and are consistent with existing descriptions of EP sounds.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Comprehensive Analysis of Age and Gender Effects in European Portuguese Oral Vowels",
        "doc_scopus_id": "85097465521",
        "doc_doi": "10.1016/j.jvoice.2020.10.021",
        "doc_eid": "2-s2.0-85097465521",
        "doc_date": "2020-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Otorhinolaryngology",
                "area_abbreviation": "MEDI",
                "area_code": "2733"
            },
            {
                "area_name": "LPN and LVN",
                "area_abbreviation": "NURS",
                "area_code": "2912"
            },
            {
                "area_name": "Speech and Hearing",
                "area_abbreviation": "HEAL",
                "area_code": "3616"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 The Voice FoundationThe knowledge about the age effects in speech acoustics is still disperse and incomplete. This study extends the analyses of the effects of age and gender on acoustics of European Portuguese (EP) oral vowels, in order to complement initial studies with limited sets of acoustic parameters, and to further investigate unclear or inconsistent results. A database of EP vowels produced by a group of 113 adults, aged between 35 and 97, was used. Duration, fundamental frequency (f0), formant frequencies (F1 to F3), and a selection of vowel space metrics (F1 and F2 range ratios, vowel articulation index [VAI] and formant centralization ratio [FCR]) were analyzed. To avoid the arguable division into age groups, the analyses considered age as a continuous variable. The most relevant age-related results included: vowel duration increase in both genders; a general tendency to formant frequencies decrease for females; changes that were consistent with vowel centralization for males, confirmed by the vowel space acoustic indexes; and no evidence of F3 decrease with age, in both genders. This study has contributed to knowledge on aging speech, providing new information for an additional language. The results corroborated that acoustic characteristics of speech change with age and present different patterns between genders.",
        "available": true,
        "clean_text": "serial JL 272877 291210 291723 291724 291743 31 Journal of Voice JOURNALVOICE 2020-12-05 2020-12-05 2020-12-05T19:39:43 S0892-1997(20)30412-4 S0892199720304124 10.1016/j.jvoice.2020.10.021 S200 S200.1 FULL-TEXT 2021-04-06T02:01:07.567985Z 0 0 20201205 2020 2020-12-05T21:42:09.803477Z aiptxt articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb yearnav figure table body mmlmath acknowledge affil appendices articletitle auth authfirstini authfull authkeywords authlast primabst ref 0892-1997 08921997 In Press, Corrected Proof 0 Available online 5 December 2020 2020-12-05 2020 article fla © 2020 The Voice Foundation. Published by Elsevier Inc. All rights reserved. ACOMPREHENSIVEANALYSISAGEGENDEREFFECTSINEUROPEANPORTUGUESEORALVOWELS ALBUQUERQUE L Introduction Background Method Participants Corpus Recording protocol Segmentation Acoustic measurements Reliability in vowel segmentation Statistical analysis Results Vowel duration increased with age Age effects in f0 were gender dependent Age effects in formant frequencies were vowel and gender dependent Age and gender effects on acoustic vowel space Discussion Vowels Duration Fundamental Frequency Formant frequencies Study limitations and future work Conclusion Acknowledgments Appendix A Vowel Duration Appendix B Fundamental Frequency Appendix C Formant Frequencies References PORTUGAL 2019 S PORTUGAL 2015 S 2017 AGINGVOICE LINVILLE 2001 S VOCALAGING MAUTNER 2011 H ACROSSSYSTEMINSTRUMENTALVOICEPROFILEAGINGVOICECONSIDERATIONSJAWPOSTUREEFFECTS SCHOTZ 2006 S TRAVAUXDELINSTITUTDELINGUISTIQUEDELUND PERCEPTIONANALYSISSYNTHESISSPEAKERAGE LINVILLE 2001 323 330 S ALBUQUERQUE 2019 3965 3969 L INTERSPEECH AGERELATEDCHANGESINEUROPEANPORTUGUESEVOWELACOUSTICS ALBUQUERQUE 2014 940 944 L INTERSPEECH IMPACTAGEINPRODUCTIONEUROPEANPORTUGUESEVOWELS PELLEGRINI 2013 852 856 T INTERSPEECH ACORPUSBASEDSTUDYELDERLYYOUNGSPEAKERSEUROPEANPORTUGUESEACOUSTICCORRELATESIMPACTSPEECHRECOGNITIONPERFORMANCE GUIMARAES 2005 592 606 I MOU 2019 77 82 Z ARIASVERGARA 2017 731 748 T NEEL 2008 574 585 A MCCLOY 2014 060007 D MEETINGSACOUSTICS MODELINGINTRINSICINTELLIGIBILITYVARIATIONVOWELSPACESIZESTRUCTURE ROY 2009 124 135 N AUDIBERT 2015 5 9 N ICPHS15 DURATIONVSSTYLEDEPENDENTVOWELVARIATIONAMULTIPARAMETRICINVESTIGATION FOUGERON 2011 687 690 C ICPHSXVII TESTINGVARIOUSMETRICSFORDESCRIPTIONVOWELDISTORTIONINDYSARTHRIA GAHL 2019 42 54 S SAPIR 2011 173 175 S MAVEBA2011 ACOUSTICMETRICSVOWELARTICULATIONINPARKINSONSDISEASEVOWELSPACEAREAVSAVSVOWELARTICULATIONINDEXVAI SAPIR 2010 114 125 S RASTATTER 1990 312 319 M RASTATTER 1997 1 8 M ESCUDERO 2009 1379 1393 P OLIVEIRA 2012 129 138 C ADVANCESINSPEECHLANGUAGETECHNOLOGIESFORIBERIANLANGUAGESIBERSPEECH ACOUSTICANALYSISEUROPEANPORTUGUESEORALVOWELSPRODUCEDBYCHILDREN MARTINS 1973 303 314 M XUE 2003 689 701 S VIPPERLA 2010 1 10 R LANITIS 2010 34 52 A SATALOFF 1997 156 160 R JOHNSIII 2011 1 6 M GOY 2013 545 555 H EICHHORN 2018 644.e1 644.e9 J MA 2010 146 152 E NISHIO 2008 120 127 M TORREIII 2009 324 333 P YAMAUCHI 2014 525 531 A XUE 2001 159 168 S STATHOPOULOS 2011 1011 1021 E RAMIG 1983 22 30 L WATSON 2007 561 564 P ICPHSXVI ACOMPARISONVOWELACOUSTICSBETWEENOLDERYOUNGERADULTS HARRINGTON 2007 2753 2756 J INTERSPEECH AGERELATEDCHANGESINFUNDAMENTALFREQUENCYFORMANTSALONGITUDINALSTUDYFOURSPEAKERS ENDRES 1971 1842 1848 W DECOSTER 1999 1 5 F FLETCHER 2015 2132 2139 A SEBASTIAN 2012 81 84 S MERTENS 2020 J SECONDWORKSHOPSPEECHPERCEPTIONPRODUCTIONACROSSLIFESPANPOSTER AGINGEFFECTSPROSODICMARKINGINGERMANACOUSTICANALYSIS SMITH 1987 522 529 B BENJAMIN 1982 159 167 B SLAWINSKI 1994 2221 2230 E FOUGERON 2018 1905 C DRAXLER 2004 559 562 C LREC04 SPEECHRECORDERAUNIVERSALPLATFORMINDEPENDENTMULTICHANNELAUDIORECORDINGSOFTWARE KISLER 2017 326 347 T SCHIEL 1999 607 610 F 14THICPHS AUTOMATICPHONETICTRANSCRIPTIONNONPROMPTEDSPEECH SMILJANIC 2017 3081 3096 R SHROUT 1979 420 428 P LUTZROSS 2013 4107 A KENT 2018 74 97 R JACEWICZ 2009 233 256 E STEFFENS 2011 Y AGINGVOICE FIKKERT 2005 263 280 P MATEUS 2000 M PHONOLOGYPORTUGUESE VELOSO 2007 55 60 J JOURNEESDETUDESLINGUISTIQUES SCHWAINEUROPEANPORTUGUESEPHONOLOGICALSTATUSIMAGE32 VELOSO 2017 191 213 J SILVA 1994 79 84 D UTAWORKINGPAPERSINLINGUISTICS VARIABLEELISIONUNSTRESSEDVOWELSINEUROPEANPORTUGUESEACASESTUDY SILVA 1998 166 178 D REUBOLD 2010 638 651 U FERRAND 2002 480 487 C HIGGINS 1991 1000 1010 M PONTES 2005 84 94 P HOLLIEN 1972 155 159 H WIELING 2016 122 143 M SCUKANEC 1991 203 208 G FOX 2010 45 48 R EXLING2010 DIALECTGENERATIONALDIFFERENCESINVOWELSPACEAREAS © 2020 The Voice Foundation. Published by Elsevier Inc. All rights reserved. 2020-11-06T11:56:03.359Z FCT Fundação para a Ciência e a Tecnologia CIDMA Center for Research and Development in Mathematics and Applications item S0892-1997(20)30412-4 S0892199720304124 10.1016/j.jvoice.2020.10.021 272877 2021-04-06T02:01:07.567985Z 2020-12-05 true 1154464 MAIN 17 100645 849 656 IMAGE-WEB-PDF 1 gr1 46387 406 640 gr10 44690 453 678 gr11 42925 422 678 gr12 45790 422 678 gr2 42565 401 631 gr3 29795 260 339 gr4 49458 442 661 gr5 46010 443 678 gr6 47273 453 678 gr7 33929 258 339 gr8 48470 453 678 gr9 47240 446 678 fx32 322 7 4 gr1 5191 139 219 gr10 4763 146 219 gr11 4721 136 219 gr12 5069 136 219 gr2 5034 139 219 gr3 8309 164 213 gr4 5275 146 219 gr5 5199 143 219 gr6 5190 146 219 gr7 9965 164 215 gr8 5042 146 219 gr9 5212 144 219 fx32 439 37 20 gr1 436420 2159 3400 gr10 428778 2406 3600 gr11 397778 2241 3600 gr12 403756 2241 3600 gr2 393919 2129 3352 gr3 305801 1382 1800 gr4 448983 2348 3515 gr5 421754 2353 3600 gr6 444671 2405 3600 gr7 359754 1369 1800 gr8 444258 2405 3600 gr9 437950 2366 3600 fx32 572 37 20 si1 803 1 11 si10 2899 20 234 si11 1751 18 186 si12 2413 20 204 si13 3847 20 391 si14 2814 20 395 si15 2822 20 378 si16 2806 20 395 si17 2841 20 378 si18 4045 20 366 si19 2744 20 395 si2 966 16 32 si20 3944 20 387 si21 3979 20 387 si3 1064 16 32 si4 3949 20 440 si5 4025 20 439 si6 844 9 11 si7 862 9 12 si8 840 13 13 si9 2347 18 192 am 797869 YMVJ 3041 S0892-1997(20)30412-4 10.1016/j.jvoice.2020.10.021 The Voice Foundation Figure 1 Scatterplot and regression lines for vowels duration by age and gender. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 1 Figure 2 Scatterplot and regression lines for mean f0 by age and gender. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 2 Figure 3 Mean value of f0 as a function of vowel and age. Top: women; bottom: men. Solid lines: 35 years; dashed lines: 100 years. This figure was drawn using equations of linear regression (of each vowel by gender) replacing the variable age by 35 and 100 (as an approximation to the age of the oldest speaker of the sample). Fig. 3 Figure 4 Scatterplot and regression lines for mean F1 by age and gender. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 4 Figure 5 Scatterplot and regression lines for mean F2 by age and gender. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 5 Figure 6 Scatterplot and regression lines for mean F3 by age and gender. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 6 Figure 7 Vowel Space for men and women as a function of age. Bold lines and symbols: women; non bold lines and symbols: men. Solid lines: 35 years; dashed lines: 100 years. This figure was drawn using equations of linear regression (of each vowel by gender for F1 and F2) replacing the variable age for 35 and 100 (as an approximation to the age of the oldest speaker of the sample). Fig. 7 Figure 8 Scatterplot of F1 as function of age and gender for vowel [a] with superimposed linear regression results. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 8 Figure 9 Scatterplot of F2 as function of age and gender for vowel [u] with superimposed linear regression results. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 9 Figure 10 Scatterplot and regression lines for F1RR by age and gender. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 10 Figure 11 Scatterplot and regression lines for F2RR by age and gender. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 11 Figure 12 Scatterplot and regression lines for VAI by age and gender. Each symbol corresponds to one speaker. Solid line and circles: females; dashed line and triangles: males. Fig. 12 Table 1 List of Words Per Vowel (International Phonetic Alphabet) Table 1 Oral vowels Words Stressed [i] [ˈfitɐ] (ribbon) [ˈbiku] (beak) [ˈfiɡu] (fig) [ˈpizɐ]* (pizza) [e] [ˈseʃtɐ] (basket) [ˈdedu] (finger) [ˈpezu] (weight) [ˈzebɾɐ] (zebra) [ɛ] [ˈsɛtɨ] (seven) [ˈtɛtu] (ceiling) [ˈsɛtɐ] (arrow) [ˈʃɛkɨ] (check) [a] [ˈʃavɨ] (key) [ˈfakɐ] (knife) [ˈɡatu] (cat) [ˈpatu] (duck) [ɔ] [ˈkɔpu] (glass) [ˈbɔtɐ] (boot) [ˈfɔkɐ] (seal) [ˈtɔʃɐ]* (torch) [o] [ˈbokɐ] (mouth) [ˈkoku] (coconut) [ˈposu] (well) [ˈɡotɐʃ] (drops) [u] [ˈʃuvɐ] (rain) [ˈʃupɐ]* (lollipop) [ˈkubu]* (cube) [ˈʒubɐ]* (mane) Unstressed [ɐ] [kɐˈfɛ] (coffee) [ʃɐˈpɛw] (hat) [pɐˈtĩʃ] (rollerblades) [pɐˈpɛɫ] (paper) [ɨ] [bɨˈbeɾ] (to drink) [dɨˈdaɫ] (thimble) [pɨˈdaɫ] (pedal) [pɨʃˈkaɾ]* (to fish) ⁎ The pictogram of this word presented a naming percentage of accuracy lower than 70%. Table 2 Results of Multiple Linear Regression: the Effect of Gender and Gender*Age Interaction on F1, F2, and F3 Values by Vowel Table 2 Intercept Gender (Male) Male * Age Female * Age Vowel B 95% CI B 95% CI B 95% CI B 95% CI F1 ɨ 371.54 329.35 413.73 − 52.17 − 112.77 8.42 0.46 − 0.21 1.14 0.11 − 0.55 0.78 ɐ 484.63 433.43 535.82 − 25.67 − 99.20 47.87 − 0.03 − 0.85 0.79 0.25 − 0.55 1.06 a 921.76 832.36 1011.17 − 99.06 − 227.47 29.36 − 1.96 − 3.39 − 0.53 − 1.05 − 2.45 0.36 e 409.51 375.87 443.14 − 62.75 − 111.06 − 14.45 0.62 0.08 1.16 − 0.04 − 0.57 0.49 ɛ 554.03 509.26 598.81 − 49.76 − 114.07 14.56 0.08 − 0.63 0.80 − 0.10 − 0.80 0.61 i 318.92 286.36 351.48 − 59.60 − 106.37 − 12.84 0.48 − 0.04 1.00 0.04 − 0.47 0.55 o 442.72 408.10 477.35 − 50.73 − 100.46 − 1.00 0.05 − 0.51 0.60 − 0.35 − 0.90 0.20 ɔ 674.54 618.92 730.16 − 96.78 − 176.66 − 16.89 − 0.75 − 1.64 0.14 − 1.21 − 2.08 − 0.33 u 382.67 349.98 415.36 − 74.90 − 121.85 − 27.95 0.06 − 0.46 0.58 − 0.65 − 1.16 − 0.14 Mean 506.70 476.77 536.64 − 63.49 − 106.49 − 20.49 − 0.11 − 0.59 0.37 − 0.33 − 0.80 0.14 F2 ɨ 1523.01 1390.08 1655.93 − 165.49 − 356.40 25.42 0.39 − 1.74 2.51 1.79 − 0.30 3.88 ɐ 1815.24 1710.26 1920.22 − 254.28 − 405.06 − 103.50 − 0.30 − 1.97 1.38 0.31 − 1.35 1.96 a 1563.46 1478.31 1648.62 − 287.37 − 409.67 − 165.06 1.07 − 0.29 2.43 − 0.06 − 1.41 1.28 e 2392.12 2239.69 2544.55 − 373.59 − 592.51 − 154.66 − 1.69 − 4.12 0.75 − 0.81 − 3.21 1.59 ɛ 2255.71 2121.54 2389.87 − 430.60 − 623.30 − 237.90 − 0.35 − 2.50 1.79 − 0.74 − 2.85 1.38 i 2642.45 2487.84 2797.06 − 264.11 − 486.18 − 42.05 − 2.85 − 5.32 − 0.38 − 0.37 − 2.80 2.07 o 924.77 859.08 990.46 − 114.96 − 209.31 − 20.61 0.40 − 0.65 1.45 − 0.31 − 1.34 0.73 ɔ 1133.93 1071.55 1196.31 − 220.95 − 310.55 − 131.36 0.56 − 0.44 1.56 − 1.19 − 2.18 − 0.21 u 1012.17 882.36 1141.98 − 219.64 − 406.08 − 33.20 2.30 0.23 4.38 − 0.14 − 2.18 1.91 Mean 1695.87 1628.39 1763.36 − 259.00 − 355.92 − 162.07 − 0.05 − 1.13 1.03 − 0.17 − 1.23 0.89 F3 ɨ 2875.24 2713.98 3036.49 − 271.48 − 503.08 − 39.88 − 1.89 − 4.47 0.69 0.47 − 2.07 3.01 ɐ 2805.99 2644.83 2967.15 − 366.92 − 598.39 − 135.46 − 1.40 − 3.98 1.17 − 0.74 − 3.28 1.80 a 2561.37 2354.48 2768.25 − 315.25 − 612.39 − 18.11 1.74 − 1.56 5.05 1.27 − 1.99 4.53 e 2912.49 2760.88 3064.10 − 424.42 − 642.17 − 206.67 − 0.11 − 2.53 2.31 − 0.02 − 2.41 2.36 ɛ 2903.41 2736.52 3070.30 − 502.15 − 741.85 − 262.44 0.82 − 1.85 3.49 − 1.15 − 3.77 1.48 i 3257.24 3024.54 3489.94 − 313.60 − 647.82 20.62 − 3.05 − 6.77 0.67 − 2.32 − 5.98 1.35 o 2913.92 2724.79 3103.05 − 484.69 − 756.33 − 213.05 − 0.37 − 3.39 2.66 − 0.98 − 3.96 2.00 ɔ 2766.86 2554.54 2979.18 − 447.83 − 752.78 − 142.88 0.62 − 2.78 4.01 − 0.69 − 4.03 2.65 u 3002.76 2819.95 3185.58 − 619.73 − 882.30 − 357.16 − 0.97 − 3.90 1.95 − 4.40 − 7.28 − 1.52 Mean 2888.81 2757.77 3019.84 − 416.23 − 604.43 − 228.03 − 0.51 − 2.61 1.58 − 0.95 − 3.01 1.11 B = Linear Coefficient ⁎ Grey cells represent significant results (P < 0.05). Table A1 Mean and Standard Deviation (SD) of Vowel Duration Values (ms) Table A1 Male Female [35–49] [50–64] [65–79] ≥ 80 [35–49] [50–64] [65–79] ≥ 80 Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) ɨ 71.5 (14.3) 75.2 (13.3) 77.7 (15.4) 75.5 (15.8) 71.2 (9.8) 74.1 (12.8) 76.6 (17.2) 80.3 (14.6) ɐ 57.6 (10.0) 59.1 (7.5) 60.9 (10.6) 67.1 (11.0) 56.5 (9.3) 58.8 (5.6) 61.8 (10.6) 64.3 (12.1) a 135.0 (17.9) 136.4 (18.7) 138.6 (23.6) 151.2 (30.8) 132.2 (27.1) 143.2 (23.9) 156.3 (20.2) 159.4 (33.3) e 132.4 (17.3) 138.0 (16.9) 141.4 (23.1) 154.5 (29.1) 130.3 (23.1) 140.9 (20.1) 153.5 (14.3) 167.2 (38.2) ɛ 114.9 (15.9) 116.2 (16.8) 121.5 (25.6) 143.1 (31.0) 113.0 (20.2) 123.4 (18.2) 141.3 (18.2) 144.4 (32.4) i 107.6 (16.5) 111.9 (21.1) 118.1 (29.1) 140.5 (30.8) 101.7 (18.9) 112.5 (20.1) 131.8 (18.0) 143.8 (32.2) o 115.7 (10.4) 120.8 (18.2) 128.1 (25.0) 146.4 (29.9) 111.5 (20.1) 118.8 (22.9) 136.9 (19.7) 146.5 (38.7) ɔ 114.9 (17.6) 119.3 (16.6) 127.7 (27.0) 145.6 (32.0) 119.7 (25.6) 127.1 (24.6) 146.9 (15.6) 152.2 (37.4) u 110.3 (23.1) 119.9 (18.4) 118.5 (23.8) 140.5 (36.4) 107.2 (27.1) 112.1 (25.5) 136.9 (17.9) 145.6 (44.8) Mean 106.7 (12.8) 110.8 (14.0) 114.7 (19.9) 129.4 (23.8) 104.8 (17.8) 112.3 (16.4) 126.9 (12.5) 133.7 (28.5) Table B1 Mean and standard deviation of vowel f0 values. Table B1 Male Female [35-49] [50-64] [65-79] ≥ 80 [35-49] [50-64] [65-79] ≥ 80 Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) ɨ 138.8 (22.3) 133.3 (27.9) 128.4 (27.5) 134.6 (19.4) 203.9 (31.3) 213.0 (45.2) 195.7 (32.5) 173.2 (17.9) ɐ 140.0 (25.6) 130.6 (23.4) 124.8 (26.2) 137.1 (19.8) 210.8 (29.7) 213.5 (41.4) 196.6 (33.0) 172.1 (14.8) a 133.1 (28.5) 126.6 (23.3) 127.5 (33.2) 143.0 (23.2) 180.8 (15.7) 188.8 (25.6) 170.0 (17.5) 174.0 (19.3) e 141.4 (29.3) 135.2 (27.9) 136.1 (36.7) 153.4 (28.3) 195.1 (21.6) 203.9 (30.9) 183.2 (20.3) 186.0 (18.6) ɛ 138.1 (31.6) 130.2 (24.2) 132.7 (34.3) 148.4 (25.8) 188.7 (19.6) 193.3 (28.3) 176.7 (22.1) 179.4 (16.3) i 146.7 (31.4) 138.6 (26.1) 143.5 (36.3) 158.9 (32.1) 202.7 (20.3) 212.8 (33.1) 192.6 (22.7) 193.2 (18.7) o 143.7 (30.6) 135.5 (25.8) 136.9 (33.0) 153.9 (28.4) 196.4 (20.2) 206.9 (33.2) 186.8 (24.3) 187.7 (18.0) ɔ 138.1 (29.3) 131.4 (25.1) 130.5 (34.6) 148.4 (24.9) 188.4 (17.7) 196.1 (29.4) 175.7 (20.9) 180.2 (18.2) u 149.3 (31.4) 142.2 (28.6) 145.8 (34.7) 160.6 (30.9) 206.2 (20.1) 214.9 (38.5) 194.8 (25.0) 195.0 (18.6) Mean 141.0 (27.6) 133.7 (25.0) 134.0 (31.7) 148.7 (25.3) 197.0 (19.3) 204.8 (30.7) 185.8 (20.9) 182.3 (15.7) Table C1 Mean and standard deviation of vowel F1, F2 and F3 values. Table C1 Male Female [35-49] [50-64] [65-79] ≥ 80 [35-49] [50-64] [65-79] ≥ 80 Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) Mean (SD) F1 ɨ 336.5 (32.2) 347.8 (25.9) 356.1 (35.5) 354.5 (44.6) 375.8 (44.8) 381.5 (45.0) 380.5 (52.0) 374.2 (27.7) ɐ 458.4 (37.9) 455.4 (34.7) 459.0 (38.3) 455.9 (41.5) 481.2 (39.2) 523.7 (74.9) 499.0 (54.2) 495.4 (37.5) a 741.7 (54.0) 698.8 (64.5) 682.5 (74.7) 668.1 (56.7) 885.4 (111.7) 860.2 (92.8) 842.1 (114.4) 835.6 (74.3) e 369.3 (34.0) 385.1 (28.3) 396.4 (36.4) 394.3 (30.8) 406.7 (24.5) 420.0 (33.3) 395.3 (27.5) 408.0 (31.2) ɛ 507.0 (39.3) 503.1 (37.2) 511.2 (50.5) 518.5 (21.9) 544.2 (37.3) 568.4 (52.3) 531.9 (40.0) 549.4 (36.4) i 288.0 (34.3) 280.3 (23.5) 286.6 (24.4) 308.6 (24.9) 312.8 (36.4) 331.3 (29.7) 323.3 (26.7) 316.7 (38.7) o 387.8 (33.9) 401.8 (29.1) 398.5 (34.4) 390.9 (34.9) 427.6 (28.8) 431.0 (35.0) 411.1 (31.3) 413.3 (28.6) ɔ 544.8 (41.3) 527.4 (38.4) 529.2 (51.4) 517.7 (33.7) 625.9 (63.1) 613.3 (68.6) 586.7 (48.6) 563.5 (60.2) u 314.8 (27.2) 308.4 (22.4) 306.2 (26.1) 319.2 (21.0) 348.6 (35.1) 359.9 (38.5) 327.4 (28.0) 332.5 (38.6) Mean 438.7 (26.5) 434.2 (25.4) 436.2 (30.6) 436.4 (22.0) 489.8 (25.8) 498.8 (33.8) 477.5 (30.8) 476.5 (21.4) F2 ɨ 1378.5 (131.8) 1366.1 (99.4) 1413.4 (115.5) 1363.6 (185.3) 1597.0 (127.1) 1607.6 (101.7) 1667.0 (131.2) 1671.9 (87.9) ɐ 1547.9 (114.2) 1534.5 (51.2) 1561.7 (96.7) 1519.8 (82.8) 1803.6 (84.1) 1835.1 (115.1) 1893.2 (89.5) 1784.3 (105.9) a 1331.5 (69.4) 1324.9 (60.4) 1358.1 (88.1) 1363.0 (104.6) 1543.3 (77.0) 1580.5 (87.4) 1548.9 (65.7) 1569.5 (90.1) e 1959.8 (99.7) 1912.2 (109.2) 1894.1 (155.9) 1875.3 (114.4) 2342.3 (162.5) 2343.8 (169.5) 2384.6 (161.8) 2272.7 (125.9) ɛ 1813.6 (110.4) 1809.1 (90.3) 1787.6 (126.7) 1801.5 (97.0) 2222.2 (146.4) 2200.8 (144.0) 2226.4 (143.6) 2181.4 (135.0) i 2256.7 (120.1) 2217.3 (113.3) 2173.2 (161.2) 2134.7 (121.4) 2623.1 (170.8) 2618.6 (147.4) 2621.5 (183.3) 2614.9 (115.9) o 825.8 (51.7) 849.3 (60.5) 825.8 (48.3) 840.4 (72.3) 908.2 (61.6) 917.5 (66.3) 887.6 (53.7) 913.5 (82.2) ɔ 945.4 (55.9) 941.8 (37.0) 951.2 (62.4) 955.4 (67.8) 1084.5 (72.6) 1071.0 (46.0) 1045.1 (56.0) 1033.1 (74.6) u 914.4 (102.3) 908.9 (65.7) 953.9 (126.5) 981.7 (136.4) 991.2 (134.6) 996.2 (147.2) 1045.4 (115.6) 967.9 (141.4) Mean 1441.5 (63.6) 1429.3 (41.3) 1435.4 (74.1) 1426.2 (72.7) 1679.5 (65.7) 1685.7 (61.6) 1702.2 (66.9) 1667.7 (52.1) F3 ɨ 2477.5 (142.4) 2532.8 (152.7) 2514.4 (146.7) 2392.5 (170.4) 2880.1 (111.9) 2930.8 (119.5) 2899.2 (202.8) 2907.3 (116.4) ɐ 2360.1 (185.0) 2355.2 (130.7) 2377.8 (120.6) 2297.3 (160.4) 2776.4 (125.7) 2733.1 (173.4) 2772.6 (183.2) 2759.1 (89.7) a 2298.1 (127.4) 2358.0 (189.4) 2402.8 (139.6) 2365.0 (215.2) 2625.5 (180.8) 2591.3 (209.1) 2668.9 (210.5) 2685.5 (279.1) e 2492.4 (110.0) 2480.1 (149.6) 2476.0 (128.3) 2474.4 (162.0) 2899.0 (97.7) 2912.9 (173.1) 2932.7 (162.5) 2891.2 (150.7) ɛ 2424.8 (178.2) 2481.7 (187.4) 2431.3 (91.2) 2479.9 (205.6) 2855.9 (105.4) 2826.5 (129.7) 2822.2 (176.4) 2825.3 (164.5) i 2777.7 (170.7) 2816.2 (223.1) 2699.2 (219.0) 2703.4 (222.0) 3175.7 (243.3) 3086.6 (194.6) 3130.9 (255.0) 3039.2 (205.9) o 2386.2 (125.5) 2429.5 (134.8) 2425.4 (123.8) 2376.2 (137.2) 2870.7 (209.1) 2860.1 (223.3) 2818.8 (238.0) 2873.5 (169.8) ɔ 2311.2 (156.0) 2389.4 (181.7) 2395.2 (136.3) 2326.5 (202.6) 2740.3 (192.4) 2709.7 (237.0) 2727.8 (259.7) 2716.6 (195.6) u 2325.7 (128.1) 2348.7 (144.5) 2328.6 (152.5) 2272.0 (153.1) 2768.9 (208.9) 2811.1 (215.4) 2698.7 (184.8) 2611.3 (158.6) Mean 2428.2 (84.0) 2465.7 (104.7) 2450.1 (112.4) 2409.7 (142.9) 2843.6 (120.0) 2829.1 (137.5) 2830.2 (162.6) 2812.1 (101.8) A Comprehensive Analysis of Age and Gender Effects in European Portuguese Oral Vowels A Comprehensive Analysis of Age and Gender Effects in European Portuguese Oral Vowels Luciana Albuquerque ⁎ ⁎ † ‡ § Catarina Oliveira ⁎ ║ António Teixeira ⁎ ‡ Pedro Sa-Couto ¶ # Daniela Figueiredo † ║ ⁎ Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Aveiro, Portugal Institute of Electronics and Informatics Engineering of Aveiro University of Aveiro Aveiro Portugal *Institute of Electronics and Informatics Engineering of Aveiro, University of Aveiro, Aveiro, Portugal † Center for Health Technology and Services Research, University of Aveiro, Aveiro, Portugal Center for Health Technology and Services Research University of Aveiro Aveiro Portugal †Center for Health Technology and Services Research, University of Aveiro, Aveiro, Portugal ‡ Department of Electronics Telecommunications and Informatics, University of Aveiro, Aveiro, Portugal Department of Electronics Telecommunications and Informatics University of Aveiro Aveiro Portugal ‡Department of Electronics Telecommunications and Informatics, University of Aveiro, Aveiro, Portugal § Department of Education and Psychology, University of Aveiro, Aveiro, Portugal Department of Education and Psychology University of Aveiro Aveiro Portugal §Department of Education and Psychology, University of Aveiro, Aveiro, Portugal ║ School of Health Science, University of Aveiro, Aveiro, Portugal School of Health Science University of Aveiro Aveiro Portugal ║School of Health Science, University of Aveiro, Aveiro, Portugal ¶ Center for Research and Development in Mathematics and Applications, University of Aveiro, Aveiro, Portugal Center for Research and Development in Mathematics and Applications University of Aveiro Aveiro Portugal ¶Center for Research and Development in Mathematics and Applications, University of Aveiro, Aveiro, Portugal # Department of Mathematics, University of Aveiro, Aveiro, Portugal Department of Mathematics University of Aveiro Aveiro Portugal #Department of Mathematics, University of Aveiro, Aveiro, Portugal ⁎ Address correspondence and reprint requests to: Luciana Albuquerque, IEETA, Universidade de Aveiro Departamento de Electronica Telecomunicacoes e Informatica, Campus Universito de Santiago, 3770-058 Aveiro, Portugal. IEETA, Universidade de Aveiro Departamento de Electronica Telecomunicacoes e Informatica, Campus Universito de Santiago Aveiro 3770-058 Portugal The knowledge about the age effects in speech acoustics is still disperse and incomplete. This study extends the analyses of the effects of age and gender on acoustics of European Portuguese (EP) oral vowels, in order to complement initial studies with limited sets of acoustic parameters, and to further investigate unclear or inconsistent results. A database of EP vowels produced by a group of 113 adults, aged between 35 and 97, was used. Duration, fundamental frequency (f0), formant frequencies (F1 to F3), and a selection of vowel space metrics (F1 and F2 range ratios, vowel articulation index [VAI] and formant centralization ratio [FCR]) were analyzed. To avoid the arguable division into age groups, the analyses considered age as a continuous variable. The most relevant age-related results included: vowel duration increase in both genders; a general tendency to formant frequencies decrease for females; changes that were consistent with vowel centralization for males, confirmed by the vowel space acoustic indexes; and no evidence of F3 decrease with age, in both genders. This study has contributed to knowledge on aging speech, providing new information for an additional language. The results corroborated that acoustic characteristics of speech change with age and present different patterns between genders. Key Words Aging voice Acoustic Oral vowel European Portuguese Introduction According to the World Health Organization 1,2 the number of people aged over 65 is increasing as a result of longer life expectancy and also of declining fertility rates. Portugal is one of the developed countries with the highest rate of older population (between 1970 and 2018, the percentage of people aged 65 and over increased from 9.7% to 21.8%). 3,4 Aging involves changes at physiological, cognitive, psychological, and social levels. Age-related changes take place in different tissues and organs, and the vocal system is no exception. 5 Moreover there are substantial gender differences in the extent and timing of the aging process. 5–8 The anatomical and physiological changes in the speech organs (eg, decreased lung capacity; ossification and calcification of the laryngeal cartilages and vocal fold atrophy) 5,8,9 are reflected in the variation of several acoustic parameters, namely in the decrease of the speaking rate, in the increase of speech pauses, in the variation of the fundamental frequency (f0), in the pattern changes of the formant frequencies and in the increase jitter and shimmer, among others. 8,9 Unlike other languages, in which age-related speech variations have been widely studied since the 1960s, 8 on what concerns European Portuguese (EP) there are only a few studies about segmental and supra-segmental changes motivated by aging. 10–13 For these reasons, the purpose of this study is to extend the analyses of the age effects on duration, f0 and formant frequencies (F1, F2, and F3) for all EP oral vowels produced by a large group of healthy speakers. A more in-depth analysis of the age effect on each vowel was performed, using the database created for the authors initial studies. 10 To complement the initial inconclusive results obtained with vowel space area (VSA), which is a well established acoustic metric, 14–18 other acoustic indexes were adopted to further investigate age and gender effects. The first formant range ratio (F1RR) and second formant range ratio (F2RR) were selected to model possible reduction in the articulatory capability of speakers. 19–21 Vowel articulation index (VAI) and formant centralization ratio (FCR) were included to maximize sensitivity to vowel formant centralization and minimize sensitivity to interspeaker variability. 22,23 The general assumption is that young speakers have a better articulation capability than older speakers, thus young adults are able to move their tongue with greater amplitudes and they are able to hold it longer in certain positions. 24,25 The present study extends the analyses of previous researches 10,11 by including F3 values and different vowel space metrics from Portuguese adults covering the age range of 35–97, which is essential to provide a more complete view of age-related changes in EP vowel acoustics. As novelty this study considers age as a continuous variable in the analysis avoiding the effects of arbitrary age groups division. Thus, age-related changes in vowel acoustics are analyzed using multiple linear regression. Since there is a paucity of literature on EP vowel acoustics and the available data were collected from a small number of speakers, 11,26–28 this study also provides valuable insights to an accurate description of these sounds. The natural process of aging has a significant impact on the acoustic measurements of speakers’ vocal output. 29 Accordingly, it is very important that voice clinicians be aware of such effects and use discretion when making acoustic diagnoses and clinical judgments of elderly clients’ voices. 29 A deeper knowledge of how speech changes with age is also essential for the development of automatic speech recognition systems suitable for elderly’s voices (eg, personalized reading aids and voice prostheses), 30 for the provision of information for biometric recognition 31 and forensics, and for clinical assessment and treatment of speech disorders. 32,33 Background Numerous studies have evaluated the effects of aging on the acoustic properties of speech. 6,8,30 Most of them have focused on f0 and have shown a decrease with aging in women 8,34–39 ; for men there is less agreement among researches, with some studies indicating that f0 significantly decreases above 60, 30,40 and others suggesting an f0 drop in men over the age range 30–50 and then an increase in f0 in older age. 6–8,35–38,41,42 Other studies have reported on age-related changes to formants (mostly F1 and F2, neglecting higher formants), particularly in the production of vowels. The conclusions across studies are inconsistent, with some studies showing an age-dependent formant frequency lowering 9,29,43–46 and others reporting no changes in formant frequencies. 47,48 In some cases the formant frequencies varied with vowel and a gender-vowel interaction was found. 8,35 In addition, some studies have referred a centralization of the vowel space in older speakers (which should result in movement to the centroid of formant space). 8,24,25,38,49 It has often been noted that older adults use slower speaking rates, 6,8,50 that is, vocal aging implies a decrease in the number of syllables and phonemes per second, which leads to the increase of segment duration. 6,8,47,51–53 The few data available for the EP have indicated an f0 drop of 20 Hz with advanced age in women, while for men no age-related changes were observed, when comparing young adults (aged 19–30) with two groups of older adults (aged 60–75 and over 75). 12 Another study 13 reported a trend of decrease in f0 with aging in both genders (comparing speakers aged between [19–40] and [41–67]) in different speech tasks, but those changes were not statistically significant. In addition, Pellegrini et al 12 showed a greater centralization of the vowel space in younger speakers, and a significant increase in vowels duration with age for both genders. Our previous studies 10,11 and Pellegrini et al, 12 presented consistent results concerning vowel duration, showing a significantly increase with aging. On the subject of f0 and formant frequencies, the results were not consistent and seemed to be different among vowels. 10,11 Given that those previous researches used different corpora and analysis procedures, and focused on different acoustic parameters, it is hard at this time to draw solid conclusions on the effects of age and gender on each EP oral vowel. In general, only the decrease in vowel duration in both genders and a trend for f0 decrease in women with aging seems to be more consensual. 10–12 Method This cross-sectional study was approved by the Ethics Committee Centro Hospitalar São João/ Faculty of Medicine, University of Porto, Portugal (approval number N38/18), and all participants agreed and signed the consent form before participating in the study. Participants A total of 113 native Portuguese speakers (56 men and 57 women), from the central region of Portugal, aged between 35 and 97, participated in this study. To ensure an equitable distribution of participants, the following age groups were covered: [35–49] (15 men, 15 women), [50–64] (15 men, 15 women), [65–79] (15 men, 16 women), and ≥ 80 (11 men, 11 women). A 80-year-old woman was excluded during the acoustic data analysis. Participants were recruited through personal contacts and through snowball technique in the community, and in Senior Universities from the center of Portugal, and also in the University of Aveiro. Each participant completed a written questionnaire to gather information about socio-demographic characteristics, medical and voice related history, smoking habits, alcohol, water and caffeine consumption, support device needs and environmental conditions. For more details about the study design see Albuquerque et al. 10 Corpus The speech corpus consisted of 36 words, with the EP vowels [i], [e], [ɛ], [a], [o], [ɔ], [u] in stressed position and the vowels [ɨ] and [ɐ] in unstressed position. Each vowel was produced in a disyllabic sequence, mostly CV.CV (C-consonant, V-vowel) (eg, “pato”, duck), where C was a voiced/ voiceless stop consonant ([p], [t], [k], [b], [d], [ɡ]) or a voiced/ voiceless fricative consonant ([f], [s], [ʃ], [v], [z], [ʒ]). The list of 36 words used in this study is listed in Table 1 . The speech stimuli were carefully chosen to allow easy and accurate formant measure since the vowel context is restricted to stop and fricative consonants. The corpus was also designed to collect data over the life span. So, the words were therefore chosen to be familiar to all ages, and also, easily represented by images (to avoid the interference of reading abilities in the words production). A pilot naming study was carried out for the selection of these images. Sixty-three pictures were selected from color pictograms of Palao 54 and presented to a group of 10 participants (5 males and 5 females), ranging from 28 to 86. The results indicated that adult participants were able to properly name most of the pictures (percentage of accuracy equal or higher than 70% excepted for 6 images) (see Table 1). However, these stimuli remained in the study due to the difficulty in finding alternative words that met the previously defined criteria. The stimuli were embedded in a carrier sentence “Diga... por favor” (“Say... please”). Recording protocol Recordings took place in quiet rooms in several institutions using an AKG condenser microphone and USB external soundcard (PreSonus), with a sampling rate of 44,100 Hz. The participants were seated at a table and the microphone was adjusted to each participant and positioned at an approximately 15–20 cm distance from the mouth. The sentences were randomized and presented on the computer screen with software system SpeechRecorder 55,56 using pictures together with the orthographic word. Participants were asked to read the sentences at comfortable pitch and loudness level, after familiarizing themselves with the structure of the sentences. Additionally, they could take a break at any time they wanted and each speaker attended a single recording session. Each carrier sentence was repeated three times. Thus, each participant produced 12 repetitions of each vowel, in a total of 108 productions by speaker (113 participants x 36 words x 3 repetitions = 12 204 recordings), and needed approx. 15 minutes to complete this task. The same researcher was present in all recording sessions. Segmentation The recorded data was first automatically segmented at word and phoneme level using WebMAUS General for Portuguese language (PT) 57,58 and then imported into Praat speech analysis software, 59 so that four trained analyzers could manually check the accuracy of the vowel boundaries. The start and end points of the vowel were determined by finding the first and last periods that had considerable amplitude and whose shape resembled that of more central periods, with both points of the selection chosen to be at a positive zero crossing of the waveform. A total of 736 recordings were discarded (approximately 6% of trials) due to problems with the recordings (eg, clipping, noise, misread, hoarseness or vocal fry) or vowel reduction (vowel [ɨ] was the most deleted vowel (359 vowels [ɨ] corresponding to 26.7%), mostly in the context of “pescar” ([pɨʃˈkaɾ] - to fish). Acoustic measurements Acoustic parameters (f0 and formant frequencies) were automatically extracted from the central 40% of each target vowel using Praat scripts. 26 Median f0 value of the vowels was estimated with the cross-correlation algorithm. 26 The pitch range for the analysis was set to 60–400 Hz for men and 120–400 Hz for women. If the analysis failed on any of the speaker’s vowel tokens, that token was excluded (31 vowels, most of them produced by an 80-year-old woman, which we decided to exclude from all the analysis). Burg-LPC algorithm, as provided by Praat, was used to compile values for F1, F2, and F3. A procedure (adapted from Escudero et al, 26 ) was applied to optimize the formant ceiling for a certain vowel of a certain speaker. The first three formants were determined 201 times for each vowel, for all ceilings between 4500 and 6500 Hz for female and between 4000 and 6000 Hz for male, in steps of 10 Hz. The chosen ceiling was the one that yielded the lowest variation. Thus, for each vowel produced by each speaker there is only one “optimal ceiling” (for more details see Escudero et al. 26 ). The duration measurements were computed from the label files with reference to the beginning and the ending points of each vowel. Vowels with duration values shorter than 20 ms were excluded (8 vowels), and outliers that exceeded 2.5 standard deviations from the mean for particular speaker by f0 and from their gender x vowel mean by F1 and F2 were also excluded from this analysis. 35,60 In this study, the measurements for duration, f0, F1–F3 were manually checked for possible extraction errors and these procedures yielded in 695 outliers removed (nearly 1.5% of the total data). The F1RR is defined as the ratio of the F1 of the low vowel [a] and the (geometric) average F1 of the high vowels [i] and [u] by speaker. 19,20,26 The F2RR is computed as the ratio of the F2 of vowel [i] and the F2 of the vowel [u] for each speaker. 19,20,23,26 The VAI is calculated using the formula: (1) VAI = ( F 2 [ i ] + F 1 [ a ] ) / ( F 2 [ u ] + F 2 [ a ] + F 1 [ u ] + F 1 [ i ] ) , and its inverse, the FCR, is calculated as: (2) FCR = ( F 2 [ u ] + F 2 [ a ] + F 1 [ u ] + F 1 [ i ] ) / ( F 2 [ i ] + F 1 [ a ] ) 18,22,23 Note that the F1 and F2 coordinates of the EP corner vowels [a], [i] and [u] were used to calculate the VAI and FCR metrics, so the FCR should increase with centralization and decrease with vowel expansion, and the opposite for VAI. 22,23 Reliability in vowel segmentation To determine inter- and intra-rater reliability of the measures, 36 textgrids of each analyzer (1 textgrid randomly selected from each word) were re-labeled for all analyzers. So, 144 (1.2%) of a total of 12 204 textgrids (113 participants * 36 stimulus * 3 repetitions) were manually re-labelled for reliability by the four judges. The scripts to obtain vowel duration and formant frequencies were then re-administered. Inter and intrarater reliability was assessed using the intraclass correlation coefficient (ICC) and two-way mixed model (the raters were considered fixed) with an absolute agreement definition. Reliability among the raters was considered excellent, with ICC values > 0.952 for all vowels/ acoustic parameters (duration, f0, F1, F2, F3), except F1 of [ɨ] where ICC was 0.846, but still considered good reliability. 61 To assess intra-rater reliability, a random sample of 36 textgrids (one of each stimuli) was manually rechecked by the same rater. Again, reliability was excellent with ICC values > 0.909 for all vowels/ acoustic parameters. 61 Statistical analysis The statistical analysis was conducted with the SPSS software package (SPSS 25.0 - SPSS Inc., Chicago, Illinois). The values of f0, F1, F2, and F3 were computed for all productions, and subsequently, the median of repetitions was performed for each vowel and speaker. Descriptive data reported the mean for age group. For each vowel and acoustic parameter (duration, f0, F1, F2, and F3), a multiple linear regression was conducted with the following explanatory variables: age (continuous), gender (male: reference group, female), and the interaction between age and gender. The model presented by the software considered age and gender (female) redundant (presenting instead the interactions “male*age” and “female*age”) and no values are presented for those (independent) variables. Also, a multiple linear regression was conducted with the same explanatory variables for F1RR, F2RR, VAI, FCR, and for mean values of all vowels by acoustic parameter. The regression coefficients and the correspondent 95% IC were calculated. The residuals Normality was tested (Kolmogorov-Smirnov Test) and verified with the visual inspection of the Q-Q plot. Results This section presents the detailed results of the acoustic measurements and statistical analysis aimed at investigating differences by age and gender for f0 and formant frequencies of all vowels. To avoid effects of arbitrary age groups division, correlation and regresssion analyses for all acoustic parameters were performed. To complement the results of the linear regression and also to provide normative acoustic data for speakers of EP, average values for all acoustic parameters by age group and vowel, separately for each gender are presented in Appendices (Tables A1, B1 and C1). Vowel duration increased with age Scatterplot and regression results for the duration are presented in Figure 1 , and show an increase of mean duration for all vowels with age, for both genders. For females duration increased from approximately 100 ms to more than 140 ms between the ages 35 and 100; the increase was lower for males, only reaching 130 ms at the age of 100. The multiple linear regression revealed a significant effect of age in both genders, for most vowels and for the mean of all vowels (males: B = 0.451; P = 0.004; IC95% = [0.144;0.759]; females: B = 0.730; P < 0.001; IC95% = [0.427;1.033]). Only vowel [ɨ] in both men (B = 0.138; P = 0.258; IC95% = [ − 0.103;0.379]) and women (B = 0.183; P = 0.129; IC95% = [ − 0.054;0.421]) did not seem significantly affected by age. There was not a significant effect of gender, with men (114.4 ms ± 19.0) and women (118.3 ms ± 21.2) producing vowels with similar mean duration (B = 12.754; P = 0.362; IC95% = [ − 14.854;40.362]). Age effects in f0 were gender dependent The scatterplot of f0 mean vowels is presented in Figure 2 and shows that mean f0 tended to decrease with age in women and slightly increase in men. Regression lines indicated a decrease for females of about 25 Hz between the ages 35 and 100, and an increase around 10 Hz for males between the same ages. Regression model revealed a main effect of gender ( B = − 88.482 ; P < 0.001 ; I C 95 % = [ − 128.000 ; − 48.964 ] ), since male speakers had significantly lower f0 (138.7 Hz ± 27.6) compared to female speakers (193.3 Hz ± 23.9), as expected. The effects of age in both genders were not significant for the majority of the vowels, except for the unstressed vowels in females ([ɨ]: B = − 0.766 ; P = 0.003 ; I C 95 % = [ − 1.271 ; − 0.262 ] ; [ɐ]: B = − 0.954 ; P < 0.001 ; I C 95 % = [ − 1.434 ; − 0.475 ] ). In these vowels f0 decreased very sharply with age. As illustrated in Figure 3 , which was drawn using equations of linear regression (of all vowels by gender and f0) replacing the variable age for 35 and 100. f0 frequencies of all vowels tended to decrease in women (mainly in unstressed vowels) and to slightly increase in men (except the unstressed vowels), with aging. So, f0 tended to approach between genders as age increase. Age effects in formant frequencies were vowel and gender dependent As in previous sections, analysis of vowel formants start by showing scatterplots and regression of mean frequencies ( Figures 4–6 ). Vowel space based on regression results by gender and age is presented in Figure 7 . Due to the complexity of the results, the multiple linear regression coefficients are displayed in Table 2 . Figure 4 shows that mean F1 tended to decrease with age in both genders, mainly in women. The results of multiple linear regression coefficients (Table 2) revealed significant differences between genders for all vowels (except for central vowels ([ɨ], [ɐ], and [a]) and [ɛ]), with women presenting significantly higher mean F1 (486.3 ms ± 29.7) than men (436.4 ms ± 25.9). There was a significant age effect: in males for vowels [a] (Figure 8 ) and [e]; and in females for vowels [ɔ] and [u]. Summing up, as illustrated in Figure 7, F1 decreased with age, especially for vowels [a] and [ɔ], but increased for vowels [e], [i], and [ɨ] in males. In females, F1 decreased with age, especially for vowels [u], [ɔ], and [a]. As shown in Figure 5, mean F2 did not reveal remarkable changes with age. The statistical analysis revealed a main effect of gender on F2 (see Table 2): women’s mean F2 frequencies (1685.5 Hz ± 62.3) were significantly higher than those of men (1433.6 Hz ± 62.1). Only vowel [ɨ] did not present significant differences between genders. A reliable effect of age was found for some vowels depending on gender. Considerable decrease in F2 was found for males in vowel [i] and an opposite tendency was observed in [u] (see Figure 9 ). Female [ɔ] displayed an F2 decrease with aging. Mean F3 of the vowels (Figure 6) tended to slightly decrease with age in both genders, mainly in women. As seen in Table 2, there was a significant effect of gender for F3, with males (2440.5 ms ± 109.3) to have lower F3 mean values than females (2830.3 ms ± 132.3). In addition, there was no significant age effect in men and women, except for female [u], that decreased sharply with age. Age and gender effects on acoustic vowel space Changes in vowel space size were computed in order to track the relationship between talker age and vowel centralization or expansion. The scatterplot of the F1RR is presented in Figure 10 . The regression lines show a decrease with age in both genders, mainly in males, whose F1RR decreased from 2.6 to 2.0 as age increased. The multiple linear regression results revealed that for F1RR only the age effect on males was significant ( B = − 0.009 ; P = 0.006 ; I C 95 % = [ − 0.015 ; − 0.003 ] ). Moreover, the average F1RR of women was 2.609 (SD = 0.422) and 2.345 (SD = 0.307) for men. The female F1 space was therefore 2.609/2.345 = 1.135 times bigger than the male F1 space, although statistical model did not reveal a main effect of gender ( B = 0.220 ; P = 0.433 ; I C 95 % = [ − 0.335 ; 0.775 ] ). Figure 11 presents the mean F2RR and indicates a decrease with age only in males (F2RR decreased around 0.5 points between the ages 35 and 100). The effect of age and gender on F2RR was also analyzed, and as with F1RR, the statistical analysis only revealed a main effect of age in males ( B = − 0.008 ; P = 0.016 ; I C 95 % = [ − 0.015 ; − 0.002 ] ). Similar to F1, the size of the F2 space was higher for women (2.662) than for men (2.382), ie, the female F2 space was therefore 1.118 times bigger than the male F2 space. Nonetheless, the model did not include a main effect of gender ( B = 0.204 ; P = 0.490 ; I C 95 % = [ − 0.380 ; 0.787 ] ). VAI (see Figure 12 ) and FCR were also analyzed. The regression lines of VAI show a decrease of approx. 18% between the ages of 35 and 100 for males. As expected, for males, the opposite trend was observed for FCR. For females, both parameters remained stable with age. The multiple linear regression results revealed that, for both parameters, the age effect is only significant for men (FCR: B = 0.003 ; P < 0.001 ; I C 95 % = [ 0.002 ; 0.005 ] ; VAI: B = − 0.003 ; P = 0.001 ; I C 95 % = [ − 0.005 ; − 0.001 ] ). As expected (they act as interspeaker normalization), the statistical model did not reveal a main effect of gender (FCR: B = − 0.127 ; P = 0.067 ; I C 95 % = [ − 0.263 ; 0.009 ] ; VAI: B = − 0.099 ; P = 0.185 ; I C 95 % = [ − 0.048 ; 0.247 ] ). Additionally, Figure 7 allows to verify that males and females showed different tendencies with age, which is reflected in differences in vowel space sizes. Discussion This study contributes to increase knowledge on EP aging speech, providing an acoustical perspective of the effects of age (as a continuous variable) in all oral vowels of the EP for several acoustic parameters. The present study extends in many ways our previous research 10 by reporting results for another formant (F3) and additional acoustic features (F1RR, F2RR, VAI and FCR). Additionally, this study aims to provide normative values for several acoustic vowel parameters of EP adult speakers. So, these normative data can be used as a database for clinical and research purposes (eg, a speech-language pathologist may wish to compare an impaired voice with a typical voice). Similarities and differences in EP vowel acoustics presented with aging by male and female speakers were analyzed. First, as in most studies, vowels’ duration increased with age. Second, a tendency for f0 to decrease in women and to slightly increase in men was observed. Thirdly, F1 and F2 space underwent a reduction in males with aging. Finally, the frequencies of F3 were essentially unchanged with age. The results obtained are in general in line with previous research. However, some features, specially f0, did not yield as much age-related variation as reported in previous studies. 12,35,38 It should be noted these studies used different methodologies and different criteria for participant selection, 35,62,63 and in this sense the differences in results are not surprising. The age-related changes on acoustical parameters are discussed in detail further below. Vowels Duration As in most studies, 8,11,12,50,51,64 vowels’ duration was the acoustic parameter mostly affected by aging. 8,50,51,64 This probably occurs as a consequence of the decrease in the speech rate 6,8,65 and seems to be related to the neuromuscular slowing, altered nerve supply, respiratory changes, increased cautiousness and to the adjustment by older speakers of their tempo to maintain speech fluency. 6,8 After the vowel [ɐ], the unstressed vowel [ɨ] had the lowest duration and tended to be deleted (26,7%). 12,66 The deletion of unstressed vowels, especially of [ɨ], has been reported for many languages and also for EP. 66–71 At the same time, [ɨ] duration remains almost unchanged with age. Fundamental Frequency The results of our study give additional support that age related changes in f0 are gender dependent, which leads to an approximation of f0 values between genders as age increases. As in the current study, most of the literature for other languages reported a lowering of f0 with age for women, and a raising of f0 for men (not always significant). 8,35,37,38,48,72 For the EP, previous studies were consistent with the decrease of f0 in females, 12,13 whereas in males no significant age-related changes in f0 were observed. 11–13 It has been suggested that the f0 drop in women with age results from the increase in vocal fold mass due to hormonal changes that occur during menopause. 6,7,32,36,73–75 The raise in f0 in men after middle age has been attributed to reduced vocal fold mass and stiffness of vocal fold tissues due to aging-induced atrophy of the internal thyroarytenoid. 6,37,48,74,76 Additionally, it is important to mention that unstressed vowels behaved differently from stressed vowels with age. So, in unstressed vowels, f0 tended to decrease in both genders (although only statistically significant for females) and presented different values than expected. In other words, [ɨ] and [ɐ] f0 tended to be lower than the f0 of vowel [a] with age in both genders. This finding raises questions about the usual physiological explanation for a rise of f0 in older men, 6,37,48,74,76 that is, it remains unclear why a reduced mass and/or stiffness of vocal folds should affect only stressed vowels, whereas unstressed vowels show quite an opposite pattern. Formant frequencies We also observed a general lowering of F1 and F2 frequencies for women in all stressed vowels (although significant differences occurred only for F1[u], F1[ɔ] and F2[ɔ]); as men showed: (1) a decrease in F1 for low vowels (specially in [a]) and an increase in high vowels (specially in [i]); (2) an F2 decrease with age for [i], and a raising of F2 for [u], which suggests that only older men showed formant frequency evidence of vowel centralization, reported in previous researches. 8,24,25,38,49 For EP, an opposite tendency was observed in Pellegrini et al, 12 which have shown a greater centralization of the vowel space in younger speakers (males and females aged 19–28). However, our study does not cover the same age range, for that it is difficult to draw solid conclusions. Although the VSA, in the previous study, 10 did not show centralization for both genders, all the vowel space ratios selected for this comprehensive study (F1RR, F2RR, VAI, and FCR) indicated significant changes consistent with the centralization of the vowel space for male speakers. These results corroborate the main hypothesis that young males have a better articulation precision than older males. 6,15,20 Also, F2RR reflect restricted movements of the tongue in the anterior-posterior direction and restricted movements of the lips (rounding for [u] and retraction for [i]). 23 For example, an increase in F2 can be caused by a more anterior tongue position, but also by a decrease in lip rounding or tongue body shape. 77 The present results tend to confirm that the F1RR, F2RR, VAI, and FCR metrics are more sensitive to mild vowel articulation changes with age than VSA. Several explanations have been advanced to account for age-related changes in vowel formant frequencies, 7,9,29,35,63 like altered dimensions of the back cavity, 29,78 changes of the shape of the oral cavity (loss of teeth and the introduction of dentures), 7,29 diachronic or intergenerational phonetic change 79 or slower tongue movements and loss of tongue strength. 6,43 Additionally, as in Schötz 8 and Eichhorn et al, 35 the frequencies of F3 were essentially unchanged with age. This result does not support the idea of vocal tract lengthening in older age reported in previous studies. 9,29,43–46 So, the lack of an aging effect on the F3 indicates that any changes found for F1 and F2 are related to specific articulatory effects. 35 And also, this claim could be corroborated by the findings about males’ patterns of f0 change in stressed vs. unstressed vowels. Study limitations and future work Given the methodological differences across previous studies, variable results are not surprising. For that, it is difficult to fix a particular age or age range where changes occur in either gender. 63 Speaker age leaves traces in all phonetic dimensions and its impact on the voice is influenced by numerous factors, such as physiological condition, occupations and lifestyle habits, 5,8 which were not handled in this study. Also the type of speech samples used could affect the results. In more conversational contexts, speakers tend to show decreases in average vowel duration coupled with a higher degree of vowel centralization. 47 It is possible that, in order to see differences in vowel centralization with age, a task which demands greater movement of speakers’ vocal tracts might be required. 47 And finally, vowel duration was not controlled, which renders comparisons across studies to be problematic in several ways. An open and interesting question remains: whether the changes that we have observed in our data are the result of passive physiological changes to the vocal tract, or whether speech production is actively modified with increasing age, in order to compensate perceptually for the influences of the age-related decline on vowel quality. For that, the relation between the vowel acoustic and the articulatory changes with aging will be addressed in future work using instrumental techniques, such as ultrasonography. Additionally, dynamic measurements of vowels’ formant would be highly desirable, to provide a more complete view of vowel characteristics, and to avoid a necessarily arbitrary choice of selecting a specific time point where the measurements are taken. Given that in cross-sectional studies speakers include various factors other than age alone, that could affect the results. In future, we plan to develop a longitudinal research study that traces the acoustic features of the same speaker over a long period of time. Conclusion The results of this study provide a base of information to establish vowel acoustics normal patterns of aging among Portuguese adults. So, this study adds to the growing body of data on the effects of age on the acoustic properties of speech, providing information on vowel acoustics from adults who speak a language different from English. In that sense, it might help to better understand cross-linguistic similarities and language-particular features of vowel aging. Summing up, the statistical analyses have shown which vowels are more affected by aging: (1) the unstressed oral vowels f0, mainly for females; (2) formants of vowels [u] and [ɔ] for females; (3) vowels [i], [u], [a], and [e] males’ formants. The acoustic changes resulting from the natural process of aging are an important basis to understand speech and voice disorders associated with health conditions that affect older individuals (eg, hearing loss, dentofacial alterations, neurodegenerative diseases, stroke, cancer, or psychological distress). 35 Wherefore, it is very important that voice clinicians are aware of such effects and take these into account in their intervention. Furthermore, the correlates of speaker age reported in this study may further be helpful for the development of methods for the automatic detection of speaker age, as well as for the synthesis of speaker age. Thereby, better age recognizers or classifiers may be achieved, as well as better and more natural-sounding synthesis of speaker age. 8 Acknowledgments This research was financially supported by the project Vox Senes POCI-01-0145-FEDER-03082 (funded by FEDER, through COMPETE2020 - Programa Operacional Competitividade e Internacionalização (POCI), and by national funds (OE), through FCT/MCTES), by the grant SFRH/BD/115381/2016, and by IEETA (UIDB/00127/2020), and by CIDMA (UID/MAT/04106/2019). We are very grateful to the institutions for having made possible the data collection, and also to all the adults who contributed as speakers. Appendix A Vowel Duration Appendix B Fundamental Frequency Appendix C Formant Frequencies References 1 World Health Organization. Ageing. 2012a. 2 World Health Organization. Definition of an older or elderly person. 2012b. 3 S. Portugal Estimativas de População Residente em Portugal - 2018 (Estimates of resident population in Portugal - 2018) Destaque: informação à comunicação social 2019 Portugal S.. Estimativas de População Residente em Portugal - 2018 (Estimates of resident population in Portugal - 2018). Destaque: informação à comunicação social 2019;. 4 S. Portugal Envelhecimento da população residente em Portugal e na União Europeia (Aging population living in Portugal and in the European Union) Destaque: informação comunicação social 2015 Portugal S.. Envelhecimento da população residente em Portugal e na União Europeia (Aging population living in Portugal and in the European Union). Destaque: informação comunicação social 2015;. 5 Aging Voice Makiyama K. Hirano S. 2017 Springer Singapore Makiyama K., Hirano S., editors. Aging Voice. Singapore: Springer; 2017. 6 S.E. Linville Vocal aging 2001 Singular Thomson Learning Australia, San Diego Linville S. E.. Vocal aging. Australia, San Diego: Singular Thomson Learning; 2001. 7 H. Mautner A Cross-System Instrumental Voice Profile of the Aging Voice: With Considerations of Jaw Posture Effects 2011 University of Canterbury Ph.D. thesis Mautner H.. A Cross-System Instrumental Voice Profile of the Aging Voice: With Considerations of Jaw Posture Effects. Ph.D. thesis; University of Canterbury; 2011. 8 S. Schötz Perception, analysis and synthesis of speaker age Travaux de l’Institut de Linguistique de Lund vol. 47 2006 Linguistics and Phonetics Lund University Schötz S.. Perception, analysis and synthesis of speaker age; vol. 47 of Travaux de l’Institut de Linguistique de Lund. Lund University: Linguistics and Phonetics; 2006. 9 S.E. Linville J. Rens Vocal tract resonance analysis of aging voice using long-term average spectra J Voice 15 2001 323 330 Linville S. E., Rens J.. Vocal Tract Resonance Analysis of Aging Voice Using Long-Term Average Spectra. Journal of Voice 2001;15(3):323–330. 10 L. Albuquerque C. Oliveira A. Teixeira Age-related changes in European Portuguese vowel acoustics INTERSPEECH 2019 ISCA Graz 3965 3969 Albuquerque L., Oliveira C., Teixeira A., Sa-Couto P., Figueiredo D.. Age-related changes in European Portuguese vowel acoustics. In: INTERSPEECH. 2019, p. 3965–3969. 11 L. Albuquerque C. Oliveira A. Teixeira Impact of age in the production of European Portuguese vowels INTERSPEECH 2014 ISCA Singapore 940 944 Albuquerque L., Oliveira C., Teixeira A., Sa-Couto P., Freitas J., Dias M. S.. Impact of age in the production of European Portuguese vowels. In: INTERSPEECH. Singapore: ISCA; 2014, p. 940–944. 12 T. Pellegrini A. Hömölöinen P.B. de Mareüil A corpus-based study of elderly and young speakers of European Portuguese: acoustic correlates and their impact on speech recognition performance INTERSPEECH 2013 852 856 Pellegrini T., Hömölöinen A., de Mareüil P. B., Tjalve M., Trancoso I., Candeias S., et al. A corpus-based study of elderly and young speakers of European Portuguese: acoustic correlates and their impact on speech recognition performance. In: INTERSPEECH. Lyon, France; 2013, p. 852–856. Lyon, France 13 I. Guimarães E. Abberton Fundamental frequency in speakers of Portuguese for different voice samples J Voice 19 2005 592 606 Guimarães I., Abberton E.. Fundamental frequency in speakers of Portuguese for different voice samples. Journal of voice 2005;19(4):592–606. 14 Z. Mou W. Teng H. Ouyang Quantitative analysis of vowel production in cerebral palsy children with dysarthria J Clin Neurosci 66 2019 77 82 Mou Z., Teng W., Ouyang H., Chen Y., Liu Y., Jiang C., et al. Quantitative analysis of vowel production in cerebral palsy children with dysarthria. Journal of Clinical Neuroscience 2019;66:77–82. 15 T. Arias-Vergara J.C. Vásquez-Correa J.R. Orozco-Arroyave Parkinson’s disease and aging: analysis of their effect in phonation and articulation of speech Cognit Comput 9 2017 731 748 Arias-Vergara T., Vásquez-Correa J. C., Orozco-Arroyave J. R.. Parkinson’s disease and aging: analysis of their effect in phonation and articulation of speech. Cognitive Computation 2017;9(6):731–748. 16 A.T. Neel Vowel space characteristics and vowel identification accuracy J Speech Lang Hear Res 51 2008 574 585 Neel A. T.. Vowel space characteristics and vowel identification accuracy. J Speech Lang Hear Res 2008;51(3):574–585. 17 D. McCloy R. Wright P. Souza Modeling intrinsic intelligibility variation: vowel-space size and structure Meetings on acoustics vol. 18 2014 ASA 060007 McCloy D., Wright R., Souza P.. Modeling intrinsic intelligibility variation: vowel-space size and structure. In: Meetings on Acoustics; vol. 18. ASA; 2014, p. 060007. 18 N. Roy S.L. Nissen C. Dromey Articulatory changes in muscle tension dysphonia: evidence of vowel space expansion following manual circumlaryngeal therapy J Commun Disord 42 2009 124 135 Roy N., Nissen S. L., Dromey C., Sapir S.. Articulatory changes in muscle tension dysphonia: Evidence of vowel space expansion following manual circumlaryngeal therapy. Journal of Communication Disorders 2009;42(2):124–135. 19 N. Audibert C. Fougeron C. Gendrot Duration-vs. style-dependent vowel variation: a multiparametric investigation ICPhS’15 2015 Glasgow 5 9 Audibert N., Fougeron C., Gendrot C., Adda-Decker M.. Duration-vs. style-dependent vowel variation: A multiparametric investigation. In: ICPhS’15. Glasgow; 2015, p. 5–9. 20 C. Fougeron N. Audibert Testing various metrics for the description of vowel distortion in dysarthria ICPhS XVII 2011 City University of Hong Kong Hong Kong 687 690 Fougeron C., Audibert N.. Testing various metrics for the description of vowel distortion in dysarthria. In: ICPhS. 2011, p. 687–690. 21 S. Gahl R.H. Baayen Twenty-eight years of vowels: tracking phonetic variation through young to middle age adulthood J Phonet 74 2019 42 54 Gahl S., Baayen R. H.. Twenty-eight years of vowels: Tracking phonetic variation through young to middle age adulthood. Journal of Phonetics 2019;74:42–54. 22 S. Sapir L.O. Ramig J. Spielman Acoustic metrics of vowel articulation in Parkinson’s disease: vowel space area (VSA) vs. vowel articulation index (VAI) MAVEBA-2011 2011 ISCA Firenze 173 175 Sapir S., Ramig L. O., Spielman J., Fox C.. Acoustic metrics of vowel articulation in Parkinson’s disease: vowel space area (VSA) vs. vowel articulation index (VAI). In: MAVEBA-2011. Firenze; 2011, p. 173–175. Firenze 23 S. Sapir L.O. Ramig J.L. Spielman Formant centralization ratio: a proposal for a new acoustic measure of dysarthric speech J Speech Lang Hear Res 53 2010 114 125 Sapir S., Ramig L. O., Spielman J. L., Fox C.. Formant Centralization Ratio: A Proposal for a New Acoustic Measure of Dysarthric Speech. J Speech Lang Hear Res 2010;53:114–125. 24 M.P. Rastatter R.D. Jacques Formant frequency structure of the aging male and female vocal tract Folia Phoniatrica 42 1990 312 319 Rastatter M. P., Jacques R. D.. Formant frequency structure of the aging male and female vocal tract. Folia phoniatrica 1990;42(6):312–319. 25 M.P. Rastatter R.A. McGuire J. Kalinowski Formant frequency characteristics of elderly speakers in contextual speech Folia Phoniatrica et Logopaedica 49 1997 1 8 Rastatter M. P., McGuire R. A., Kalinowski J., Stuart A.. Formant frequency characteristics of elderly speakers in contextual speech. Folia Phoniatrica et Logopaedica 1997;49(1):1–8. 26 P. Escudero P. Boersma A.S. Rauber A cross-dialect acoustic description of vowels: Brazilian and European Portuguese J Acoust Soc Am 126 2009 1379 1393 Escudero P., Boersma P., Rauber A. S., Bion R.. A cross-dialect acoustic description of vowels: Brazilian and European Portuguese. J Acoust Soc Am 2009;126(3):1379–1393. 27 C. Oliveira M.M. Cunha S. Silva Acoustic analysis of European Portuguese oral vowels produced by children Advances in Speech and Language Technologies for Iberian Languages: IberSPEECH vol. 328 2012 Springer Madrid, Spain 129 138 Oliveira C., Cunha M. M., Silva S., Teixeira A., Sa-Couto P.. Acoustic analysis of European Portuguese oral vowels produced by children. In: Advances in Speech and Language Technologies for Iberian Languages: IberSPEECH; vol. 328. Madrid, Spain: Springer; 2012, p. 129–138. 28 M.R.D. Martins Análise acústica das vogais orais tónicas em Português (Acoustic analysis of stressed oral vowels in Portuguese) Boletim de Filologia 22 1973 303 314 Martins M. R. D.. Análise acústica das vogais orais tónicas em Português (Acoustic analysis of stressed oral vowels in Portuguese). Boletim de Filologia 1973;22:303–314. 29 S.A. Xue G.J. Hao Changes in the Human vocal tact due to aging and the acoustic correlates of speech production: a pilot study J Speech Lang Hear Res 46 2003 689 701 Xue S. A., Hao G. J.. Changes in the Human vocal tact due to aging and the acoustic correlates of speech production: a pilot study. J Speech Lang Hear Res 2003;46(3):689–701. 30 R. Vipperla S. Renals J. Frankel Ageing voices: the effect of changes in voice parameters on ASR performance EURASIP J Aud Speech Music Process 2010 2010 1 10 10.1155/2010/525783 Vipperla R., Renals S., Frankel J.. Ageing voices: The effect of changes in voice parameters on ASR performance. EURASIP J Aud Speech Music Process 2010;2010:1–10. 31 A. Lanitis A survey of the effects of aging on biometric identity verification Int J Biometr 2 2010 34 52 Lanitis A.. A survey of the effects of aging on biometric identity verification. International Journal of Biometrics 2010;2(1):34–52. 32 R.T. Sataloff D. Caputo Rosen M. Hawkshaw The aging adult voice J Voice 11 1997 156 160 Sataloff R. T., Caputo Rosen D., Hawkshaw M., Spiegel J. R.. The aging adult voice. Journal of Voice 1997;11(2):156–160. 33 M.M. Johns III L.C. Arviso F. Ramadan Challenges and opportunities in the management of the aging voice Otolaryngol 145 2011 1 6 Johns III M. M., Arviso L. C., Ramadan F.. Challenges and opportunities in the management of the aging voice. Otolaryngology - Head and Neck Surgery 2011;145(1):1–6. 34 H. Goy D.N. Fernandes M.K. Pichora-Fuller Normative voice data for younger and older adults J Voice 27 2013 545 555 Goy H., Fernandes D. N., Pichora-Fuller M. K., van Lieshout P.. Normative voice data for younger and older adults. Journal of Voice 2013;27(5):545–555. 35 J.T. Eichhorn R.D. Kent D. Austin Effects of aging on vocal fundamental frequency and vowel formants in men and women J Voice 32 2018 644.e1 644.e9 Eichhorn J. T., Kent R. D., Austin D., Vorperian H. K.. Effects of aging on vocal fundamental frequency and vowel formants in men and women. Journal of Voice 2018;32(5):644.e1–644.e9. 36 E.P.M. Ma A.L. Love Electroglottographic evaluation of age and gender effects during sustained phonation and connected speech J Voice 24 2010 146 152 Ma E. P. M., Love A. L.. Electroglottographic Evaluation of Age and Gender Effects During Sustained Phonation and Connected Speech. Journal of Voice 2010;24(2):146–152. 37 M. Nishio S. Niimi Changes in speaking fundamental frequency characteristics with aging Folia Phoniatrica et Logopaedica 60 2008 120 127 Nishio M., Niimi S.. Changes in speaking fundamental frequency characteristics with aging. Folia Phoniatrica et Logopaedica 2008;60(3):120–127. 38 P. Torre III J.A. Barlow Age-related changes in acoustic characteristics of adult speech J Commun Disord 42 2009 324 333 Torre III P., Barlow J. A.. Age-related changes in acoustic characteristics of adult speech. Journal of Communication Disorders 2009;42:324–333. 39 A. Yamauchi H. Yokonishi H. Imagawa Age-and gender-related difference of vocal fold vibration and glottal configuration in normal speakers: analysis with glottal area waveform J Voice 28 2014 525 531 Yamauchi A., Yokonishi H., Imagawa H., Sakakibara K.-I., Nito T., Tayama N., et al. Age-and gender-related difference of vocal fold vibration and glottal configuration in normal speakers: analysis with glottal area waveform. Journal of Voice 2014;28(5):525–531. 40 S.A. Xue D. Deliyski Effects of aging on selected acoustic voice parameters: preliminary normative data and educational implications Educ Gerontol 27 2001 159 168 Xue S. A., Deliyski D.. Effects of aging on selected acoustic voice parameters: preliminary normative data and educational implications. Educational Gerontology 2001;27(2):159–168. 41 E.T. Stathopoulos J.E. Huber J.E. Sussman Changes in acoustic characteristics of the voice across the life span: measures from individuals 4–93 years of age J Speech Lang Hear Res 54 2011 1011 1021 Stathopoulos E. T., Huber J. E., Sussman J. E.. Changes in acoustic characteristics of the voice across the life span: measures from individuals 4-93 years of age. J Speech Lang Hear Res 2011;54(4):1011–1021. 42 L. Ramig R. Ringel Effects of physiological aging on selected acoustic characteristics of voice J Speech Hear Res 26 1983 22 30 Ramig L., Ringel R.. Effects of Physiological Aging on Selected Acoustic Characteristics of Voice. J Speech Hear Res 1983;26(1):22–30. 43 P.J. Watson B. Munson A comparison of vowel acoustics between older and younger adults ICPhS XVI 2007 Saarbrücken 561 564 Watson P. J., Munson B.. A comparison of vowel acoustics between older and younger adults. In: ICPhS XVI. Saarbrücken; 2007, p. 561–564. 44 J. Harrington S. Palethorpe C.I. Watson Age-related changes in fundamental frequency and formants: a longitudinal study of four speakers INTERSPEECH 2007 Belgium 2753 2756 Harrington J., Palethorpe S., Watson C. I.. Age-related changes in fundamental frequency and formants: a longitudinal study of four speakers. In: INTERSPEECH. Belgium; 2007, p. 2753–2756. 45 W. Endres W. Bambach G. Flösser Voice spectrograms as a function of age, voice disguise, and voice imitation J Acoust Soc of Am 49 6B 1971 1842 1848 Endres W., Bambach W., Flösser G.. Voice spectrograms as a function of age, voice disguise, and voice imitation. J Acoust Soc of Am 1971;49(6B):1842–1848. 46 F. Decoster W. Debruyne Acoustic differences between sustained vowels perceived as young or old Log Phon Vocol 24 1999 1 5 Decoster F., Debruyne W.. Acoustic differences between sustained vowels perceived as young or old. Log Phon Vocol 1999;24(1):1–5. 47 A.R. Fletcher M.J. McAuliffe K.L. Lansford The relationship between speech segment duration and vowel centralization in a group of older speakers J Acoust Soc Am 138 2015 2132 2139 Fletcher A. R., McAuliffe M. J., Lansford K. L., Liss J. M.. The relationship between speech segment duration and vowel centralization in a group of older speakers. J Acoust Soc Am 2015;138(4):2132–2139. 48 S. Sebastian S. Babu N.E. Oommen A. Ballraj Acoustic measurements of geriatric voice J Laryngol Voice 2 2012 81 84 Sebastian S., Babu S., Oommen N. E., Ballraj A.. Acoustic measurements of geriatric voice. Journal of Laryngology and Voice 2012;2(2):81–84. 49 J. Mertens D. Mücke A. Hermes Aging effects on prosodic marking in German: an acoustic analysis Second workshop on speech perception and production across the lifespan (poster) 2020 UCL London Mertens J., Mücke D., Hermes A.. Aging effects on prosodic marking in German: An acoustic analysis. In: 2nd Workshop on Speech Perception and Production across the Lifespan (Poster). London: UCL; 2020, p. Poster. 50 B.L. Smith J. Wasowicz J. Preston Temporal characteristics of the speech of normal elderly adults J Speech Hear Res 30 1987 522 529 Smith B. L., Wasowicz J., Preston J.. Temporal characteristics of the speech of normal elderly adults. J Speech Hear Res 1987;30(4):522–529. 51 B.J. Benjamin Phonological performance in gerontological speech J Psycholinguist Res 11 1982 159 167 Benjamin B. J.. Phonological performance in gerontological speech. Journal of Psycholinguistic Research 1982;11(2):159–167. 52 E.B. Slawinski Acoustic correlates of [b] and [w] produced by normal young to elderly adults J Acoust Soc of Am 95 1994 2221 2230 10.1121/1.408682 Slawinski E. B.. Acoustic correlates of [b] and [w] produced by normal young to elderly adults. J Acoust Soc of Am 1994;95(4):2221–2230. 10.1121/1.408682. 10.1121/1.408682 53 C. Fougeron D. D’Alessandro L. Lancia Reduced coarticulation and aging J Acoust Soc Am 144 2018 1905 10.1121/1.5068348 Fougeron C., D’Alessandro D., Lancia L.. Reduced coarticulation and aging. J Acoust Soc Am 2018;144(3):1905. 10.1121/1.5068348. 10.1121/1.5068348 54 Palao, S. (2017). Color Pictograms ARASAAC. Aragonese Portal of Augmentative and Alternative Communication. 55 C. Draxler K. Jönsch SpeechRecorder - a universal platform independent multi-channel audio recording software LREC’04 2004 ELRA Lisbon 559 562 Draxler C., Jönsch K.. SpeechRecorder - A universal platform independent multi-channel audio recording software. In: LREC’04. Lisbon, Portugal; 2004, p. 559–562. 56 Draxler C., Jönsch K.. SpeechRecorder. 2017. 57 T. Kisler U. Reichel F. Schiel Multilingual processing of speech via web services Comput Speech Lang 45 2017 326 347 Kisler T., Reichel U., Schiel F.. Multilingual processing of speech via web services. Computer Speech & Language 2017;45:326–347. 58 F. Schiel Automatic phonetic transcription of non-prompted speech 14th ICPhS 1999 University of California San Francisco 607 610 Schiel F.. Automatic phonetic transcription of non-prompted speech. In: 14th ICPhS. San Francisco; 1999, p. 607–610. 59 Boersma P., Weenink D.. Praat: doing phonetics by computer. 2012. 60 R. Smiljanic R.C. Gilbert Acoustics of clear and noise-adapted speech in children, young, and older adults J Speech Lang Hear Res 60 2017 3081 3096 Smiljanic R., Gilbert R. C.. Acoustics of clear and noise-adapted speech in children, young, and older adults. J Speech Lang Hear Res 2017;60(11):3081–3096. 61 P.E. Shrout J.L. Fleiss Intraclass correlations: uses in assessing rater reliability Psychol Bull 86 1979 420 428 Shrout P. E., Fleiss J. L.. Intraclass correlations: uses in assessing rater reliability. Psychological bulletin 1979;86(2):420–428. 62 A. Lutzross W. Schuerman R. Sprouse Development of vowel spaces from age 21 to age 49 in a group of 8 talkers J Acoust Soc Am 134 2013 4107 Lutzross A., Schuerman W., Sprouse R., Gahl S.. Development of vowel spaces from age 21 to age 49 in a group of 8 talkers. J Acoust Soc Am 2013;134(5):4107. 63 R.D. Kent H.K. Vorperian Static measurements of vowel formant frequencies and bandwidths: a review J Commun Disord 74 2018 74 97 Kent R. D., Vorperian H. K.. Static Measurements of Vowel Formant Frequencies and Bandwidths: A Review. Journal of communication disorders 2018;74:74–97. 64 E. Jacewicz R.A. Fox C. O’Neill Articulation rate across dialect, age, and gender Lang Variation Change 21 2009 233 256 Jacewicz E., Fox R. A., O’Neill C., Salmons J.. Articulation rate across dialect, age, and gender. Language variation and change 2009;21(02):233–256. 65 Y. Steffens The Aging Voice 2011 GRIN Verlag Steffens Y.. The Aging Voice. GRIN Verlag; 2011. 66 P. Fikkert From phonetic categories to phonological features specification: acquiring the European Portuguese vowel system Lingue e linguaggio 4 2005 263 280 Fikkert P.. From phonetic categories to phonological features specification: Acquiring the European Portuguese vowel system. Lingue e linguaggio 2005;4(2):263–280. 67 M.H.M. Mateus E. D’Andrade The phonology of Portuguese 2000 Oxford University Press Oxford Mateus M. H. M., D’Andrade E.. The phonology of Portuguese. Oxford: Oxford University Press; 2000. 68 J. Veloso Schwa in European Portuguese: the phonological status of [ Image 32 ] Journees d’Etudes Linguistiques 2007 55 60 Veloso J.. Schwa in European Portuguese: the phonological status of [1]. In: Journees d’Etudes Linguistiques. Nantes; 2007, p. 55–60. Nantes 69 J. Veloso Central, epenthetic, unmarked vowels and schwas: a brief outline of some essential differences Linguística: Revista de Estudos Linguísticos da Universidade do Porto 5 2017 191 213 Veloso J.. Central, epenthetic, unmarked vowels and schwas: a brief outline of some essential differences. Linguística: Revista de Estudos Linguísticos da Universidade do Porto 2017;5:191–213. 70 D.J. Silva The variable elision of unstressed vowels in European Portuguese: a case study Herring S.C. Paolillo J.C. UTA working papers in linguistics vol. 1 1994 Program in Linguistics University of Texas at Arlington 79 84 Silva D. J.. The variable elision of unstressed vowels in European Portuguese: A case study. In: Herring S. C., Paolillo J. C., editors. UTA Working Papers in Linguistics; vol. 1. University of Texas at Arlington: Program in Linguistics; 1994, p. 79–84. 71 D.J. Silva Vowel lenition in São Miguel Portuguese Hispania 81 1998 166 178 Silva D. J.. Vowel lenition in São Miguel Portuguese. Hispania 1998;81(1):166–178. 72 U. Reubold J. Harrington F. Kleber Vocal aging effects on F0 and the first formant: a longitudinal analysis in adult speakers Speech Commun 52 2010 638 651 Reubold U., Harrington J., Kleber F.. Vocal aging effects on F0 and the first formant: A longitudinal analysis in adult speakers. Speech Communication 2010;52:638–651. 73 C.T. Ferrand Harmonics-to-noise ratio: an index of vocal aging J Voice 16 2002 480 487 Ferrand C. T.. Harmonics-to-Noise Ratio: An index of vocal aging. Journal of Voice 2002;16(4):480–487. 74 M.B. Higgins J.H. Saxman A comparison of selected phonatory behaviors of healthy aged and young adults J Speech Hear Res 34 1991 1000 1010 Higgins M. B., Saxman J. H.. A Comparison of Selected Phonatory Behaviors of Healthy Aged and Young Adults. J Speech Hear Res 1991;34(5):1000–1010. 75 P. Pontes A. Brasolotto M. Behlau Glottic characteristics and voice complaint in the elderly J Voice 19 2005 84 94 Pontes P., Brasolotto A., Behlau M.. Glottic characteristics and voice complaint in the elderly. Journal of Voice 2005;19(1):84–94. 76 H. Hollien T. Shipp Speaking fundamental frequency and chronologic age in males J Speech Hear Res 15 1972 155 159 Hollien H., Shipp T.. Speaking Fundamental Frequency and Chronologic Age in Males. J Speech Hear Res 1972;15(1):155–159. 77 M. Wieling F. Tomaschek D. Arnold Investigating dialectal differences using articulography J Phonet 59 2016 122 143 Wieling M., Tomaschek F., Arnold D., Tiede M., Bröker F., Thiele S., et al. Investigating dialectal differences using articulography. Journal of Phonetics 2016;59:122–143. 78 G.P. Scukanec L. Petrosino K. Squibb Formant frequency characteristics of children, young adult, and aged female speakers Percept Motor Skills 73 1991 203 208 Scukanec G. P., Petrosino L., Squibb K.. Formant frequency characteristics of children, young adult, and aged female speakers. Perceptual and motor skills 1991;73(1):203–208. 79 R.A. Fox E. Jacewicz Dialect and generational differences in vowel space areas ExLing2010 2010 ISCA Athens, Greece 45 48 Fox R. A., Jacewicz E.. Dialect and generational differences in vowel space areas. In: ExLing2010. Athens, Greece: ISCA; 2010, p. 45–48. "
    },
    {
        "doc_title": "Contributions to a quantitative unsupervised processing and analysis of tongue in ultrasound images",
        "doc_scopus_id": "85087274917",
        "doc_doi": "10.1007/978-3-030-50516-5_15",
        "doc_eid": "2-s2.0-85087274917",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic analysis",
            "Computational approach",
            "Electromagnetic articulography",
            "Speech motor control",
            "Speech production",
            "Temporal coherence",
            "Tongue segmentation",
            "Ultrasound imaging"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Speech production studies and the knowledge they bring forward are of paramount importance to advance a wide range of areas including Phonetics, speech therapy, synthesis and interaction. Several technologies have been considered to study static and dynamic features of the articulators and speech motor control, such as electromagnetic articulography (EMA), real-time magnetic resonance (RTMRI) and ultrasound (US) imaging. While the latest advances in RTMRI provide a wealth of data of the full vocal tract, it is an expensive resource that requires specialized facilities. In this sense, US is a more affordable alternative for several contexts, enabling the acquisition of larger datasets, but demands adequate computational approaches for processing and analysis. While the literature is prolific in proposing methods for tongue segmentation from US, the noisy nature of the images and the specificities of the equipment often dictate a poor performance on novel datasets, a matter that needs to be assessed, before large data acquisition, to devise suitable acquisition and processing methods. In the scope of a line of research studying speech changes with age, this work describes the first results of an automatic tongue segmentation method from US, along with a characterization of the main challenges posed by the image data. Even though improvements are still needed, particularly to ensure temporal coherence, at its current stage, this method can already provide the required data for an automatic analysis of maximum tongue height, a relevant parameter to assess speech changes on vowel production.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enabling Multimodal Emotionally-Aware Ecosystems Through a W3C-Aligned Generic Interaction Modality",
        "doc_scopus_id": "85086143464",
        "doc_doi": "10.1007/978-3-030-49289-2_11",
        "doc_eid": "2-s2.0-85086143464",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Affective Computing",
            "Daily lives",
            "Emotional state",
            "Fast tracks",
            "Interactive system",
            "Key resources",
            "Life experiences",
            "Multi-modal"
        ],
        "doc_abstract": "© ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2020.Emotions play a key role in our life experiences. In interactive systems, the user’s emotional state can be relevant to provide increased levels of adaptation to the user, but can also be paramount in scenarios where such information might enable us to help users manage and express their emotions (e.g., anxiety), with a positive impact on their daily life and on how they interact with others. However, although there is a clear potential for emotionally-aware applications, they still have a long road to travel to reach the desired potential and availability. This is mostly due to the still low translational nature of the research in affective computing, and to the lack of straightforward, off-the-shelf methods for easy integration of emotion in applications without the need for developers to master the different concepts and technologies involved. In light of these challenges, we advance our previous work and propose an extended conceptual vision for supporting emotionally-aware interactive ecosystems and a fast track to ensure the desired translational nature of the research in affective computing. This vision then leads to the proposal of an improved iteration of a generic affective modality, a key resource to the accomplishment of the proposed vision, enabling off-the-shelf support for emotionally-aware applications in multimodal interactive contexts.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards automatic determination of critical gestures for European Portuguese sounds",
        "doc_scopus_id": "85081601133",
        "doc_doi": "10.1007/978-3-030-41505-1_1",
        "doc_eid": "2-s2.0-85081601133",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic determination",
            "Conservative approaches",
            "Critical articulator",
            "Data-driven approach",
            "Data-driven methods",
            "Real time",
            "Speech production",
            "Vocal-tract data"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Technologies, such as electromagnetic midsagittal articulography (EMA) and real-time magnetic resonance (RT-MRI), can contribute to improve our understanding of the static and dynamic aspects of speech, namely by providing information regarding which articulators are essential (critical) in producing specific sounds and how (gestures). Previous work has successfully demonstrated the possibility to determine critical articulators considering vocal tract data obtained from RT-MRI. However, these works have adopted a conservative approach by considering vocal tract representations analogous to the flash points obtained with EMA data, i.e., landmarks fixed over the articulators, e.g., tongue. To move towards a data-driven method able to determine gestural scores, e.g., driving articulatory speech synthesis, one important step is to move into a representation aligned with Articulatory Phonology and Task Dynamics. This article advances towards this goal by exploring critical articulators determination considering a vocal tract representation aligned with this framework is adopted and presents first results considering 50 Hz RTMRI data for two speakers of European Portuguese.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Editorial for special issue \"iberSPEECH2018: Speech and language technologies for Iberian languages\"",
        "doc_scopus_id": "85079271534",
        "doc_doi": "10.3390/app10010384",
        "doc_eid": "2-s2.0-85079271534",
        "doc_date": "2020-01-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Process Chemistry and Technology",
                "area_abbreviation": "CENG",
                "area_code": "1508"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Fluid Flow and Transfer Processes",
                "area_abbreviation": "CENG",
                "area_code": "1507"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2020 by the authors.The main goal of this Special Issue is to present the latest advances in research and novel applications of speech and language technologies based on the works presented at the IberSPEECH edition held in Barcelona in 2018, paying special attention to those focused on Iberian languages. IberSPEECH is the international conference of the Special Interest Group on Iberian Languages (SIG-IL) of the International Speech Communication Association (ISCA) and of the Spanish Thematic Network on Speech Technologies (Red Tematica en Tecnologias del Habla, or RTTH for short). Several researchers were invited to extend their contributions presented at IberSPEECH2018 due to their interest and quality. As a result, this Special Issue is composed of 13 papers that cover different topics of investigation related to perception, speech analysis and enhancement, speaker verification and identification, speech production and synthesis, natural language processing, together with several applications and evaluation challenges.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards european portuguese conversational assistants for smart homes",
        "doc_scopus_id": "85071088633",
        "doc_doi": "10.4230/OASIcs.SLATE.2019.5",
        "doc_eid": "2-s2.0-85071088633",
        "doc_date": "2019-07-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Conversational Assistants",
            "Device integration",
            "Green homes",
            "Home devices",
            "Low costs",
            "Smart devices",
            "Smart environment",
            "Smart homes"
        ],
        "doc_abstract": "© Maksym Ketsmur, António Teixeira, Nuno Almeida, and Samuel Silva.Nowadays, smart environments, such as Smart Homes, are becoming a reality, due to the access to a wide variety of smart devices at a low cost. These devices are connected to the home network and inhabitants can interact with them using smartphones, tablets and smart assistants, a feature with rising popularity. The diversity of devices, the user’s expectations regarding Smart Homes, and assistants’ requirements pose several challenges. In this context, a Smart Home Assistant capable of conversation and device integration can be a valuable help to the inhabitants, not only for smart device control, but also to obtain valuable information and have a broader picture of how the house and its devices behave. This paper presents the current stage of development of one such assistant, targeting European Portuguese, not only supporting the control of home devices, but also providing a potentially more natural way to access a variety of information regarding the home and its devices. The development has been made in the scope of Smart Green Homes (SGH) project.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Contribute for an ontology for smart homes and their conversational assistants",
        "doc_scopus_id": "85070062478",
        "doc_doi": "10.23919/CISTI.2019.8760934",
        "doc_eid": "2-s2.0-85070062478",
        "doc_date": "2019-06-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Conversational assistants",
            "Green homes",
            "Knowledge base",
            "Semantic information",
            "Smart homes",
            "User need"
        ],
        "doc_abstract": "© 2019 AISTI.Despite the improved capabilities of recent Assistants, control and access to the information regarding Smart Homes stills limited. More information is needed on what should be a Smart Home Assistant and how-to have the structured semantic information to support their answers and actions, i.e., how to structure the knowledge and make it simple to use by current approaches to Assistants, based commonly in intentions and entities. As contribute to increase Assistants capabilities in Smart Homes environments, based on analyses of the domain and user enquiries, we propose the basis for an ontology to support both, the interaction and the Smart Home knowledge base. The proposed ontology is being used to support the creation of an enhanced version of a Conversational Assistant for the Smart Green Homes project with novel functionalities, aligned with advanced user needs and expectations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The am4i architecture and framework formultimodal interaction and its application to smart environments",
        "doc_scopus_id": "85067542443",
        "doc_doi": "10.3390/s19112587",
        "doc_eid": "2-s2.0-85067542443",
        "doc_date": "2019-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Analytical Chemistry",
                "area_abbreviation": "CHEM",
                "area_code": "1602"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biochemistry",
                "area_abbreviation": "BIOC",
                "area_code": "1303"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Adaptation",
            "Devices",
            "Multi-devices",
            "Multi-Modal Interactions",
            "Smart environment"
        ],
        "doc_abstract": "© 2019 by the authors. Licensee MDPI, Basel, Switzerland.Technologies, such as smart sensors, actuators, and other kinds of devices, are often installed in our environments (e.g., our Homes) and available to integrate our daily lives. Despite their installation being motivated by the pursuit of automation and increased efficiency, making these environments usable, acceptable and enjoyable in a sustainable, energy efficient way is not only a matter of automation. Tackling these goals is a complex task demanding the combination of different perspectives including building and urban Architecture, Ubiquitous Computing and Human-Computer Interaction (HCI) to provide occupants with the means to shape these environments to their needs. Interaction is of paramount relevance in the creation of adequate relations of users with their environments, but it cannot be seen independently from the ubiquitous sensing and computing or the environment’s architecture. In this regard, there are several challenges to HCI, particularly in how to integrate this multidisciplinary effort. Although there are several solutions to address some of these challenges, the complexity and dynamic nature of the smart environments and the diversity of technologies involved still present many challenges,� particularly for its development. In general, the development is complex, and it is hard to create a dynamic environment providing versatile and adaptive forms of interaction. To participate in the multidisciplinary effort, the development of interaction must be supported by tools capable of facilitating co-design by multidisciplinary teams. In this article, we address the development of interaction for complex smart environments and propose the AM4I architecture and framework,� a novel modular approach to design and develop adaptive multiplatform multilingual multi-device multimodal interactive systems. The potential of the framework is demonstrated by proof-of-concept applications in two different smart environment contexts, non-residential buildings and smart homes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and Development for Individuals with ASD: Fostering Multidisciplinary Approaches Through Personas",
        "doc_scopus_id": "85064929398",
        "doc_doi": "10.1007/s10803-019-03898-1",
        "doc_eid": "2-s2.0-85064929398",
        "doc_date": "2019-05-15",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Developmental and Educational Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3204"
            }
        ],
        "doc_keywords": [
            "Autism Spectrum Disorder",
            "Humans",
            "Personality",
            "Psychotherapy",
            "User-Computer Interface"
        ],
        "doc_abstract": "© 2019, Springer Science+Business Media, LLC, part of Springer Nature.Developing technologies to support individuals with ASD is a growing field of research facing numerous challenges. First, while the individual with ASD is central, the motivations of others, such as parents, are often taken as the motivations of the individual. Second, the desirable cross-disciplinary pollination for improved intervention can often face difficulties due to a lack of a common language among disciplines. Thirdly, the literature often lacks enough information to allow a clear understanding of the targeted contexts and goals not enabling an assessment of outcomes and building on past advances. To tackle these challenges, we propose that families of Personas and scenarios are used throughout the design and development process, and as dissemination resources, and provide illustrative examples.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring critical articulator identification from 50Hz RT-MRI data of the vocal tract",
        "doc_scopus_id": "85074701958",
        "doc_doi": "10.21437/Interspeech.2019-2897",
        "doc_eid": "2-s2.0-85074701958",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Copyright © 2019 ISCAThe study of the static and dynamic aspects of speech production can profit from technologies such as electromagnetic midsagittal articulography (EMA) and real-time magnetic resonance (RTMRI). These can improve our knowledge on which articulators and gestures are involved in producing specific sounds and foster improved speech production models, paramount to advance, e.g., articulatory speech synthesis. Previous work, by the authors, has shown that critical articulator identification could be performed from RTMRI data of the vocal tract, with encouraging results, by extending the applicability of an unsupervised statistical identification method previously proposed for EMA data. Nevertheless, the slower time resolution of the considered RT-MRI corpus (14 Hz), when compared to EMA, potentially influencing the ability to select the most suitable representative configuration for each phone - paramount for strongly dynamic phones, e.g., nasal vowels -, and the lack of a richer set of contexts - relevant for observing coarticulation effects -, were identified as limitations. This article addresses these limitations by exploring critical articulator identification from a faster RTMRI corpus (50 Hz), for European Portuguese, providing a richer set of contexts, and testing how fusing the articulatory data of two speakers might influence critical articulator determination.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the role of oral configurations in European Portuguese nasal vowels",
        "doc_scopus_id": "85074699107",
        "doc_doi": "10.21437/Interspeech.2019-2232",
        "doc_eid": "2-s2.0-85074699107",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Copyright © 2019 ISCAThe characterisation of nasal vowels is not only a question of studying velar aperture. Recent work shows that oropharyngeal articulatory adjustments enhance the acoustics of nasal coupling or, at least, magnify differences between oral/nasal vowel congeners. Despite preliminary studies on the oral configurations of nasal vowels, for European Portuguese, a quantitative analysis is missing, particularly one to be applied systematically to a desirably large number of speakers. The main objective of this study is to adapt and extend previous methodological advances for the analysis of MRI data to further investigate: how velar changes affect oral configurations; the changes to the articulators and constrictions when compared with oral counterparts; and the closest oral counterpart. High framerate RT-MRI images (50fps) are automatically processed to extract the vocal tract contours and the position/configuration for the different articulators. These data are processed by evolving a quantitative articulatory analysis framework, previously proposed by the authors, extended to include information regarding constrictions (degree and place) and nasal port. For this study, while the analysis of data for more speakers is ongoing, we considered a set of two EP native speakers and addressed the study of oral and nasal vowels mainly in the context of stop consonants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Age-related changes in European Portuguese vowel acoustics",
        "doc_scopus_id": "85074694647",
        "doc_doi": "10.21437/Interspeech.2019-1818",
        "doc_eid": "2-s2.0-85074694647",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Copyright © 2019 ISCAThis study addresses effects of age and gender on acoustics of European Portuguese oral vowels, given to the fact of conflicting findings reported in prior research. Fundamental frequency (F0), formant frequencies (F1 and F2) and duration of vowels produced by a group of 113 adults, aged between 35 and 97 years old, were measured. Vowel space area (VSA) according to gender and age was also analysed. The results revealed that the most consistent age-related effect was an increase in vowel duration in both genders. F0 decreases above [50-64] for female and for male data suggests a slight drop over the age range [35-64] and then an increase in an older age. That is, F0 tends to be closer between genders as age increases. In general, there is no evidence that F1 and F2 frequencies were lowering as age increased. Furthermore, there were no changes to VSA with ageing. These results provide a base of information to establish vowel acoustics normal patterns of ageing among Portuguese adults.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multimodal interaction for accessible smart homes",
        "doc_scopus_id": "85061404918",
        "doc_doi": "10.1145/3218585.3218595",
        "doc_eid": "2-s2.0-85061404918",
        "doc_date": "2018-06-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Accessibility",
            "Context-Aware",
            "Design for all",
            "Multi-Modal Interactions",
            "Multi-platform",
            "Personas",
            "Smart homes",
            "Speech interface"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery.Nowadays, houses are being equipped with new smart products and smart sensors, from multiple manufacturers, each offering their own options for interaction with providing varying degrees of usability and user experience. This diverse nature of smart homes and buildings, in general, poses new challenges to interaction design and accessibility. To tackle them, human-building interaction needs to move from articulating different interactive artifacts towards an holistic view of the house as an interactive ecosystem. In a joint effort with Bosch Termotecnologia, S.A., and profiting from recent contributions including an architecture and framework supporting multimodal Interaction, the authors aim to explore novel ways of approaching interaction design with a smart house and proposing smart home applications for all. This paper presents the status of this ongoing work, in the scope of project Smart Green Homes, proposing how multimodal interaction can be supported in the scenario of a smart home and showing first results of tackling human-building interaction through a home assistant serving a family in their daily interactions with the house.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "EmotionalIy-aware multimodal interfaces: Preliminary work on a generic affective modality",
        "doc_scopus_id": "85061374910",
        "doc_doi": "10.1145/3218585.3218589",
        "doc_eid": "2-s2.0-85061374910",
        "doc_date": "2018-06-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Affective Computing",
            "Affective interaction",
            "Multi-Modal Interactions",
            "Multi-modal interfaces",
            "Multi-platform",
            "Multimodal interactive systems",
            "Natural interactions",
            "Straight-forward method"
        ],
        "doc_abstract": "© 2018 Association for Computing Machinery.In interactive systems, knowing the user's emotional state is not only important to understand and improve overall user experience, but also of the utmost relevance in scenarios where such information might foster our ability to help users manage and express their emotions (e.g., anxiety), with a strong impact on their daily life and on how they interact with others. Nevertheless, although there is a clear potential for emotionally-aware applications, several challenges preclude their wider availability, sometimes resulting from the low translational nature of the research in affective computing methods, and from a lack of straightforward methods for easy integration of emotion in applications. In light of these challenges, we propose a conceptual vision for the consideration of emotion in the scope of multimodal interactive systems, and how it can articulate with research in affective computing. Aligned with this vision, a first instantiation of an affective generic modality is presented, and a proof-of-concept application, enabling multimodal interaction with Spotify, illustrates how the modality can provide emotional context in interactive scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inner Speech in Portuguese: Acquisition Methods, Database and First Results",
        "doc_scopus_id": "85053911262",
        "doc_doi": "10.1007/978-3-319-99722-3_44",
        "doc_eid": "2-s2.0-85053911262",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Brain functions",
            "Exploratory analysis",
            "Functional magnetic resonance imaging",
            "Human speech",
            "Inferior frontal gyrus",
            "Intervention strategy",
            "Resources",
            "Supplementary motor areas"
        ],
        "doc_abstract": "© 2018, Springer Nature Switzerland AG.In this paper, we present a database developed for studying inner speech brain related areas using functional Magnetic Resonance Imaging (fMRI) in the context of the European Portuguese. First, we addressed the type of stimuli used in inner speech studies. In this sense, considering a preliminary study using a picture naming task, we defined a corpus. The corpus was designed based on cardinal vowels, syllable, disyllabic words and sentences with structure S(ubject)V(erb)O(bject) which were balanced in syllable number (six to ten). All the words used are common words from the Portuguese lexicon and possible ambiguities were excluded. Currently, the dataset includes data from twenty healthy participants native Portuguese speakers. Preliminary, exploratory analysis on the data allowed us to identify the most relevant areas part of the inner speech network, that include inferior frontal gyrus (including Broca’s area), supplementary motor area and precentral gyrus. Ultimately, the better understanding of the inner speech mechanisms will pave way to the development of novel intervention strategies in linguistic disorders.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Functional Mapping of Inner Speech Areas: A Preliminary Study with Portuguese Speakers",
        "doc_scopus_id": "85053749081",
        "doc_doi": "10.1007/978-3-319-99579-3_18",
        "doc_eid": "2-s2.0-85053749081",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "First keyword",
            "fMRI",
            "fMRI data analysis",
            "Functional mapping",
            "Inferior frontal gyrus",
            "Inner speech portuguese",
            "Portuguese",
            "Supplementary motor areas"
        ],
        "doc_abstract": "© 2018, Springer Nature Switzerland AG.Inner speech can be defined as the act of talking silently with ourselves. Several studies aimed to understand how this process is related to speech organization and language. Despite the advances, some results are still contradictory. Importantly, language dependency is scarcely studied. For this first study of inner speech for Portuguese native speakers using fMRI, we selected a confrontation naming task, consisting of 40 black and white line drawings. Five healthy participants were instructed to name in inner and in overt speech the visually presented image. fMRI data analysis considering the proposed inner speech paradigm identified several brain areas such as the left inferior frontal gyrus, including Broca’s area, supplementary motor area, precentral gyrus and left middle temporal gyrus including Wernicke’s area. Our results also show more pronounced bilateral activations during the overt speech task when compared to inner speech, suggesting that inner and overt speech activate similar areas but stronger activation can be found in the later. However, this difference stems in particular from significant activation differences in the right pre-central gyrus and middle temporal gyrus.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Natural transmission of information extraction results to end-users – a proof-of-concept using data-to-text",
        "doc_scopus_id": "85032615325",
        "doc_doi": "10.4230/OASIcs.SLATE.2017.20",
        "doc_eid": "2-s2.0-85032615325",
        "doc_date": "2017-10-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Geography, Planning and Development",
                "area_abbreviation": "SOCI",
                "area_code": "3305"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Automatic translation",
            "Data-to-Text",
            "Natural language generation",
            "Opinions",
            "Portuguese",
            "Tourism"
        ],
        "doc_abstract": "© José Casimiro Pereira, António J. S. Teixeira, Mário Rodrigues, Pedro Miguel, and Joaquim Sousa PintoInformation Extraction from natural texts has a great potential in areas such as Tourism and can be of great assistance in transforming customers’ comments in valuable information for Tourism operators, governments and customers. After extraction, information needs to be e ciently transmitted to end-users in a natural way. Systems should not, in general, send extracted information directly to end-users, such as hotel managers, as it can be di cult to read. Naturally, humans transmit and encode information using natural languages, such as Portuguese. The problem arising from the need of e cient and natural transmission of the information to end-user is how to encode it. The use of natural language generation (NLG) is a possible solution, for producing sentences, and, with them, texts. In this paper we address this, with a data-to-text system, a derivation of formal NLG systems that use data as input. The proposed system uses an aligned corpus, which was defined, collected and processed, in about approximately 3 weeks of work. To build the language model were used three di erent in-domain and out-of-domain corpora. The e ects of this approach were evaluated, and results are presented. Automatic metrics, BLEU and Meteor, were used to evaluate the di erent systems, comparing their values with similar systems. Results show that expanding the corpus has a major positive e ect in BLEU and Meteor scores and use of additional corpora (in-domain and out-of-domain) in training language model does not result in significantly di erent performance. The scores obtained, combined with their comparison with other systems performance and informal evaluation by humans of the sentences produced, give additional support for the capabilities of the translation based approach for fast development of data-to-text for new domains.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and development of Medication Assistant: older adults centred design to go beyond simple medication reminders",
        "doc_scopus_id": "84978174530",
        "doc_doi": "10.1007/s10209-016-0487-7",
        "doc_eid": "2-s2.0-84978174530",
        "doc_date": "2017-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Design and Development",
            "Easy-to-use products",
            "Interaction design",
            "Iterative development",
            "Medication management",
            "Medication non-adherence",
            "Mobile applications",
            "Older adults"
        ],
        "doc_abstract": "© 2016, Springer-Verlag Berlin Heidelberg.Older adults have much to gain from bringing technology into their daily lives. The extent to which this is possible strongly depends on careful design and accessible, easy-to-use products, developed using an older adults centred methodology. This paper follows this design approach and puts it to the test in developing “Medication Assistant”, an application aimed to contribute to lower the high levels of non-adherence to medication in the ageing population. This application is developed following an iterative method centred on the older adults and interaction design. The method repeats short development cycles encompassing the definition of scenarios and goals, requirements engineering, design, prototyping and evaluation by the target users. The evaluation of the increasingly refined prototypes is of paramount importance in this methodology, gathering information about the strengths and weaknesses of the application. These, along with user suggestions, constitute an important starting point to support further improvements in the subsequent development cycle. The first three development cycles for “Medication Assistant” are presented, highlighting the main aspects of each stage, and how the evaluation performed, at the end of each cycle, provided feedback to further refine the application with new and improved features. At its current stage, “Medication Assistant” obtained very positive evaluation outcomes and already provides a set of useful features concerning medication management. These features go beyond the typical medication reminders and aim to provide a first contribution towards a more holistic approach to medication non-adherence.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preface",
        "doc_scopus_id": "85101170948",
        "doc_doi": "10.2307/j.ctv6gqqkx.3",
        "doc_eid": "2-s2.0-85101170948",
        "doc_date": "2017-01-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SSI Modalities II: Articulation and Its Consequences",
        "doc_scopus_id": "85101170935",
        "doc_doi": "10.1007/978-3-319-40174-4_3",
        "doc_eid": "2-s2.0-85101170935",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Muscular activities",
            "Silent speech interfaces",
            "Speech production",
            "Speech sounds",
            "Visual aspects",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2017, The Author(s).Brain and muscular activity originates the change in shape or position of articulators such as the tongue or lips and, as a consequence, the vocal tract assumes different configurations. Most of these changes, namely of articulators and tract, are internal and are not easy to measure but, in some cases, like the lips or the tongue tip, such changes are visible or have visible effects. Even without the production of speech sound, these different configurations of articulators provide valuable information that can be used in the context of silent speech interfaces (SSIs). In this chapter, the reader finds an overview of the technologies used to assess articulatory and visual aspects of speech production and how researchers have exploited their capabilities for the development of silent speech interfaces.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SSI Modalities I: Behind the Scenes—From the Brain to the Muscles",
        "doc_scopus_id": "85101161211",
        "doc_doi": "10.1007/978-3-319-40174-4_2",
        "doc_eid": "2-s2.0-85101161211",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Direct use",
            "Muscular activities",
            "Myoelectric signals",
            "Sensorimotor cortex",
            "Silent speech",
            "Silent speech interfaces",
            "Speech production"
        ],
        "doc_abstract": "© 2017, The Author(s).Silent speech approaches can profit not only from an understanding of the brain and motor stages associated with speech production, but also from a direct use of the information coming from these stages. Therefore, in this chapter, readers can find a short overview of recent work regarding the study of brain and muscular activity related to speech production, and the use of such knowledge to serve silent speech interfaces (SSIs). In this context, the chapter takes in consideration the importance of understanding the sensorimotor cortex’s role in speech production and the application of such knowledge in the context of brain–computer interfaces (BCI). Regarding muscular activity, the concept of myoelectric signal is introduced and the literature surveyed on the technologies used to measure it. For each of the mentioned speech production stages, recent accomplishments in the domain of silent speech interfaces are also covered.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Introduction",
        "doc_scopus_id": "85101116561",
        "doc_doi": "10.1007/978-3-319-40174-4_1",
        "doc_eid": "2-s2.0-85101116561",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Acoustic signals",
            "Background noise",
            "Complex Processes",
            "Computer interaction",
            "Different stages",
            "Muscular activities",
            "Silent speech interfaces",
            "Speech production"
        ],
        "doc_abstract": "© 2017, The Author(s).The concept of silent speech, when applied to human–computer interaction (HCI), describes a system that allows for speech communication between humans and machines in the absence of an audible acoustic signal. This type of system can be used as an input HCI modality in high-background-noise environments such as in living rooms, or in aiding speech-impaired individuals. The audible acoustic signal is, in fact, just the end result of the complex process of speech production, which starts at the brain, triggers relevant muscular activity, and results in movements of the articulators. It is this information that silent speech interfaces (SSIs) strive to harness and, in this context, understanding the different stages of speech production is of major importance. In this chapter, the reader finds a brief introduction into the historical context for the rising interest in silent speech, followed by an overview on the different stages involved in speech production. Along the way, we establish a correspondence between the natural speech production process and the technology, which will be further discussed in the following chapters, leading to the existing silent speech interface (SSI) systems. Additionally, we identify overall challenges in the development of SSI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Conclusions",
        "doc_scopus_id": "85101108170",
        "doc_doi": "10.1007/978-3-319-40174-4_6",
        "doc_eid": "2-s2.0-85101108170",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Daily lives",
            "Enabling technologies",
            "Silent speech interfaces"
        ],
        "doc_abstract": "© 2017, The Author(s).In this book, we covered silent speech interfaces (SSIs), from the core principles and motivations that have fostered research on the topic, up to the enabling technologies that serve its development and the methods applied to analyze and process the data they provide. We have realized that research in silent speech interfaces is just at its infancy and evolving at a fast pace, boosted by novel technologies and views from a growing multidisciplinary community. In this chapter, the reader may find a reflection about upcoming steps and the future of SSI systems. After summarizing the topics addressed in this book, we discuss our views regarding the future of research in this area, a future brimming with challenges, but also with unquestionable potential for SSI to become a technological asset on our daily lives.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Application Examples",
        "doc_scopus_id": "85101086423",
        "doc_doi": "10.1007/978-3-319-40174-4_5",
        "doc_eid": "2-s2.0-85101086423",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Multi-modal",
            "Silent speech interfaces",
            "Surface electromyography",
            "Technical aspects",
            "Visual speech recognition"
        ],
        "doc_abstract": "© 2017, The Author(s).In the previous chapters, we covered the main concepts and technologies behind silent speech interfaces (SSIs). While that material provides the reader with the necessary knowledge to understand the different concepts and proposed solutions, the technical aspects behind designing and developing interactive SSI systems are still a challenge given that they involve articulating different technologies and methods. In this chapter, we provide some examples to allow the reader to go from theory to practice. We start with a tutorial of a basic visual speech recognition system, using accessible hardware and resources. Then, we describe a more complex practical example that shows how to leverage the multimodal SSI concept introduced in Chap. 4. With this illustration, the reader has the opportunity to assess, hands-on, the capabilities of surface electromyography sensors. The last part of the chapter describes the creation of an SSI system that handles live data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Combining Modalities: Multimodal SSI",
        "doc_scopus_id": "85101060727",
        "doc_doi": "10.1007/978-3-319-40174-4_4",
        "doc_eid": "2-s2.0-85101060727",
        "doc_date": "2017-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Baseline methods",
            "Different stages",
            "Information concerning",
            "Muscular activities",
            "Silent speech interfaces",
            "Speech production",
            "Ultrasonic doppler",
            "Visual speech features"
        ],
        "doc_abstract": "© 2017, The Author(s).In previous chapters, we have seen how various silent speech interface (SSI) modalities gather information concerning the different stages of speech production, covering brain and muscular activity, articulation, acoustics, and visual speech features. In this chapter, the reader is introduced to the combination of different modalities, not only to drive silent speech interfaces, but also to further enhance the understanding regarding emerging and promising modalities, e.g., ultrasonic Doppler. This approach poses several challenges dealing with the acquisition, synchronization, processing and analysis of the multimodal data. These challenges lead the authors to propose a framework to support research on multimodal silent speech interfaces (SSIs) and to provide concrete examples of its practical application, considering several of the SSI modalities covered in previous chapters. For each example, we propose baseline methods for comparison with the collected data.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An anthropomorphic perspective for audiovisual speech synthesis",
        "doc_scopus_id": "85051795847",
        "doc_doi": "10.5220/0006150201630172",
        "doc_eid": "2-s2.0-85051795847",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Audio-visual speech",
            "Complex services",
            "Conceptual frameworks",
            "Data-driven methods",
            "Lip movements",
            "Multi-Modal Interactions",
            "Research and development",
            "Speech production"
        ],
        "doc_abstract": "Copyright © 2017 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.In speech communication, both the auditory and visual streams play an important role, ensuring both a certain level of redundancy (e.g., lip movement) and transmission of complementary information (e.g., to emphasize a word). The common current approach to audiovisual speech synthesis, generally based on data-driven methods, yields good results, but relies on models controlled by parameters that do not relate with how humans do it, being hard to interpret and adding little to our understanding of the human speech production apparatus. Modelling the actual system, adopting an anthropomorphic perspective would provide a myriad of novel research paths. This article proposes a conceptual framework to support research and development of an articulatory-based audiovisual speech synthesis system. The core idea is that the speech production system is modelled to produce articulatory parameters with anthropomorphic meaning (e.g., lip opening) driving the synthesis of both the auditory and visual streams. A first instantiation of the framework for European Portuguese illustrates its viability and constitutes an important tool for research in speech production and the deployment of audiovisual speech synthesis in multimodal interaction scenarios, of the utmost relevance for the current and future complex services and applications.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A question and answer system for factual queries in Portuguese on DBPEDIA",
        "doc_scopus_id": "85041241044",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85041241044",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Knowledge base",
            "Knowledge basis",
            "Native language",
            "Natural language queries",
            "Natural languages",
            "Portuguese",
            "Question and answer system",
            "Question Answering"
        ],
        "doc_abstract": "© 2017.The creation of generic natural language query and answering (QA) systems is an active goal of the Semantic Web since it would allow people to conduct any query using their native language. Current solutions already handle factual questions mostly in English. The aim of this work was to develop a QA system to query knowledge bases (KB) such as DBpedia using factual questions in Portuguese. This involves representing Portuguese queries in terms of the KB ontology using SPARQL. The process of constructing a SPARQL query representing the natural language input involves determining: (1) the type of answer that is being sought - a person, a place, etc. - which is done by looking at the Portuguese equivalent of the Wh- words; (2) the main topic of the question - which person, place, etc. - obtained by morphosyntactic analysis to discover the potential subjects of the question; and (3) the properties that can be mapped to the KB ontology for creating a SPARQL query as precise as possible. The system was tested by with 22 questions without guarantee that the answer was in the KB. The correctness of the answers was verified, and was verified if the answer existed in the KB when the system did not produced results. A correct answer was produced in 67% of times that the answer existed in the KB, and for the remaining the system failed to produce an answer. Results show that this approach is promising and further investigation should be done to improve the system. The robustness observed fosters future work to expand the system to answer questions of other types.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Critical articulators identification from RT-MRI of the vocal tract",
        "doc_scopus_id": "85039167406",
        "doc_doi": "10.21437/Interspeech.2017-742",
        "doc_eid": "2-s2.0-85039167406",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Critical articulators",
            "Data-driven approach",
            "Dynamic aspects",
            "Quantitative assessments",
            "Real time",
            "Speech production",
            "Vocal-tracts"
        ],
        "doc_abstract": "Copyright © 2017 ISCA.Several technologies, such as electromagnetic midsagittal articulography (EMA) or real-time magnetic resonance (RTMRI), enable studying the static and dynamic aspects of speech production. The resulting knowledge can, in turn, inform the improvement of speech production models, e.g., for articulatory speech synthesis, by enabling the identification of which articulators and gestures are involved in producing specific sounds. The amount of data available from these technologies, and the need for a systematic quantitative assessment, advise tackling these matters through data-driven approaches, preferably unsupervised, since annotated data is scarce. In this context, a method for statistical identification of critical articulators has been proposed, in the literature, and successfully applied to EMA data. However, the many differences regarding the data available from other technologies, such as RT-MRI, and language-specific aspects create a challenging setting for its direct and wider applicability. In this article, we address the steps needed to extend the applicability of the proposed statistical analyses, initially applied to EMA, to an existing RT-MRI corpus and test it for a different language, European Portuguese. The obtained results, for three speakers, and considering 33 phonemes, provide phonologically meaningful critical articulator outcomes and show evidence of the applicability of the method to RT-MRI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Silent speech interaction for ambient assisted living scenarios",
        "doc_scopus_id": "85025166995",
        "doc_doi": "10.1007/978-3-319-58530-7_29",
        "doc_eid": "2-s2.0-85025166995",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Ambient assisted living (AAL)",
            "Automatic recognition",
            "Elderly",
            "Silent speech interfaces",
            "Speech recognition performance",
            "System accuracy",
            "Visual speech recognition"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.In many Ambient Assisted Living (AAL) contexts, the speech signal cannot be used or speech recognition performance is highly affected due to ambient noise from televisions or music players. Trying to address these difficulties resulted in the exploration of Silent Speech interfaces (SSI), making use of other means to obtain information regarding what the user is uttering, even when no acoustic speech signal is produced. The automatic recognition of what has been said, based only on images of the face, is the purpose of Visual Speech Recognition (VSR) systems, a type of SSI. However, despite the potential of VSR for enabling the interaction of older adults with new AAL applications, and current advances in SSI technologies, no real VSR application can be found in the literature. Based on recent work in SSI, for European Portuguese, a first working application of VSR targeting older adults is presented along with and results from an initial evaluation. The system performed well, enabling real-time control of a media player with an accuracy of 81.3% and performing classification in around 1.3 s. At this stage, the results vary from speaker to speaker and the system performs better if the words are correctly articulated. The effect of distance of the speaker to the video apparatus (a Kinect One) proved not to be an issue in terms of the system accuracy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "“Tell your day”: Developing multimodal interaction applications for children with ASD",
        "doc_scopus_id": "85025140611",
        "doc_doi": "10.1007/978-3-319-58706-6_43",
        "doc_eid": "2-s2.0-85025140611",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autism spectrum disorders",
            "Children",
            "Design and Development",
            "Information exchanges",
            "Multi-Modal Interactions",
            "Multi-modality"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.The development of applications for children, and particularly for those diagnosed with autism spectrum disorders (ASD), is a challenging task. In this context, careful consideration of the characteristics of these users, along with those of different stakeholders, such as parents and teachers, is essential. Also, it is important to provide different ways of using applications through multimodal interaction, in order to adapt, as much as possible, to the users’ needs, capabilities and preferences. Providing multimodality does not mean that users will interact multimodally, but provides freedom of choice to the user. Additionally, enabling multiple forms of interaction might also help understanding what actually works better, for an audience that is not always able to express an opinion regarding what might work. In this article, we take on previous work regarding the definition of a Persona for a child diagnosed with ASD and, considering the goals above, propose and evaluate a first prototype of an application targeting the audience represented by this Persona. This application, aims to serve as a place for communication and information exchange among the child, her family, and teachers and supports multimodal interaction.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Services &amp; Products Gamified Design (SPGD) a methodology for game thinking design",
        "doc_scopus_id": "85018263752",
        "doc_doi": "10.1145/3019943.3019953",
        "doc_eid": "2-s2.0-85018263752",
        "doc_date": "2016-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Development and testing",
            "Emotions and personality",
            "Gamification",
            "Interdisciplinary activities",
            "Iterative development",
            "Probability of success",
            "User-centered development",
            "User-centered services"
        ],
        "doc_abstract": "© 2016 ACM.Successful services or products have in common being well suited to users' needs and expectations, having high usability, and generating positive feelings towards its usage. The probability of success is maximized when such concerns are taken into consideration from the very beginning and users take center stage in the development process, and continue throughout the development and testing stages. Centering in the users also increases inclusion. Furthermore, design is an interdisciplinary activity conducted by people with distinct backgrounds. It is a challenging task to build cohesive design teams working seamlessly towards a common goal. For these reasons is important to develop consistent methodologies that are able to put users at the center of development, and use information on their emotional states and personality. Such methodologies are being used successfully in the computer game industry, where the process of user-centered development is a natural concept to achieve success. This work proposes a gamified methodology named Services & Products Gamified Design (SPGD) that can be used to design any user centered service or product. The proposal is based on a card game that offers a low cost solution to easily manipulate information and allows a complete understanding of the target objectives regardless of the designers' background. SPGD provides an innovative way to create a highly communicative environment based on game design by treating users holistically, including their needs, emotions and personality, and putting them at center of the design process. Treating the users holistically and putting them at the center of the project, the SGPD has strong potential for application in services and products for persons with special needs and/or disabilities.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Effects of language experience on the discrimination of the Portuguese palatal lateral by nonnative listeners",
        "doc_scopus_id": "84961615782",
        "doc_doi": "10.3109/02699206.2016.1152508",
        "doc_eid": "2-s2.0-84961615782",
        "doc_date": "2016-08-02",
        "doc_type": "Note",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Speech and Hearing",
                "area_abbreviation": "HEAL",
                "area_code": "3616"
            }
        ],
        "doc_keywords": [
            "Discrimination (Psychology)",
            "Female",
            "Humans",
            "Language",
            "Learning",
            "Male",
            "Multilingualism",
            "Phonetics",
            "Speech Perception",
            "Young Adult"
        ],
        "doc_abstract": "© 2016 Taylor & Francis.The purpose of this study was to investigate (1) whether manner or place takes precedence over the other during a phonological category discrimination task and (2) whether this pattern of precedence persists during the early stages of acquisition of the L2. In doing so, we investigated the Portuguese palatal lateral approximant /ʎ/ since it differs from English /l/ only by the place of articulation, and from English /j/ only by the manner of articulation. Our results indicate that monolinguals’ perception of the non-native sound is dominated by manner while Portuguese learners show a different pattern of results. The results are interpreted as being consistent with evidence suggesting that manner may be neurophysiologically dominant over place of articulation. The study adds further details to the literature on the effects of experience on language acquisition, and has significant clinical implications for bilingualism in general, and foreign accent training, in particular.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Technology-based innovations to foster personalized healthy lifestyles and well-being:a targeted review",
        "doc_scopus_id": "84977488419",
        "doc_doi": "10.2196/jmir.4863",
        "doc_eid": "2-s2.0-84977488419",
        "doc_date": "2016-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Computer Security",
            "Confidentiality",
            "Healthy Lifestyle",
            "Humans",
            "Telemedicine"
        ],
        "doc_abstract": "Background: New community-based arrangements and novel technologies can empower individuals to be active participants in their health maintenance, enabling people to control and self-regulate their health and wellness and make better health-and lifestyle-related decisions. Mobile sensing technology and health systems responsive to individual profiles combined with cloud computing can expand innovation for new types of interoperable services that are consumer-oriented and community-based. This could fuel a paradigm shift in the way health care can be, or should be, provided and received, while lessening the burden on exhausted health and social care systems. Objective: Our goal is to identify and discuss the main scientific and engineering challenges that need to be successfully addressed in delivering state-of-the-art, ubiquitous eHealth and mHealth services, including citizen-centered wellness management services, and reposition their role and potential within a broader context of diverse sociotechnical drivers, agents, and stakeholders. Methods: We review the state-of-the-art relevant to the development and implementation of eHealth and mHealth services in critical domains. We identify and discuss scientific, engineering, and implementation-related challenges that need to be overcome to move research, development, and the market forward. Results: Several important advances have been identified in the fields of systems for personalized health monitoring, such as smartphone platforms and intelligent ubiquitous services. Sensors embedded in smartphones and clothes are making the unobtrusive recognition of physical activity, behavior, and lifestyle possible, and thus the deployment of platforms for health assistance and citizen empowerment. Similarly, significant advances are observed in the domain of infrastructure supporting services. Still, many technical problems remain to be solved, combined with no less challenging issues related to security, privacy, trust, and organizational dynamics. Conclusions: Delivering innovative ubiquitous eHealth and mHealth services, including citizen-centered wellness and lifestyle management services, goes well beyond the development of technical solutions. For the large-scale information and communication technology-supported adoption of healthier lifestyles to take place, crucial innovations are needed in the process of making and deploying usable empowering end-user services that are trusted and user-acceptable. Such innovations require multidomain, multilevel, transdisciplinary work, grounded in theory but driven by citizens' and health care professionals' needs, expectations, and capabilities and matched by business ability to bring innovation to the market.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantitative systematic analysis of vocal tract data",
        "doc_scopus_id": "84949548344",
        "doc_doi": "10.1016/j.csl.2015.05.004",
        "doc_eid": "2-s2.0-84949548344",
        "doc_date": "2016-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Objective measure",
            "Quantitative comparison",
            "Speech production",
            "Systematic analysis",
            "Traditional approaches",
            "Visual representations",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2015 Elsevier Ltd. All rights reserved.Articulatory data can nowadays be obtained using a wide range of techniques, with a notable emphasis on imaging modalities such as ultrasound and real-time magnetic resonance, resulting in large amounts of image data. One of the major challenges posed by these large datasets concerns how they can be efficiently analysed to extract relevant information to support speech production studies. Traditional approaches, including the superposition of vocal tract profiles, provide only a qualitative characterisation of notable properties and differences. While providing valuable information, these methods are rather inefficient and inherently subjective. Therefore, analysis must evolve towards a more automated, replicable and quantitative approach. To address these issues we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative normalised data regarding differences covering meaningful regions under the influence of various articulators. An important part of the framework is the visual representation of the data, proposed to support analysis, and depicting the differences found and corresponding direction of change. The normalised nature of the computed data allows comparison among different sounds and speakers in a common representation. Representative application examples, concerning the articulatory characterisation of European Portuguese vowels, are presented to illustrate the capabilities of the proposed framework, both for static configurations and the assessment of dynamic aspects during speech production.",
        "available": true,
        "clean_text": "serial JL 272453 291210 291718 291723 291743 291782 291874 31 Computer Speech & Language COMPUTERSPEECHLANGUAGE 2015-06-03 2015-06-03 2015-12-09 2015-12-09 2015-12-09T16:48:50 S0885-2308(15)00054-6 S0885230815000546 10.1016/j.csl.2015.05.004 S300 S300.1 FULL-TEXT 2021-03-08T20:15:19.730842Z 0 0 20160301 20160331 2016 2015-06-03T05:31:18.457577Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes grantsponsor grantsponsorid highlightsabst primabst ref 0885-2308 08852308 true 36 36 C Volume 36 9 307 329 307 329 201603 March 2016 2016-03-01 2016-03-31 2016 Speech Production in Speech Technologies Edited by Karen Livescu, Frank Rudzicz, Eric Fosler-Lussier, Mark Hasegawa-Johnson and Jeff Bilmes Automated analysis of articulatory data article fla Copyright © 2015 Elsevier Ltd. All rights reserved. QUANTITATIVESYSTEMATICANALYSISVOCALTRACTDATA SILVA S 1 Introduction 1.1 Challenges 1.2 Related work 1.3 Contributions and overview 2 Vocal tract configuration comparison 2.1 Requirements and definitions 2.2 Segments and features of interest 2.3 Comparison methods 2.4 Taking advantage of repeated utterances 3 Visual representations 3.1 Parameter location 3.2 Value representation 3.3 Difference level 3.4 Overall interpretation 3.5 Directional information 3.6 Variability 4 Support for analysis of dynamic aspects 4.1 Aligning multiple utterance realisations 4.2 Dynamic analysis of the vocal tract 4.3 Dynamic data representation 5 Application examples 5.1 Application scenario 5.1.1 Data acquisition and feature extraction 5.2 Static analysis 5.2.1 Intra-speaker analysis 5.2.2 Inter-speaker comparison 5.2.3 Overall characterisation of articulatory differences 5.3 Dynamic analysis 5.3.1 Intra-sound dynamics 5.3.2 Inter-sound production differences 6 Discussion 7 Conclusions Acknowledgements References BADIN 2014 17 20 P PROCISSP COMPARISONARTICULATORYSTRATEGIESFORBILINGUALSPEAKERPRELIMINARYDATAMODELS BENITEZ 2014 701 705 A PROCINTERSPEECH REALTIMEMRIARTICULATORYSETTINGINSECONDLANGUAGESPEECH BERNDT 1994 229 248 D PROCAAAIWORKSHOPKNOWLEDGEDISCOVERYINDATABASES USINGDYNAMICTIMEWARPINGFINDPATTERNSINTIMESERIES BIRKHOLZ 2011 1422 1433 P BOERSMA 2014 P PRAATDOINGPHONETICSBYCOMPUTERCOMPUTERPROGRAMVERSION5342 BRESCH 2010 1584 1587 E PROCINTERSPEECH STATISTICALMULTISTREAMMODELINGREALTIMEMRIARTICULATORYSPEECHDATA BRESCH 2009 323 338 E CLELAND 2011 163 170 J PROCINTSEMINARSPEECHPRODUCTIONISSP COMPARINGARTICULATORYIMAGESMRIULTRASOUNDTONGUEIMAGEDATABASE DAVIDSON 2006 407 415 L DELVAUX 2002 348 352 V PROCJOURNEESDETUDESURLAPAROLE PROPRIETESACOUSTIQUESEARTICULATOIRESDESVOYELLESNASALESDUFRANCAIS ENGWALL 2003 43 48 O PROC6THINTSEMINARSPEECHPRODUCTIONISSP AREVISITAPPLICATIONMRIANALYSISSPEECHPRODUCTIONTESTINGASSUMPTIONS GICK 2004 220 233 B GREGIO 2006 F CONFIGURACAOTRATOVOCALSUPRAGLOTICONAPRODUCAODASVOGAISPORTUGUESBRASILEIRODADOSDEIMAGENSDERESSONANCIAMAGNETICASUPRAGLOTTICVOCALTRACTSHAPINGINPRODUCTIONBRAZILIANPORTUGUESEVOWELSDATAMAGNETICRESONANCEIMAGING HAGEDORN 2011 409 412 C PROCINTERSPEECH AUTOMATICANALYSISSINGLETONGEMINATECONSONANTARTICULATIONUSINGREALTIMEMAGNETICRESONANCEIMAGING HOWING 1999 59 67 F KIM 2014 EL115 EL121 J LAMMERT 2010 1572 1575 A PROCINTERSPEECH DATADRIVENANALYSISREALTIMEVOCALTRACTMRIUSINGCORRELATEDIMAGEREGIONS LAPRIE 2014 245 248 Y PROC10THINTSEMINARSPEECHPRODUCTIONISSP STUDYINGMRIACQUISITIONPROTOCOLSSUSTAINEDSOUNDSAMULTIMODALACQUISITIONSYSTEM LOVATTO 2007 549 552 L PROC16THINTCONGRESSPHONETICSCIENCESICPHS AFIBERSCOPICANALYSISNASALVOWELSINBRAZILIANPORTUGUESE MARTINS 2008 925 952 P MARTINS 2010 33 36 P PROCFALA ARTICULATORYCHARACTERISTICSEUROPEANPORTUGUESELATERALSA2D3DMRISTUDY MARTINS 2011 P PROCIADISINTLCONFERENCESCOMPUTERGRAPHICSVISUALIZATIONCOMPUTERVISIONIMAGEPROCESSING TONGUESEGMENTATIONSMRIIMAGESUSINGITKSNAPPRELIMINARYEVALUATION MARTINS 2012 231 240 P PROCIBERSPEECH2012VIIJORNADASENTECNOLOGIADELHABLAIIIIBERIANSLTECHWORKSHOP VELARMOVEMENTINEUROPEANPORTUGUESENASALVOWELS MILLER 2014 N NAM 2015 H TADATASKDYNAMICAPPLICATION NIEBERGALL 2013 477 485 A OLIVEIRA 2012 2690 2693 C PROCINTERSPEECH MRISTUDYORALARTICULATIONEUROPEANPORTUGUESENASALVOWELS OLIVEIRA 2009 480 483 C PROCINTERSPEECH SPEECHRATEEFFECTSEUROPEANPORTUGUESENASALVOWELS PRATT 2007 W DIGITALIMAGEPROCESSING PROCTOR 2010 1576 1579 M PROCINTERSPEECH RAPIDSEMIAUTOMATICSEGMENTATIONREALTIMEMAGNETICRESONANCEIMAGESFORPARAMETRICVOCALTRACTANALYSIS PROCTOR 2012 113 116 M PROCSPEECHSCITECH ARTICULATIONMANDARINSIBILANTSAMULTIPLANEREALTIMEMRISTUDY PROCTOR 2011 281 284 M PROCINTERSPEECH DIRECTESTIMATIONARTICULATORYKINEMATICSREALTIMEMAGNETICRESONANCEIMAGESEQUENCES RAMANARAYANAN 2010 1994 1997 V PROCINTERSPEECH INVESTIGATINGARTICULATORYSETTINGPAUSESREADYPOSITIONRESTUSINGREALTIMEMRI RAMANARAYANAN 2013 510 519 V SALTZMAN 1989 333 382 E SCOTT 2014 A SHADLE 2008 C PROCACOUSTICS08PARISJOINTMEETINGASAEAASOCIETEFRANCAISEDACOUSTIQUE MRISTUDYEFFECTVOWELCONTEXTENGLISHFRICATIVES SHOSTED 2012 2182 2185 R PROCINTERSPEECH USINGMAGNETICRESONANCEIMAGEPHARYNXDURINGARABICSPEECHSTATICDYNAMICASPECTS SILVA 2014 901703 S PROCSPIEVISUALIZATIONDATAANALYSIS AFRAMEWORKFORANALYSISUPPERAIRWAYREALTIMEMRISEQUENCES SILVA 2014 399 402 S PROCINTSEMINARSPEECHPRODUCTIONISSP RTMRIBASEDDYNAMICANALYSISVOCALTRACTCONFIGURATIONSPRELIMINARYWORKREGARDINGINTRAINTERSOUNDVARIABILITY SILVA 2014 403 406 S PROCINTSEMINARSPEECHPRODUCTIONISSP SYSTEMATICQUANTITATIVEANALYSISVOCALTRACTDATAINTRAINTERSPEAKERANALYSIS SILVA 2015 25 46 S SILVA 2013 459 466 S PROCICIAR2013LNCSVOL7950 SEGMENTATIONANALYSISVOCALTRACTMIDSAGITTALREALTIMEMRI SILVA 2013 1307 1311 S PROCINTERSPEECH TOWARDSASYSTEMATICQUANTITATIVEANALYSISVOCALTRACTDATA SMITH 2014 413 416 C PROCISSP COMPLEXTONGUESHAPINGINLATERALLIQUIDPRODUCTIONWITHOUTCONSTRICTIONBASEDGOALS TEIXEIRA 2012 306 317 A PROCPROPOR2012LNCSVOL7243 REALTIMEMRIFORPORTUGUESEDATABASEMETHODSAPPLICATIONS TEIXEIRA 2011 243 250 A PROC9THINTSEMINARSPEECHPRODUCTIONISSP MRISTUDYCONSONANTALCOARTICULATIONRESISTANCEINPORTUGUESE TEIXEIRA 2002 31 34 A PROCIEEEWORKSHOPSPEECHSYNTHESIS SAPWINDOWSTOWARDSAVERSATILEMODULARARTICULATORYSYNTHESIZER TEIXEIRA 1999 2557 2560 A PROCINTCONGRESSPHONETICSCIENCESICPHS INFLUENCEDYNAMICSINPERCEIVEDNATURALNESSPORTUGUESENASALVOWELS TIEDE 2000 25 28 M PROC5THSEMINARSPEECHPRODUCTIONISSP CONTRASTSINSPEECHARTICULATIONOBSERVEDINSITTINGSUPINECONDITIONS WANG 2011 Y SMOOTHINGSPLINESMETHODSAPPLICATIONS ZHARKOVA 2009 248 256 N ZHARKOVA 2011 118 140 N ZHARKOVA 2011 374 388 N ZHU 2013 1292 1296 Y PROCINTERSPEECH FASTER3DVOCALTRACTREALTIMEMRIUSINGCONSTRAINEDRECONSTRUCTION SILVAX2016X307 SILVAX2016X307X329 SILVAX2016X307XS SILVAX2016X307X329XS CHU_DOE publishAcceptedManuscriptIndexable 2016-12-08T00:00:00Z UnderEmbargo 2016-12-08T00:00:00Z item S0885-2308(15)00054-6 S0885230815000546 10.1016/j.csl.2015.05.004 272453 2020-10-25T02:43:49.897481Z 2016-03-01 2016-03-31 true 5938618 MAIN 23 56189 849 656 IMAGE-WEB-PDF 1 fx1 3233 56 219 fx2 2089 24 219 fx3 2981 69 203 fx4 1794 56 37 gr1 16527 164 129 gr10 20330 153 219 gr11 17693 75 219 gr12 13336 73 219 gr13 20999 164 192 gr14 23960 164 199 gr2 15498 123 219 gr3 21128 164 218 gr4 15474 105 219 gr5 19556 164 167 gr6 14428 145 219 gr7 26260 164 160 gr8 32878 143 219 gr9 15421 76 219 fx1 1134 19 76 fx2 2539 19 170 fx3 829 15 45 fx4 393 12 8 gr1 89059 620 489 gr10 141554 499 713 gr11 95587 243 713 gr12 80268 238 713 gr13 119153 476 558 gr14 122181 456 553 gr2 106795 411 732 gr3 83525 396 527 gr4 46248 234 489 gr5 102010 583 595 gr6 39535 261 395 gr7 226518 730 713 gr8 78945 365 558 gr9 78406 249 713 si1 152 14 16 si10 1249 40 279 si11 1773 43 306 si12 1086 38 211 si2 176 15 18 si3 924 16 280 si4 644 15 245 si5 1135 48 321 si6 701 20 251 si7 1741 30 431 si8 1143 16 343 si9 773 49 135 Samuel_Silva_quantitative_comparison_2015 5158083 Samuel_Silva_quantitative_comparison_2015_P01 524800 01 Samuel_Silva_quantitative_comparison_2015_P02 612705 02 Samuel_Silva_quantitative_comparison_2015_P03 616551 03 Samuel_Silva_quantitative_comparison_2015_P04 707321 04 Samuel_Silva_quantitative_comparison_2015_P05 733536 05 Samuel_Silva_quantitative_comparison_2015_P06 651903 06 Samuel_Silva_quantitative_comparison_2015_P07 307439 07 Samuel_Silva_quantitative_comparison_2015_P08 643667 08 Samuel_Silva_quantitative_comparison_2015_P09 541157 09 Samuel_Silva_quantitative_comparison_2015_P10 449891 10 Samuel_Silva_quantitative_comparison_2015_P11 579634 11 Samuel_Silva_quantitative_comparison_2015_P12 577739 12 Samuel_Silva_quantitative_comparison_2015_P13 610263 13 Samuel_Silva_quantitative_comparison_2015_P14 257027 14 Samuel_Silva_quantitative_comparison_2015_P15 585514 15 Samuel_Silva_quantitative_comparison_2015_P16 521898 16 Samuel_Silva_quantitative_comparison_2015_P17 591985 17 Samuel_Silva_quantitative_comparison_2015_P18 343480 18 Samuel_Silva_quantitative_comparison_2015_P19 641547 19 Samuel_Silva_quantitative_comparison_2015_P20 647612 20 Samuel_Silva_quantitative_comparison_2015_P21 511358 21 Samuel_Silva_quantitative_comparison_2015_P22 625422 22 Samuel_Silva_quantitative_comparison_2015_P23 500351 23 Samuel_Silva_quantitative_comparison_2015_P24 698632 24 Samuel_Silva_quantitative_comparison_2015_P25 613651 25 Samuel_Silva_quantitative_comparison_2015_P26 507531 26 Samuel_Silva_quantitative_comparison_2015_P27 554649 27 Samuel_Silva_quantitative_comparison_2015_P28 549451 28 Samuel_Silva_quantitative_comparison_2015_P29 595354 29 Samuel_Silva_quantitative_comparison_2015_P30 519552 30 Samuel_Silva_quantitative_comparison_2015_P31 527787 31 Samuel_Silva_quantitative_comparison_2015_P32 724763 32 Samuel_Silva_quantitative_comparison_2015_P33 691255 33 Samuel_Silva_quantitative_comparison_2015_P34 716152 34 Samuel_Silva_quantitative_comparison_2015_P35 510220 35 Samuel_Silva_quantitative_comparison_2015_P36 533221 36 Samuel_Silva_quantitative_comparison_2015_P37 541984 37 Samuel_Silva_quantitative_comparison_2015_P38 521645 38 Samuel_Silva_quantitative_comparison_2015_P39 557884 39 Samuel_Silva_quantitative_comparison_2015_P40 566140 40 Samuel_Silva_quantitative_comparison_2015_P41 184026 41 am 5211457 YCSLA 725 S0885-2308(15)00054-6 10.1016/j.csl.2015.05.004 Elsevier Ltd Fig. 1 Pipeline depicting the main steps involved in performing speech production analysis based on image data. At the centre, in more detail, the analysis step, the focus of this work. Fig. 2 Examples of contour superpositions used to assess the differences between the configurations assumed by the vocal tract to produce various sounds. Fig. 3 Illustration of the different aspects involved in computing the difference between two vocal tract profiles. Please refer to the text for additional details on how the different features are computed. Fig. 4 Comparison between two sounds taking into account existing repetitions. The contours are cross-compared between the sounds and the average difference, D ¯ , and standard deviation, σ, computed. Fig. 5 Different steps involved in building the abstract representation for comparison data: (a) different orientations are considered for the chosen features; (b) each feature is assigned an orientation over the unitary circle; (c) the value computed for each feature is represented by a dot along the corresponding orientation; (d) circular coronas are added to easy interpretation; (e) the different dots are connected by lines, forming a polygon; (f) for dots in the yellow and red regions, directional information is added; and (g) abstract representation and corresponding vocal tract contours side-by-side. Fig. 6 Aligning multiple realisations (or utterances) by normalising duration. The duration of one realisation is used as reference and the corresponding image frames interpolated to obtain a fixed set of evenly distributed frames. The second realisation is compressed/stretched to the duration of the first and a similar interpolation procedure is applied enabling common time points for comparison. Fig. 7 Extension of the comparison framework to cover vocal tract differences over time. Multiple realisations of [ e ˜ ] and [ɛ] are aligned by normalising duration. At 10 points in time the vocal tract configurations are cross-compared between vowels resulting in comparison data along the vowels’ duration. At the bottom, the comparison values obtained for each feature (e.g., velum (VEL), tongue tip (TT)) are presented, over time, using a line plot. Fig. 8 Sample images extracted from RT-MRI database acquired to study European Portuguese oral and nasal vowels and depicting the vocal tract contours. Fig. 9 Vowel production variability per speaker obtained by cross-comparing all occurrences available. From left to right: [a] (speaker CM), [ũ] (speaker CO) and [ɛ] (speaker SV). Fig. 10 Comparison between the vocal tract configurations for two vowels for each speaker. The top row shows the considered vocal tract contours and the bottom row shows the corresponding difference diagram. From left to right: [ĩ] vs [i] (speaker CM), [ũ] vs [u] (speaker CO) and [ɛ] vs [e] (speaker SV). Fig. 11 Inter-speaker comparison of differences between vowels. From left to right: [a] vs [i], [a] vs [u] and [õ] vs [O]. Fig. 12 Differences among vowel pairs computed using data from all speakers. From left to right: [a][i][u], and [ũ][u]. Fig. 13 Analysis of variation over time among utterances, along the production of [ ], for speaker CM (top) and [ũ] for all speakers (bottom). Fig. 14 Comparison between the productions of [ e ˜ ] and [ɛ] (top) and [ũ] and [ĩ] (bottom). Analysis performed considering data from all speakers. Table 1 Features considered to compare vocal tract profiles, abbreviated name and vocal tract segments used to compute them. Feature Notation Feature context (F′) Tongue back F TB Tongue back Tongue dorsum F TD Tongue dorsum Velum position F VEL Velum Tongue tip position F TT Tongue tip, alveolar ridge Lip protrusion F LP Upper lip, lower lip, alveolar ridge Lip aperture F LA Upper lip, lower lip, alveolar ridge Pharynx F Ph Pharynx ☆ This paper has been recommended for acceptance by Shrikanth Narayanan. Quantitative systematic analysis of vocal tract data Samuel Silva a b ⁎ António Teixeira a b a Dep. Electronics, Telecommunications and Informatics (DETI), University of Aveiro, Aveiro, Portugal Dep. Electronics, Telecommunications and Informatics (DETI), University of Aveiro Aveiro Portugal b Inst. Electronics Eng. and Telematics of Aveiro (IEETA), University of Aveiro, Aveiro, Portugal Inst. Electronics Eng. and Telematics of Aveiro (IEETA), University of Aveiro Aveiro Portugal ⁎ Corresponding author at: IEETA, Campus Univ. de Santiago, 3810-193 Aveiro, Portugal. Tel.: +351 234370500. Articulatory data can nowadays be obtained using a wide range of techniques, with a notable emphasis on imaging modalities such as ultrasound and real-time magnetic resonance, resulting in large amounts of image data. One of the major challenges posed by these large datasets concerns how they can be efficiently analysed to extract relevant information to support speech production studies. Traditional approaches, including the superposition of vocal tract profiles, provide only a qualitative characterisation of notable properties and differences. While providing valuable information, these methods are rather inefficient and inherently subjective. Therefore, analysis must evolve towards a more automated, replicable and quantitative approach. To address these issues we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative normalised data regarding differences covering meaningful regions under the influence of various articulators. An important part of the framework is the visual representation of the data, proposed to support analysis, and depicting the differences found and corresponding direction of change. The normalised nature of the computed data allows comparison among different sounds and speakers in a common representation. Representative application examples, concerning the articulatory characterisation of European Portuguese vowels, are presented to illustrate the capabilities of the proposed framework, both for static configurations and the assessment of dynamic aspects during speech production. Keywords Vocal tract analysis Quantitative comparison RT-MRI 1 Introduction Speech production studies are currently served by a wide range of technologies that allow research on the dynamic aspects of speech. Methods such as ultrasound (US) and real-time magnetic resonance imaging (RT-MRI) (Scott et al., 2014) provide data regarding the position and coordination of the different articulators over time (Hagedorn et al., 2011). Furthermore, they offer the possibility to improve on the studies based on information regarding a static sustained production by reducing the hyperarticulation effect (Engwall, 2003). After image acquisition, the different regions of interest must be segmented (e.g., Bresch and Narayanan, 2009; Silva and Teixeira, 2015), or points of interest identified, often resulting in contours delimiting the vocal tract or specific structures such as the tongue or velum. Analysis of different vocal tract contours is typically performed visually by characterising the position of the different articulators or by describing articulator differences between different sounds (e.g., Delvaux et al., 2002; Shadle et al., 2008). This is often done by superimposing contours and performing qualitative analysis of the main differences (Martins et al., 2008; Cleland et al., 2011; Badin et al., 2014). Adding to the subjective nature of such analysis, when the database is large, e.g., as happens when RT-MRI is used (Niebergall et al., 2013), it becomes an almost infeasible task to explore all available data. 1.1 Challenges Beyond the sheer amount of data made available by current technologies, the field of speech production faces several challenges that should be addressed to allow further advances, harnessing the full potential of the data available. A framework should be proposed that tackles the large amounts of data addressing, among others, the following aspects: • Objectivity — The subjective nature of the methods used to describe articulatory differences, for example, results in variability among researchers that precludes true comparison among works in the literature describing the same phenomena. • Intra-speaker assessment — The analysis of articulatory features for different sounds produced by one speaker lacks methods to profit from multiple repetitions and common grounds for comparison among sounds. • Inter-speaker assessment — The comparison among speakers lacks common grounds for comparison, e.g., a common normalised measure of difference, without losing sight of the contributions provided by each articulator. • Variability — Not only average behaviour is relevant for the researchers, there is also a strong need to have information on variability (across repetitions, across speakers, etc.). • Inter-language comparison — Data from multiple speakers of one language could be jointly used to provide overall quantitative characterisation of its main features. This would allow new ways of comparing sounds in multiple languages, advancing on the current status of inter-language comparisons mostly based on qualitative assessment of data from different speakers. • Multimodality — Data provided by different modalities and concerning similar phenomena or providing complementary data might benefit from joint analysis. For example, several technologies that support speech production studies (e.g., EMA, Kim et al., 2014 and ultrasound, Laprie et al., 2014) are used in combination with MRI (Scott et al., 2014). Regardless of how the different data is analysed, if their individual contributions to the understanding of specific phenomena could be gathered in joint representations it might motivate a generalisation of multimodal studies and an easier interpretation of the data. One important route to attain a systematic analysis addressing these issues is to move towards quantitative methods that allow it to be performed automatically, in an expedite and replicable way, resulting in data providing a summary of the most important features which researchers can analyse. In the work presented here we consider these challenges in the scope of real-time MRI data. 1.2 Related work Considering that we are mostly addressing vocal tract data analysis from vocal tract profiles extracted from image data (e.g., RT-MRI), in this overview of related work we focus on methods applied to image data of the vocal tract, or to full (or partial) vocal tract profiles extracted from them. The rationale is to overview notable recent literature that describes some level of quantification regarding the analysis and comparison of the data extracted from these images. Therefore, works supported on subjective analysis based, for example, on visual assessment of articulatory configuration differences, are not covered. Regarding pixel-based methods, i.e., without an explicit extraction of vocal tract profiles, notable works include the detection of constriction regions along the tract (Lammert et al., 2010; Hagedorn et al., 2011) and estimation of articulator trajectories (Proctor et al., 2011; Shosted et al., 2012) contributing to improved analysis of articulatory configurations, articulatory gestures and their coordination. Other authors have performed quantitative comparison of specific regions/articulators, the most common being the tongue. Davidson (2006) compare tongue contours using smoothing splines ANOVA (Wang, 2011). The use of Bayesian confidence intervals allows a distinction of which parts of the tongue contour are statistically different, but fails to quantify the extent of the difference between the curves (Zharkova and Hewlett, 2009). In Teixeira et al. (2011) tongue contours are compared using the Pratt index (Pratt, 2007). In the work by Zharkova et al. (2014) tongue contours are compared by computing the Euclidean distances between each point in one contour to the nearest point in the other and vice-versa and averaging the sum of all computed distances. When considering the whole vocal tract, Proctor et al. (2010) and Shosted et al. (2012), for example, have extracted distance functions, which might not be a clear way to detect which articulator has the most influence. On a different level, Miller et al. (2014) gather vocal tract configuration data (also including the cervical spine and airway), from multiple speakers, by building an active shape model (ASM). Through the identification of independent modes of variation, and selecting those that account for the most part of the explained variance, the authors study coordinated changes affecting the cervical spine, airway and vocal structures. One interesting idea to note concerns gathering the data for all speakers to reach a quantitative measure of mean behaviour and variability, which allows the characterisation of the articulatory space defined by the vocal tract configurations used for training. Comparison among vocal tract profiles might eventually be performed based on the weights of each of the modes of variation, but with the inherent difficulties in isolating the contribution of individual articulators. Furthermore, the resulting ASM model depends on the images used to train it and, therefore, a comparison cannot be performed with weights obtained for models resulting from different training sets. Ramanarayanan et al. (2010, 2013) have proposed a set of vocal tract area descriptors, analogous to the distance functions, by dividing the vocal tract in clearly defined regions considering lip and velum aperture and the constriction locations at tongue tip, dorsum and root. The authors compute the area contained in the vocal tract regions delimited by the above locations and use these area descriptors to compare among vocal tract postures (Benítez et al., 2014). While it does not exactly provide information on specific articulators it allows a slightly better grasp of overall localisation than with distance functions. On the subject of inter-speaker or inter-language comparison and regarding specific articulators, Zharkova et al. (2011) normalised measures performed over tongue contours (to compare between pre-adolescents and adults) by computing the tongue's length for specific tongue postures. To allow direct comparison among different speakers, accounting for the morphological differences, vocal tract normalisation has also been proposed by several authors. A common approach is the normalisation of vocal tract length and the presentation of distance functions (e.g., Martins et al., 2008; Proctor et al., 2010). As alluded before, the use of distance functions, even though providing a good overall idea of the differences among speakers does not explicitly address particular articulators. A normalisation of area descriptors of the vocal tract has also been performed, e.g., by Benítez et al. (2014), with advantages over distance functions, due to an association between each descriptor and a region of the vocal tract. Badin et al. (2014) use articulatory modelling to compare between classes of sounds produced by one speaker for two languages. By using data from vowels, analogous consonants and the full corpus, obtained using MRI, articulatory models are created. While this provides a quantitative method to gather the different data and characterise the articulatory space for each class of sounds, involving a selection of the variation modes accounting for a large percentage of the explained variance, the comparison is still performed by varying model parameters, in specific ranges, and visually comparing the resulting model configuration. The authors propose to evaluate how the articulatory spaces from both languages overlap by using the models built for one language to reconstruct the articulations collected for the other. Nevertheless, it is still not clear how this procedure can be used to draw any conclusion since the models are built for more than one sound and are, therefore, naturally capable of encompassing some variability. Overall, while several methods have been proposed to support quantitative analysis and comparison of vocal tract data, there are no clear approaches to systematically handling the large amounts of data available from imaging technologies, such as RT-MRI, to take advantage of multiple realisations of each sound and explicitly supporting the comparison within and among speakers and languages. 1.3 Contributions and overview The pipeline in Fig. 1 shows the main steps associated with image-based speech production studies, taking into consideration what might be at stake to address the challenges identified above, comprising: data acquisition (e.g., using RT-MRI); feature extraction such as vocal tract segmentation (e.g., Bresch and Narayanan, 2009; Silva and Teixeira, 2015); comparison of features between vocal tract profiles; and an additional step regarding computational analysis. This last step can build on the data computed for vocal tract configuration differences, apply more advanced analysis methods, such as cluster analysis, and will depend on the nature of the data made available by the preceding step. Considering the literature surveyed above, and to the best of our knowledge, no method has been proposed to support a more complete vocal tract profile comparison, providing meaningful data regarding the regions under the influence of the different articulators and addressing the different challenges posed to current speech production studies. To address these aspects, dealing with the analysis of such data, as depicted at the centre of the pipeline presented in Fig. 1, and following on work presented in (Silva et al., 2013b; Silva and Teixeira, 2014b,c), we propose a framework which, among others, provides the following notable features: • Quantitative comparison of vocal tract profiles considering the regions under the influence of different articulators. • Normalised differences allowing intra- and inter-speaker comparisons. • Analysis of dynamic aspects of speech production by gathering data from multiple speakers and considering multiple realisations for each sound or utterance. • Abstract representation to support analysis and comparison of the computed difference data. To illustrate, on a real scenario, the potential and different applications made available by the proposed methods, examples are presented dealing with data from an RT-MRI dataset collected to study the nasal vowels in European Portuguese (Teixeira et al., 2012). The remainder of this paper is organised as follows: Section 2 provides a description of the proposed methods for vocal tract comparison; Section 3 presents the abstract representation built to depict the comparison data; Section 4 extends the vocal tract comparison method to the analysis of dynamic aspects of speech production; Section 5 describes the application of the proposed methods to data gathered in an RT-MRI study involving European Portuguese (EP) oral and nasal vowels, providing a brief illustration of their application in a real scenario; Section 6 briefly discusses the work carried out; finally, Section 7 presents conclusions and ideas for further work. 2 Vocal tract configuration comparison Vocal tract comparison is traditionally performed to assess articulatory differences among sounds and speakers. While the analysis for each speaker is usually performed by superimposing vocal tract profiles, the comparison among speakers is performed by comparing these superpositions side-by-side: since the anatomy for each speaker is different it is not possible to extract meaningful data by superposing vocal tract contours from different speakers. Fig. 2 presents some examples of different pairs of superimposed contours for different speakers and sounds. Notice, for example, the subjectivity inherent to the judgement of what is a meaningful (or comparably large) difference and the difficulty to apply the same criteria when the analysis is performed by different researchers (e.g., in different studies). The framework to support quantitative analysis of differences among vocal tract profiles (centre blocks in Fig. 1) is grounded on three main blocks: • Static analysis, dealing with the comparison between vocal tract profiles deemed representative of the configurations assumed to produce different sounds (e.g., representative image frames of the vocal tract configuration for [a], [ĩ] or [u]). • Dynamic analysis, dealing with the comparison between the configurations assumed by the vocal tract along utterances and conveying the differences along time. • Visual representation, concerning the depiction of the quantitative data computed in the analysis blocks using abstract representations to support interpretation of the data and dissemination. Vocal tract comparison occupies a central role in the proposed analysis framework (Fig. 1) as it provides the grounds for all the provided analysis features. In what follows we set the requirements for a vocal tract comparison method, describe the adopted terminology, discuss a first set of considered regions of interest and define the normalised measures proposed to compare them between vocal tract profiles. 2.1 Requirements and definitions To tackle the challenges faced by speech production studies, discussed in the introduction, the vocal tract comparison method must go beyond current practice towards an objective measure of difference that can be systematically applied to the increasingly large real-time datasets. As a first set of requirements, the differences between vocal tract profiles should be: • Obtained using objective, comparable measures. • Computed for the different anatomical regions of interest and/or vocal tract features to provide a meaningful regional measure of difference. • Normalised to allow comparison among different speakers and languages. • Informative on how the difference occurred (e.g., in what direction did the tongue back move). • Represented visually to help users understand the resulting data and support results dissemination. Before advancing to a detailed definition of the comparison method, and in order to set a terminology to be used throughout the remainder of this article, let us consider each vocal tract contour, C i , defined as a set of points in 2D: (1) C i = ( x i 1 , y i 1 ) , ( x i 2 , y i 2 ) , … , ( x iM , y iM ) , where M is the number of points (x, y) in the contour and can vary among contours. For each contour, a fixed number of segments of interest, N S , can be considered (2) C i = { S i 1 , S i 2 , … , S iN S } , N S ≤ M , where each segment is defined as a subset of contour points of size 1< l s < M, as given by (3) S s = [ ( x w s , y w s ) ( x ( w s + l s ) , y ( w s + l s ) ] , w s = ∑ i = 1 s − 1 l i . Each segment of interest identifies a relevant part of the vocal tract such as the tongue, the hard palate or the degenerate case of a single point (landmark). After identification of the segments of interest, a set of N F features to consider for comparison between vocal tract profiles can be chosen. A feature can be associated to a particular segment or can rely on multiple segments for its computation. For example, lip aperture needs the segments corresponding to both lips. The feature context (F′), i.e., the relevant data that is needed to compute and compare each feature between vocal tract profiles, can then be defined based on the segments of interest. As remarked, each feature might depend on a variable number of segments: (4) F f ′ = { S ia , S ib , … } , a , b ∈ [ 1 … N S ] The difference between two contours, C i and C j , can be expressed as (5) D ( C i , C j ) = F 1 ( F i 1 ′ , F j 1 ′ ) , F 2 ( F i 2 ′ , F j 2 ′ ) , … , F N F ( F iN F ′ , F jN F ′ ) , with 0≤ Ff (A, B)≤1 denoting the normalised difference computed for feature f and defined individually according to the feature characteristics. 2.2 Segments and features of interest At this stage, our purpose was to provide a first set of features that could cover the whole vocal tract. To select the segments and features considered for comparing vocal tract profiles we considered the speech production studies literature and the relevance given to the different articulators and landmarks. Considering the vocal tract, the literature (e.g., Bresch and Narayanan, 2009) identifies multiple anatomical regions with relevance for speech production, namely: upper lip, lower lip, tongue, tongue tip, velum, hard palate, pharyngeal wall, epiglottis and glottis. Regarding relevant features extracted from the vocal tract, notable examples in the literature (Saltzman and Munhall, 1989; Gick et al., 2004; Bresch et al., 2010; Niebergall et al., 2013) include: lip protrusion; lip aperture; tongue tip constriction degree, defined by the distance from the tongue tip to the alveolar ridge; tongue dorsum constriction location and tongue dorsum constriction degree, based on the point of minimum distance between the tongue dorsum and the hard palate; tongue root constriction degree, obtained from the distance between the tongue root and the pharyngeal wall; and velar aperture. The literature regarding the characterisation of articulatory differences (typically from contours) also provides useful information regarding which aspects are the most important in vocal tract configuration characterisation: tongue height and backness, lip aperture and protrusion and velum height. Considering the different features described in the literature we aimed to choose a set that would cover the whole vocal tract and provide data regarding the aspects described and used the most, by researchers, to differentiate among vocal tract configurations. In this first instantiation of the framework, the considered features for comparison are: tongue back differences, tongue dorsum differences, velum differences, tongue tip position variation, lip protrusion variation, lip aperture variation and pharynx configuration differences. Table 1 presents a summary of the considered features, the adopted notation and segments considered as part of their feature context. The alveolar ridge is included in the context of various features because it is used for normalisation purposes, as explained ahead. The difference between two vocal tract profiles can, therefore, be expressed as a features vector: (6) D ( C i , C j ) = F TB , F VEL , F TD , F TT , F LP , F LA , F Ph It is important to make it clear that in the context of our work we are not currently dealing with intrinsic features, i.e., features that are defined only considering one vocal tract profile. All features should be understood in the context of a comparison between two vocal tract profiles. For example, tongue backness, F TB , is not an absolute measure of the tongue back position towards the pharyngeal wall, but the comparison between the position of the tongue backs in two vocal tract configurations. Please also note that the consideration of this set of features should not be understood as a restriction of the framework to it. By definition, this set is extendible to other features, as long as they abide to the requirements described in Section 2.1. The details regarding how each of the selected features is computed are provided in the following section. 2.3 Comparison methods When performing the comparison for the multiple features defined above, three different types of comparisons need to be performed: (1) between segments of the vocal tract (e.g., tongue dorsum); (2) between landmarks (e.g., tongue tip); or (3) between distances (e.g., lip aperture). Therefore, for each particular situation, a different comparison approach is needed. It should be noted that comparison measures are always computed between vocal tracts of the same speaker, considering they are already aligned, e.g., as a result of the acquisition procedure, or post acquisition alignment. Fig. 3 illustrates the different vocal tract regions that need to be compared and relevant aspects of their computation. The pharynx (F Ph ), tongue back (F TB ), tongue dorsum (F TD ) and velum (F VEL ) are part of the features that involve the comparison between contour segments. These are compared by computing the Pratt index for each pair of corresponding contours and given by Pratt (2007): P = 1 N ∑ i = 1 N 1 1 + α d i 2 , where N is the number of corresponding points between the compared contour segments (e.g., tongue back), d i is the Euclidean distance between two corresponding points, and α is a constant set to 1/9, based on Pratt's work and similar works in the literature (Pratt, 2007). At this stage, the same constant value has been used for all regions, but it might be tuned for each region if different sensibilities to differences are desired. To obtain corresponding points between segments, the segment with the smallest number of points is selected and, for each point, the closest point in the other segment is considered the corresponding point. The Pratt's figure of merit provides values in the range ]0, 1] where 1 is attained when there are no differences between the contour segments. The tongue tip position variation, F TT , is the only feature that deals with the comparison of two landmarks. Comparison is performed by computing the distance between the tongue tips (d TT ), in both contours, and normalised by the longest distance from each tongue tip to the alveolar ridge (AR) between the two contours. Fig. 3 depicts the values considered and used as follows: F TT = 1.0 − d TT max ( dA ( TT – AR ) , dB ( TT – AR ) ) . The remaining two features belong to the third type and deal with the comparison between two distances that are first computed from each of the vocal tracts. Lip protrusion, F LP , is obtained (refer to Fig. 3) by computing the horizontal displacement of the mid-point between the upper and lower lips, MPL. The mid-point is computed considering the line that connects the lowest point of the upper lip with the highest point of the lower lip. To perform normalisation, the horizontal distance between the mid-points between the lips and the alveolar ridge (AR) is obtained (dA (MPL–AR) and dB (MPL–AR)) and used as follows (Fig. 3): F LP = 1.0 − dB ( MPL – AR ) − dA ( MPL – AR ) max ( dA ( MPL – AR ) , dB ( MPL – AR ) ) . Lip aperture, F LA , is computed based on the lip aperture values for both vocal tract profiles (LA A , LA B ) and is normalised by considering the longest of the two: F LA = 1.0 − LA B − LA A max ( LA A , LA B ) . The features described above still do not provide any information regarding the direction in which the difference occurred (e.g., did the tongue tip go up or down). For each of the features the direction can be computed as the vector characterising the movement of the centre of gravity of the feature context (excluding the alveolar ridge, where present) or the difference between notable points (e.g., the midpoint between the lips in F LP . The obtained directional information is processed to provide only the component of interest. For example, for the tongue back there is no particular interest in the vertical component of displacement and the opposite happens for the tongue dorsum for which the horizontal displacement, if exists, has no major relevance. 2.4 Taking advantage of repeated utterances It is common that the corpora acquired during speech production studies include repetitions of each sound considered. Therefore, analysis should consider these repetitions when comparing between sounds by cross comparing all occurrences of each sound. Fig. 4 illustrates the comparison between two sounds. The relevant repetitions of each sound are considered and cross compared using the methods previously described. From each of these comparisons results a set of difference values covering each of the considered regions. The average values are obtained along with the standard deviation and used as representative of the comparison between the two sounds. 3 Visual representations The quantitative comparison method presented above, resulting in a set of numerical values, demands a visualisation method that can help users to interpret/compare and disseminate them in an easy way as opposed, for example, to value tables. The adoption of a visual representation for the data, to improve how it can be read, should not be understood as the ’end of the line’ for the gathered numerical data. It will serve to provide users with an insight over the numbers, but additional methods (e.g., cluster analysis) can be applied to further explore them as depicted in the pipeline of Fig. 1. One possible representation would be to present the values in a line plot, with the parameters shown in a fixed order, in the X-axis. Nevertheless, we considered that a different representation might bring easier interpretation if, somehow, one could build some analogy with the vocal tract and the position of the different articulators. Fig. 5 illustrates the construction of the proposed representation. The computed comparison values are represented over the unitary circle in a radar-like representation and its design is grounded in the ideas described in what follows. 3.1 Parameter location First of all, we consider the traditionally adopted orientation for speech production images with the lips presented to the left (e.g., Höwing et al., 1999; Ramanarayanan et al., 2013). The location of each computed parameter in the representation is chosen as if the vocal tract was inscribed in the unitary circle and selecting, at each time, the parameter concerning the closest region or articulator. Furthermore, the orientation associated with each value is, where applicable/possible, related with its movement direction (Niebergall et al., 2013). Accounting for these criteria, tongue back differences, for example, should be on the opposite side regarding lip protrusion. Starting at zero degrees, for the tongue back (F TB ), it follows the velum (F VEL ), tongue dorsum (F TD ), tongue tip (F TT ), lip protrusion (F LP ), lip aperture (F LA ) and pharynx (F Ph ). In the case of lip aperture, the direction used in the representation is not directly related with the direction of the expected difference. Notice, however, that considering the two parameters associated with the lips, F LP and F LA , the directions adopted for the pair resemble an open mouth, with F LA as the jaw. Each feature name is presented on the exterior of the circle close to its corresponding orientation. 3.2 Value representation The values obtained for each of the considered features are represented over the corresponding direction in the unitary circle using a dot. Since it is the unitary circle, with the origin at its centre, the closest the dot is to the centre, the lowest the value it represents. As the represented value gets closer to 1.0, the dot gets closer to the circumference of the unitary circle. 3.3 Difference level To help on the interpretation of the presented values, three circular coronas are proposed. The first, between 0.75 and 1.0, in green, is suggested to mean no relevant difference. The interval lower bound is chosen based on the value typically interpreted as meaning a good match between contours compared using the Pratt index. The second circular corona, between 0.50 and 0.75 is proposed to be interpreted as a mild difference and the last, from 0.0 to 0.50, as a strong difference. These are proposed interpretations and different criteria can be adopted, but their intent is to help users to have common interpretation criteria and to move away from direct numerical comparison which would lead, yet again, to a rather subjective assessment. Furthermore, the intent is also not to have too much difference levels. 3.4 Overall interpretation As a first approach, trying to provide easier analysis of the proposed representation, namely a faster perception of each point location, and a simple way of “grouping” the points belonging to the same comparison, when multiple comparison data is shown, the points are connected using line segments, forming a polygon (hereafter know as the difference polygon). This should not be interpreted as implying anatomical connection or proximity between articulators, but as a way to ease visualisation. The difference polygon also allows reducing the size of the value dots, which leaves more space in the representation for adding other features, in the future. 3.5 Directional information One important aspect regarding the differences found when comparing vocal tract profiles is to characterise how that difference occurred from one configuration to the other, e.g., by a rise in the tongue dorsum or lowering of the velum. To provide such information, a vector is used, with origin in the parameter value and the direction of displacement. For the sake of simplicity, displacement direction is only represented for values corresponding to mild or strong differences (i.e., <0.75, according to the proposed interpretation thresholds). If the difference for a particular feature is deemed irrelevant (green circular corona), then representing a direction would have no meaning. Furthermore, it helps to highlight features depicting notable differences. 3.6 Variability Since differences are often computed for multiple realisations of two sounds, and the average difference represented, it is also important to depict the variability associated with each value. To this end, several alternatives were tested such as bars and/or lines associated to each represented value, depicting data as in a boxplot, or a representation of dispersion outside the unitary circle. These tended to clutter the representation and became problematic if more that one difference polygon was represented. Therefore, to provide variability data, the standard deviation for each parameter is represented over its direction and all parameter points used to draw an orange polygon (hereafter known as the deviation polygon) at the centre of the representation (Fig. 5g). 4 Support for analysis of dynamic aspects The previous sections proposed a method to compare between vocal tract contours and an abstract representation of the gathered data to support analysis, but leaving behind the assessment of dynamic aspects of speech production. Considering the proposed quantitative comparison methods, they can be extended to assess these dynamic aspects (Silva et al., 2013b; Silva and Teixeira, 2014b). We propose a method for dynamic analysis of articulatory data jointly exploring multiple realisations of each sound and covering multiple articulators. Instead of analysing each utterance and then inferring notable features, all realisations for a particular utterance can be considered simultaneously, possibly from more than one speaker, aiming towards a characterisation of average dynamic aspects and their corresponding variation. 4.1 Aligning multiple utterance realisations Multiple realisations of the same utterance (or different utterances), identified by previously performed annotation (e.g., using Praat, Boersma and Weenink, 2014), commonly present different durations. Before performing any kind of analysis, taking advantage of multiple realisations to characterise the mean dynamic behaviour and dynamic intra- and inter-sound variability, they have to be properly matched. The method used to perform the match can account for aspects such as the duration and complexity of the utterances and adapt accordingly. For example, for small utterance durations (or reduced number of phones), time axis normalisation can be adopted, with each realisation's duration assumed to vary between 0 and 1. Other approaches, when the utterances are longer or more complex, might include Dynamic Time Warping (DTW) (Berndt and Clifford, 1994). Based on the utterance realisations matching, the vocal tract contours extracted along the utterance are interpolated in order to allow comparison of corresponding contours between the realisations. Fig. 6 shows an example of matching two realisations of [ĩ] using time axis normalisation. One of the realisations is considered as reference and the existing vocal tract contours (associated to the corresponding image frames) are interpolated to obtain 10 vocal tract configurations equally spaced along the realisation. The second realisation is assumed to last the same amount of time as the first and the existing vocal tract contours interpolated to obtain 10 configurations. 4.2 Dynamic analysis of the vocal tract After matching the different realisations for each utterance and obtaining the corresponding contours, the vocal tract comparison method presented earlier can be used to compare each contour pair along the production. Fig. 7 depicts an illustrative example for the comparison between two sounds and considering 10 comparisons along the utterances’ duration. Since several realisations are considered for each utterance there are multiple contours to consider at each time instant. For each of these comparisons the vocal tract contours of the two utterances are cross-compared (as detailed in section 2.4) and the standard deviation computed. 4.3 Dynamic data representation Depicting dynamic data by representing the difference polygon for each vocal tract profile pair along the utterances (top rows in Fig. 7) is space consuming and can be difficult to grasp the full extent of the represented data. Therefore, we propose that the dynamic data is presented differently. The representation of the data gathered for the dynamic analysis has four different parts as depicted, at the bottom, in Fig. 7. On the left, a difference diagram for the comparison among the first frames and, on the right, a difference diagram for the comparison among the last frames of the considered utterances. The dynamic aspects are depicted in the middle. A line plot, with a line for each of the considered difference parameters, is provided. The line plot shows three horizontal colour bands as the difference diagrams, to hint on possible interpretations of the obtained values: as before, while green is proposed to stand for negligible differences, yellow and red stand for mild and strong differences. Since utterance matching has been performed, the time axis shows percentages instead of any particular time range. Below the central line plot, another line plot shows the standard deviation for each of the difference parameters. If needed, more compact representations can be easily obtained, based on this first proposal, by not showing the difference diagrams for the first and last frame, only showing the standard deviation plot if it goes above a certain threshold, or just plotting the lines for features with notable variation. 5 Application examples In this section we show some examples of what can be accomplished using the framework, in its current state of development, by addressing different tasks relevant for articulatory analysis performed over an existing RT-MRI corpus for European Portuguese (EP). The main purpose is not to perform a detailed characterisation of the articulatory characteristics of EP, or to fully explore the existing corpus, but to highlight those innovative aspects provided by the proposed quantitative analysis framework. 5.1 Application scenario As subject we selected EP nasal vowels due to the availability of the data, its reported dynamic nature (Teixeira et al., 1999) and our group continued interest. The articulation of EP nasal vowels has been studied by the authors and colleagues mainly focusing on velar dynamics as provided by electromagnetic midsagittal articulography (EMMA) (Oliveira et al., 2009). To extend these studies, with a characterisation of the oral configuration of EP nasal vowels, static (Martins et al., 2008) and, more recently, real-time magnetic resonance imaging (RT-MRI) data of the vocal tract was acquired (Oliveira et al., 2012; Martins et al., 2012).This imaging modality provides adequate data regarding the position and coordination of the different articulators over time (Hagedorn et al., 2011) and might provide a good choice to tackle the hyperarticulation effect observed in sustained productions (Engwall, 2003). Additionally, it might also help reduce the gravity effect on articulators for acquisitions in supine position (Tiede and Vatikiotis-Bateson, 2000). 5.1.1 Data acquisition and feature extraction Image sequences were acquired containing: (a) the five European Portuguese (EP) nasal vowels uttered in three word positions: initial, internal and final (e.g., the nonsense words “ampa, pampa, pan” ( ); and (b) the eight EP oral vowels (e.g., “papa” or “pupa”). Acquisition was performed at the midsagittal plane of the vocal tract. The images were acquired on an unmodified 3.0 T MR scanner (Magneton Tim Trio, Siemens, Erlanger, Germany) equipped with high performance gradients (Gmax=45mT/m, rise time=0.2s, slew rate=200T/m/s, and FOV=50cm). Custom 12-channel head and 4-channel neck phased-array coils were used for data acquisition. Parallel imaging (GRAPPA 2) and magnetic field gradients operating at FAST mode were used to speed up the acquisition. After localisation images, a T1 W 2D-midsagittal MRI slice of the vocal tract was obtained, using an Ultra-Fast RF-spoiled Gradient Echo (GE) pulse sequence (Single-Shot TurboFLASH), with a slice thickness of 8mm and the following parameters: TR/TE/FA=72ms/1.02ms/5°, Bandwidth=1395Hz/pixel, FOV(mm2)=210×210, reconstruction matrix of (128×128) elements with 50% phase resolution, in-plane resolution (mm2)=3.3×1.6, yielding a frame rate of 14images/s. Typically, each recorded sequence contained 75 images (taking around 5 seconds to acquire) although some longer sequences (300 images) were also acquired, mostly with the speakers producing sequences of isolated vowels (e.g., “am, em, im, om, um, am, …” , …). Audio was recorded simultaneously with the RT images inside the MR scanner, at a sampling rate of 16,000Hz, using a fiberoptic microphone, and manually annotated using the software tool Praat (Boersma and Weenink, 2014), in order to identify the time intervals corresponding to different sounds. The time intervals allow the determination (because both data are aligned) of the corresponding image frames. Data was acquired for three female speakers (CM, CO and SV), aged between 21 and 33, phonetically trained, with no history of hearing or speech disorders. For additional details regarding the acquisition and corpus the reader is forwarded to Teixeira et al. (2012). At an early stage of our research, vocal tract segmentation for this dataset was performed using a region-growing based method (Silva et al., 2013a) with the different segments of interest identified with the help of manually positioned landmarks, for each speaker (Silva et al., 2013b). Currently, the segmentation is handled by a model-based method (Silva and Teixeira, 2015) which results in a contour with the different segments of interest already tagged. Fig. 8 shows some examples of image frames taken from the RT-MRI dataset. The examples that follow were computed gathering the data for each vowel irrespective of its context and considering the isolated vowels, which were not considered previously (Silva and Teixeira, 2014b,c). 5.2 Static analysis For the static analysis, since we are working with a real-time dataset, a representative contour of the configuration assumed by the vocal tract for each sound utterance is chosen. For example, based on previous experience with the considered dataset (Oliveira et al., 2012) we chose the middle frame for oral vowels and the final frame for nasal vowels. The different criterion was adopted since nasal vowels generally presented the lowest velum position at the end of the annotated segment. Nevertheless, any other criterion for choosing the vocal tract contours is possible since it does not influence how the differences are computed, but only how the resulting data is interpreted. 5.2.1 Intra-speaker analysis As already noted, multiple realisations of each vowel, produced by the same speaker, can be cross compared. This allows the assessment of the variability associated with the production of each vowel, for each speaker. Fig. 9 presents some representative examples of the differences obtained for several vowels and speakers. In general, for the studied vowels, no major intra-vowel differences were found. Quantitative analysis can also be performed to compare among vocal tract configurations for different sounds and for the same speaker. In EP, such comparisons are useful, for example, to study oral vs nasal vowels. Fig. 10 depicts comparisons among different vowel pairs. The corresponding contour superpositions are also presented to allow observing how the computed data and abstract representation relate with the traditional visual assessment. To note, the clearly depicted differences: (1) at the velum between [ĩ] and [i] and [ũ] and [u]; (2) in lip aperture, for all comparisons; and (3) at the tongue tip and tongue back, between [ɛ] and [e]. Regarding the directional information, note that it should be read as an answer to the question “In which direction did the change occur from the first to the second vocal tract configuration?”. 5.2.2 Inter-speaker comparison One important feature provided by the proposed framework is the comparison among speakers. Since the provided comparison values are normalised, it is possible to visualise the differences for various speakers on a common representation. The difference polygons for each speaker and for each vowel pair were superimposed to assess inter-speaker differences (see Fig. 11 ). In general, the difference polygons for all speakers, for the same vowel pair, were similar. Notable inter-speaker differences were sometimes observed for lip aperture: notice, for example, for the comparison between [a] and [i], that speaker SV presents a mild difference for lip aperture. 5.2.3 Overall characterisation of articulatory differences The data gathered for all speakers can now be used to assess the overall articulatory differences between vowels. Observing Fig. 12 , a notable aspect, evidenced by the standard deviation, is that, overall, the strongest variability among the different productions is mostly observed at the tongue tip and lip aperture (protruding vertices of the orange polygon at the centre). 5.3 Dynamic analysis Considering the available data, three different kinds of dynamic analysis can be performed, covering the different articulators: • Intra-sound dynamics, i.e., how does the vocal tract vary from the beginning to the end of the production and how consistent it is across productions. For example, cross compare all utterances of [i]. • Inter-sound comparison, i.e., compare the production of two different sounds along time to check differences/similarities. For example, vowel [o] and its nasal congener [õ]. • Comparison, over time, with a reference, i.e., analyse production of a sound using a particular (static) vocal tract configuration, as reference, to check how similar (and when) is the vocal tract configuration along the production to other relevant configurations (e.g., [ e ˜ ] compared to representative (static) configuration of [ɛ]). 5.3.1 Intra-sound dynamics All utterances of a particular vowel are matched and cross compared. The resulting comparison data for each considered frame is averaged and represented in the line plot to depict evolution over time. Fig. 13 shows, as representative examples, variation data for vowels [ ] and [ũ]. For [ ] only data from speaker CM was considered, but since all utterances can be matched and the vocal tract comparison method provides normalised data, the presented dynamic analysis for vowel [ũ] was performed taking into account the data for all three speakers in the RT-MRI dataset. Visualisations made possible by the proposed method show no major discrepancy of vocal tract configuration across utterances, for both vowels, i.e., all utterances exhibit a similar vocal tract variation pattern: a slight difference is observed for the lip aperture (LA), in vowel [ ], at the beginning and end of the production, but the remaining parameters keep above 0.75 with low standard deviations. 5.3.2 Inter-sound production differences Fig. 14 shows two examples regarding the assessment of dynamic differences between sounds. In the first example (top row), the production of [ e ˜ ] was compared with the production of [ɛ]: both start very similar with only a slight difference at tongue tip and gradually differ at lip aperture and velum. This gradual variation at the velum is in agreement with previous studies (Gregio, 2006; Lovatto et al., 2007; Martins et al., 2012; Oliveira et al., 2012) describing different phases in the production of nasal vowels. The second example (Fig. 14, bottom row) shows the comparison between vowels [ũ] and [ĩ]. As expected there are strong differences at the tongue tip (TT), tongue back (TB) and tongue dorsum (TD). Nevertheless, note how no substantial difference exists at the velum (VEL) between the two sounds. Since both are nasal vowels, the velum is expected to gradually open along the production (Martins et al., 2012). The dynamic analysis shows that both sounds exhibit a similar velum aperture pattern. 6 Discussion Taking into consideration the challenges identified in the introduction, we consider that the proposed framework provides important contributions: addresses subjectivity by proposing quantitative methods for comparing vocal tract profiles; allows comparing data among speakers due to the normalised nature of the computed differences; makes it possible to gather the data from different realisations of each sound in order to compute variability; and supports gathering data from all available speakers to depict overall characterisation of articulatory differences, e.g., for a particular language. The application examples presented show how the proposed methods can be useful to tackle the analysis of large datasets and provide data enabling the characterisation of the articulatory differences between different sounds both considering static and dynamic analysis. The only challenge that we do not explicitly address, from those pointed in the introduction, is multimodality. It is our understanding that the defined framework is general enough to allow its extension to encompass data from other modalities. Nevertheless, it is important to note that this extension poses interesting new challenges such as how to deal with the different number of features covered by each modality (e.g., the tongue in US and the whole vocal tract in MRI) or how to consider the contributions of each modality to the presented average data. In a systematic, unsupervised approach to the analysis of articulatory data (e.g., considering our pipeline in Fig. 1), it is possible that, due to image noise or artefacts, the quality of the unsupervised vocal tract segmentation (Silva and Teixeira, 2015) might sometimes be compromised (Ramanarayanan et al., 2013). The inclusion of such segmentations in the analysis (albeit in small number) might eventually affect the results and the pipeline would profit from methods to filter the data or signal these problematic cases for further analysis. One possible approach, a relevant additional application made possible by the proposed framework, might be the detection of outliers based on intra-sound comparisons. The features proposed for this first instantiation of the framework, and the methods used to compute them, are a first proposal and can be subject to changes and improvements. For example, comparison of regions using the Pratt index is performed by determining the corresponding point by a proximity criterion. This might be improved by searching for the corresponding points along the expected principal direction of movement, e.g., radial, for tongue dorsum. Regarding the velum, if the available vocal tract contours allow analysis of velar aperture, adding to the velum position (e.g., Silva and Teixeira, 2015), this data can be considered for the computation of F VEL . Nevertheless, despite the possible improvements and changes to the methods used to compute each feature, it is important to note that the proposed framework allows systematic and objective application of an uniform comparison criteria throughout the compared data. Furthermore, the proposed comparison features are not to be understood as a closed set. For this first instantiation of the framework, we chose a set of features that, overall, cover the different articulators, but new features (e.g., Saltzman and Munhall, 1989; Smith, 2014) can be added to better serve the study of specific sounds. For example, considering features that deal with constriction degree and location would allow the study of consonantal constriction. One of the aspects we consider relevant for the framework is the possibility to depict the comparison outcomes in such a way that it is easily interpreted by researchers. The abstract visual representations are a first proposal to help gather insight over the computed comparison values and, even though simple in nature, serve their main purpose well, as demonstrated in the application examples. Nevertheless, these representations present some limitations. When multiple difference polygons are represented and, hence, multiple polygons depicting the corresponding standard deviations, there is overlap among the polygons that might hinder the interpretation: matching between the difference polygons and the corresponding standard deviations is not possible and the larger polygon might cover the remaining polygons, if drawn last. Nonetheless, this already provides an idea of the worst case scenario. We have been gathering feedback from different researchers concerning the proposed representations, a first informal evaluation was presented in Silva and Teixeira (2014a) and we intend to further explore user inputs. It is important to note that using the proposed framework results in a set of quantitative difference data that can be further explored for computational analysis. The proposal of the visual representations should not be understood as the final stage for the computed comparison data. Although not explored in this article, methods such as clustering can be tested to study the proximity between sounds, speakers or languages and the importance of particular features in differencing between sounds. 7 Conclusions This article proposes a quantitative framework for vocal tract profile comparison. To the best of our knowledge, this is the first time a quantitative analysis framework is presented encompassing the assessment of per sound intra-speaker variability, inter-speaker differences and overall inter-sound differences using data from all speakers in a single representation. The presented application examples show the potential of the proposed framework to take advantage of the increasing amounts of articulatory data available and move towards quantitative inter-speaker and inter-language comparison, both for static and dynamic analysis scenarios. The application of the proposed methods to other classes of sounds (e.g., laterals, Martins et al., 2010) and the study (and inclusion in the proposed visualisations) of landmark trajectories and their variability, considering all available speakers, should provide further insight into the usefulness of the proposed methods. Adding to the possible improvements already mentioned in the discussion, there are a few other lines of work that deserve further attention. Grounded on the principles and methods encompassed by the proposed framework, promoting systematic analysis of the available vocal tract data, the presented work can evolve to provide support to the use of high-level models of articulator organisation and control (e.g., TAsk Dynamics Application (TADA), Nam et al., 2015). Regarding the identification of relevant gestures, the analysis based on multiple realisations of each sound can already be helpful, and the instantiation of the framework considering other features (e.g., tongue body and tip constriction location and degree) can bring the outcomes closer to the variables required by the models. The aspect that needs to be further developed is the computation of temporal aspects that, despite being present in the dynamic analysis, is not explicitly addressed. In this regard, the long-term goal is the generation of gestural scores, from the data, that can serve as input, for example, to articulatory synthesisers (e.g., Teixeira et al., 2002; Birkholz et al., 2011) The use of multi-planar (e.g., Proctor et al., 2012) or 3D imaging (e.g., Martins et al., 2011; Zhu et al., 2013) of the vocal tract assumes importance for studying sounds exhibiting important characteristics not observable in the sagittal plane (Zhu et al., 2013). While the work presented here does not explicitly address the application of the framework to these kind of MRI data, there is no impediment to it, as long as the new features comply with the requirements set in section 2 and analysis follows the systematic procedure inherent to the framework. Instead of one midsagittal vocal tract contour, several contours can be considered, in different planes of interest. For instance, regarding lateral sounds (e.g., /l/), a comparison feature might be used accounting for the asymmetric nature of the lateral channels forming on the tongue sides (Martins et al., 2010), based on their section area, in the coronal plane, and their length. One route we are considering for further development of the proposed framework, addressing the multimodality challenge discussed earlier, is its use with data from different/multiple imaging modalities. We are currently considering the analysis of RT-MRI and ultrasound data simultaneously. The presented methods (and envisaged steps regarding computational analysis) are computationally demanding and generate large amounts of data. Therefore, their deployment in a cloud environment would provide a more suitable scenario for further developments and an important first step towards their validation and use by third parties such as phoneticians. The use of the proposed framework and its developments by other research groups, over their data, would pave the way for comparisons between dialects and languages. We are currently starting this migration process in the scope of projects Cloud Thinking 1 1 and IRIS. 2 2 www.microsoft.com/pt-pt/mldc/iris/default.aspx. Acknowledgements The authors thank the anonymous reviewers for their helpful comments and suggestions. Research partially funded by FEDER through the Program COMPETE and by National Funds (FCT) in the context of HERON II (PTDC/EEA-PLP/098298/2008), Project Marie Curie IAPP “IRIS” (FP7-PEOPLE-2013-IAPP, ref. 610986) and project Cloud Thinking (QREN Mais Centro, ref. CENTRO-07-ST24-FEDER-002031). References Badin et al., 2014 P. Badin T.R. Sawallis S. Crépel L. Lamalle Comparison of articulatory strategies for bilingual speaker: preliminary data and models Proc. ISSP Germany 2014 17 20 Benítez et al., 2014 A. Benítez V. Ramanarayanan L. Goldstein S. Narayanan Real-time MRI of articulatory setting in second language speech Proc. Interspeech Singapore 2014 701 705 Berndt and Clifford, 1994 D. Berndt J. Clifford Using dynamic time warping to find patterns in time series Proc. AAAI Workshop on Knowledge Discovery in Databases 1994 229 248 Birkholz et al., 2011 P. Birkholz B. Kroger C. Neuschaefer-Rube Model-based reproduction of articulatory trajectories for consonant-vowel sequences IEEE Trans. Audio Speech Lang. Process. 19 2011 1422 1433 10.1109/TASL.2010.2091632 Boersma and Weenink, 2014 P. Boersma D. Weenink Praat: Doing Phonetics by Computer [Computer Program]. Version 5.3.42 2014 Bresch et al., 2010 E. Bresch A. Katsamanis L. Goldstein S. Narayanan Statistical multi-stream modeling of real-time MRI articulatory speech data Proc. Interspeech Makuhari, Japan 2010 1584 1587 Bresch and Narayanan, 2009 E. Bresch S. Narayanan Region segmentation in the frequency domain applied to upper airway real-time magnetic resonance images IEEE Trans. Med. Imaging 28 2009 323 338 10.1109/TMI.2008.928920 Cleland et al., 2011 J. Cleland A.A. Wrench J.M. Scobbie S. Semple Comparing articulatory images: an MRI ultrasound tongue image database Proc. Int. Seminar on Speech Production (ISSP) Montreal, Canada 2011 163 170 Davidson, 2006 L. Davidson Comparing tongue shapes from ultrasound imaging using smoothing spline analysis of variance J. Acoust. Soc. Am. 120 2006 407 415 Delvaux et al., 2002 V. Delvaux T. Metens A. Soquet Propriétés acoustiques e articulatoires des voyelles nasales du Français Proc. Journées d’Étude sur la Parole Nancy 2002 348 352 Engwall, 2003 O. Engwall A revisit to the application of MRI to the analysis of speech production – testing our assumptions Proc. 6th Int. Seminar on Speech Production (ISSP) Sydney, Australia 2003 43 48 Gick et al., 2004 B. Gick I. Wilson K. Koch C. Cook Language-specific articulatory settings: evidence from inter-utterance rest position Phonetica 61 2004 220 233 Gregio, 2006 F.N. Gregio Configuração do trato vocal supraglótico na produção das vogais do Português Brasileiro: dados de imagens de ressonância magnética [Supraglottic vocal tract shaping in the production of Brazilian Portuguese vowels: data from magnetic resonance imaging] 2006 PUC/SP (Master thesis) Hagedorn et al., 2011 C. Hagedorn M.I. Proctor L. Goldstein Automatic analysis of singleton and geminate consonant articulation using real-time magnetic resonance imaging Proc. Interspeech Florence, Italy 2011 409 412 Höwing et al., 1999 F. Höwing S. Dooley D. Wermser Tracking of non-rigid articulatory organs in X-ray image sequences Comput. Med. Imaging Graph. 23 1999 59 67 Kim et al., 2014 J. Kim A. Lammert P. Kumar Ghosh S. Narayanan Co-registration of speech production datasets from electromagnetic articulography and real-time magnetic resonance imaging J. Acoust. Soc. Am. 135 2014 EL115 EL121 10.1121/1.4862880 Lammert et al., 2010 A. Lammert M. Proctor S. Narayanan Data-driven analysis of realtime vocal tract MRI using correlated image regions Proc. Interspeech Makuhari, Japan 2010 1572 1575 Laprie et al., 2014 Y. Laprie M. Aron M.O. Berger B. Wrobel-Dautcourt Studying MRI acquisition protocols of sustained sounds with a multimodal acquisition system Proc. 10th Int. Seminar on Speech Production (ISSP) Cologne, Germany 2014 245 248 Lovatto et al., 2007 L. Lovatto A. Amelot L. Crevier-Buchman P. Basset J. Vaissière A fiberscopic analysis of nasal vowels in Brazilian Portuguese Proc. 16th Int. Congress of Phonetic Sciences (ICPhS) Saarbrücken, Germany 2007 549 552 Martins et al., 2008 P. Martins I. Carbone A. Pinto A. Silva A. Teixeira European Portuguese MRI based speech production studies Speech Commun. 50 2008 925 952 Martins et al., 2010 P. Martins C. Oliveira A. Silva A. Teixeira Articulatory characteristics of European Portuguese laterals: a 2D & 3D MRI study Proc. FALA Vigo, Spain 2010 33 36 Martins et al., 2011 P. Martins C. Oliveira S. Silva A. Silva A. Teixeira Tongue segmentations from MRI images using ITK-Snap: preliminary evaluation Proc. IADIS Intl Conferences on Computer Graphics, Visualization, Computer Vision and Image Processing 2011 Martins et al., 2012 P. Martins C. Oliveira S. Silva A. Teixeira Velar movement in European Portuguese nasal vowels Proc. IberSpeech 2012 – VII Jornadas en Tecnología del Habla and III Iberian SLTech Workshop Madrid, Spain 2012 231 240 Miller et al., 2014 N.A. Miller J.S. Gregory R.M. Aspden P.J. Stollery F.J. Gilbert Using active shape modeling based on MRI to study morphologic and pitch-related functional changes affecting vocal structures and the airway J. Voice 2014 10.1016/j.jvoice.2013.12.002 (in press) Nam et al., 2015 H. Nam C. Browman L. Goldstein M. Proctor P. Rubin E. Saltzman TADA: TAsk Dynamic Application 2015 Niebergall et al., 2013 A. Niebergall S. Zhang E. Kunay G. Keydana M. Job M. Uecker J. Frahm Real-time MRI of speaking at a resolution of 33ms: undersampled radial FLASH with nonlinear inverse reconstruction Magn. Reson. Med. 69 2013 477 485 Oliveira et al., 2012 C. Oliveira P. Martins S. Silva A. Teixeira An MRI study of the oral articulation of European Portuguese nasal vowels Proc. Interspeech Portland, OR, USA 2012 2690 2693 Oliveira et al., 2009 C. Oliveira P. Martins A. Teixeira Speech rate effects on European Portuguese nasal vowels Proc. Interspeech Brighton, UK 2009 480 483 Pratt, 2007 W.K. Pratt Digital Image Processing 2007 Wiley-Interscience Proctor et al., 2010 M. Proctor D. Bone A. Katsamanis S. Narayanan Rapid semi-automatic segmentation of real-time magnetic resonance images for parametric vocal tract analysis Proc. Interspeech Makuhari, Japan 2010 1576 1579 Proctor et al., 2012 M. Proctor L.H. Lu Y. Zhu L. Goldstein S. Narayanan Articulation of Mandarin sibilants: a multi-plane realtime MRI study Proc. Speech Sci. Tech. Sydney, Australia 2012 113 116 Proctor et al., 2011 M.I. Proctor A.C. Lammert A. Katsamanis L.M. Goldstein C. Hagedorn S.S. Narayanan Direct estimation of articulatory kinematics from real-time magnetic resonance image sequences Proc. Interspeech Florence, Italy 2011 281 284 Ramanarayanan et al., 2010 V. Ramanarayanan D. Byrd L. Goldstein S.S. Narayanan Investigating articulatory setting – pauses, ready position, and rest – using real-time MRI Proc. Interspeech Makuhari, Japan 2010 1994 1997 Ramanarayanan et al., 2013 V. Ramanarayanan L. Goldstein D. Byrd S. Narayanan An investigation of articulatory setting using real-time magnetic resonance J. Acoust. Soc. Am. 134 2013 510 519 Saltzman and Munhall, 1989 E. Saltzman K. Munhall A dynamical approach to gestural patterning in speech production Ecol. Psychol. 1 1989 333 382 Scott et al., 2014 A.D. Scott M. Wylezinska M.J. Birch M.E. Miquel Speech MRI: morphology and function Phys. Med. 2014 10.1016/j.ejmp.2014.05.001 (in press) Shadle et al., 2008 C. Shadle M.I. Proctor K. Iskarous An MRI study of the effect of vowel context on English fricatives Proc. Acoustics ’08 Paris: Joint meeting of the ASA, EAA & Société Française d’Acoustique 2008 Shosted et al., 2012 R. Shosted B.P. Sutton A. Benmamoun Using magnetic resonance to image the pharynx during Arabic speech: static and dynamic aspects Proc. Interspeech Portland, OR, USA 2012 2182 2185 Silva and Teixeira, 2014a S. Silva A. Teixeira A framework for analysis of the upper airway from real-time MRI sequences Proc. SPIE Visualization and Data Analysis SF, CA, USA 2014 901703 10.1117/12.2042081 Silva and Teixeira, 2014b S. Silva A. Teixeira RT-MRI based dynamic analysis of vocal tract configurations: preliminary work regarding intra- and inter-sound variability Proc. Int. Seminar on Speech Production (ISSP) Cologne, Germany 2014 399 402 Silva and Teixeira, 2014c S. Silva A. Teixeira Systematic and quantitative analysis of vocal tract data: Intra- and inter-speaker analysis Proc. Int. Seminar on Speech Production (ISSP) Cologne, Germany 2014 403 406 Silva and Teixeira, 2015 S. Silva A. Teixeira Unsupervised segmentation of the vocal tract from real-time MRI sequences Comput. Speech Lang. 33 2015 25 46 10.1016/j.csl.2014.12.003 Silva et al., 2013a S. Silva A. Teixeira C. Oliveira P. Martins Segmentation and analysis of vocal tract from midsagittal real-time MRI Proc. ICIAR 2013. LNCS vol. 7950 Póvoa de Varzim, Portugal 2013 459 466 Silva et al., 2013b S. Silva A. Teixeira C. Oliveira P. Martins Towards a systematic and quantitative analysis of vocal tract data Proc. Interspeech Lyon, France 2013 1307 1311 Smith, 2014 C. Smith Complex tongue shaping in lateral liquid production without constriction-based goals Proc. ISSP Cologne, Germany 2014 413 416 Teixeira et al., 2012 A. Teixeira P. Martins C. Oliveira C. Ferreira A. Silva R. Shosted Real-time MRI for Portuguese: database, methods and applications Proc PROPOR 2012. LNCS vol. 7243 Coimbra, Portugal 2012 306 317 Teixeira et al., 2011 A. Teixeira P. Martins A. Silva C. Oliveira An MRI study of consonantal coarticulation resistance in Portuguese Proc. 9th Int. Seminar on Speech Production (ISSP) Montreal, Canada 2011 243 250 Teixeira et al., 2002 A. Teixeira L. Silva R. Martinez F. Vaz Sapwindows – towards a versatile modular articulatory synthesizer Proc. IEEE Workshop on Speech Synthesis 2002 31 34 10.1109/WSS.2002.1224366 Teixeira et al., 1999 A. Teixeira F. Vaz J.C. Prncípe Influence of dynamics in the perceived naturalness of Portuguese nasal vowels Proc. Int. Congress on Phonetic Sciences (ICPhS) San Francisco, CA, USA 1999 2557 2560 Tiede and Vatikiotis-Bateson, 2000 M.K. Tiede E. Vatikiotis-Bateson Contrasts in speech articulation observed in sitting and supine conditions Proc. 5th Seminar on Speech Production (ISSP) Chiemgau, Germany 2000 25 28 Wang, 2011 Y. Wang Smoothing Splines: Methods and Applications 2011 CRC Press Zharkova and Hewlett, 2009 N. Zharkova N. Hewlett Measuring lingual coarticulation from midsagittal tongue contours: description and example calculations using English /t/and /a J. Phonet. 37 2009 248 256 10.1016/j.wocn.2008.10.005 Zharkova et al., 2011 N. Zharkova N. Hewlett W.J. Hardcastle Coarticulation as an indicator of speech motor control development in children: An ultrasound study Motor Control 15 2011 118 140 Zharkova et al., 2014 N. Zharkova N. Hewlett W.J. Hardcastle R.J. Lickley Spatial and temporal lingual coarticulation and motor control in preadolescents J. Speech Lang. Hearing Res. 57 2011 374 388 Zhu et al., 2013 Y. Zhu A. Toutios S. Narayanan K. Nayak Faster 3D vocal tract real-time MRI using constrained reconstruction Proc. Interspeech Lyon, France 2013 1292 1296 "
    },
    {
        "doc_title": "Applications of the multimodal interaction architecture in ambient assisted living",
        "doc_scopus_id": "85009674996",
        "doc_doi": "10.1007/978-3-319-42816-1_12",
        "doc_eid": "2-s2.0-85009674996",
        "doc_date": "2016-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Ambient assisted living (AAL)",
            "Heterogeneous environments",
            "Multi-Modal Interactions",
            "Multimodal application",
            "Research outcome",
            "User groups",
            "User interaction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2017.Developing applications for ambient assisted living (AAL) scenarios requires dealing with diverse user groups, heterogeneous environments, and a large plethora of devices. These requirements pose several challenges on how to design and develop user interaction with the proposed applications and services. In this context, the versatility provided by multimodal interaction (MMI) is paramount and the adopted architecture should be instrumental in harnessing its full potential. This chapter offers an insight on how AAL challenges can be tackled by multimodal-based solutions. It presents the authors’ views and research outcomes in multimodal application development for AAL grounded on an architecture for MMI aligned with the W3C recommendations.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multi-device applications using the multimodal architecture",
        "doc_scopus_id": "85009673838",
        "doc_doi": "10.1007/978-3-319-42816-1_17",
        "doc_eid": "2-s2.0-85009673838",
        "doc_date": "2016-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Architecture-based",
            "Concrete applications",
            "Design and Development",
            "Multi-devices",
            "Multi-Modal Interactions",
            "Multimodal architectures",
            "Research outcome",
            "Screen sizes"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2017.Nowadays, users have access to a multitude of devices at their homes, workplaces or that they can carry around. Each of these devices, given its features (e.g., interaction modalities, screen size), might be more suitable for particular users, tasks, and contexts. While having one application installed in several devices might be common, they mostly work isolated, not exploring the possibilities of several devices working together to provide a more versatile and richer interaction scenario. Adopting a multimodal interaction (MMI) architecture based on the W3C recommendations, beyond the advantages to the design and development of MMI, provides, we argue, an elegant approach to tackle multi-device interaction scenarios. In this regard, this chapter conveys our views and research outcomes addressing this subject, presenting concrete application examples.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preface",
        "doc_scopus_id": "84997254257",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84997254257",
        "doc_date": "2016-01-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interactive, multi-device visualization supported by a multimodal interaction framework: Proof of concept",
        "doc_scopus_id": "84978914860",
        "doc_doi": "10.1007/978-3-319-39943-0_27",
        "doc_eid": "2-s2.0-84978914860",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Device characteristics",
            "First impressions",
            "Future improvements",
            "Interactive visualizations",
            "Multi-devices",
            "Multi-Modal Interactions",
            "Multiple representation",
            "User satisfaction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.Nowadays, users can interact with a system using a wide variety of modalities, such as touch and speech. Nevertheless, multimodal interaction has yet to be explored for interactive visualization scenarios. Furthermore, users have access to a wide variety of devices (e.g., smartphones, tablets) that could be harnessed to provide a more versatile visualization experience, whether by providing complementary views or by enabling multiple users to jointly explore the visualization using their devices. In our effort to gather multimodal interaction and multi-device support for visualization, this paper describes our first approach to an interactive multi-device system, based on the multimodal interaction architecture proposed by the W3C, enabling interactive visualization using different devices and representations. It allows users to run the application in different types of devices, e.g., tablets or smartphones, and the visualizations can be adapted to multiple screen sizes, by selecting different representations, with different levels of detail, depending on the device characteristics. Groups of users can rely on their personal devices to synchronously visualize and interact with the same data, maintaining the ability to use a custom representation according to their personal needs. A preliminary evaluation was performed, mostly to collect users’ first impressions and guide future developments. Although the results show a moderate user satisfaction, somehow expected at this early stage of development, user feedback allowed the identification of important routes for future improvement, particularly regarding a more versatile navigation along the data and the definition of composite visualizations (e.g., by gathering multiple representations on the same screen).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the creation of a persona to support the development of technologies for children with autism spectrum disorder",
        "doc_scopus_id": "84978904050",
        "doc_doi": "10.1007/978-3-319-40238-3_21",
        "doc_eid": "2-s2.0-84978904050",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autism spectrum disorders",
            "Children",
            "Children with autisms",
            "Design and Development",
            "Persona"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.When developing technologies for persons with autism spectrum disorder (ASD) there are multiple aspects posing challenges to the community. First of all, there are several viewpoints at stake, from the targeted person to family and caretakers, needing careful consideration and yielding conflicting interests and motivations that need to be considered. Second, design and development teams often include people with a very diverse background, from psychologists to software engineers, who need to be able to fully communicate their knowledge and ideas regarding the users, and understand the different team viewpoints towards the best possible outcome. In this context, we argue that Personas (and in particular, families of Personas) can be a powerful tool to tackle these challenges. As a first stage of our work, we present the methods considered for the creation of a Persona for a 10 years old kid with ASD along with its full description. At this stage, the Persona has been evaluated by a panel of experts and was considered in the design of a first application prototype for children with ASD.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "\"Read that article\": Exploring synergies between gaze and speech interaction",
        "doc_scopus_id": "84962648230",
        "doc_doi": "10.1145/2700648.2811369",
        "doc_eid": "2-s2.0-84962648230",
        "doc_date": "2015-10-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Active-assisted",
            "Gaze",
            "HCI system",
            "Human computer interaction (HCI)",
            "Input modalities",
            "Multi-modal",
            "Speech interaction",
            "User intention"
        ],
        "doc_abstract": "© 2015 ACM.Gaze information has the potential to benefit Human-Computer Interaction (HCI) tasks, particularly when combined with speech. Gaze can improve our understanding of the user intention, as a secondary input modality, or it can be used as the main input modality by users with some level of permanent or temporary impairments. In this paper we describe a multimodal HCI system prototype which supports speech, gaze and the combination of both. The system has been developed for Active Assisted Living scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a hybrid NLG system for Data2Text in Portuguese",
        "doc_scopus_id": "84943279860",
        "doc_doi": "10.1109/CISTI.2015.7170419",
        "doc_eid": "2-s2.0-84943279860",
        "doc_date": "2015-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Automatic translation",
            "Data2Text",
            "False positive",
            "Linguistic features",
            "Natural language generation",
            "Portuguese",
            "Quality evaluation",
            "Template-based"
        ],
        "doc_abstract": "© 2015 AISTI.In many new interactions with machines, such as dialogue or output using voice, there is the need to convert information internal to a system into sentences, using Data2Text systems. Trying to avoid the limitations of template-based and classical NLG methods, systems based on automatic translation have been proposed in recent years. Despite providing sentences with the important variability needed for a better interaction, this doesn't come without a cost. Contrary to template-based, these systems produce sentences with heterogeneous quality. In this paper we proposed to combine a translation based NLG system with a classifier module capable of providing information on the Intelligibility or Quality of the sentences. Sentences marked as unacceptable are replaced by template-based generated ones. This classifier module is the main focus of the paper and combines extraction of linguistic features with a classifier trained in a manually annotated corpus. Results suggest that our approach is valid as best results obtained have false positives below 8% and this metric can be even lower in practical applications, decreasing to around 3%, as the generation module produces low quality sentences at a rate lower than 30%.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Trainable NLG for data to Portuguese - With application to a Medication Assistant",
        "doc_scopus_id": "84939484820",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84939484820",
        "doc_date": "2015-07-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2015 Linguamática.New equipments, such as smartphones and tablets, are changing human computer interaction. These devices present several challenges, especially due to their small screen and keyboard. In order to use text and voice in multimodal interaction, it is essential to deploy modules to translate the internal information of the applications into sentences or texts, in order to display it on screen or synthesize it. Also, these modules must generate phrases and texts in the user's native language; the development should not require considerable resources; and the outcome of the generation should achieve a good degree of variability. Our main objective is to propose, implement and evaluate a method of data conversion to Portuguese which can be developed with a minimum of time and knowledge, but without compromising the necessary variability and quality of what is generated. The developed system, for a Medication Assistant, is intended to create descriptions, in natural language, of medication to be taken. Motivated by recent results, we opted for an approach based on machine translation, with models trained on a small parallel corpus. For that, a new corpus was created. With it, two variants of the system were trained: phrase-based translation and syntax-based translation. The two variants were evaluated by automatic measurements - BLEU and Meteor - and by humans. The results showed that a phrase-based approach produced better results than a syntax-based one: human evaluators evaluated 60% of phrase-based responses as good, or very good, compared to only 46% of syntax-based responses. Considering the corpus size, we judge this value (60%) as good.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detecting nasal vowels in speech interfaces based on surface electromyography",
        "doc_scopus_id": "84936817954",
        "doc_doi": "10.1371/journal.pone.0127040",
        "doc_eid": "2-s2.0-84936817954",
        "doc_date": "2015-06-12",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Adult",
            "Electromyography",
            "Female",
            "Humans",
            "Magnetic Resonance Imaging",
            "Phonetics",
            "Reproducibility of Results",
            "Speech Acoustics",
            "User-Computer Interface",
            "Young Adult"
        ],
        "doc_abstract": "© 2015 Freitas et al.Nasality is a very important characteristic of several languages, European Portuguese being one of them. This paper addresses the challenge of nasality detection in surface electromyography (EMG) based speech interfaces. We explore the existence of useful information about the velum movement and also assess if muscles deeper down in the face and neck region can be measured using surface electrodes, and the best electrode location to do so. The procedure we adopted uses Real-Time Magnetic Resonance Imaging (RT-MRI), collected from a set of speakers, providing a method to interpret EMG data. By ensuring compatible data recording conditions, and proper time alignment between the EMG and the RT-MRI data, we are able to accurately estimate the time when the velum moves and the type of movement when a nasal vowel occurs. The combination of these two sources revealed interesting and distinct characteristics in the EMG signal when a nasal vowel is uttered, which motivated a classification experiment. Overall results of this experiment provide evidence that it is possible to detect velum movement using sensors positioned below the ear, between mastoid process and the mandible, in the upper neck region. In a frame-based classification scenario, error rates as low as 32.5% for all speakers and 23.4% for the best speaker have been achieved, for nasal vowel detection. This outcome stands as an encouraging result, fostering the grounds for deeper exploration of the proposed approach as a promising route to the development of an EMG-based speech interface for languages with strong nasal characteristics.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Usability, accessibility and ambient-assisted living: a systematic literature review",
        "doc_scopus_id": "84924220833",
        "doc_doi": "10.1007/s10209-013-0328-x",
        "doc_eid": "2-s2.0-84924220833",
        "doc_date": "2015-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Accessibility",
            "Ambient assisted living",
            "Systematic literature review",
            "Usability",
            "User interaction"
        ],
        "doc_abstract": "© 2013, Springer-Verlag Berlin Heidelberg.Ambient-assisted living (AAL) is, nowadays, an important research and development area, foreseen as an important instrument to face the demographic aging. The acceptance of the AAL paradigm is closely related to the quality of the available systems, namely in terms of intelligent functions for the user interaction. In that context, usability and accessibility are crucial issues to consider. This paper presents a systematic literature review of AAL technologies, products and services with the objective of establishing the current position regarding user interaction and how are end users involved in the AAL development and evaluation processes. For this purpose, a systematic review of the literature on AAL was undertaken. A total of 1,048 articles were analyzed, 111 of which were mainly related to user interaction and 132 of which described practical AAL systems applied in a specified context and with a well-defined aim. Those articles classified as user interaction and systems were further characterized in terms of objectives, target users, users’ involvement, usability and accessibility issues, settings to be applied, technologies used and development stages. The results show the need to improve the integration and interoperability of the existing technologies and to promote user-centric developments with a strong involvement of end users, namely in what concerns usability and accessibility issues.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Identifying Things, Relations, and Semantizing Data",
        "doc_scopus_id": "85101187147",
        "doc_doi": "10.1007/978-3-319-15563-0_3",
        "doc_eid": "2-s2.0-85101187147",
        "doc_date": "2015-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Extract informations",
            "Information extraction systems",
            "Named entity recognition",
            "Ontology-based information extraction",
            "Pipelined architecture"
        ],
        "doc_abstract": "© 2015, The Authors.This chapter concludes the presentation of the generic pipelined architecture of Information Extraction (IE) systems, by presenting its domain dependent part. After preparation and enrichment, the document’s contents are now characterized and suitable to be processed to locate and extract information. This chapter explains how this can be performed, addressing both extraction of entities and relations between entities. Identifying entities mentioned in texts is a pervasive task in IE. It is called Named Entity Recognition (NER) and seeks to locate and classify textual mentions that refer to specific types of entities, such as, for example, persons, organizations, addresses and dates. The chapter also dedicates attention to how to store the extracted information and how to take advantage of semantics to improve the information extraction process, presenting the basis of Ontology-Based Information Extraction (OBIE) systems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Data Gathering, Preparation and Enrichment",
        "doc_scopus_id": "85101183472",
        "doc_doi": "10.1007/978-3-319-15563-0_2",
        "doc_eid": "2-s2.0-85101183472",
        "doc_date": "2015-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Domain independents",
            "General architectures",
            "Information extraction systems",
            "NAtural language processing",
            "Part of speech tagging",
            "Sentence boundary detection",
            "Syntactic information",
            "Syntactic parsing"
        ],
        "doc_abstract": "© 2015, The Authors.This chapter presents the domain independent part of the general architecture of Information Extraction (IE) systems. This first part aims at preparing documents by the application of several Natural Language processing tasks that enrich the documents with morphological and syntactic information. This is made in successive processing steps which start by making contents uniform, and end by identifying the roles of the words and how they are arranged. Here are described the most common steps: sentence boundary detection, tokenization, part-of-speech tagging, and syntactic parsing. The description includes information on a selection of relevant tools available to implement each step. The chapter ends with the presentation of three very representative software suites that make easier the integration of the several steps described.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Application Examples",
        "doc_scopus_id": "85101157887",
        "doc_doi": "10.1007/978-3-319-15563-0_5",
        "doc_eid": "2-s2.0-85101157887",
        "doc_date": "2015-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Computer programming skills",
            "Electronic government",
            "Extract informations",
            "Extracting information",
            "Natural languages",
            "Stanford",
            "Wikipedia"
        ],
        "doc_abstract": "© 2015, The Authors.In this chapter are presented two concrete examples of applications. The first example is a tutorial that is easy to replicate (almost) without requiring computer programming skills. This example elaborates on extracting information useful in a wide range of scenarios: detection of people, organizations, and dates. It shows how to extract information from a Wikipedia page. Most of the system is implemented using the Stanford CoreNLP suite. The second example is more complex and instantiates the OBIE architecture presented in the previous chapter using software tools from different sources that need to be adapted to work together. The application is related to electronic government, and processes publically available documents of municipalities. This second example targets contents written in a natural language not often available out of the box: Portuguese.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Introduction",
        "doc_scopus_id": "85101147095",
        "doc_doi": "10.1007/978-3-319-15563-0_1",
        "doc_eid": "2-s2.0-85101147095",
        "doc_date": "2015-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Background information",
            "Co-reference resolutions",
            "Extracting information",
            "Morphological analysis",
            "Named entity recognition",
            "NAtural language processing",
            "Part of speech tagging",
            "Unstructured documents"
        ],
        "doc_abstract": "© 2015, The Authors.Chapter 1 introduces the problem of extracting information from natural language unstructured documents, which is becoming more and more relevant in our “document society”. Despite the many useful applications that the information in these documents can potentiate, it is harder and harder to obtain the wanted information. Major problems result from the fact that much of the documents are in a format non usable by humans or machines. There is the need to create ways to extract relevant information from the vast amount of natural language sources. After this, the chapter presents, briefly, background information on Semantics, knowledge representation and Natural Language Processing, to support the presentation of the area of Information Extraction [IE, “the analysis of unstructured text in order to extract information about pre-specified types of events, entities or relationships, such as the relationship between disease and genes or disease and food items; in so doing value and insight are added to the data.” (Text mining of web-based medical content, Berlin, p 50)], its challenges, different approaches and general architecture, which is organized as a processing pipeline including domain independent components—tokenization, morphological analysis, part-of-speech tagging, syntactic parsing—and domain specific IE components—named entity recognition and co-reference resolution, relation identification, information fusion, among others.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preface",
        "doc_scopus_id": "85101107820",
        "doc_doi": "10.1524/stuf.2004.57.1.3",
        "doc_eid": "2-s2.0-85101107820",
        "doc_date": "2015-01-01",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Conclusion",
        "doc_scopus_id": "85101093741",
        "doc_doi": "10.1007/978-3-319-15563-0_6",
        "doc_eid": "2-s2.0-85101093741",
        "doc_date": "2015-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Error prone tasks",
            "Formal representations",
            "Formal structures",
            "Natural languages",
            "Specialized knowledge"
        ],
        "doc_abstract": "© 2015, The Authors.In this book was discussed the need to provide formal structures to contents originally created in unstructured formats using natural language. The volume of relevant information in such formats increases every day as people use the Internet to communicate, and as organizations create and publish documentation. The contents are often without formalized markups because marking contents manually can be a time consuming and error prone task that requires some specialized knowledge. The objective of information extraction is to analyze these contents and produce fixed format, unambiguous and formal representations of them, including the identification of the entities involved and the relations they establish among them.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Extracting Relevant Information Using a Given Semantic",
        "doc_scopus_id": "85101059530",
        "doc_doi": "10.1007/978-3-319-15563-0_4",
        "doc_eid": "2-s2.0-85101059530",
        "doc_date": "2015-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Extract informations",
            "Human operator",
            "Natural languages",
            "Ontology concepts",
            "Ontology-based information extraction",
            "Proposed architectures"
        ],
        "doc_abstract": "© 2015, The Authors.This chapter presents an example of software architecture, developed by the authors, for performing Ontology Based Information Extraction (OBIE) using an arbitrary ontology. The goal of the architecture is to allow the deployment of applications for arbitrary domains without need of system reprogramming. For that, human operator(s) define the semantics of the application and provide some examples of ontology concepts in target texts; then the system learns how to extract information according to the defined ontology. An instantiation of the proposed architecture using freely available and high performance software tools is also presented. This instantiation is made for processing texts in a natural language, Portuguese, that was not the original target for most of the tools, showing and discussing the preparation of tools for other languages than the ones provided out of the box.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The living usability lab architecture: Support for the development and evaluation of new ambient assisted living services for the elderly",
        "doc_scopus_id": "85053942221",
        "doc_doi": "10.1201/b18520",
        "doc_eid": "2-s2.0-85053942221",
        "doc_date": "2015-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            },
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2015 by Taylor & Francis Group, LLC.The introduction of technology in the domestic environment is a reality. For the elderly, a significant percentage of the population, technology can have a positive impact on their quality of life. However, this can only be true if technologies for domestic environments are made accessible and usable by those staying at home. The Living Usability Lab (LUL) project aims to create conditions for the development and evaluation of innovative services for the elderly. Two of the main characteristics of the project are its attention to usability and design for all, and the exploration of next-generation networks. This chapter presents the conceptual architecture adopted for the living lab and the support architecture for development of new, complex ambient assisted living (AAL) services. The genesis of the living lab ecosystem is the aim to create an environment where developers and care professionals are able to create innovative AAL applications and services with ease and with access to a whole set of test groundings. To support our proposal, a new service for telerehabilitation-developed by the project-is used as a scenario within the proposed architectures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Characterization and classification of existing ambient assisted living systems: A systematic literature review",
        "doc_scopus_id": "85049045074",
        "doc_doi": "10.1201/b18520",
        "doc_eid": "2-s2.0-85049045074",
        "doc_date": "2015-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            },
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            },
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2015 by Taylor & Francis Group, LLC.This chapter describes a systematic review that summarizes and characterizes existing literature on ambient assisted living (AAL). This presents an added value for the future development in this area. To be included in this review, articles must have defined innovative concepts or characterized innovative technologies, products, or systems that can contribute to the development of the AAL paradigm, with the aim of enabling people with specific demands (e.g., elderly) to live longer in their natural environment. AAL could therefore be translated best as intelligent systems of assistance for a better and safer life. Results indicate that most publications regarding AAL are technology-oriented with only a few articles found describing applications and scenarios. Another interesting finding is the effort made to adjust the technology to the characteristics and needs of the user considering the context in which the activity is taking place.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multilingual Speech Recognition for the Elderly: The AALFred Personal Life Assistant",
        "doc_scopus_id": "84962832783",
        "doc_doi": "10.1016/j.procs.2015.09.272",
        "doc_eid": "2-s2.0-84962832783",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "ageing",
            "Automatic speech recognition",
            "elderly",
            "Multi-modal",
            "multilingual"
        ],
        "doc_abstract": "© 2015 Published by Elsevier B.V.The PaeLife project is a European industry-academia collaboration in the framework of the Ambient Assisted Living Joint Programme (AAL JP), with a goal of developing a multimodal, multilingual virtual personal life assistant to help senior citizens remain active and socially integrated. Speech is one of the key interaction modalities of AALFred, the Windows application developed in the project; the application can be controlled using speech input in four European languages: French, Hungarian, Polish and Portuguese. This paper briefly presents the personal life assistant and then focuses on the speech-related achievements of the project. These include the collection, transcription and annotation of large corpora of elderly speech, the development of automatic speech recognisers optimised for elderly speakers, a speech modality component that can easily be reused in other applications, and an automatic grammar translation service that allows for fast expansion of the automatic speech recognition functionality to new languages.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2015-11-06 2015-11-06 2015-11-06 2015-11-06 2016-01-06T20:55:21 S1877-0509(15)03118-X S187705091503118X 10.1016/j.procs.2015.09.272 S300 S300.2 HEAD-AND-TAIL 2016-01-06T16:14:49.492416-05:00 0 0 20150101 20151231 2015 2015-11-06T03:04:30.74972Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 true 67 67 C Volume 67 32 283 292 283 292 2015 2015 2015-01-01 2015-12-31 2015 Proceedings of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion Dr. Carlos Velasco Dr. Gerhard Weber Dr. João Barroso Dr. Yehya Mohamad Dr. Hugo Paredes article fla Copyright © 2015 Published by Elsevier B.V. MULTILINGUALSPEECHRECOGNITIONFORELDERLYAALFREDPERSONALLIFEASSISTANT HAMALAINEN A HAMALAINENX2015X283 HAMALAINENX2015X283X292 HAMALAINENX2015X283XA HAMALAINENX2015X283X292XA Full 2015-09-26T00:02:03Z ElsevierWaived OA-Window item S1877-0509(15)03118-X S187705091503118X 10.1016/j.procs.2015.09.272 280203 2016-01-06T16:14:49.492416-05:00 2015-01-01 2015-12-31 true 1280916 MAIN 10 50141 849 656 IMAGE-WEB-PDF 1 Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 Available online at www.sciencedirect.com 1877-0509 Â© 2015 Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of organizing committee of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2015) doi: 10.1016/j.procs.2015.09.272 ScienceDirect 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Infoexclusion (DSAI 2015) Multilingual speech recognition for the elderly: The AALFred personal life assistant Annika HÃ¤mÃ¤lÃ¤inena,b, AntÃ³nio Teixeirac, Nuno Almeidac, Hugo Meinedoa, Tibor FegyÃ³d, Miguel Sales Diasa,b aMicrosoft Language Development Center, Lisbon, Portugal bISCTE â€“ University Institute of Lisbon (ISCTE-IUL), Lisbon, Portugal cDepartment of Electronics, Telecommunications & Informatics/IEETA, University of Aveiro, Aveiro, Portugal dDepartment of Telecommunications & Media Informatics, Budapest University of Technology & Economics, Budapest, Hungary Abstract The PaeLife project is a European industry-academia collaboration in the framework of the Ambient Assisted Living Joint Programme (AAL JP), with a goal of developing a multimodal, multilingual virtual personal life assistant to help senior citizens remain active and socially integrated. Speech is one of the key interaction modalities of AALFred, the Windows application developed in the project; the application can be controlled using speech input in four European languages: French, Hungarian, Polish and Portuguese. This paper briefly presents the personal life assistant and then focuses on the speech-related achievements of the project. These include the collection, transcription and annotation of large corpora of elderly speech, the development of automatic speech recognisers optimised for elderly speakers, a speech modality component that can easily be reused in other applications, and an automatic grammar translation service that allows for fast expansion of the automatic speech recognition functionality to new languages. Â© 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of organizing committee of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2015). Keywords: ageing; automatic speech recognition; elderly; human-computer interaction; multilingual; multimodal; speech. 5 Published by Elsevier B.V. This i an open access article under the CC BY-NC-ND license rg/licenses/by-nc- d/4.0/). Peer-review under responsibility of organizing committee of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2015) 284 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 1. Introduction Information and communication technology (ICT) has considerable potential when it comes to facilitating the lives of the elderly. However, due to the complexity of existing user interfaces and the limited set of available interaction modalities, combined with physical limitations such as poor eyesight, the elderly often have difficulties using ICT1. Therefore, it is very important to investigate the use of natural, easy-to-use interaction modalities in applications aimed at the elderly. Speech would be a particularly interesting interaction modality in the case of the elderly, as it offers a natural form of human-computer interaction (HCI) that requires neither visual attention nor the use of hands2. However, our voices change as we age3, and currently available automatic speech recognisers do not usually work well with elderly speech. This is because, to serve mainstream business requirements, they have been optimised for younger adultsâ€™ speech. The degradation in automatic speech recognition (ASR) performance on elderly speech has been illustrated, for instance, by Wilpon & Jacobsen4 and Vipperla et al.5. The same authors, however, also showed that performance improves considerably when automatic speech recognisers are specifically optimised for elderly speech. Such findings show that there is a real need for adapting ASR to elderly speech. There is growing international interest, both in academia and in industry, in developing speech-enabled applications for improving the daily lives of the elderly. Examples of already developed products, applications and services include, for instance, a telerehabilitation service with multimodal interaction6, Windows Phone applications tailored for the needs of the elderly7, a humanoid robot to help the elderly in their daily activities at home8, and a humanoid robot designed to be used for rehabilitation, fall detection and entertainment purposes at care institutions9. In this paper, we describe work done in the area of multilingual ASR in the AAL PaeLife project10. The goal of the project was to develop a multimodal virtual personal life assistant (PLA) that would help the elderly â€“ in particular those who have retired recently and have some experience in using technology â€“ to remain active, productive, independent, and socially integrated. The resulting application was named AALFred, and the ASR functionality was optimised for elderly speech in four European languages: French, Hungarian, Polish and Portuguese. Despite its potential, the integration of multilingual ASR in applications aimed at the elderly poses various challenges. First, as mentioned before, automatic speech recognisers need to be optimised for elderly speech â€“ a task that requires the availability of a sufficient amount of speech data collected from elderly speakers. In the context of the PaeLife project, we collected large corpora of French, Hungarian and Polish elderly speech â€“ which, in itself, is a time-consuming, demanding effort â€“ and then optimised the speech recognisers used in AALFred for elderly speech using these data. We also developed ASR for Portuguese elderly speech, using an elderly speech corpus collected in the context of the Living Usability Lab (LUL) project6. Second, the development of a multimodal application requires several components to seamlessly work together. To make this possible, we designed a speech modality component11 that works decoupled from the services available in AALFred and, therefore, makes it easier to integrate ASR in any future services, as well as in any future applications or products. Third, the speech modality needs to work in multiple languages. In the absence of speech interaction designers with relevant language skills, we proposed a way of automatically deriving the first versions of ASR grammars, which define the allowed speech input in speech-enabled applications, for new languages. This paper is further organised as follows. Section 2 describes AALFred and the services it offers. Section 3 describes our efforts to develop multilingual ASR for the four PaeLife languages, and illustrates the performance improvements we achieved when optimising ASR for elderly speech. In Section 4, we present the speech modality component and the way new languages can be integrated into it. Finally, in Section 5, we formulate our conclusions. 2. The AALFred Personal Life Assistant AALFred supports five human-computer interaction (HCI) modalities for easy, natural HCI: mouse, keyboard, speech, touch and gesture. Speech input is currently available and optimised for elderly speech in French, Hungarian, Polish and Portuguese. In those four languages, elderly users can use voice commands to access and operate services, and dictation to compose messages and to add descriptions of appointments (in the Messaging and Agenda services described below). 285 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 Physically, AALFred comprises a stationary main unit, a desktop computer connected to a large screen (e.g. an LCD TV), as well as a portable device, a tablet. In the main unit, the large screen supports graphical output, the internal microphone and speakers support speech input (ASR) and output (speech synthesis), and a Kinect sensor supports gesture input. In the portable unit, on the other hand, the display supports graphical output, the internal microphone and speakers enable speech input and output, and the multi-touch support of the operating system makes touch input possible. The main and portable units can work either together or separately as stand-alone devices, and can be connected to the internet and to the cloud for providing the user with online services (see Fig. 1). AALFred offers the elderly a wide range of services in the areas of social communication, entertainment, information management and information search, accessible through different modalities: â€¢ Agenda â€“ managing appointments â€¢ Contacts â€“ managing contacts â€¢ Messaging â€“ receiving and sending messages (email, Twitter, Facebook, Skype) â€¢ Audio or videoconference â€“ establishing audiovisual communication (Skype) â€¢ Media â€“ viewing audiovisual information â€¢ Find My â€“ searching for local services (pharmacies, police, etc.) â€¢ News Reader â€“ having the latest news read out by a speech synthesiser â€¢ WeatherForAll â€“ checking the weather forecast Fig. 1. The architecture of AALFred. To give a simple illustration of the interaction modalities, let us consider the view of available services in AALFred in Fig. 2. To see the services hidden on the right side of the screen, the user can, for example, swipe right with their hand (gesture), drag the screen right with their finger (touch), or say, â€œMove rightâ€� (speech). To access a service, the user can touch the relevant icon, or use a voice command such as â€œShow my Agendaâ€� or â€œOpen my Agendaâ€�. In fact, several different voice commands are able to perform the same action. The user can also use various interaction modalities to operate a service. In the case of the Agenda service, (s)he might, for instance, want to add a new appointment. As illustrated in Fig.3, the user can touch the desired day or speak out the day of the week (e.g. â€œThursdayâ€� or â€œOpen Thursdayâ€�). When the desired day is displayed, (s)he can add a new appointment by tapping the +-button on the screen or say, for example, â€œAdd a new appointmentâ€�. (S)he can then add the details of the appointment using the keyboard (either an on-screen, inbuilt or external keyboard, depending on the device used) or speech input (dictation). The details of the appointment will be saved when the user, for instance, says, â€œSave.â€� $ %!\" % # ! % ) %* ! % $ â€¢ â€¢ â€¢ ! # â€¢ (% %! \" â€¢ \" ! % ! â€¢ % # #' $ !& #' $ 286 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 Fig. 2 Interaction modalities available in AALFred include gesture, touch and speech. Fig. 3 Operating the Agenda service using different interaction modalities. 3. Providing State-of-the-Art Multilingual ASR for the Elderly In this section, we summarise how ASR works and describe how it was optimised for the elderly and for multiple input languages in the PaeLife project. ASR is technology that translates acoustic speech signals into the 287 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 corresponding sequences of (written) words. To be able to do that, automatic speech recognisers typically use three types of language-specific knowledge bases: 1) a language model, which contains information about the possible words and sequences of words in the input language, together with their probabilities of occurrence (e.g. â€˜I use Bingâ€™ is a probable sequence of words while â€˜I chair Bingâ€™ is not), and/or grammars, which contain information about the allowed speech input in different situations (e.g. numbers from 1 to 31 are allowed in the case of grammars meant for recognising dates, while e.g. 32 is not), 2) a pronunciation lexicon, which represents each word in the ASR vocabulary (the finite set of words the system can recognise) in terms of individual speech sounds (phones) (e.g. â€˜Bingâ€™ word contains three phones: /b Éª Å‹/), and 3) acoustic models, which represent the stochastic time-based relationship between the input speech signal and the phones occurring in the speech. Together, the language model and/or the grammars, the lexicon and the acoustic models are used by the speech recogniser to find the most probable sequence of words for the input speech signal. State-of-the-art ASR systems employ statistical modelling techniques and are developed using large quantities of speech for training the acoustic models, and large quantities of text data for training the language model. Acoustic models are typically trained using speech collected from young to middle-aged adults. Because the acoustic properties of speech produced by elderly speakers differ from those produced by younger adults3, acoustic models expected to successfully recognise elderly speech have to be (re)trained using a sufficient amount of elderly speech. In the following subsections, we describe the work we did to collect, transcribe and annotate large corpora of elderly speech, to train acoustic models optimised for this kind of speech, and to test the performance of the models. 3.1. Collecting, Transcribing and Annotating Corpora of Elderly Speech To support the development of acoustic models optimised for the elderly, we collected a large corpus of elderly speech for each of the languages supported by AALFred. We selected speakers from 3rd age universities, care institutions, and social clubs and associations for seniors in different parts of France, Hungary, Poland and Portugal, to ensure a variety of regional accents in the data. All speakers were 60 years of age or older. The French and Polish corpora consist of read newspaper sentences, while the Portuguese corpus also contains a large amount of read command & control prompts. The Hungarian corpus comprises both read newspaper sentences (about 80% of the recordings) and spontaneous command & control utterances (about 20%). The main statistics of the corpora are detailed in Table 1. Table 1. Main statistics of the EASR corpora. #Speakers Total Audio (hh:mm:ss) Portuguese 986 185:10:25 French 328 76:09:07 Hungarian 1229 183:42:15 Polish 781 203:17:23 Once the data collection was finished, we used the prompts presented to the speakers as the starting point for orthographically transcribing the recordings with read speech. We verified and corrected those initial transcriptions to ensure that they matched what the speakers said in the recordings. In the case of the Hungarian spontaneous speech, we transcribed the recordings from scratch. In addition, using an annotation scheme, we marked the presence of noises and other audio events in the recordings. These audio events included filled pauses (e.g. ah, hmm), non-human (e.g. door banging) or human (e.g. coughing) noises, damaged words (e.g. false starts, mispronounced, unintelligible or truncated words), and speech from non-primary speakers (e.g. the recording supervisor). The corpora are collectively called the EASR Corpora of European Portuguese, Hungarian, French and Polish Elderly Speech, and are described in detail by HÃ¤mÃ¤lÃ¤inen et al.12. In the case of French, Hungarian and Polish, the data-related work was done in the PaeLife project, whereas the Portuguese data was collected, transcribed and annotated in the scope of the Living Usability Lab6 and Smartphones for Seniors7 projects. 288 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 3.2. Development of Elderly-Specific Acoustic Models In AALFred, speech input is handled using two different ASR systems: Microsoft Public Speech Platform Runtime version 1113, which supports French, Polish and Portuguese, and VOXerver14, which is a SAPI/Microsoft Speech Server-compatible system that supports Hungarian. The goals of the ASR-related work were to create French, Hungarian, Polish and Portuguese acoustic models optimised for the elderly users of AALFred, and to obtain the best possible recognition performance using existing techniques and tools compatible with the requirements of the Microsoft Public Speech Platform. We divided all elderly speech corpora into three datasets (training set: 85% of the speakers; development test set used for optimisation purposes: 5% of the speakers; evaluation test set used for measuring the final performance of the acoustic models: 10% of the speakers). In the case of French, there was a total of 60.3 hours of audio in the training set, whereas the same figures were 107 hours, 165.8 hours and 147.5 hours for Hungarian, Polish and Portuguese, respectively. We adapted t $ $ \" \" $ # # \" \" \" ! ! $ $ The hesitation and noise models were retrained using the stretches of audio signal that correspond to the hesitation and noise tags inserted into the transcriptions during the transcription and annotation phase. In addition to the above, in the case of French, we tested a new acoustic modelling paradigm. This paradigm, based on Deep Belief Neural Networks (DNNs), is currently the state-of-the-art acoustic modelling approach, with ample evidence in the literature showing significant performance gains when compared with classic approaches, such as GMM-based acoustic models15. For our work, we used existing French DNN-based acoustic models as a starting point, and adapted them to elderly speech using the data in the French training set. Similar DNN-based acoustic models for Polish and Portuguese are currently under development. The Hungarian acoustic models are trained using a Gaussian Mixture Model (GMM) -based, gender-independent, position-dependent cross-word triphone approach. The training methodology included speaker normalisation and discriminative training. Similar to the other PaeLife languages, the Hungarian acoustic models also include silence, hesitation and noise models. In the case of Hungarian, however, we merged the noise and silence models into an extended silence model. Unlike the acoustic models for the other languages, we trained the Hungarian models from scratch (rather than using younger adult speech models as a starting point). As the transcription and annotation work was still ongoing when we trained the models, the training set included both read speech with manually verified transcriptions and annotations, spontaneous speech with manual transcriptions and annotations, as well as read speech without annotations. We used a lightly supervised selection method to eliminate mispronounced sentences from the unannotated read speech. In the future, we also intend to train DNN-based acoustic models for Hungarian. 3.3. Evaluation Results To illustrate the improvements in ASR performance that can be achieved by using acoustic models optimised for elderly speech, we trained comparable bigram language models (LMs) for all four PaeLife languages. The French, Hungarian and Polish corpora are the most comparable with each other in terms of contents; they mainly contain read out newspaper sentences (the Hungarian corpus also contains some spontaneous commands; cf. Section 3.1). In addition to newspaper sentences, the Portuguese corpus contains a considerable amount of command & control material; only about half of the recorded utterances are read out newspaper sentences. To keep the ASR results as comparable as possible across languages, we used the sentences in the Hungarian and Polish training sets â€“ excluding the sentences that also appear in the test sets â€“ to train the LMs but, in the case of Portuguese, we also excluded the command & control material from the training material. To compensate for this loss of training sentences in the case of Portuguese, we appended the training material with unused sentences from the original pool 289 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 of newspaper texts available for collecting the EASR corpus. In the case of French, we had to use all the sentences in the training set for training the LM. Otherwise, the number of words in the test set that are not included in the ASR vocabulary, the so-called out-of-vocabulary (OOV) words, would have been very high; this would have masked the performance improvement arising from the elderly-specific acoustic models. In the case of Hungarian, the LM perplexity (an information theory -derived measure of how well a probability model (the LM) is able to predict a sample (the test set utterances); the lower a perplexity values is, the more accurately the LM is able to predict the word sequences in the test set utterances) and OOV rate are significantly higher than in the case of the other languages. This is due to the agglutinative nature of the language, resulting in thousands of possible word forms for a single stem. Table 2 summarises the key details of the LMs, as well as the results and improvements gained with the specialised acoustic models, as compared with standard acoustic models trained with young to middle-aged adultsâ€™ speech (baseline). Table 2. ASR results with acoustic models optimised for elderly speech. The 6th and 7th columns present the word error rates (WERs) obtained on the evaluation test sets using the baseline and the elderly-specific acoustic models. The last column indicates the relative reduction in the word error rates. Language ASR vocabulary words Word tokens (test set) LM perplexity (test set) OOV words (test set) WER (%) Baseline AMs WER (%) Elderly Speech AMs WER (%) relative reduction French 11287 36423 31.9 68 24.2 20.9 13.6 French - DNN 11287 36423 31.9 68 20.2 13.7 32.2 Hungarian 39151 40865 147.1 2372 27.0 19.4 28.1 Polish 19781 83135 54.3 283 16.0 13.6 15.0 Portuguese 8934 52970 60.0 219 18.3 16.4 10.4 As we can see in Table 2, the elderly-specific acoustic models provide considerable improvements in ASR performance over the baseline models. As expected, DNN-based models result in better ASR performance and a higher relative WER reduction than comparable GMM-based models. Once we are able to test ASR performance using LMs specifically developed for the dictation scenarios in AALFred (composing messages and agenda appointments), which â€“ from the language point of view â€“ are much harder ASR tasks than â€œpredictingâ€� newspaper sentences, the gains obtained from the acoustic model optimisation will be even higher. Conversely, the gains are expected to be lower in the case of commands, which are usually easy to recognise using relatively simple grammars. For now, we do not have suitable or enough data for the PaeLife languages to run experiments representing such scenarios. 4. Multilingual Speech-Enabled Interaction in AALFred In this section, we briefly describe the implementation of the speech modality in AALFred: a generic speech modality component that works decoupled from AALFred services, the interpretation of speech input using Spoken Language Understanding (SLU), and support for the fast integration of new languages by automatically deriving the first versions of semantic and ASR grammars. 4.1. Generic Speech Modality Component AALFred is based on a MultiModal Interaction (MMI) framework16, which follows the W3C recommendation of a multimodal architecture17. The major components of the architecture that are relevant for this paper include the modalities and the interaction manager (IM), which controls the HCI-related information. Communication between the modalities and the IM is based on events (life-cycle events17), and information is encoded for transmission using a mark-up language (Extensible MultiModal Annotation (EMMA)17). One important benefit of this architecture is its decoupled nature; the components are developed independently of the application and, as such, can easily be integrated in other applications. In the PaeLife project, we developed a generic speech modality component18 to support speech-based interaction with AALFred19. One major benefit of the speech modality component is that it is decoupled from the services available in AALFred and can, therefore, easily be integrated in new services (see Fig. 4). Furthermore, it can 290 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 handle multiple languages and is very scalable when it comes to adding new languages (see Section 4.3). Similarly to the touch and gesture modality components, the speech modality component communicates with the IM. Whenever an event occurs in a modality component, the component in question uses an EMMA-encoded message wrapped inside an MMI life-cycle event to send the event information to the IM for processing and, if needed, the IM creates a new MMI life-cycle event with the same EMMA-encoded message to be forwarded to the application (AALFred). For instance, if a user has opened the Agenda service and uses a voice command (e.g. â€œOpen Thursdayâ€�) to request the application to display their schedule for Thursday, the speech modality component will send the corresponding event to the IM, with the semantic output resulting from the SLU processing of the ASR output (see Section 4.2). The IM processes the event and creates a new event to be sent to the application, which then displays the userâ€™s schedule for Thursday (see Fig. 3). Fig. 4 AALFred uses an architecture in which the speech modality is decoupled from the available services. [Main] ([ACTION]) ([HELP]) ; [ACTION] ([AGENDA]) ([APPOINTMENTS]) [...] ; [AGENDA] (agenda) (show my agenda) (go to my agenda) ([CHANGEDATE]) (*open [WEEKDAYS]) [...] ; [WEEKDAYS] ([MONDAY]) [...] ; [THURSDAY] ([Thursday]) ; [...] Fig. 5. Example of a semantic grammar used in AALFred. ASR output: open Thursday Semantic Result: [ACTION].[AGENDA].[WEEKDAYS].[THURSDAY] 291 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 Fig. 6. Example of the semantic tags resulting from the semantic parsing of the ASR output â€œopen Thursdayâ€�. 4.2. Automatic Speech Recognition with Spoken Language Understanding To be able to use speech input to access and operate the AALFred services, we needed a way of robustly extracting the intended meaning of the speech input from the ASR output (i.e. the string of words that the automatic speech recogniser â€œthinksâ€� it â€œheardâ€�). This can be done using an SLU parser, which makes use of semantic grammars. Semantic grammars map possible ASR output to semantic tags that capture the intended meaning of speech input, and the SLU parser parses ASR output into a sequence of semantic tags defined in the grammars. The tags are then used by the application to produce the desired actions (e.g. opening the Agenda service). We chose to use the Phoenix20 parser and grammar specification format. This is because the Phoenix parser has been designed to be robust to errors in ASR output and disfluencies in speech. Let us consider SLU in AALFred. Whenever a user speaks and the ASR engine recognises the speech input, the speech modality component requests the service in question to extract the semantic tags for the ASR output, and sends the tags for the IM to process. Fig. 5 presents an example of a semantic grammar used in AALFred, and Fig. 6 is an example of the semantic tags for the ASR output â€œopen Thursdayâ€�. 4.3. Multilingual Support The speech modality needs semantic grammars for all supported languages. To produce these semantic grammars, we developed a service21 that enables the automatic translation of an English grammar into other languages, for instance, using Microsoft Translator API22. In other words, the developer only needs to create a semantic grammar for English, and this grammar can then automatically be translated into other languages. The semantic tags are the same for all languages, i.e., the tags in the English grammar are copied over to the automatically translated grammars. The ASR grammars, which are used by the automatic speech recognisers during recognition time (cf. Section 3), are generated by extracting all possible commands from the semantic grammars. Due to the limitations of machine translation, the grammar translation service also supports the manual revision and correction of the automatically translated grammars. Furthermore, AALFred is capable of dynamically updating grammars based on user input (e.g. when a user adds new contacts in the Contacts service). Of course, the semantic and ASR grammars could also be created manually. However, the grammar translation service that we developed allows for a fast integration of new languages into AALFred. 5. Conclusions In this paper, we described the work done in the area of multilingual automatic speech recognition in the scope of the PaeLife project10, which had the goal of developing a multimodal virtual personal life assistant to help the elderly remain active and socially integrated. The personal life assistant, AALFred, supports four European languages: French, Hungarian, Polish and Portuguese. We implemented the speech modality of AALFred as a generic component that is decoupled from the available AALFred services and can, therefore, be easily integrated in any future services. To be able to quickly increase the number of supported languages, we developed a grammar generation service that allows the automatic translation of English-language grammars into other languages, and supports the manual verification and correction of these automatically translated grammars. To provide the best possible user experience for the target audience, the automatic speech recognisers used in AALFred were optimised for elderly speech. For this purpose, we collected large corpora of elderly speech for all the supported languages. The results of our experiments show that the optimised speech recognisers can provide a considerable improvement in automatic speech recognition performance on elderly speech as compared with standard speech recognisers tuned for younger adult speech. Furthermore, the collected corpora are valuable resources for the international speech community. They will soon be available for research and development purposes through the Linguistic Data Consortium (LDC)23. 292 Annika HÃ¤mÃ¤lÃ¤inen et al. / Procedia Computer Science 67 ( 2015 ) 283 â€“ 292 Acknowledgements Authors acknowledge the funding from AAL JP and national agencies: MLDC was funded by the Portuguese Government through the Ministry of Science, Technology and Higher Education (MCES); University of Aveiro was funded by FEDER, COMPETE and FCT in the context of AAL/0015/2009 and IEETA Research Unit funding FCOMP-01-0124-FEDER-022682 (FCT-PEstC/EEI/UI0127/2011)). BME acknowledges the support of the FuturICT project (TÃ�MOP-4.2.2.C-11/1/KONV-2012-0013) and the PaeLife project (AAL-08-1-2001-0001). References 1. Teixeira, V., Pires, C., Pinto., F., Freitas, J., Dias, M.S., Mendes Rodrigues, E. Towards elderly social integration using a multimodal human-computer interface. In Proc. Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Vilamoura, Portugal; 2012. 2. Bernsen, N.O. Towards a tool for predicting speech functionality. Speech Communication 1997; 23(3):181-210. 3. Xue, S.A., Hao, G.J. Changes in the human vocal tract due to aging and the acoustic correlates of speech production: A pilot study. Journal of Speech, Language, and Hearing Research 2003; 46:689-701. 4. Wilpon, J.G., Jacobsen, C.N. A study of speech recognition for children and the elderly. In Proc. ICASSP, Atlanta, GA, USA; 1996. 5. Vipperla, R., Renals, S., Frankel, J. Longitudinal study of ASR performance on ageing voices. In Proc. Interspeech, Brisbane, Australia; 2008. 6. Living Usability Lab. [Online]. Available: [Accessed: 5-Feb-2015]. 7. Smartphones for Seniors. [Online]. Available: [Accessed: 5-Feb-2015] 8. Project ROMEO. [Online]. Available: [Accessed: 5-Feb-2015] 9. Zora. [Online]. Available: [Accessed: 5-Feb-2015] 10. PaeLife: Personal Assistant to Enhance the Social Life of Seniors. [Online]. Available: [Accessed: 5- Feb-2015] 11. Francisco, P., Almeida, N., Pereira, C., Silva, S. Services to support use and development of speech input for multilingual multimodal applications for mobile scenarios. In Proc. ICIW, Paris, France; 2014. 12. HÃ¤mÃ¤lÃ¤inen, A., Avelar, J., Rodrigues, S., Dias, M.S., KolesiÅ„ski, A., FegyÃ³, T., NÃ©meth, G., CsobÃ¡nka, P., Lan, K., Hewson, D. The EASR Corpora of European Portuguese, French, Hungarian and Polish Elderly Speech. In Proc. LREC, Reykjavik, Iceland; 2014. 13. Microsoft Speech Platform 11.0. [Online]. Available: [Accessed: 12-Feb-2015]. 14. TarjÃ¡n, B., SÃ¡rosi, G., FegyÃ³, T., Mihajlik, P. Improved recognition of Hungarian call center conversations. In Proc. SpeD, Cluj-Napoca, Romania; 2013. 15. Dahl, G.E., Yu, D., Deng, L., Acero, A. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special Issue on Deep Learning for Speech and Language Processing 2012; 20(1):33-42. 16. Almeida, N., Teixeira, A. Enhanced interaction for the elderly supported by the W3C multimodal architecture. In Proc. InteracÃ§Ã£o, Vila Real, Portugal; 2013. 17. Bodell, M., Dahl, D., Kliche, I., Larson, J., Porter, B. Multimodal Architecture and Interfaces. arch/; 2012. 18. Almeida, N., Silva, S., Teixeira, A. Design and development of speech interaction: A methodology. In Proc. HCI International, Crete, Greece; 2014. 19. Teixeira, A., HÃ¤mÃ¤lÃ¤inen, A., Avelar, J., Almeida, N., NÃ©meth, G., FegyÃ³, T., ZainkÃ³, C., CsapÃ³, T., TÃ³th, B., Oliveira, A., Dias, M.S. Speech-centric multimodal interaction for easy-to-access online services: A personal life assistant for the elderly. In Proc. DSAI, Vigo, Spain; 2013. 20. Ward, W. Understanding spontaneous speech: The Phoenix system. In Proc. ICASSP, Toronto, Canada; 1991. 21. Teixeira, A., Francisco, P., Almeida, N., Pereira, C., Silva, S. Services to support use and development of speech input for multilingual multimodal applications for mobile scenarios. In Proc. ICIW, Paris, France; 2014. 22. Microsoft Translator API. [Online]. Available: [Accessed: 12-Feb- 2015]. 23. Linguistic Data Consortium. [Online]. Available: [Accessed: 12-Feb-2015]. CT in the context of AAL/0015/2009 and IEETA Research Unit funding FCOMP-01-0124-FEDER-022682 (FCT-PEstC/EEI/UI0127/2011)). BME acknowledges the support of the FuturICT project (TÃ�MOP-4.2.2.C-11/1/KONV-2012-0013) and the PaeLife project (AAL-08-1-2001-0001). References 1. Teixeira, V., Pires, C., Pinto., F., Freitas, J., Dias, M.S., Mendes Rodrigues, E. Towards elderly social integration using a multimodal human-computer interface. In Proc. Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Vilamoura, Portugal; 2012. 2. Bernsen, N.O. Towards a tool for predicting speech functionality. Speech Communication 1997; 23(3) PROCS 7136 S1877-0509(15)03118-X 10.1016/j.procs.2015.09.272 ☆ Peer-review under responsibility of organizing committee of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2015). Multilingual Speech Recognition for the Elderly: The AALFred Personal Life Assistant Annika Hämäläinen a b António Teixeira c Nuno Almeida c Hugo Meinedo a Tibor Fegyó d Miguel Sales Dias a b a Microsoft Language Development Center, Lisbon, Portugal Microsoft Language Development Center, Lisbon, Portugal b ISCTE – University Institute of Lisbon (ISCTE-IUL), Lisbon, Portugal ISCTE – University Institute of Lisbon (ISCTE-IUL), Lisbon, Portugal c Department of Electronics, Telecommunications & Informatics/IEETA, University of Aveiro, Aveiro, Portugal Department of Electronics, Telecommunications & Informatics/IEETA, University of Aveiro, Aveiro, Portugal d Department of Telecommunications & Media Informatics, Budapest University of Technology & Economics, Budapest, Hungary Department of Telecommunications & Media Informatics, Budapest University of Technology & Economics, Budapest, Hungary The PaeLife project is a European industry-academia collaboration in the framework of the Ambient Assisted Living Joint Programme (AAL JP), with a goal of developing a multimodal, multilingual virtual personal life assistant to help senior citizens remain active and socially integrated. Speech is one of the key interaction modalities of AALFred, the Windows application developed in the project; the application can be controlled using speech input in four European languages: French, Hungarian, Polish and Portuguese. This paper briefly presents the personal life assistant and then focuses on the speech-related achievements of the project. These include the collection, transcription and annotation of large corpora of elderly speech, the development of automatic speech recognisers optimised for elderly speakers, a speech modality component that can easily be reused in other applications, and an automatic grammar translation service that allows for fast expansion of the automatic speech recognition functionality to new languages. Keywords ageing automatic speech recognition elderly human-computer interaction multilingual multimodal speech. References [1] Teixeira, V., Pires, C., Pinto., F., Freitas, J., Dias, M.S., Mendes Rodrigues, E. Towards elderly social integration using a multimodal human-computer interface. In Proc. Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Vilamoura, Portugal; 2012. [2] Bernsen, N.O. Towards a tool for predicting speech functionality. Speech Communication 1997; 23(3):181-210. [3] Xue, S.A., Hao, G.J. Changes in the human vocal tract due to aging and the acoustic correlates of speech production: A pilot study. Journal of Speech, Language, and Hearing Research 2003; 46:689-701. [4] Wilpon, J.G., Jacobsen, C.N. A study of speech recognition for children and the elderly. In Proc. ICASSP, Atlanta, GA, USA; 1996. [5] Vipperla, R., Renals, S., Frankel, J. Longitudinal study of ASR performance on ageing voices. In Proc. Interspeech, Brisbane, Australia; 2008. [6] Living Usability Lab. [Online]. Available: [Accessed: 5-Feb-2015]. [7] Smartphones for Seniors. [Online]. Available: [Accessed: 5-Feb-2015]. [8] Project ROMEO. [Online]. Available: [Accessed: 5-Feb-2015]. [9] Zora. [Online]. Available: [Accessed: 5-Feb-2015] . [10] PaeLife: Personal Assistant to Enhance the Social Life of Seniors. [Online]. Available: [Accessed: 5-Feb-2015]. [11] Francisco, P., Almeida, N., Pereira, C., Silva, S. Services to support use and development of speech input for multilingual multimodal applications for mobile scenarios. In Proc. ICIW, Paris, France; 2014. [12] Hämäläinen, A., Avelar, J., Rodrigues, S., Dias, M.S., Kolesiński, A., Fegyó, T., Németh, G., Csobánka, P., Lan, K., Hewson, D. The EASR Corpora of European Portuguese, French, Hungarian and Polish Elderly Speech. In Proc. LREC, Reykjavik, Iceland; 2014. [13] Microsoft Speech Platform 11.0. [Online]. Available: [Accessed: 12-Feb-2015]. [14] Tarján, B., Sárosi, G., Fegyó, T., Mihajlik, P. Improved recognition of Hungarian call center conversations. In Proc. SpeD, Cluj-Napoca, Romania; 2013. [15] Dahl, G.E., Yu, D., Deng, L., Acero, A. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special Issue on Deep Learning for Speech and Language Processing 2012; 20(1):33-42. [16] Almeida, N., Teixeira, A. Enhanced interaction for the elderly supported by the W3C multimodal architecture. In Proc. Interacção, Vila Real, Portugal; 2013. [17] Bodell, M., Dahl, D., Kliche, I., Larson, J., Porter, B. Multimodal Architecture and Interfaces. 2012. [18] Almeida, N., Silva, S., Teixeira, A. Design and development of speech interaction: A methodology. In Proc. HCI International, Crete, Greece; 2014. [19] Teixeira, A., Hämäläinen, A., Avelar, J., Almeida, N., Németh, G., Fegyó, T., Zainkó, C., Csapó, T., Tóth, B., Oliveira, A., Dias, M.S. Speech-centric multimodal interaction for easy-to-access online services: A personal life assistant for the elderly. In Proc. DSAI, Vigo, Spain; 2013. [20] Ward, W. Understanding spontaneous speech: The Phoenix system. In Proc. ICASSP, Toronto, Canada; 1991. [21] Teixeira, A., Francisco, P., Almeida, N., Pereira, C., Silva, S. Services to support use and development of speech input for multilingual multimodal applications for mobile scenarios. In Proc. ICIW, Paris, France; 2014. [22] Microsoft Translator API. [Online]. Available: [Accessed: 12-Feb-2015]. [23] Linguistic Data Consortium. [Online]. Available: [Accessed: 12-Feb-2015]. "
    },
    {
        "doc_title": "Trip 4 All: A Gamified App to Provide a New Way to Elderly People to Travel",
        "doc_scopus_id": "84962809876",
        "doc_doi": "10.1016/j.procs.2015.09.274",
        "doc_eid": "2-s2.0-84962809876",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Design approaches",
            "Easy-to-use products",
            "Elderly people",
            "Innovative technology",
            "Iterative development",
            "Mobile applications",
            "Social integrations",
            "Virtual assistants"
        ],
        "doc_abstract": "© 2015 The Authors.Older adults have much to gain from bringing technology into their daily lives. The extent to which this is possible strongly depends on careful design and accessible, easy to use products, developed using an elderly centered methodology. The senior tourism is a market in expansion and the old travelers need new and innovative technologies to help and support their trips. These technologies should contribute to a fun and safe experience, while promoting feelings of pleasure and self realization. In this paper we follow this design approach and put it to the test in developing the \"Trip 4 All\"(T4A), an application that works as a gamified virtual assistant to the elderly during a walking tourist visit. The gamified interaction with the visited environment intend to improve motivation to accomplish the visit and make the content absorption more fun and easier. The T4A works on georeferenced maps where the users' geoposition is a trigger to launch storytelling content and/or challenges based on the aspects of the visited site as such: geographical, art, religious, historic, cultural and human. The success in the challenges give the user prizes, new resources and abilities to try more complex challenges that brings more valuable prizes and so on. Furthermore, the proposed application intend to work as a companion that provides self confidence, support and social integration to elderly tourists.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2015-11-06 2015-11-06 2015-11-06 2015-11-06 2016-01-06T20:55:21 S1877-0509(15)03120-8 S1877050915031208 10.1016/j.procs.2015.09.274 S300 S300.2 HEAD-AND-TAIL 2016-01-06T16:14:49.492416-05:00 0 0 20150101 20151231 2015 2015-11-06T03:04:30.753283Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 true 67 67 C Volume 67 34 301 311 301 311 2015 2015 2015-01-01 2015-12-31 2015 Proceedings of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion Dr. Carlos Velasco Dr. Gerhard Weber Dr. João Barroso Dr. Yehya Mohamad Dr. Hugo Paredes article fla Copyright © 2015 The Authors. Published by Elsevier B.V. TRIP4AGAMIFIEDAPPPROVIDEANEWWAYELDERLYPEOPLETRAVEL SIGNORETTI A CASTELLA 2005 27 J TEIXEIRA 2014 389 397 A BALLAGAS 2008 244 261 R FERREIRA 2012 4 S CASIMIRO 2012 400 409 J AHMAD 2010 185 196 W SIGNORETTIX2015X301 SIGNORETTIX2015X301X311 SIGNORETTIX2015X301XA SIGNORETTIX2015X301X311XA Full 2015-09-26T00:02:03Z ElsevierWaived OA-Window item S1877-0509(15)03120-8 S1877050915031208 10.1016/j.procs.2015.09.274 280203 2016-01-06T16:14:49.492416-05:00 2015-01-01 2015-12-31 true 2283284 MAIN 11 58257 849 656 IMAGE-WEB-PDF 1 Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 Available online at www.sciencedirect.com 1877-0509 Â© 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of organizing committee of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2015) doi: 10.1016/j.procs.2015.09.274 ScienceDirect 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Infoexclusion (DSAI 2015) Trip 4 All: A Gamiï¬�ed App to Provide a New Way to Elderly People to Travel Alberto Signorettia,âˆ—, Ana I. Martinsb,c, Nuno Almeidab,c, Diogo Vieirab,c, Ana Filipa Rosab,c, Carlos M. M. Costad, AntoÂ´nio Texeirab,c aDepartment of Informatics of Natal (DI/CAN), State University of Rio Grande do Norte, Av. Ayrton Senna s/n, Natal, 59056-400, Brasil bDepartment of Electronics Telecommunications and Informatics (DETI), University of Aveiro, C. Santiago, Aveiro, 3810-193, Portugal cInstitute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro, C. Santiago, Aveiro, 3810-193, Portugal dDepartment of Economics, Management and Industrial Engineering (DEGEI), University of Aveiro, C. Santiago, Aveiro, 3810-193, Portugal Abstract Older adults have much to gain from bringing technology into their daily lives. The extent to which this is possible strongly depends on careful design and accessible, easy to use products, developed using an elderly centered methodology. The senior tourism is a market in expansion and the old travelers need new and innovative technologies to help and support their trips. These technologies should contribute to a fun and safe experience, while promoting feelings of pleasure and self realization. In this paper we follow this design approach and put it to the test in developing the â€œTrip 4 Allâ€�(T4A), an application that works as a gamiï¬�ed virtual assistant to the elderly during a walking tourist visit. The gamiï¬�ed interaction with the visited environment intend to improve motivation to accomplish the visit and make the content absorption more fun and easier. The T4A works on georeferenced maps where the usersâ€™ geoposition is a trigger to launch storytelling content and/or challenges based on the aspects of the visited site as such: geographical, art, religious, historic, cultural and human. The success in the challenges give the user prizes, new resources and abilities to try more complex challenges that brings more valuable prizes and so on. Furthermore, the proposed application intend to work as a companion that provides self conï¬�dence, support and social integration to elderly tourists. cÂ© 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of organizing committee of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2015). Keywords: Active Aging, Elderly-centred design, Mobile application evaluation, Iterative development method; âˆ— Corresponding author. Tel.: +351234370520 ; Fax: +351234370545. E-mail address: alberto.signoretti@ua.pt or albertosignoretti@uern.br 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license rg/licenses/by-nc-nd/4.0/). Peer-review under responsibility of organizing committee of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2015) 302 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 1. Introduction 1.1. Concept of Senior Tourist The World Tourism Organization1 deï¬�nes tourists as people â€œtraveling to a country other than that in which s/he has his/her usual residence but outside his/her usual environment for a period not exceeding 12 months and whose main purpose of visit is other than the exercise of an activity remunerated from/with the country visited, and who stay at least one night in a collective or private accommodation in the country visitedâ€�1. The age at which a tourist is deï¬�ned as senior is no consensus among authors and organizations. AleÂ´n2 deï¬�nes senior tourist as tourist with more than 65 years, but other institution deï¬�ne a senior tourist as a tourist with more than 50, 55 or 60 years. This age is associated with the decrease of professional activity and family responsibilities as caring for children. 1.2. The Elderly Tourism and Traveling Market Currently more than 128 million citizens in the European Union are aged between 55 and 80 years, representing about 25% of the total population3. Worldwide, the proportion of people over 60 years is growing as a result of the decline in fertility rates and increased life expectancy, due to improvements in nutrition, in basic care and health care, and control of many infectious diseases4 The increase in the number and proportion of elderly is accompanied by a change in the age structure of the population. It is expected that in 2050, globally, the number of elderly will exceed the number of young people under 15 years5. All European countries are facing a similar demographic trend3. This demographic development is having a considerable impact on the increase of the tourism demand6. During the recent economic and ï¬�nancial crisis in Europe, the over 65 age group signiï¬�cantly contributed to counterbalance this negative impact: between 2006 and 2011 the number of tourists dropped in all age groups except for the over 65, posting a 10% increase over 20066. In 2011, the over 65 group made 29% more trips and 23% more overnight stays than ï¬�ve years earlier. Their tourism expenditure grew by 33% and accounted for 20% of all tourism spending of Europeans, compared with just 15% in 20066. At the same time, the senior touristic market quota is in expansion, and it is going to be even more accentuated in new generations, in contrast with the major part of the present older population, being much used to travel, which shall be reï¬‚ected in their future behaviors, even in more advanced ages7. In general, older people in retirement ages have more free time and are willing to spend it on tourism. Increasingly, as a result of the changes in health care and society, seniors are healthier and wealthier than in previous generations in many European countries. Besides that, the tendency is that this population show an increase in savings and assets with fewer ï¬�nancial commitments, especially in the early years of retirement. They also tend to be increasingly quality conscious and demanding, particularly committed with safety, responsible and sustainable services and infrastructures. Nevertheless, there are also seniors with less purchasing power and seniors with health problems. In fact, seniors are rather a heterogeneous group of individuals with diï¬€erent needs and motivations6. Seniors are more ï¬‚exible in travel patterns and take advantage of low seasons oï¬€ers that appeal senior travellers, such as less congested facilities and lower prices6. Nowadays seniors present a much more agile proï¬�le, much wider interests and a bigger desire to experience new things, until an advanced age. The European Commission considers that the contribution of senior citizens to the European tourism industry is indeed signiï¬�cant and should be reinforced to face the challenge of seasonality, stimulating economic growth and jobs in Europe3. This is still a new market area as just 3 out of 10 senior European citizens travel abroad. Therefore there is clearly a high potential for increasing the number of travels undertaken by this segment of the market6. The motivations more frequently referred on behalf of old people to enjoy holidays are: get away from routine, rest and relax, social motives /socialize (as well as to do new friendships as to be in family), as well as cultural motives (to enlarge horizons, visit new places, experience new things) and the carrying out physical activities, mainly outdoor8. 1.3. The Traveling Experience The creation of more services and tourist activities, including individual and group tourism, is a necessity at the des- tinations during the low and medium seasons6. The tourist demand of the elderly focuses on varied types of tourism. 303 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 In terms of traditional tourism the most common are: cultural, urban, religious, rural, cruises and health/thermal tourism9. On the other hand, there is also a kind of tourism whose character, in the case of senior tourism, can be considered innovative: adventure tourism, eco-tourism, trips for tourists with special medical needs, educational trips and inter-generational travel. The strong potential of this tourism segment requires a diï¬€erentiated and appropriate approach by the tourism industry and other intervening factors in the development of this sector. This approach should be supported by a thor- ough knowledge of the characteristics, needs and expectations of older tourists. A diversiï¬�ed combination of activities that include, among others, activities that promote the knowledge of culture of the visited destination, activities that encourage a high interaction and social interaction and open door activities. As travelers get older, they tend to become less self-conï¬�dent, more apprehensive and concerned about safety and unexpected changes in travel arrangements. Elderly travelers prefer to travel with escorted and credible groups. Permanent accompanying of seniors by people with training in languages and that have a good knowledge of the spoken language in the visited destination and the own destination may also be appreciated by the senior population8. In this situation, a virtual companion that works as a virtual assistant for the travel seems to be a good strategy. 1.4. Why Gamiï¬�cation? Considering the report published by Gartener Group in 20124, by 2015 more than 50% of innovative organizations will use gamiï¬�ed process, and more than 70% of the biggest organizations of the world will use, at least, one applica- tion based in games in their business. This report, calculated the gamiï¬�cation market in some about US $2.8 billion dollars10,11. The authors of this paper did not ï¬�nd any report of Gartener Group conï¬�rming or refuting this prediction, but the game market is rising substantially in the past years and, with this growth, the term â€�gamiï¬�cationâ€� became a very important subject when the discussion is about peopleâ€™s motivation. The Y generation phenomenon (people born between years of 1980 and 2000) is not new and is a challenge for educators as for CEOs. This people was raised playing games and using the game thinking in a lot of life ï¬�elds, and represent something about 25% of economically active population. Nowadays they are leading some companies in the world and, for they, think about a game as a business process is not something unreal or unexpected11,12. Gamiï¬�cation refers to the process of using game thinking and game mechanic for traditional solving problem process or for engaging. In other words, it is the use of game elements in a non-game context. The use of these techniques has became common to achieve behavioral changes especially when it comes to encouraging people to adopt new technologies and/or methods to accomplish their tasks. This is forcing to change the traditional model of design focused on functionality for the design focused on human needs. Unlike the game design aimed at the sole purpose of entertaining, the gamiï¬�cation intend to use the mechanism of the games to transform or develop new behaviors11,12. Johan Huizinga13 established that the act of play is set in the most diverse social relations, such as politics, work, poetry and even the nature. Thus, the game is a cultural element which is much more than a physiological phenomenon or a psychological reï¬‚ection14. According to Maslowâ€™s hierarchy of needs theory15, gaming could be related to the top position since it is classiï¬�ed as a desirable activity, but not essential for survival. Ysmar11 points out: â€œit is understandable that we have created games, as they quench more easily, quickly, and eï¬ƒciently clear this constant search that plagues us for securing or meet goals when our relations and day by day work is composed by fuzzy rules, no feedback and rewards that are too small or normally absentâ€�. Bernard Suits, in his book The Grasshopper: Games, Life and Utopia16, says that â€œplaying a game is the voluntary attempt to overcome unnecessary obstaclesâ€� For the design of an application for the senior tourism it is perceived that the target audience is not deï¬�ned in the data presented above. This population certainly did not have access to various technological equipment which their grandchildren are familiar. These people however also had their childhood and also had times when games were very important things. Within this context and considering the fact that the technology is now an everyday factor in everyoneâ€™s life, the development of an application whose experience provided to elderly users is fun, engaging, intuitive, motivating and socializing, is a real challenge. But, from the point of view of an active aging process, is a challenge that will decrease the technical gap between them and their grandchildren. 304 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 1.5. An Alternate Reality Game As mentioned before15, the games are connected to the search of the human being by the feeling of satisfaction and happiness. Therefore, the inclusion of gamiï¬�cation in daily activities can be a way to provide to individuals a set of good experiences, basic condition for what is known as happiness17,18. McGonigal19 describes three basic attributes of the rewards found in games that meet the requirement of meeting basic psychosocial needs of human beings: 1. (More) Satisfactory Work: the tasks oï¬€ered in games are oriented by clear goals, achievable and motivating on which it is always possible to see the direct impact of the eï¬€orts. This relates to the persistent and obstinate behavior shown by players. This type of behavior faces failures as necessary steps to achieve victory and not as a failure situation. 2. Social Connectivity: possibility of creating social ties among participants. This feature is common in online games where users form communities. 3. Adherence to socially relevant projects: associated with the communities, raise collaborative sense to join forces in favor of the major causes instead of personal interests both within the ï¬�ctional universe of games like in the real universe of players Since our users are elderly from whom is not expected knowledge or use of current technology, the choice of games that can work together real and virtual reality seems to be the best for obtaining the engagement of these new players. These kind of games are called Alternative Reality Games (ARGs) and they are considered an interesting way to conduct a gamiï¬�cation of some real activity or task. The purpose of the T4A is to encourage and motivate the elderly to create the behavior (or change an existing one) to visit tourist sites on foot. In other words, the ARG used as a process of gamiï¬�cation has the main objective to get the engagement and motivation of the elderly user. For this to happen it is necessary to understand the reasons why the games get a motivated and engaged behavior of its members, and for that, the BJ Foggâ€™s Behavior Model (FBM)was used20. It is a valuable tool for determining why people react in certain ways, or even why they fail to react in the manner expected. As Fogg notes: â€œif users are not performing a target behaviour, such as rating hotels on a travel web site (or completing an eLearning course), the FBM helps designers to see what psychological element is lackingâ€�. According to the FBM there are three key elements involved to inï¬‚uence the activation of a targeted behavior: motivation, ability and triggers. The activation demands that these three elements are provided simultaneously20. Users may be able to solve a problem, but if there is no motivation for them to do so, why should they? The user can be motivated but this is absolutely not suï¬ƒcient to do something he/she is not capable of (ability). But motivation and ability alone are not enough to determine behavior. There is the need of something that prompts the user to complete a certain action within a deï¬�ned time-frame (trigger). 1.5.1. The T4A Gamiï¬�cation The gamiï¬�cation process of the application T4A is based on a platform called RPGT4A. The RPGT4A is a platform for development of Role Playing Games (RPG)21 that can be used as a tool to support the process of engagement of people, preferably elderly, in virtually assisted touristic tours by foot. The platform consists of a set of hierarchically interlinked georeferenced maps. The application interacts with the player using the tourist real time location provided by the geolocation smartphone system. The maps are divided into several areas of interest that make the interaction occur. Each area of interest may be related to multimodal storytelling information based on text, sound, image or video, as well as related to challenges. The exploitation of the areas of interest of the maps will cause the application to give the player the possibility to have access to information and challenges based on a story that takes into account geographic, artistic, religious, historical, cultural and human aspects of the places that are been visited. The success in the proposed challenges is rewarded with points, improving the player score and unlocking new and more complex challenges to access more valuable prizes. Thus, the character that represents the player (avatar) is improved at each step with more resources to help address the new challenges presented. The challenges are considered as minigames coupled to the main application, allowing each challenge to be treated as an independent application. That is, respecting the communication protocol between the main application and the 305 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 minigame, generic challenges may be attached to the platform without any changes in the core application. This ï¬‚exibility allows the same content to be approached by many diï¬€erent challenges whose creation can be left to third parties. This allows not only strengthening of the content but also the diversity that makes the activity becomes interesting to play. The main application will use a multiagent system based on agents with aï¬€ective perception focus implementing a virtual assistant22,23,24. Based on the userâ€™s proï¬�le and context (environment), this virtual assistant will oï¬€er the help needed by the user during the visit including personalized information on the site visited. The agents with aï¬€ective perception focus allows customization of agent behavior to make it compatible with each userâ€™s proï¬�le. Additionally, the assistant will be based on the multimodal architecture presented by Teixeira25. 1.6. Related Works Applications using the userâ€™s geoposition as a trigger are not new as the REXplorer project26 demonstrates. This is an interesting project of a pervasive game for tourists. Following the experience developed on 2007 and 2008 in the city of Regensburg, Germany, some guidelines that helped the design of the T4A were found, as such: about the sequencing problems of storytelling associated with the site seeing, the hot zones associated with the georeferenced map and the importance of the game not became more important than the touristic view. The T4A developed an platform to store all the information about the storytelling and the challenges using these guidelines. The result was a platform that allows use the application in any site and with diï¬€erent gamiï¬�cation processes. Furthermore, The T4A application was developed using the elderly centered design and work as a customized virtual assistant. Others expirences that worth to be cited are the Travel plot Porto27, the Tripzoon project28 although it is not directed to tourists, but it is about walking behavior, and the applications JiTT City Guides 1 developed by iClio 2 that works as managers for audio storytelling content. All these works use gamiï¬�cation to improve the userâ€™s experience but none of then were developed using an elderly centered design and, also, none of then intent to be a customized virtual assistant for the user. 2. Elderly Centered Development Method Over the First Prototype The development method was made according with a three phase spiral: obtaining the requirements (phase 1), prototype creation (phase 2) and evaluation (phase 3). This iterative methodology continues with additional cycles of requirements, prototypes and evaluations towards an increasingly reï¬�ned application as shown in Figure 1. Each phase of the development method was accompanied by a speciï¬�c evaluation process based on the method- ology described in29. This evaluation methodology consists in submitting each phase (requirements, prototype and evaluation) to a evaluation process based in the following steps: conceptual validation, prototype test and pilot test. In what follows, a general description of the overall aspects involved in each of the phases of the development method is described below. 2.1. Requirements This phase intended to verify if the idea of the T4A was sustainable in terms of functionalities to be included and interface trough the implementation of methodologies such as personas and brainstorming. â€¢ The persona was constructed based on the literature about elderly touristsâ€™, as described in Section 1.2. â€¢ The brainstorming was planned and oriented in accordance with Rawlinson30. The results of both this methodologies were used to decide the requirements of the very ï¬�rst prototype of T4A. For the next spiral cycle this process will be repeated based on the results of the evaluation process applied on the evaluation phase of the development method. 1 2 306 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 Fig. 1. The methodology consists in an iterative process, in which multiple development cycles increasingly reï¬�ne requirements and prototypes. 2.2. Prototype The T4A prototype was developed as a way to mediate the interaction between the end users and the developers31. This will be accomplished using the diï¬€erent evolutions of the application prototype. Without discarding the possibil- ity of changes along the diï¬€erent iterations, to better accommodate the reï¬�ned requirements, the architecture initially adopted for the prototype should account, as best as possible, for the incremental nature of the work to be carried out, minimizing the development eï¬€ort and time. 2.3. Evaluation During the development of an application, designed taking into account the accessibility and usability features of end users, the application is evaluated several times (several iterations of phases 1 through 3). As described in29 the prototype test intended to collect information regarding the usability and user satisfaction. The prototype test is conducted in a controlled environment. When this prototype can no longer evolve, and the users are satisï¬�ed with the functionalities and usability, then this prototype is ready for a pilot test. This test intends to evaluate, in addition with usability and satisfaction, the meaning that a product or service has on usersâ€™ lives. For this reason, this last step of testing diï¬€ers from the prototype phase in the context where it happens. The product or service should be installed in usersâ€™ homes and integrated into their daily live routines. 3. Overall System Architecture The Figure 2 shows a basic illustration of the technological structure used in the development of the T4A applica- tion. Fig. 2. The technological structure of the solution T4A. Given the importance of the prototypes, in the described development methodology(Section 2), the chosen archi- tecture should be able to support the incremental reï¬�nement of the prototype, minimizing the development eï¬€orts. To support the development was adopted an architecture based on two main blocks: the smarphone and web services to complement its capabilities. 307 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 To combine more ways of interaction, giving users others possibilities to interact with the application and leading to greater ease of use, the W3CMultimodal Interaction (MMI) Architecture32 was adopted. This is, a loosely coupled and extensible architecture supporting multiple modalities. Modalities can be added or modiï¬�ed without any changes in the application core and this ï¬‚exibility allows the developer of the application to code without having knowledge about the modalities, this way he only have to focus on the applications and other developer would focus only in the modalities. Previous experience33 showed that for instance speech can easily be integrated in our project using this architecture. 3.1. Modalities In the adopted architecture, three modalities were initially consider, the graphical user interface, speech and touch. One of the advantages of working with decoupled modalities is the chance to easily exchange them by improved versions or even by new modalities. This is important because it allows the developers to focus on the application core and use readily available modalities, instead of having to develop them from scratch, which might be a complex task. 3.2. Services Only part of the modalities are implemented in the smartphone and they use cloud based services, this ways completing the work of the modality. Considering the speech modality, for example, the ASR and TTS are provided by services communicating with the modality. The modality sends data (e.g., the audio of a spoken utterance to be recognized or a message with information to be read) and receives the required results. Another important service, which is already considered, is the natural language generator (NLG). It can be used to generate more natural sentences, with particular information. It uses a statistical machine translation system and includes a trained language model that can be adapted depending on the requirements34,35. The data for all users is gathered in the User Service manager, freeing the smartphone from providing persistent storage and allows synchronization between applications and devices. This set of generic services, considered from the start, can be expanded with the addition of specialized services to encompass the gathered requirements. 3.3. Georeferenced Gamifying Platform As previously explained, the T4A application was developed taking into consideration the usersâ€™ geoposition as a trigger. The data structure was designed to be attached to the map used by the main application, so it is possible to use the same structure at any place. The data structure has to be ï¬�lled considering that a Base Map is composed of at least one Sub-Map, each Sub-Map has to have at least one Area of Interest and, ï¬�nally, each Area of Interest has to have at least one Point of Interest (PoI). The use of digital games as an instructional framework has proved to be interesting by allow the playful insertion in the learning process36. The playful, brought by games, helps making the instructional content more interesting, interactive, meaningful and challenging37. The Georeferenced Gamifying Platform called RPGT4A has as main objective the easy and fast development of RPGs (Role Playing Games) that can be used as a support to people engagement process in virtually assisted walking tours. The use of RPG games can provide interactivity and encourage participation in the instructional process38. This type of game is an eï¬€ective tool to assist in the complex issues of questioning peopleâ€™s lives, such as urban violence, social inequality and racial conï¬‚icts, as well as learning curricula39. Figure 3 shows the Georeferenced Gamifying Platform RPGT4A connected hierarchies. As can be seen, the maps hierarchy is connected to the storytelling hierarchy, which in turn, is connected to the challenges hierarchy. The main application of T4A receives this conï¬�guration through a Json 3 ï¬�le. 3 308 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 Fig. 3. Overall architecture of georeferenced gamifying platform used in the prototype. In the next section will be explained the implementation process used for the ï¬�rst prototype of T4A and the content of the georeferenced gamifying platform used. 4. Trip4All - T4A For this ï¬�rst prototype an application area nearby the development lab was chosen in order to avoid long shifts and improve the testing/correction process. The map of campus Santiago of the University of Aveiro was used as the base map for the prototype design. The T4A application will be based on the multimodal architecture presented by Teixeira25. Because of this, the prototype of T4A has been initially developed for Windows Phone 8.1 using the Visual Studio 2013 IDE. To future work the application intends to work in all architectures of mobile devices available on the market. Figure 4 shows the map structure used in the prototype development. The map structure was composed by two sub-maps identiï¬�ed by CSM1 and CSM2. The ï¬�rst sub-map was composed by two areas of interest identi- ï¬�ed by CSM1.CAI1 and CSM1.CAI2. For each of these areas of interest there is a point of interest identiï¬�ed by CSM1.CAI1.PoI1 and CSM1.CAI2.PoI1 respectively. The second sub-map is composed by one area of interest iden- tiï¬�ed by CSM2.CAI1 and this area has one point of interest identiï¬�ed by CSM2.CAI1.PoI1. To start the gamiï¬�ed experience the users needs to: 1) log in; 2) choose location of the visit and 3) start the visit walking through the site. To the extent that the visit is happening the visitorâ€™s geoposition works as a trigger to start the associated storytelling and challenges. Figure 5 shows an example of the concatenation of callbacks considering the visitor is walking through the base map, entering in a sub-map, then entering in a area of interest and, ï¬�nally, achieving a point of interest. For the ï¬�rst prototype a very simple storytelling was deï¬�ned explaining only the basics about the architecture of the buildings pointed by the points of interest. The points of interest were placed in front of the library building, in front of the the main door of the rectory building and in front of the main door of civil engineering department building. The main objective of the prototype is obtain a proof of concept so there are callbacks functions deï¬�ned only for the PoIs. That is, the ï¬�rst three steps shown in Figure 5 do not starts anything in the application, only the fourth step starts a pop up with a storytelling and a link pointing to a challenge when the user achieve the PoI area. The challenges were created using the web language package HTML5/CSS3/JS and they are initialized by the main application as web-views. This approach grants the use a huge quantity of games and features available in the internet and the possibility of easier development of new components for the T4A. Three kinds of challenges were developed for the prototype, a crossword game to be used in the PoI of the library building, a treasure hunt for the PoI of the rectory building and a quiz to be used in the PoI of the civil engineering department. Figure 6 shows some snapshots of the T4A prototype application. 309 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 Fig. 4. Maps hierarchy used in the Prototype. Fig. 5. Concatenation of call backs used in the Prototype. 5. Conclusions and Future Work The main idea behind the Elderly Centered Design Method is the focus in the inclusion of the end-user in the development process since the beginning to achieve faster and more productive iterations. The participation of the user is crucial to generate better ideas in the brainstorming sessions to a more reï¬�ned development of personas and scenarios. In addition, their participation is important to a development of an accurate knowledge about the real needs and capabilities to enable a process of continuous development of the application by successive reï¬�nements and redesign. The application T4A is just starting the evaluating phase of the ï¬�rst cycle of the process requirements, prototyping and evaluating. Because of that, there is still no complete data set to point out the strengths and weaknesses of the current stage of development, but the initial feedback provides the feeling that the work is in the correct way. Considering this prototype as a proof of concept for the Georeferenced Gamifying Platform (discussed in section 3) the main goal was achieved. The design of the platform is functional and can be expanded to be used in any desired site. The hierarchical structure of maps, storytelling and challenges using Json conï¬�guration ï¬�les proved to be easy to use and to maintain. The use of HTML5/CSS3/JS to develop the challenges show that it is possible to incorporate to the main application a lot of new features with little programming eï¬€ort. 310 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 Log In. Instructions. Selection of the Visit. â€œYou are here!â€�. The route of the visit. Storytelling content. Fig. 6. Snap shots of T4A application prototype. With the concept of the Georeferenced Gamifying Platform settled, the new step is the development of a road map for the historical midtown of the city of Aveiro, using as the gamiï¬�ed aspect of the application the history of the salt exploration and the cultural aspects of the moliceiros boats. This will be take place in a partnership with the Tourism Departament of University of Aveiro and City Hall of Aveiro. Acknowledgements This work has the ï¬�nancial support of CNPq (Brazilian Research Council), under process number PDE 201461/2014- 5, was partially funded by Project QREN N 13852 I&D â€œAAL4ALL Ambient Assisted Living for Allâ€�, UI 127/94 IEETA and has received partial funding from European Unions Seventh Framework Programme for research, tech- nological development and demonstration under grant agreement n 610986 (project IRIS, FP7-PEOPLE-2013-IAPP, References 1. United Nations, , World Tourism Organization, . Recommendations on Tourism Statistics. Tech. Rep.; United Nations publication; 2013. 2. AleÂ´n, E., DomÄ±Â´nguez, T., Losada, N.. New opportunities for the tourism market: Senior tourism and accessible tourism. . . . (a cura di) Visions for Global . . . 2012;URL: 3. European Commission, . Tourism for Seniors. 2014. URL: tourism-seniors/index_en.htm. 4. Goasduï¬€, L., Pettey, C.. Gartner says by 2015, more than 50 percent of organizations that manage innovation processes will gamify those processes. 2011. 5. United Nations, Department of Economic and Social Aï¬€airs, Population Division, . World Population Ageing 2013. ST/ESA/SER.A/348. 2013. 6. European Commission, . Europe, the best destination for seniors. Facilitating cooperation mechanisms to increase senior touristâ€™s travels within Europe and from third countries in the low and medium seasons - Experts draft report. Tech. Rep.; Brussels; 2014. 311 Alberto Signoretti et al. / Procedia Computer Science 67 ( 2015 ) 301 â€“ 311 7. Urhausen, J.. Eurostat Report: Tourism in Europe ? Does Age Matter? Tech. Rep.; Eurostat; 2009. 8. EuseÂ´bio, M., Carneiro, M., Kastenholz, E., Alvelos, H.. Potential Beneï¬�ts of the Development of an European Programme of Social Tourism for Seniors. Tech. Rep.; Department of Economy, Management and Industrial Engineering Aveiro University; 2012. URL: http: 9. Cavaco, C.. Turismo seÂ´nior: perï¬�s e praÂ´ticas 2009;URL: 10. Anderson, J.Q., Rainie, H.. Gamiï¬�cation: Experts Expectâ€™Game Layersâ€™ to Expand In the Future, With Positive and Negative Results. Pew Internet & American Life Project; 2012. 11. Ysmar Vianna Mauricio Vianna, B.M.S.T.. Gamiï¬�cation, Inc.: como reinventar empresas a partir de jogos; vol. 1. mjv Press; 1 ed.; 2013. Isbn 978-85-65424-08-0. 12. Ed Boswell Diane Youden, e.a.. The power of the net generation. Tech. Rep.; PricewaterhouseCoopers LLP; 2012. URL: 13. Huizinga, J.H.. Homo Ludens- Study of the Play Element in Culture (International Library of Society. Routledge; 1980. ISBN: 0-7100- 0578-4. 14. Caillois, R., Palha, J.. Os jogos e os homens: a maÂ´scara e a vertigem. Cotovia; 1990. ISBN 9789729013287. URL: google.pt/books?id=NVajOgAACAAJ. 15. Maslow, A.H.. A theory of human motivation. Start Publishing LLC; 2013. 16. Suits, B., Hurka, T.. The Grasshopper: Games, Life and Utopia. Broadview encore editions. Broadview Press; 2005. ISBN 9781551117720. URL: 17. Csikszentmihalyi, M., Csikzentmihaly, M.. Flow: The psychology of optimal experience; vol. 41. HarperPerennial New York; 1991. 18. Seligman, M.E., Csikszentmihalyi, M.. Positive psychology: An introduction.; vol. 55. American Psychological Association; 2000. 19. McGonigal, J.. Reality is broken: Why games make us better and how they can change the world. Penguin; 2011. 20. Fogg, B.. A behavior model for persuasive design. In: Proceedings of the 4th international Conference on Persuasive Technology. ACM; 2009, p. 40. 21. Castella, J.C., Trung, T.N., Boissau, S.. Participatory simulation of land-use changes in the northern mountains of vietnam: the combined use of an agent-based model, a role-playing game, and a geographic information system. Ecology and Society 2005;10(1):27. 22. Xavier-Junior, J.C., Signoretti, A., Canuto, A.M., Campos, A.M., GoncÂ¸alves, L.M., Fialho, S.V.. Introducing aï¬€ective agents in recommendation systems based on relational data clustering. In: Database and Expert Systems Applications. Springer; 2011, p. 303â€“310. 23. Signoretti, A., Feitosa, A., Campos, A.M., Canuto, A.M., Xavier, J., Fialho, S.V.. Using an aï¬€ective attention focus for improving the reasoning process and behavior of intelligent agents. In: Web Intelligence and Intelligent Agent Technology (WI-IAT), 2011 IEEE/WIC/ACM International Conference on; vol. 2. IEEE; 2011, p. 97â€“100. 24. Signoretti, A.. Agentes Inteligentes com Foco de AtencÂ¸aËœo Afetivo em SimulacÂ¸oËœes Baseadas em Agentes. Ph.D. thesis; UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE; 2012. 25. Teixeira, A., HaÂ¨maÂ¨laÂ¨inen, A., Avelar, J., Almeida, N., NeÂ´meth, G., FegyoÂ´, T., et al. Speech-centric multimodal interaction for easy-to- access online servicesâ€“a personal life assistant for the elderly. Procedia Computer Science 2014;27:389â€“397. 26. Ballagas, R., Kuntze, A., Walz, S.P.. Gaming tourism: Lessons from evaluating rexplorer, a pervasive game for tourists. In: Pervasive computing. Springer; 2008, p. 244â€“261. 27. Ferreira, S., Alves, A., Quico, C.. Location based transmedia storytelling: The travelplot porto experience design. Journal of Tourism and Development [Revista Turismo & Desenvolvimento] 2012;17(18):4. 28. Broll, G., Cao, H., Ebben, P., Holleis, P., Jacobs, K., Koolwaaij, J., et al. Tripzoom: an app to improve your mobility behavior. In: Proceedings of the 11th International Conference on Mobile and Ubiquitous Multimedia. ACM; 2012, p. 57. 29. Martins, A.I., QueiroÂ´s, A., Cerqueira, M., Rocha, N., Teixeira, A.. The international classiï¬�cation of functioning, disability and health as a conceptual model for the evaluation of environmental factors. Procedia Computer Science 2012;14:293â€“300. URL: sciencedirect.com/science/article/pii/S1877050912007958. 30. Rawlinson, J.G.. Creative thinking and brainstorming. Gower Farnborough, Hants; 1981. 31. Compagna, D., Kohlbacher, F.. The limits of participatory technology development: The case of service robots in care facilities for older people. Technological Forecasting and Social Change 2014;(0):â€“. 32. Dahl, D.A.. The W3C multimodal architecture and interfaces standard. Journal on Multimodal User Interfaces 2013;7(3):171â€“182. doi:10. 1007/s12193-013-0120-5. 33. Almeida, N., Silva, S., Teixeira, J.S.A.. Design and development of speech interaction: A methodology. In: Human-Computer Interaction. Advanced Interaction Modalities and Techniques Lecture Notes in Computer Science Volume 8511. Springer; 2014, p. 370â€“381. 34. Casimiro, J., Teixeira, A., Pinto, J.S.. Natural language generation in the context of multimodal interaction in Portuguese. ElectroÂ´nica e TelecomunicacÂ¸oËœes 2012;5(4):400â€“409. 35. Casimiro, J., Pinto, J.S., Teixeira, A.. Natural language generation in the context of multimodal interaction in Portuguese. In: 8th Iberian Conference on Informaton Systems and Technologies (CISTI). 2013, . 36. Ahmad, W., Shaï¬�e, A.B., Latif, M.. Role-playing game-based learning in mathematics. The Electronic Journal of Mathematics and Technology 2010;4(2):185â€“196. 37. Medeiros, M.d.O., Schimiguel, J.. Uma abordagem para avaliacÂ¸ao de jogos educativos: enfase no ensino fundamental. In: Anais do SimpoÂ´sio Brasileiro de InformaÂ´tica na EducacÂ¸aËœo; vol. 23. 2012, . 38. KLIMICK, C.. Rpg & educacÂ¸aËœo: metodologia para o uso paradidaÂ´tico dos role playing games. Design MeÂ´todo Rio de Janeiro: Ed PUC-Rio; TeresoÂ´polis: Novas IdeÂ´ias 2006;. 39. MARCATTO, A.. Saindo do quadro: Uma metodologia educacional luÂ´dica e participativa baseada no role playing game. SaËœo Paulo: A Marcatto 1996;. mountains of vietnam: the combined use of an agent-based model, a role-playing game, and a geographic information system. Ecology and Society 2005;10(1):27. 22. Xavier-Junior, J.C., Signoretti, A., Canuto, A.M., Campos, A.M., GoncÂ¸alves, L.M., Fialho, S.V.. Introducing aï¬€ective agents in recommendation systems based on relational data clustering. In: Database and Expert Systems Applications. Springer; 2011, p. 303â€“310. 23. Signoretti, A., Feitosa, A., Campos, A.M., Canuto, A.M., Xavier, J., Fialho, S.V.. Using an aï¬€ective attention focus for improving the reasoning process and behavior of intelligent agents. In: Web Intelligence and Intelligent Agent Technology (WI-IAT), 2011 IEEE/WIC/ACM International Conference on; vol. 2. IEEE; 2011, p. 97â€“100. 24. Signoretti, A.. Agentes Inteligentes com Foco de AtencÂ¸aËœo Afetivo em SimulacÂ¸oËœes Baseadas em Agentes. Ph.D. thesis; UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE; 2012. 25. Teixeira, A., HaÂ¨maÂ¨laÂ¨inen, A., Avelar, J., Almeida, N., NeÂ´meth, G., FegyoÂ´, T., et al. Speech-centric multimodal interaction for easy-to- access online servicesâ€“a personal life assistant for the elderly. Procedia Computer Science 2014;27:389â€“397. 26. Ballagas, R., Kuntze, A., Walz, S.P.. Gaming tourism: Lessons from evaluating rexplorer, a pervasive game for tourists. In: Pervasive computing. Springer; 2008, p. 244â€“261. 27. Ferreira, S., Alves, A., Quico, C.. Location based transmedia storytelling: The travelplot porto experience design. Journal of Tourism and Development [Revista Turismo & Desenvolvimento] 2012;17(18):4. 28. Broll, G., Cao, H., Ebben, P., Holleis, P., Jacobs, K., Koolwaaij, J., et al. Tripzoom: an app to improve your mobility behavior. In: Proceedings of the 11th International Conference on Mobile and Ubiquitous Multimedia. ACM; 2012, p. 57. 29. Martins, A.I., QueiroÂ´s, A., Cerqueira, M., Rocha, N., Teixeira, A.. The international classiï¬�cation of functioning, disability and health as a conceptual model for the evaluation of environmental factors. Procedia Computer Science 2012;14:293â€“300. URL: sciencedirect.com/science/article/pii/S1877050912007958. 30. Rawlinson, J.G.. Creative thinking and brainstorming. Gower Farnborough, Hants; 1981. 31. Compagna, D., Kohlbacher, F.. The limits PROCS 7138 S1877-0509(15)03120-8 10.1016/j.procs.2015.09.274 The Authors ☆ Peer-review under responsibility of organizing committee of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2015). Trip 4 All: A Gamified App to Provide a New Way to Elderly People to Travel Alberto Signoretti a ⁎ Ana I. Martins b c Nuno Almeida b c Diogo Vieira b c Ana Filipa Rosa b c Carlos M.M. Costa d António Texeira b c a Department of Informatics of Natal (DI/CAN), State University of Rio Grande do Norte, Av. Ayrton Senna s/n, Natal, 59056-400, Brasil Department of Informatics of Natal (DI/CAN), State University of Rio Grande do Norte, Av. Ayrton Senna s/n, Natal, 59056-400 Brasil b Department of Electronics Telecommunications and Informatics (DETI), University of Aveiro, C. Santiago, Aveiro, 3810-193, Portugal Department of Electronics Telecommunications and Informatics (DETI), University of Aveiro, C. Santiago, Aveiro, 3810-193 Portugal c Institute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro, C. Santiago, Aveiro, 3810-193, Portugal Institute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro, C. Santiago, Aveiro, 3810-193 Portugal d Department of Economics, Management and Industrial Engineering (DEGEI), University of Aveiro, C. Santiago, Aveiro, 3810-193, Portugal Department of Economics, Management and Industrial Engineering (DEGEI), University of Aveiro, C. Santiago, Aveiro, 3810-193 Portugal ⁎ Corresponding author. Tel.: +351234370520; fax: +351234370545. Older adults have much to gain from bringing technology into their daily lives. The extent to which this is possible strongly depends on careful design and accessible, easy to use products, developed using an elderly centered methodology. The senior tourism is a market in expansion and the old travelers need new and innovative technologies to help and support their trips. These technologies should contribute to a fun and safe experience, while promoting feelings of pleasure and self realization. In this paper we follow this design approach and put it to the test in developing the “Trip 4 All”(T4A), an application that works as a gamified virtual assistant to the elderly during a walking tourist visit. The gamified interaction with the visited environment intend to improve motivation to accomplish the visit and make the content absorption more fun and easier. The T4A works on georeferenced maps where the users’ geoposition is a trigger to launch storytelling content and/or challenges based on the aspects of the visited site as such: geographical, art, religious, historic, cultural and human. The success in the challenges give the user prizes, new resources and abilities to try more complex challenges that brings more valuable prizes and so on. Furthermore, the proposed application intend to work as a companion that provides self confidence, support and social integration to elderly tourists. Keywords Active Aging Elderly-centred design Mobile application evaluation Iterative development method References [1] United Nations,, World Tourism Organization, . Recommendations on Tourism Statistics. Tech. Rep.; United Nations publication; 2013. [2] Alén, E., Doḿınguez, T., Losada, N. New opportunities for the tourism market: Senior tourism and accessible tourism. . . . (a cura di) Visions for Global . . . 2012;URL: [3] European Commission, . Tourism for Seniors. 2014. URL: [4] Goasduff, L., Pettey, C. Gartner says by 2015, more than 50 percent of organizations that manage innovation processes will gamify those processes. 2011. [5] United Nations, Department of Economic and Social Affairs, Population Division, . World Population Ageing 2013. ST/ESA/SER.A/348. 2013. [6] European Commission, . Europe, the best destination for seniors. Facilitating cooperation mechanisms to increase senior tourist's travels within Europe and from third countries in the low and medium seasons - Experts draft report. Tech. Rep.; Brussels; 2014. [7] Urhausen, J. Eurostat Report: Tourism in Europe ? Does Age Matter? Tech. Rep.; Eurostat; 2009. [8] Eusébio, M., Carneiro, M., Kastenholz, E., Alvelos, H. Potential Benefits of the Development of an European Programme of Social Tourism for Seniors. Tech. Rep.; Department of Economy, Management and Industrial Engineering Aveiro University; 2012. URL: http: [9] Cavaco, C. Turismo sénior: perfis e práticas 2009;URL: [10] Anderson, J.Q., Rainie, H. Gamification: Experts Expect’Game Layers’ to Expand In the Future, With Positive and Negative Results. Pew Internet & American Life Project; 2012. [11] Ysmar Vianna Mauricio Vianna, B.M.S.T. Gamification, Inc.: como reinventar empresas a partir de jogos; vol. 1. mjv Press; 1 ed.; 2013. Isbn 978-85-65424-08-0. [12] Ed Boswell Diane Youden, e.a. The power of the net generation. Tech. Rep.; PricewaterhouseCoopers LLP; 2012. URL: [13] Huizinga, J.H. Homo Ludens- Study of the Play Element in Culture (International Library of Society. Routledge; 1980. ISBN: 0-7100-0578-4. [14] Caillois, R., Palha, J. Os jogos e os homens: a ma'scara e a vertigem. Cotovia; 1990. ISBN 9789729013287. URL: google.pt/books?id=NVajOgAACAAJ. [15] Maslow, A.H. A theory of human motivation. Start Publishing LLC; 2013. [16] Suits, B., Hurka, T. The Grasshopper: Games, Life and Utopia. Broadview encore editions. Broadview Press; 2005. ISBN 9781551117720. URL: [17] Csikszentmihalyi, M., Csikzentmihaly, M. Flow: The psychology of optimal experience; vol. 41. HarperPerennial New York; 1991. [18] Seligman, M.E., Csikszentmihalyi, M. Positive psychology: An introduction.; vol. 55. American Psychological Association; 2000. [19] McGonigal, J. Reality is broken: Why games make us better and how they can change the world. Penguin; 2011. [20] Fogg, B. A behavior model for persuasive design. In: Proceedings of the 4th international Conference on Persuasive Technology. ACM; 2009, p. 40. [21] J.C. Castella T.N. Trung S. Boissau Participatory simulation of land-use changes in the northern mountains of vietnam: the combined use of an agent-based model, a role-playing game, and a geographic information system Ecology and Society 10 1 2005 27 [22] Xavier-Junior, J.C., Signoretti, A., Canuto, A.M., Campos, A.M., Gonc¸alves, L.M., Fialho, S.V. Introducing affective agents in recommendation systems based on relational data clustering. In: Database and Expert Systems Applications. Springer; 2011, p. 303-310. [23] Signoretti, A., Feitosa, A., Campos, A.M., Canuto, A.M., Xavier, J., Fialho, S.V. Using an affective attention focus for improving the reasoning process and behavior of intelligent agents. In: Web Intelligence and Intelligent Agent Technology (WI-IAT), 2011 IEEE/WIC/ACM International Conference on; vol. 2. IEEE; 2011, p. 97-100. [24] Signoretti, A. Agentes Inteligentes com Foco de Atenc¸ão Afetivo em Simulac¸ões Baseadas em Agentes. Ph.D. thesis; UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE; 2012. [25] A. Teixeira A. Hämäläinen J. Avelar N. Almeida Németh, G., Fegyó, T., et al. Speech-centric multimodal interaction for easy-to- access online services–a personal life assistant for the elderly Procedia Computer Science 27 2014 389 397 [26] R. Ballagas A. Kuntze S.P. Walz Gaming tourism: Lessons from evaluating rexplorer, a pervasive game for tourists In: Pervasive computing. Springer; 2008 244 261 [27] S. Ferreira A. Alves C. Quico Location based transmedia storytelling: The travelplot porto experience design Journal of Tourism and Development [Revista Turismo & Desenvolvimento] 17 18 2012 4 [28] Broll, G., Cao, H., Ebben, P., Holleis, P., Jacobs, K., Koolwaaij, J., et al. Tripzoom: an app to improve your mobility behavior. In: Proceedings of the 11th International Conference on Mobile and Ubiquitous Multimedia. ACM; 2012, p. 57. [29] Martins, A.I., Queirós, A., Cerqueira, M., Rocha, N., Teixeira, A. The international classification of functioning, disability and health as a conceptual model for the evaluation of environmental factors. Procedia Computer Science 2012;14:293-300. URL: sciencedirect.com/science/article/pii/S1877050912007958. [30] Rawlinson, J.G. Creative thinking and brainstorming. Gower Farnborough, Hants; 1981. [31] Compagna, D., Kohlbacher, F. The limits of participatory technology development: The case of service robots in care facilities for older people. Technological Forecasting and Social Change 2014;(0):–. [32] Dahl, D.A. The W3C multimodal architecture and interfaces standard. Journal on Multimodal User Interfaces 2013;7(3):171-182. doi:10. 1007/s12193-013-0120-5. [33] Almeida, N., Silva, S., Teixeira, J.S.A. Design and development of speech interaction: A methodology. In: Human-Computer Interaction. Advanced Interaction Modalities and Techniques Lecture Notes in Computer Science Volume 8511. Springer; 2014, p. 370-381. [34] J. Casimiro A. Teixeira J.S. Pinto Natural language generation in the context of multimodal interaction in Portuguese Electrónica e Telecomunicac¸ões 5 4 2012 400 409 [35] Casimiro, J., Pinto, J.S., Teixeira, A. Natural language generation in the context of multimodal interaction in Portuguese. In: 8th Iberian Conference on Informaton Systems and Technologies (CISTI). 2013,. [36] W. Ahmad A.B. Shafie M. Latif Role-playing game-based learning in mathematics The Electronic Journal of Mathematics and Technology 4 2 2010 185 196 [37] Medeiros, M.d.O., Schimiguel, J. Uma abordagem para avaliac¸ao de jogos educativos: enfase no ensino fundamental. In: Anais do Simpósio Brasileiro de Informática na Educac¸ão; vol. 23. 2012,. [38] KLIMICK, C. Rpg & educac¸ão: metodologia para o uso paradidático dos role playing games. Design Método Rio de Janeiro: Ed PUC-Rio; Teresópolis: Novas Idéias 2006; [39] MARCATTO, A. Saindo do quadro: Uma metodologia educacional lúdica e participativa baseada no role playing game. São Paulo: A Marcatto 1996;. "
    },
    {
        "doc_title": "Velar movement assessment for speech interfaces: An exploratory study using surface electromyography",
        "doc_scopus_id": "84955320352",
        "doc_doi": "10.1007/978-3-319-26129-4_16",
        "doc_eid": "2-s2.0-84955320352",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Exploratory studies",
            "Movement detection",
            "Nasal vowels",
            "Neck region",
            "Noninvasive methods",
            "Silent speech interfaces",
            "Speech interface",
            "Surface electromyography"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.In the literature several silent speech interfaces based on Surface Electromyography (EMG) can be found. However, it is yet unclear if we are able to sense muscles activity related to nasal port opening/closing. Detecting the nasality phenomena, would increase the performance of languages with strong nasal characteristics such as European Portuguese. In this paper we explore the use of surface EMG electrodes, a non-invasive method, positioned in the face and neck regions to explore the existence of useful information about the velum movement. For an accurate interpretation and validation of the proposed method, we use velum movement information extracted from Real-Time Magnetic Resonance Imaging (RT-MRI) data. Overall, results of this study show that differences can be found in the EMG signals for the case of nasal vowels, by sensors positioned below the ear between the mastoid process and the mandible in the upper neck region.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of complex distributed multimodal applications: Evaluating a TeleRehabilitation system when it really matters",
        "doc_scopus_id": "84949789710",
        "doc_doi": "10.1007/978-3-319-20913-5_14",
        "doc_eid": "2-s2.0-84949789710",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living (AAL)",
            "Dynamic environments",
            "Evaluation",
            "Evaluation methodologies",
            "Hardware and software",
            "Multi-modality",
            "Multimodal application",
            "Telerehabilitation"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.The evaluation of applications or systems within dynamic environments is complex. The existence of multiple hardware and software items which share the same space can provoke concurrency issues and result in erratic interactions. A sudden change within the environment can result is dramatic changes both to the user and application itself which can pass unnoticed in traditional evaluation methodologies. To verify if a component is compatible with a given environment is of paramount importance for areas like pervasive computing, ambient intelligence or ambient assisted living (AAL). In this paper, a semi-automatic platform for evaluation is presented and integrated with a TeleRehabilitation system in an AAL scenario to enhance evaluation. Preliminary results show the advantages of the platform in comparison with typical observation solutions mainly in terms of achieved data and overall ease of use.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and development of multimodal applications: A vision on key issues and methods",
        "doc_scopus_id": "84947276451",
        "doc_doi": "10.1007/978-3-319-20678-3_11",
        "doc_eid": "2-s2.0-84947276451",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Application scenario",
            "Constant improvement",
            "Design and Development",
            "Evaluation",
            "Evaluation methodologies",
            "Multi-Modal Interactions",
            "Multimodal application",
            "Multimodal user interface"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Multimodal user interfaces provide users with different ways of interacting with applications. This has advantages both in providing interaction solutions with additional robustness in environments where a single modality might result in ambiguous input or output (e.g., speech in noisy environments), and for users with some kind of limitation (e.g., hearing difficulties resulting from ageing) by yielding alternative andmore natural ways of interacting. The design and development of applications supporting multimodal interaction involves numerous challenges, particularly if the goals include the development of multimodal applications for a wide variety of scenarios, designing complex interaction and, at the same time, proposing and evolving interaction modalities. These require the choice of an architecture, development and evaluation methodologies and the adoption of principles that foster constant improvements at the interaction modalities level without disrupting existing applications. Based on previous and ongoing work, by our team, we present our approach to the design, development and evaluation of multimodal applications covering several devices and application scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Giving Voices to Multimodal Applications",
        "doc_scopus_id": "84944244352",
        "doc_doi": "10.1007/978-3-319-20916-6_26",
        "doc_eid": "2-s2.0-84944244352",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Age effects",
            "Intrinsic property",
            "Multi-Modal Interactions",
            "Multimodal application",
            "Speech interaction",
            "Speech output",
            "Synthetic voices",
            "User satisfaction"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.The use of speech interaction is important and useful in a wide range of applications. It is a natural way of interaction and it is easy to use by people in general. The development of speech enabled applications is a big challenge that increases if several languages are required, a common scenario, for example, in Europe. Tackling this challenge requires the proposal of methods and tools that foster easier deployment of speech features, harnessing developers with versatile means to include speech interaction in their applications. Besides, only a reduced variety of voices are available (sometimes only one per language) which raises problems regarding the fulfillment of user preferences and hinders a deeper exploration regarding voices’ adequacy to specific applications and users. In this article, we present some of our contributions to these different issues: (a) our generic modality that encapsulates the technical details of using speech synthesis; (b) the process followed to create four new voices, including two young adult and two elderly voices; and (c) some initial results exploring user preferences regarding the created voices. The preliminary studies carried out targeted groups including both young and older-adults and addressed: (a) evaluation of the intrinsic properties of each voice; (b) observation of users while using speech enabled interfaces and elicitation of qualitative impressions regarding the chosen voice and the impact of speech interaction on user satisfaction; and (c) ranking of voices according to preference. The collected results, albeit preliminary, yield some evidence of the positive impact speech interaction has on users, at different levels. Additionally, results show interesting differences among the voice preferences expressed by both age groups and genders.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Unsupervised segmentation of the vocal tract from real-time MRI sequences",
        "doc_scopus_id": "84921672332",
        "doc_doi": "10.1016/j.csl.2014.12.003",
        "doc_eid": "2-s2.0-84921672332",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Active appearance models",
            "Inter-frame differences",
            "Model convergence",
            "Real-Time MRI",
            "Regions of interest",
            "Speech production",
            "Unsupervised segmentation",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2014 Elsevier Ltd. All rights reserved.Advances on real-time magnetic resonance imaging (RT-MRI) make it suitable to study the dynamic aspects of the upper airway. One of the main challenges concerns how to deal with the large amount of data resulting from these studies, particularly to extract relevant features for analysis such as the vocal tract profiles. A method is proposed, based on a modified active appearance model (AAM) approach, for unsupervised segmentation of the vocal tract from midsagittal RT-MRI sequences. The described approach was designed considering the low inter-frame difference. As a result, when compared to a traditional AAM approach, segmentation is performed faster and model convergence is improved, attaining good results using small training sets. The main goal is to extract the vocal tract profiles automatically, over time, providing identification of different regions of interest, to allow the study of the dynamic features of the vocal tract, for example, during speech production. The proposed method has been evaluated against vocal tract delineations manually performed by four observers, yielding good agreement.",
        "available": true,
        "clean_text": "serial JL 272453 291210 291718 291723 291743 291782 291874 31 Computer Speech & Language COMPUTERSPEECHLANGUAGE 2014-12-25 2014-12-25 2015-03-29T00:56:13 S0885-2308(14)00128-4 S0885230814001284 10.1016/j.csl.2014.12.003 S300 S300.1 FULL-TEXT 2015-05-15T02:21:12.156754-04:00 0 0 20150901 20150930 2015 2014-12-25T02:28:36.199723Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantsponsor grantsponsorid highlightsabst orcid primabst ref 0885-2308 08852308 true 33 33 1 1 Volume 33, Issue 1 2 25 46 25 46 201509 September 2015 2015-09-01 2015-09-30 2015 Research Articles article fla Copyright © 2014 Elsevier Ltd. All rights reserved. UNSUPERVISEDSEGMENTATIONVOCALTRACTREALTIMEMRISEQUENCES SILVA S 1 Introduction 1.1 Related work 1.2 Contributions and overview 2 Active shape and appearance models 2.1 Shape modelling 2.2 Appearance modelling 3 Methods 3.1 Image data acquisition 3.2 Model building 3.3 Vocal tract segmentation 4 Evaluation 4.1 Precision 4.2 Accuracy 4.3 Computational performance 4.4 Discussion 5 Conclusions 5.1 Future work Acknowledgements References AVILAGARCIA 2004 288 291 M PROCINTCONFSIGNALPROCESSINGICSP EXTRACTINGTONGUESHAPEDYNAMICSMAGNETICRESONANCEIMAGESEQUENCES BABALOLA 2009 1435 1447 K BIRKHOLZ 2011 1422 1433 P BOUIX 2007 1207 1224 S BRESCH 2009 323 338 E CARIGNAN 2011 408 411 C PROC17THINTERNATIONALCONGRESSPHONETICSCIENCESICPHS ORALARTICULATIONNASALVOWELSINFRENCH CHALANA 1997 642 652 V CHANG 2009 122 135 H CHEN 2013 567 570 Y COOTES 1998 484 498 T PROCEUROPEANCONFERENCECOMPUTERVISION ACTIVEAPPEARANCEMODELS COOTES 2001 681 685 T COOTES 1994 327 336 T PROCBRITISHMACHINEVISIONCONFERENCE ACTIVESHAPEMODELSEVALUATIONAMULTIRESOLUTIONMETHODFORIMPROVINGIMAGESEARCH COOTES 1995 38 59 T DEMOLIN 2002 547 556 D ENGWALL 2003 43 48 O PROC6THINTSEMINARSPEECHPRODUCTIONISSP AREVISITAPPLICATIONMRIANALYSISSPEECHPRODUCTIONTESTINGASSUMPTIONS ENGWALL 2006 3 10 O PROC7THINTSEMINARSPEECHPRODUCTIONISSP INTERSPEAKERVARIATIONINARTICULATIONFRENCHNASALVOWELS ERYILDIRIM 2011 61 65 A PROCEUROPEANSIGNALPROCESSINGCONFERENCEEUSIPCO AGUIDEDAPPROACHFORAUTOMATICSEGMENTATIONMODELINGVOCALTRACTINMRIIMAGES FREITAS 2014 13 20 J PROCBIOSIGNALS2014 VELUMMOVEMENTDETECTIONBASEDSURFACEELECTROMYOGRAPHYFORSPEECHINTERFACE GAO 2010 145 158 X HAGEDORN 2011 409 412 C PROCINTERSPEECH AUTOMATICANALYSISSINGLETONGEMINATECONSONANTARTICULATIONUSINGREALTIMEMAGNETICRESONANCEIMAGING HOWING 1999 59 67 F IONITA 2011 453 460 M PROCCOMPUTERVISIONWORKSHOPSICCVWORKSHOPS REALTIMEFEATUREPOINTTRACKINGAUTOMATICMODELSELECTION JAYENDER 2013 281 292 J KATSAMANIS 2011 2841 2844 A PROCINTERSPEECH VALIDATINGRTMRIBASEDARTICULATORYREPRESENTATIONSVIAARTICULATORYRECOGNITION KOUWENHOVEN 2009 2863 2873 E KUMAR 2013 215 219 K LAMMERT 2010 1572 1575 A PROCINTERSPEECH DATADRIVENANALYSISREALTIMEVOCALTRACTMRIUSINGCORRELATEDIMAGEREGIONS MAEDA 1979 S PROC10EMEJOURNEESDETUDESURLAPAROLE UNMODELEARTICULATOIREDELALANGUEAVECDESCOMPOSANTESLINEAIRES MARTINS 2008 925 952 P MARTINS 2012 231 240 P PROCIBERSPEECH2012VIIJORNADASENTECNOLOGIADELHABLAIIIIBERIANSLTECHWORKSHOP VELARMOVEMENTINEUROPEANPORTUGUESENASALVOWELS MCGUINNESS 2010 434 444 K NARAYANAN 2011 837 840 S PROCINTERSPEECH AMULTIMODALREALTIMEMRIARTICULATORYCORPUSFORSPEECHRESEARCH OLIVEIRA 2012 2690 2693 C PROCINTERSPEECH MRISTUDYORALARTICULATIONEUROPEANPORTUGUESENASALVOWELS OLIVEIRA 2009 480 483 C PROCINTERSPEECH SPEECHRATEEFFECTSEUROPEANPORTUGUESENASALVOWELS OLIVEIRA 2007 405 408 C PROCINTERNATIONALCONGRESSPHONETICSCIENCESICPHS GESTURESTIMINGINEUROPEANPORTUGUESENASALS PENG 2010 662 665 T PROCINTCONFACOUSTICSSPEECHSIGNALPROCESSINGICASSP ASHAPEBASEDFRAMEWORKSEGMENTATIONTONGUECONTOURSMRIDATA POPOVIC 2007 169 181 A PROCTOR 2010 1576 1579 M PROCINTERSPEECH RAPIDSEMIAUTOMATICSEGMENTATIONREALTIMEMAGNETICRESONANCEIMAGESFORPARAMETRICVOCALTRACTANALYSIS RAEESY 2013 1328 1331 Z PROC10THINTERNATIONALSYMPOSIUMBIOMEDICALIMAGINGISBI AUTOMATICSEGMENTATIONVOCALTRACTMRIMAGES SALTZMAN 1989 333 382 E SEISE 2007 666 677 M SESHADRI 2012 1255 1269 K SHOSTED 2012 455 465 R SHOSTED 2012 2182 2185 R PROCINTERSPEECH USINGMAGNETICRESONANCEIMAGEPHARYNXDURINGARABICSPEECHSTATICDYNAMICASPECTS SILVA 2012 214 221 S PROCICIAR2012LNCSVOL7325 SEGMENTATIONANALYSISORALNASALCAVITIESMRTIMESEQUENCES SILVA 2013 459 466 S PROCICIAR2013LNCSVOL7950POVOADEVARZIM SEGMENTATIONANALYSISVOCALTRACTMIDSAGITTALREALTIMEMRI SILVA 2013 1307 1311 S PROCINTERSPEECH TOWARDSASYSTEMATICQUANTITATIVEANALYSISVOCALTRACTDATA STONE 2001 1026 1040 M SUNG 2009 359 367 J TEIXEIRA 2005 1435 1448 A TEIXEIRA 2012 306 317 A PROCPROPOR2012LNCSVOL7243 REALTIMEMRIFORPORTUGUESEDATABASEMETHODSAPPLICATIONS TEIXEIRA 2012 318 328 A COMPUTATIONALPROCESSINGPORTUGUESELANGUAGEVOL7243LECTURENOTESINCOMPUTERSCIENCE PRODUCTIONMODELINGEUROPEANPORTUGUESEPALATALLATERAL TIEDE 2000 25 28 M PROC5THSEMINARSPEECHPRODUCTIONISSP CONTRASTSINSPEECHARTICULATIONOBSERVEDINSITTINGSUPINECONDITIONS UDUPA 2006 75 87 J VANGINNEKEN 2002 924 933 B VANBELLE 2009 82 100 S VASCONCELOS 2011 732 742 M WILLIAMS 1976 619 627 G WRENCH 2011 2161 2164 A PROCINTERNATIONALCONGRESSPHONETICSCIENCESICPHS ULTRASOUNDPROTOCOLCOMPARETONGUECONTOURSUPRIGHTVSSUPINE ZHANG 2008 260 280 H ZHANG 2012 265 277 S ZIJDENBOS 1994 716 724 A SILVAX2015X25 SILVAX2015X25X46 SILVAX2015X25XS SILVAX2015X25X46XS item S0885-2308(14)00128-4 S0885230814001284 10.1016/j.csl.2014.12.003 272453 2015-03-29T01:53:17.160521-04:00 2015-09-01 2015-09-30 true 5079405 MAIN 22 57749 849 656 IMAGE-WEB-PDF 1 si9 433 16 98 si8 669 48 101 si7 144 14 15 si6 361 14 83 si5 919 48 216 si4 524 48 84 si3 120 10 11 si2 693 19 209 si12 1121 49 172 si11 718 37 119 si10 131 13 11 si1 319 15 64 gr9 112932 420 721 gr8 18538 109 414 gr7 34626 442 301 gr6 108242 295 715 gr5 68713 325 667 gr4 24148 272 272 gr3 22082 167 341 gr2 42713 218 715 gr15 18906 197 275 gr14 68226 367 592 gr13 58329 221 689 gr12 22964 223 316 gr11 83480 615 715 gr10 20218 163 338 gr1 30174 256 349 fx3 32916 168 658 fx2 107 7 7 fx1 123 11 7 fx2 395 68 67 fx1 491 102 68 gr9 15953 128 219 gr8 2822 58 219 gr7 4563 163 111 gr6 7382 90 219 gr5 12768 107 219 gr4 12615 164 164 gr3 10681 108 219 gr2 5482 67 219 gr15 3542 157 219 gr14 12699 136 219 gr13 8727 70 219 gr12 14908 155 219 gr11 7032 164 190 gr10 10148 106 219 gr1 13867 160 219 fx3 2671 56 219 YCSLA 697 S0885-2308(14)00128-4 10.1016/j.csl.2014.12.003 Elsevier Ltd Fig. 1 Example of an MRI image depicting relevant anatomical structures. Fig. 2 Processing pipeline depicting the main stages of articulatory studies using RT-MRI. At the centre, in more detail, the segmentation stage, the subject matter of this paper. Fig. 3 Desired segmentations for an open (left) and closed (right) velum configuration. Fig. 4 Example image showing the landmarks defined over the vocal tract. Fig. 5 Vocal tract images on the training set depicting the defined landmarks for nasal (top) and oral (bottom) vowels. Fig. 6 First four variation modes for the oral (top row) and nasal (bottom row) AAM models. Variation shown for the variance interval − 3 σ , 3 σ . Fig. 7 Detail of the segmentation method showing the steps considered to choose the proper model (oral or nasal) to segment each image frame. Fig. 8 Velar port region analysis: a small region between the segmented velum tip and pharynx is used to detect if the velum is open by computing the average intensity and comparing it with a specified threshold. Fig. 9 Segmentation examples. Each row presents vocal tract segmentations for one speaker. These cover not only oral and nasal vowels but also configurations presenting closed lips. Different regions of interest in the vocal tract are represented in alternating colours and line styles. Fig. 10 Two examples of the regions covered by moving the mean model on the [−15, 15] interval, on both axes. The covered regions are presented brighter than the remaining image. The mean model is presented in the initial position for reference. Fig. 11 Influence of initial model location on segmentation: (a) and (b) depict examples of the DSC values distribution obtained for different model initialisation positions (brighter regions correspond to good initialisations); (c) shows box plots for the overall results on how the method performed using all good initialisations to segment four sequences (30 frames per sequence), for all speakers, when compared to a reference segmentation. Fig. 12 Screenshot of the delineation tool while one observer is setting the vocal tract contour. The contour of the lower lip and tongue have already been marked. Fig. 13 Examples of segmentations performed by the observers involved in the evaluation study and the corresponding segmentation provided by the proposed method. Fig. 14 Regional difference between the observers and the segmentation method: (a) box plot depicting distance (in pixels) for each region considered and a coloured mask of difference incidence for speaker CM and (b) notable examples of observed differences between the observers and the proposed method. Fig. 15 Box plots for the average agreement provided by the Williams index, over all images, computed for the segmentations performed by each observer and for the proposed segmentation method. Notches, on the box plots, represent the 95% confidence interval. Table 1 Comparison of proposed segmentation method with observer segmentations: mean (μ) and standard deviation (σ) values for DSC, sensitivity (p), specificity (q) and specificity corrected to the bounding box enclosing both segmentations (q′). DSC (%) p (%) q (%) q′ (%) Observer μ σ μ σ μ σ μ σ OBS1 83 5 86 7 99 0 94 3 OBS2 80 5 89 5 99 0 92 3 OBS3 84 3 85 6 99 0 95 2 OBS4 82 5 80 8 99 0 95 3 Overall 82 5 85 7 99 0 94 3 Table 2 Summary of the main features characterising the AAM alternatives tested. Feature Proposed Tested alternatives Alternate oral/nasal Single oral nasal Oral Nasal Num. models 2 2 1 1 1 Training images Oral/nasal Oral/nasal Oral+nasal Oral Nasal Initialisation Yes No Init. model per frame Prev. frame Mean model Search strategy One stage Four stages Full resolution Multi-resolution Table 3 On the left, overall comparison results of segmentations using traditional AAM based segmentations with those performed manually by observers. The comparison values for the proposed method are shown for reference, on the top row. On the right, presented in decreasing order of mean DSC value, box plots depicting the distribution of the DSC values obtained. Table 4 Segmentation times in seconds (per frame) for the manual segmentations, the different AAM alternatives tested and for the proposed method. Segmentation method μ(s) σ(s) Observers 54 18 AAM alternatives 7 1 Proposed method 3 1 ☆ This paper has been recommended for acceptance by P. Jackson. Unsupervised segmentation of the vocal tract from real-time MRI sequences Samuel Silva a ⁎ António Teixeira b a a Inst. Electronics Eng. and Telematics of Aveiro (IEETA), University of Aveiro, Aveiro, Portugal Inst. Electronics Eng. and Telematics of Aveiro (IEETA), University of Aveiro Aveiro Portugal b Dept. Electronics, Telecommunications and Informatics (DETI), University of Aveiro, Aveiro, Portugal Dept. Electronics, Telecommunications and Informatics (DETI), University of Aveiro Aveiro Portugal ⁎ Corresponding author at: IEETA, Campus Univ. de Santiago, 3810-193 Aveiro, Portugal. Tel.: +351 234370500. Advances on real-time magnetic resonance imaging (RT-MRI) make it suitable to study the dynamic aspects of the upper airway. One of the main challenges concerns how to deal with the large amount of data resulting from these studies, particularly to extract relevant features for analysis such as the vocal tract profiles. A method is proposed, based on a modified active appearance model (AAM) approach, for unsupervised segmentation of the vocal tract from midsagittal RT-MRI sequences. The described approach was designed considering the low inter-frame difference. As a result, when compared to a traditional AAM approach, segmentation is performed faster and model convergence is improved, attaining good results using small training sets. The main goal is to extract the vocal tract profiles automatically, over time, providing identification of different regions of interest, to allow the study of the dynamic features of the vocal tract, for example, during speech production. The proposed method has been evaluated against vocal tract delineations manually performed by four observers, yielding good agreement. Keywords Vocal tract Segmentation Real-time MRI 1 Introduction During speech production, the vocal tract configuration continuously changes over time. This dynamic nature has long been recognised as of paramount importance to study speech production (Saltzman and Munhall, 1989) and should cover the characterisation of the position and movement of the different articulators involved, such as the tongue, lips and velum (Fig. 1 ). Several techniques have been proposed and used to gather data to enable the study of these dynamic aspects such as electromagnetic midsagittal articulography (EMMA) (e.g., Oliveira and Teixeira, 2007) or ultrasound (e.g., Wrench et al., 2011). Even though they provide high frame rates, they present some limitations by focusing in a restricted set of regions of the vocal tract. In recent years, real-time magnetic resonance imaging (RT-MRI) has been used for speech studies (e.g., Narayanan et al., 2011; Teixeira et al., 2012), allowing enough frame rate to provide useful data regarding the position and coordination of the different articulators, over time, while potentially avoiding the hyperarticulation effect observed in sustained productions (i.e., the speaker sustains vowel production while a single static image is acquired) (Engwall, 2003). Although typically acquired at the midsagittal plane, any other plane of interest may be considered (Silva et al., 2012). A wide range of applications can profit from these dynamic studies (Höwing et al., 1999), such as the assessment of swallowing disorders (Kumar et al., 2013) or the characterisation of the articulatory properties of speech (Oliveira et al., 2012; Shosted et al., 2012; Silva et al., 2013). A notable example of the latter is the study of nasal and oral vowels (for example, considering European Portuguese, the second sound in “canto” ([I] sing) and “cato” (cactus)). These have traditionally been considered to differ essentially on the lowering of the velum, for nasal vowels, without any additional articulatory adjustment, but a few studies have recently shown evidence of modifications occurring also at the tongue and lips (Engwall et al., 2006; Carignan, 2011; Shosted et al., 2012a). The articulation of European Portuguese (EP) nasal vowels has been addressed by the authors and colleagues (Martins et al., 2008; Oliveira et al., 2009), focusing on velum dynamics or using limited tongue information obtained from EMMA studies. Real-time MRI has been acquired (Teixeira et al., 2012) to extend these studies with a characterisation of the oral configuration of EP nasal vowels, important, for example, for articulatory synthesis (Teixeira et al., 2005; Birkholz et al., 2011). The often used visual comparison among different image frames is of limited use due to the subjective nature of the comparison and the difficulty to cross compare among multiple images. Therefore, relevant data regarding the vocal tract and different articulators must be extracted for analysis. Real-time MRI studies rapidly result in several thousands of images and one of the main challenges concerns how to deal with such a large amount of data in order to extract relevant features and provide researchers with the materials and visualisations that allow systematic analysis (e.g., Silva et al., 2013). In this scenario, manual segmentation of each image is not only unthinkable, given the large number of images, but also inadvisable in a scenario where a large number of observers might be available to perform the annotation, since the noisy nature of the images makes it difficult to maintain consistency intra- and inter-observer. Therefore, a systematic (semi-)automatic method to extract the relevant data should be used. 1.1 Related work Analysis of dynamic MRI vocal tract data has typically been performed looking into pattern variations at pixel level, with no segmentation of the anatomical structures of interest (Demolin et al., 2002), or focusing on specific regions, e.g., tongue dorsum (Stone et al., 2001). In recent years, a few authors have presented methods to segment the vocal tract (or specific articulators) from MRI. Avila-Garcia et al. (2004) perform automatic tongue segmentation on dynamic MRI studies by combining active shape models (ASM), trained from 39 manually annotated images, with the dynamic Hough transform. To constrain the search in Hough space, the full image sequences are considered in order to find a global optimum. Although the proposed method seems promising, the authors report a high computational complexity of this approach to the point that only a single variation mode for the ASM could be used. This fact, and the lack of quantitative data regarding its performance and the effects of considering the Hough transform, preclude any consideration regarding its merits. Bresch and Narayanan (2009) proposed a method that uses data on the spatial frequency domain, extracted from k-space, to perform segmentation of the vocal tract from RT-MRI. Each image in the sequence is processed independently and explicit identification of the different articulators is provided based on an anatomically informed geometric model. Peng et al. (2010) use a shape-based method to segment the tongue contour in MRI images of sustained productions of different sounds. Shape priors are obtained from a set of 39 images from one speaker and then used to define a curve representation which is the base for segmenting 64 images from three other speakers. Prior to segmentation, the images are roughly aligned and scaled to match the pose of the speaker considered for the training set. Enhancements to this method are proposed by Eryildirim et al. (2011) to allow a correspondence between the contours and include an automated method to detect the tongue extremities (lingual frenulum and epiglottis). Proctor et al. (2010) describe a vocal tract segmentation method applied to RT-MRI. Based on the works of Maeda (1979) a grid is superimposed on each image frame. To perform segmentation, the pixel intensity profiles along the line grids are analysed looking for local minima and the vocal tract defined by an optimal path between the relevant minima. A post-processing is applied to the extracted vocal tract profile to smooth the contour segment corresponding to the tongue. Vasconcelos et al. (2011) have presented an approach to vocal tract segmentation using active shape/appearance models applied to MRI images of the vocal tract for several relevant sustained productions of EP sounds. Nevertheless, the application scope is limited as segmentation is only applied to images from a single speaker and validation of the proposed method is performed using four images (with 21 images in the training set). Katsamanis et al. (2011) also propose an active shape model trained from 460 utterances of a single speaker (circa 30,000 vocal tract images). Note, however, that the purpose of the presented work was not segmentation, which was performed using the method by Bresch and Narayanan (2009). The main goal was to obtain a model that could provide data on the different/relevant shape properties of the vocal tract during speech production to distinguish between different types of articulation. Raeesy et al. (2013) perform vocal tract segmentation from RT-MRI images by using oriented active shape models on each image frame separately. They also propose an automatic method to set the landmarks used for training. Since these are computed from manually segmented contours, it is not yet clear how much is gained from such method and it introduces further variability in landmark position, regarding anatomical landmarks. This results in a lack of landmarks in important points, such as the upper lip, leading to poor segmentation results as depicted in Fig. 4 of Raeesy et al. (2013). Furthermore, careful positioning of landmarks might be very useful for easier contour partitioning (e.g., identifying different regions of interest, such as the tongue or the velum). The authors (Silva et al., 2013) presented a first segmentation method to tackle vocal tract segmentation from RT-MRI, based on region growing propagation over time and considering different regions of the vocal tract, with different intensity properties. Even though this method generally provides very good segmentations, it poses some difficulties when a strong contact between the tongue dorsum and the hard palate is observed. In these situations the segmentation can be edited manually, to separate both structures, but it requires a considerable amount of work and often affects tongue shape. Furthermore, even considering that, at most times, user intervention was not needed, it was still important to supervise the segmentation outputs in case it was necessary to tweak seed point position or adjust threshold intervals. Regarding image acquisition, no standard protocol exists for RT-MRI of the vocal tract. Even though, for static images, the final results might be comparable in terms of image resolution and quality, for RT-MRI acquisitions there is a lot of variability among research groups regarding the used acquisition protocols, resulting in different frame rates with significant impact on image resolution and noise levels. It is also important to highlight that none of the works presented in the literature performs thorough evaluation of the proposed methods. Evaluation is often qualitative (Bresch and Narayanan, 2009), performed over a small image set (Vasconcelos et al., 2011; Raeesy et al., 2013) or considering annotations by a single observer (Proctor et al., 2010). 1.2 Contributions and overview A novel segmentation approach is proposed, based on active appearance models (AAMs), to address the task of segmenting a large database of RT-MRI images acquired to study EP vowels (for which k-space data is not available and avoiding the frequency domain analysis complexity). Its main purpose is to tackle the issues detected in our previous work (Silva et al., 2013) and to build a more versatile framework, providing suitable data for parametric vocal tract analysis (Silva et al., 2013) It is important to clarify that these RT-MRI databases, acquired for articulatory studies, involve a fair amount of resources and do not typically include data for many speakers. Instead, they include extensive data for each speaker (in our case, around 4000 images/speaker). Therefore, our aim is not to provide a segmentation method that can be used, off-the-shelf, with new speakers. We aim, instead, to propose a segmentation method that, following clearly defined criteria for selecting a small number of manually annotated training images, allows unsupervised segmentation of the full database and later extension to other speakers, following the same criteria. Since we need to rely on the segmentations provided by the proposed method to perform automatic articulatory analysis, proper evaluation of the method is of the utmost importance and must be carried out to assess its precision and accuracy. Regarding performance, it was not a major goal to provide a very fast method since it can be left running offline, generating the data for later analysis. The work presented in this paper, considering the surveyed literature, provides several notable contributions: (a) Consideration of the sequential nature of the data, exploring the proximity between adjacent frames to minimise convergence problems, which, to the best of our knowledge, has never been successfully considered for the segmentation of the vocal tract in this kind of image. (b) Explicit consideration of two configurations of the vocal tract with an open and closed velum. This can have a major impact on automated parametric analysis of velum height if, as happens for some speakers, velum aperture does not necessarily result in a considerable height variation and automated analysis must rely on other factors to assess velar differences. Furthermore, it is a requirement for velar aperture assessment and very important if the extracted contours are to be inspected by observers, for exploratory analysis of the data. (c) Clear separation between the contour segments corresponding to different articulators (e.g., tongue and hard palate) is guaranteed at all times, even when they are in contact (coincide). (d) Single, high level, user defined initialisation of the method for each speaker and unsupervised operation thereafter. (e) Automatic identification of different regions of interest in the vocal tract based on the criterial landmark placement during training. The proposed method has also been evaluated regarding the influence of different initialisations and by comparing its outputs against segmentations performed manually by four observers on 50 images, yielding very good results. The evaluation procedure and the number of observers and images considered for the evaluation setting are also a notable difference regarding related works where evaluation, when performed, has been mostly done qualitatively or considering a very limited number of images and/or observers. The remainder of this paper is organised as follows: Section 2 presents a brief description of the basic aspects involved in using active shape/appearance models for segmentation; Section 3 describes how the AAM models were created and applied to segment the vocal tract from a large set of RT-MRI image sequences; Section 4 presents the evaluation protocol used to assess the quality of the resulting segmentations and discusses evaluation data regarding precision, accuracy and performance; finally, Section 5 presents some conclusions and ideas for further work. 2 Active shape and appearance models A brief description of the main aspects of active shape/appearance models is presented in order to provide a context to the presented work. For additional details concerning these methods, the reader is forwarded to Cootes et al. (1995, 2001), Van Ginneken et al. (2002). 2.1 Shape modelling To build active shape models (ASM) (Cootes et al., 1995; Van Ginneken et al., 2002) shapes need to be described by a set of L points (landmarks) which are initially defined manually over a set of N training images. For shape i, with i =1, …, N, the landmark vector is given by: (1) x i = ( x i 1 , y i 1 ) , … , ( x iL , y iL ) T After alignment of the different shapes, the mean shape x ¯ is given by: (2) x ¯ = 1 N ∑ i = 0 N x i The covariance can then be computed by (3) S = 1 N − 1 ∑ i = 1 N ( x i − x ¯ ) ( x i − x ¯ ) T . The eigenvectors, ϕ i , also known as variation modes, corresponding to the t largest eigenvalues, λ i , of the covariance matrix are considered, Φ =(ϕ 1|ϕ 2|…|ϕ t ), and used to express the shape of a new object (i.e., its landmarks vector) as a point distribution model (PDM): (4) x ≈ x ¯ + Φ b , where b =(b 1, b 2, …, b t ) T is a vector of weights associated to each variation mode. The number of variation modes considered, t, is chosen as the minimum that allows explaining a certain proportion, f v , of the total variance in the training shapes and typically ranges from 90% to 99.5%: (5) ∑ i = 1 t λ i ≥ f v ∑ i = 1 2 L λ i To attain the ASM, a grey-level appearance model must also be built. This is performed by considering the neighbourhood of each landmark: the normal direction at each landmark is determined (based on neighbour landmarks) and a grey-level profile is obtained sampling k pixels to either side. Similarly to what was performed with landmark coordinates, the mean grey-level profile and covariance can be computed leading to an expression analogous to Eq. (4). The grey-level appearance model used in conjunction with the PDM described in Eq. (2) can then be used to segment objects in new images by positioning the mean model over it and then searching for the best landmark positions that satisfy both the geometric characteristics defined by the PDM and the expected grey-level profiles. This search operation can be optimised if a multi-resolution scheme is used as proposed by Cootes et al. (1994). Instead of searching over the image at the original resolution, search can be performed at different resolution levels, starting at a coarser resolution and finishing at the original resolution. This improves speed and, since the grey-level profiles are also adjusted according to resolution, it also avoids that, during the first search iterations, the model is caught by fine features near its target structure. 2.2 Appearance modelling The ASM method can be further enhanced by building an active appearance model (AAM) (Cootes et al., 1998; Gao et al., 2010) which uses all the available data instead of just the landmark neighbourhoods. A PDM also needs to be built, but appearance is now modelled by distorting each of the training images so that the landmarks defined in each match the mean shape. Next, the region covered by the mean shape is sampled and the grey values used to build a texture model. Similar to what was performed for the PDM, the appearance model can then be expressed by: (6) g ≈ g ¯ + Φ g b g , where g ¯ is the mean texture, Φ g is the matrix of eigenvectors (texture variation modes derived from the training set), and b g is the weight vector. 3 Methods The research work carried out by the authors, concerning the articulatory characterisation of EP, includes all stages from RT-MRI image acquisition (Teixeira et al., 2012) to the extracted data analysis (Oliveira et al., 2012; Silva et al., 2013). To provide context, Fig. 2 depicts the main stages of the work highlighting, at the centre, the focus of this paper, image segmentation. In what follows details are provided regarding image acquisition and the different aspects concerning the development and application of the proposed segmentation method are described. 3.1 Image data acquisition The articulation of European Portuguese (EP) nasal vowels has been studied by our group using modalities such as EMMA (Oliveira et al., 2009) and static MRI Martins et al. (2008). More recently, RT-MRI has been acquired to extend these studies with further characterisation of the oral configuration of EP nasal vowels (Oliveira et al., 2012; Martins et al., 2012). It is important to note, as previously mentioned, that this imaging modality provides adequate data regarding the position and coordination of the different articulators over time (Hagedorn et al., 2011) and might provide a good choice to tackle the hyperarticulation effect observed in sustained productions (Engwall, 2003). Additionally, it might also help reduce the gravity effect on articulators for acquisitions in supine position (Tiede and Vatikiotis-Bateson, 2000). Therefore, beyond providing the data regarding the dynamic aspects of speech, RT-MRI can also support static studies, tackling some of the associated issues. Image sequences were acquired containing: (a) the five European Portuguese (EP) nasal vowels uttered in three word positions: initial, internal and final (e.g., the nonsense words “ampa, pampa, pan” [ p , p p , p ]); and (b) the eight EP oral vowels (e.g., “papa” or “pupa”). Acquisition was performed at the midsagittal plane of the vocal tract. The images were acquired on an unmodified 3.0T MR scanner (Magneton Tim Trio, Siemens, Erlanger, Germany) equipped with high performance gradients (Gmax = 45mT/m, rise time=0.2s, slew Rate=200T/m/s, and FOV=50cm). Custom 12-channel head and 4-channel neck phased-array coils were used for data acquisition. Parallel imaging (GRAPPA 2) and magnetic field gradients operating at FAST mode were used to speed up the acquisition. After localisation images, a T1 W 2D-midsagittal MRI slice of the vocal tract was obtained, using an Ultra-Fast RF-spoiled Gradient Echo (GE) pulse sequence (Single-Shot TurboFLASH), with a slice thickness of 8mm and the following parameters: TR/TE/FA=72ms/1.02ms/5°, Bandwidth=1395Hz/pixel, FOV (mm2)=210×210, reconstruction matrix of (128×128) elements with 50% phase resolution, in-plane resolution (mm2)=3.3×1.6, yielding a frame rate of 14images/s. Typically, each recorded sequence contained 75 images (taking around 5s to acquire) although some longer sequences (300 images) were also acquired, mostly with the speakers producing sequences of isolated vowels (e.g., [ ], [˜e], [ĩ], [õ], [ũ], [ ], etc.). Audio was recorded simultaneously with the RT-MRI images inside the MR scanner, at a sampling rate of 16,000Hz, using a fibreoptic microphone and manually annotated, using the software tool Praat in order to identify the time intervals corresponding to different sounds. The time intervals allow the determination (because both data are aligned) of the corresponding image frames. Data was acquired for three female speakers (CM, CO and SV), aged between 21 and 33, phonetically trained, with no history of hearing or speech disorders. It is important to note that RT-MRI acquisitions require a large amount of resources. A greater diversity of speakers, at this moment, could have been accomplished by reducing the number of sequences acquired per speaker, but this, beyond increasing the overhead regarding speaker preparation, would have a negative impact on the range of studies that can be performed using the data. The obtained frame rate of 14 frames/s, for this dataset, although not as high as reported by other authors (e.g., Lammert et al., 2010; Raeesy et al., 2013), has already been useful to support articulatory studies (e.g., Oliveira et al., 2012; Martins et al., 2012) and research on velar movement detection using surface electromyography (Freitas et al., 2014). Therefore, it constitutes an adequate dataset to demonstrate the proposed segmentation method. Further details concerning the image acquisition protocol and corpus can be found in Teixeira et al. (2012). 3.2 Model building There are two different kinds of vocal tract configurations that result in slightly different segmentations: those with an open velum and those with a closed velum (refer to Fig. 3 for an example of each case). From an analysis point of view by, for instance, a linguist, when looking just to the extracted vocal tract profile, it is easier to distinguish between an open and closed velum if the segmentation for the closed velum does not include the nasal cavity. Furthermore, velar aperture assessment requires that the velar passage is segmented. Quantitative assessment of velar differences might also profit from this approach (Silva et al., 2013): for some speakers velum height does not vary much between the open and closed velum configurations and the assessment of the bottom side of the velum is not enough to detect the difference. Building a single model to accommodate both oral and nasal options resulted in a less stable option as it often failed to converge to a proper solution. This happened because such a model encompasses much more variability in the velar region which, considering the noise in the images, was often a problem. An approach based on bifurcating contours (Seise et al., 2007) might be suitable, but added complexity which we considered avoidable. We opted for building a different model for each situation (hereafter referred to as the nasal model, which contains a nasal cavity, and the oral model) and define rules to decide, at each image frame, which model to use. The choice of which images to use for training the model was based on three main aspects: (a) there is significant variability among speakers concerning some characteristics of the vocal tract (e.g., size, angle of the pharynx) and therefore it would be important to include images from all three speakers; (b) training images should cover the most notably different vocal tract configurations present, relying on the model to adapt to intermediate configurations; and (c) as manual segmentation of the vocal tract is a tiresome, time consuming task, the number of images included should not be unnecessarily large. The goal is to build an AAM model with a small set of annotated images that allows proper segmentation of the entire database. Both oral and nasal models have been built using the same procedure, with the sole differences residing on the images used for training and the inclusion of the nasal cavity in the nasal model. Twenty-six landmark points have been defined manually along the vocal tract and covering the main anatomical structures of interest. Fig. 4 shows the landmarks depicted over a vocal tract image: 3 for each lip, 2 for the lingual frenulum, 1 at the tongue tip, 6 evenly distributed over tongue dorsum and back and 1 at tongue root, 4 along the pharynx, 3 on the velum and 3 at the hard palate and alveolar ridge. Interpolation was performed, adding 18 points between landmarks. The method used to manually define the landmarks also allowed setting secondary points, between landmarks, in order to guide how interpolation was performed between them. For the nasal model a few secondary points were used to include the nasal cavity. The training set contained a total of 51 images, 30 images for the nasal model, covering the three speakers and all EP nasal vowels and 21 images for the oral model, covering all speakers and oral vowels. These were selected from all occurrences available in the database, identified according to the annotations of the audio signal and, for each occurrence, selecting the most representative frame of the interval (e.g., velum clearly open). Images presenting acquisition artefacts that might hinder proper creation of the statistical model, e.g., unclear separation between tongue and velum, were not considered. Vocal tract configurations with the lips completely closed were also not included as preliminary experiments seemed to show better results when these were not used. Fig. 5 shows some of the images used and defined landmarks and Fig. 6 presents the first four variation modes for each model (oral and nasal). Although not the subject matter of this paper it is important to note that these variation modes are also an important result as they describe the most important articulatory characteristics of the vocal tract and might be an important tool for articulatory recognition (Vasconcelos et al., 2011; Katsamanis et al., 2011). 3.3 Vocal tract segmentation The AAM models created were used to perform segmentation of the vocal tract along the image sequences. An initialisation stage, requiring user supervision, was performed once for each speaker and for the nasal and oral models separately. Initialisation consisted in two tasks: the definition of the initial mean model position, over one image frame, and verification if a proper segmentation was obtained with such positioning. Mean model positioning is performed interactively: the user moves the mean model over the image and clicks on the desired position. Then, the segmentation method takes over and the model is automatically adjusted to the image using a four level multi-resolution search approach (Cootes et al., 1994). To assess if a proper segmentation resulted the user is required to validate it by simply checking if the model converged to the speaker's vocal tract size and if it reached the lips and nasal cavity. If any major adjustment problem is detected the mean model can be repositioned and the process repeated. This is the only step of the presented method requiring user intervention and the complete initialisation step takes around 10–15s. When compared with the initialisation of the methods presented by Bresch and Narayanan (2009) and Proctor et al. (2010), it is simpler, faster and performed at a higher level, since a single click is needed and no manual annotation of the vocal tract is required. The initialisation is performed over images of the vocal tract for [ ] and [a], since these configurations are closer to those depicted by the mean models. The two segmentations obtained in this stage were thereafter used as the initial models for each speaker (hereafter known as the speaker-specific initial models), instead of the mean models, to perform the segmentation of the first oral/nasal image frame in each sequence. Since this initial model is already close to several speaker dependant features (e.g., dimension, hard palate and pharynx configurations) search for the optimum solution could be performed at the highest image resolution and the model generally converged better/faster than when the mean model was used. For the remaining images beyond the first, in each sequence, using only the highest resolution for search, segmentation was performed using the final segmentation of the previous frame (Cootes et al., 1994; Ionita et al., 2011) as the starting condition. This approach was chosen since neighbour image frames typically presented vocal tract configurations which did not strongly differ and allowed for faster segmentation as it required a smaller number of iterations to converge. As noted by Ionita et al. (2011), when small displacements occur, it is inefficient to discard the current solution, at high resolution, and start the search back at the lowest resolution as this increases the chance of error (e.g., in the presence of image noise or contact between articulators). On the other hand, for large displacements, this approach might fail, but no severe case has been observed for our data. At most, the contour might not adjust immediately and take a couple of frames to do it. It might also be argued that using one segmentation as the initialisation of the next frame might pose problems if the segmentation starts to diverge. We observed that this does not occur often, but it might start to happen for configurations with strong contact between articulators (e.g., between tongue and hard palate), particularly if the contact lasts for a few frames and the lips are blurry. In this situation, the model typically returns to normal once the vocal tract configuration changes, a few frames ahead (e.g., the tongue or the lips move). For each segmented image frame, the proper model (oral or nasal) needed to be selected. One possibility would be to perform an a priori analysis of the image region concerned with the velar port (e.g., Silva et al., 2013), to extract velum aperture data, choosing the model accordingly. Taking into account that the change between models was not expected to be very frequent, i.e., the same model would be used in 5–6 frames before the other model was chosen, we apply one of the models and then perform analysis of the image in the vicinity of the landmarks located at the velum tip and at the top of the pharynx. Fig. 7 shows a diagram depicting the main steps considered to choose the proper model. The region between the two landmarks is analysed in order to compute the mean image intensity value (see Fig. 8 ). When the mean intensity between the landmarks exceeds half the maximum intensity of the image, the velum is considered closed and the oral model should be applied. If this is not the current model, then the oral model is applied to the same image frame. The error associated with this segmentation is compared with that obtained with the nasal model to check if it is smaller. If, even though the region between velum tip and top of the pharynx is above the intensity threshold, the error got larger, then the nasal model is kept. A similar procedure is followed for the case when the region between velum and pharynx is below the intensity threshold leading to the choice of the nasal model. Fig. 9 shows several examples of the obtained segmentations, for all speakers, and covering a wide variety of vocal tract configurations. These show smooth contours presenting very good adjustment to the different articulators. One important aspect to note is that, due to the positions chosen for the different landmarks, when creating the models, it is easy to automatically identify different regions of interest from the segmented contours (as depicted, in different colours and line styles, in Fig. 9) namely, the lips, tongue body and tip, velum, hard palate and pharynx, important to support analysis (e.g., Silva et al., 2013). Considering other ASM/AAM based vocal tract segmentation methods (Vasconcelos et al., 2011; Raeesy et al., 2013) our approach adds to them by: (1) requiring a single manual initialisation step per speaker (as opposed to one initialisation per image); (2) using two models to explicitly tackle oral and nasal configurations; (3) addressing data from multiple speakers (only one speaker considered in Vasconcelos et al., 2011); (4) using criterial positioning of landmarks allowing subsequent identification of different regions of interest in the vocal tract; and (5) starting the segmentation of each frame with the segmentation of the previous frame. 4 Evaluation The evaluation of segmentation methods should typically consider three main aspects (Udupa et al., 2006): precision, accuracy and performance. To assess how the proposed segmentation method performs on these three aspects, a set of evaluation tasks was devised and carried out as described in what follows. 4.1 Precision Precision, or reliability, deals with the similarity among segmentations obtained for the same data at different times. In our case, one important aspect that might affect segmentation is the mean model initial position, defined by the user, regarding how it affects repeatability (Chang et al., 2009; McGuinness and O’Connor, 2010; Seshadri and Savvides, 2012). The proposed method includes an initialisation stage in which the initial position for the mean model is set and the segmentation is performed for a single image frame to attain the speaker-specific initial model. Therefore, since the initialisation step is supervised by an observer, the initial mean model localisation will always result in a proper initialisation (otherwise, the localisation is changed until it does). Nevertheless, two aspects deserve attention: (1) how the position chosen for the mean model influences the obtained speaker-specific initial model and (2) how different speaker-specific initial models influence image sequence segmentation. Effect of position in speaker-specific model initialisation – considering the image used for initialisation, for each speaker, a proper vocal tract segmentation (as judged by an observer) was used as the reference initialisation. Considering a position where the mean model was roughly aligned with the vocal tract, translations from that position, on both axes, were applied in the (empirically set) range −15 to 15 pixels in 5 pixels steps. The initial speaker-specific model was then obtained for each of these locations. The reason by which the position used as a starting point for the displacements is not of paramount importance is that our main interest is not to find the optimal initial position (or all positions yielding good segmentations) for the model, but to characterise the effects of changing that position. The maximum mean model displacement values were chosen in order to encompass the region where a user could typically position the mean model. Fig. 10 presents two examples of the region covered by moving the mean model on the chosen intervals. For the sake of simplicity, the initial mean model positions for the oral and nasal models were kept the same. The new speaker-specific initial models thus obtained were compared with the reference initialisation using the Dice similarity coefficient (DSC) (Popovic et al., 2007): (7) DSC = 2 A ∩ B A + B The DSC measures the amount of overlap between two segmentations (A and B) normalised by the summed areas of both: therefore, it is 1 when the regions contained inside both contours coincide and 0 (zero) when they are completely different. The literature suggests that a value of DSC >0.7 represents a good overlap (Zijdenbos et al., 1994). Fig. 11 a and b presents two examples of DSC values distribution over the tested regions, obtained for two of the speakers. The brighter regions depict locations where the obtained speaker-specific initial models were closer to the reference model, while darker regions depict increasing differences between both. The plot in Fig. 11b, for example, shows how, for that speaker, initialisation needed more care, noticeable by a smaller region leading to good initialisation than for the speaker in Fig. 11a. Nevertheless, the gathered data shows evidence that the region where the user can place the mean model (and will result in a good initialisation) is not very restricted. Effect of different speaker-specific models on image sequence segmentation – in the evaluation step described above several mean model initial positions were tested resulting in different model initialisations (per speaker). Then, we analysed which positions provided good initialisations by comparing with a reference initialisation using the DSC. We now wanted to assess how these different initialisations resulted in differences during the segmentation of image sequences. First, the initialisations used as reference were considered for the segmentation of four image sequences per speaker (30 frames per sequence) providing reference segmentations. Then, all initialisations evaluated earlier which obtained a DSC above 0.8 were considered (Zijdenbos et al., 1994). The rationale behind the choice of only these initialisations for testing was that, in our method, the model initialisation is supervised by the user and, therefore, a good initialisation will always be used for segmentation. Next, the chosen initialisations were used to perform the segmentation of the four image sequences per speaker and the resulting segmentations compared with the reference segmentations. Fig. 11c shows the overall box plots drawn considering all sequences and speakers depicting how the segmentation generally evolves along the sequences. One notable aspect observed is that several initial locations yield a segmentation that differs more from the reference on the first few frames, depicted by the wider box plots (although generally keeping high DSC values), but then comes closer to the reference on the remaining frames. This shows that the different initialisations have a small influence on the final segmentation results. 4.2 Accuracy Accuracy, or validity, concerns how the proposed segmentations compare with true segmentation or, in most cases, a surrogate of true segmentation (Udupa et al., 2006). Several aspects of accuracy are assessed for the proposed method: (a) the overall comparison with manual segmentations performed by observers; (b) regional assessment of which parts of the vocal tract were the main source of existing differences; (c) agreement between the proposed method and the group of observers; and (d) the influence of design choices on accuracy. Experimental setting – To assess the accuracy of the proposed segmentation method, image frames of every oral/nasal vowel available on the database were randomly chosen from all speakers by selecting the last frame of the vowel occurrence based on the audio annotation. Furthermore, additional images where considered covering intermediate vocal tract configurations, namely presenting closed lips and the first frame for the nasal congeners of the EP cardinal vowels ([ , ĩ, ũ]), as some difference is expected on the vocal tract configuration over their production for EP (Oliveira et al., 2012). In total, 50 images were considered, none of which was part of the training sets for the AAMs. A supervised evaluation method was adopted (Zhang et al., 2008). A set of four observers, two radiographers (OBS1 and OBS3), and two phoneticists (OSB2 and OBS4) were asked to segment the vocal tract using a completely manual contour delineation tool (Fig. 12 ). The first radiographer (OBS1) and the first phoneticist (OBS2) had previous experience with RT-MRI images and speech studies, while the second element of both groups (OBS3 and OBS4) had general experience in their fields of work. None of these observers was involved in the vocal tract annotations of the training image set. The session started with a brief explanation regarding what was required, how the tool worked and each observer was allowed to experiment with contour delineation (by clicking points along the desired contour) over a training image which was not considered for evaluation. When the observers felt comfortable working with the delineation tool, they could proceed with the 50 segmentations. Each observer segmented the images in a different order to minimise the effects of fatigue or previous segmentations on the overall results. All observers performed the segmentations using the same desktop computer and similar ambient light conditions. Fig. 13 presents some of the segmented images, representative of the overall results, showing the manual and automatic segmentations. The vocal tract contours, obtained using the proposed segmentation method, were compared with the manual segmentations using the DSC, sensitivity (p) and specificity (q) (Jayender et al., 2013). Overall comparison (method vs. observers) – Table 1 presents the mean and standard deviation values for DSC, p and q obtained by comparing between the proposed method and the manual segmentations performed by each observer. It can be noted that the values for each observer, along with the overall values, express good agreement between the segmentations provided by the proposed method and those performed by observers. The specificity value is always 99% which was expected and explained by the fact that the size of the vocal tract is relatively small compared to the whole image (Zhang et al., 2012). To minimise this effect on specificity, the bounding box enclosing the corresponding segmentations (observer and segmentation method) was computed and a new specificity value computed (q′). These new specificity values are lower, but do not fall below 92%. Regional difference assessment – Even though the differences between the manual segmentations and those obtained using the proposed method were small, we also assessed if any particular region of the vocal tract had a more frequent discrepancy between the manual segmentations and those provided by the proposed segmentation method. To perform regional difference assessment, the contours provided by the segmentation method were divided into different regions exhibiting anatomical/articulatory relevance and the Euclidean distance computed from each point in the region towards the observer contours. Fig. 14 a shows box plots of the distances computed for each of the regions considered. A mask of the difference between the regions contained inside each manual segmentation and its corresponding automatic segmentation was computed and all masks added, by speaker. This resulted in an image in which the regions where differences are more frequent appear with a higher pixel value. These masks should not be looked at as an absolute measure of error incidence, but as a complementary view to that provided by the box plots. Fig. 14b shows an example of the overall difference mask for speaker CM. Excluding the difference denoted at the lips, which was expected as there is no clear feature delimiting where to start/stop the segmentation, it is possible to note that there is a stronger difference for the lingual frenulum and velar regions and that the lingual frenulum is where differences occur more frequently. In the case of the lingual frenulum, difference originated mostly from the automatic segmentation not adjusting properly (or as tightly as the observers) or different segmentation criteria used by the observers. Regarding the velar region, segmentation was more difficult due to the frequent presence of motion artefacts or an unclear passage between the oral and nasal cavities resulting in some observers assuming a closed velum. Fig. 14c shows examples of different situations which originated discrepancy between segmentations. On the top left, the observer considered a closed velum, while the segmentation method (and remaining observers) considered it open; on the top right, the observer, due to artefacts, did not fully segment the pharynx; on the bottom left image, the segmentation method did not segment the region between the lower lip and the tongue as tightly as the observer; and, on the bottom right, the observer considered that the epiglottis was clearly separated from the tongue while the segmentation method included it with the tongue. Agreement with observers group – Similarity measures such as the DSC allow to compare pairs of segmentations. Since we have a set of surrogate truths (provided by qualified observers) it is also interesting to assess the agreement of the proposed segmentation method with the group of observers. To determine a consensus among all observers is not trivial and several methods have been discussed in the literature (Vanbelle and Albert, 2009), which might even lead to different conclusions about the same data. To address these issues some measures have been proposed (Williams, 1976; Chalana and Kim, 1997; Kouwenhoven et al., 2009) such as the Williams agreement index. This index, widely used in the literature (Bouix et al., 2007; Babalola et al., 2009), can be expressed as: (8) WI i = ( n − 2 ) ∑ j ≠ i n D ij 2 ∑ j ≠ i n ∑ k ≠ i j − 1 D jk where D ij is a similarity/discrepancy measure between segmentations i and j (e.g., DSC) and n is the number of segmentations. The value provided by the Williams index expresses how a particular segmentation i compares with the group of the remaining segmentations and can be interpreted as follows: if greater than 1, the remaining segmentations are more similar to i than to each other; if close to 1, segmentations are as similar to i as they are to each other; and if below 1, remaining segmentations are more similar to each other than each is to i. The Williams index was computed for the proposed method versus the observers group and for each observer considering the remaining observers. Fig. 15 shows box plots depicting agreement as provided by the Williams index and presenting notches marking the 95% confidence interval. It can be noted that, for the segmentation method proposed, the Williams index attains a median slightly above 1. This provides some evidence that the segmentation method produces segmentations that are closer to those performed by the observers than the observers are among themselves. Assessment of alternative segmentation approaches – The evaluation data gathered allows to conclude that the proposed segmentation method provides very good accuracy results. Nevertheless, the different design choices made, such as initialising the segmentation of each frame with the segmentation of the previous frame, or using two models with different nasal cavity configurations, were performed considering the application scenario and on the base of preliminary experiences. In order to confirm that, as expected, these design choices actually improved the overall results of the segmentation, when compared with a more traditional AAM approach, additional assessment was performed. All the 50 image frames, manually segmented by the four observers, were also segmented using the traditional AAM approach, segmenting each image frame individually and considering four different alternatives to the proposed method which mainly differ in the training data used to create the models (main features summarised in Table 2 ): Alternate oral/nasal – similarly to the proposed method, this alternative also uses two models, each trained for one of the considered configurations (oral or nasal). The model to use, at each frame, is also chosen as described in Fig. 7. The major differences towards the proposed method are that no speaker specific initialisation is performed and, for each frame, the segmentation starts from the mean model and uses a four-stage multi-resolution search strategy (Cootes et al., 1994). Single oral+nasal – this alternative uses a single model trained using all the annotated images (oral and nasal). It does not include speaker specific initialisation and search is also performed using a multi-resolution strategy. Oral – this alternative uses a single model trained using the images annotated with no nasal cavity (oral model). No speaker specific initialisation is performed and search is also performed using a multi-resolution strategy. Nasal – this alternative is similar to the previous, but the model is trained using only the annotated images including the nasal cavity. The initial mean model position for all alternatives was set to the one used previously for the initialisation of the proposed method. The resulting segmentations were compared with the observer segmentations and Table 3 shows the overall comparison results for each of the alternatives. The comparison values show that the proposed segmentation method provides the best results. Notice also that the single model trained considering both oral and nasal configurations yielded the poorest results. This is explained by the greater instability created by using a single model to deal with the different configurations at the velum and nasal cavity. Furthermore, the box plots presented show that the proposed method consistently obtains good results while the tested alternatives present more dispersion. 4.3 Computational performance Our main goals, at this stage, did not consider any specific computational performance requirement other than allowing unsupervised segmentation in reasonable time (e.g., image data for one speaker processed overnight). To provide a reference against which to assess the computational performance of the proposed method we also measured segmentation times per image frame for the manual segmentations performed by the observers and for the other (more traditional) AAM alternatives tested in the previous section. The segmentations were performed on a quad-core i7-3520M CPU, 8GB RAM, running Matlab 2011. Table 4 presents the different segmentation times measured. As expected, the proposed method performs a lot faster (3s/frame) than the manual segmentations (54s/frame) and, despite the added features, such as the automatic selection of the proper model to use (oral or nasal), it still performs better than the tested AAM alternatives (7s/frame), mostly due to fact that a single search stage, at full resolution, is performed. 4.4 Discussion In summary, the evaluation of different aspects of the proposed segmentation method provides evidence that: 1. The segmentations resulting from the proposed method show good accuracy when compared with segmentations performed manually by observers. Agreement, as measured by the Williams index, is stronger between the proposed method and the observers than among observers. 2. The choice of using two different models (oral and nasal), and initialisation of each frame segmentation with the segmentation of the previous frame, improved accuracy when compared with alternatives using individual image frame segmentation. This also reduced the number of search stages from four to a single stage performed at the highest resolution. 3. Positioning of the mean model, to build the speaker-specific initialisation, works well on a reasonably sized region. Nevertheless, as this varies from speaker to speaker, user supervision at this stage is still needed, to check if initialisation is done properly. 4. For the different speaker-specific initialisations possible, the resulting segmentations do not present much variability among them, with an exception to the first few frames of the image sequence where differences are more prominent. This is evidence that the only user dependant task, on our method, is prone to have little influence on the resulting segmentations. 5. Although not an important goal at this stage, the proposed method, beyond including automatic choice of the proper model (oral or nasal) to use, and being more accurate than the tested alternatives, also runs faster. Considering the works discussed in the related work, no quantitative comparison with those methods is performed due to a lack of common image databases that could be used for that purpose. Note that all proposed methods address the problem dealing with databases containing images obtained using different acquisition protocols and frame rates, yielding very different image resolution and quality, which might influence the resulting segmentations. Nevertheless, based on what can be observed in the literature, for full vocal tract segmentations, when compared to Vasconcelos et al. (2011) and Raeesy et al. (2013), dealing with static images (expected to have better quality), our method provides better segmentations resulting in smoother contours, correctly covering all articulators. Compared to Bresch and Narayanan (2009), our approach provides similar results with the main difference of being applied in the image rather than the frequency domain and taking less time (Bresch and Narayanan (2009) report 20min/per frame). Furthermore, the works presented in the literature limit evaluation of the proposed methods to a qualitative evaluation (e.g., Bresch and Narayanan, 2009) or to the assessment of segmentation results in a very small number of images (Vasconcelos et al., 2011; Raeesy et al., 2013) with quantitative evaluations applied to no more than six images or using a single human observer as reference (Proctor et al., 2010). 5 Conclusions In this paper we propose a model based method to perform the segmentation of the vocal tract from midsagittal RT-MRI image sequences. While segmentation of this kind of data has been previously addressed, that was performed without attending to the sequential nature of the data, i.e., by processing each image without accounting for its neighbours. The proposed method allowed tackling the segmentation of a large RT-MRI database by building a model-based approach using a small set of annotated images. Although not specifically included in the training set, the vocal tract configurations along the sequences, such as those exhibiting tightly closed lips, were no particular challenge to the proposed method. This was possible by exploring the small inter-frame variability, in each image sequence, using the segmentation of one image as starting point for the next. Evaluation results, comparing the segmentations provided by the proposed method with those performed by four observers, show that the presented approach performs well, providing good levels of precision, accuracy and performance. Our approach, using two different models (nasal and oral) to address the different velum configurations (open and closed), allowed accurate segmentations that provide important extra data regarding velum aperture assessment, whether it is performed visually, by a phoneticist, or using computational tools. This is also a notable difference to the works of Proctor et al. (2010), Vasconcelos et al. (2011) and Raeesy et al. (2013). One of the main positive implications resulting from the proposed method is that it allows innovative approaches regarding automatic quantitative analysis of vocal tract data, as presented by the authors in Silva et al. (2013), considering the whole database available instead of a few chosen occurrences. The image processing is performed sequentially, hindering parallel processing of the images inside a sequence which, as advocated by Bresch and Narayanan (2009), might improve performance on a cloud computing scenario. Nevertheless, we do not consider it a problem since parallel processing can still be performed for multiple sequences at once. The dominant factor in our database is not the size of each sequence, but their number. Besides, although, at this moment, we do not aim for a scenario where on-the-fly segmentation must be performed, the performance of the current implementation (in Matlab) is fast enough to provide, upon request, the segmentation of a particular image sequence (75 images) in less than four minutes. Considering the different characteristics of the images gathered in RT-MRI studies of the upper airway, due to the different acquisition protocols used, the specific AAM models created for our database are most probably not directly applicable to other upper airway RT-MRI databases. Nevertheless, we consider that the proposed methodology is general enough to be used to deal with any upper airway RT-MRI database. The only requirement is that the models are retrained using a new training set, chosen according to the same criteria used here, i.e., selecting the frames for the most distinctive vocal tract configurations in the database. This, of course, is not limited to oral and nasal vowels as is the case of our database. For example, if an RT-MRI study of the upper airway is to include the articulation of lateral sounds (Teixeira et al., 2012) (e.g., /l/ as in “sal” (salt)), since the configuration of such sounds presents very distinctive characteristics on the range of movements of the tongue tip, when compared to vowels, frames showing that configuration should also be included in the training set. 5.1 Future work The difficulty of imaging the hard palate using MRI is well known. This results in considerable variability in identifying its contour. For the proposed method not much variability has been observed in this region. Nevertheless, as the hard palate is a rigid structure, it is expected that it generally stays the same for every image. One simple approach that can be used to minimise the variability is to choose the hard palate segment of one of the contours (e.g., the one in the speaker specific initialisation), for each speaker, and replace the hard palate in every segmentation with that segment (Proctor et al., 2010). The addition of more speakers to the database was not tested, given the scarcity of speakers. Nevertheless, based on the obtained results, we consider that the proposed methodology can easily encompass additional speakers through the inclusion of a small set (no more than five per model) of annotated images of that speaker in the training set, chosen based on the same criteria. Enlarging the training set with data from additional speakers also iteratively improves the model in such a way that, in the future, it might be able to deal properly with new speakers without needing to be retrained. Although, at this moment, the training set is small and retraining the models can still be done in reasonable time, in the future, and to avoid completely retraining the models whenever a new speaker is added, incremental approaches for model evolution and learning might be considered (Sung and Kim, 2009; Chen et al., 2013). Regarding the possibility that using one segmentation as starting point for the next might cause problems, if the segmentation starts to diverge, we did not observe any severe situation for our data. Nevertheless, in case this problem becomes an issue, for other datasets, there are several solutions that might be adopted. For example, since the speaker-specific initialisations provide a reference of vocal tract dimension and location of different regions of interest, it is possible to test for divergence and then act accordingly (e.g., reverting to the speaker-specific initialisations). Contour self intersections were not addressed. As far as we could observe, self intersections do not happen often, and when they occur it is usually at the lips, when they are closed, and at the hard palate, when there is strong tongue contact. These do not pose critical problems for analysis and will be addressed in the future, for example, by post-processing the extracted contours. Considering the work being carried out regarding speech production, a validated segmentation framework, as the one presented, providing complete segmentations of the vocal tract, is an important tool for the systematic study of speech production, not only on normal speakers, but also on those presenting pathologies such as a cleft palate, allowing our research work to follow that path. Acknowledgements The authors thank the observers involved in the evaluation study and the anonymous reviewers for their helpful comments and suggestions. Research partially funded by FEDER through the Program COMPETE and by National Funds (FCT) in the context of HERON II (PTDC/EEA-PLP/098298/2008), IEETA Research Unit funding FCOMP-01-0124-FEDER-022682 (FCT-PEst-C/EEI/UI0127/2011) and project Cloud Thinking (funded by the QREN Mais Centro program, ref. CENTRO-07-ST24-FEDER-002031). References Avila-Garcia et al., 2004 M.S. Avila-Garcia J.N. Carter R.I. Damper Extracting tongue shape dynamics from magnetic resonance image sequences Proc. Int. Conf. on Signal Processing (ICSP) Beijing, China 2004 288 291 Babalola et al., 2009 K.O. Babalola B. Patenaude P. Aljabar J. Schnabel D. Kennedy W. Crum S. Smith T. Cootes M. Jenkinson D. Rueckert An evaluation of four automatic methods of segmenting the subcortical structures in the brain NeuroImage 47 2009 1435 1447 10.1016/j.neuroimage.2009.05.029 Birkholz et al., 2011 P. Birkholz B. Kroger C. Neuschaefer-Rube Model-based reproduction of articulatory trajectories for consonant-vowel sequences IEEE Trans. Audio Speech Lang. Process. 19 2011 1422 1433 10.1109/TASL.2010.2091632 Bouix et al., 2007 S. Bouix M. Martin-Fernandez L. Ungar M. Nakamura M.S. Koo R.W. McCarley M.E. Shenton On evaluating brain tissue classifiers without a ground truth NeuroImage 36 2007 1207 1224 10.1016/j.neuroimage.2007.04.031 Bresch and Narayanan, 2009 E. Bresch S. Narayanan Region segmentation in the frequency domain applied to upper airway real-time magnetic resonance images IEEE Trans. Med. Imaging 28 2009 323 338 10.1109/TMI.2008.928920 Carignan, 2011 C. Carignan Oral articulation of nasal vowels in French Proc. 17th International Congress of Phonetic Sciences (ICPhS) Hong Kong, China 2011 408 411 Chalana and Kim, 1997 V. Chalana Y. Kim A methodology for evaluation of boundary detection algorithms on medical images IEEE Trans. Med. Imaging 16 1997 642 652 10.1109/42.640755 Chang et al., 2009 H.H. Chang A.H. Zhuang D.J. Valentino W.C. Chu Performance measure characterization for evaluating neuroimage segmentation algorithms NeuroImage 47 2009 122 135 10.1016/j.neuroimage.2009.03.068 Chen et al., 2013 Y. Chen F. Yu C. Ai Sequential active appearance model based on online instance learning IEEE Signal Process. Lett. 20 2013 567 570 10.1109/LSP.2013.2257753 Cootes et al., 1998 T. Cootes G. Edwards C. Taylor Active appearance models Proc. European Conference on Computer Vision Freiburg, Germany 1998 484 498 Cootes et al., 2001 T. Cootes G. Edwards C. Taylor Active appearance models IEEE Trans. Pattern Anal. Mach. Intell. 23 2001 681 685 10.1109/34.927467 Cootes et al., 1994 T. Cootes C. Taylor A. Lanitis Active shape models: evaluation of a multi-resolution method for improving image search Proc. British Machine Vision Conference York, UK 1994 327 336 Cootes et al., 1995 T.F. Cootes C.J. Taylor D.H. Cooper J. Graham Active shape models – their training and application Comput. Vision Image Underst. 61 1995 38 59 Demolin et al., 2002 D. Demolin S. Hassid T. Metens A. Soquet Real-time MRI and articulatory coordination in speech Comptes Rendus Biol. 325 2002 547 556 Engwall, 2003 O. Engwall A revisit to the application of MRI to the analysis of speech production – testing our assumptions Proc. 6th Int. Seminar on Speech Production (ISSP) Sydney, Australia 2003 43 48 Engwall et al., 2006 O. Engwall V. Delvaux T. Metens Interspeaker variation in the articulation of French nasal vowels Proc. 7th Int. Seminar on Speech Production (ISSP) Ubatuba, Brazil 2006 3 10 Eryildirim et al., 2011 A. Eryildirim M.O. Berger A guided approach for automatic segmentation and modeling of the vocal tract in MRI images Proc. European Signal Processing Conference (EUSIPCO) Barcelona, Spain 2011 61 65 Freitas et al., 2014 J. Freitas M.S. Dias S. Silva A. Teixeira Velum movement detection based on surface electromyography for speech interface Proc. BIOSIGNALS 2014 Angers, France 2014 13 20 Gao et al., 2010 X. Gao Y. Su X. Li D. Tao A review of active appearance models IEEE Trans. Syst. Man Cybern. C: Appl. Rev. 40 2010 145 158 10.1109/TSMCC.2009.2035631 Hagedorn et al., 2011 C. Hagedorn M.I. Proctor L. Goldstein Automatic analysis of singleton and geminate consonant articulation using Real-Time Magnetic Resonance Imaging Proc. Interspeech Florence, Italy 2011 409 412 Höwing et al., 1999 F. Höwing L.S. Dooley D. Wermser Tracking of non-rigid articulatory organs in X-ray image sequences Comput. Med. Imaging Gr. 23 1999 59 67 10.1016/S0895-6111(98)00067-6 Ionita et al., 2011 M. Ionita P. Tresadern T. Cootes Real time feature point tracking with automatic model selection Proc. Computer Vision Workshops (ICCV Workshops) Barcelona, Spain 2011 453 460 10.1109/ICCVW.2011.6130276 Jayender et al., 2013 J. Jayender E. Gombos S. Chikarmane D. Dabydeen F.A. Jolesz K.G. Vosburgh Statistical learning algorithm for in situ and invasive breast carcinoma segmentation Comput. Med. Imaging Gr. 37 2013 281 292 10.1016/j.compmedimag.2013.04.003 Katsamanis et al., 2011 A. Katsamanis E. Bresch Ramanarayanan V. Nara Validating RT-MRI based articulatory representations via articulatory recognition Proc. Interspeech Florence, Italy 2011 2841 2844 Kouwenhoven et al., 2009 E. Kouwenhoven M. Giezen H. Struikmans Measuring the similarity of target volume delineations independent of the number of observers Phys. Med. Biol. 54 2009 2863 2873 Kumar et al., 2013 K.V. Kumar V. Shankar R. Santosham Assessment of swallowing and its disorders – a dynamic MRI study Eur. J. Radiol. 82 2013 215 219 10.1016/j.ejrad.2012.09.010 Lammert et al., 2010 A. Lammert M. Proctor S. Narayanan Data-driven analysis of realtime vocal tract MRI using correlated image regions Proc. Interspeech Makuhari, Japan 2010 1572 1575 Maeda, 1979 S. Maeda Un modèle articulatoire de la langue avec des composantes lineaires Proc. 10ème Journées d’Etude sur la Parole 1979 Martins et al., 2008 P. Martins I. Carbone A. Pinto A. Silva A. Teixeira European Portuguese MRI based speech production studies Speech Comm. 50 2008 925 952 Martins et al., 2012 P. Martins C. Oliveira S. Silva A. Teixeira Velar movement in European Portuguese nasal vowels Proc. IberSpeech 2012 – VII Jornadas en Tecnología del Habla and III Iberian SLTech Workshop Madrid, Spain 2012 231 240 McGuinness and O’Connor, 2010 K. McGuinness N.E. O’Connor A comparative evaluation of interactive segmentation algorithms Pattern Recognit. 43 2010 434 444 10.1016/j.patcog.2009.03.008 Narayanan et al., 2011 S. Narayanan E. Bresch P. Ghoosh L. Goldstein A. Katsamanis Y. Kim A.C. Lammert M. Proctor V. Ramanarayanan Y. Zhu A multimodal real-time MRI articulatory corpus for speech research Proc. Interspeech Florence, Italy 2011 837 840 Oliveira et al., 2012 C. Oliveira P. Martins S. Silva A. Teixeira An MRI study of the oral articulation of European Portuguese nasal vowels Proc. Interspeech Portland, Oregon, USA 2012 2690 2693 Oliveira et al., 2009 C. Oliveira P. Martins A. Teixeira Speech rate effects on European Portuguese nasal vowels Proc. Interspeech Brighton, UK 2009 480 483 Oliveira and Teixeira, 2007 C. Oliveira A. Teixeira On gestures timing in European Portuguese nasals Proc. International Congress of Phonetic Sciences (ICPhS) Saarbrücken, Germany 2007 405 408 Peng et al., 2010 T. Peng E. Kerrien M.O. Berger A shape-based framework to segmentation of tongue contours from MRI data Proc. Int. Conf. Acoustics Speech and Signal Processing (ICASSP) Dallas, Texas, USA 2010 662 665 10.1109/ICASSP.2010.5495123 Popovic et al., 2007 A. Popovic M. Fuente M. Engelhardt K. Radermacher Statistical validation metric for accuracy assessment in medical image segmentation Int. J. Comput. Assist. Radiol. Surg. 2 2007 169 181 10.1007/s11548-007-0125-1 Proctor et al., 2010 M. Proctor D. Bone A. Katsamanis S. Narayanan Rapid semi-automatic segmentation of real-time Magnetic Resonance images for parametric vocal tract analysis Proc. Interspeech Makuhari, Japan 2010 1576 1579 Raeesy et al., 2013 Z. Raeesy S. Rueda J.K. Udupa J. Coleman Automatic segmentation of vocal tract MR images Proc. 10th International Symposium on Biomedical Imaging (ISBI) San Francisco, CA, USA 2013 1328 1331 10.1109/ISBI.2013.6556777 Saltzman and Munhall, 1989 E. Saltzman K. Munhall A dynamical approach to gestural paterning in speech production Ecol. Psychol. 1 1989 333 382 Seise et al., 2007 M. Seise S. McKenna I. Ricketts C. Wigderowitz Learning active shape models for bifurcating contours IEEE Trans. Med. Imaging 26 2007 666 677 10.1109/TMI.2007.895479 Seshadri and Savvides, 2012 K. Seshadri M. Savvides An analysis of the sensitivity of active shape models to initialization when applied to automatic facial landmarking IEEE Trans. Inf. Forensics Secur. 7 2012 1255 1269 10.1109/TIFS.2012.2195175 Shosted et al., 2012a R. Shosted C. Carignan P. Rong Managing the distinctiveness of phonemic nasal vowels: articulatory evidence from Hindi JASA 131 2012 455 465 Shosted et al., 2012 R. Shosted B.P. Sutton A. Benmamoun Using magnetic resonance to image the pharynx during Arabic speech: static and dynamic aspects Proc. Interspeech Portland, Oregon, USA 2012 2182 2185 Silva et al., 2012 S. Silva P. Martins C. Oliveira A. Silva A. Teixeira Segmentation and analysis of the oral and nasal cavities from MR time sequences Proc. ICIAR 2012, LNCS, vol. 7325 Aveiro, Portugal 2012 214 221 Silva et al., 2013 S. Silva A. Teixeira C. Oliveira Martins Segmentation and analysis of vocal tract from midsagittal real-time MRI Proc. ICIAR 2013, LNCS, vol. 7950, Póvoa de Varzim Portugal 2013 459 466 Silva et al., 2013 S. Silva A. Teixeira C. Oliveira Paula Towards a systematic and quantitative analysis of vocal tract data Proc. Interspeech Lyon, France 2013 1307 1311 Stone et al., 2001 M. Stone E.P. Davis A.S. Douglas M.N. Aiver R. Gullapalli W.S. Levine A.J. Lundberg Modeling tongue surface contours from cine-MRI images J. Speech Lang. Hear. Res. 44 2001 1026 1040 Sung and Kim, 2009 J. Sung D. Kim Adaptive active appearance model with incremental learning Pattern Recognit. Lett. 30 2009 359 367 10.1016/j.patrec.2008.11.006 Teixeira et al., 2005 A. Teixeira R. Martinez L. Silva L. Jesus J.C. Principe F. Vaz Simulation of human speech production applied to the study and synthesis of European Portuguese EURASIP J. Adv. Signal Process. 9 2005 1435 1448 Teixeira et al., 2012 A. Teixeira P. Martins C. Oliveira C. Ferreira A. Silva R. Shosted Real-time MRI for Portuguese: database, methods and applications Proc PROPOR 2012, LNCS, vol. 7243 Coimbra, Portugal 2012 306 317 Teixeira et al., 2012 A. Teixeira P. Martins C. Oliveira A. Silva Production and modeling of the European Portuguese palatal lateral H. Caseli A. Villavicencio A. Teixeira F. Perdigão Computational Processing of the Portuguese Language, vol. 7243 of Lecture Notes in Computer Science 2012 Springer Berlin Heidelberg 318 328 10.1007/978-3-642-28885-2_36 Tiede and Vatikiotis-Bateson, 2000 M.K. Tiede E. Vatikiotis-Bateson Contrasts in speech articulation observed in sitting and supine conditions Proc. 5th Seminar on Speech Production (ISSP) Chiemgau, Germany 2000 25 28 Udupa et al., 2006 J.K. Udupa V.R. LeBlanc Y. Zhuge C. Imielinska H. Schmidt L.M. Currie B.E. Hirsch J. Woodburn A framework for evaluating image segmentation algorithms Comput. Med. Imaging Gr. 30 2006 75 87 10.1016/j.compmedimag.2005.12.001 Van Ginneken et al., 2002 B. Van Ginneken A. Frangi J. Staal B. ter Haar Romeny M. Viergever Active shape model segmentation with optimal features IEEE Trans. Med. Imaging 21 2002 924 933 10.1109/TMI.2002.803121 Vanbelle and Albert, 2009 S. Vanbelle A. Albert Agreement between an isolated rater and a group of raters Stat. Neerl. 63 2009 82 100 10.1111/j.1467-9574.2008.00412.x Vasconcelos et al., 2011 M.J.M. Vasconcelos S.M.R. Ventura D.R.S. Freitas J.M.R. Tavares Towards the automatic study of the vocal tract from magnetic resonance images J. Voice 25 2011 732 742 10.1016/j.jvoice.2010.05.002 Williams, 1976 G. Williams Comparing the joint agreement of several raters with another rater Biometrics 32 1976 619 627 Wrench et al., 2011 A. Wrench J. Cleland J. Scobbie An ultrasound protocol to compare tongue contours: upright vs supine Proc. International Congress of Phonetic Sciences (ICPhS) Hong Kong, China 2011 2161 2164 Zhang et al., 2008 H. Zhang J.E. Fritts S.A. Goldman Image segmentation evaluation: a survey of unsupervised methods Comput. Vis. Image Underst. 110 2008 260 280 10.1016/j.cviu.2007.08.003 Zhang et al., 2012 S. Zhang Y. Zhan M. Dewan J. Huang D.N. Metaxas X.S. Zhou Towards robust and effective shape modeling: sparse shape composition Med. Image Anal. 16 2012 265 277 10.1016/j.media.2011.08.004 Zijdenbos et al., 1994 A. Zijdenbos B. Dawant R. Margolin A. Palmer Morphometric analysis of white matter lesions in MR images: method and validation IEEE Trans. Med. Imaging 13 1994 716 724 10.1109/42.363096 "
    },
    {
        "doc_title": "Live evaluation within ambient assisted living scenarios",
        "doc_scopus_id": "84939248460",
        "doc_doi": "10.1145/2674396.2674414",
        "doc_eid": "2-s2.0-84939248460",
        "doc_date": "2014-05-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Comprehensive evaluation",
            "Dynamical environment",
            "Evaluation approach",
            "Integrative platforms",
            "Real time monitoring",
            "Telerehabilitation",
            "User-centric evaluations"
        ],
        "doc_abstract": "Copyright 2014 ACM.Ambient Assisted Living is a new challenge within evaluation processes. It represents high levels of data and a strong focus on the user itself by encompassing dynamical environments with contextual data and by offering several interaction modalities. In such scenarios, traditional question-answer processes fail short to retrieve the necessary amount of data for more comprehensive evaluations. In this paper, we propose an enhanced evaluation approach adapted to these new demands. The proposal follows a user-centric approach and provides an integrative platform for evaluators to gather more information. This platform is characterized by a reusable enquiry module, its ability for real time monitoring and a event-driven dynamic module for adaptable evaluation processes. As a proof of concept, a concrete scenario based on a telerehabilitation application is also presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards e-government information platforms for enterprise 2.0",
        "doc_scopus_id": "84960478703",
        "doc_doi": "10.4018/978-1-4666-5942-1.ch047",
        "doc_eid": "2-s2.0-84960478703",
        "doc_date": "2014-04-30",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            }
        ],
        "doc_keywords": [
            "Conceptual model",
            "Data organization",
            "Government information",
            "Information infrastructures",
            "Natural languages",
            "Open Standards",
            "Proof of concept",
            "Semantic descriptions"
        ],
        "doc_abstract": "© 2014, IGI Global. All rights reserved.Enterprise 2.0 aims to help employees, customers, and suppliers collaborate, share, and organize information. As governments are relevant partners for enterprises (legislation, contracts, etc.) e-government platforms need to be ready for Enterprise 2.0 to what concerns e-government interactions. The public sector holds huge quantities of information and just a small proportion is relevant to each enterprise. Enterprises should only be confronted with relevant information and not flooded with lots of data. This implies data organization with semantic description and services using open standards. The goal is to build a durable information infrastructure for government that can be readily accessed by enterprises. The authors propose a conceptual model for government information provisioning. The rationale for this proposal is to motivate the creation of durable, standard, and open government information infrastructures. The model acquires information from natural language documents and represents it using ontology. A proof-of-concept prototype and its preliminary results are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A framework for analysis of the upper airway from real-time MRI sequences",
        "doc_scopus_id": "84894565763",
        "doc_doi": "10.1117/12.2042081",
        "doc_eid": "2-s2.0-84894565763",
        "doc_date": "2014-03-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "In recent years, real-time Magnetic Resonance Imaging (RT-MRI) has been used to acquire vocal tract data to support articulatory studies. The large amount of images resulting from these acquisitions needs to be processed and the resulting data analysed to extract articulatory features. This analysis is often performed by linguists and phoneticists and requires not only tools providing a high level exploration of the data, to gather insight over the different aspects of speech, but also a set of features to compare different vocal tract configurations in static and dynamic scenarios. In order to make the data available in a faster and systematic fashion, without the continuous direct involvement of image processing specialists, a framework is being developed to bridge the gap between the more technical aspects of raw data and the higher level analysis required by speech researchers. In its current state it already includes segmentation of the vocal tract, allows users to explore the different aspects of the acquired data using coordinated views, and provides support for vocal tract configuration comparison. Beyond the traditional method of visual comparison of vocal tract profiles, a quantitative method is proposed, considering relevant anatomical features, supported by an abstract representation of the data both for static and dynamic analysis. © 2014 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multimodal corpora for silent speech interaction",
        "doc_scopus_id": "85025134622",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85025134622",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            }
        ],
        "doc_keywords": [
            "Automatic speech recognition",
            "Data collection",
            "Environmental noise",
            "Multi-modal",
            "Research and development",
            "Silent speech",
            "Silent speech interfaces",
            "Surface electromyography"
        ],
        "doc_abstract": "A Silent Speech Interface (SSI) allows for speech communication to take place in the absence of an acoustic signal. This type of interface is an alternative to conventional Automatic Speech Recognition which is not adequate for users with some speech impairments or in the presence of environmental noise. The work presented here produces the conditions to explore and analyze complex combinations of input modalities applicable in SSI research. By exploring non-invasive and promising modalities, we have selected the following sensing technologies used in human-computer interaction: Video and Depth input, Ultrasonic Doppler sensing and Surface Electromyography. This paper describes a novel data collection methodology where these independent streams of information are synchronously acquired with the aim of supporting research and development of a multimodal SSI. The reported recordings were divided into two rounds: a first one where the acquired data was silently uttered and a second round where speakers pronounced the scripted prompts in an audible and normal tone. In the first round of recordings, a total of 53.94 minutes were captured where 30.25% was estimated to be silent speech. In the second round of recordings, a total of 30.45 minutes were obtained and 30.05% of the recordings were audible speech.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Online health information semantic search and exploration: Reporting on two prototypes for performing information extraction on both a hospital intranet and the world wide web",
        "doc_scopus_id": "84978452962",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84978452962",
        "doc_date": "2014-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Arts and Humanities (all)",
                "area_abbreviation": "ARTS",
                "area_code": "1200"
            },
            {
                "area_name": "Social Sciences (all)",
                "area_abbreviation": "SOCI",
                "area_code": "3300"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014 Walter de Gruyter Inc., Boston/Berlin. All rights reserved.In this chapter, we apply ontology-based information extraction to unstructured natural language sources to help enable semantic search of health information. We propose a general architecture capable of handling both private and public data. Two of our novel systems that are based on this architecture are presented here. The first system, MedInX, is a Medical Information eXtraction system which processes textual clinical discharge records, performing automatic and accurate mapping of free text reports onto a structured representation. MedInX is designed to be used by health professionals, and by hospital administrators and managers, allowing its users to search the contents of such automatically populated ontologies. The second system, SPHInX, attempts to perform semantic search on health information publicly available on the web in Portuguese. The potential of the proposed approach is clearly shown with usage examples and evaluation results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Application of text mining to biomedical knowledge extraction: Analyzing clinical narratives and medical literature",
        "doc_scopus_id": "84978451438",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84978451438",
        "doc_date": "2014-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Arts and Humanities (all)",
                "area_abbreviation": "ARTS",
                "area_code": "1200"
            },
            {
                "area_name": "Social Sciences (all)",
                "area_abbreviation": "SOCI",
                "area_code": "3300"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014 Walter de Gruyter Inc., Boston/Berlin. All rights reserved.One of the tools that can aid researchers and clinicians in coping with the surfeit of biomedical information is text mining. In this chapter, we explore how text mining is used to perform biomedical knowledge extraction. By describing its main phases, we show how text mining can be used to obtain relevant information from vast online databases of health science literature and patients’ electronic health records. In so doing, we describe the workings of the four phases of biomedical knowledge extraction using text mining (text gathering, text preprocessing, text analysis, and presentation) entailed in retrieval of the sought information with a high accuracy rate. The chapter also includes an in depth analysis of the differences between clinical text found in electronic health records and biomedical text found in online journals, books, and conference papers, as well as a presentation of various text mining tools that have been developed in both university and commercial settings.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A critical analysis of speech- based interaction in healthcare robots: Making a case for the increased use of speech in medical and assistive robots",
        "doc_scopus_id": "84978445314",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84978445314",
        "doc_date": "2014-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Arts and Humanities (all)",
                "area_abbreviation": "ARTS",
                "area_code": "1200"
            },
            {
                "area_name": "Social Sciences (all)",
                "area_abbreviation": "SOCI",
                "area_code": "3300"
            }
        ],
        "doc_keywords": [
            "Aging population",
            "Assistive robots",
            "Critical analysis",
            "Health-care system",
            "Healthcare sectors",
            "Human robot Interaction (HRI)",
            "Position papers",
            "System designers"
        ],
        "doc_abstract": "© 2014 Walter de Gruyter Inc., Boston/Berlin/Munich. All rights reserved.Healthcare systems around the world face serious challenges related to an aging population and the lack of enough qualified professionals to serve the needs of the elderly. To meet these challenges, health care must place greater emphasis on effective use of technology. Healthcare robots are viewed as a possible answer, and in fact, more and more service robots are expected to enter the healthcare sector in the near future. To improve acceptance of such robots, it is important to focus on how they interact with humans. Research on Human- robot interaction (HRI) clearly indicates that speech is one of the preferred ways of interacting. This chapter is a position paper championing the importance of developing adequate speech- enabled interfaces for medical and assistive robots. To do so, the author surveys the professional literature detailing the design and use of healthcare robots. Exploring some of the principal challenges presented to system designers, the author also shows how some of those challenges might be addressed. A critique of the current systems and future directions are proposed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preface",
        "doc_scopus_id": "84945207003",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84945207003",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Assessing the applicability of surface EMG to tongue gesture detection",
        "doc_scopus_id": "84921403499",
        "doc_doi": "10.1007/978-3-319-13623-3_20",
        "doc_eid": "2-s2.0-84921403499",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "False positive rates",
            "Gesture detections",
            "Sensor positioning",
            "Silent speech interfaces",
            "Surface electromyography",
            "Synchronous acquisition",
            "Tongue gestures",
            "Ultrasound imaging"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.The most promising approaches for surface Electromyography (EMG) based speech interfaces commonly focus on the tongue muscles. Despite the interesting results in small vocabularies tasks, it is yet unclear which articulation gestures these sensors are actually detecting. To address these complex aspects, in this study we propose a novel method, based on synchronous acquisition of surface EMG and Ultrasound Imaging (US) of the tongue, to assess the applicability of EMG to tongue gesture detection. In this context, the US image sequences allow us to gather data concerning tongue movement over time, providing the grounds for the EMG analysis. Using this multimodal setup, we have recorded a corpus that covers several tongue transitions (e.g. back to front) in different contexts. Considering the annotated tongue movement data, the results from the EMG analysis show that tongue transitions can be detected using the EMG sensors, with some variability in terms of sensor positioning, across speakers, and the possibility of high false-positive rates.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "RT-MRI based dynamic analysis of vocal tract configurations: Preliminary work regarding intra- and inter-sound variability",
        "doc_scopus_id": "84921385715",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84921385715",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Articulatory data",
            "Different class",
            "Dynamic aspects",
            "Large amounts",
            "Nasal vowels",
            "Repeated utterance",
            "Speech production"
        ],
        "doc_abstract": "© 2014 10th International Seminar on Speech Production, ISSP 2014. All rights reserved.The study of the dynamic aspects of speech production is relevant for different classes of sounds. To improve knowledge regarding this subject it is important to consider data from various repeated utterances, from different speakers, leading to the challenging scenario of processing and analysing a large amount of data in a common framework. We propose a method for dynamic analysis of articulatory data jointly exploring multiple utterances of each sound using a quantitative analysis framework and covering multiple articulators. Instead of analysing each utterance and then inferring notable features, all utterances for a particular sound are considered simultaneously, possibly from more than one speaker, aiming towards a characterization of average dynamic aspects and their corresponding variation. Application examples are provided involving the analysis of vowel dynamics and inter-vowel comparison of European Portuguese nasal vowels data gathered using real-time Magnetic Resonance (RT-MRI).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantitative analysis of /l/ production from RT-MRI: First results",
        "doc_scopus_id": "84921383919",
        "doc_doi": "10.1007/978-3-319-13623-3_4",
        "doc_eid": "2-s2.0-84921383919",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Co-articulation",
            "Dynamic property",
            "Large amounts of data",
            "Laterals",
            "Position effect",
            "Quantitative frameworks",
            "Systematic analysis",
            "Temporal aspects"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.Lateral consonants are complex and variable sounds. Static MRI provides relevant information regarding /l/ geometry, but does not address dynamic properties. Real-time MRI is a well suited technique for dealing with temporal aspects. However, large amounts of data have to be processed to harness its full potential. The main goal of this paper is to extend a recently proposed quantitative framework to the analysis of real-time MRI data for European Portuguese /l/. Several vocal tract configurations of the alveolar consonant, acquired in different syllable positions and vocalic contexts, were compared. The quantitative framework revealed itself capable of dealing with the data for the /l/, allowing a systematic analysis of the multiple realisations. The results regarding syllable position effects and coarticulation of /l/ with adjacent vowels are in line with previous findings.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Systematic and quantitative analysis of vocal tract data: Intra- and inter-speaker analysis",
        "doc_scopus_id": "84921374092",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84921374092",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Articulatory data",
            "Articulatory differences",
            "Current technology",
            "Quantitative assessments",
            "Quantitative frameworks",
            "Vocal-tract data",
            "Vocal-tracts"
        ],
        "doc_abstract": "© 2014 10th International Seminar on Speech Production, ISSP 2014. All rights reserved.Articulatory analysis has traditionally been performed visually. Besides the inherent subjectivity, such approach is not feasible when dealing with the large amount of articulatory data made available by current technologies, such as real-time Magnetic Resonance Imaging. Therefore, analysis of the available data should move towards systematic quantitative assessment. As a first approach to this issue, we present the main aspects of a quantitative framework for vocal tract profile comparison which provides normalized comparison data concerning different articulators. Application examples are shown illustrating how it can be used to perform analysis of the articulatory differences between sounds, comparison among speakers and overall articulatory characterization obtained from multiple speakers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enhancing multimodal silent speech interfaces with feature selection",
        "doc_scopus_id": "84910070482",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84910070482",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Classification accuracy",
            "Curse of dimensionality",
            "Dynamic time warping",
            "Relative performance",
            "Silent speech interfaces",
            "Sources of informations",
            "Supervised classification",
            "Surface electromyography"
        ],
        "doc_abstract": "Copyright © 2014 ISCA.In research on Silent Speech Interfaces (SSI), different sources of information (modalities) have been combined, aiming at obtaining better performance than the individual modalities. However, when combining these modalities, the dimensionality of the feature space rapidly increases, yielding the well-known \"curse of dimensionality\". As a consequence, in order to extract useful information from this data, one has to resort to feature selection (FS) techniques to lower the dimensionality of the learning space. In this paper, we assess the impact of FS techniques for silent speech data, in a dataset with 4 non-invasive and promising modalities, namely: video, depth, ultrasonic Doppler sensing, and surface electromyography. We consider two supervised (mutual information and Fisher's ratio) and two unsupervised (meanmedian and arithmetic mean geometric mean) FS filters. The evaluation was made by assessing the classification accuracy (word recognition error) of three well-known classifiers (knearest neighbors, support vector machines, and dynamic time warping). The key results of this study show that both unsupervised and supervised FS techniques improve on the classification accuracy on both individual and combined modalities. For instance, on the video component, we attain relative performance gains of 36.2% in error rates. FS is also useful as pre-processing for feature fusion.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Impact of age in the production of European Portuguese vowels",
        "doc_scopus_id": "84910031154",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84910031154",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Acoustic characteristic",
            "Developed countries",
            "Elderly populations",
            "European portuguese",
            "Fundamental frequencies",
            "Language development",
            "Oral vowels",
            "Speech acoustics"
        ],
        "doc_abstract": "Copyright © 2014 ISCA.The elderly population is quickly increasing in the developed countries. However, in European Portuguese (EP) no studies have examined the impact of age-related structural changes in speech acoustics. The purpose of this paper is to analyse the effect of age ([60-70], [71-80] and [81-90]), gender and type of vowel in the acoustic characteristics (fundamental frequency (F0), first formant (F1), second formant (F2) and duration) of the EP vowels. A sample of 78 speakers was selected from the database of elderly speech collected by Microsoft Language Development Center (MLDC) within the Living Usability Lab (LUL) project. It was observed that duration is the only parameter that significantly changes with ageing, being the highest value found in the [81-90] group. Moreover, F0 decreases in females and increases in males with ageing. In general, F1 and F2 decreases with ageing, mainly in females. Comparing the data obtained with the results of previous studies with adult speakers, a trend towards the centralization of vowels with ageing is observed. This investigation is the starting point for a broader study which will allow to analyse the changes in vowels acoustics from childhood to old age in EP.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic annotation of an ultrasound corpus for studying tongue movement",
        "doc_scopus_id": "84908661679",
        "doc_doi": "10.1007/978-3-319-11758-4_51",
        "doc_eid": "2-s2.0-84908661679",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic annotation",
            "Environmental noise",
            "Manual annotation",
            "Movement detection",
            "Silent speech interfaces",
            "Surface electromyography",
            "Systematic analysis",
            "Ultrasound imaging"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2014.Silent speech interfaces can work as an alternative way of interaction in situations where the acoustic speech signal is absent (e.g., speech impairments) or is not suited for the current context (e.g., environmental noise). The goal is to use external data to infer/improve speech recognition. Surface electromyography (sEMG) is one of the modalities used to gather such data, but its applicability still needs to be further explored involving methods to provide reference data about the phenomena under study. A notable example concerns exploring sEMG to detect tongue movements. To that purpose, along with the acquisition of the sEMG, a modality that allows observing the tongue, such as ultrasound imaging, must also be synchronously acquired. In these experiments, manual annotation of the tongue movement in the ultrasound sequences, to allow the systematic analysis of the sEMG signals, is mostly infeasible. This is mainly due to the size of the data involved and the need to maintain uniform annotation criteria. Therefore, to address this task, we present an automatic method for tongue movement detection and annotation in ultrasound sequences. Preliminary evaluation comparing the obtained results with 72 manual annotations shows good agreement.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Can ultrasonic doppler help detecting nasality for silent speech interfaces? An exploratory analysis based on alignement of the doppler signal with velum aperture information from real-time MRI",
        "doc_scopus_id": "84907308788",
        "doc_doi": "10.5220/0004725902320239",
        "doc_eid": "2-s2.0-84907308788",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Movement detection",
            "Nasal vowels",
            "Portuguese",
            "Silent speech",
            "Ultrasonic doppler"
        ],
        "doc_abstract": "This paper describes an exploratory analysis on the usefulness of the information made available from Ultrasonic Doppler signal data collected from a single speaker, to detect velum movement associated to European Portuguese nasal vowels. This is directly related to the unsolved problem of detecting nasality in silent speech interfaces. The applied procedure uses Real-Time Magnetic Resonance Imaging (RT-MRI), collected from the same speaker providing a method to interpret the reflected ultrasonic data. By ensuring compatible scenario conditions and proper time alignment between the Ultrasonic Doppler signal data and the RT-MRI data, we are able to accurately estimate the time when the velum moves and the type of movement under a nasal vowel occurrence. The combination of these two sources revealed a moderate relation between the average energy of frequency bands around the carrier, indicating a probable presence of velum information in the Ultrasonic Doppler signal. Copyright © 2014 SCITEPRESS - Science and Technology Publications. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "AgeCI: HCI and age diversity",
        "doc_scopus_id": "84903463873",
        "doc_doi": "10.1007/978-3-319-07446-7_18",
        "doc_eid": "2-s2.0-84903463873",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "age diversity",
            "Current practices",
            "Driving factors",
            "Interaction design",
            "overview"
        ],
        "doc_abstract": "We present an overview of recent works in which age is an important driving factor for Human-Computer Interaction design and development. These serve as starting grounds to discuss current practices and highlight challenges that might serve as beacons for future research in the field. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Design and development of speech interaction: A methodology",
        "doc_scopus_id": "84903128327",
        "doc_doi": "10.1007/978-3-319-07230-2_36",
        "doc_eid": "2-s2.0-84903128327",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Computer interaction",
            "decoupled modalities",
            "Design and Development",
            "Multimodal architectures",
            "Multimodal frameworks",
            "Speech interaction",
            "Speech modality",
            "Speech recognizer"
        ],
        "doc_abstract": "Using speech in computer interaction is advantageous in many situation and more natural for the user. However, development of speech enabled applications presents, in general, a big challenge when designing the application, regarding the implementation of speech modalities and what the speech recognizer will understand. In this paper we present the context of our work, describe the major challenges involved in using speech modalities, summarize our approach to speech interaction design and share experiences regarding our applications, their architecture and gathered insights. In our approach we use a multimodal framework, responsible for the communication between modalities, and a generic speech modality allowing developers to quickly implement new speech enabled applications. As part of our methodology, in order to inform development, we consider two different applications, one targeting smartphones and the other tablets or home computers. These adopt a multimodal architecture and provide different scenarios for testing the proposed speech modality. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An ontology for human-machine computation workflow specification",
        "doc_scopus_id": "84902491377",
        "doc_doi": "10.1007/978-3-319-07617-1_5",
        "doc_eid": "2-s2.0-84902491377",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Crowdsourcing",
            "Domain knowledge",
            "Human-machine",
            "Multiple applications",
            "Task",
            "Workflow",
            "Workflow specification"
        ],
        "doc_abstract": "Lately, a focus has been given to the re-usability of workflow definitions and to flexible and re-usable workflow components, culminating with approaches that harness the benefits of the enriched semantics provided by ontologies. Following this trend and the needs of multiple application domains, such as micro-task crowdsourcing and ambient assisted living, of incorporating cooperation between the efforts of human and machine entities, this paper proposes an ontology and process for the definition, instantiation and execution of semantically enriched workflows. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Velum movement detection based on surface electromyography for speech interface",
        "doc_scopus_id": "84902334838",
        "doc_doi": "10.5220/0004741100130020",
        "doc_eid": "2-s2.0-84902334838",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Acoustic signals",
            "Movement detection",
            "Nasal vowels",
            "Noisy environment",
            "Noninvasive methods",
            "Silent speech interfaces",
            "Speech interface",
            "Surface electromyography"
        ],
        "doc_abstract": "Conventional speech communication systems do not perform well in the absence of an intelligible acoustic signal. Silent Speech Interfaces enable speech communication to take place with speech-handicapped users and in noisy environments. However, since no acoustic signal is available, information on nasality may be absent, which is an important and relevant characteristic of several languages, particularly European Portuguese. In this paper we propose a non-invasive method - surface Electromyography (EMG) electrodes - positioned in the face and neck regions to explore the existence of useful information about the velum movement. The applied procedure takes advantage of Real-Time Magnetic Resonance Imaging (RT-MRI) data, collected from the same speakers, to interpret and validate EMG data. By ensuring compatible scenario conditions and proper alignment between the EMG and RT-MRI data, we are able to estimate when the velum moves and the probable type of movement under a nasality occurrence. Overall results of this experiment revealed interesting and distinct characteristics in the EMG signal when a nasal vowel is uttered and that it is possible to detect velum movement, particularly by sensors positioned below the ear between the mastoid process and the mandible in the upper neck region. Copyright © 2014 SCITEPRESS - Science and Technology Publications. All rights reserved.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Elderly centered design for interaction - The case of the S4S Medication Assistant",
        "doc_scopus_id": "84897816217",
        "doc_doi": "10.1016/j.procs.2014.02.044",
        "doc_eid": "2-s2.0-84897816217",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Development method",
            "Elderly evaluation",
            "Interaction",
            "Medication",
            "Mobile applications",
            "User-centred"
        ],
        "doc_abstract": "Several aspects of older adults' life can benefit from recent technological developments, but success in harnessing this potential depends on careful design and accessible, easy to use products. Design and development must be centered on the elderly and adequately consider interaction. In this paper we follow this design approach and put it to the test in developing a concrete application, aimed to contribute to lower the high levels of non-adherence to medication in the elderly population. The \"Medication Assistant\" application was developed following an iterative method centered, from the start, on the elderly and interaction design. The method repeats short-time development cycles integrating definition of scenarios and goals, requirements engineering, design, prototyping and evaluation. Evaluation, by end-users, of the increasingly refined prototypes, is a key characteristic of the method. The evaluation results provide information related to strengths and weaknesses of the application and yield suggestions regarding changes and improvements, valuable support further development. Results regarding evaluation of the second prototype of \"Medication Assistant\" are presented. © 2013 The Authors. Published by Elsevier B.V.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2014-02-25 2014-02-25 2014-10-23T08:54:46 S1877-0509(14)00046-5 S1877050914000465 10.1016/j.procs.2014.02.044 S300 S300.2 HEAD-AND-TAIL 2021-10-14T13:20:00.670388Z 0 0 20140101 20141231 2014 2014-02-25T00:00:00Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 false 27 27 C Volume 27 45 398 408 398 408 2014 2014 2014-01-01 2014-12-31 2014 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, DSAI 2013 Dr. Manuel Pérez Cota Dr. João Barroso Dr. Simone Bacellar Leal Ferreira Dr. Benjamim Fonseca Dr. Tassos Mikropoulos Dr. Hugo Paredes article fla Copyright © 2013 The Authors. Published by Elsevier B.V. ELDERLYCENTEREDDESIGNFORINTERACTIONCASES4SMEDICATIONASSISTANT FERREIRA F FERREIRAX2014X398 FERREIRAX2014X398X408 FERREIRAX2014X398XF FERREIRAX2014X398X408XF Full 2014-02-25T10:44:25Z OA-Window ElsevierWaived 0 item S1877-0509(14)00046-5 S1877050914000465 10.1016/j.procs.2014.02.044 280203 2014-10-23T04:53:38.277668-04:00 2014-01-01 2014-12-31 true 770861 MAIN 11 52416 849 656 IMAGE-WEB-PDF 1 P r o c e d i a C o m p u t e r S c i e n c e 2 7 ( 2 0 1 4 ) 3 9 8 4 0 8 1877-0509 ' 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). doi: 10.1016/j.procs.2014.02.044 ScienceDirect Available online at www.sciencedirect.com 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, DSAI 2013 Elderly centered design for Interaction â€“ the case of the S4S Medication Assistant FlÃ¡vio Ferreira a , Nuno Almeida a,b , Ana Filipa Rosa a , AndrÃ© Oliveira a , JosÃ© Casimiro c , Samuel Silva a,b , AntÃ³nio Teixeira a,b, * a Institute of Electronics and Telematics Engineering of Aveiro (IEETA), 3810-193 Aveiro, Portugal b Dep. of Electronics Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal c Polytecnic Institute of Tomar, 2300-531 Tomar, Portugal Abstract Several aspects of older adultsâ€™ life can benefit from recent technological developments, but success in harnessing this potential depends on careful design and accessible, easy to use products. Design and development must be centered on the elderly and adequately consider interaction. In this paper we follow this design approach and put it to the test in developing a concrete application, aimed to contribute to lower the high levels of non-adherence to medication in the elderly population. The â€œMedication Assistantâ€� application was developed following an iterative method centered, from the start, on the elderly and interaction design. The method repeats short-time development cycles integrating definition of scenarios and goals, requirements engineering, design, prototyping and evaluation. Evaluation, by end-users, of the increasingly refined prototypes, is a key characteristic of the method. The evaluation results provide information related to strengths and weaknesses of the application and yield suggestions regarding changes and improvements, valuable support further development. Results regarding evaluation of the second prototype of â€œMedication Assistantâ€� are presented. Â© 2013 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013) * Corresponding author. Tel.: +351 234370500; fax: +351 234370545. E-mail address: ajst@ua.pt. ' 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and peer-review under responsibility of the Scientific Programme Com ittee of the 5th International Conference on Soft are Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). 399 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 \"Keywords: development method; interaction; user centred; elderly evaluation; mobile applications; medication\" 1. Introduction The impact of using advanced technologies has been very positive for the general population. Due to the continuous increase in the elderly population worldwide [1], development starts to contemplate this group, taking into account the significant contribution that new technologies may provide to improve the quality of life of these population. This meets the guidelines of the EU countries for the elderly population, which highlight active aging and independent living, by the development of services based on the specific needs of the population, helping them to live in community [2]. Developers know the importance of involving the target users in the design process [3] and this is even more important when the target users are elderly [4]. Throughout the design process of products suitable for and usable by elderly it must be taken into account their unique needs, limitations and capabilities [5]. To accomplish this, it is essential to know and use effective approaches for interaction and gather data on how they are used. This data will give information about the features and attributes that elderly prefer, and consequently allow an understanding of which factors can improve the usability of the product [6]. The design process in this perspective arises in the literature as User Centered Design (UCD), but the traditional user centered design provides little guidance on how to involve the elderly [3]. UCD is a collection of methods that aim to involve the users in an appropriate way during development. The basic principle of this approach is placing users in the focus of the design process through the use of various techniques, first to collect information from them and then to initiate the design of prototypes which they will be asked to test [7]. â€œNeeds assessment and requirements analysis are the most important activities for initiating system improvement because, done well, they are the foundation upon which all other activities build\" [5]. With the continuous progress and sophistication of the prototypes, the user should be asked to perform tasks with minimum guidance by the testers. The results of this tests, according to the iterative process inherited from UCD, are analyzed and will guide the design of subsequent prototypes [7]. Product development based in UCD should adhere to the following principles [8, 9]: knowledge gathering concerning usersâ€™ needs, capabilities, attitudes and characteristics; active involvement of users; prototypes redesign, as often as necessary; iterations of design solutions (repetition of a cyclic process of design, evaluation and redesign as often as necessary); multidisciplinary design teams. According to the international standard ISO 13407 this process should be developed through four stages in the following order: specify the context of use, define the requirements, design and evaluation [8]. One of the important contributions to guide the process of product development according to the principles of UCD, particularly at an early stage of requirement definition, is the creation of Personas [10]. Personas are fictional persons, with name, occupations, age, gender, socioeconomic status, hobbies, stories and goals and are used to personify the principal characteristics and functions for the product design [11, 12]. There are various data-driven Personas for elderly based on European statistics, which should be followed by teams that intend to develop products to the elderly [13]. Personas are an important and valuable tool, mainly if the objective of the team is to test and evaluate the usability and effectiveness of a product. The Personas â€œallow us to see the scope and nature of the design problem. They make it clear exactly what the user's goals are, so we can see what the product must do\" [13]. Despite the general applicability of the UCD development method, it must be tuned and adapted when developing applications for older adults. With the increased use of new interaction modalities (e.g. touch), multimodal interaction requirements, design, development and evaluation must also be part of the development process. 400 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 Considering the elderly population, one important issue affecting their daily life and health, concerns medication non-adherence [14]. This is often due to the increasing number of medicines to take, to age related changes (such as memory loss, or different levels of physical and cognitive impairment), to lack of information on its usefulness and transient side-effects and to demotivation. The impact of medication non-adherence on the quality of life and associated costs dealing, for example, with reduced or unwanted therapeutic effects (e.g. antibiotic resistance and longer treatment timespans) [15], configure this as a very important problem to tackle in which mobile devices and adequate interaction design can play an important role. This, we argue, is a context were the UCD development method can prove its mettle. In this paper, we present an UCD development method centered on the elderly and interaction, aimed at increased usability and accessibility [9], and its application to the development of a concrete system, the â€œMedication Assistantâ€�, that allows voice and touch interaction to facilitate the access by elderly. The main features of this system are the alerts of medication to intake, the visualization of information of each medication and the advice service that allows users to obtain information about what to do in case of forgetting to take a medication. The prototype of this application has been already evaluated in two tests, with two different groups of users, in order to detect problems and collect opinions and suggestions to further meet the needs and specificities of the users. This paper starts by providing an overview of the proposed development method. After, a description of the primary Persona, context scenarios and application requirements for the â€œMedication Assistantâ€� are presented. It is also discussed how the application meets the requirements, followed by the evaluation method, main results and the conclusions. 2. Elderly Centered Development Method The proposed development method is aligned with the methodology described in [16]. After obtaining the requirements (phase 1), a prototype is proposed (phase 2) and evaluated (phase 3), in order to refine the requirements. This iterative methodology continues with additional prototypes and evaluations towards an increasingly refined application. In order to get system Personas, the context scenarios and the system requirements for phase 1 we adopted a five stages method aligned with Cooper and collaborators [17]. The first stage aims at identifying the behavioral variables, such as age and demographic localization, activities, attitudes, aptitudes, motivations and skills and significant behavior patterns, by analyzing the interview results. Following these guides, it is possible to synthetize the characteristics and relevant goals for the Persona. After, the description of the Persona is expanded in order to have a small story about the Persona and its daily life. Finally, Persona types are designated. At the beginning only the primary Persona should be created. On the second stage, to get the requirements and context scenarios a problem and vision statement must me produced. The third stage consists in brainstorming with people from different domains and end-users. The brainstorming should last a couple of hours and comprise a few questions in order to stimulate the ideas flow. After the brainstorming the main ideas should be filtered. On the fourth stage Persona expectations are set by the same people who brainstormed earlier. This is very important in order to understand the main end-user and its expectations. At the end of this stage, context scenarios are defined, typically as a group of short stories. They should be simple and represent use of the application by end-users. Finally, the requirements analysis should be made. To create the requirements it is important to be aware of the information gathered in the previous stages. It is important to note that the Persona and the scenarios should always be present during the development process (phase 2) and not only in requirements elicitation. 401 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 Towards the development of an application designed taking into account the accessibility and usability features of end users, the application is evaluated along all the development process (several iterations of phase 3). The evaluation method adopted is based on a methodology recently proposed [16]. This methodology is adapted to the characteristics of the application and its end users. The evaluation consists of three phases: conceptual validation, prototype test and pilot test. These three phases are connected, in a cyclic process, as the results of the several evaluations will influence the development process of the application. The first phase of evaluation aims at collecting information to verify the viability of the application interface and functions. The second phase comprises a test performed by the users through interaction with the application prototype, with the presence of an evaluator to register opinions and suggestions of the user. The pilot test is similar to the prototype test, but it adds the assessment of the impact the application has on the lives of its end users. 3. Application to a Medication Assistant for the Elderly In this section, after the generic presentation of the method in the previous section, we show how it can be applied in a real scenario using, as an example, the development of a new multimodal application for Smartphones, named â€œMedication Assistantâ€�. The main purpose of this application is to make elders become healthier, addressing the problem of medication non-adherence due to age related factors such as the inherent medication increase, cognitive losses and demotivation. It helps them with medication management, providing multimodality and context awareness, fully advising and supporting with the medication. 3.1. Personas, Context Scenarios and Requirements As already explained, Personas and Scenarios have a key role in supporting requirement analysis. Information on the Persona and scenarios developed for â€œMedication Assistantâ€� are presented below. 3.1.1. Persona Our Persona should live in Portugal, does not need to have the aptitude to use electronic devices, must be an elder person with health problems (although it is not necessary to have regular doctor appointments), and should have the desire to control its own medication. Furthermore, the Persona does not need to have a family, but commonly speaks with friends and family through a computer or mobile device. In the first phase we are developing our application in order to respond to the requirements of the Primary Persona. The Primary Persona is the Persona at which the application should address all the requirements. We identified the following expectations: the Persona wants to be able to use the same application with arthritis; the application should help prevent gaps in medication, report medication, report side effects, and provide alerts. Furthermore, the Persona wants to use the application even with little aptitude for working with electronic devices. Thereafter, we created the context scenarios, after which we identified the requirements. Primary Persona, Mrs. EmÃ­lia, lives in Coimbra with her husband Filipe Rodrigues. Sheâ€™s a housewife and does not have experience with electronic devices. She is right handed. She is diabetic and has arthritis in her right superior member. Her health condition requires a regular and daily medication. Mrs. EmÃ­lia has the habit to call her daughter during dinner preparation. However, she has some difficulty in doing the two activities simultaneously due to her limitation in the right superior member. She has weekly appointments in her local health center for surveillance. Mrs. EmÃ­lia would like to buy equipment that facilitates the contact with her daughter and allows her to control her medication, which she often forgets to take. 402 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 3.1.2. An example of a context scenario Our context scenarios intend to describe several scenarios of application usage. These scenarios outline different ways to interact with the application, as well as its features. Each scenario is represented by a short story. Two scenario examples are presented below. Mrs. EmÃ­lia waked up to prepare breakfast. The application showed an alert (with the medication names and dosage) warning that Mrs. EmÃ­lia needed to take her medication in the fasted state. Mrs. EmÃ­lia, warned that she had to take the medication, took it immediately and then prepared breakfast. After a while, the application asked Mrs. EmÃ­lia if she had already taken the medication. The application interacted through speech, since Mrs. EmÃ­lia hands were busy. She answered â€œYes, I took my medicationâ€�. A few hours later Mrs. EmÃ­lia starts to feel unwell. She felt concern and, through speech, she asked the application if that condition was normal. In order to answer that, the application explained that she took a medication that could induce a feeling of being unwell, also giving information about what is the reason to take the medication and what are the side effects of it. Thereafter, Mrs. EmÃ­lia felt relieved and returned to her tasks. Table 1 - Context Scenario Mrs. EmÃ­lia Action Action on smartphone application Output from smartphone application to user Wakes up and prepares breakfast. Prepares an alert to show The application triggers an alert: â€œYou need to take medication in the fasted stateâ€� Takes the smartphone and read the alert. The user opens the alert [IM=Touch]. Opens the alert. Shows the list of medications to take [IM=Text and Images] Takes the medication and lock de mobile. Closes application and lock. Locks Screen Eats the breakfast. Needs to know if the medication was taken. Prepares a speech message. Shows a message: â€œDid you take the medication in fasted state?â€� [IM=Text and Speech] Answers â€œYes, I took my medicationâ€� [IM=Speech] Recognizes the answer. Notes the take of the medication. Locks. Locks Screen. A few hours later, starts to feel unwell. The user unlocks the mobile and asks â€œIâ€™m felling unwell, is it normal?â€� [IM=Speech] Recognizes the sentence. Find if any medication taken has side effects and prepares the response. Shows a message: â€œYes, the MEDX could induce a feeling of being unwell. It can induce nauseas too. But you should take it for arthritis.â€� [IM=Text and Speech] Feels relieved. 3.2. Main requirements The requirements were divided in two main groups: the functional requirements and the user requirements. The main functional requirements are: (1) the application should provide medication insertion and management by third parties, so that seniors do not need to perform this task since it can be complicated; (2) the application should provide medication alerts to remember users about medication schedules; (3) the application should provide medication advice to help elders in daily medication questions. It must be able to respond to commands given in Portuguese expressing questions such as â€œWhat should I do if I forget to take 403 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 my medication?â€�, â€œPlan the day because Iâ€™m leavingâ€� or â€œIâ€™m with headaches. Is it a side effect?â€�; (4) the application should provide configurable interaction; (5) the application should provide multimodal interaction; (6) the application should allow registry of medication taken. The main user requirements are: (1) the application should inform users in an everyday language since technical language could be misunderstood; (2) the application should provide touch and speech interaction for everything in order to facilitate user interaction since some users may have physical or cognitive limitations; (3) the application should be reliable and credible because the user must trust its advices and alerts; (4) as elders usually have low proficiency with technology and vision problems it is crucial that the application avoids overloading content and small icons; (5) the application should have extra care with language and dimension; (6) the application should adapt to the user and the context; (7) the application should provide personalization; (8) the application should avoid information overload; (9) the application should be able to provide more information when the user wants it; (10) the application should provide help to the elder when they present difficulties interacting with the application; (11) the application should provide a group of â€œhow to use guidesâ€�. 3.3. Prototype 2 â€“ Meeting the requirements After completing one development cycle, which resulted in a first prototype, the data gathered during its evaluation was used to feed the following development cycle. Furthermore, beyond the consideration of user feedback, existing features have been expanded and new features added, resulting in a second application prototype. In order to enable the insertion of medication and its management by third parties, we created an external service. The smartphone uses the service to get all the information related to the medication. When the application is open for the first time it will ask for a login that will be used to get the elderly medication plan. In the second prototype the insertion was made by a formal or informal caregiver, i.e., the elderly didnâ€™t need to perform this task. This has the advantage of preventing the elderly from getting bored and demotivated to use the application, since this task can be tiresome and time consuming. However, this feature needs to be improved in order to simplify the insertion process. The application provides medication alerts using both Windows Phone push notification and local notification. If the application is connected to the Internet, the user can receive push notifications. However, when the application opens, it creates local notifications for the next four alerts and informs the push notification service that it only needs to work for the fifth alert. This process is executed whenever the application opens. Therefore, the push notification service is only required when the elderly do not open the application for the next four straight alerts. This way it is very likely that the application will inform the user in the need of taking the medication in a timely fashion, even without constant access to the Internet, creating a high level of credibility and reliability, very important for older adults. Furthermore, the application allows the elderly to see the next alerts list. 404 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 When the elderly forget to take medication, or want to know what its side effects are, they can refer to the application. All the medication should have information about what to do if the user forgets to take it, what are the side effects, and why the elderly needs to take it. In this way, the application can give that information to the elderly when needed. In the prototype, it is possible to know the reason of taking that medication, what are the side effects, the expiration date of the medication, and what to do if a medication intake was forgotten. We will add additional information to the medication in order to allow the elderly to get more advice about it. This information is based on the medicines leaflets, increasing the credibility and reliability of the information given. The application provides a set of options so the elderly can change the application settings. The user can allow noise control features, change the font size, change between dark and light mode and activate the help mode. Regarding the noise control feature, for example, if it is activated the application will automatically detect the user context noise and adapt its volume. The user can also change interaction features such as how speech commands are input (touch-to-speak or use of speech by voice activation) and activate the auto-zoom. The touch-to-speak feature will allow elderly to touch a button and speak to the application. The application can also be configured to automatically detect speech inputs without any type of touch interaction. However, this alternative is not so precise and has not yet been carefully evaluated. The auto-zoom feature adapts the size of text and images to the user distance to the smartphone. If this feature is activated, the size of the items on the screen will automatically increase when the user moves away or closer to the smartphone depending on the user context: if the user has nearsightedness, when approaching the screen the size of the images/text will increase; if the elderly has astigmatism, when moving farther from the screen the size of the images/text will increase too. Furthermore, when the elderly needs to know more information about a specific medication, the application will adapt to the needs. Thereby, the application provides personalization, configurable interaction, extra information and user/context adaptation. The elderly can use touch or speech as input in order to interact with the application. The speech can be used to get advice (e.g. â€œI forgot to take the lunch medicationâ€�) or to get extra information (e.g. â€œWhat are the side effects of this medication?â€�). The advantage of speech is that it normally provides a faster and more intuitive way to get response from the system than touch. As output we use text, speech and images. Thus, in the second prototype we already used multiple modalities both for input and output, making it really multimodal [18]. As the elders usually have low proficiency with technology and vision problems, we created a User Interface based in big text and big items/images. The UI follows the Metro Style guidelines, avoiding extra bars, icons, buttons, etc. [19]. The application has a simple and clear layout that gives the user the opportunity to get more information when required (through speech) to avoid information overload in the views. Furthermore, the auto- zoom feature will be aware of the user difficulties and adapts the UI to it. On the other hand, the application tries to give information in a common and informal language avoiding the technical one. However, this is a requirement that needs more improvement since it is hard to replace the medication names and side effects by informal and common names. Lastly, the application provides auto adaptable help. When the elderly are inexperienced, the application offers many suggestions, but when they learn to interact with the application it will stop providing them. However, if an expert user starts to show some difficulty (e.g. increasing time to perform tasks) using one of the features, the application adapts and starts offering suggestions again. In addition, the application provides a Fig. 1 - Medication Assistant example views: (a) application starting; (b) main menu; (c) advice menu; (d) next alerts; (e) about the medication. 405 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 group of â€œhow to use guidesâ€� composed by some example phrases about how to interact with it through speech. In the future, we will add video guides in order to help inexperienced users. 3.4. Evaluation 3.4.1. Methodology As mentioned, the evaluation method adopted for â€œMedication Assistantâ€� is based on a methodology proposed recently in [16]. A first evaluation of the prototype was conducted with a heterogeneous group of engineers and health professionals, with the intent of gathering an extended number of opinions and suggestions to validate the general design options and guide the following development. In the first prototype test the user was accompanied by an evaluator, member of the project, to explain the test and respond to user doubts. During this test the think aloud method was used and the audio was recorded. The group was composed by three women and one man with age ranges between 25 and 60 years. The results of this first evaluation of prototype was presented in [20] and used to support the development of a second prototype, for which evaluation results are presented in this paper. The second evaluation was made with a group of end users and followed a previously defined structured plan of evaluation with a set of tasks. The evaluation consisted of two phases: the interaction assessment phase and the usability evaluation phase. In both phases the user is accompanied by an evaluator to explain the evaluation method. The interaction phase is composed by a set of tasks requiring the user to interact with the application: for instance â€œSee the list of medications to be taken at breakfastâ€� and â€œInform the application that you missed the last medicationâ€�. In this phase the think aloud method is used and the evaluator takes notes regarding users behavior and their main difficulties, comments, doubts, suggestions and problems during the completion of the tasks. In the second phase, pertaining usability, the evaluator applies a questionnaire about the user interaction experience with the application. In this stage the user answers some questions about the application, such as â€œIn your opinion what are the strengths and weaknesses of the application?â€� and â€œWhat would you change in the application?â€�. The user should give an opinion about some aspects of the application such as the layout, font size and color, the features and the interaction. The second evaluation of the prototype was with a group of three women and one man, with ages between 57 and 76 years. Accordingly to [21], the sample size is appropriate to a qualitative evaluation, considered adequate for a second prototype evaluation. 3.4.2. Results In this section we present the results of the evaluation. Regarding usability evaluation, through the analysis of the opinion questionnaires, the results that stand out are the strengths and weaknesses of the application and the suggestions of changes to be made. Fig. 2 shows the strengths and weaknesses of the â€œMedication Assistantâ€�. In the tag clouds the words with larger font size are the most referred by the users and the smaller ones are the least identified by the sample. Fig. 2 â€“ (left box) strengths of the application; (right box) weaknesses of the application 406 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 The changes suggested by the users are presented in the following table (In each type of alteration are presented the object and the action to be taken, accordingly to the user suggestions. The last column presents the priority of the alteration, according to the development team opinion. Regarding interaction evaluation, based on the data collected from observation and notes of the evaluators several features were identified that users considered more difficult or easier to use. The overall results presented in the two graphics depicted in Fig. 3, showing the number of users that considered each feature difficult or easy to use. ) and organized by types. In each type of alteration are presented the object and the action to be taken, accordingly to the user suggestions. The last column presents the priority of the alteration, according to the development team opinion. Regarding interaction evaluation, based on the data collected from observation and notes of the evaluators several features were identified that users considered more difficult or easier to use. The overall results presented in the two graphics depicted in Fig. 3, showing the number of users that considered each feature difficult or easy to use. Table 2 - Application improvements suggested by users on the evaluation questionnaire Type Object Action Comment New features Tutorial Add Priority Register Add Priority Interaction Speech input Improve Priority Design/Layout Panoramic view Improve Nonpriority Real menu Improve Nonpriority 4 3 4 2 1 Difficulties 3 4 2 4 Easy to use 407 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 4. Conclusions Since the beginning of the â€œMedication Assistantâ€� application development, one of the main focuses was the inclusion of end-users. User intervention is crucial since the initial phase, mostly with their participation in brainstorming sessions to generate ideas and support the definition of Persona and scenarios. During the continuous development process of the application users have an active role, by participating in the process of evaluation, thus enabling the application to be shaped accordingly to the usersâ€™ needs and capabilities, suffering a continuous process of redesign. In this paper all this process is explained and the results of the second prototype are presented. The data analysis was qualitative, taking into account the size of the evaluation sample. In the data analysis priority was given to the questionnaires, which provide information related to strengths and weaknesses of the application and give suggestions of changes proposed by users. By analyzing the strengths of the application we can say that the application is already useful, even being on a prototype development phase. Features like forgetfulness support, medication images and the expiration date were proposed in the brainstorming, and as we can see they seem to be of interest for the end-users. The Help menu feature is a result of the first prototype evaluation. It shows that our method can lead to the development of useful applications for end-users. The weaknesses can provide important information too. First, they provide basis for the next iteration and prototype. Second, they show that interaction is really important as we defend in our proposed method. Since we are in the second prototype, the interaction features are only in the initial phase of development, unlike other features of the application, being the reported weaknesses expected. With the intent tackle the needs identified in the evaluation of the second prototype, the future work should focus, mainly, in the development of a new prototype, considering particularly touch and speech interaction. Acknowledgements This work is part of the Smart Phones for Seniors (S4S) project, a QREN project (QREN 21541), co-funded by COMPETE and FEDER. References [1] DESA. (2013). Available: [2] H. Matlabi, S. Parker, and K. McKee, \"The contribution of home-based technology to older people's quality of life in extra care housing,\" BMC Geriatrics, vol. 11, p. 68, 2011. [3] A. Newell, J. Arnott, A. Carmichael, and M. Morgan, \"Methodologies for Involving Older Adults in the Design Process,\" in Universal Acess in Human Computer Interaction. Coping with Diversity. vol. 4554, C. Stephanidis, Ed., ed: Springer Berlin Heidelberg, 2007, pp. 982-989. [4] R. Eisma, A. Dickinson, J. Goodman, O. Mival, A. Syme, and L. Tiwari, \"Mutual inspiration in the development of new technology for older people,\" in In Proceedings of Include 2003, 2003, pp. 7--252. [5] E. Mynatt and W. Rogers, \"Developing technology to support the functional independence of older adults,\" Ageing International, vol. 27, pp. 24-41, 2001/12/01 2001. [6] R. Eisma, A. Dickinson, J. Goodman, A. Syme, L. Tiwari, and F. Newell, \"Early user involvement in the development of information technology-related products for older people,\" Univers. Access Inf. Soc., vol. 3, pp. 131-140, 2004. [7] S. Chamberlain, H. Sharp, and N. Maiden, \"Towards a framework for integrating agile development and user-centred design,\" presented at the Proceedings of the 7th international conference on Extreme Programming and Agile Processes in Software Engineering, Oulu, Finland, 2006. [8] J. Gulliksen, A. Lantz, and I. Boivie, User Centered Design in Practice - Problems and Possibilities, 1999. [9] R. D. Buurman, \"User-centred design of smart products,\" Ergonomics, vol. 40, pp. 1159-1169, 1997/10/01 1997. [10] J. Pruitt and J. Grudin, \"Personas: practice and theory,\" presented at the Proceedings of the 2003 conference on Designing for user experiences, San Francisco, California, 2003. [11] J. Grudin and J. Pruitt. (2002, Personas, Participatory Design and Product Development: An Infrastructure for Engagement. Fig. 3 â€“ Application features and corresponding number of users who considered them difficult (left) and easy (right). 408 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 [12] R. Sinha, \"Persona development for information-rich domains,\" presented at the CHI '03 Extended Abstracts on Human Factors in Computing Systems, Ft. Lauderdale, Florida, USA, 2003. [13] R. Casas, R. B. Maro, A. Robinet, A. R. Delgado, A. R. Yarza, J. Mcginn, R. Picking, and V. Grout, \"User Modelling in Ambient Intelligence for Elderly and Disabled People,\" presented at the Proceedings of the 11th international conference on Computers Helping People with Special Needs, linz, Austria, 2008. [14] C. M. Hughes, \"Medication non-adherence in the elderly: how big is the problem?,\" Drugs Aging, vol. 21, pp. 793-811, 2004. [15] M. BÃ¶hm, H. Schumacher, U. Laufs, P. Sleight, R. Schmieder, T. Unger, K. Teo, and S. Yusuf, \" Effects of nonpersistence with medication on outcomes in high-risk patients with cardiovascular disease,\" American Heart Journal, vol. 166, 2013. [16] A. I. Martins, A. QueirÃ³s, M. Cerqueira, N. P. da Rocha, and A. J. S. Teixeira, \"The International Classification of Functioning, Disability and Health as a Conceptual Model for the Evaluation of Environmental Factors.,\" Procedia CS, vol. 14, pp. 293-300, 2012. [17] A. Cooper, R. Reimann, and D. Cronin, About Face 3: The Essentials of Interaction Design: Wiley Pub., 2007. [18] A. J. S. Teixeira, F. Ferreira, N. Almeida, A. F. Rosa, J. Casimiro, S. Silva, A. QueirÃ³s, and A. Oliveira, \"Multimodality and Adaptation for an Enhanced Mobile Medication Assistant for the Elderly,\" in Third Mobile Accessibility Workshop (MOBACC), CHI 2013 Extended Abstracts, France, 2013. [19] Microsoft. (2013, Metro Design Language of Windows Phone 7 | Microsoft Design .toolbox. Available: [20] F. Ferreira, N. Almeida, J. C. Pereira, A. F. Rosa, A. Oliveira, and A. Teixeira, \"Multimodal and Adaptable Medication Assistant for the Elderly - A prototype for Interaction and Usability in Smartphones,\" presented at the CISTI'2013 - 8Âª ConferÃªncia IbÃ©rica de Sistemas e Tecnologias de InformaÃ§Ã£o, Lisbon, 2013. [21] J. Nielsen. (2012, How Many Test Users in a Usability Study? Jakob Nielsen's Alertbox. Available: odologies for Involving Older Adults in the Design Process,\" in Universal Acess in Human Computer Interaction. Coping with Diversity. vol. 4554, C. Stephanidis, Ed., ed: Springer Berlin Heidelberg, 2007, pp. 982-989. [4] R. Eisma, A. Dickinson, J. Goodman, O. Mival, A. Syme, and L. Tiwari, \"Mutual inspiration in the development of new technology for older people,\" in In Proceedings of Include 2003, 2003, pp. 7--252. [5] E. Mynatt and W. Rogers, \"Developing technology to support the functional independence of older adults,\" Ageing International, vol. 27, pp. 24-41, 2001/12/01 2001. [6] R. Eisma, A. Dickinson, J. Goodman, A. Syme, L. Tiwari, and F. Newell, \"Early user involvement in the development of information technology-related products for older people,\" Univers. Access Inf. Soc., vol. 3, pp. 131-140, 2004. [7] S. Chamberlain, H. Sharp, and N. Maiden, \"Towards a framework for integrating agile development and user-centred design,\" presented at the Proceedings of the 7th international conference on Extreme Programming and Agile Processes in Software Engineering, Oulu, Finland, 2006. [8] J. Gulliksen, A. Lantz, and I. Boivie, User Centered Design in Practice - Problems and Possibilities, 1999. [9] R. D. Buurman, \"User-centred design of smart products,\" Ergonomics, vol. 40, pp. 1159-1169, 1997/10/01 1997. [10] J. Pruitt and J. Grudin, \"Personas: practice and theory,\" presented at the Proceedings of the 2003 conference on Designing for user experiences, San Francisco, California, 2003. [11] J. Grudin and J. Pruitt. (2002, Personas, Participatory Design and Product Development: An Infrastructure for Engagement. Fig. 3 â€“ Application features and corresponding number of users who considered them difficult (left) and easy (right). 408 FlÃ†vio Ferreira et al. / Procedia Computer Science 27 ( 2014 ) 398 408 [12] R. Sinha, \"Persona development for information-rich domains,\" presented at the CHI '03 Extended Abstracts on Human Factors in Computing Systems, Ft. Lauderdale, Florida, USA, 2 PROCS 2894 S1877-0509(14)00046-5 10.1016/j.procs.2014.02.044 The Authors ☆ Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). Elderly Centered Design for Interaction – The Case of the S4S Medication Assistant Flávio Ferreira a Nuno Almeida a b Ana Filipa Rosa a André Oliveira a José Casimiro c Samuel Silva a b António Teixeira a b ⁎ a Institute of Electronics and Telematics Engineering of Aveiro (IEETA), 3810-193 Aveiro, Portugal b Dep. of Electronics Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal c Polytecnic Institute of Tomar, 2300-531 Tomar, Portugal ⁎ Corresponding author. Tel.: +351 234370500; fax: +351 234370545. Several aspects of older adults’ life can benefit from recent technological developments, but success in harnessing this potential depends on careful design and accessible, easy to use products. Design and development must be centered on the elderly and adequately consider interaction. In this paper we follow this design approach and put it to the test in developing a concrete application, aimed to contribute to lower the high levels of non-adherence to medication in the elderly population. The “Medication Assistant” application was developed following an iterative method centered, from the start, on the elderly and interaction design. The method repeats short-time development cycles integrating definition of scenarios and goals, requirements engineering, design, prototyping and evaluation. Evaluation, by end-users, of the increasingly refined prototypes, is a key characteristic of the method. The evaluation results provide information related to strengths and weaknesses of the application and yield suggestions regarding changes and improvements, valuable support further development. Results regarding evaluation of the second prototype of “Medication Assistant” are presented. Keywords development method interaction user centred elderly evaluation mobile applications medication References [1] DESA. (2013). Available: [2] H. Matlabi, S. Parker, and K. McKee, “The contribution of home-based technology to older people's quality of life in extra care housing,” BMC Geriatrics, vol. 11, p. 68, 2011. [3] A. Newell, J. Arnott, A. Carmichael, and M. Morgan, “Methodologies for Involving Older Adults in the Design Process,” in Universal Acess in Human Computer Interaction. Coping with Diversity. vol. 4554, C. Stephanidis, Ed., ed: Springer Berlin Heidelberg, 2007, pp. 982-989. [4] R. Eisma, A. Dickinson, J. Goodman, O. Mival, A. Syme, and L. Tiwari, “Mutual inspiration in the development of new technology for older people,” in In Proceedings of Include 2003, 2003, pp. 7--252. [5] E. Mynatt and W. Rogers, “Developing technology to support the functional independence of older adults,” Ageing International, vol. 27, pp. 24-41, 2001/12/01 2001. [6] R. Eisma, A. Dickinson, J. Goodman, A. Syme, L. Tiwari, and F. Newell, “Early user involvement in the development of information technology-related products for older people,” Univers. Access Inf. Soc., vol. 3, pp. 131-140, 2004. [7] S. Chamberlain, H. Sharp, and N. Maiden, “Towards a framework for integrating agile development and user-centred design,” presented at the Proceedings of the 7th international conference on Extreme Programming and Agile Processes in Software Engineering, Oulu, Finland, 2006. [8] J. Gulliksen, A. Lantz, and I. Boivie, User Centered Design in Practice - Problems and Possibilities, 1999. [9] R. D. Buurman, “User-centred design of smart products,” Ergonomics, vol. 40, pp. 1159-1169, 1997/10/01 1997. [10] J. Pruitt and J. Grudin, “Personas: practice and theory,” presented at the Proceedings of the 2003 conference on Designing for user experiences, San Francisco, California, 2003. [11] J. Grudin and J. Pruitt. (2002, Personas, Participatory Design and Product Development: An Infrastructure for Engagement. [12] R. Sinha, “Persona development for information-rich domains,” presented at the CHI’03 Extended Abstracts on Human Factors in Computing Systems, Ft. Lauderdale, Florida, USA, 2003. [13] R. Casas, R.B. Maro, A. Robinet, A.R. Delgado, A.R. Yarza, J. Mcginn, R. Picking, and V. Grout, “User Modelling in Ambient Intelligence for Elderly and Disabled People,” presented at the Proceedings of the 11th international conference on Computers Helping People with Special Needs, linz, Austria, 2008. [14] C. M. Hughes, “Medication non-adherence in the elderly: how big is the problem?,” Drugs Aging, vol. 21, pp. 793-811, 2004. [15] M. Böhm, H. Schumacher, U. Laufs, P. Sleight, R. Schmieder, T. Unger, K. Teo, and S. Yusuf, “Effects of nonpersistence with. medication on outcomes in high-risk patients with cardiovascular disease,” American Heart Journal, vol. 166, 2013. [16] A. I. Martins, A. Queirós, M. Cerqueira, N.P. da Rocha, and A. J. S. Teixeira, “The International Classification of Functioning, Disability and Health as a Conceptual Model for the Evaluation of Environmental Factors.,” Procedia CS, vol. 14, pp. 293-300, 2012. [17] A. Cooper, R. Reimann, and D. Cronin, About Face 3: The Essentials of Interaction Design: Wiley Pub., 2007. [18] A. J. S. Teixeira, F. Ferreira, N. Almeida, A.F. Rosa, J. Casimiro, S. Silva, A. Queirós, and A. Oliveira, “Multimodality and Adaptation for an Enhanced Mobile Medication Assistant for the Elderly,” in Third Mobile Accessibility Workshop (MOBACC), CHI 2013 Extended Abstracts, France, 2013. [19] Microsoft. (2013, Metro Design Language of Windows Phone 7 | Microsoft Design .toolbox. Available: [20] F. Ferreira, N. Almeida, J.C. Pereira, A.F. Rosa, A. Oliveira, and A. Teixeira, “Multimodal and Adaptable Medication Assistant for the Elderly - A prototype for Interaction and Usability in Smartphones,” presented at the CISTI’2013-8ª Conferência Ibérica de Sistemas e Tecnologias de Informação, Lisbon, 2013. [21] J. Nielsen. (2012, How Many Test Users in a Usability Study? Jakob Nielsen's Alertbox. Available: "
    },
    {
        "doc_title": "ICF inspired personas to improve development for usability and accessibility in Ambient Assisted Living",
        "doc_scopus_id": "84897787639",
        "doc_doi": "10.1016/j.procs.2014.02.045",
        "doc_eid": "2-s2.0-84897787639",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Ambient assisted living (AAL)",
            "Health condition",
            "Personas",
            "Research and development",
            "Scenarios",
            "Usability and accessibility",
            "User interaction"
        ],
        "doc_abstract": "Ambient Assisted Living (AAL) is an important research and development area. The acceptance of the AAL paradigm is closely related to the quality of the available systems and services, namely in terms of the user interaction. This means that usability and accessibility are crucial issues. The paper presents how the concepts of the International Classification of Functioning, Disability and Health (ICF) can be used to optimize the role of personas and scenarios in the development and evaluation of AAL systems and services, especially in aspects related with human functioning and health conditions. © 2013 The Authors. Published by Elsevier B.V.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2014-02-25 2014-02-25 2014-10-23T08:54:46 S1877-0509(14)00047-7 S1877050914000477 10.1016/j.procs.2014.02.045 S300 S300.2 HEAD-AND-TAIL 2021-10-14T13:10:44.354035Z 0 0 20140101 20141231 2014 2014-02-25T00:00:00Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 false 27 27 C Volume 27 46 409 418 409 418 2014 2014 2014-01-01 2014-12-31 2014 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, DSAI 2013 Dr. Manuel Pérez Cota Dr. João Barroso Dr. Simone Bacellar Leal Ferreira Dr. Benjamim Fonseca Dr. Tassos Mikropoulos Dr. Hugo Paredes article fla Copyright © 2013 The Authors. Published by Elsevier B.V. ICFINSPIREDPERSONASIMPROVEDEVELOPMENTFORUSABILITYACCESSIBILITYINAMBIENTASSISTEDLIVING QUEIROS A QUEIROSX2014X409 QUEIROSX2014X409X418 QUEIROSX2014X409XA QUEIROSX2014X409X418XA Full 2014-02-25T10:44:25Z OA-Window ElsevierWaived 0 item S1877-0509(14)00047-7 S1877050914000477 10.1016/j.procs.2014.02.045 280203 2014-10-23T04:53:38.255571-04:00 2014-01-01 2014-12-31 true 332084 MAIN 10 56611 849 656 IMAGE-WEB-PDF 1 P r o c e d i a C o m p u t e r S c i e n c e 2 7 ( 2 0 1 4 ) 4 0 9 4 1 8 1877-0509 ' 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). doi: 10.1016/j.procs.2014.02.045 ScienceDirect 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, DSAI 2013 ICF inspired Personas to improve development for usability and accessibility in Ambient Assisted Living Alexandra QueirÃ³s a *, Margarida Cerqueira a,e , Ana Isabel Martins b,d , Anabela G. Silva a , Joaquim AlvarelhÃ£o a , AntÃ³nio Teixeira b,c , Nelson Pacheco Rocha b,d a Health Sciences School, University of Aveiro, Campus UniversitÃ¡rio, 3810-193 Aveiro, Portugal b Institute of Electronics and Telematics Engineering of Aveiro, Campus UniversitÃ¡rio, 3810-193 Aveiro, Portugal c Department of Electronics Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal d Health Sciences Department, University of Aveiro, Campus UniversitÃ¡rio, 3810-193 Aveiro, Portugal e Unidade de InvestigaÃ§Ã£o e FormaÃ§Ã£o sobre Adultos e Idosos, 4050-313 Porto, Portugal Abstract Ambient Assisted Living (AAL) is an important research and development area. The acceptance of the AAL paradigm is closely related to the quality of the available systems and services, namely in terms of the user interaction. This means that usability and accessibility are crucial issues. The paper presents how the concepts of the International Classification of Functioning, Disability and Health (ICF) can be used to optimize the role of personas and scenarios in the development and evaluation of AAL systems and services, especially in aspects related with human functioning and health conditions. Â© 2013 The Authors. Published by Elsevier B.V. Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). Keywords: Personas; Scenarios; Usability and accessibility, Ambient Assisted Living developments 1. Introduction The development of technological solutions with natural and effective user interaction mechanisms can facilitate the daily lives of the elderly, decrease isolation and info-exclusion, and promote ability to work, independence and wellness conditions. Based on this view the Microsoft Language Development Centre (the research and development centre of Microsoft Portugal), the University of Aveiro, the Engineering Faculty of the University of Porto, the Institute of Electronics and Telematics Engineering of Aveiro, the Institute of Systems and Computers * Corresponding author. Tel.: +351 234 401 558; fax: +351 234 370 089. E-mail address: alexandra@ua.pt Available online at www.sciencedirect.com ' 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). 410 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 Engineering of Porto and two small and medium enterprises, Micro I/O and Plux Wireless Biosignals, implemented a Living Usability Lab (LUL) located on the University of Aveiro campus (Aveiro, Portugal) [1, 2]. Although traditional approaches for the systems and services development usually consider the involvement of all stakeholders, including end users, it is nevertheless also true that the experts are the responsible for the conception, design and development of the systems and services, deciding which functions to integrate and how users interact with them. Users are only involved at an advanced stage, when the concepts are already consolidated, which means, in general, that the initial conceptualization is not based on their experiences or mental models. In contrast, a central aspect of a living lab methodology [2, 3] is the continuing involvement of the different stakeholders at all stages of the development process: idealization of concepts, requirements analysis, systems and services development and respective validation and assessment in semi-realists and structured contexts. The LUL aims to improve translational research and to promote the development, integration, validation and evaluation of new user interaction technologies related with Ambient Assisted Living (AAL) systems and services. These objectives are of most importance, given that ALL systems and services must have a broad range of intelligent functions in terms of user interaction, supported by usable and accessible interfaces with adaptive mechanisms [4], which is not always taken into due account. A recent systematic literature review [5] shows that only a low percentage of papers related with AAL refer the involvement of end-users, namely elderly and disabled people both in the development phases and in the results validation and evaluation. Therefore, it is clear that the usability and accessibility issues are not sufficiently explored within the AAL domain. Furthermore, knowing that AAL systems and services have complex interaction mechanisms, including explicit and implicit interactions, and that multimodality is a fundamental aspect [6], it becomes evident the need for additional concerns in terms of usability and accessibility. To solve the usability and accessibility issues, a close involvement of the end users is recommended. However, these could be less successful when the user population is varied and the involvement of elderly and disabled people introduces additional difficulties [7, 8]. Moreover, methods to obtain requirements and evaluation data from elderly and disabled people are not straightforward and monitoring or interacting with them in their home environments, rather than in a workplace situation, require additional organisational challenges [9, 10]. In these aspects, infrastructures like LUL could have an important role, if there is a methodological framework for the specification, development, validation and evaluation of new systems and services. This paper is not intended to be a comprehensive description of the LUL and the related technological services, nor the presentation of its wide range of methodological tools, but rather the explanation of how the concepts of personas and scenarios can be enhanced to ensure that the different stakeholders involved in the development of a given AAL system or service have a common view of the target users in all the development stages, which, consequently, may contribute to better solutions in terms of usability and accessibility. Considering that the AAL paradigm focuses on services that aim to maintain the human functioning and minimize the consequences of health conditions, within the methodological development of the LUL it was a major concern the use of systematic mechanisms to characterize the issues related to functioning, health conditions and quality of life. Thanks to an interdisciplinary collaboration in which the authors are involved [11, 12], it was possible to use the International Classification of Functioning, Disability and Health (ICF) conceptual model [13] developed by the World Health Organization (WHO) to optimize the essential role of personas and scenarios. This approach has the advantage of emphasizing the functioning in the development of new AAL systems and services, as well as the contextual factors that impact on those systems and services. It has been used, and consequently improved, in various projects related with AAL, namely AAL4ALL and Smartphones for Seniors. In addition to this introductory section the paper is composed of another four sections: Living Usability Lab, Conceptual Model for the Definition of Personas and Scenarios, Example of a Persona and Respective Scenario, and Conclusions. 411 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 2. Living Usability Lab The LUL is a collaborative, multi-contextual and empirical environment to allow [1, 2]: i) the development, integration and evaluation of systems and services with demanding interactive requirements, such as AAL solutions; ii) the development of comprehensive methodologies to support the different design and development phases of systems and services, from the initial conception to the final evaluation, involving users and other stakeholders; iii) the translational research related to interaction mechanisms and their adequacy to the users, the nature of the tasks, the surrounding contexts and the technological platforms and iv) the conception of reference models to consolidate knowledge related to the users' needs and critical mechanisms associated with the quality of services, interaction paradigms and development platforms. Living lab methodologies follow a perspective that conceives the users and the society as sources of innovation [14]. Therefore, within LUL, the stakeholders (including potential end users, formal and informal care providers, governmental and local organizations, research and development entities, funding agencies or industrial partners) must actively participate in the iterative process of specification, development, validation and evaluation that has three reference phases: the first one is the conceptual validation which is followed by the prototype test and, finally, the pilot test. The main objective of the conceptual validation is to verify the sustainability of the initial concepts for potential systems or services and if they deserve to be explored. Additionally, the conceptual validation should gather all the required information to allow the development of consolidated prototypes. In turn, the main purpose of the prototype test phase is to evaluate the systems and services in terms of usability and accessibility. The prototype test is conducted in a controlled environment involving real users and the results should be incorporated in the development to allow the implementation of systems and services to be evaluated during the pilot test. Finally, the third methodological phase, the pilot test, aims to assess not only usability and accessibility, but also the degree of user satisfaction and determine how the systems and services impact on the usersâ€™ daily lives. These different phases are not isolated from each other, but complementary. Moreover, the process, although sequential must be sufficiently flexible to allow advance and retreat along the different phases, since the assessment performed at each phase dictates the next steps in the course of the development of the systems and services. The methodology relies thus on a spiral development, in which it is possible to advance and retreat through the different methodological phases (Figure 1). Figure 1. Reference phases of the LUL methodology. Within the LUL, end users may be [1] i) elderly interested in maintaining their typical activities and everyday life tasks, including social participation; ii) elderly with activity limitations and participation restrictions due to specific 412 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 health conditions and iii) other informal caregivers that to provide care to people with special needs. It is also a LUL goal the involvement of formal carers (health care or social care providers) potentially interested in innovative systems and services to supervise and assist remotely their clients. Other service providers to consider are i) professionals of occupational or day centres able to offer, not necessarily for free, leisure activities and ii) other professionals that intend to develop specialized business solutions targeted to elderly. To complete a categorization which includes all the necessary contributions of the different stakeholders for the LUL success, four additional categories must be considered: i) researchers (either because their research can boost innovative projects, or because they can benefit from the LUL synergies, namely the ability to engage a large number of users and the proximity to social and business organizations); ii) students (whether in the engineering area, health or social sciences areas); iii) industrial partners interested in innovative solutions and to take advantage of the LUL added value and iv) policy makers who may help to create and maintain a supporting network for the LULâ€™s innovative environment. Given the potential users range, whether end users or services providers, efficient tools are needed to characterize and to ensure that this characterization is taken into account throughout the development process. In particular, the experience of research and development fostered by LUL has shown that the elaboration of personas is an essential tool so that the involved parties always consider, in a clear and throughout manner, the design phases (development, validation and assessment), the essential characteristics and needs of the potential end users, which obviously have a significant impact in terms of usability and accessibility of the interaction mechanisms. 3. Conceptual Model for the Definition of Personas and Scenarios Personas are potential usersâ€™ group representations with characteristics, needs, behaviours, motivations and common expectations. According to Cooper [15], despite being fictitious, personas must be precisely and accurately defined from detailed knowledge about the target population. The details are important to promote that all stakeholders share a consistent view of potential end users, allowing one â€œhuman face\" for a set of demographic data. Therefore, it is necessary to consider various aspects: Î¾ The characteristics of the potential users of the systems and services being developed and what roles they can play. Î¾ The potential usersâ€™ needs and behaviours, and how these may vary over time. Î¾ The potential usersâ€™ motivations and expectations in respect of the systems and services being developed. In systems development methodologies, scenarios often appear associated with personas. Typically, a scenario describes how a given system or service can be used, in a given context and in a given period of time. It is usually developed by usability experts, in a close collaboration with users and developers. The scenarios are written in a simple language, with minimal technical details, in order to provide to concerned parties a common example to focus their discussions, and should provide: Î¾ The surrounding contexts that need to be explored. Î¾ The situations or events that have impact on personas. Î¾ The information requirements that the system needs to incorporate. Î¾ The functions or actions requirements that the system must be able to perform. Î¾ The technological requirements that may be significant for the system performance. Î¾ The interaction requirements, i.e. what kind of precaution should be considered in the management of the interaction mechanisms that may vary accordingly to the scenarios being considered. Î¾ Other requirements, such as business or corporate requirements. Given the various methods available, there are many alternatives that can be considered as a starting point for the elaboration of personas and scenarios: interviews, questionnaires, observational studies, focus groups, brainstorms, nominal groups, creation workshops and day-books fulfilments. The information gathered through these methods allows systematizing the different personasâ€™ characteristics and elaborating the different scenariosâ€™ requirements. 413 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 Considering that potential end users of AAL systems and services are elderly, health conditions are important factors. Furthermore, AAL aims to increase the autonomy of older people and assist their daily living activities, improving, therefore, their functioning. This implies that end usersâ€™ health conditions and functioning must be highlighted. The ICF conceptualizes functioning associated to health conditions in an extended socio-ecological context which also considers the role of services, systems and technologies. The ICF considers the structures and body functions, activities, participation and contextual factors [13] as components of the personâ€™s functioning: Î¾ The body's functions entail the individualâ€™s physiological functions. The ICF defines impairment as any problem of the individual with his/her body functions. Physical functions depreciations can, in principle, have no consequences for the individual's ability to do activities, especially if there is a help aid that compensates a particular function depreciation (e.g. an individual with weak eyesight wearing glasses would not have a limitation; an individual with a missing leg with a prosthesis will be able to do activities). Î¾ Activities are the individualâ€™s recital of assignments and tasks. Difficulties with these activities are noted as activity limitations. Limitations are usually due to function depreciation of body functions but also due to environmental hindrances. When the individual's ability to do activities is evaluated the consequences of function depreciation or environmental barriers become clear. Î¾ Participation covers the individualâ€™s involvement in daily life and society. Difficulties in participation are classified as participation restrictions. The same way as activities limitations, the participation restrictions can be caused by weakness, illness and/or handicap but also due to environmental hindrances. There is also a restriction when the individual is not being able to do activities according to his/her own norms of what is considered acceptable. Î¾ The contextual factors are the environmental and personal factors which either enhance or limit the individual's functioning. The environmental factors are the physical, social or attitudinal world ranging from the immediate to more general environment. The personal factors entail elements that make people different and unique, such as life style, education level, gender, etnicity, life events or psychological characteristics. These factors are important to explain certain situations (e.g. two individuals with the same diagnosis or physical function depreciation may have different limitations when it comes to activities and participation). The environmental factors can have a positive (i.e. be facilitators) or negative impact (i.e. be barriers) on the individualâ€™s performance as a member of society, on the individualâ€™s capacity to execute actions or tasks, or on the individualâ€™s body function or structure. When coding an environmental factor as a facilitator, issues such as the accessibility of the resource, and whether access is dependable or variable, of good or poor quality, should be considered. In the case of barriers, it might be relevant to take into account how often a factor hinders the person, whether the hindrance is great or small, or avoidable or not. It should also be kept in mind that an environmental factor can be a barrier either because of its presence (e.g. negative attitudes towards people) or its absence (e.g. the unavailability of a needed service). The authors propose to use the ICF fundamental concepts related to functioning and performance in activities and participation for the specification, development and evaluation of AAL systems and services. In particular, the ICF concepts may be used for building personas, in order to consider and structure aspects such as: Î¾ The bodyâ€™s structures and functions. Î¾ The relevant aspects of health conditions that may constrain the systems or services use (health conditions should not be understood only as a disease, but also as specific features which are present in the persona, and that will restrain the systems or services use). Additionally, the ICF can also help to systematize particular aspects of personas, including: Î¾ Activities and participation (which has consequences on the daily routines). Î¾ Environmental factors (e.g. a noisy environment turns difficult to use the telephone). Furthermore, the ICF allows alternative forms over what is normally considered in personasâ€™ defining aspects, such as personal factors (e.g. the specific life history and individual lifestyle), which the ICF considers alongside 414 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 with environmental factors, or the contextual factors. In this respect, the ICF considers that the individualâ€™s functioning results of his/her own health conditions interacting with his/her contextual factors [16]. This dynamic interaction is important for scenariosâ€™ definition. A narrative of events or persona experienced situations (a persona can remain unchanged, but the scenario demands changes in the use of systems and services) can be enriched by the identification, in a given situation, of the events and the positive or negative factors (facilitators or barriers, according to the ICF) that impact on activity and participation of personas. In the following section it is explained how the ICF was used for personas and scenarios development within the LUL framework. 4. Example of a Persona and Respective Scenario As already mentioned, the conceptualisation of a persona must consider the potential end users of the systems and services being developed, based on existing or specifically collected data, namely demographic data. However, there should be always present the need for personalization, as can be seen in the persona Anselmo Pires represented in Table 1. Table 1. Persona Anselmo Pires. Example of a Persona: Anselmo Pires Persona: Principal. Name: Anselmo Pires. Age: 67 years old. Location: Matosinhos. Technological Proficiency: Basic computer skills. Position: Senior partner and manager of a carrier business. Personal History: Anselmo Pires is married to Marieta Pires and has two sons, Alberto and Romeu, and a 23 years old grandson, CÃ©sar. Recently, he had a heart attack that made him weaker. Anselmo started cycling with his son Alberto at the weekends. Since he is the senior partner and manager he has the habit to get up early in order to be the first to reach the office. Normally, he goes out of home without taking breakfast, which compensates with a mid-morning break in the patisserie near the office. Anselmo stays at the office up to 8PM and has dinner at home around 8:30PM with Marieta. Once a week, he plays chess with his grandson, a habit started in the first scholar holidays of his grandson. Motivation: His grandson is now in London to continue his studies and Anselmo wants to keep contact and proximity with him, since they have always been very close. Frustrations: Whenever he tries to talk to his grandson using a video call, Anselmo eventually gets bored with the process. Although he has some basic computer skills, making a video call is something that frustrates him, because of the associated complexity. Quote: \"I play chess with my grandson every week, there are already 16 years!\" The persona Anselmo Pires was developed according to the principles proposed by Cooper [15]. Analysing the persona description, we can easily see that the personal history consists of a set of personal characteristics and description of some aspects of daily routines. In particular, some information appears described in various ways. This is the case of computer literacy that is referred in the topic technological proficiency and also in the topic frustrations (\"Although he has some basic computer skills... \"). On the other hand, the points related to health conditions are focused on an existing pathology. Finally, it is possible to state a negative point of view, as stated in the discrimination of possible frustrations. 415 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 In contrast, in the model proposed by the authors, there is a concern to identify different groups of information: Î¾ Personal factors: individualâ€™s life history and characteristics. For example, he/she likes blue, he/she prefers voice interaction or he/she is funny. Î¾ Health conditions: generic term for diseases (acute or chronic), injuries or traumas. It may also include other circumstances like pregnancy, age, stress, congenital abnormality or predisposition. For example, an individual has limitations in mobility of the right upper limb and he/she suffers of lowback pain. Î¾ Daily routines: individualâ€™s planning and management of demands, tasks and obligations of everyday life. Î¾ Activity types: individualâ€™s execution of tasks or actions. For example, buy a book, write a letter, do a mental exercise or clean the floor. Î¾ Participation: individualâ€™s involvement in life situations. For example, social events, educational activities or household management. Therefore, instead of the heterogeneous topic personal history, the authors propose a clear distinction between personal factors and daily routines, because this information is considered essential in the context of AAL systems and services. On the other hand, the authors strongly suggest the neutral perspective associated to the ICF (it should only be identified those facts that determine functioning or performance), which implies some concerns with the description. For instance, instead of referring to \"Since he is the senior partner and manager he has the habit to get up early in order to be the first to reach the officeâ€� we should refer â€œhe is the senior partner and manager and he has the habit to get up early\", i.e. only neutral information. In this perspective, there should not also be references to frustrations, but only abilities and personal factors such as personal tastes. In terms of health conditions, these should not appear associated with a deficiency in the structure or function of the body or to an existing pathology, but rather with a holistic perspective. Often a limitation in activities or a restriction in participation occur due to health conditions, but they are also influenced by the context in which the activities or participation will be held. The definition of a persona using the ICF concepts should clarify what health conditions and context situations may interfere with the performance of the individual along with the description of daily routines and types of participation. Incidentally the concept participation appears as something innovative in the elaboration of personas, because it is something more complex than activity that contains information related to a life event. The modified version of the persona Anselmo Pires (Table 2), which has been used in the project Smartphones for Seniors[17], includes personal factors that may influence the way the user interacts with the systems and services: Anselmo has 67 years old, lives with his wife, has the fourth grade and basic computer skills and is the senior partner and manager of a carrier business. According to this information the interaction may have to be simplified due to his lower academic or computer literacy. The daily routines are also important elements in defining the personas as they will constrain the use of the available systems and services. In the current example, Anselmo has the habit of getting up early, not taking breakfast at home because he prefers to take a mid-morning break in the patisserie near his office and he works up to 8PM. The fact of taking breakfast in the patisserie can interfere with a smart phone interaction and the fact of working up to 8PM means, in principle, that he will only access leisure features after that time. The health conditions are associated to relevant information that will impact on the performance of some activities and participation. For example, Anselmo had an episode of hospital admission and was recommended to start performing physical exercises and to manage his health condition via regular monitoring of blood pressure. Activity and participation turn out to be implicit in health conditions. For example, the regular monitoring of blood pressure has impact in daily routines. It is also possible identify specific activities that are also driven by his health conditions (e.g. cycling with his eldest son). The case of participation is similar to activities since they may result from variations in health conditions and daily routines, or be identified as specific interests (e.g. Anselmo wants to keep a close contact with his grandson who is now in London). 416 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 Table 2. Modified version of the Persona Anselmo Pires. Example of a Persona: Anselmo Pires Name: Anselmo Pires [Personal factor]. Age: 67 years old [Personal factor]. Anselmo Pires lives in Matosinhos, with his wife Marieta Pires. Their marriage has 30 years and they still have the same complicity of the first years [Personal factor]. He has elementary education and basic computer skills. He is the senior partner and manager of a carrier business [Personal factor]. Recently Anselmo had to be hospitalized due to a suspect of a heart attack. He was recommended to do daily exercises and to monitor his blood pressure [Health condition]. Anselmo usually wakes up early. He doesnâ€™t take breakfast at home, he prefers to take it in the middle of the morning in a patisserie near his office. He stays at the office up to 8PM. Around 8:30PM, he has dinner with his wife at home [Daily routine]. At the weekend Anselmo goes cycling with his eldest son [Activity type]. He has a 23 years old grandson, CÃ©sar, and they play chess once a week. [Participation type], a habit started in the first scholar holidays of his grandson. Anselmo wants to keep a close contact with his grandson who is now in London. They have a strong relationship. In many occasions his grandson asks his opinion about what to do, whether is related with his personal life or work [Participation type]. The definition of scenarios should also consider all the existing information on the target usersâ€™ needs, in order to specify the functional requirements of the systems and services to be developed: Î¾ Problems scenarios: identification of the actual situations, events (positive or negative) that have impact in personasâ€™ activities. For example, the voice interaction is a barrier in a noisy environment or the keyboard should be big enough to facilitate the writing of a SMS. Î¾ Data requirements: information elements that the systems have to include. For example, in an email application the basic elements can be messages or contact lists. Î¾ Functional requirements: actions that the system has to complete, information that the system has to present as well technological requirements relevant to the way the system should perform (e.g. requirements in terms of video calls, the types of interfaces required or the need of a Wi-Fi network). Î¾ Interaction requirements: detailed information about how user interaction should be, which can vary according to the created scenarios, even if thereâ€™s no alteration in userâ€™s characteristics. For example, Anselmo likes to use keyboard, but when heâ€™s driving he uses voice interaction (the persona is the same, but the context has changed). Î¾ Other requirements: business, corporative or customer requirement. For example, prices, installation conditions, company characteristics that should be reflected in the systems and services. Identifying the problem is an important issue because it situates the activity to be achieved. For example, Anselmo likes to receive oral information related with his trucks in order to perform, simultaneously, two activities: getting information about the trucks situation while reading the documents left by his secretary in the previous day. In this example, associated with the identification of the problem, there are some data requirements (e.g. information on trucks) and interaction requirements (e.g. he likes to receive orally the information related with the trucks situation). Another problem identified was the need to monitor his blood pressure. Therefore, he needs an easy to use application that should not interfere with his activities. Table 3 presents how the authorâ€™s proposal was applied in the Smartphones for Seniors [17]. 417 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 Table 3. An illustrative application example of scenarios. Example of a Scenario for the Persona Anselmo Pires Anselmo starts his work at 8AM and the first task he does is to check the localization of the transports for that day [Problem scenario] and he wants this information early in the morning [Functional requirement]. Because he is an active person, he likes to receive this information orally [Interaction requirement] while he reads the documents left by his secretary in the previous day. Today Anselmo is upset because two trucks are immobilized in Switzerland due to an adverse whether situation. He is following the weather report with particular interest [Data requirement] and he has activated the feature of sound alert [Functional requirement] to know when the trucks start to travel. When the night comes, he still doesnâ€™t have any information. Anselmo has a family dinner to celebrate his 30 years of marriage, so he deactivates the sound alert and activates the vibrating alert [Interaction requirement]. Once a week Anselmo plays chess online [Problem scenario] with his grandson for keeping the regular contact and healthy competition, so he wants to have a shortcut in his personal computer to this application [Functional requirement]. Anselmo has several friends in Brazil [Problem scenario] and he uses Facebook to communicate with them, as it allows communication asynchronously, which is important due to the time difference [Functional requirement]. He uses his desktop personal computer, which he has to boot up every time, as it is usually off. By selecting a desktop shortcut he opens the browser to activate the Facebook He prefers an automatic access to Facebook, because he does not want to be bothered to log in every time he uses it and, furthermore, he often forgets the password [Functional requirement]). The first thing he does is to check his wall for new posts, scrolling up and down with the mouse to see if something grabs his eye. If so, he often replies using the keyboard [Interaction requirement], especially direct inquiries. He keeps a regular contact through the Facebook, sharing photographs (he would like to have a way to convert automatically the pictures size [Functional requirement]) and information about his daily routines [Data requirement]. Sometimes he also uses the video call [Functional requirement]. He thinks that the size of a smart phone is a problem because it is too small [Interaction requirement]. Anselmo has to monitor his blood pressure everyday [Problem scenario] and he decided to install an application to help him in this task. He is looking for an easy to use application. He is also looking for an application that can give him the latest information about his disease [Functional requirement]. 5. Conclusion The described methodology is being used to develop applications on several projects, including telerehabilitation applications [18], multimodal applications [19] smart phones applications [17] and applications for a Living Home system Center (LHC) [20]. The developed tools proved suitable for their purposes and are expected to be explored in future developments. One of the problems of the development of AAL systems and services is the difficulty of communication between users, care providers and technological professionals. Stakeholders with different backgrounds need a common language in order to make teamwork more efficient and effective. The ICF conceptual framework can contribute to this goal. However, there is still a long way to go so that everybody involved in AAL developments be able to share a common language and a set of common concepts. Acknowledgements This work was supported by COMPETE (Sistema de Incentivos Ã InvestigaÃ§Ã£o e Desenvolvimento TecnolÃ³gico, Projetos de I&DT Empresas em co-promoÃ§Ã£o) and FEDER from the European Union, under QREN Living Usability Lab for Next Generation Networks, QREN AAL4ALL and QREN Smartphones for Seniors. References [1] Rocha, N., Teixeira, A., Pacheco, O., QueirÃ³s, A., Oliveira, C., Pereira, C. and Martins, A., Integrated Development and Evaluation of Innovative AAL Services - A Living Lab Approach. 8Âª ConferÃªncia IbÃ©rica de Sistemas e Tecnologias de InformaÃ§Ã£o, Lisboa, 2013. [2] Teixeira, A. et al., A New Living Lab for Usability Evaluation of ICT and Next Generation Networks for Elderly@Home. AAL 2011, 1st International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Rome, 2011. [3] Moumtzi, V. and Wills, C., Utilizing Living Labs Approach for the Validation of Services for the Assisting Living of Elderly People. 3rd IEEE International Conference on Digital Ecosystems and Technologies, Instanbul, 2009. 418 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 [4] Cook, D. and Das, S. How Smart are our Environments? An Updated Look at the State of the Art. Pervasive and Mobile Computing, 2007, 3(2), p. 53-73. [5] QueirÃ³s, A., Silva, A.G., AlvarelhÃ£o, J., Rocha, N. and Teixeira, A., Usability, Accessibility and Ambient Assisted Living: a Systematic Literature Review. Special Issue of Universal Acess in the Information Society, in press. [6] Teixeira, A. et al., Speech as the Basic Interface for Assistive Technology. DSAI 2009, 2nd International Conference on Software Development for Enhancing Acessibility and Fighting Info-Exclusion, Vila Real, 2009. [7] Newell, A., Gregor, P., Morgan, M., Pullin, G. and Macaulay, C., User-Sensitive Inclusive Design. Universal Access in the Information Society, 2011, 10(3), p. 235-243. [8] Astell, A., Alm, N., Gowans, G., Ellis, M., Dye, R. and Vaughan, P., Involving Older People with Dementia and their Carers in Designing Computer Based Support Systems: some Methodological Considerations. Universal Access in the Information Society, 2009, 8(1), p. 49-58. [9] Zajicek, M., Successful and Available: Interface Design Exemplars for Older Users. Interacting with Computers, 2004, 16, p. 411-430. [10] Goodman-Deane, J., Keith, S. and Whitney, G., HCI and the Older Population. Universal Access in the Information Society, 2009, 8(1), p. 1-3. [11] QueirÃ³s, A., AlvarelhÃ£o, J., Silva, A., Amaro, A., Teixeira, A. and Rocha, N., The International Classification of Functioning, Disability and Health as a Conceptual Framework for the Design, Development and Evaluation of AAL Services for Older Adults. AAL 2011, 1st International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Rome, 2011. [12] Martins, A. et al., The International Classification of Functioning, Disability and Health as a Conceptual Model for the Evaluation of Environmental Factors, Procedia Computer Science, 2012, 14, p. 293-300. [13] The International Classification of Functioning, Disability and Health (ICF).World Health Organization, Geneva, 2001. [14] Study on the Potential of the Living Labs Approach Including its Relation to Experimental Facilities for Future Internet Related Technologies. European Commission, Brussels, 2009. [15] Cooper, A., The Inmates are Running the Asylum: Why High-Tech Products Drive us Crazy and how to Restore the Sanity. Pearson Education, Upper Saddle River, 2004. [16] AlvarelhÃ£o, J., Silva, A., Martins, A., QueirÃ³s, A., Amaro, A., Rocha, N. and LaÃ­ns, J., Comparing the Content of Instruments Assessing Environmental Factors using the International Classification of Functioning, Disability, and Health. Journal of Rehabilitation Medicine, 2012, 44(1), p. 1-6. [17] Teixeira, A., et al., A., Multimodality and Adaptation for an Enhanced Mobile Medication Assistant for the Elderly. 3rd Workshop on Mobile Accessibility in Conference on Human Factors in Computing Systems, Paris, 2013. [18] Teixeira, A. et al., New Telerehabilitation Services for the Elderly. In: I. Miranda, M. Cruz-Cunha, Handbook of Research on ICTs for Healthcare and Social Services: Developments and Applications, IGI Global, 2013, p. 109-132. [19] AAL4ALL - Ambient Assisted Living for All [Online]. Available: [20] Teixeira, V., Pires, C. Pinto, F., Freitas, J. Dias, M. and Rodrigues, E., Towards Elderly Social Integration using a Multimodal Human- Computer Interface. AAL 2012, 2nd International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Algarve, 2012. ility Evaluation of ICT and Next Generation Networks for Elderly@Home. AAL 2011, 1st International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Rome, 2011. [3] Moumtzi, V. and Wills, C., Utilizing Living Labs Approach for the Validation of Services for the Assisting Living of Elderly People. 3rd IEEE International Conference on Digital Ecosystems and Technologies, Instanbul, 2009. 418 Alexandra Queir s et al. / Procedia Computer Science 27 ( 2014 ) 409 418 [4] Cook, D. and Das, S. How Smart are our Environments? An Updated Look at the State of the Art. Pervasive and Mobile Computing, 2007, 3(2), p. 53-73. [5] QueirÃ³s, A., Silva, A.G., AlvarelhÃ£o, J., Rocha, N. and Teixeira, A., Usability, Accessibility and Ambient Assisted Living: a Systematic Literature Review. Special Issue of Universal Acess in the Information Society, in press. [6] Teixeira, A. et al., Speech as the Basic Interface for Assistive Technology. DSAI 2009, 2nd International Conference on Software Development for Enhancing Acessibility and Fighting Info-Exclusion, Vila Real, 2009. [7] Newell, A., Gregor, P., Morgan, M., Pullin, G. and Macaulay, C., User-Sensitive Inclusive Design. Universal Access in the Information Society, 2011, 10(3), p. 235-243. [8] Astell, A., Alm, N., Gowans, G., Ellis, M., Dye, R. and Vaughan, P., Involving Older People with Dementia and their Carers in Designing Computer Based Support Systems: some Methodological Considerations. Universal Access in the Information Society, 2009, 8(1), p. 49-58. [9] Zajicek, M., Successful and Available: Interface Design Exemplars for Older Users. Interacting with Computers, 2004, 16, p. 411-430. [10] Goodman-Deane, J., Keith, S. and Whitney, G., HCI and the Older Population. Universal Access in the Information Society, 2009, 8(1), p. 1-3. [11] QueirÃ³s, A., AlvarelhÃ£o, J., Silva, A., Amaro, A., Teixeira, A. and Rocha, N., The International Classification of Functioning, Disability and Health as a Conceptual Framework for the Design, Development and Evaluation of AAL Services for Older Adults. AAL 2011, 1st International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Rome, 2011. [12] Martins, A. et al., The International Classification of Functioning, Disability and Health as a Conceptual Model for the Evaluation of Environmental Factors, Procedia Computer Science, 2012, 14, p. 293-300. [13] The International Classification of Functioning, Disability and Health (ICF).World Health Organization, Geneva, 2001. [14] Study on the Potential of the Living Labs Approach Including its Relation to Experimental Facilities for Future Internet Related Technologies. European Commission, Brussels, 2009. [15] Cooper, A., The Inmates are Running the Asylum: Why High-Tech Products Drive us Crazy and how to Restore the Sanity. Pearson Education, Upper Saddle River, 2004. [16] AlvarelhÃ£o, J., Silva, A., Martins, A., QueirÃ³s, A., Amaro, A., Rocha, N. and LaÃ­ns, J., Comparing the Content of Instruments Assessing Environmental Factors using the International Classification of Functioning, Disability, and Health. Journal of Rehabilitation Medicine, 2012, 44(1), p. 1-6. [17] Teixeira, A., et al., A., Multimodality and A PROCS 2895 S1877-0509(14)00047-7 10.1016/j.procs.2014.02.045 The Authors ☆ Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). ICF Inspired Personas to Improve Development for Usability and Accessibility in Ambient Assisted Living Alexandra Queirós a ⁎ Margarida Cerqueira a e Ana Isabel Martins b d Anabela G. Silva a Joaquim Alvarelhão a António Teixeira b c Nelson Pacheco Rocha b d a Health Sciences School, University of Aveiro, Campus Universitário, 3810-193 Aveiro, Portugal b Institute of Electronics and Telematics Engineering of Aveiro, Campus Universitário, 3810-193 Aveiro, Portugal c Department of Electronics Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal d Health Sciences Department, University of Aveiro, Campus Universitário, 3810-193 Aveiro, Portugal e Unidade de Investigação e Formação sobre Adultos e Idosos, 4050-313 Porto, Portugal ⁎ Corresponding author. Tel.: +351 234 401 558; fax: +351 234 370 089. Ambient Assisted Living (AAL) is an important research and development area. The acceptance of the AAL paradigm is closely related to the quality of the available systems and services, namely in terms of the user interaction. This means that usability and accessibility are crucial issues. The paper presents how the concepts of the International Classification of Functioning, Disability and Health (ICF) can be used to optimize the role of personas and scenarios in the development and evaluation of AAL systems and services, especially in aspects related with human functioning and health conditions. Keywords Personas Scenarios Usability and accessibility Ambient Assisted Living developments References [1] Rocha, N., Teixeira, A., Pacheco, O., Queirós, A., Oliveira, C., Pereira, C. and Martins, A., Integrated Development and Evaluation of Innovative AAL Services - A Living Lab Approach. 8ª Conferência Ibérica de Sistemas e Tecnologias de Informação, Lisboa, 2013. [2] Teixeira, A. et al., A New Living Lab for Usability Evaluation of ICT and Next Generation Networks for Elderly@Home. AAL 2011, 1st International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Rome, 2011. [3] Moumtzi, V. and Wills, C., Utilizing Living Labs Approach for the Validation of Services for the Assisting Living of Elderly People. 3rd IEEE International Conference on Digital Ecosystems and Technologies, Instanbul, 2009. [4] Cook, D. and Das, S. How Smart are our Environments? An Updated Look at the State of the Art. Pervasive and Mobile Computing, 2007, 3(2), p. 53-73. [5] Queirós, A., Silva, A.G., Alvarelhão, J., Rocha, N. and Teixeira, A., Usability, Accessibility and Ambient Assisted Living: a Systematic Literature Review. Special Issue of Universal Acess in the Information Society, in press. [6] Teixeira, A. et al., Speech as the Basic Interface for Assistive Technology. DSAI 2009, 2nd International Conference on Software Development for Enhancing Acessibility and Fighting Info-Exclusion, Vila Real, 2009. [7] Newell, A., Gregor, P., Morgan, M., Pullin, G. and Macaulay, C., User-Sensitive Inclusive Design. Universal Access in the Information Society, 2011, 10(3), p. 235-243. [8] Astell, A., Alm, N., Gowans, G., Ellis, M., Dye, R. and Vaughan, P., Involving Older People with Dementia and their Carers in Designing Computer Based Support Systems: some Methodological Considerations. Universal Access in the Information Society, 2009, 8(1), p. 49-58. [9] Zajicek, M., Successful and Available: Interface Design Exemplars for Older Users. Interacting with Computers, 2004, 16, p. 411-430. [10] Goodman-Deane, J., Keith, S. and Whitney, G., HCI and the Older Population. Universal Access in the Information Society, 2009, 8(1), p. 1-3. [11] Queirós, A., Alvarelhão, J., Silva, A., Amaro, A., Teixeira, A. and Rocha, N., The International Classification of Functioning, Disability and Health as a Conceptual Framework for the Design, Development and Evaluation of AAL Services for Older Adults. AAL 2011, 1st International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Rome, 2011. [12] Martins, A. et al., The International Classification of Functioning, Disability and Health as a Conceptual Model for the Evaluation of Environmental Factors, Procedia Computer Science, 2012, 14, p. 293-300. [13] The International Classification of Functioning, Disability and Health (ICF).World Health Organization, Geneva, 2001. [14] Study on the Potential of the Living Labs Approach Including its Relation to Experimental Facilities for Future Internet Related Technologies. European Commission, Brussels, 2009. [15] Cooper, A., The Inmates are Running the Asylum: Why High-Tech Products Drive us Crazy and how to Restore the Sanity. Pearson Education, Upper Saddle River, 2004. [16] Alvarelhão, J., Silva, A., Martins, A., Queirós, A., Amaro, A., Rocha, N. and Laíns, J., Comparing the Content of Instruments Assessing Environmental Factors using the International Classification of Functioning, Disability, and Health. Journal of Rehabilitation Medicine, 2012, 44(1), p. 1-6. [17] Teixeira, A., et al., A., Multimodality and Adaptation for an Enhanced Mobile Medication Assistant for the Elderly. 3rd Workshop on Mobile Accessibility in Conference on Human Factors in Computing Systems, Paris, 2013. [18] Teixeira, A. et al., New Telerehabilitation Services for the Elderly. In: I. Miranda, M. Cruz-Cunha, Handbook of Research on ICTs for Healthcare and Social Services: Developments and Applications, IGI Global, 2013, p. 109-132. [19] AAL4ALL - Ambient Assisted Living for All [Online]. Available: [20] Teixeira, V., Pires, C. Pinto, F., Freitas, J. Dias, M. and Rodrigues, E., Towards Elderly Social Integration using a Multimodal Human- Computer Interface. AAL 2012, 2nd International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, Algarve, 2012. "
    },
    {
        "doc_title": "Speech-centric multimodal interaction for easy-to-access online services - A personal life assistant for the elderly",
        "doc_scopus_id": "84897750559",
        "doc_doi": "10.1016/j.procs.2014.02.043",
        "doc_eid": "2-s2.0-84897750559",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic speech recognition",
            "Elderly",
            "Multi-Modal Interactions",
            "Multilingual",
            "Personal assistants",
            "Social interactions",
            "Spoken languages",
            "Synthetic voices"
        ],
        "doc_abstract": "The PaeLife project is a European industry-academia collaboration whose goal is to provide the elderly with easy access to online services that make their life easier and encourage their continued participation in the society. To reach this goal, the project partners are developing a multimodal virtual personal life assistant (PLA) offering a wide range of services from weather information to social networking. This paper presents the multimodal architecture of the PLA, the services provided by the PLA, and the work done in the area of speech input and output modalities, which play a key role in the application. © 2013 The Authors. Published by Elsevier B.V.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2014-02-25 2014-02-25 2014-10-23T08:54:46 S1877-0509(14)00045-3 S1877050914000453 10.1016/j.procs.2014.02.043 S300 S300.2 HEAD-AND-TAIL 2021-10-18T13:04:21.805003Z 0 0 20140101 20141231 2014 2014-02-25T00:00:00Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 false 27 27 C Volume 27 44 389 397 389 397 2014 2014 2014-01-01 2014-12-31 2014 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, DSAI 2013 Dr. Manuel Pérez Cota Dr. João Barroso Dr. Simone Bacellar Leal Ferreira Dr. Benjamim Fonseca Dr. Tassos Mikropoulos Dr. Hugo Paredes article fla Copyright © 2013 The Authors. Published by Elsevier B.V. SPEECHCENTRICMULTIMODALINTERACTIONFOREASYTOACCESSONLINESERVICESAPERSONALLIFEASSISTANTFORELDERLY TEIXEIRA A TEIXEIRAX2014X389 TEIXEIRAX2014X389X397 TEIXEIRAX2014X389XA TEIXEIRAX2014X389X397XA Full 2014-02-25T10:44:25Z OA-Window ElsevierWaived 0 item S1877-0509(14)00045-3 S1877050914000453 10.1016/j.procs.2014.02.043 280203 2014-10-23T04:53:38.247626-04:00 2014-01-01 2014-12-31 true 688643 MAIN 9 47855 849 656 IMAGE-WEB-PDF 1 P r o c e d i a C o m p u t e r S c i e n c e 2 7 ( 2 0 1 4 ) 3 8 9 3 9 7 1877-0509 ' 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). doi: 10.1016/j.procs.2014.02.043 ScienceDirect 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion, DSAI 2013 Speech-Centric Multimodal Interaction for Easy-To-Access Online Services â€“ A Personal Life Assistant for the Elderly AntÃ³nio Teixeira a , Annika HÃ¤mÃ¤lÃ¤inen b,c , Jairo Avelar b,c , Nuno Almeida a , GÃ©za NÃ©meth d , Tibor FegyÃ³ d , Csaba ZainkÃ³ d , TamÃ¡s CsapÃ³ d , BÃ¡lint TÃ³th d , AndrÃ© Oliveira a , Miguel Sales Dias b,c a Department of Electronics Telecom. & Informatics/IEETA, University of Aveiro, Aveiro, Portugal b Microsoft Language Development Center, Lisbon, Portugal c ISCTE - University Institute of Lisbon/ADETTI-IUL, Portugall d Department of Telecommunications & Media Informatics, Budapest University of Technology & Economics, Budapest, Hungary Abstract The PaeLife project is a European industry-academia collaboration whose goal is to provide the elderly with easy access to online services that make their life easier and encourage their continued participation in the society. To reach this goal, the project partners are developing a multimodal virtual personal life assistant (PLA) offering a wide range of services from weather information to social networking. This paper presents the multimodal architecture of the PLA, the services provided by the PLA, and the work done in the area of speech input and output modalities, which play a key role in the application. Â© 2013 The Authors. Published by Elsevier B.V. Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). Keywords: active aging, automatic speech recognition, elderly, multilingual, multimodal interaction, personalized synthetic voices, personal assistant, social interaction, spoken language modalities. Available online at www.sciencedirect.com ' 2013 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. election and peer-review under responsibility of the Scientific Progra me Co mittee of the 5th International Conference on Software Development and Technologies for E han ing Accessibility and Fighting Info-exclusion (DSAI 2013). 390 Ant nio Teixeira et al. / Procedia Computer Science 27 ( 2014 ) 389 397 1. Introduction The population aged over 60 is rapidly growing worldwide [1]. This social change is so dramatic that the World Health Organization is actively promoting policies and programs to keep the older generations active, productive and independent as long as possible. From the point of view of information and communication technology (ICT), ensuring active ageing translates into developing applications that enhance the health, the social participation and the security of the elderly. Previous research suggests that the elderly have more difficulties using ICT than younger adults do [2], [3]. The main reasons for this are the complexity of the existing user interfaces and the limited set of available interaction modalities caused by the technology mainly being designed with younger users in mind. One of the most promising ways of adapting the technology to better suit the needs of the elderly is to increase the choice of available interaction modalities. Adding speech to the available modalities is a particularly interesting alternative. The advantages of speech include it offering a natural and fast (about 150-250 words/minute) form of communication, and it requiring neither visual attention nor the use of hands [4]. In fact, speech is part of the three most popular input modality combinations mentioned in [5]: 1) speech and lip movements, 2) speech and gestures, and 3) speech, gestures and facial expressions. Several popular output modality combinations also include speech: 1) speech and graphics, 2) speech plus avatar, and 3) speech, text and graphics [5]. Usability evaluation studies (e.g. [6]) also suggest that speech is the easiest and most natural modality of human-computer interaction (HCI). There is growing international interest, both in academia and in industry, in developing speech-driven applications aimed at improving the quality of life of the elderly. Past R&D projects in the area include, for instance, the Living Usability Lab (LUL) project [7], which developed a telerehabilitation service for the elderly [8], with a toolkit for multimodal interaction (including speech), and the possibility to adapt speech and graphical output to the context and to the user [9]. Speech input and output have recently also been used, for example, in a medication assistant for smartphones [10]. The PaeLife project, an ongoing industry-academia collaboration that is part of the Ambient Assisted Living (AAL) Joint Programme [11], is aimed at keeping the European elderly active and socially integrated. To this end, the project is developing a multimodal personal life assistant (PLA) offering the elderly a wide set of services from unified messaging (e.g. email, twitter, videoconferencing) through to relevant feeds (e.g. the latest news, weather information). The platform of the PLA comprises a personal computer connected to a TV-like big screen, as well as a portable device (a tablet) for mobility. One of the key modalities of the PLA is speech; speech input and output will be available in four European languages: French, Hungarian, Polish, and Portuguese. Apart from the challenges of developing speech technology for use in multimodal interaction in several languages, the project partners are faced with the challenges of customizing it for the elderly. It is well known that current speech recognizers do not work well with elderly speech. This is (in part) because many parameters of the speech signal (such as the fundamental frequency) change with age [12], and because most current speech recognizers have been optimized to recognize younger adult speech. To address this issue, we are collecting large databases of elderly speech for the four target languages, and training elderly-specific speech recognizers using those data. A successful speech interface should also be able to take into account the usersâ€™ preferences. This is particularly important in the case of the elderly who might not be very familiar with technology. For increased user acceptance of speech output (speech synthesis), we are providing the elderly users with several synthesized voices to choose from based on their personal preferences [13]. In this paper, we present the multimodal architecture adopted for the PLA, the services planned and already available in the PLA, as well as the work aimed at tailoring automatic speech recognition and speech synthesis to the elderly and at making these technologies available to the four languages targeted by the project. 391 Ant nio Teixeira et al. / Procedia Computer Science 27 ( 2014 ) 389 397 2. The PaeLife Personal Life Assistant 2.1. User-Centric Design The PaeLife project adopted a user-centric approach for designing the PLA. We carried out extensive user studies amongst the elderly in France, Hungary and Poland to try to identify what kinds of online services the elderly are interested in, and what kinds of limitations and preferences they might have when it comes to HCI [14]. Based on the user studies, we made up personas (archetypal users of the application) and use scenarios to explore the set of tasks and interactions required for the application, and to help us evaluate the application in the future. The PaeLife personas are aged 60 or over and have some experience in using computers â€“ although most of them are not expert users. While they do not have major health issues, they might suffer from typical age-related ailments (reduced dexterity, some degree of visual impairment etc.). In other words, they do not have any serious health-related conditions that would require physiological interfaces (e.g. electromyography), which are cumbersome, intrusive and difficult to use. However, as they might not be very proficient in using traditional user interfaces, we considered it very important to provide them with easy-to-use, natural interaction modalities â€“ such as speech, touch and gestures. Table 1 presents part of an example use scenario featuring MÃ¡ria KovÃ¡cs, a relatively healthy 68-year-old Polish woman who still works part-time and, apart from her proofreading work, mainly uses the computer for social networking and for looking for new recipes and information about local cultural events. Table 1: Example use scenario with MÃ¡ria KovÃ¡cs, a 68-year-old Polish woman with basic computer skills MÃ¡ria is watching TV in the living-room. She decides to use the tablet to check the latest posts of her Facebook friends. As one of the posts contains a video, MÃ¡ria says, â€œWatch videoâ€�. The video starts playing on the TV screen, and the TV channel that she was watching now appears as a miniature window in the corner of the TV screen. The tablet continues displaying the latest posts but the controls for the video are shown at the bottom of the screen, allowing the user to pause, stop, fast-forward and rewind the video. At the end of the video, three buttons appear on the TV: â€œLikeâ€�, â€œDonâ€™t Likeâ€� and â€œLeaveâ€�. MÃ¡ria says â€œLikeâ€�, and the video is tagged with her like. MÃ¡ria then decides to use the tablet to search for a friend on Facebook by her name. She chooses the option â€œSearchâ€� and uses the tablet as a virtual keyboard; the results of her search are shown on the TV screen. MÃ¡ria uses a swiping hand gesture to see the full list of results. When the photo of her friend shows up on the TV screen, she asks for more information by saying, â€œSee the profile of the second photoâ€�. The profile of the friend now gets displayed on the TV screen. MÃ¡ria says, â€œSend friend requestâ€�. Figure 1: The W3C-recommended architecture adopted for supporting multimodal interaction in the personal life assistant (PLA). 392 Ant nio Teixeira et al. / Procedia Computer Science 27 ( 2014 ) 389 397 2.2. Multimodal Interaction Based on the user studies, we identified the following key requirements for HCI in the PLA: 1) support for several input and output modalities, 2) support for the distribution of modalities across different devices (PCs, tablets etc.), 3) adhering to international standards and avoiding closed solutions, and 4) possibility to change or add modules (supporting services or interaction) without the rest of the system being affected. These requirements were met by developing an integrated framework that supports multimodal interaction specifically tailored to the elderly, with the available interaction modalities including speech, touch, gestures, keyboard and mouse. The framework is based on the recent recommendations of the World Wide Web Consortium (W3C) regarding multimodal interaction [15] â€“ a choice motivated by the open-standard nature of the recommended architecture and the ease of integrating new modules and already existing tools into the system. The architecture (see Figure 1) has three major components: Î¾ The Interaction Manager (IM), which manages the different interaction modalities Î¾ The Modality Components, which represent the input and output modalities Î¾ The Runtime Framework, which acts as a container for all other components and provides communication between the different modalities and the IM From the point of view of the PLA, the Modality Components are the most important components of the architecture because they provide a simple way to integrate the chosen input and output technologies â€“ including speech input and output, which are discussed in more detail in Section 3 â€“ into the system. 2.3. The PLA and the Available Online Services The PLA itself comprises a stationary main unit that runs on a desktop computer, as well as a portable unit that runs on a tablet (see Figure 2). In the main unit, a big screen (e.g. an LCD TV) supports graphical output, the internal microphone and speakers support speech input and output, and a Kinect sensor supports gesture input. In the portable unit, on the other hand, the display supports graphical output, the internal microphone and speakers enable speech input and output, and the multi-touch support of the operating system makes touch input possible. The main unit and the portable units work together and can be connected to the internet and to the cloud for providing the user with online services. The two units can also work as stand-alone devices. As illustrated in Figure 2, two different types of tablets can be used as the portable unit: premium tablets, with all the services available for the PLA, and low-cost tablets, with a subset of those services. Apart from a more limited set of services, the low- cost tablets might have to use remote or cloud-based services, for example, for handling speech input and output. 393 Ant nio Teixeira et al. / Procedia Computer Science 27 ( 2014 ) 389 397 Main Unit Home computer Screen PLA Database Portable Unit Portable Unit (Premium Tablet) l i i l Portable Unit (Low Cost Tablet) l i l Home Office 365 Voice SearchAutomatic Speech Recognition Internet authentification services Text to Speech kinect Figure 2: The architecture of the PLA. In practice, the PLA functions as the multimodal assistant for the elderly and supports a wide range of services, for instance, in the areas of social interaction and entertainment.The PLA is divided into modules that are responsible for delivering different types of services. The modules and the corresponding services are presented in Table 2. Table 2: The modules and services of the PLA. Module Services Unified messaging 1. Quasi-instant messaging 2. Voice call 3. Videoconferencing on Skype 4. Messaging using email, Facebook and/or Twitter 5. Voice mail Calendar 1. To-do list 2. Birthday reminders 3. Name day reminders 4. Cultural events Social networking 1. Geographical location of contacts 2. Graphical representation of contactsâ€™ availability 3. Email, Facebook and/or Twitter activity with contacts 4. Visualisation of the time that has passed from previous contact (e.g. photos of contacts getting larger), aimed at encouraging regular communication 5. Information about contacts (e.g. status) TV schedule Weather information 394 Ant nio Teixeira et al. / Procedia Computer Science 27 ( 2014 ) 389 397 Media manager 1. Photos 2. Videos 3. Music 4. Text documents etc. Relevant feeds Examples: latest news, accessibility of places for mobility-impaired users, local health care services and pharmacies Facebook groups of interest Examples: recipes, organised travel, and clubs 3. Towards Better Speech Recognition and Synthesis for the Elderly In the PLA, speech input (automatic speech recognition; ASR) and output (speech synthesis; TTS) are handled using two different speech platforms: one provided by Microsoft [16] and already supporting French, Polish and Portuguese, and another provided by the Budapest University of Technology & Economics (BME) and already supporting Hungarian. In this section, we describe how these speech technologies are optimized for the elderly users in the PaeLife project. ASR is technology that translates acoustic speech signal into a sequence of words. To be able to do that, automatic speech recognisers typically use three knowledge bases: 1) a language-specific language model (or grammar), which contains information on the possible sequences of words and their probability in the language in question (e.g. â€˜I use Bingâ€™ is a probable sequence of words, while â€˜I chair Bingâ€™ is not), 2) a language-specific pronunciation lexicon, which represents words in terms of individual speech sounds, or phonemes (e.g. Bing contains three phonemes: /b Éª Å‹/), and 3) language-specific acoustic models, which model the acoustic properties of the phonemes used in the pronunciation lexicon. Before a recognition task can be performed, the language model and the acoustic models must be trained using large corpora of language-specific texts and speech, respectively. Together, the language model, the lexicon and the acoustic models then â€˜modelâ€™ the acoustic realisations of all possible sentences in the language in question, and are used to find the most probable sequence of words to represent the incoming acoustic speech signal. To be able to train acoustic models that model the acoustic properties of phonemes as accurately as possible, the speech corpus used for the training must contain orthographic (word-level) transcriptions of the spoken material. The lexicon will then be used to identify the underlying phonemes, whose acoustic properties in the speech signal will be used to train the acoustic models. Standard speech recognisers are usually trained using speech corpora collected from younger adult speakers. Because the acoustic properties of speech produced by elderly speakers differ from those produced by younger adult speakers [12], they are not able to recognise elderly speech as well as they are able to recognise younger adult speech. To successfully recognise elderly speech, it is important to collect a sufficient amount of elderly speech for training elderly-specific acoustic models [17â€“19]. In the PaeLife project, we have already finished collecting large corpora of domain-specific speech from subjects aged 60 or over for two of the target languages: Portuguese (180 hours of read speech) [20], [21], and Polish (170 hours of read speech). At the time of writing this paper, we have also already collected about 46 and 35 hours of read speech for French and Hungarian, respectively. The goal is to collect 200 hours of read speech for both of those languages, as well as an additional 60 hours of spontaneous speech for Hungarian. One of the benefits of collecting read â€“ rather than spontaneous â€“ speech is that, apart from some small corrections, the sentences presented to the speakers can also be used as the orthographic transcriptions of the spoken material. On the other hand, spontaneous speech must be transcribed from scratch. We have already trained elderly-specific acoustic models for Portuguese, and integrated them into the ASR- based services that are already available in the PLA. We will do the same for the remaining three languages as soon as the data collection and/or the transcription work has ended; for now, the other languages are using standard acoustic models that have been trained using younger adult speech. TTS is technology that converts text into artificially produced speech. Users are more likely to identify with and accept synthesized voices that match their preferences and/or their own age group, gender etc. [13]. In terms of TTS, the main goal of the PaeLife project is to increase the user acceptance of synthesized voices by offering the elderly users of the PLA a wide variety of voices to choose from. In practice, they are currently provided with the 395 Ant nio Teixeira et al. / Procedia Computer Science 27 ( 2014 ) 389 397 younger adult voices (female and male) that the two project partners developing speech technologies for the PLA, Microsoft and BME, already have available in their speech platforms. In addition, one elderly voice will be developed for each target language, and the users of the PLA will also be offered the possibility to create personalised voices. To generate new synthesised voices, speech is usually recorded from people with the desired kind of voice, and the recorded speech is then manipulated to form new spoken words and sentences. As few as 200 sentences are enough to generate intelligible voices using the Microsoft methodology [22]. Therefore, personalised voices can even be the voices of the relatives of the users of the PLA â€“ an alternative that might be particularly attractive to the elderly. 4. Currently Available Speech-Driven Services in the PLA The speech-driven services that are already available and in integration in the PLA system include the following: Î¾ Weather information service. This service uses Hungarian TTS voices to read out weather information from a Hungarian weather information website. Î¾ Unified messaging. This service employs speech input and output for using email, Twitter, YouTube, Skype and Facebook via a single, simplified, easy-to-use interface. The service is currently available in French and Portuguese, with the Portuguese ASR already optimized for elderly speech. Î¾ News feeds. This service offers ASR- and gesture-driven interaction with news feeds. It is already possible to navigate news items in French, Polish, English and Portuguese using simple voice commands (e.g. â€œto the rightâ€�) and by starting to read the contents (e.g. the first 3-4 words) of a news item select that specific item. Hungarian news feeds are also already available in the system but can only be accessed using ASR when acoustic models have been trained for Hungarian. Figure 3 illustrates the easy-to-use multimodal interaction with news feeds. 396 Ant nio Teixeira et al. / Procedia Computer Science 27 ( 2014 ) 389 397 Figure 3: Interaction with the news module of the PLA. 5. Conclusions Due to the rapid, worldwide growth of the elderly population, it is of paramount importance to devise applications for enhancing the health, the social participation and the security of the elderly. Because of the special needs and limitations of the target population, such applications need to pay special attention to easy and natural human-computer interaction. In this paper, we presented a multimodal personal life assistant aimed at providing the elderly with easy access to a wide range of online services â€“ in particular, information services and services related to social interaction â€“ and, thus, making their lives easier and encouraging their continued participation in the society. As speech is one of the easiest and most natural modalities of human-computer interaction, speech input (automatic speech recognition) and output (speech synthesis) pay a key role in the application, and will be available 397 Ant nio Teixeira et al. / Procedia Computer Science 27 ( 2014 ) 389 397 in four European languages (besides English): French, Hungarian, Polish and Portuguese. In this paper, we discussed the framework that we have adopted for handling the interaction modalities available in the application (speech, touch, gestures, keyboard and mouse), the services planned and already available in the personal life assistant, as well as our approach to adapting automatic speech recognition and speech synthesis to the elderly and making these technologies available in the four target languages. Acknowledgments Authors acknowledge the funding from AAL JP and national agencies: MLDC was funded by Portuguese Government through the Ministry of Science, Technology and Higher Education (MCES); University of Aveiro was funded by FEDER, COMPETE and FCT in the context of AAL/0015/2009 and IEETA Research Unit funding FCOMP-01-0124-FEDER-022682 (FCT-PEstC/EEI/UI0127/2011). BME acknowledge the support of the FuturICT project (TÃ�MOP-4.2.2.C-11/1/KONV-2012- 0013) and the PAELIFE project (AAL-08-1-2011-0001). References [1] W. H. Organization, â€œActive aging: A policy framework,â€� in Second United Nations World Assembly on Ageing, 2002. [2] D. A. C. Stephanidis, â€œUniversal accessibility in HCI: Process-oriented design guidelines and tool requirements,â€� in ERCIM Workshop on User Interfaces for All, 1998. [3] V. Teixeira, C. Pires, F. Pinto, J. Freitas, M. S. Dias, and E. M. Rodrigues, â€œTowards elderly social integration using a multimodal human-computer interface,â€� in Proc. International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, AAL, 2012. [4] N. O. Bernsen, â€œTowards a tool for predicting speech functionality,â€� Speech Communication, vol. 23, no. 3, pp. 181â€“210, Nov. 1997. [5] T. H. Bui, â€œMultimodal Dialogue Management - State of the art,â€� 2006, no. TR-CTIT-06â€“01. [6] A. Teixeira, D. Braga, L. Coelho, A. Fonseca, J. AlvarelhÃ£o, I. MartÃ­n, A. QueirÃ³s, N. Rocha, A. Calado, and M. Dias, â€œSpeech as the Basic Interface for Assistive Technology,â€� in DSAI, 2009. [7] â€œLiving Usability Lab.â€� [Online]. Available: [Accessed: 18-Mar-2013]. [8] A. J. S. Teixeira, C. Pereira, M. Oliveira e Silva, J. AlvarelhÃ£o, A. Silva, M. Cerqueira, A. I. Martins, O. Pacheco, N. Almeida, C. Oliveira, R. Costa, and A. J. . Neves, â€œNew Telerehabilitation Services for the Elderly,â€� in I.M. Miranda and M.M. Cruz-Cunha [Eds], Handbook of research on ICTs for healthcare and social services: Developments and applications, IGI Global, 2013. [9] A. Teixeira, C. Pereira, M. Silva, O. Pacheco, A. Neves, and J. Casimiro, â€œAdaptO - Adaptive Multimodal Output,â€� in Proc. PECCS, 2011. [10] A. Teixeira, F. Ferreira, N. Almeida, A. Rosa, J. Casimiro, S. Silva, A. QueirÃ³s, and A. Oliveira, â€œMultimodality and Adaptation for an Enhanced Mobile Medication Assistant for the Elderly,â€� in Proc. Third Mobile Accessibility Workshop (MOBACC), CHI 2013, 2013. [11] â€œAmbient Assisted Living Joint Programme.â€� [Online]. Available: [Accessed: 11-Jul-2013]. [12] S. A. Xue and G. J. Hao, â€œChanges in the human vocal tract due to aging and the acoustic correlates of speech production: A pilot study.,â€� Journal of Speech, Language and Hearing Research, vol. 46, no. 3, pp. 689â€“701, 2003. [13] C. Nass and S. Brave, â€œWired for speech: How voice activates and advances the human-computer relationship,â€� in MIT Press, 2007. [14] N. Saldanha, J. Avelar, M. Dias, A. Teixeira, D. GonÃ§alves, E. Bonnet, K. Lan, N. GÃ©za, P. Csobanka, and A. Kolesinski, â€œA Personal Life Assistant for â€˜naturalâ€™ interaction: the PaeLife project,â€� in AAL Forum 2013 Forum, 2013. [15] M. Bodell, D. Dahl, I. Kliche, J. Larson, B. Porter, D. Raggett, T. Raman, B. H. Rodriguez, M. Selvaraj, R. Tumuluri, A. Wahbe, P. Wiechno, and M. Yudkowsky, â€œMultimodal architecture and interfaces: W3C Recommendation,â€� 2012. [Online]. Available: [Accessed: 18-Mar-2013]. [16] â€œMicrosoft Speech Platform 11.0.â€� [Online]. Available: [Accessed: 18-Mar-2013]. [17] R. Vipperla, S. Renals, and J. Frankel, â€œLongitudinal study of ASR performance on ageing voices,â€� in Proc. Interspeech, 2008. [18] A. Baba, S. Yoshizawa, M. Yamada, A. Lee, and K. Shikano, â€œAcoustic models of the elderly for large-vocabulary continuous speech recognition,â€� Electronics and Communications in Japan, vol. 87, no. 7, pp. 49â€“57, 2004. [19] T. Pellegrini, I. Trancoso, A. HÃ¤mÃ¤lÃ¤inen, A. Calado, M. Dias, and D. Braga, â€œImpact of age in ASR for the elderly: Preliminary experiments in European Portuguese,â€� in Proc. IberSPEECH, 2012. [20] A. HÃ¤mÃ¤lÃ¤inen, F. Pinto, M. Dias, A. JÃºdice, J. Freitas, C. Pires, V. Teixeira, A. Calado, and D. Braga, â€œThe first European Portuguese elderly speech corpus,â€� in Proc. IberSPEECH, 2012. [21] A. JÃºdice, J. Freitas, D. Braga, A. Calado, M. Sales Dias, A. J. S. Teixeira, and C. Oliveira, â€œElderly speech collection for speech recognition based on crowd sourcing.,â€� in Proc. DSAI, 2010. [22] D. Braga, P. Silva, M. Ribeiro, M. Henriques, and M. Dias, â€œHMM-based Brazilian Portuguese TTS,â€� in Propor 2008 Special Session: Applications of Portuguese Speech and Language Technologies, 2008. Workshop on User Interfaces for All, 1998. [3] V. Teixeira, C. Pires, F. Pinto, J. Freitas, M. S. Dias, and E. M. Rodrigues, â€œTowards elderly social integration using a multimodal human-computer interface,â€� in Proc. PROCS 2893 S1877-0509(14)00045-3 10.1016/j.procs.2014.02.043 The Authors ☆ Selection and peer-review under responsibility of the Scientific Programme Committee of the 5th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2013). Speech-centric Multimodal Interaction for Easy-to-access Online Services – A Personal Life Assistant for the Elderly António Teixeira a Annika Hämäläinen b c Jairo Avelar b c Nuno Almeida a Géza Németh d Tibor Fegyó d Csaba Zainkó d Tamás Csapó d Bálint Tóth d André Oliveira a Miguel Sales Dias b c a Department of Electronics Telecom. & Informatics/IEETA, University of Aveiro, Aveiro, Portugal b Microsoft Language Development Center, Lisbon, Portugal c ISCTE - University Institute of Lisbon/ADETTI-IUL, Portugall d Department of Telecommunications & Media Informatics, Budapest University of Technology & Economics, Budapest, Hungary The PaeLife project is a European industry-academia collaboration whose goal is to provide the elderly with easy access to online services that make their life easier and encourage their continued participation in the society. To reach this goal, the project partners are developing a multimodal virtual personal life assistant (PLA) offering a wide range of services from weather information to social networking. This paper presents the multimodal architecture of the PLA, the services provided by the PLA, and the work done in the area of speech input and output modalities, which play a key role in the application. Keywords active aging automatic speech recognition elderly multilingual multimodal interaction personalized synthetic voices personal assistant social interaction spoken language modalities References [1] W. H. Organization, “Active aging: A policy framework,” in Second United Nations World Assembly on Ageing, 2002. [2] D. A. C. Stephanidis, “Universal accessibility in HCI: Process-oriented design guidelines and tool requirements,” in ERCIM Workshop on User Interfaces for All, 1998. [3] V. Teixeira, C. Pires, F. Pinto, J. Freitas, M.S. Dias, and E. M. Rodrigues, “Towards elderly social integration using a multimodal human-computer interface,” in Proc. International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications, AAL, 2012. [4] N. O. Bernsen, “Towards a tool for predicting speech functionality,” Speech Communication, vol. 23, no. 3, pp. 181-210, Nov. 1997. [5] T. H. Bui, “Multimodal Dialogue Management - State of the art,” 2006, no. TR-CTIT-06-01. [6] A. Teixeira, D. Braga, L. Coelho, A. Fonseca, J. Alvarelhão, I. Martín, A. Queirós, N. Rocha, A. Calado, and M. Dias, “Speech as the Basic Interface for Assistive Technology,” in DSAI,;1; 2009. [7] “Living Usability Lab.” [Online]. Available: [Accessed: 18-Mar-2013]. [8] A. J. S. Teixeira, C. Pereira, M. Oliveira e Silva, J. Alvarelhão, A. Silva, M. Cerqueira, A.I. Martins, O. Pacheco, N. Almeida, C. Oliveira, R. Costa, and A. J. Neves, “New Telerehabilitation Services for the Elderly,” in I.M. Miranda and M.M. Cruz-Cunha [Eds], Handbook of research on ICTs for healthcare and social services: Developments and applications, IGI Global, 2013. [9] A. Teixeira, C. Pereira, M. Silva, O. Pacheco, A. Neves, and J. Casimiro, “AdaptO - Adaptive Multimodal Output,” in Proc. PECCS,;1; 2011. [10] A. Teixeira, F. Ferreira, N. Almeida, A. Rosa, J. Casimiro, S. Silva, A. Queirós, and A. Oliveira, “Multimodality and Adaptation for an Enhanced Mobile Medication Assistant for the Elderly,” in Proc. Third Mobile Accessibility Workshop (MOBACC), CHI 2013, 2013. [11] “Ambient Assisted Living Joint Programme.” [Online]. Available: [Accessed: 11-Jul-2013]. [12] S. A. Xue and G. J. Hao, “Changes in the human vocal tract due to aging and the acoustic correlates of speech production: A pilot study.,” Journal of Speech, Language and Hearing Research, vol. 46, no. 3, pp. 689-701, 2003. [13] C. Nass and S. Brave, “Wired for speech: How voice activates and advances the human-computer relationship,” in MIT Press, 2007. [14] N. Saldanha, J. Avelar, M. Dias, A. Teixeira, D. Gonçalves, E. Bonnet, K. Lan, N. Géza, P. Csobanka, and A. Kolesinski, “A. Personal;1; Life Assistant for ‘natural’ interaction: the PaeLife project,” in AAL Forum 2013 Forum, 2013. [15] M. Bodell, D. Dahl, I. Kliche, J. Larson, B. Porter, D. Raggett, T. Raman, B.H. Rodriguez, M. Selvaraj, R. Tumuluri, A. Wahbe, P. Wiechno, and M. Yudkowsky, “Multimodal architecture and interfaces: W3C Recommendation,” 2012. [Online]. Available: [Accessed: 18-Mar-2013]. [16] “Microsoft Speech Platform 11.0.” [Online]. Available: [Accessed: 18-Mar-2013]. [17] R. Vipperla, S. Renals, and J. Frankel, “Longitudinal study of ASR performance on ageing voices,” in Proc. Interspeech, 2008. [18] A. Baba, S. Yoshizawa, M. Yamada, A. Lee, and K. Shikano, “Acoustic models of the elderly for large-vocabulary continuous speech recognition,” Electronics and Communications in Japan, vol. 87, no. 7, pp. 49-57, 2004. [19] T. Pellegrini, I. Trancoso, A. Hämäläinen, A. Calado, M. Dias, and D. Braga, “Impact of age in ASR for the elderly: Preliminary experiments in European Portuguese,” in Proc. IberSPEECH, 2012. [20] A. Hämäläinen, F. Pinto, M. Dias, A. Júdice, J. Freitas, C. Pires, V. Teixeira, A. Calado, and D. Braga, “The first European Portuguese elderly speech corpus,” in Proc. IberSPEECH, 2012. [21] A. Júdice, J. Freitas, D. Braga, A. Calado, M. Sales Dias, A.J. S. Teixeira, and C. Oliveira, “Elderly speech collection for speech recognition based on crowd sourcing.,” in Proc. DSAI, 2010. [22] D. Braga, P. Silva, M. Ribeiro, M. Henriques, and M. Dias, “HMM-based Brazilian Portuguese TTS,” in Propor 2008 Special Session: Applications of Portuguese Speech and Language Technologies, 2008. "
    },
    {
        "doc_title": "Evaluation of a dialogue manager for a mobile robot",
        "doc_scopus_id": "84889583545",
        "doc_doi": "10.1109/ROMAN.2013.6628466",
        "doc_eid": "2-s2.0-84889583545",
        "doc_date": "2013-12-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Dialogue manager",
            "Experimental evaluation",
            "Information state",
            "Intelligent Service robots",
            "Usability evaluation"
        ],
        "doc_abstract": "This paper presents an evaluation of the dialogue manager (DM) used on Carl, a prototype of an intelligent service robot, designed and developed having in mind hosting tasks in a building or event. The developed DM, based on the 'Information State' approach, is described. In an experimental evaluation, in which 10 participants attempted to complete several interaction tasks with the robot, 81% of tasks were performed successfully. The results of an usability evaluation are also presented and discussed. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A conceptual framework for the design and development of AAL services",
        "doc_scopus_id": "84898322445",
        "doc_doi": "10.4018/978-1-4666-3986-7.ch030",
        "doc_eid": "2-s2.0-84898322445",
        "doc_date": "2013-12-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            },
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "A digital environment with a pervasive and unobtrusive intelligence able to proactively support elderly people in their daily lives, enabling them to live independently for longer, and reducing the need for long term care is the fundamental idea of the Ambient Assisted Living (AAL). After considerable research investment, there is a good understanding of the domain problem. However, the need to broaden the scope of problems being addressed is undeniable. Ecological approaches for design and development of AAL services are required in order to reinforce a strong focus on people. The chapter presents a comprehensive model based on the International Classification of Functioning Disability and Health (ICF) to characterize users, theirs contexts, activities, and participation, and to structure a semantic framework for AAL services. © 2013, IGI Global.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Medical information extraction in European Portuguese",
        "doc_scopus_id": "84898263082",
        "doc_doi": "10.4018/978-1-4666-3986-7.ch032",
        "doc_eid": "2-s2.0-84898263082",
        "doc_date": "2013-12-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            },
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The electronic storage of medical patient data is becoming a daily experience in most of the practices and hospitals worldwide. However, much of the available data is in free text form, a convenient way of expressing concepts and events but especially challenging if one wants to perform automatic searches, summarization, or statistical analyses. Information Extraction can relieve some of these problems by offering a semantically informed interpretation and abstraction of the texts. MedInX, the Medical Information eXtraction system presented in this chapter is designed to process textual clinical discharge records in order to perform automatic and accurate mapping of free text reports onto a structured representation. MedInX components are based on Natural Language Processing principles and provide several mechanisms to read, process, and utilize external resources, such as terminologies and ontologies. MedInX current practical applications include automatic code assignment and an audit system capable of systematically analyze the content and completeness of the clinical reports. Recent evaluation efforts on a set of authentic patient discharge letters indicate that the system performs with 95% precision and recall. © 2013, IGI Global.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ontology-based health information search: Application to the neurological disease domain",
        "doc_scopus_id": "84887971150",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887971150",
        "doc_date": "2013-11-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Health informations",
            "Named-entity recognition",
            "Neurological disease",
            "OWL",
            "RDF",
            "Semantic annotations"
        ],
        "doc_abstract": "The amount of information on health subjects on the Web and users' interest in having efficient access to it continues to increase. This information can also be very useful for professionals, researchers and students in this field. But the usual word-based search does not allow full exploration of the information, pointing to the need for more semantic approaches. Aiming to contribute to providing a semantic search for information on Health, available using written Portuguese, the article presents enhanced search platform architecture based on the enterprise search platform (FAST), providing a processing pipeline capable of exploring annotation of entities and relations defined in a domain. A modular architecture was defined in which an ontology-based knowledge base has an essential role. As it is essential to make the user task as simple as possible, the platform also includes ontology navigation in the classes and relations of the entity to guide queries; use of the most frequent domain classes in the documents relating to a search to provide additional ways of navigation; and inclusion in the score of the ontology-guided annotations. As a first proof of concept, the architecture was instantiated in a specific domain, neurological diseases, for which an ontology was created. © 2013 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrated development and evaluation of innovative AAL services - A Living Lab approach",
        "doc_scopus_id": "84887956835",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887956835",
        "doc_date": "2013-11-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Ambient assisted living (AAL)",
            "General architectures",
            "Integrated development",
            "Living lab",
            "Methodological approach",
            "services evaluation",
            "usability"
        ],
        "doc_abstract": "The paper aims to systematize the work associated with the consolidation of the geographically distributed Living Lab for the development of Ambient Assisted Living (AAL) systems and services. The paper presents the developed methodological approach, the general architecture that supports the development of AAL applications, the implemented physical infrastructure and also an instantiation of the existing infrastructure for the implementation of a telerehabilitation service. © 2013 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Multimodal and adaptable medication assistant for the elderly: A prototype for interaction and usability in smartphones",
        "doc_scopus_id": "84887888539",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887888539",
        "doc_date": "2013-11-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "adaptation",
            "mobile",
            "Multi-modality",
            "Natural language generation",
            "Windows phones"
        ],
        "doc_abstract": "In order to address the elderly population higher levels of non-adherence to medication we present a mobile application to help them with medication management, providing multimodal interaction and context awareness. The application was developed following an iterative method. In each phase we identify some requirements, develop a prototype and evaluate it. The first prototype uses speech and touch as input and speech and graphical modalities as output. In addition to the traditional medication alerts, the application provides medication advices, and some provision for handling situations when the elder forgets to take the medication, in complement with classical functionalities on medication alerts to the user. Furthermore, it includes some adaptation mechanisms to the user and context of use. The results of the application tests and end-user evaluation have shown that the adaptation, the interaction based on spoken language and the recommendations had a positive impact in the users. © 2013 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Segmentation and analysis of vocal tract from midsagittal real-time MRI",
        "doc_scopus_id": "84884492397",
        "doc_doi": "10.1007/978-3-642-39094-4_52",
        "doc_eid": "2-s2.0-84884492397",
        "doc_date": "2013-09-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Anatomical structures",
            "Data processing and analysis",
            "Dynamic aspects",
            "Large amounts",
            "Nasal vowels",
            "Real-Time MRI",
            "Speech production",
            "Vocal-tracts"
        ],
        "doc_abstract": "The articulatory description of European Portuguese (EP) requires the analysis of different anatomical structures (e.g. tongue dorsum and velum), and the study of dynamic aspects of speech production. The use of real-time magnetic resonance imaging (RT-MRI), with frame rates above 10 frames/s, provides adequate support for these studies and results in a large amount of images that need to be processed to extract relevant data to be analysed by linguists. To tackle the required data processing and analysis this article presents methods to perform segmentation of the vocal tract from midsagittal real-time MR image sequences and provide researchers with visualizations of the relevant extracted data. Examples are provided illustrating the analysis of dynamic aspects of EP nasal vowels. © 2013 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards e-government information platforms for enterprise 2.0",
        "doc_scopus_id": "84978035605",
        "doc_doi": "10.4018/978-1-4666-4373-4.ch033",
        "doc_eid": "2-s2.0-84978035605",
        "doc_date": "2013-07-31",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (miscellaneous)",
                "area_abbreviation": "MEDI",
                "area_code": "2701"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2014, IGI Global.Enterprise 2.0 aims to help employees, customers, and suppliers collaborate, share, and organize information. As governments are relevant partners for enterprises (legislation, contracts, etc.) e-government platforms need to be ready for Enterprise 2.0 to what concerns e-government interactions. The public sector holds huge quantities of information and just a small proportion is relevant to each enterprise. Enterprises should only be confronted with relevant information and not flooded with lots of data. This implies data organization with semantic description and services using open standards. The goal is to build a durable information infrastructure for government that can be readily accessed by enterprises. The authors propose a conceptual model for government information provisioning. The rationale for this proposal is to motivate the creation of durable, standard, and open government information infrastructures. The model acquires information from natural language documents and represents it using ontology. A proof-of-concept prototype and its preliminary results are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "New telerehabilitation services for the elderly",
        "doc_scopus_id": "84887892291",
        "doc_doi": "10.4018/978-1-4666-3990-4.ch006",
        "doc_eid": "2-s2.0-84887892291",
        "doc_date": "2013-04-30",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            },
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            }
        ],
        "doc_keywords": [
            "Cognitive capability",
            "Health-care system",
            "Multi-Modal Interactions",
            "Older People",
            "Telerehabilitation",
            "Unsolved problems",
            "Vision problems",
            "Work in progress"
        ],
        "doc_abstract": "© 2013 by IGI Global. All rights reserved.The world's population is getting older with the percentage of people over 60 increasing more rapidly than any other age group. Telerehabilitation may help minimise the pressure this puts on the traditional healthcare system, but recent studies showed ease of use, usability, and accessibility as unsolved problems, especially for older people who may have little experience or confidence in using technology. Current migration towards multimodal interaction has benefits for seniors, allowing hearing and vision problems to be addressed by exploring redundancy and complementarity of modalities. This chapter presents and contextualizes work in progress in a new telerehabilitation service targeting the combined needs of the elderly to have professionally monitored exercises without leaving their homes with their need regarding interaction, directly related to age-related effects on, for example, vision, hearing, and cognitive capabilities. After a brief general overview of the service, additional information on its two supporting applications are presented, including information on user interfaces. First results from a preliminary evaluation are also included.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cloud computing enhanced service development architecture for the living usability lab",
        "doc_scopus_id": "84898310737",
        "doc_doi": "10.4018/978-1-4666-3667-5.ch003",
        "doc_eid": "2-s2.0-84898310737",
        "doc_date": "2013-03-31",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            },
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living (AAL)",
            "Innovative services",
            "Life expectancies",
            "Living laboratories",
            "Local processing",
            "Multiple data streams",
            "Service development",
            "Storage resources"
        ],
        "doc_abstract": "© 2013, IGI Global.As life expectancy increases, so does the number of Ambient Assisted Living (AAL) initiatives. The Living Usability Lab is a user-centered living laboratory aimed at open-innovation and evaluation of new approaches to AAL applications and services, where the different stakeholders may develop and evaluate innovative services for the elderly in near-real life conditions. These AAL initiatives often traverse several research fields, from embedded devices to multiple data streams analysis. Advanced processing, reasoning, and storage of such data streams poses a complex problem usually solved using local processing and storage resources. This chapter presents an overview of the LUL initiative, its services, and applications, and explores the problem of advanced processing, reasoning, and storage from a cloud computing perspective.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a systematic and quantitative analysis of vocal tract data",
        "doc_scopus_id": "84906221888",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84906221888",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            }
        ],
        "doc_keywords": [
            "Application examples",
            "Large amounts of data",
            "Quantitative approach",
            "Quantitative comparison",
            "Real-Time MRI",
            "Traditional approaches",
            "Visual representations",
            "Vocal-tracts"
        ],
        "doc_abstract": "Articulatory data can nowadays be obtained using a wide range of techniques, such as real-time magnetic resonance (RT-MRI), enabling acquisitions of large amounts of data. A major challenge arises: Analysing these new large data sets to extract meaningful information regarding speech production in an expedite and replicable way. Traditional approaches such as superimposing vocal tract profiles and qualitatively characterizing relevant properties and differences, although providing valuable information, are rather inefficient and subjective. Therefore, analysis must evolve towards a more automated, quantitative approach. To tackle this issue we propose the use of objective measures to compare the configurations assumed by the vocal tract during the production of different sounds. The proposed framework provides quantitative data regarding differences pertaining meaningful regions under the influence of various articulators. Visual representation of such data is a key part of the proposal and some concrete forms of visualization are proposed to depict the differences found and corresponding direction of change. Application examples concerning the articulatory characterization of EP vowels are presented with promising results, paving the way towards automated and objective analyses of articulatory data. Copyright © 2013 ISCA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Communications in Computer and Information Science: Preface",
        "doc_scopus_id": "84871521605",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84871521605",
        "doc_date": "2012-12-28",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic speech recognition based on Ultrasonic Doppler Sensing for European Portuguese",
        "doc_scopus_id": "84871491183",
        "doc_doi": "10.1007/978-3-642-35292-8_24",
        "doc_eid": "2-s2.0-84871491183",
        "doc_date": "2012-12-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Acoustic information",
            "Audio signal",
            "Automatic speech recognition",
            "Automatic speech recognition system",
            "Environmental noise",
            "European Portuguese",
            "Information disclosure",
            "Isolated word recognition",
            "Nasality",
            "Ultrasonic doppler",
            "Word error rate"
        ],
        "doc_abstract": "Conventional Automatic Speech Recognition systems solely rely on acoustic information, making them susceptible to problems like environmental noise, privacy, information disclosure and also excluding users with speech impairments. An Ultrasonic Doppler Sensing (UDS) based interface may be used to tackle these issues since it does not rely on audio signal information. This paper describes the first speech recognition experiments based on UDS for European Portuguese (EP). The work here presented analyzes the UDS signal and explores the recognition of EP digits and minimal pairs of words that only differ on nasality of one of the phones. The results of our experiments show a best word error rate of 27.8% using data collected with the device at different distances from the speaker in an isolated word recognition problem. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Acoustic analysis of European Portuguese oral vowels produced by children",
        "doc_scopus_id": "84871478257",
        "doc_doi": "10.1007/978-3-642-35292-8_14",
        "doc_eid": "2-s2.0-84871478257",
        "doc_date": "2012-12-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Acoustic analysis",
            "Age effects",
            "Formant frequency",
            "Fundamental frequencies",
            "Male speakers"
        ],
        "doc_abstract": "This study investigates acoustic changes in the speech of European Portuguese children, as a function of age and gender. Fundamental frequency, formant frequencies and duration of vowels produced by a group of 30 children, ages 7 and 10 years, were measured. The results revealed that, for male speakers, F0, F1 and F2 decrease as age increases, although the age effect was not statistically significant for F0 and F1. A similar trend was observed for female speakers, but only in F2. Moreover, F0 and formant frequencies were found to be similar between male and female children. Between ages 7 and 10, vowel durations decreased significantly, and the values for females were higher than those for males. These results provide a base of information for establishing the normal pattern of development in European Portuguese children. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Parametric asynchronous eye diagram for optical performance monitoring",
        "doc_scopus_id": "84893512925",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84893512925",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            }
        ],
        "doc_keywords": [
            "Bit rates",
            "Differential group delay",
            "Eye diagrams",
            "Modulation formats",
            "Optical performance monitoring",
            "Optical signal to noise ratio"
        ],
        "doc_abstract": "We experimentally demonstrate the novel Parametric Asynchronous Eye Diagram (PAED) applicable to Optical Performance Monitoring. Parametric Asynchronous Eye Diagram uses samples of the signal and samples of the derivative of the signal to construct a diagram similar in shape and behavior to the Synchronous Eye Diagram. This diagram allows to identify separately the impact of Chromatic Dispersion (CD), Differential Group Delay (DGD) and Optical Signal to Noise Ratio (OSNR). The technique is completely independent of the modulation format, and bit rate. © 2012 OSA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An MRI study of the oral articulation of European Portuguese nasal vowels",
        "doc_scopus_id": "84878599176",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84878599176",
        "doc_date": "2012-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "European Portuguese",
            "Imaging data",
            "Magnetic Resonance Imaging (MRI)",
            "Nasal vowels",
            "Production of",
            "Real-Time MRI",
            "Vocal-tracts",
            "Vowel production"
        ],
        "doc_abstract": "There is increasing evidence that, in addition to velopharyngeal coupling, lingual position may also change during production of phonemic nasal vowels. In order to investigate differences in oral articulation between European Portuguese (EP) nasal vowels and oral counterparts, imaging data (both static and real-time MRI) of several EP speakers (male and female) are used. Superimposition of outlines of the vocal tract profiles, semi-automatically extracted from MRI images, were used to compare the position of tongue and lips during nasal and oral vowel production. The results suggest that lingual and labial differences between nasal vowels and their oral counterparts are quite subtle in EP. Nasal vowels [ã], [õ] exhibited more articulatory adjustments with respect to oral congeners than [1̃] and [ũ].",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards ontology based health information search in Portuguese - A case study in neurologic diseases",
        "doc_scopus_id": "84869022464",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84869022464",
        "doc_date": "2012-11-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "General publics",
            "Health informations",
            "Health professionals",
            "Health related informations",
            "Knowledge base",
            "Neurological disease",
            "Ontology-based",
            "Ontology-based information extraction",
            "Proof of concept",
            "Relevant documents",
            "Search interfaces",
            "Semantic annotations"
        ],
        "doc_abstract": "Health related information is spread across different locations, making difficult gathering, structuring and managing all this data to make it available through search and navigation to health professionals, students, researchers or even general public. In this paper, we present a workflow to support enhanced search, comprising the development of an ontology for the domain, entity annotation, advanced queries to the created knowledge base and a search interface to explore this information. The aim was to build a searching platform that would be able to present the most relevant documents as response to advanced user's queries. A proof of concept was implemented for neurologic diseases domain, demonstrating how the workflow can be used to obtain a functional ontology-based information extraction tool. © 2012 AISTI.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Segmentation and analysis of the oral and nasal cavities from MR time sequences",
        "doc_scopus_id": "84864125184",
        "doc_doi": "10.1007/978-3-642-31298-4_26",
        "doc_eid": "2-s2.0-84864125184",
        "doc_date": "2012-07-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Dynamic aspects",
            "Large images",
            "MR images",
            "Nasal cavity",
            "Nasal vowels",
            "Oral cavity",
            "Real-Time MRI",
            "Segmentation tool",
            "Speech production",
            "Temporal resolution",
            "Time sequences",
            "Time-consuming tasks",
            "Vocal-tracts"
        ],
        "doc_abstract": "The study of dynamic aspects of speech production in Portuguese is very important to characterize vowel nasalization. In this context, the analysis of velum movement remains a challenging task and only a few studies present articulatory descriptions of Portuguese nasal vowels. Advances in real-time MRI (magnetic resonance imaging) allow the acquisition of vocal tract images with reasonable spatial and temporal resolution to enable observation and quantification of articulatory movements. The resulting data consists of large image sequences and the structures of interest (e.g., oral cavity) have to be identified (segmented) throughout to enable analysis which can be a time consuming task. This article presents a segmentation tool for real-time MR image sequences of the oral and nasal cavities. The proposed tool has been implemented using MevisLab and provides features for the analysis of the resulting data. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ambient assisted living technologies, systems and services: A systematic literature review",
        "doc_scopus_id": "84861998009",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861998009",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Environmental factors",
            "Innovative technology",
            "Natural environments",
            "Systematic literature review",
            "Systematic Review"
        ],
        "doc_abstract": "This paper intends to demonstrate that it is possible to classify Ambient Assisted Living (AAL) services using the International Classification of Functioning, Disability and Health (ICF), in particular its components activities, participation and environmental factors. For this purpose a systematic review of the literature on AAL services was undertaken and existing AAL services summarized and characterized. To be included in this review articles must have defined innovative concepts or characterized innovative technologies, products or systems that can contribute to the development of the AAL paradigm, with the aim of enabling people with specific demands (e.g. elderly) to live longer in their natural environment. Results indicate that most publications regarding AAL are technology-oriented with only a few articles describing applications and scenarios. Results also indicate that it is possible to link tasks to categories of the ICF components activities, participation and environmental factors.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a silent speech interface for portuguese surface electromyography and the nasality challenge",
        "doc_scopus_id": "84861985191",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861985191",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Acoustic information",
            "Acoustic signals",
            "Automatic speech recognition",
            "Environmental noise",
            "European portuguese",
            "Human computer interfaces",
            "Information disclosure",
            "Living room",
            "Nasality",
            "Native language",
            "Performance degradation",
            "Speech interface",
            "State-of-the-art approach",
            "Surface electromyography"
        ],
        "doc_abstract": "A Silent Speech Interface (SSI) aims at performing Automatic Speech Recognition (ASR) in the absence of an intelligible acoustic signal. It can be used as a human-computer interaction modality in high-background-noise environments, such as living rooms, or in aiding speech-impaired individuals, increasing in prevalence with ageing. If this interaction modality is made available for users own native language, with adequate performance, and since it does not rely on acoustic information, it will be less susceptible to problems related to environmental noise, privacy, information disclosure and exclusion of speech impaired persons. To contribute to the existence of this promising modality for Portuguese, for which no SSI implementation is known, we are exploring and evaluating the potential of state-of-the-art approaches. One of the major challenges we face in SSI for European Portuguese is recognition of nasality, a core characteristic of this language Phonetics and Phonology. In this paper a silent speech recognition experiment based on Surface Electromyography is presented. Results confirmed recognition problems between minimal pairs of words that only differ on nasality of one of the phones, causing 50% of the total error and evidencing accuracy performance degradation, which correlates well with the exiting knowledge.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From an autonomous soccer robot to a robotic platform for elderly care",
        "doc_scopus_id": "84861984252",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861984252",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Developed countries",
            "Elderly care",
            "Hardware and software",
            "Home care",
            "Independent living",
            "Innovative solutions",
            "Number of peoples",
            "Nursing homes",
            "Robotic platforms",
            "Robotic soccer",
            "Soccer robot"
        ],
        "doc_abstract": "Current societies in developed countries face a serious problem of aged population. The growing number of people with reduced health and capabilities, allied with the fact that elders are reluctant to leave their own homes to move to nursing homes, requires innovative solutions since continuous home care can be very expensive and dedicated 24/7 care can only be accomplished by more than one care-giver. This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Assessment of ambient assisted living services in a living lab approach: A methodology based on icf",
        "doc_scopus_id": "84861983745",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861983745",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Conceptual frameworks",
            "Daily lives",
            "Evaluation methodologies",
            "Living lab",
            "Products and services"
        ],
        "doc_abstract": "A major problem inherent to the development of Ambient Assisted Living (AAL) products and services is its assessment and validation. It's crucial to involve the users in the validation/development process. In a Living Lab approach, the validation of AAL products and services is focused in user's needs and preferences, integrating their daily lives and social roles. The International Classification of Functioning, Disability and Health (ICF) arises as a conceptual framework to develop instruments for the evaluation of AAL products and services. In this sense, the purpose of this paper is the description of an evaluation methodology of AAL products and services in a Living Lab approach based on ICF.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Health@Home scenario: Creating a new support system for home telerehabilitation",
        "doc_scopus_id": "84861978367",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861978367",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Conceptual architecture",
            "Early evaluation",
            "Elderly people",
            "Innovative method",
            "Multi-modality",
            "Support systems",
            "Telerehabilitation",
            "User-centric",
            "Video surveillance"
        ],
        "doc_abstract": "The creation of innovative methods and technologies for elderly is the main purpose for Ambient Assisted Living. This paper provides a description on all the associated stages and development questions required for the establishment of a new telerehabilitation service. The service intends to provide elderly people with the possibility of performing rehabilitation sessions in their houses, with constant medical supervision via video surveillance. Following the principles of a new conceptual architecture for services, and developed according to user-centric paradigms such as multimodality and high usability criteria, early evaluation results point the service as an asset for remote rehabilitation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Foreword",
        "doc_scopus_id": "84861964371",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861964371",
        "doc_date": "2012-06-13",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Spoken communication with CAMBADA@Home service robot",
        "doc_scopus_id": "84861961543",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861961543",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Automatic speech recognition",
            "Hands-free",
            "Home service robot",
            "In-line",
            "Mobile service robots",
            "NAtural language processing",
            "Natural response",
            "Robotic platforms",
            "Spoken languages",
            "Text to speech",
            "Two-component"
        ],
        "doc_abstract": "Spoken language is a natural way to control the human-robot interaction, especially for mobile service robots. It has some important advantages over other communication approaches: eyes and hands free, communication from a distance, even without being in line of sight and no need for additional learning for humans. In this paper, we present the spoken dialog framework integrated in our mobile service robot CAMBADA@Home, a robotic platform aimed at move into a living space and interact with users of that space. The proposed framework comprises three major spoken and natural language processing components: an Automatic Speech Recognition component to process the human requests, a Text-to-Speech component to generate more natural responses from the robot side, and a dialog manager to control how these two components work together.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Production and modeling of the European Portuguese palatal lateral",
        "doc_scopus_id": "84858404691",
        "doc_doi": "10.1007/978-3-642-28885-2_36",
        "doc_eid": "2-s2.0-84858404691",
        "doc_date": "2012-03-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Acoustic modeling",
            "Area function",
            "Convex shapes",
            "Cross sectional area",
            "European Portuguese",
            "Frequency ranges",
            "Lateral compression",
            "magnetic ressonance",
            "MRI Image",
            "palatal lateral",
            "Pole-zero",
            "Vocal-tracts"
        ],
        "doc_abstract": "In this study, an articulatory characterization of the palatal lateral is provided, using MRI images of the vocal tract acquired during the production of /L/ by several speakers of European Portuguese. The production of this sound involves: a complete linguo-alveolopalatal closure; inward lateral compression of the tongue and a convex shape of the posterior tongue body, allowing airflow around the sides of the tongue; large cross-sectional areas in the upper pharyngeal and velar regions. The lengths and area functions derived from MRI are analysed and used to model the articulatory-acoustic relations involved in the production of /L/. The results obtained in the first simulations show that the vocal-tract model (VTAR) is reasonably able to estimate the frequencies of the first formants and zeros. The lateral channels combined with the supralingual cavity create pole-zero clusters around and above F3, in the frequency range of 2-4.5 kHz. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface",
        "doc_scopus_id": "84858390032",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84858390032",
        "doc_date": "2012-03-22",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Morphosyntactic analysis of language in children with autism spectrum disorder",
        "doc_scopus_id": "84858387153",
        "doc_doi": "10.1007/978-3-642-28885-2_4",
        "doc_eid": "2-s2.0-84858387153",
        "doc_date": "2012-03-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autism spectrum disorders",
            "Automated tools",
            "Children with autisms",
            "Social interactions",
            "Speech therapy",
            "Syntactic structure"
        ],
        "doc_abstract": "Autism is a neurodevelopmental disorder that involves severe and persistent deficits in social interaction, language and communication. This study aims to contribute to the characterization of the language of children with ASD (Autism Spectrum Disorders) in regard to morphology and syntax (morphosyntactic classes, most frequent words, use of personal pronouns, inflections of verbs: tense and person and syntactic structure). The four selected children, diagnosed with ASD, were recorded during activities that promoted speech. The records were transcribed and processed by PALAVRAS parser. The more frequent morphosyntactic classes detected were verbs and names. The most used determinants were the definite articles and, as far as adverbs are concerned, the most frequently used ones were adverbs of place. The syntactic structures were simple and short. The automated tools used for processing and analysis proved to be extremely functional and practical, with fast and effective results, opening the possibility of its usage for evaluation purposes in speech therapy practice. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Real-time MRI for Portuguese: Database, methods and applications",
        "doc_scopus_id": "84858377622",
        "doc_doi": "10.1007/978-3-642-28885-2_35",
        "doc_eid": "2-s2.0-84858377622",
        "doc_date": "2012-03-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "European Portuguese",
            "laterals",
            "nasal vowels",
            "Real-Time MRI",
            "trills"
        ],
        "doc_abstract": "In this paper, we present a database of synchronized audio and Real-Time Magnetic Resonance Imaging (RT-MRI) in order to study dynamic aspects of the production of European Portuguese (EP) sounds. Currently, data have been acquired from one native speaker of European Portuguese. The speech corpus was primarily designed to investigate nasal vowels in a wide range of phonological contexts, but also includes examples of other EP sounds. The RT-MRI protocol developed for the acquisition of the data is detailed. Midsagittal and oblique images were acquired with a frame rate of 14 frames/s, resulting in a temporal resolution of 72 ms. Different image processing tools (automatic and semi-automatic) applied for inspection and analysis of the data are described. We demonstrate the potential of this database and processing techniques with some illustrative examples of Portuguese nasal vowels, taps, trills and laterals. © 2012 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Parametric asynchronous eye diagram for optical performance monitoring",
        "doc_scopus_id": "84893999975",
        "doc_doi": "10.1364/nfoec.2012.jw2a.33",
        "doc_eid": "2-s2.0-84893999975",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Bit rates",
            "Differential group delay",
            "Eye diagrams",
            "Modulation formats",
            "Optical performance monitoring",
            "Optical signal to noise ratio"
        ],
        "doc_abstract": "We experimentally demonstrate the novel Parametric Asynchronous Eye Diagram (PAED) applicable to Optical Performance Monitoring. Parametric Asynchronous Eye Diagram uses samples of the signal and samples of the derivative of the signal to construct a diagram similar in shape and behavior to the Synchronous Eye Diagram. This diagram allows to identify separately the impact of Chromatic Dispersion (CD), Differential Group Delay (DGD) and Optical Signal to Noise Ratio (OSNR). The technique is completely independent of the modulation format, and bit rate. © 2012 OSA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The international classification of functioning, disability and health as a conceptual model for the evaluation of environmental factors",
        "doc_scopus_id": "84886585368",
        "doc_doi": "10.1016/j.procs.2012.10.033",
        "doc_eid": "2-s2.0-84886585368",
        "doc_date": "2012-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The concept of Design for All emphasizes the impact of the surrounding environment in the individuals' functionality. The International Classification of Functioning, Disability and Health brought the concepts of functionality and disability into a comprehensive whole of multiple dimensions of human functioning, such as biological, psychological, social and environmental. In order to contribute to a greater overall functionality of the individual, the use of software and complex systems can be decisive to assist the people with special needs in all areas of life. The paradigm introduced with ICF is inclusive and universal, so it favors not only the old people, but all others, whether they have a limitation or not. The characteristics of Ambient Assisted Living (AAL) are appropriate to fulfill elderly needs. However, the current state of development is still mostly oriented to a technological perspective, where the individuals' functionality has not been fully addressed. Under the Living Usability Lab project we have defined a methodology and created some evaluation tools for assessment of AAL services according to a Living Lab perspective, based on the ICF. In this paper we intend to describe the base fundamentals of this proposal as well as present some results concerning a practical implementation of this methodology.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2012-12-01 2012-12-01 2015-11-08T21:06:03 S1877-0509(12)00795-8 S1877050912007958 10.1016/j.procs.2012.10.033 S300 S300.4 HEAD-AND-TAIL 2015-11-09T13:43:35.00848-05:00 0 0 20120101 20121231 2012 2012-12-01T00:00:00Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 false 14 14 C Volume 14 35 293 300 293 300 2012 2012 2012-01-01 2012-12-31 2012 Proceedings of the 4th International Conference on Software Development for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2012) Leontios Hadjileontiadis Paulo Martins Robert Todd Hugo Paredes João Rodrigues João Barroso article fla Copyright © 2012 Published by Elsevier B.V. INTERNATIONALCLASSIFICATIONFUNCTIONINGDISABILITYHEALTHACONCEPTUALMODELFOREVALUATIONENVIRONMENTALFACTORS MARTINS A MARTINSX2012X293 MARTINSX2012X293X300 MARTINSX2012X293XA MARTINSX2012X293X300XA Full 2013-07-16T12:28:30Z ElsevierWaived OA-Window item S1877-0509(12)00795-8 S1877050912007958 10.1016/j.procs.2012.10.033 280203 2015-11-09T13:43:35.00848-05:00 2012-01-01 2012-12-31 true 918803 MAIN 8 58041 849 656 IMAGE-WEB-PDF 1 Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 1877-0509 Â© 2012 2012 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the Scientifi c Programme Committee of the 4th International Conference on Software Development for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2012) doi: 10.1016/j.procs.2012.10.033 Proceedings of the 4th International Conference on Software Development for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2012) The International Classification of Functioning, Disability and Health as a conceptual model for the evaluation of environmental factors Ana Isabel Martinsa*, Alexandra QueirÃ³sb, Margarida Cerqueirab, Nelson Rochaa,d AntÃ³nio Teixeiraa,c a Institute of Electronics and Telematics Engineering of Aveiro, Campus UniversitÃ¡rio, 3810 Aveiro, Portugal b Health Sciences School, University of Aveiro, Campus UniversitÃ¡rio, 3810 Aveiro, Portugal c Department of. Electronics Telecommunications and Informatics, University of Aveiro, Portugal d Health Sciences Department, University of Aveiro, Campus UniversitÃ¡rio, 3810 Aveiro, Portugal Abstract The concept of Design for All International Classification of Functioning, Disability and Health brought the concepts of functionality and disability into a comprehensive whole of multiple dimensions of human functioning, such as biological, psychological, social and environmental. In order to contribute to a greater overall functionality of the individual, the use of software and complex systems can be decisive to assist the people with special needs in all areas of life. The paradigm introduced with ICF is inclusive and universal, so it favors not only the old people, but all others, whether they have a limitation or not. The characteristics of Ambient Assisted Living (AAL) are appropriate to fulfill elderly needs. functionality has not been fully addressed. Under the Living Usability Lab project we have defined a methodology and created some evaluation tools for assessment of AAL services according to a Living Lab perspective, based on the ICF. In this paper we intend to describe the base fundamentals of this proposal, as well as present some results concerning a practical implementation of this methodology. Â© 2012 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of Scientific Programme Committee of the 4th International Conference on Software Development for Enhancing Accessibility and Fighting Info-exclusion (DSAI 2012) Keywords: International Classification of Functioning, Disability and Health; environmental factors, ambiente assisted living, assessibility; living lab; evaluation; Design for All Available online at www.sciencedirect.com 2012 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the cientifi c Programm Committee of the 4th International Conferenc on Sof ware D velopme t f r Enha cing Accessibility and Fighting Info-exclusion (DSAI 2012) Open access under CC BY-NC-ND license. Open access under CC BY-NC-ND license. 294 Ana Isabel Martins et al. / Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 1. Introduction The approval of the International Classification of Functioning, Disability and Health (ICF) [1] by the World Health Organization (WHO) has marked a paradigm shift in the way health and disability are understood and measured [2]. The ICF describes the health and health-related states of an individual and indicates the positive and negative aspects of the interaction between the individual (with a health condition) and the contextual factors (environmental and personal). Until ICF publication, disability has been seen as an unrelated entity that imposes restriction on the individual. ICF has brought these concepts into a comprehensive whole of multiple dimensions of human functioning, such as biological, psychological, social and environmental [2]. Previous concepts, like Design for All, also emphasize the impact of the surrounding environment in the functionality of individuals. This perspective changes how care is provided, and empowers people as part involved in the care provision motivation to control participation and develop skills to adapt and to influence the environment. This approach underlines the role of participation and control, supporting functionality versus disability [3]. This enabling perspective is one of the premises of active aging, as it corresponds to the process of optimizing opportunities for health, participation and security in order to enhance quality of life as people age [4]. In order to contribute to a greater overall functionality of the individual, the use of software and complex systems can be decisive to assist the elderly in all areas of life. These products and services play a key role in the interaction of users with the surrounding environment. In order to make this products and services easily usable and adapted to users, it is crucial that the development and evaluation process happens according to it is essential that the functionality of individuals and the environment where they conduct their lives is contemplated in the development and evaluation of such products or services, thereby insuring their suitability. However, the current state of development is still mostly oriented to a ICF arises as a possible conceptual model for the development and evaluation of environmental factors such as products or services. In this paper we intend to describe the base fundamentals of this proposal, as well as present some results concerning a practical implementation of this methodology. This paper consists of four sections: 1) Introduction: exposition of the problem; 2) Background: overview of the conceptual modal of ICF; 3) A practical implementation using an ICF based methodology: brief methodology description and presentation of an illustrative application example using a Telerehabilitation service and 4) Conclusions: final considerations and presentation of suggestions for future work. 2. Background 2.1. International Classification of Functioning, Disability and Health The ICF is one of the international classifications developed by the World Health Organization (WHO). Its general objective is to provide a unified and standardized language as well as a framework for the description of health and health-related states. The ICF is a hierarchical classification based on the perspective of the body, individual and society, organized in two parts; (1) functionality and disability, (2) contextual factors. Each part is subdivided into two components, the first, functionality and disability, includes body functions and structures and activities and participation. The second, contextual factors, is divided into personal factors and environmental factors [1]. In this classification, functionality indicates the positive aspects of the interaction between an individual (with a particular health condition) and the contextual factors (environmental and personal) [1]. On the opposite, in negative terms, disability refers to the generic term for disabilities, limitations in activity and participation restrictions. The utilization of ICF allows a complete and multidisciplinary 295 Ana Isabel Martins et al. / Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 approach focused on the individual [5]. The ICF model replaces the negative focus of impairment and disability, by a neutral perspective embracing all components that promote or hinder the execution of their functions, both biological and social [1, 6] . With this paradigm shift, the disease is no longer seen as the only responsible for disability and impairments, but as one of the factors influencing health, such as environmental factors, social and personal factors [7]. The ICF evidence the importance of interaction between the health condition, the individual and the surrounding environment. Figure1 presents the spectrum of interactions between the components of ICF. The environment is crucial in attenuation or elimination of the disability caused by injuries that occur as a result of adverse health conditions. In ICF, an environmental factor is classified as a facilitator if contributes to increase users performance and participation. On the other hand, if an environmental factor restricts user performance and participation, then it is classified as a barrier. Different environments can have a distinct impact on the same individual with a specific health condition. An environment with barriers, restricts the individual's performance, on the other hand, other environments with facilitators improve his functionality [1]. For example, barriers to mobility in the home context may limit an individual's ability to carry out activities (for example stairs or ports), the same happens with the mobility barriers in the community context (for example, irregular sidewalks) that may limit involvement in community activities [8]. Conversely, products to support mobility (for example, canes and wheelchairs) may enhance the person's participation in daily activities by providing physical assistance in performing specific tasks. Similarly, transport facilitators (for example, availability of a car or public transport) can enable greater participation in community activities [8]. Environmental factors are organized in the ICF in two distinct levels [1]: home, workplace, among others. This level includes, apart from environmental materials, direct contact with other individuals, such as family members; laws and government bodies. Fig. 1 - Interactions between the components of ICF. The functionality of an individual in a particular area is a complex interaction or relationship between health condition and the contextual factors. There is a dynamic interaction between these entities: an intervention in an element can potentially modify one or more other elements. These interactions are specific and operate in all directions [1]. 296 Ana Isabel Martins et al. / Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 The personal factors are not yet classified in the ICF due to the high social and cultural variety among subjects. Recent studies reveal that there is a growing recognition that the role of environmental factors changed the problematic and, consequently, the focus of intervention, disability is no longer understood as a single category, but rather the result of an interaction between an individual's health condition and environmental factors [9]. It is essential that the environment where an individual lives is inclusive. Ensure full accessibility is an essential aspect of the quality of life of all citizens. A fully facilitator environment enable all people to have equal opportunities for participation in all aspects of life in society. Creating an inclusive environment manufacturers, and political leaders. To achieve this, all that is designed, in particular, the surroundings, the everyday life, culture and information, should be developed considering the concept of Design for All [10]. This concept aims to develop social inclusion and equality considering human diversity enabling all people to have equal opportunities for participation in all aspects of life in society. Design for All approach should apply to the development of [10]: impact on mobility, work and leisure within the community); standards); rfaces (development of services, products and systems that eliminate social exclusion and functional difficulties of individuals); prejudices and other negative social attitudes). The elderly emerges as obvious beneficiaries of a physical, social and atitudinal facilitator environment. Ageing affects all areas of life of an individual and causes several disabilities, including limitations in terms of mobility, sensory deficits, and increase susceptibility to diseases, particularly chronic diseases such as cardiovascular disease and dementias [11]. The age pyramid inversion and the increased number of elderly, evidence even more the need to make the services and products, and consequently the environment, accessible to this population. 3. A practical implementation using an ICF based methodology The paradigm introduced with ICF is inclusive and universal, so it favors not only the old people, as well as all others, whether they have a limitation or not [10, 12]. The characteristics of Ambient Assisted Living (AAL) are appropriate to fulfil elderly needs as it refers to concepts, products or services that interconnect and improve new technologies and social systems, with the aim of promoting the quality of life of all people during all stages of their lives [13]. AAL enables the utilization of products and technologies and the provision of distance services such as health care, helping to achieve autonomy, independence and dignity [12]. The Living Usability Lab (LUL) intends to develop AAL services to fulfil some needs that are common to older adults: full participation in society, health and quality of life or living with security [14]. The LUL is essentially an interactive environment to facilitate the research, development, integration, validation and evaluation of multimodal, adaptability and user monitoring technologies, new modes of interaction and new services supported by new generation networks using a strong involvement of users for the specification of new services. A central idea of the Living Lab concept is a strong involvement of the users in all the development phases, including the conceptual design and, later, the prototypes. This method allows a more realistic validation of environmental factors centered in the user. Therefore, the living Lab approach seems to be a good method to evaluate environmental factors is this perspective because it is in line with the ICF approach as both adopt a holistic perspective of user involvement. Therefore, new evaluation methodologies are required to allow the 297 Ana Isabel Martins et al. / Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 generating of new design solutions and the evaluation of design solutions derived from the first phases of user involvement. A major problem inherent in the development of AAL products and services is its validation. Only through and developing guidelines for users and developers [15]. The utilization of AAL products and services aims to improve people's participation and performance in carrying out activities i.e., improve the individuals functionality. Because AAL products and services intent to change the personal environment surrounding, in order to improve individuals participation, they should be considered as environmental factors in an ICF approach. In fact, if an individual is surrounded with services and products tailored to their characteristics, he will be able to reach a higher level of functionality. Accordingly, the ICF may arise as a conceptual model for the holistic development of a methodology for evaluation of environmental factors and, consequently, AAL products and services. Using the ICF as a framework to develop instruments for the evaluation of environmental factors permits that the terminology, concepts and coded information can be aggregated with the available information, and can also be used as a comprehensive model to characterize users and their contexts, activities and participation [16]. Therefore, the ICF can be used to specify, develop and characterize AAL products and services, as well as to develop appropriate tools to assess them and their imp [16]. Generally, in the development of AAL products or services, experts define which functions and services should be used and how the user should interact with the product or service. The prototypes, based on those conceptual design ideas, are then evaluated by users. Therefore, this approach is inadequate for the development of such services and products because the initial design ideas are not based on the mindsets, experiences and mental models of the users as in a living lab approach [14]. Under the Living Usability Lab project we have defined a methodology and created some evaluation tools for evaluation of AAL services according to a Living Lab perspective, based on the ICF. This evaluation methodology assumes three phases of reference. The first phase is the conceptual validation, followed by a prototype test, and finally a pilot test. Those phases are not isolated, and they are based on a spiral approach that follows the development progress since the beginning [14]. The first phase of evaluation, conceptual validation, aims to determine if an idea of a product or service is sustainable in terms of interface and functions. In prototype test, the second phase of the evaluation, it is Fig. 2. Phases of reference of the Living Lab methodology 298 Ana Isabel Martins et al. / Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 intended to collect information regarding the usability and user satisfaction. At this phase there is already a physical implementation of the product or service prototype in order to be tested by users. The prototype test is conducted in a controlled environment. Finally, the third phase of evaluation, pilot test, intends to evaluate, in For this reason, this last phase of testing differs from the prototype phase in the context where it happens. The product The phases of reference in the Living Lab methodology are present in Figure 2. 3.1. An illustrative application example using a Telerahabilitation service The ICF based evaluation methodology was first used to evaluate the TeleRehabilitation service under develop in the LUL project [17, 18]. In general terms, the TeleRehabilitation service allows supervised remote exercise sessions, as a way to maintaining health and prevent illness [18]. The results of a first effort to evaluate an AAL service, the Telerehabilitaion service, possible to use ICF as a conceptual framework to evaluate environmental products in a Living Lab approach. It was conducted an exploratory pretest to assess the evaluation tools adequacy based on the ICF, developed in this methodology. Although the results seemed to point toward a good measure to discriminate facilitators and barriers, the instrument was not sensitive to recognize why a particular component or feature acted as a facilitator or barrier. Tests held revealed a difficulty in identifying the reason why a particular component or characteristic of a component is identified as a barrier [14]. Thus, we suggest that, whenever a component is identified as a barrier, a set of predefined categories should be presented and the user should mark the ones that correspond to the reason why this component acts as a barrier. This selection should be complete with additional written information. This procedure enables the developers to identify what changes to perform on the product or service in order to make it more adapted to users, and must be performed for all the components identified as a barrier. 2 How would you characterize the components present in the service? Barrier Facilitator -3 -2 -1 1 2 3 N A A The description of the session status X 1 Icons too small A 1. Icons size 2. Sharpness 3. Contrast 4. Other Fig. 3.- Excerpt of the ICF based assessment tool 299 Ana Isabel Martins et al. / Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 classified as a medium barrier. The reason why this component acts as a barrier is reflected in the category 1 (icons size) and completed in the additional information (small size). it can be useful to define priority in the development progress of the environmental factors, both in terms of Another advantage of this specification is the possibility to match the barriers identified with the characteristics of users, verifying p component features as barriers. The gradation of facilitators may also be important because it permits the definition of good practices regarding the development of environmental factors. If a particular component or feature of an environmental factor is repeatedly identified as a facilitator in similar products or services then it may have the potential to be a good practice for services of the same type. 4. Conclusion The development of new products and services should promote the participation of all in society, such as people with disabilities, elderly or any other. A holistic approach is necessary to understand these special needs and to contribute to an inclusive society. The ICF describes the health and health-related states of an individual in a holistic way as it participation and, in the context of quality of life, health and security, this products and services can be relevant. Therefore, developing and classifying AAL products and services, using the ICF language, may facilitate communication between different intervenient interested in AAL products and services. Since the current stage of development/evaluation of AAL products and services is still very technology-oriented [19] and its functionality has not been addressed adequately, we have developed assessment tools that address the The first tests held showed that ICF seems to be useful in a first level of screening, discriminating facilitators and barriers. In order to address this, we have completed the instruments with categories that help us to understand the reason why a certain component of the environmental factor acts as a facilitator or a barrier. This allow us to collect more and more significant information in order to understand what changes should be performed in the environmental factor in order to make it more adapted to its users so it can enhance their functionality contributing to a higher participation in the society, a better performance and an improved quality of life. In addition, this methodological option can also be useful to guide the definition of good practices. This is a very characteristics. The verification of possible relationships between groups of users and the identification of certain components as barriers or environment. The definition of a conceptual model for the evaluation of environmental factors is still in a very early stage of development and the ICF based assessment tools still needs further work in order to verify the general applicability of this methodology and determinate the validity and reliability. Acknowledgements This work is part of the Living Usability Lab for Next Generation Networks (www.livinglab.pt) project, a QREN project, co-funded by COMPETE and FEDER. 300 Ana Isabel Martins et al. / Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 References [1] WHO, International Classification of Functioning, Disability and Health, 2001. [2] Kostanjsek, N., Use of The International Classification of Functioning, Disability and Health (ICF) as a conceptual framework and common language for disability statistics and health information systems. BMC Public Health, 2011. [3] Riva, G. and A. Gaggioli, Rehabilitation as Empowerment: The Role of Advanced Technologies. Studies in Health Technology and Informatics, 2009. [4] WHO. Ageing and life course. 2012; Available from: [5] Klaus, S., et al., Interrater Reliability of the Extended ICF Core Set for Stroke Applied by Physical Therapists. Physical therapy, 2008. 88(7): p. 841-856. [6] Nordenfelt, L., Action theory, disability and ICF. Disability and rehabilitation, 2003. 25(18): p. 1075-9. [7] Stefano Federici, Fabio Meloni, and A.L. Presti, International Literature Review on WHODAS II Life Span and Disability / XI, 2009. 1: p. 83-110. [8] Keysor, J.J., et al., Association of environmental factors with levels of home and community participation in an adult rehabilitation cohort. Archives of physical medicine and rehabilitation, 2006. 87(12): p. 1566-75. [9] Schneidert, M., et al., The role of environment in the International Classification of Functioning, Disability and Health (ICF). Disability and rehabilitation, 2003. 25(11-12): p. 588-95. [10] Commission, E., The Build-for-All Reference Manual 2006: Luxemburg. [11] GaÃŸner, K. and M. Conrad, ICT enabled independent living for elderly: A status-quo analysis on products and the research landscape in the field of Ambient Assisted Living (AAL) in EU-27. Instuitute for innovation and Technology 2010. [12] Living, R.E. Wichert, B., (Eds), Editor 2011, pringer: Germany. [13] Storf, H., M. Becker, and M. Riedl, Rule-based Activity Recognition Framework: Challenges, Technique and Learning. Pervasive Health, 2009. [14] Martins, A.I., et al., Assessment of Ambient Assisted Living Services in a Living Lab Approach: a Methodology based on ICF, in 2nd International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications - AAL 20122012: Algarve, Portugal. [15] Hoppe, A., Technological Stress Mental Strain of Younger and Older Users If Technology Fails, in Ambient Asssisted Living, R. Wichert and B. Eberhardt, Editors. 2011, Springer. [16] QueirÃ³s, A., et al., The International Classification of Functioning, Disability of Health as a Conceptual Framework for the Design, Development and Evaluation of AAL Services for Older Adults, in Workshop AAL2011: Rome, Italy. p. 46-59. [17] Teixeira, A., et al., New Telerehabilitation Services for the Elderly, in Handbook of Research on ICTs for Healthcare and Social Services2011. [18] Teixeira, A., et al., Output Matters! Adaptable Multimodal Output for new Telerehabilitation Services for the Elderly. 2010. [19] QueirÃ³s, A., et al., Ambient Assisted Living Technologies, Systems and Services: a Systematic Literature Review, in 2nd International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications - AAL 20122012: Algarve - Portugal. ance and an improved quality of life. In addition, this methodological option can also be useful to guide the definition of good practices. This is a very characteristics. The verification of possible relationships between groups of users and the identification of certain components as barriers or environment. The definition of a conceptual model for the evaluation of environmental factors is still in a very early stage of development and the ICF based assessment tools still needs further work in order to verify the general applicability of this methodology and determinate the validity and reliability. Acknowledgements This work is part of the Living Usability Lab for Next Generation Networks (www.livinglab.pt) project, a QREN project, co-funded by COMPETE and FEDER. 300 Ana Isabel Martins et al. / Procedia Computer Science 14 ( 2012 ) 293 â€“ 300 References [1] WHO, International Classification of Functioning, Disability and Health, 2001. [2] Kostanjsek, N., Use of The International Classification of Functioning, Disability and Health (ICF) as a conceptual framework and common language for disability statistics and health information systems. BMC Public Health, 2011. [3] Riva, G. and A. Gaggioli, Rehabilitation as Empowerment: The Role of Advanced Technologies. Studies in Health Technology and Informatics, 2009. [4] WHO. Ageing and life course. 2012; Available from: [5] Klaus, S., et al., Interrater Reliability of the Extended ICF Core Set for Stroke Applied by Physical Therapists. Physical therapy, 2008. 88(7): p. 841-856. [6] Nordenfelt, L., Action theory, disability and ICF. Disability and rehabilitation, 2003. 25(18): p. 1075-9. [7] Stefano Federici, Fabio Meloni, and A.L. Presti, International Literature Review on WHODAS II Life Span and Disability / XI, 2009. 1: p. 83-110. [8] Keysor, J.J., et al., Association of environmental factors with levels of home and community participation in an adult rehabilitation cohort. Archives of physical medicine and rehabilitation, 2006. 87(12): p. 1566-75. [9] Schneidert, M., et al., The role of environment in the International Classification of Functioning, Disability and Health (ICF). Disability and rehabilitation, 2003. 25(11-12): p. 588-95. [10] Commission, E., The Build-for-All Reference Manual 2006: Luxemburg. [11] G PROCS 1590 S1877-0509(12)00795-8 10.1016/j.procs.2012.10.033 The International Classification of Functioning, Disability and Health as a Conceptual Model for the Evaluation of Environmental Factors Ana Isabel Martins a ⁎ Alexandra Queirós b Margarida Cerqueira b Nelson Rocha a d António Teixeira a c a Institute of Electronics and Telematics Engineering of Aveiro, Campus Universitário, 3810 Aveiro, Portugal b Health Sciences School, University of Aveiro, Campus Universitário, 3810 Aveiro, Portugal c Department of. Electronics Telecommunications and Informatics, University of Aveiro, Portugal d Health Sciences Department, University of Aveiro, Campus Universitário, 3810 Aveiro, Portugal ⁎ Corresponding author. The concept of Design for All emphasizes the impact of the surrounding environment in the individual's functionality. The International Classification of Functioning, Disability and Health brought the concepts of functionality and disability into a comprehensive whole of multiple dimensions of human functioning, such as biological, psychological, social and environmental. In order to contribute to a greater overall functionality of the individual, the use of software and complex systems can be decisive to assist the people with special needs in all areas of life. The paradigm introduced with ICF is inclusive and universal, so it favors not only the old people, but all others, whether they have a limitation or not. The characteristics of Ambient Assisted Living (AAL) are appropriate to fulfill elderly needs. However, the current state of development is still mostly oriented to a technological perspective, where the individual's functionality has not been fully addressed. Under the Living Usability Lab project we have defined a methodology and created some evaluation tools for assessment of AAL services according to a Living Lab perspective, based on the ICF. In this paper we intend to describe the base fundamentals of this proposal, as well as present some results concerning a practical implementation of this methodology. Keywords International Classification of Functioning Disability and Health environmental factors ambiente assisted living assessibility living lab evaluation Design for All References [1] WHO, International Classification of Functioning, Disability and Health, 2001. [2] Kostanjsek, N., Use of The International Classification of Functioning, Disability and Health (ICF) as a conceptual framework and common language for disability statistics and health information systems. BMC Public Health, 2011. [3] Riva, G. and A. Gaggioli, Rehabilitation as Empowerment: The Role of Advanced Technologies. Studies in Health Technology and Informatics, 2009. [4] WHO. Ageing and life course. 2012; Available from: [5] Klaus, S., et al., Interrater Reliability of the Extended ICF Core Set for Stroke Applied by Physical Therapists. Physical therapy, 2008. 88(7): p. 841-856. [6] Nordenfelt, L., Action theory, disability and ICF. Disability and rehabilitation, 2003. 25(18): p. 1075-9. [7] Stefano Federici, Fabio Meloni, and A.L. Presti, International Literature Review on WHODAS II. Life Span and Disability/XI, 2009. 1: p. 83-110. [8] Keysor, J.J., et al., Association of environmental factors with levels of home and community participation in an adult rehabilitation cohort. Archives of physical medicine and rehabilitation, 2006. 87(12): p. 1566-75. [9] Schneidert, M., et al., The role of environment in the International Classification of Functioning, Disability and Health (IC F). Disability and rehabilitation, 2003. 25(11-12): p. 588-95. [10] Commission, E., The Build-for-All Reference Manual 2006: Luxemburg. [11] Gaßner, K. and M. Conrad, ICT enabled independent living for elderly: A status-quo analysis on products and the research landscape in the field of Ambient Assisted Living (AAL) in EU-27. Instuitute for innovation and Technology 2010. [12] Wojciechowski, et al., Architecture of the ‘Daily Care Journal’ for the Support of Health Care Networks, in Ambient Assisted Living, R.E. Wichert, B., (Eds), Editor 2011, pringer: Germany. [13] Storf, H., M. Becker, and M. Riedl, Rule-based Activity Recognition Framework: Challenges, Technique and Learning. Pervasive Health, 2009. [14] Martins, A.I., et al., Assessment of Ambient Assisted Living Services in a Living Lab Approach: a Methodology based on ICF, in 2nd International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications - AAL 20122012: Algarve, Portugal. [15] Hoppe, A., Technological Stress – Mental Strain of Younger and Older Users If Technology Fails, in Ambient Asssisted Living, R. Wichert and B. Eberhardt, Editors. 2011, Springer. [16] Queirós, A., et al., The International Classification of Functioning, Disability of Health as a Conceptual Framework for the Design, Development and Evaluation of AAL Services for Older Adults, in Workshop AAL2011: Rome, Italy. p. 46-59. [17] Teixeira, A., et al., New Telerehabilitation Services for the Elderly, in Handbook of Research on ICTs for Healthcare and Social Services2011. [18] Teixeira, A., et al., Output Matters! Adaptable Multimodal Output for new Telerehabilitation Services for the Elderly. 2010. [19] Queirós, A., et al., Ambient Assisted Living Technologies, Systems and Services: a Systematic Literature Review, in 2nd International Living Usability Lab Workshop on AAL Latest Solutions, Trends and Applications - AAL 20122012: Algarve - Portugal. "
    },
    {
        "doc_title": "Tongue segmentation from MRI images using ITK-SNAP: Preliminary evaluation",
        "doc_scopus_id": "84864986322",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84864986322",
        "doc_date": "2011-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "3D Visualization",
            "European Portuguese",
            "Jaccard coefficients",
            "Magnetic resonance images",
            "Manual segmentation",
            "MRI Image",
            "Region competition",
            "Semi-automatic segmentation",
            "Semi-automatics",
            "Semiautomatic methods",
            "Similarity metrics",
            "Speech production",
            "Speech synthesizer",
            "Tongue segmentation",
            "Validation",
            "Volumetric data"
        ],
        "doc_abstract": "The purpose of this study was to evaluate and compare the efficiency, reliability and accuracy of manual and semiautomatic segmentation techniques to segment tongue images. This work is included in a vast framework (HERON II) that aims to improve an articulatory-based speech synthesizer (SAP-Windows), for European Portuguese (EP). Volumetric data from Magnetic resonance images (MRI) were used to extract tongue configurations from several speakers uttering different EP sounds, or the same sound produced in different contexts or syllabic positions, in a speech production study. Segmentations were performed manually and using a semi-automatic approach implemented in ITK-SNAP (Region Competition Snakes). Results from similarity metrics (Jaccard coefficient and voxelwise comparison) revealed that the semi-automatic (SA) method presents good agreement with the manual segmentation method (Jaccard=0.9002 and 10.495 voxel error). Furthermore, the semi-automatic method is more reproducible (Jaccard=0.9382 and 6.388 voxel error) than manual segmentation method (Jaccard= 0.9170 and 8.662 voxel error). The semi-automatic approach provides an efficient and reliable method to segment tongue images providing 3D visualizations that allows description and comparison of tongue configurations during the production of different sounds. This information is of great relevance in speech production field contributing to a better understanding of speech production mechanisms. © 2011 IADIS.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cloud computing enhanced service development architecture for the living usability lab",
        "doc_scopus_id": "80054071972",
        "doc_doi": "10.1007/978-3-642-24352-3_31",
        "doc_eid": "2-s2.0-80054071972",
        "doc_date": "2011-10-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Complex problems",
            "Data stream",
            "Life expectancies",
            "Living lab",
            "Local processing",
            "Multiple data streams",
            "Research fields",
            "Service development",
            "Telerehabilitation"
        ],
        "doc_abstract": "As life expectancy increases, so does the number of ambient assisted living (AAL) initiatives. These IT initiatives often traverse several research fields; from embed devices to multiple data streams analysis. Advanced processing and reasoning of such data streams poses a complex problem usually solved using local processing resources. This paper addresses this problem from a cloud computing perspective. © 2011 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "AdaptO - Adaptive multimodal output",
        "doc_scopus_id": "80052477307",
        "doc_doi": null,
        "doc_eid": "2-s2.0-80052477307",
        "doc_date": "2011-09-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Context-awareness",
            "Distributed systems",
            "Ehealth",
            "Multimodal output",
            "Smart environment",
            "Universal access",
            "User"
        ],
        "doc_abstract": "Currently, most multimodal output mechanisms use a very centralized architecture in which the various output modalities are completely devoid of any autonomy. Our proposal, AdaptO, uses an alternative approach, proving output modalities with the capacity to make decisions, thus collaborating with the fission output mechanism towards a more effective, modular, extensible and decentralized solution. In addition, our aim is to provide the mechanisms for a highly adaptable and intelligent multimodal output system, able to adapt itself to changing environment conditions (light, noise, distance, etc.) and to its users needs, limitations and personal choices.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new living lab for usability evaluation of ICT and next generation networks for elderly@home",
        "doc_scopus_id": "79960543375",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960543375",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Conceptual model",
            "General population",
            "Living lab",
            "Next generation network",
            "Universal Design",
            "Usability evaluation",
            "Usability testing",
            "Work in progress"
        ],
        "doc_abstract": "Living Usability Lab for Next Generation Networks (www.livinglab.pt) is a Portuguese industry-academia collaborative R&D project, active in the field of live usability testing, focusing on the development of technologies and services to support healthy, productive and active citizens. The project adopts the principles of universal design and natural user interfaces (speech, gesture) making use of the benefits of next generation networks and distributed computing. Therefore, it will have impact on the general population, including the elderly and citizens with permanent or situational special needs. This paper presents project motivations, conceptual model, architecture and work in progress.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Output matters! Adaptable multimodal output for new telerehabilitation services for the elderly",
        "doc_scopus_id": "79960505802",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960505802",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Intelligent adaptation",
            "Multimodal output",
            "New services",
            "Older adults",
            "Prototype implementations",
            "Telehealth",
            "Telerehabilitation"
        ],
        "doc_abstract": "Technologies capable of being used by the older adults, with their specificities, to provide new services in the areas of telehealth and Ambient Assisted Living are needed. In this paper, we describe a new service, and its first prototype implementation, in the area of elderly health support at home. Special care was taken to improve the usability, adaptability to user and context and inclusiveness capabilities of the output. The basis for intelligent adaptation of the multimodal output - called AdaptO - is proposed and first versions of the needed services and agents were created.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A mobile robotic platform for elderly care",
        "doc_scopus_id": "79960496492",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960496492",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Elderly care",
            "Hardware and software",
            "Independent living",
            "Mobile robotic",
            "Next generation network",
            "Robotic platforms",
            "Robotic soccer"
        ],
        "doc_abstract": "This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The international classification of functioning, disability of health as a conceptual framework for the design, development and evaluation of AAL services for older adults",
        "doc_scopus_id": "79960476868",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960476868",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Comprehensive model",
            "Conceptual frameworks",
            "Holistic approach",
            "Older adults"
        ],
        "doc_abstract": "The paper presents the International Classification of Functioning, Disability of Health (ICF) as a comprehensive model for a holistic approach for the design, development and evaluation of Ambient Assisted Living (AAL) services for older adults. ICF can be used to systemize the information that influence individual's performance and to characterize users, theirs contexts, activities and participation. Furthermore, ICF can be used to structure a semantic characterization of AAL services and as a basis to develop methodological instruments for the services evaluation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An MRI study of consonantal coarticulation resistance in Portuguese",
        "doc_scopus_id": "84926296637",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84926296637",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Co-articulation",
            "MR images",
            "Qualitative evaluations",
            "Quantitative evaluation",
            "Quantitative result",
            "Semi-automatic segmentation",
            "Similarity metrics"
        ],
        "doc_abstract": "This study aimed to evaluate the effect of vocalic context, particularly at tongue level, on the articulation of European Portuguese (EP) consonants. Magnetic Resonance (MR) images were acquired for three speakers (two male and one female) during the production of consonants in a symmetric VCV context, with the cardinal vowels [i,a,u], Midsagittal contours of the tongue were extracted from MR images (by using a semi-automatic segmentation technique) and superimposed in order to allow a qualitative evaluation. Furthermore, a quantitative evaluation using a similarity metric (Pratt Index) was also conducted. Qualitative and quantitative results showed that stop consonants and nasals were especially sensitive to the vocalic context. Also, labial consonants were clearly more influenced by surrounding vowels than alveolars and dorsals. In comparison with all the consonantal segments analyzed, the fricatives [3 ∫] and the lateral [γ] were minimally affected by vowel context.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "3D MRI and semi-automatic segmentation techniques applied to the study of European Portuguese lateral sounds",
        "doc_scopus_id": "84910077128",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84910077128",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Common property",
            "Context effects",
            "Lateral compression",
            "MR images",
            "Reliable methods",
            "Semi-automatic segmentation",
            "Semi-automatics",
            "Vocal-tracts"
        ],
        "doc_abstract": "This study used 3D MR images and explored semi-automatic segmentation techniques to investigate the articulatory characteristics of European Portuguese (EP) laterals. Syllabic position and vowel context effects were also evaluated. Seven speakers of EP were scanned while uttering the consonants /I, U in the context of the cardinal vowels/i, a, u/. The semi-automatic approach provided an efficient and reliable method to segment tongue and vocal tract images. The analysis of MR images revealed several common properties for both /l/ and /U: lateral compression of the tongue and a convex tongue body, which enables the creation of lateral channels. However, /U exhibited greater lateral compression, when compared to /I/, and also larger lateral channels. The EP laterals were also distinguished from each other by means of the extension of constriction. For most of the speakers, /// exhibited a similar tongue body configuration at the word edges, with small pharyngeal/velar areas due to tongue-root retraction and/or raising the tongue dorsum. Vocalic effects were more evident in the alveolar lateral than in /IV. c.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Voice quality of European Portuguese emotional speech",
        "doc_scopus_id": "78650279848",
        "doc_doi": "10.1007/978-3-642-12320-7_19",
        "doc_eid": "2-s2.0-78650279848",
        "doc_date": "2010-12-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Acoustic analysis",
            "Emotional speech",
            "Fundamental frequencies",
            "Harmonic noise",
            "Reduced size",
            "Voice quality"
        ],
        "doc_abstract": "In this paper we investigate parameters related to voice quality in European Portuguese (EP) emotional speech. Our main objectives were to obtain, to our knowledge for the first time, values for the parameters commonly contemplated in acoustic analyses of emotional speech and investigate if there is any difference for EP relative to the results obtained for other languages. A small corpus contemplating five emotions (joy, sadness, despair, fear, cold anger) and neutral speech produced by a professional actor was used. Parameters investigated include fundamental frequency, jitter, shimmer and Harmonic Noise Ratio. In general, results were in accordance with the consulted literature regarding F0 and HNR. For jitter and shimmer our results were, in certain aspects, similar to the ones reported in a study of emotional speech for Spanish, another Latin language. From our analyses, and taking into consideration the reduced size of our corpus and the use of an actor as informant, no clear EP characteristic emerged, except for a possible, needing confirmation, difference regarding joy, with values similar to neutral speech. © Springer-Verlag Berlin Heidelberg 2010.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic extraction and representation of geographic entities in egovernment",
        "doc_scopus_id": "77957825443",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77957825443",
        "doc_date": "2010-10-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Automatic extraction",
            "Auxiliary data",
            "e-Government",
            "E-Government applications",
            "Geo coding",
            "Geographic information",
            "Google maps",
            "Human intervention",
            "Human language technologies",
            "Named entities",
            "Natural languages",
            "System use",
            "Textual documents"
        ],
        "doc_abstract": "in this paper we present a system that automatically extracts and geocodes named entities from unstructured, natural language textual documents. The system uses the Geo-Net-PT ontology and Google maps as auxiliary data sources. This type of system is particularly useful to automate the geocoding of existing information in e-government applications, which usually requires human intervention. Within the paper we introduce the relevant human language technologies, describe the system that was developed, present and discuss the preliminary results and draw the relevant conclusions and future work.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dynamic language modeling for European Portuguese",
        "doc_scopus_id": "77950867864",
        "doc_doi": "10.1016/j.csl.2010.02.003",
        "doc_eid": "2-s2.0-77950867864",
        "doc_date": "2010-10-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Adaptation framework",
            "Automatic speech recognition",
            "Baseline systems",
            "Broadcast news",
            "Broadcast news transcription",
            "Data sets",
            "Dynamic languages",
            "Language model",
            "Language modeling",
            "Large sizes",
            "Multi-pass",
            "Optimal vocabulary",
            "Out-of-vocabulary words",
            "Part Of Speech",
            "Portuguese languages",
            "Relative reduction",
            "Relevant documents",
            "Training corpus",
            "TV broadcast",
            "Vocabulary size",
            "Written texts"
        ],
        "doc_abstract": "This paper reports on the work done on vocabulary and language model daily adaptation for a European Portuguese broadcast news transcription system. The proposed adaptation framework takes into consideration European Portuguese language characteristics, such as its high level of inflection and complex verbal system. A multi-pass speech recognition framework using contemporary written texts available daily on the Web is proposed. It uses morpho-syntactic knowledge (part-of-speech information) about an in-domain training corpus for daily selection of an optimal vocabulary. Using an information retrieval engine and the ASR hypotheses as query material, relevant documents are extracted from a dynamic and large-size dataset to generate a story-based language model. When applied to a daily and live closed-captioning system of live TV broadcasts, it was shown to be effective, with a relative reduction of out-of-vocabulary word rate (69%) and WER (12.0%) when compared to the results obtained by the baseline system with the same vocabulary size. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 272453 291210 291718 291723 291743 291782 291874 31 Computer Speech & Language COMPUTERSPEECHLANGUAGE 2010-02-19 2010-02-19 2010-10-31T23:46:54 S0885-2308(10)00004-5 S0885230810000045 10.1016/j.csl.2010.02.003 S300 S300.1 FULL-TEXT 2015-05-15T02:19:55.925493-04:00 0 0 20101001 20101031 2010 2010-02-19T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0885-2308 08852308 24 24 4 4 Volume 24, Issue 4 12 750 773 750 773 201010 October 2010 2010-10-01 2010-10-31 2010 article fla Copyright © 2010 Elsevier Ltd. All rights reserved. DYNAMICLANGUAGEMODELINGFOREUROPEANPORTUGUESE MARTINS C 1 Introduction 1.1 Linguistic properties of European Portuguese 1.2 Related work 1.3 Outline of this paper 2 Baseline system and resources 2.1 Broadcast news transcription system 2.2 Evaluation datasets 3 Dynamic language modeling for European Portuguese 3.1 OOV word analysis for European Portuguese 3.2 Vocabulary selection 3.2.1 Computation of normalized counts 3.2.2 Estimation of a word weighting factor 3.2.3 Generation of an ordered word list W 3.2.4 Definition of the POS-classes to use 3.2.5 Estimation of POS distribution using an in-domain corpus 3.2.6 Selection of |V| words from the word list W 3.3 Multi-pass language model adaptation framework 4 Evaluation results 4.1 OOV word rate results 4.2 WER results 4.3 Computational costs 5 Discussion and future work Acknowledgments References BELLEGARDA 2004 J BLEI 2003 A FEDERICO 2004 417 435 M GAUVAIN 2002 89 108 J KIRCHHOFF 2006 589 608 K NEY 1997 174 207 H CORPUSBASEDMETHODSINLANGUAGESPEECHPROCESSING STATISTICALLANGUAGEMODELINGUSINGLEAVINGONEOUT PALMER 2005 107 128 D RIBEIRO 2003 143 150 R COMPUTATIONALPROCESSINGPORTUGUESELANGUAGELECTURENOTESINCOMPUTERSCIENCESUBSERIESLNAI USINGMORPHOSSYNTACTICINFORMATIONINTTSSYSTEMSCOMPARINGSTRATEGIESFOREUROPEANPORTUGUESE RIBEIRO 2004 R MORPHOSYNTACTICTAGGINGACASESTUDYLINGUISTICRESOURCESREUSE XU 2007 105 152 P MARTINSX2010X750 MARTINSX2010X750X773 MARTINSX2010X750XC MARTINSX2010X750X773XC item S0885-2308(10)00004-5 S0885230810000045 10.1016/j.csl.2010.02.003 272453 2010-12-06T20:41:31.823019-05:00 2010-10-01 2010-10-31 true 863709 MAIN 24 56348 849 656 IMAGE-WEB-PDF 1 si2 1398 17 306 si1 968 44 197 si3 506 15 77 gr10 36388 271 492 gr10 4542 120 219 gr11 21265 274 510 gr11 3784 117 219 gr15 22386 274 508 gr15 4302 118 219 gr2 21135 193 489 gr2 4188 86 219 gr3 17680 263 460 gr3 3498 125 219 gr4 14464 262 462 gr4 4277 124 219 gr5 21241 269 503 gr5 4769 117 219 gr6 23765 276 463 gr6 6335 131 219 gr7 20406 272 507 gr7 4512 117 219 gr8 46280 326 622 gr8 6971 115 219 gr9 38027 300 534 gr9 8802 123 219 gr1 37884 274 483 gr1 5292 124 219 gr12 33061 290 520 gr12 4347 122 219 gr13 31155 284 511 gr13 3979 122 219 gr14 30920 278 508 gr14 4145 120 219 gr16 31386 280 489 gr16 4194 126 219 gr17 29090 277 480 gr17 4236 126 219 gr18 37089 594 517 gr18 3001 163 142 YCSLA 443 S0885-2308(10)00004-5 10.1016/j.csl.2010.02.003 Elsevier Ltd Fig. 1 Vocabulary growth rates for broadcast news data in European Portuguese (ALERT-SR dataset), Italian (IBNCII dataset) and English (HUB4 dataset). Fig. 2 AUDIMUS.media: a hybrid HMM/MLP recognition system. Extracted from Meinedo (2008). Fig. 3 OOV word rates (in%) measured on the seven news shows of the BN.RTP-2004 dataset with the baseline vocabulary of 57K words. Fig. 4 Average distribution (in%) of words by POS-classes measured for the seven news shows of the BN.RTP-2004 dataset. Fig. 5 Average distribution (in%) by POS-classes of the words recognized incorrectly in the seven news shows of the BN.RTP-2004 dataset. Recognition results obtained with the baseline system (vocabulary size of 57K words). Fig. 6 OOV word rates reduction (in%) by POS-classes measured for the seven news shows of the BN.RTP-2004 dataset with the baseline vocabulary of 57K words augmented with new words found in the written text news of the same day of the news show tested. Fig. 7 Distribution (in%) of word types by POS-classes measured on two corpora with the same size: an in-domain corpus (broadcast news texts) and an out-of-domain corpus (written news texts). Fig. 8 Proposed multi-pass language model adaptation framework: first-pass (live version). Fig. 9 Proposed multi-pass language model adaptation framework: second-pass (offline version). Fig. 10 Results of evaluation, regarding relevance, for the 1000 documents resulting from IR queries for three randomly selected topics of the BN.RTP-2007 evaluation set. Fig. 11 Average OOV word rate (in%) measured for the two news shows of the BN.RTP-2007 dataset when applying the baseline system and the proposed multi-pass LM adaptation frameworks (with a vocabulary size of 57K words). Fig. 12 Average OOV word rates (in%) measured for the two evaluation datasets (BN.RTP-2004 and BN.RTP-2007) when applying the baseline system and the proposed multi-pass adaptation frameworks with four different vocabulary sizes (30K, 57K, 100K and 150K words). Fig. 13 Proportion of verbs in OOVs measured for the two evaluation datasets (BN.RTP-2004 and BN.RTP-2007) when applying the baseline system and the proposed multi-pass adaptation frameworks with four different vocabulary sizes (30K, 57K, 100K and 150K words). Fig. 14 Comparison of average OOV word rates (in%) measured for the two news shows of the BN.RTP-2007 dataset when applying the baseline system and the proposed multi-pass adaptation frameworks with four different vocabulary sizes (30K, 57K, 100K and 150K words) and extending the training corpora (NP.train and BN.train) used for vocabulary and LM estimation. Fig. 15 Average WER measured for the two news shows of the BN.RTP-2007 dataset when applying the baseline system and the proposed multi-pass adaptation frameworks (with a vocabulary size of 57K words). Fig. 16 Average WER measured for the two evaluation datasets (BN.RTP-2004 and BN.RTP-2007) when applying the baseline system and the proposed multi-pass adaptation frameworks with four different vocabulary sizes (30K, 57K, 100K and 150K words). Fig. 17 Comparison of average WER measured for the two news shows of the BN.RTP-2007 dataset when applying the baseline system and the proposed multi-pass adaptation frameworks with four different vocabulary sizes (30K, 57K, 100K and 150K words) and extending the training corpora (NP.train and BN.train) used for vocabulary and LM estimation. Fig. 18 CPU time (in seconds) and memory allocated to the ASR decoding process and measured for the two news shows of the BN.RTP-2007 dataset when applying the baseline system and the proposed 1-PASS-POS adaptation framework with four different vocabulary sizes and extending the training corpora. Table 1 BN.RTP-2004 dataset: text statistics. 8th 9th 10th 11th 12th 13th 14th #word tokens 8.7K 1.9K 8.4K 8.3K 8.8K 7.0K 9.4K #word types 2.4K 0.7K 2.4K 2.0K 2.1K 1.8K 2.1K Table 2 BN.RTP-2007 dataset: text statistics. May 24th May 31st #tokens 8.1K 7.9K #types 2.3K 2.3K Table 3 Distribution (in%) of OOV words by POS-classes measured on the seven news shows of the BN.RTP-2004 dataset with the baseline vocabulary of 57K words. The last column shows the average values for this dataset. Class 8th 9th 10th 11th 12th 13th 14th Week Names 27.0 15.4 18.9 20.4 28.2 42.9 34.7 27.9 Adjectives 10.1 7.7 18.2 16.9 16.4 8.9 9.8 13.2 Verbs 61.6 69.2 62.8 58.0 53.6 47.2 53.3 56.7 Others 1.3 7.7 0.1 4.7 1.8 1.0 2.2 2.2 Table 4 Distribution (in%) of new words by grammatical category and the percentage of them correctly recognized by the 2-PASS-POS-IR approach (a vocabulary size of 57K words). POS % of Occurrences % Correctly recognized Names 30.8 80.2 Adjectives 1.6 66.7 Verbs 66.5 58.9 Others 1.1 50.0 ☆ Parts of this study were presented in conferences (Martins et al., 2007a,b, 2008). Dynamic language modeling for European Portuguese Ciro Martins a b António Teixeira a ⁎ João Neto b a Department Electronics, Telecommunications & Informatics/IEETA – Aveiro University, Aveiro, Portugal b L2F – Spoken Language Systems Lab – INESC-ID/IST, Lisbon, Portugal ⁎ Corresponding author. Address: Departamento de Electrónica Telec. & Informática/IEETA, Universidade de Aveiro, Campus Universitário de Santiago, 3810 193 AVEIRO, Portugal. Tel.: +351 234370500; fax: +351 234370545. This paper reports on the work done on vocabulary and language model daily adaptation for a European Portuguese broadcast news transcription system. The proposed adaptation framework takes into consideration European Portuguese language characteristics, such as its high level of inflection and complex verbal system. A multi-pass speech recognition framework using contemporary written texts available daily on the Web is proposed. It uses morpho-syntactic knowledge (part-of-speech information) about an in-domain training corpus for daily selection of an optimal vocabulary. Using an information retrieval engine and the ASR hypotheses as query material, relevant documents are extracted from a dynamic and large-size dataset to generate a story-based language model. When applied to a daily and live closed-captioning system of live TV broadcasts, it was shown to be effective, with a relative reduction of out-of-vocabulary word rate (69%) and WER (12.0%) when compared to the results obtained by the baseline system with the same vocabulary size. Keywords Vocabulary selection Language modeling Information retrieval techniques Automatic speech recognition (ASR) Broadcast news transcription 1 Introduction Up-to-date vocabulary and, consequently, language modeling, is a critical aspect for maintaining the level of performance for a speech recognizer over time for applications such as transcription of broadcast news (BN) and broadcast conversations (BC). In particular, the subtitling of broadcast news programs is starting to become a very interesting application due to the technological advances in ASR and associated technologies. However, for this challenging task, the ability to correctly address new words appearing in a daily basis is an important factor to take into account for performance. Due to the changes in common topics over time, which characterizes this kind of task, the appearance of new stories leads to high out-of-vocabulary (OOV) word rates and consequently to a degradation of recognition performance. Moreover, for languages with complex morphology, i.e., productive word formation processes (inflection, derivation and compounding), which create a large number of possible word forms, the OOV word rates tend to be even higher (Geutner et al., 1998; Kirchhoff et al., 2006). In this paper, we present the work done in terms of dynamic language modeling for European Portuguese, which is a morphologically complex language whose inflectional structure represents an additional problem to overcome, mainly the morpho-syntactic class of verbs (Martins, 1998; Martins et al., 2006). We describe a daily and unsupervised LM adaptation framework, which was applied to a live closed-captioning system of live European Portuguese TV broadcasts (Meinedo, 2008; Neto et al., 2008). This subtitling system runs daily at RTP (the Portuguese public broadcast company), successfully processing the 8 o’clock evening news and allowing subtitling to be created live and in real-time for the TV broadcaster. Based on texts that are available daily on the Web, we proposed a morpho-syntactic algorithm to dynamically select the target vocabulary by trading off between the OOV word rate and vocabulary size (Martins et al., 2007a). Using an information retrieval (IR) engine (Strohman et al., 2005) and the automatic speech recognition (ASR) hypotheses as query material, relevant documents were extracted from a dynamic and large-size dataset to generate a story-based language model (LM) for the multi-pass speech recognition framework. Because the hypotheses are quite small and may contain recognition errors, a relevance feedback method for automatic query expansion was used (Lavrenko and Croft, 2001). 1.1 Linguistic properties of European Portuguese The European Portuguese language shares its characteristics with many other inflectional languages, especially those of the Romance family. European Portuguese words often exhibit clearer morphological patterns in comparison to English words. A morpheme is the smallest part of a word with its own meaning. In order to form different morphological patterns (derivations, conjugations, gender, number inflections, etc.), two parts of a word are distinguished: stem and ending. The stem is the part of the inflected word that carries its meaning, while the ending specifically denotes categories of person, gender and number, or the final part of a word, regardless of its morphemic structure. The following example of two semantically equal sentences (“O meu amigo é professor” and “A minha amiga é professora”) differing in subject gender (masculine and feminine, respectively) but identical in English (“My friend is a teacher”), outlines its linguistic characteristics. European Portuguese language distinguishes between three types of gender: masculine, feminine and neuter, while English only has one form. All nouns, adjectives and verbs in European Portuguese have a gender. They present far more variant forms than their English counterparts. Words have augmentative, diminutive and superlative forms (e.g. “small house”=“casinha”, where – inha is the suffix that indicates a diminutive). Moreover, European Portuguese is a very rich language in terms of verbal forms. While the regular verbs in English have only four variations (e.g. talk, talks, talked, talking), the European Portuguese regular verbs have over 50 different forms, with each one having its specific suffix (Orengo and Huyck, 2001). The verbs can vary according to gender, person, number, tense and mood. Three types for the grammatical category of person (1st, 2nd and 3rd person) reflect the relationship between communication participants. There are five tenses: present, past, past perfect, past imperfect, past pluperfect and future. Another grammatical category, mood, denotes the feeling of the speaker towards the act, which is defined by the verb. There are eight different types of mood in European Portuguese: indicative, subjunctive, imperative, conditional, infinitive, inflected infinitive, participle and gerund. The rich morphology of European Portuguese causes a large number of possible word types, which in turn decreases the overall quality of the recognition process (high OOV rates). Fig. 1 shows a comparison of the vocabulary growth rates (the increase in the number of word types versus number of word tokens for a given text) for European Portuguese, English and, as an example of a language of the same family as Portuguese, Italian. Information for European Portuguese was calculated on the European Portuguese Broadcast News Speech corpus (ALERT-SR) used in this work. For English the 1997 English Broadcast News Speech corpus (HUB4) available from the Linguistic Data Consortium (LDC) was used. Italian information, using the IBNC II, was extracted from Federico et al. (2000). As one can observe, the vocabulary growth rate of European Portuguese and Italian exceeds that of English significantly. For a corpus size of 600K word tokens, the HUB4 subset has a vocabulary size of 20,099 words (20K), while the vocabulary size for the ALERT-SR corpus is 26,704K (26K), i.e., about 32.9% more. This means European Portuguese, as well other languages like Italian, French, German, Spanish, etc., demand a larger vocabulary to obtain the same degree of coverage as English. In Lamel et al. (2004) the authors describe a work on broadcast and conversational speech transcription for multiple languages, where European Portuguese presents one of the smallest lexical coverages for a vocabulary size of 65K words (99.4% of coverage for English, 98.8% for French, 94.3% for Spanish and 94.0% for Portuguese). 1.2 Related work Statistical language models have been successfully applied to many state-of-the-art ASR systems, with n-gram models being the dominant technology in language modeling. Usually, large corpora are used to estimate the LM parameters, and different smoothing techniques are applied to accurately estimate the LM probabilities (Xu and Jelinek, 2007). However, the collection of suitable training corpora is an expensive, time-consuming and sometimes unfeasible task. Therefore, the idea of language model adaptation is to use a small amount of domain specific data (in-domain data) to adjust the LM and reduce the impact of linguistic differences between the training and testing data over time. For that propose, several techniques have been developed by the research community (Bellegarda, 2004). In terms of vocabulary selection, one approach is to choose the words in such a way as to reduce the OOV word rate by as much as possible – a strategy usually called vocabulary optimization. This optimization could either involve increasing the vocabulary size of the ASR component or selecting the words most representative for the target domain/task. The most common approaches are typically based on word frequency, including words from each training corpus that exceed some empirically defined threshold, which mainly depends on the relevance of the corpus to the target task (Gauvain et al., 2002). In Geutner et al. (1998) an approach targeted at reducing the OOV word rates for heavily inflected languages is suggested. Their work uses a multi-pass recognition strategy to generate morphological variations of the list of all words in the lattice, thus dynamically adapting the recognition vocabulary for a second recognition pass. By applying this so called adaptation algorithm (HDLA-Hypothesis Driven Lexical Adaptation) both on Serbo-Croatian and German news data, OOV word rates were reduced by 35–45%. In Venkataraman and Wang (2003) three principled methods for selecting a single vocabulary from many corpora were evaluated, and they concluded that the maximum-likelihood-based approach is a robust way to select the vocabulary of a domain, especially when a reasonable amount of in-domain texts are available. A similar framework to the one presented in Geutner et al. (1998) was proposed in Palmer and Ostendorf (2005), but it focused on names rather than morphological word differences. They proposed an approach for generating targeted name lists for candidate OOV words, which can be used in a second-pass of recognition. The approach involves offline generation of a large list of names and online pruning of that list by using phonetic distance to rank the items in a vocabulary list according to their similarity to the hypothesized word. Their reported experiments showed that OOV word coverage could be improved by nearly a factor or two with only a 10% increase in the vocabulary size. In Allauzen and Gauvain (2005b) a vectorial algorithm for vocabulary adaptation was used, which combines word frequency vectors estimated on adaptation corpora to directly maximize lexical coverage on a development corpus, thus eliminating the need for human intervention during the vocabulary selection process. The experiments reported by the authors showed a significant reduction of the OOV word rate compared with the baseline vocabulary: a relative decrease of 61% in French and 56% in English. The work presented in Oger et al. (2008) suggests that the local context of the OOV words contains relevant information about them. Using that information and the Web, different methods were proposed to build locally-augmented lexicons to be used in a final local decoding pass. This technique allowed the recovery of 7.6% of the significant OOV words, and the accuracy of the system was improved. For broadcast news and conversational speech applications there have been various works using data from the Web as an additional source of training data for unsupervised language modeling adaptation over time, also referred to as dynamic LM adaptation. In Federico and Bertoldi (2004), a rolling language model with an updated vocabulary was implemented for an Italian broadcast news transcription system using a single-step adaptation framework. An open vocabulary language model was introduced by assuming a special OOV word class. Hence, the addition of new words to the vocabulary was done by extending the OOV word class and re-estimating its unigram distribution from the adaptation data. The baseline vocabulary of 62K words was extended by adding 60K new words selected from the contemporary written news and the baseline LM interpolated with a new language model estimated from the adaptation data. This approach allowed an average relative reduction of 58% in terms of OOV word rate and 3.4% in terms of word error rate (WER). Another straightforward approach for unsupervised adaptation is to use different information retrieval (IR) techniques for dynamic adaptation of vocabulary and/or LM to the topics presented in a BN show using relevant documents obtained from a large general corpus or from the Web. These multi-pass speech recognition approaches use the ASR hypotheses as queries to an IR system in order to select additional on-topic adaptation data. In Bigi et al. (2004) an approach of this type using the Kullback–Leibler symmetric distance to retrieve documents was implemented to select a dynamic vocabulary instead of a static one, obtaining an OOV word rate reduction of about 28% with the same vocabulary size as the baseline vocabulary. Moreover, a new topic LM was trained on the retrieved data and interpolated with the baseline LM, allowing for a relative improvement of 1.8% in terms of WER. A similar approach was implemented in Chen et al. (2004). In this case the vocabulary remained static, with only the language model being updated at each automatically detected story of the BN show, which meant estimating multiple story-based language models for each BN show. A relative WER reduction of 4.7% was obtained in English and a 5.6% relative character error rate reduction in Mandarin. In Boulianne et al. (2006) a quite different approach was implemented for closed-captioning a live TV broadcast in French, which used texts from a number of websites, newswire feeds and broadcaster’s internal archives to adapt its eight topic-specific vocabularies, re-estimating the corresponding language models according to the minimum discrimination information (MDI) method. In Tam and Schultz (2006) the same MDI method was used in a similar LM adaptation approach, but it used the latent Dirichlet allocation (LDA) model (Blei and Jordan, 2003) to estimate the marginal unigram distribution based on the ASR hypotheses. Results computed on a Mandarin BN test set showed a relative character error rate reduction of 2% when compared to baseline LM. Recently, various language model adaptation strategies, including unsupervised LM adaptation from ASR hypotheses and ways to integrate supervised maximum a posteriori (MAP) and marginal adaptation within an unsupervised adaptation framework, were investigated (Wang and Stolcke, 2007). By combining these adaptation approaches on a multi-pass ASR system, a relative gain of 1.3% on the final recognition error rate in the BC genre was achieved. More recently, in Lecorvé et al. (2008) and Lecorvé et al. (2009) some improvements were obtained by using IR methods for unsupervised LM adaptation. The work presented here can be seen as an advancement and adaptation of previous works to deal with the specific characteristics of the European Portuguese language in the following terms: the lexicon adaptation used morphological knowledge related to the language, the language model adaptation approach used both morphological knowledge and information retrieval analysis, both language model and lexicon were adapted during the first and second recognition pass, and the adaptation was applied on a daily frequency, which is very important for real applications like the live closed-caption broadcast news transcription systems. 1.3 Outline of this paper The remainder of this paper is structured as follows: Section 2 provides a brief description of the baseline European Portuguese broadcast news transcription system and datasets used to evaluate the proposed adaptation framework. Section 3 describes the new vocabulary selection and language model adaptation procedures, with experimental results reported in Section 4. Finally, in Section 5 those results are discussed, drawing some conclusions and describing future research trends. 2 Baseline system and resources Through participation in several projects, we acquired over the years vast experience in developing ASR systems for the English language. Since then, we have been using that experience to develop ASR systems for the European Portuguese language. Currently, our ASR core system, AUDIMUS, has been ported successfully to several different tasks like dictation, telephone, portable devices, and broadcast news. This section gives a brief overview of the BN transcription system we used, which we improved with the work reported here. Further details about the overall system can be found in Neto et al. (2008). Finally, it also describes the resources used for training and evaluation proposes. 2.1 Broadcast news transcription system All experiments reported in this paper were done with the AUDIMUS.media ASR system (Meinedo et al., 2003). This system is part of a closed-captioning system of live TV broadcasts in European Portuguese that produces online captions daily for the main news show of the main Portuguese broadcaster – RTP (Neto et al., 2008). It features a hybrid HMM/MLP system using three processing streams, each of them associated with a different feature extraction process and an MLP that is specifically trained, where the MLPs are used to estimate the context independent posterior phone probabilities given the acoustic data at each frame (Fig. 2 ). The phone probabilities generated at the output of the MLP classifiers are combined using an appropriate algorithm (Meinedo and Neto, 2000). All MLPs use the same phone set constituted by 38 phones for the Portuguese language plus silence. The training and development of this system was based on the European Portuguese ALERT BN database (Neto et al., 2003). The acoustic models used on this work were trained with over 47h of manually transcribed speech, plus 378h of automatically transcribed speech (used for unsupervised training of the acoustic model). The decoder of this baseline system is based on a weighted finite-state transducer (WFST) approach to large vocabulary speech recognition (Caseiro, 2003). In this approach, the decoder search space is a large WFST that maps observation distributions to words. The recognition vocabulary of the baseline system was selected from two training corpora: a 604M word corpus of newspapers texts collected from the Web since 1991 until the end of 2003 (out-of-domain dataset – “NP.train”), and a 531K word corpus resulting from the manual transcription process of the 47-h training set (in-domain dataset – “BN.train”). This vocabulary was created using words selected from the written news set according to their weighted class frequencies of occurrence. All new different words present in the acoustic training data were added to the vocabulary, giving a total of 57,564 words (57K). This word list was then phonetically transcribed by a rule grapheme-to-phone system (Caseiro et al., 2002), generating an initial set of pronunciations. This automatically generated lexicon was then hand revised by a specialized linguist, generating a multi-pronunciation lexicon with 66,132 different pronunciations. The hand-held revisions and grammatical rules added to the generation of our phonetic pronunciations dictionary significantly contributed to the results obtained with the adaptation framework proposed here. The regular update of the pronunciation dictionary is especially useful in the case of new words such as proper names and foreign names. In fact, even if we are able to generate a better lexicon and an LM containing new names, if these have poor phonetic pronunciations, they will not be recognized in most cases. The baseline LM (Martins et al., 2005) combines a backoff 4-g LM trained on the 604M word corpus and a backoff 3-g LM estimated on the 531K word corpus. The two models were combined by means of linear interpolation, generating a mixed model. This baseline system is state-of-the-art in terms of BN transcription systems for the European Portuguese language. 2.2 Evaluation datasets To evaluate the proposed adaptation frameworks we selected two BN datasets consisting of BN shows collected from the 8 o’clock pm (prime time) news from the main public Portuguese channel, RTP. The “BN.RTP-2004” dataset was drawn from the week starting on March 8th and ending on March 14th of 2004. Due to the unexpected and awful events occurring on March 11th of 2004 in Madrid (the Madrid train bombing), we would expect to cover a special situation of rich content and topic change over time. The “BN.RTP-2004” dataset has a total duration of about 5h of speech and 52.5K word tokens. Later on, another evaluation dataset was collected, the “BN.RTP-2007”, consisting of two BN shows that were randomly selected. Those two BN shows have a total duration of about 2h of speech and 16K word tokens and were collected on May 24th and 31st of 2007. Tables 1 and 2 present more detailed statistics related to these two evaluation datasets (“BN.RTP-2004” and “BN.RTP-2007”, respectively). As one can observe, each BN show has an average size of about 8.3K tokens and only 2.1K different words (types). For day 9 of the “BN.RTP-2004” dataset, due to a technical problem, only half an hour of speech was collected. For that reason, this day has different statistics when considering the average values. 3 Dynamic language modeling for European Portuguese This section addresses the problem of dynamically adapting over time both the vocabulary and language model of our European Portuguese broadcast news transcription system, using additional adaptation texts extracted from the Web. First, we present an analysis of the OOV words for the European Portuguese language using the evaluation datasets described in Section 2, which lead us to propose the multi-pass speech recognition approach described in the following sub-sections. With respect to vocabulary selection, we describe the proposed algorithm and its integration in both single and multi-pass adaptation approaches. 3.1 OOV word analysis for European Portuguese A major part of building a language model is to select the vocabulary of the ASR component that will have maximal coverage for the expected task/domain. Thus, the appearance of OOV words during the recognition process is closely related to the way the system vocabulary is chosen. Thus, an important aspect to take into account for vocabulary design is to identify and rank the most relevant vocabulary words in order to improve the coverage rate of that vocabulary on unseen data. Fig. 3 shows the coverage statistics related to the BN.RTP-2004 evaluation dataset for a baseline vocabulary of 57K words. For this vocabulary the OOV word rate averages 1.20%. To figure out what words contribute to these OOV rates, and which adaptation procedures we should pursue in order to better address this problem specific to highly inflected languages such as European Portuguese, we derived various analyses at the OOV word level. First, we examined their classification into part-of-speech (POS) classes using a morpho-syntactic tagging system developed for the European Portuguese language (Ribeiro et al., 2003, 2004), presenting an overall success rate of 94.23% in identifying content words (proper and common names, verbs, adjectives and adverbs). In Table 3 we break down OOV words into three different categories using the morpho-syntactic tagging system: names (including proper names), adjectives and verbs. Other type of words, such as function words, are absent from the list shown in Table 3 because almost all those words are already in the 57K baseline vocabulary. We simply merged all of them together (“Others” category in Table 3). According to findings reported in the literature, OOV words are mostly names. In Hetherington (1995), Bazzi (2002), Allauzen and Gauvain (2005a) a strong correlation between names and OOV words is reported. A similar conclusion is reported in Palmer and Ostendorf (2005), with names accounting for 43.66% of the OOV word types. Hence, as a first idea, we would expect to observe a similar behavior for the BN.RTP-2004 dataset, i.e., a strong correlation between names and OOV words, especially for this specific week with new and infrequent words appearing (train station names, terrorist names, journalist names, etc.). However, as one can observe from Table 3, verbs make up for the largest portion of OOV words. In fact, although verbs represent only 17.6% (see Fig. 4 ) of the words in the BN.RTP-2004 dataset, they account for 56.7% of the OOV words. From the 56.7% of verb OOVs, only 13.2% are due to new lemmas not already included in the lemma set of the 57K baseline vocabulary, i.e., the major part of verb OOVs are due to new inflection forms of lemmas already present in the vocabulary. Moreover, in this dataset, verbs are also very frequently the source of recognition errors, representing the largest portion of wrongly recognized words: 26.3% (see Fig. 5 ). In a second analysis, and because our adaptation proposal was to take advantage of contemporary written news to dynamically adapt the system vocabulary, we examined the effect of augmenting the vocabulary with new words found in the same day of each tested BN show. Thus, taking into consideration these written text news collected for each day and the 57K word baseline vocabulary, an average of 5K new words were found in a daily basis, accounting for an upgraded vocabulary of 62K words (57K from the baseline vocabulary plus the average 5K new words). By expanding the baseline vocabulary with those extra 5K words one can observed an OOV word reduction ranging between 15.4% (for March 9th) and 36.4% (for March 12th), with an average value around 28.4%. There are two main reasons for those variations: on day 9 of the BN.RTP-2004 dataset, due to a technical problem, only 12min of speech was collected (in average one news show has about 45min). For that reason, this day has different statistics in terms of the average values. The news show of day 13 has a different structure compared to the other days because it is composed mainly of a street interview in Madrid-Spain (outside studio with a large amount of spontaneous speech). The graph in Fig. 6 gives us an overview of the kinds of words (POS-classes) we covered by adding the 5K extra new words. From that, we concluded that a significant OOV word reduction was obtained in the class of names. Some examples of OOV names recovered include important names related to the bombing (Alcalá, Atocha, El Pozo, Gregorio, Henares, Marañón, Rajoy, …). Remarkably, on the news show of March 13th a reduction of 72% was achieved in the class of names. Based on the above observations, we concluded that the strategy of using contemporary written text news to adapt the baseline vocabulary is useful, especially in covering new names that appear over time. However, even though verbs represent the largest portion of OOV words, the reduction for this class using contemporary written texts is not so significant. This is mainly due to the inflectional structure of verbs class for the European Portuguese language, which makes the use of verbs significantly different in written and spoken language. Moreover, the differences in terms of vocabulary growth and coverage for different domains and time periods makes it necessary to devise new vocabulary selection strategies that take into account those specific characteristics. 3.2 Vocabulary selection In our preliminary works we tried to use a large vocabulary of 213K words selected according to their frequency of occurrence, obtaining an OOV word rate reduction of 79.2%, i.e., from 1.20% to 0.25%. However, this approach does not solve the problem of new words and infrequent words related to some important events, which are critical and therefore need to be recognized accurately. Moreover, for applications such as the one we are addressing in our work, the processing time is a very important issue because we are doing online and real-time BN closed-caption transcription. Having larger vocabularies implies that the recognition process will require more time. Tables 2.1 and 2.2 in Section 2.2 show a maximum of only 2.4K word types occurring in a news show. Thus, defining a more rational approach to selecting the vocabulary other than by simple frequency of occurrence is needed. In Martins et al. (2006) we proposed a procedure for dealing with the OOV problem by dynamically increasing the baseline system vocabulary, reducing the impact of linguistic differences over time. Our approach to compensate and reduce the OOV word rate related to verbs was supported by the fact that almost all the OOV verb tokens were inflections of verbs whose lemmas were already among the lemma set (L) of the words found in contemporary written news. Thus, the baseline system vocabulary is automatically extended with all the words observed in the language model training texts whose lemmas belong to L. Applying this adaptation approach on the seven news shows of the BN.RTP-2004 dataset, the baseline system vocabulary of 57K was expanded by an average of 43K new words each day, generating a significant improvement in terms of OOV word rate, which was reduced on average by 70.8%, i.e., from 1.20% to 0.35%. However, this procedure assumes an a priori selected static list of words – the baseline vocabulary – and adds new words in a daily basis. In this way, the system vocabulary is always extended, resulting in a vocabulary with an average size of 100K words. Thus, in Martins et al. (2007a) we derived a new algorithm for selection that allows for the definition of the size of the target vocabulary, selecting it from scratch. Using the same morpho-syntactic analysis tool as before, we annotated both the in-domain and out-of-domain training corpora (BN.train and NP.train, respectively). In Fig. 7 , we summarize the POS statistics obtained for both corpora by breaking down words into four main classes: names (including proper and common names), verbs, adjectives and adverbs. Other types of words, such as the functional words, are absent from the list shown in Fig. 7 because they represent almost closed grammatical classes in the European Portuguese language. These statistics are related to word types, i.e., only unique occurrences of a word/class are counted. As one can see, there is a significant difference in POS distribution when comparing in-domain and out-of-domain corpora, especially in terms of names and verbs. For in-domain data we observe a significant increment (from 30.5% to 36.9%) in the relative percentage of verbs when compared with the out-of-domain data, with the percentage of names decreasing from 45% to 40.6%. Based on the observations above, we proposed a new approach for vocabulary selection that uses the part-of-speech word classification to compensate for word usage differences across the various training and adaptation corpora. This approach is based on the hypothesis that the similarities between different domains can be characterized in terms of style (represented by the POS sequences). In Iyer and Ostendorf (1997) these similarities have already been integrated to more effectively use out-of-domain data in sparse domains by introducing a modified representation of the standard word n-grams model using part-of-speech labels that compensate for word usage differences across domains. Thus, in this new approach, instead of simply adding new words to the fixed baseline system vocabulary, we use now the statistical information related to the distribution of POS word classes on the in-domain corpus to dynamically select words from the various training corpora available. Assume that we want to select a vocabulary V with |V| words from n training corpora Tj , with j =1, … , n. The proposed approach can be described as follows: 3.2.1 Computation of normalized counts Let ci , j be the counts from each one of the available training corpus Tj for the word wi . In our case, we used three training corpora (i.e., j =3): the written news text corpus, the broadcast news text corpus, and the written news texts collected from Internet on a daily basis. Due to the differences in the amount of available data for each training corpus, we started by normalizing the counts according to their respective corpus length, obtaining ηi , j as the normalized counts. The Witten–Bell discounting strategy was used to ensure non-zero frequency words in the normalization process. 3.2.2 Estimation of a word weighting factor From the normalized counts ηi , j we wanted to estimate some kind of weighting factor ηi for each word wi in order to select a vocabulary from the union of the vocabularies of T 1 through Tn that minimizes the OOV word rate for the in-domain task. In Venkataraman and Wang (2003) this weighting is obtained by means of linear interpolation of the different counts, with the mixture coefficients calculated in order to maximize the probability of the in-domain corpus. However, in our evaluation framework this technique performed slightly worst (in terms of OOV word rate) than simply applying the uniform distribution, maybe due to the relatively small size of our in-domain corpus. Hence, we simply assigned identical values to all the mixture coefficients, (3.1) η i = ∑ j = 1 n λ j η i , j with λ j = 1 n 3.2.3 Generation of an ordered word list W All the words wi were sorted in descending order according to the weighting factor ηi . 3.2.4 Definition of the POS-classes to use In our implementation we used the following set of POS-classes: (3.2) POSset = { names,verbs,adjectives,adverbs } All the remaining words (mainly functional words) were automatically added to the vocabulary. In fact, in the training corpus used in this work we obtained only 468 words whose POS-classes did not belong to POSset. 3.2.5 Estimation of POS distribution using an in-domain corpus Using an in-domain dataset the distribution of words by POS-classes, M(p) with p ∈ POSset , was computed through the maximum likelihood estimation (MLE). 3.2.6 Selection of |V| words from the word list W According to M(p), the number of words selected from each class p will be |V|× M(p). Hence, for each class p, the first |V|× M(p) words of W belonging to class p were selected and included in the target vocabulary V. However, because a word can belong to more than one class, the first run of this process can produce a vocabulary list with less than |V| words. In that case, the selection process is iterated until the target number of words is achieved. For the proposed language model adaptation framework we used this new POS-based algorithm for vocabulary selection. In addition, we combined the POS-based method with our previous work based on lemmatization and inflection generation (Martins et al., 2006) to integrate them into the second-pass of the language model adaptation procedure as described in the next sub-section. 3.3 Multi-pass language model adaptation framework As stated in Section 2.1, the baseline AUDIMUS.media ASR system is part of a closed-captioning system of live TV broadcasts, which is state-of-the-art in terms of broadcast news transcription systems for European Portuguese. However, the language modeling component of this baseline system uses a static vocabulary and language model, which is not able to cope with changes in vocabulary and linguistic content over time. To overcome that limitation we proposed and implemented an adaptation approach (Martins et al., 2007b), which creates from scratch both vocabulary and language model components on a daily basis using a multi-pass speech recognition process. The first-pass was used to produce online captions for the closed-captioning system of live TV broadcasts (see Fig. 8 ). A new vocabulary V 0 was selected for each day d by applying the POS-based algorithm described in Section 3.2 and using three corpora as training data: the newspaper texts corpus NP.train, the broadcast news transcriptions corpus BN.train and the contemporary texts daily extracted from the Web (as adaptation data). The Web texts were selected from the online latest news of all major Portuguese newspapers, which included newspapers of different styles (daily newspapers covering all topics, weekly newspapers with a broad coverage of topics, economics newspapers and sports news). These newspapers were selected for their content and reliability to better reflect the lexical and linguistic content of current news events. However, only an average of 80K words was collected per day as adaptation data. Thus, to construct a more homogeneous adaptation dataset and collect enough n-grams containing new words, we merged Web data from several consecutive days. In our work we considered a heuristic time span of 7 days. Similar approaches were taken in Federico and Bertoldi (2004) and Allauzen and Gauvain (2005b). Hence, for each day d, we used the texts from the current day and the six preceding days (we denote this adaptation subset as O 7 (d) – 7days of online written news). For the POS-based algorithm, we used BN.train as the in-domain corpus to estimate the POS distribution function. Using the selected vocabulary V 0, three language models were estimated: a generic backoff 4-g language model trained on NP.train, an in-domain backoff 3-g language model trained on BN.train and an adaptation backoff 3-g language model trained on O 7 (d). The generic language model was estimated using the modified Kneser–Ney smoothing (Ney et al., 1997), with absolute discounting used to estimate the other two language models. Finally, the three LMs were linearly combined. The mixture coefficients were estimated using the expectation–maximization (EM) algorithm to maximize the likelihood of a held-out dataset. For that purpose, we defined as our held-out dataset the set of ASR transcriptions generated by the broadcast news transcription system itself for the 21 preceding days (noted here as T 21 (d)), i.e., 3weeks of automatically generated captions (generated by the second-pass). A confidence measure was used to select only the most accurately recognized transcription words. Thus, all the words wi with a confidence value P(correct|wi ) higher than 91.5% were included in the T 21 (d) dataset. This is an important issue because recognition errors can skew the n-gram estimates and thus deteriorate the adapted language model. In fact, in Tam and Schultz (2006) and Wang and Stolcke (2007) a degradation of the recognition performance was reported when the baseline language model was adapted based on automatic transcriptions, with the authors postulating that this may have been caused by the recognition errors that were not smoothed properly. Finally, the mixed language model was pruned using entropy-based pruning (Stolcke, 1998). For a pruning threshold equal to 1e−09, we can obtain a language model that is about 30% smaller than the original one without significant degradation of WER (Martins et al., 2005). In this multi-pass adaptation framework, a second-pass (Fig. 9 ) was used to produce improved offline transcriptions for each day using the initial set of ASR hypotheses generated during the live version. The basic idea is as follows. The initial set of ASR hypotheses (the result of the first decoding pass), which include texts on multiple topics, is automatically segmented into individual stories with each story ideally concerning a single topic. These segmentation boundaries are located by the audio partitioner (Meinedo and Neto, 2005) and topic segmentation procedure (Amaral et al., 2006) currently implemented on the baseline system. The text of each segment is then used as a query for an information retrieval engine to extract relevant documents from a dynamic and large-size database. This way, a story-based dataset is extracted for each segment and used to dynamically build an adapted vocabulary and language model for each story present in the news show being recognized. For this framework we used the information retrieval engine INDRI – a language-based search engine (Strohman et al., 2005). As the starting point, the indexing of all training datasets (NP.train and BN.train) was done, generating a total of about 1.5M articles indexed. For the indexing process we defined as term the concept of word. During the indexing/retrieval process we removed all the function words and the 500 most frequent words, creating a stoplist of 800 words. The current IR dynamic database is now updated in a daily basis with the contemporary texts, i.e., the texts used to generate the O 7 (d) dataset. For the work presented in this study we used the cosine as the similarity measure for the retrieval phase. Thus, all articles with an IR score exceeding an empirically determined threshold were extracted for each story. To collect, for each topic, a dataset of texts similar in size to the BN.train and O 7 (d) datasets (531K and 560K words respectively), we extracted the first 1000 articles with the highest IR score for each one of the queries, which gave an average of 610K words. Because the number of words in the hypothesized transcript of each story was usually small and contained transcription errors, a pseudo-relevance feedback mechanism was used for automatic query expansion (Lavrenko and Croft, 2001). This method uses the ASR hypotheses as an initial query, does some processing and then returns a list of expansion terms. The original query is then augmented with the expansion terms and rerun. In our framework the 30 top retrieved and returned articles were used for query expansion. For training proposes only the 1000 articles with the highest IR score returned by the pseudo-relevance feedback procedure were used. The functioning of the IR step was assessed by manual annotation regarding the relevance of each of the 1000 returned documents for three randomly topics/stories of BN.RTP-2007. Because IR evaluation is not the main focus of the present paper and topics were randomly selected and attributed to annotators, we consider the reduced number not to be a problem. Each topic was annotated by a different annotator. Despite preventing judge-to-judge correlation, it made it possible to have results for the three topics with a reasonable effort while solving the natural limitations of a unique annotator. In the annotations, a somewhat broad definition of the topic was adopted. As an example, for one of the selected topics, regarding losses due to bad weather, different weather problems (heavy rain, wind, etc.) were considered. Fig. 10 presents the accumulated number of relevant documents as a function of the number of documents. We considered this information more meaningful than presenting only the mean average precision. For the three topics, the average number of relevant documents was 682 and the minimum 536. Mean average precision obtained was 0.79, which is the average precision for the individual topics of 0.74, 0.76 and 0.88. Therefore, the results show that the IR step is capable of returning a reasonable number of relevant documents. For each story S a topic-related dataset Ds was extracted from the IR dynamic database, and all words found in Ds were added to the vocabulary V 0 selected on the first-pass, generating in this way a story-specific vocabulary VS . Note that for each new word found in Ds we removed from V 0 the word with the lowest frequency, keeping the vocabulary size of vocabulary VS equal to V 0. After this procedure, we augmented the VS vocabulary by using the lemmatization and inflection generation method reported in our previous work (Martins et al., 2006). Hence, the VS vocabulary was expanded with all the inflection forms of verbs whose lemmas were present in the text of each story S. Applying this lemmatization method resulted in an average of 800 new words added to the vocabulary VS of each topic. Finally, with VS , an adaptation backoff 3-g LM trained on Ds was estimated and linearly combined with the first-pass LM (MIX 0) to generate a story-specific LM (MIXs ). Using VS and MIXs in a second decoding pass the final set of ASR hypotheses was generated for each story S. According to the proposed framework, the following daily steps have been implemented in the current live system, which is generating closed-captions in real-time for the 8 o’clock evening news show of the Portuguese public broadcast company RTP: – Using the RSS News feeds services of six different Portuguese news channels, latest news are collected from the Web, normalized, stored and indexed by the IR engine (an average of 130 articles and 80K words collected per day). This process checks for more news blocks every hour. These six news channels were selected for their content and reliability to better reflect the lexical and linguistic content of current news events (news feeds with a broad coverage of topics, economics and sports news feeds). In particular, the news feed available at the RTP website and provided by the news agency of Portugal (LUSA – Agência de Notícias de Portugal) was selected because some of the stories presented at news shows are based on them. – At 5 o’clock in the evening, and using the news texts collected up until then, a new vocabulary and LM are generated according the first-pass of the proposed adaptation approach and used by the ASR module at 8 o’clock to generate closed-captions for the TV news show. – At the end of the TV news show, the second-pass is processed, generating improved BN transcriptions. By applying this multi-phase adaptation approach we expect to improve the system performance over the first-pass. In the next section we will describe the experiments we have performed for evaluation purposes. 4 Evaluation results To compare and evaluate the proposed adaptation approaches, a range of experimental results are reported using the baseline system described in Section 2.1. In these experiments we used two evaluation metrics: OOV word rate and WER over the two evaluation datasets (BN.RTP-2004 and BN.RTP-2007). Comparison of results for both live and offline approaches using different vocabulary sizes is also described, allowing us to evaluate the effects of plain vocabulary growth on our adaptation framework. Hence, besides the baseline vocabulary size of 57K, we defined three more vocabulary sizes for evaluation purposes: a smaller one with about fifty percent of the baseline vocabulary size (30K words), another one with almost the double of the size (100K words) and a third with roughly three times the size (150K). To better evaluate the relationship between the LM training data (quantity and recency) and the results obtained with our adaptation techniques, we performed another additional experiment by extended both NP.train and BN.train datasets and compared the results measured on the two news shows of the BN.RTP-2007 test dataset when applying the baseline system and the proposed multi-pass adaptation frameworks with the four vocabulary sizes (30K, 57K, 100K and 150K words). Additionally, we present some statistics comparing the amount of memory and processing time allocated to the ASR decoding process in each one of the various experiments. When available for the two test sets, the daily values of OOVs and WER were subject to statistical analysis. Due to the violation of the normality or homogeneity of variances assumptions in several groups, non-parametric Friedman ANOVA for repeated measures was used, followed by pair-wise comparisons (Wilcoxon test). 4.1 OOV word rate results We started our evaluation by fixing our vocabulary size to 57K, the value used by the baseline ASR used for this research. The average values for BN.RTP-2007 and the corresponding relative improvements are presented in Fig. 11 . The proposed second-pass speech recognition approach (2-PASS-POS-IR) using the morpho-syntactic algorithm (POS) plus the lemmatization method for vocabulary adaptation and the Information Retrieval Engine (IR) for language model adaptation yields a relative reduction of 69% in OOV word rate, i.e., from 1.40% to 0.44%, when compared to the results obtained for the baseline system. Moreover, this approach outperformed the one based on one single-pass (1-PASS-POS). After the evaluation of our two methods with the same vocabulary size as our baseline system, and to better understand the performance of this new adaptation procedure, we calculated and compared the OOV word rate results for four different vocabulary sizes (30K, 57K, 100K and 150K words) on the two evaluation datasets. In Fig. 12 we present the average OOV word rate obtained from the daily values as a function of the evaluation dataset, method and vocabulary size. The graph shows the relatively good performance of 1-PASS-POS and 2-PASS-POS-IR approaches for the selection of large-sized vocabularies. We can observe that the proposed second-pass speech recognition approach (2-PASS-POS-IR) yields, for all vocabulary sizes and both test datasets, the lower OOV word rate. Moreover, this approach outperformed the one based on one single-pass (1-PASS-POS). The reduction of this metric with vocabulary size is also evident. Furthermore, as we would expect, for the selection of small vocabularies better results are achieved by using the 2-PASS-POS-IR method. In fact, as one can see, with a vocabulary of 30K words, we were able to obtain better lexical coverage than the one obtained for the baseline system with a vocabulary of 57K words. For both test datasets the best results were obtained for the 2-PASS-POS-IR and a vocabulary size of 150K, with a value of 0.17%. This OOV word rate corresponds to an improvement, relative to the 57K baseline, of 85.8% for the BN.RTP-2004 dataset (from 1.20% to 0.17%) and 87.9% for the BN.RTP-2007 dataset (from 1.40% to 0.17%). Even comparing the best results with the baseline for a 150K vocabulary (0.26% for BN.RTP-2004 and 0.32% for BN.RTP-2007) the improvement is at least 34.6%. The comparison of results obtained for both datasets allow us to conclude that even when the evaluation dataset is closer to the LM training corpora (BN.RTP-2004), the relative improvement is almost the same, showing that these adaptation approaches are useful even in that case. However, as one can observe, there is an inversion between the behavior of BN.RTP-2004 and BN.RTP-2007 when using the proposed adaptation techniques: for the baseline there are more OOV in the 2007 dataset, but for the first-pass adaptation, there is no difference, and by using the second-pass adaptation, there are even more OOV in the 2004 dataset. In fact, due to the gap between the 2007 dataset and the adaptation data, the proposed adaptation procedures were shown to perform well, in particular the second-pass adaptation using the IR framework for smaller vocabularies. For larger vocabularies, there is almost no gain between the first and the second-pass. In this case, the remaining OOV words are specific names or verbal lemmas not contained in the training/adaptation data, and the IR procedure is not able to recover them at all. The graph of Fig. 13 shows the proportion of verbs in OOVs when we restrict or extend the vocabulary size and apply the proposed adaptation frameworks. As we can observe, for both evaluation datasets, there is a decrease in the proportion of verbs in OOVs for both adaptation approaches, with an average relative reduction of 57% for the 1-PASS-POS approach and 79% for the 2-PASS-POS-IR approach when compared to the baseline. In fact, the decrease obtained with the 2-PASS-POS-IR comprises the reduction due to the POS-based technique, applied in the first-pass, plus the IR and lemmatization processes, applied to each topic in the second-pass. We also see that the effect of the second-pass on the reduction of OOV verbs is larger on the BN.RTP-2004 dataset, while for the baseline and first-pass, the two evaluation datasets seem to behave identically. Observing the OOV verbs recovered, we could conclude this difference is mainly due to the fact that more verbal inflections were recovered by applying the IR procedure over the 2004 dataset, which can be justified by the fact that these BN shows comprise more street interviews with a large amount of spontaneous speech. Moreover, by analyzing the OOV reduction by POS-classes, one can observe a similar behavior for both adaptation approaches: an average reduction of 31% for OOV names and 57% for OOV verbs when applying the first-pass, and an average reduction of 35% for OOV names and 52% for OOV verbs when applying the second-pass over the first one. This means that both approaches contribute in the same way to the type of OOV recovered. Finally, to evaluate the benefit of our adaptation approaches even in the case of adding more LM training data, we extended both training corpora (NP.train and BN.train) with more data. In the case of NP.train, we added newspaper texts collected from 2004 until the end of 2006, going from the previous 604M words to approximately 740M words (NP.train extended dataset). In terms of broadcast news data, we added 5h of BN in-domain texts to the previous 47h (BN.train extended dataset). The results of training with the extended training dataset and keeping the two factors (vocabulary size and adaptation method) constant are presented in Fig. 14 . For comparison, the average values presented in previous figures are included. Results were only obtained for BN.RTP-2007 because the training material is more recent than our other evaluation dataset (BN.RTP-2004). From the graph we can conclude that the addition of more training data mainly produced better results in the case of the baseline vocabulary. In fact, for the proposed adaptation approaches there were only slight improvements comparing to the previous results. However, for all the vocabulary sizes, both the 1-PASS-POS and 2-PASS-POS-IR methods still produced lower OOV word rates than the baseline ones. The Friedman test confirms as significant the effect of method [χ 2 (2)=18.00, p <0.001] and vocabulary size [χ 2 (3)=27.00, p <0.001]. The effect of test dataset did not prove to be significant (p =0.698). Regarding method, the pair-wise application of the Wilcoxon test with Bonferroni correction shows that OOV word rates for 2-PASS-POS-IR are significantly lower than for the 1-PASS-POS only method, (p <0.001), and OOVs for 1-PASS-POS only are significantly lower than for the baseline (p <0.001). Even with the reduced number of days on the test datasets and the variation in OOV word rates, the non-parametric test was capable of confirming that the 2-PASS-POS-IR method was significantly better than 1-PASS-POS and baseline methods. For vocabulary size, the pair-wise application of the Wilcoxon test with Bonferroni correction showed a significant WER decrease between each pair of consecutive vocabulary sizes (p <0.001). 4.2 WER results In terms of WER, the average values for BN.RTP-2007 and the corresponding relative improvements are presented in Fig. 15 . As we can observe, the new approach (1-PASS-POS) resulted in an 8% relative gain, from 19.9% to 18.25%, for a vocabulary size of 57K words. Moreover, the proposed second-pass approach yielded a relative reduction of 12% in WER when compared to the WER obtained for the baseline system, outperforming the 1-PASS-POS approach (a slight decrease in WER, from 18.25% to 17.55%). To better evaluate the accuracy of our approach we performed a more detailed analysis of the WER obtained by the 2-PASS-POS-IR approach with a vocabulary of 57K words. For that analysis, we divided the adapted vocabulary VS of each story S into two sets: the set of word types that were already present in the baseline vocabulary V 0 and the set of all new word types. From this last set (denoted by Ns ), we removed all the word types except the ones occurring in the reference transcripts of the tested BN dataset (BN.RTP-2007). The number of word types in Ns was 105, with 182 occurrences in the reference transcripts. From these 182 occurrences, 133 were correctly recognized by the 2-PASS-POS-IR approach, which means 73.1% of new words found by our IR-based framework were correctly recognized. In Table 4 we present the distribution of those 182 occurrences by grammatical category. In the “Names” category we generically included both proper and common names, even the foreign ones. The “Others” category included other foreign words, acronyms and abbreviations. As one can observe, more than 66% of those new words found by our algorithm belong to the verbs class. Moreover, the class of names is the one with the best recognition rate (80.2% of new names were correctly recognized), slightly outperforming the average value (73.1%). This shows that a significant number of relevant terms like proper and common names (including names of persons, locations and organizations) were correctly recognized, making the framework especially useful for novel applications like information dissemination, where those types of words contain a great deal of information. In Fig. 16 , we compared the accuracy of our two methods and the baseline for four different vocabulary sizes. One can observe that WER was reduced as the vocabulary size increased and was, for all vocabulary sizes, better with the 2-PASS-POS-IR method. For the BN.RTP-2004 dataset, with the proposed multi-pass adaptation approach and increasing the vocabulary size to 150K words, we could obtain a relative gain of 10.4% in terms of WER, with a final WER of 19.81% against the 22.10% mean for the baseline system. For the BN.RTP-2007 test dataset, the corresponding gain was 13.1%, from 19.9% to 17.3%. Even using a vocabulary with only 30K words, we were able to get a better WER for both test datasets with our adaptation framework than the one obtained for the baseline system with a 57K word vocabulary. Finally, in Fig. 17 we present the comparison of the average WER measured for the two news shows of the BN.RTP-2007 evaluation dataset when applying the baseline system and the proposed multi-pass adaptation frameworks with different vocabulary sizes and extending the training corpora. In this case, both the baseline and 1-PASS-POS produced similar improvements in terms of WER. For the 2-PASS-POS-IR only slight improvements were obtained, mainly in the case of smaller vocabularies. However, both approaches produced better WER results even when more recent data was added to the LM training corpora. Separate application of Friedman’s test showed that there were some statistically significant changes in the distribution of WERs over the three methods [χ 2 (2)=18.00, p <0.001] and over the four vocabulary sizes [χ 2 (3)=24.71, p <0.001]. A pair-wise application of the Wilcoxon test with Bonferroni correction levels showed that the WER for 2-PASS-POS-IR was significantly lower than for 1PASS-POS only method, (p <0.001), and the WER for 1-PASS-POS only was significantly lower than for the baseline (p <0.001). A similar application of Wilcoxon test with correction for the vocabulary size factor showed that each vocabulary size had a WER significantly lower than that of all smaller sizes (p <0.001). 4.3 Computational costs To compare the baseline system and the proposed adaptation frameworks in terms of computational costs, we used the amount of memory and processing time allocated to the ASR decoding process. The averages presented were calculated for the two news shows of the BN.RTP-2007 dataset when applying the baseline system and the proposed online adaptation framework (1-PASS-POS) for the four different vocabulary sizes and using the extended training corpora. Because computational costs are critical for our online task – the live and real-time subtitling of RTP news shows – we only compared the baseline with the 1-PASS-POS approach. Memory requirements (in GBytes) and CPU time (in seconds) were measured on an Intel(R) Core(TM) 2 Quad CPU Q6600 @ 2.40GHz equipped with 8Gb of RAM. Results are presented in Fig. 18 . We can observe that for the same vocabulary size, the two methods require similar memory and processing time, and both memory and CPU time increase with vocabulary size. This increase was more significant in the case of CPU time. We can observe an increase of almost 10% when using a vocabulary size of 150K words instead of a 57K word vocabulary. 5 Discussion and future work In this paper, we addressed the task of language modeling for the automatic transcription of European Portuguese BN speech, proposing a framework to dynamically address new words by adapting both the vocabulary and language model on a daily basis. Analyzing the distribution of OOV words according to their grammatical properties in a given sentence (i.e., their part-of-speech), we found that more than 56% of them were verbs. This was an interesting result because from other findings published in the literature, OOV words are mostly names. This led us to study the use of specific linguistic knowledge (POS tagging) to improve the lexical coverage of a selected vocabulary. In terms of LM adaptation, we proposed and implemented a multi-pass speech recognition approach that creates from scratch both vocabulary and LM components on a daily basis using adaptation texts extracted from the Internet and the new POS-based vocabulary selection algorithm. This framework was integrated into the fully functional system described in Section 2, which is being used to produce live captions for a closed-captioning system of live European Portuguese TV broadcasts. For the BN.RTP-2007 evaluation dataset, considering the same vocabulary size as the baseline one (57K words), a relative gain of 12% in the WER and a relative reduction of 69% in OOV word rate were observed. Moreover, by applying the proposed adaptation framework and increasing the vocabulary size to 150K words we obtained a relative gain of 13.1% in terms of WER with a relative OOV word rate reduction of 87.9%. Taking into consideration the application framework we are using, the processing time is crucial. In fact, generating the live captions as quickly as possible and with negligible delay is a very important issue for us. Hence, improving the system accuracy without compromising its response speed is clearly beneficial for this kind of application. Combining results for WER and CPU time, the benefit of using our online adaptation approach (1-PASS-POS) is clear. Using the 1-PASS-POS method with a vocabulary of 57K words results in a relative decrease of 12.4% in terms of CPU time compared to the baseline system with a vocabulary of 150K without compromising system accuracy. Even better, 1-PASS-POS for 57K words presents a slight better accuracy than the 150K baseline (18.1% versus 18.5%). The accuracy gain of the proposed method allows us to use more reduced vocabulary sizes to comply with the live application demands. In fact, even if we can take advantage of more powerful machines both in terms of memory and CPU capacity, in our opinion it is useful to restrict the vocabulary size for the application type we are using. Using large-sized vocabularies may be desirable from the point of view of lexical coverage. However, there is always the additional problem of increased acoustic confusability. Moreover, as we reported in Section 2.2, each BN show comprises an average of 8300 word tokens and only 2200 word types. Thus, the majority of the vocabulary words are irrelevant when adapting to a single BN show. Therefore, future research should focus on methods to better constrain vocabulary growth while preserving adaptation performance. All the work described on this paper was motivated and influenced by the finding that verbs constitute the largest portion of OOV. Hence, both vocabulary optimization and language model adaptation approaches were based on the integration of different knowledge sources and techniques (language modeling, information retrieval and morphological knowledge). However, even if we were able to improve the overall system performance, we think it is worthwhile to investigate it further in future work, especially to find solutions that can directly solve the verb OOV problem. While our focus was on European Portuguese broadcast news, where morphological variants such as inflectional verb endings have been shown to be an important problem to address, we believe the framework proposed here would likely lead to improved performance for other inflectional languages and/or applications, especially the POS-based vocabulary selection procedure. In terms of future work, there are other directions that, in our opinion, can be investigated to enhance the global performance of the language model component. Considering the vocabulary selection, we believe that better results can be achieved by exploring more deeply the linguistic knowledge of the in-domain corpus. We could use not only the grammatical property of words (POS), but also their morphological information (gender, number, conjugation, etc.). To follow this research trend, two resource constraints must be overcome: more in-domain data and a morphological analyzer for European Portuguese that could give us that level of morphological information with an acceptable accuracy. Moreover, we can extend the effectiveness of our multi-pass adaptation framework by using the IR techniques in the first-pass to select and cluster data, reducing its redundancy and improving the generic language model estimation. In fact, entropy-based clustering and data selection have been used with significant gains for defining topic-specific subsets and pruning less useful documents from training data, both for acoustic and language model adaptation (Hwang et al., 2007; Ramabhadran et al., 2007; Wu et al., 2007). Acknowledgments This work was partially funded by PRIME National Project TECNOVOZ number 03/165 and by the FCT Project POSC/PLP/58697/2004. Ciro Martins was sponsored by an FCT scholarship (SFRH/BD/23360/2005). Special thanks to our colleague Hugo Meinedo for his efforts to make available all the manual transcriptions for the evaluation datasets we used. We also thank the 3 annotators involved in IR evaluation. References Allauzen and Gauvain, 2005a Allauzen, A., Gauvain, J., 2005. Open vocabulary ASR for audiovisual document indexation. In: Proceedings of ICASSP, 2005. Allauzen and Gauvain, 2005b Allauzen, A., Gauvain, J., 2005a. Diachronic vocabulary adaptation for broadcast news transcription. In: Proceedings of Interspeech, 2005. Amaral et al., 2006 Amaral, R., Meinedo, H., Caseiro, D., Trancoso, I., Neto, J., 2006. Automatic vs. manual topic segmentation and indexation in broadcast news. In: IV Jornadas en Tecnologia del Habla, November 2006, pp. 123–128. Bazzi, 2002 Bazzi, I., 2002. Modeling out-of-vocabulary words for robust speech recognition. Ph.D. Thesis, Massachusetts Institute of Technology. Bellegarda, 2004 J. Bellegarda Statistical language model adaptation: review and perspectives Speech Communication 42 2004 Bigi et al., 2004 Bigi, B., Huang, Y., Mori, R., 2004. Vocabulary and language model adaptation using information retrieval. In: Proceedings of ICSLP, 2004. Blei and Jordan, 2003 A. Blei M. Jordan Latent dirichlet allocation Journal of Machine Learning Research 2003 Boulianne et al., 2006 Boulianne, G., Beaumont, J., Boisvert, M., Brousseau, J., Cardinal, P., Chapdelaine, C., et al., 2006. Computer-assisted closedcaptioning of live TV broadcast in French. In: Proceedings of Interspeech, 2006. Caseiro, 2003 Caseiro, D., 2003. Finite-state methods in automatic speech recognition. Ph.D. Thesis, Instituto Superior Técnico, Universidade Técnica de Lisboa, Portugal. Caseiro et al., 2002 Caseiro, D., Trancoso, I., Oliveira, L., Ribeiro, M., 2002. Grapheme-to-phone using finite-state transducers. In: 2002 IEEE Workshop on Speech Synthesis. Chen et al., 2004 Chen, L., Gauvain, J., Lamel, L., Adda, G., 2004. Dynamic language modeling for broadcast news. In: Proceedings of ICSLP, 2004. Federico and Bertoldi, 2004 M. Federico N. Bertoldi Broadcast news LM adaptation over time Computer Speech and Language 18 4 2004 417 435 Federico et al., 2000 Federico, M., Giordani, D., Coletti, P., 2000. Development and evaluation of an Italian broadcast news corpus. In: Proc. LREC, Athens, Greece, 2000. Gauvain et al., 2002 J. Gauvain L. Lamel G. Adda The LIMSI broadcast news transcription system Speech Communication 37 2002 89 108 Geutner et al., 1998 Geutner, P., Finke, M., Sheytt, P., Waibel, A., Wactlar, H., 1998. Transcribing multilingual broadcast news using hypothesis driven lexical adaptation. In: Proceedings of ICASSP, 1998. Hetherington, 1995 Hetherington, I., 1995. A characterization of the problem of new, out-of-vocabulary words in continuous-speech recognition and understanding. Ph.D. Thesis, Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science. Hwang et al., 2007 Hwang, M., Peng, G., Wang, W., Faria, A., Heidel, A., Ostendorf, M., 2007. Building a highly accurate mandarin speech recognizer. In: Proceedings of ASRU, 2007. Iyer and Ostendorf, 1997 Iyer, R., Ostendorf, M., 1997. Transforming out-of-domain estimates to improve in-domain language models. In: Proceedings of Eurospeech, 1997. Kirchhoff et al., 2006 K. Kirchhoff D. Vergyri J. Bilmes K. Duh A. Stolcke Morphology-based language modeling for conversational Arabic speech recognition Computer Speech and Language 20 4 2006 589 608 Lamel et al., 2004 Lamel, L., Gauvain, J., Adda, G., Adda-Decker, M., Canseco, L., Chen, L., et al., 2004. Speech transcription in multiple languages. In: Proceedings of ICASSP, 2004. Lavrenko and Croft, 2001 Lavrenko, V., Croft, W., 2001. Relevance-based language models. In: Proceedings of SIGIR’01, 2001. Lecorvé et al., 2008 Lecorvé, G., Gravier, G., Sébillot, P., 2008. An unsupervised web-based topic language model adaptation method. In: Proceedings of ICASSP, 2008. Lecorvé et al., 2009 Lecorvé, G., Gravier, G., Sébillot, P., 2009. Constraint selection for topic-based MDI adaptation of language models. In: Proceedings of InterSpeech, 2009. Martins, 1998 Martins, C., 1998. Modelos de Linguagem no reconhecimento de Fala Contínua. Master’s Thesis, Instituto Superior Técnico, Universidade Técnica de Lisboa. Martins et al., 2005 Martins, C., Teixeira, A., Neto, J., 2005. Language models in automatic speech recognition. Revista Electrónica e Telecomunicações, Departamento de Electrónica e Telecomunicações, Universidade de Aveiro, Aveiro, vol. 4, no. 4, 2005. Martins et al., 2006 Martins, C., Teixeira, A., Neto, J., 2006. Dynamic vocabulary adaptation for a daily and real-time broadcast news transcription system. In: Proceedings of IEEE/ACL Workshop on Spoken Language Technology, 2006. Martins et al., 2007a Martins, C., Teixeira, A., Neto, J., 2007a. Vocabulary selection for a broadcast news transcription system using a morpho-syntactic approach. In: Proceedings of Interspeech, 2007. Martins et al., 2007b Martins, C., Teixeira, A., Neto, J., 2007b. Dynamic language modeling for a daily broadcast news transcription system. In: Proceedings of ASRU, 2007. Martins et al., 2008 Martins, C., Teixeira, A., Neto, J., 2008. Dynamic language modeling for the European Portuguese. In: Proceedings of PROPOR 2008, Curia, Portugal. Meinedo, 2008 Meinedo, H., 2008. Audio pre-processing and speech recognition for broadcast news. Ph.D. Thesis, Instituto Superior Técnico, Universidade Técnica de Lisboa. Meinedo and Neto, 2000 Meinedo, H., Neto, J., 2000. Combination of acoustic models in continuous speech recognition hybrid systems. In: Proceedings of ICSLP 2000, China. Meinedo and Neto, 2005 Meinedo, H., Neto, J., 2005. A stream-based audio segmentation, classification and clustering preprocessing system for broadcast news using ANN models. In: Proceedings of Interspeech, 2005. Meinedo et al., 2003 Meinedo, H., Caseiro, D., Neto, J., Trancoso, I., 2003. AUDIMUS.MEDIA: a broadcast news speech recognition system for the European Portuguese language. In: Proceedings of PROPOR 2003, Portugal. Neto et al., 2003 Neto, J., Meinedo, H., Amaral, R., Trancoso, I., 2003. The development of an automatic system for selective dissemination of multimedia information. In: Proceedings of Third International Workshop on Content-based Multimedia Indexing – CBMI 2003. Neto et al., 2008 Neto, J., Meinedo, H., Viveiros, M., Cassaca, R., Martins, C., Caseiro, D., 2008. Broadcast news subtitling system in Portuguese. In: Proceedings of ICASSP, 2008. Ney et al., 1997 H. Ney S. Martin F. Wessel Statistical language modeling using leaving-one-out S. Young G. Bloothooft Corpus-based Methods in Language and Speech Processing 1997 Kluwer Academic Publishers Dordrecht 174 207 (Chapter 6) Oger et al., 2008 Oger, S., Linares, G., Bechet, F., Nocera, P., 2008. On-demand new word learning using world wide web. In: Proceedings of ICASSP, 2008. Orengo and Huyck, 2001 Orengo, V., Huyck, C., 2001. A stemming algorithm for the Portuguese language. In: Proceedings of the Eighth International Symposium on String Processing and Information Retrieval, 2001. Palmer and Ostendorf, 2005 D. Palmer M. Ostendorf Improving out-of-vocabulary name resolution Computer Speech and Language 19 2005 107 128 Ramabhadran et al., 2007 Ramabhadran, B., Siohan, O., Sethy, A., 2007. The IBM 2007 speech transcription system for European parliamentary speeches. In: Proceedings of ASRU 2007. Ribeiro et al., 2003 R. Ribeiro L. Oliveira I. Trancoso Using morphossyntactic information in TTS systems: comparing strategies for European Portuguese Computational Processing of the Portuguese Language. Lecture Notes in Computer Science (Subseries LNAI) vol. 2721 2003 Springer 143 150 Ribeiro et al., 2004 R. Ribeiro N. Mamede I. Trancoso Morpho-syntactic Tagging: a Case Study of Linguistic Resources Reuse Language Technology for Portuguese: Shallow Processing Tools and Resources 2004 Edições Colibri Lisbon, Portugal Stolcke, 1998 Stolcke, A., 1998. Entropy-based pruning of backoff language models. In: Proceedings of DARPA News Transcription and Understanding Workshop, 1998. Strohman et al., 2005 Strohman, T., Metzler, D., Turtle, H., Croft, W.B., 2005. Indri: a language-model based search engine for complex queries (extended version). CIIR Technical Report, 2005. Tam and Schultz, 2006 Tam, Y., Schultz, T., 2006. Unsupervised language model adaptation using latent semantic marginals. In: Proceedings of Interspeech, 2006. Venkataraman and Wang, 2003 Venkataraman, A., Wang, W., 2003. Techniques for effective vocabulary selection. In: Proceedings of Eurospeech, 2003. Wang and Stolcke, 2007 Wang, W., Stolcke, A., 2007. Integrating MAP, marginals, and unsupervised language model adaptation. In: Proceedings of Interspeech, 2007. Wu et al., 2007 Wu, Y., Zhang, R., Rudnicky, A., 2007. Data selection for speech recognition. In: Proceedings of ASRU, 2007. Xu and Jelinek, 2007 P. Xu F. Jelinek Random forests and the data sparseness problem in language modeling Computer Speech and Language 21 1 2007 105 152 "
    },
    {
        "doc_title": "Human language technologies for e-gov",
        "doc_scopus_id": "77956270605",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77956270605",
        "doc_date": "2010-09-09",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "e-gov",
            "Human language technologies",
            "Information Extraction",
            "Life-event",
            "Natural language interfaces"
        ],
        "doc_abstract": "Effective provision of government services implies that, besides being provided online, services become available through other channels, are organized according to citizen's expectations, are accessible to everyone, anytime and anywhere, and include information from unstructured sources. It is also essential to provide the tools that allow citizens to correctly identify the services they need. In this paper we will discuss how it is possible to improve e-gov service delivery by using human language technologies. We argue that these technologies can contribute to: deliver services in more inclusive manners; provide human centered and multilingual service and support; and include non-structured information scattered across different sources.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Speech rate effects on European Portuguese nasal vowels",
        "doc_scopus_id": "70450179792",
        "doc_doi": null,
        "doc_eid": "2-s2.0-70450179792",
        "doc_date": "2009-11-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Sensory Systems",
                "area_abbreviation": "NEUR",
                "area_code": "2809"
            }
        ],
        "doc_keywords": [
            "Gestures",
            "Nasal vowels",
            "Speech rates",
            "Temporal characteristics",
            "Temporal information"
        ],
        "doc_abstract": "This paper presents new temporal information regarding the production of European Portuguese (EP) nasal vowels, based on new EMMA data. The influence of speech rate on duration of velum gestures and their coordination with consonantic and glottal gestures were analyzed. As information on relative speed of articulators is scarce, the parameter stiffness for the nasal gestures was also calculated and analyzed. Results show clear effects of speech rate on temporal characteristics of EP nasal vowels. Speech rate reduces the duration of velum gestures, increases the stiffness and inter-gestural overlap. Copyright © 2009 ISCA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Automatic estimation of language model parameters for unseen words using morpho-syntactic contextual information",
        "doc_scopus_id": "84867228242",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84867228242",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Sensory Systems",
                "area_abbreviation": "NEUR",
                "area_code": "2809"
            }
        ],
        "doc_keywords": [
            "Automatic estimation",
            "Broadcast news",
            "Class-based language model",
            "Contextual information",
            "Information sources",
            "Language model",
            "Morpho-syntactic analysis",
            "Part Of Speech",
            "POS tags",
            "Relative reduction",
            "Speech recognition systems",
            "Training data",
            "Word error rate"
        ],
        "doc_abstract": "Various information sources naturally contains new words that appear in a daily basis and which are not present in the vocabulary of the speech recognition system but are important for applications such as closed-captioning or information dissemination. To be recognized, those words need to be included in the vocabulary and the language model (LM) parameters updated. In this context, we propose a new method that allows including new words in the vocabulary even if no well suited training data is available, as is the case of archived documents, and without the need of LM retraining. It uses morpho-syntatic information about an in-domain corpus and part-of-speech word classes to define a new LM unigram distribution associated to the updated vocabulary. Experiments were carried out for a European Portuguese Broadcast News transcription system. Results showed a relative reduction of 4% in word error rate, with 78% of the occurrences of those newly included words being correctly recognized. Copyright © 2008 ISCA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Ontology-driven vaccination information extraction",
        "doc_scopus_id": "58049149465",
        "doc_doi": null,
        "doc_eid": "2-s2.0-58049149465",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "Association rule minings",
            "Class relationships",
            "Clinical decision support systems",
            "Clinical guidelines",
            "Clinical informations",
            "Health policies",
            "Information extractions",
            "Interpretable representations",
            "Medical errors",
            "Medical institutions",
            "Medical texts",
            "Patient cares",
            "Semantic approaches",
            "System identifies"
        ],
        "doc_abstract": "Increasingly, medical institutions have access to clinical information through computers. The need to process and manage the large amount of data is motivating the recent interest in semantic approaches. Data regarding vaccination records is a common in such systems. Also, being vaccination is a major area of concern in health policies, numerous information is available in the form of clinical guidelines. However, the information in these guidelines may be difficult to access and apply to a specific patient during consultation, The creation of computer interpretable representations allows the development of clinical decision support systems, improving patient care with the reduction of medical errors, increased safety and satisfaction. This paper describes the method used to model and populate a vaccination ontology and the system which recognizes vaccination information on medical texts.The system identifies relevant entities on medical texts and populates an ontology with new instances of classes. An approach to automatically extract information regarding inter-class relationships using association rule mining is suggested.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Voice quality of psychological origin",
        "doc_scopus_id": "56349133191",
        "doc_doi": "10.1080/02699200802175974",
        "doc_eid": "2-s2.0-56349133191",
        "doc_date": "2008-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Speech and Hearing",
                "area_abbreviation": "HEAL",
                "area_code": "3616"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Variations in voice quality are essentially related to modifications of the glottal source parameters, such as: F0, jitter, and shimmer. Voice quality is affected by prosody, emotional state, and vocal pathologies. Psychogenic vocal pathology is particularly interesting. In the present case study, the speaker naturally presented a ventricular band voice whereas in a controlled production he was able to use a more normal phonation process. A small corpus was recorded which included sustained vowels and short sentences in both registers. A normal speaker was also recorded in similar tasks. Annotation and extraction of parameters were made using Praat's voice report function. Application of the Hoarseness Diagram to sustained productions situates this case in the pseudo-glottic phonation region. Analysis of several different parameters related to F0, jitter, shimmer, and harmonicity revealed that the speaker with psychogenic voice was capable of controlling certain parameters (e.g. F0 maximum) but was unable to correct others such as shimmer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "European Portuguese MRI based speech production studies",
        "doc_scopus_id": "54149108753",
        "doc_doi": "10.1016/j.specom.2008.05.019",
        "doc_eid": "2-s2.0-54149108753",
        "doc_date": "2008-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            },
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "2d images",
            "Area functions",
            "Articulatory synthesis",
            "Coarticulation",
            "European",
            "European Portuguese",
            "High qualities",
            "Image processing techniques",
            "Key features",
            "Nasal vowels",
            "Nasals",
            "Pharyngeal cavities",
            "Reasonable times",
            "Soft tissues",
            "Speech production",
            "Speech production models",
            "Speech productions",
            "Vocal tracts"
        ],
        "doc_abstract": "Knowledge of the speech production mechanism is essential for the development of speech production models and theories. Magnetic resonance imaging delivers high quality images of soft tissues, has multiplanar capacity and allows for the visualization of the entire vocal tract. To our knowledge, there are no complete and systematic magnetic resonance imaging studies of European Portuguese production. In this study, a recently acquired magnetic resonance imaging database including almost all classes of European Portuguese sounds, excluding taps and trills, is presented and analyzed. Our work contemplated not only image acquisition but also the utilization of image processing techniques to allow the exploration of the entire database in a reasonable time. Contours extracted from 2D images, articulatory measures (2D) and area functions are explored and represent valuable information for articulatory synthesis and articulatory phonetics descriptions. Some European Portuguese distinctive characteristics, such as nasality are addressed in more detail. Results relative to oral vowels, nasal vowels and a comparison between both classes are presented. The more detailed information on tract configuration supports results obtained with other techniques, such as EMMA, and allows the comparison of European Portuguese and French nasal vowels articulation, with differences detected at pharyngeal cavity level and velum port opening quotient. A detailed characterization of the central vowels, particularly the [{A figure is presented}], is presented and compared with classical descriptions. Results for consonants point to the existence of a single positional dark allophone for [l], a more palato-alveolar place of articulation for [y{turned}], a more anterior place of articulation for [y{turned}] relative to [n{left tail at left}], and the use, by our speaker, of a palatal place of articulation for [k]. Some preliminary results concerning coarticulation are also reported. European Portuguese stops revealed less resistant to coarticulatory effects than fricatives. Among all the sounds studied, [sh{phonetic}] and [z] present the highest resistance to coarticulation. These results follow the main key features found in other studies performed for different languages. © 2008 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271578 291210 291718 291723 291743 291782 291874 31 Speech Communication SPEECHCOMMUNICATION 2008-06-13 2008-06-13 2010-11-19T11:27:02 S0167-6393(08)00080-0 S0167639308000800 10.1016/j.specom.2008.05.019 S300 S300.1 FULL-TEXT 2015-05-14T05:01:34.741028-04:00 0 0 20081101 20081231 2008 2008-06-13T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav absattachment articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast nomenclature primabst ref alllist content subj ssids 0167-6393 01676393 50 50 11 12 11 12 Volume 50, Issues 11–12 8 925 952 925 952 200811 200812 November–December 2008 2008-11-01 2008-12-31 2008 Iberian Languages Isabel Trancoso Nestor Becerra-Yoma Plínio Barbosa Rubén San-Segundo Kuldip Paliwal article fla Copyright © 2008 Elsevier B.V. All rights reserved. EUROPEANPORTUGUESEMRIBASEDSPEECHPRODUCTIONSTUDIES MARTINS P Nomenclature 1 Introduction 1.1 Measurement methods 1.2 European Portuguese 1.3 Coarticulation 1.4 MRI in speech production studies: an overview 2 Image acquisition 2.1 MRI acquisition 2.2 Corpus 2.2.1 2D corpus 2.2.2 3D corpus 2.3 Speaker 3 Image processing 3.1 2D corpus 3.2 3D Corpus 4 Results I: vowels 4.1 Oral vowels 4.1.1 Anterior oral vowels 4.1.2 Central oral vowels 4.1.3 Posterior oral vowels 4.2 Nasal vowels 4.2.1 Nasal vs. Oral vowels 4.2.2 VPOQ 5 Results II: consonants 5.1 Nasals 5.2 Stops 5.3 Fricatives 5.4 Laterals 6 Discussion 6.1 Corpus, MRI acquisition and image processing 6.2 Oral vowels 6.3 Nasals 6.4 Stops and fricatives 6.5 Laterals 6.6 Coarticulation 7 Conclusions 7.1 Future Acknowledgements References ADAMS 1994 641 647 R ALWAN 1997 1078 1089 A BAER 1991 799 828 T BRYMAN 2001 A QUANTITATIVEDATAANALYSISSPSSRELEASE10FORWINDOWSAGUIDEFORSOCIALSCIENTISTS DANG 1994 1765 J DANG 1994 2088 2100 J DEMOLIN 2003 454 467 D FARNETANI 1999 371 404 E HANDBOOKPHONETICSCIENCES COARTICULATIONCONNECTEDSPEECHPROCESSES GICK 2002 357 371 B HARDCASTLE 1976 W PHYSIOLOGYSPEECHPRODUCTIONINTRODUCTIONFORSPEECHSCIENTISTS HARDCASTLE 1999 W COARTICULATIONTHEORYDATATECHNIQUES HOOLE 1993 43 64 P HOOLE 1999 260 269 P COARTICULATIONTHEORYDATATECHNIQUES ELECTROMAGNETICARTICULOGRAPHY JESUS 2002 437 464 L KIM 2004 234 251 H KIRITANI 1986 119 140 S KUHNERT 1999 B COARTICULATIONTHEORYDATATECHNIQUES ORIGINSCOARTICULATION LADEFOGED 1996 P SOUNDSWORLDSLANGUAGES MAGEN 1997 187 205 H MANUEL 1999 179 198 S COARTICULATIONTHEORYDATATECHNIQUES CROSSLANGUAGESTUDIESRELATINGLANGUAGEPARTICULARCOARTICULATIONPATTERNSOTHERLANGUAGEPARTICULARFACTS MATHIAK 2000 419 425 K MORAISBARBOSA 1994 A INTRODUCAOAOESTUDODAFONOLOGIAEMORFOLOGIAPORTUGUES NARAYANAN 1995 2511 2524 S NARAYANAN 2000 328 344 S NARAYANAN 1995 1325 1347 S NARAYANAN 1997 1064 1077 S NARAYANAN 2004 1771 1776 S PERKELL 1969 J PHYSIOLOGYSPEECHPRODUCTIONRESULTSIMPLICATIONSAQUANTITATIVECINERADIOGRAPHICSTUDY RECASENS 1999 D COARTICULATION LINGUALCOARTICULATION RECASENS 2005 1 25 D RECASENS 2006 295 318 D RECASENS 1997 544 561 D SACHS 1984 L APPLIEDSTATISTICSAHANDBOOKTECHNIQUES SANOGUEIRA 1938 R ELEMENTOSPARAUMTRATADODEFONETICAPORTUGUESA SANTOS 2004 868 878 B SERRURIER 2005 195 211 A SHADLE 1996 C MRISTUDYEFFECTSVOWELCONTEXTFRICATIVES STONE 1999 11 32 M HANDBOOKPHONETICSCIENCES LABORATORYTECHNIQUESFORINVESTIGATINGSPEECHARTICULATION STONE 2001 1026 1040 M STORY 1996 537 554 B STREVENS 1954 5 29 P TAKEMOTO 2004 468 474 H TEIXEIRA 2005 1435 1448 A TIEDE 1996 399 421 M TULLER 1981 175 188 E VIANA 1996 113 167 M INTRODUCAOALINGUISTICAGERALEPORTUGUESA FONETICA MARTINSX2008X925 MARTINSX2008X925X952 MARTINSX2008X925XP MARTINSX2008X925X952XP item S0167-6393(08)00080-0 S0167639308000800 10.1016/j.specom.2008.05.019 271578 2010-12-28T19:56:56.229472-05:00 2008-11-01 2008-12-31 true 3258794 MAIN 28 69155 849 656 IMAGE-WEB-PDF 1 si9 325 17 35 si8 194 13 9 si7 245 16 23 si6 270 13 27 si5 125 2 8 si43 194 13 9 si42 125 2 8 si41 125 2 8 si40 125 2 8 si4 194 13 9 si39 125 2 8 si38 125 2 8 si37 125 2 8 si36 125 2 8 si35 125 2 8 si34 194 13 9 si33 194 13 9 si32 194 13 9 si31 194 13 9 si30 194 13 9 si3 125 2 8 si29 194 13 9 si28 194 13 9 si27 194 13 9 si26 194 13 9 si25 125 2 8 si24 488 17 78 si23 488 17 78 si22 488 17 78 si21 1052 19 219 si20 1072 19 220 si2 194 13 9 si19 337 17 55 si18 217 15 14 si17 633 26 120 si16 125 2 8 si15 194 13 9 si14 125 2 8 si13 194 13 9 si12 333 17 37 si11 125 2 8 si10 125 2 8 si1 270 13 27 gr1 13645 141 577 gr1 1420 31 125 gr10 51458 556 575 gr10 2359 93 96 gr11 84510 846 579 gr11 2033 93 64 gr12 18115 170 566 gr12 1605 38 125 gr13 88749 671 610 gr13 3504 93 85 gr14 26193 269 574 gr14 2048 59 125 gr15 73164 851 564 gr15 1989 94 62 gr16 117074 725 486 gr16 3863 93 62 gr17 71026 851 564 gr17 1998 94 62 gr18 80348 734 539 gr18 2258 93 68 gr19 86568 745 580 gr19 2902 94 73 gr2 8250 142 571 gr2 1237 31 125 gr20 21045 184 600 gr20 1649 38 125 gr3 23129 306 306 gr3 4735 93 93 gr4 33560 295 300 gr4 5567 93 95 gr5 45413 298 609 gr5 2831 61 125 gr6 108311 619 555 gr6 4476 94 84 gr7 100582 902 610 gr7 1828 93 63 gr8 63007 774 600 gr8 1921 93 72 gr9 101006 897 513 gr9 2634 93 53 fx1 true 378 8 3 fx1 true 977 41 16 SPECOM 1722 S0167-6393(08)00080-0 10.1016/j.specom.2008.05.019 Elsevier B.V. Fig. 1 Boxplots of the Pratt Index differences obtained by using different starting points (seeds). Results for oral vowels, nasal vowels and nasal consonants are presented. Fig. 2 Boxplots comparing Pratt Index of all contours obtained with different starting points for a fixed image (intra) and contours of different EP sounds (inter). In the calculation, part of 2D corpus was used: all oral vowels, all five nasal vowels and the consonants [m], [s] and [l]. Fig. 3 Midsagittal profile obtained during the production of a sustained [ ] by PAA, as in the word (devi) [ d i vi ], showing measured articulatory points. Articulatory points used for this work are: highest tongue dorsum point (TD), tongue tip (TT), tongue root position at C3–C4 level (TR), jaw height (JH) and lower (LL) and upper lip (UL) spatial coordinates. Fig. 4 Example of a resliced midsagittal cut, for [a], obtained from the volumetric information (between a few centimeters above hard palate to C5 vertebral level). Superimposed, the generated adaptative grid is shown. With this procedure all obtained slices are orthogonal to the vocal tract centerline. Fig. 5 Examples of coronal oblique views obtained from nasal consonants 3D data: [m] at left and [ɲ] at right. The cut passes through the velum (orthogonal to the vocal tract centerline). Two passages can be observed: one (at the top) refers to nasal cavity and the other to oral cavity (bottom). Fig. 6 Midsagittal images with superimposed contours for the EP oral vowels: from the top, [i], [ ], [u], [e], [ɐ], [o], [ε], [a] and [ɔ]. Fig. 7 Six articulatory measures for EP vowels. From the top left: Tongue dorsum highest position (TD), tongue root at C3–C4 level (TR), tongue tip (TT), jaw height (JH), and lower (LL) and upper (UL) lip. Fig. 8 Area functions for seven of the EP oral vowels. They were grouped in anterior, central and posterior, with higher vowels at the top. From the top, [i], [e], [ɐ], [a], [u], [o] and [ɔ]. In the area functions, information regarding the constriction point (distance from reference, at basis of C5 vertebra, and area) is included. Note the difference in y-axis scale for the three last area functions, with a maximum twice of what was used in the others. Fig. 9 Results for the 5 EP nasal vowels: from the top, [ĩ], [ e ˜ ], [ ɐ ˜ ], [ũ] and [õ]. In each row, are presented, from left, the midsagittal image with superimposed contour and area function. In the area functions, information regarding constriction point (distance from reference point and area) is included. Fig. 10 Midsagittal vocal tract profiles comparisons for nasal vowels and their possible oral counterparts: (a) superimposition of [i] (solid line) and [ĩ] (dash-dotted); (b) superimposition of [e] (solid line), [ e ˜ ] (dash-dotted) and [ε] (dotted); (c) superimposition of [a] (solid line), [ ɐ ˜ ] (dash-dotted) and [ɐ] (dotted) and (d) superimposition of [o] (solid line), [õ] (dash-dotted) and [ɔ] (dotted). Fig. 11 Area functions comparison between EP nasal and oral vowels. On the left, a plot of area functions; on the right the absolute differences between nasal vowel and oral counterparts. Fig. 12 Boxplots of VPOQ for oral vowels, nasal vowels, and consonants. Dots represent the VPOQ average value. Fig. 13 Results for the EP nasal consonants. From the top, bilabial [m], dental [n] and palatal [ɲ]. All the three sounds were sustained having a reference word with the same symmetric vocalic context, the oral vowel [ɐ]. In each row the following are presented: the image with superimposed contour (at left) and area function. In the area functions, information regarding occlusion point (distance from reference point and area) is included. Fig. 14 Midsagittal contour superimposition for nasal consonants and stops with the same place of articulation. At the left, bilabials [p] and [m]; at the right the dentals [t] and [n]. The two nasal consonants were sustained having an example word with the same symmetric vocalic context, oral vowel [ɐ]. The stops are the ones produced in the [aCa] context. Fig. 15 Midsagittal contours relative to stop consonants, obtained in VCV context with the point vowels [a] (dashed), [i] (solid line) and [u] (dash-dotted). At the top row appears the bilabial unvoiced [p] (left) and the voiced [b] (right); at center appear the dental unvoiced [t] (left) and the voiced stop [d] (right); at bottom the velar stops: the unvoiced [k] (left) and voiced [g] (right). Fig. 16 Midsagittal MRI images with superimposed contour relative to EP fricative sounds. At the top row the labiodental fricatives [f] and [v]; at the center the alveolar fricatives [s] and [z] and at bottom the palato-alveolar fricatives [ʃ] and [ʒ]. All were sustained having an example word with the fricative at the beginning and followed by the oral vowel [a]. Fig. 17 Midsagittal contours relative to fricatives, obtained in VCV context with the vowels [a] (dashed), [i] (solid line) and [u] (dash-dotted). At the top appears the labiodental unvoiced [f] (left) and the voiced [v] (right); in the middle row appear the alveolar unvoiced [s] (left) and alveolar voiced [z] (right); at bottom the palato-alveolar fricatives: the unvoiced [ʃ] (left) and the voiced [ʒ] (right). Fig. 18 Area functions for the fricatives [f], [s], and [ʃ] in three vocalic contexts (left) and absolute differences (right). Fig. 19 MRI images (with contours) and area functions for the EP laterals. Top 3 rows presents results for [l]: top row [l] in [lasu]; second row [ɫ] in [maɫ]; third row a comparison of the contours previously presented, on the left, and right, area function for a third context with only 3D data available, intervocalic position [palɐ]. Finally, on the bottom row, image and area function for [ʎ]. Fig. 20 Comparison of the three area functions obtained for EP lateral [l]. Three contexts are represented: beginning of word and syllable (solid), end of word or syllable (dash-dotted) and in syllable onset but intervocalic (dotted). Table 1 MRI sequence parameters used in imaging acquisition Parameter TSE T1 weighted (2D) 3D flash VIBE TR (time to repeat) 400ms 4.89ms TE (time to echo) 8.3ms 2.44ms ETL 15 1 FA 180° 10° FOV(x, y) [mm] 200×200 270×216 Slabs – 1 Slices per slab – 60 Slice thickness 5mm 2mm Orientation Sagittal Axial Distance factor – 0.2mm Base resolution 256mm 256mm Phase resolution 75% 60% Phase direction Anterior–posterior Right–left Phase partial Fourier – 6/8 BW (Hz/pixel) 235 350 Acquisition time 5.6s 18s NEX 1 1 Image size (x, y) [pixels] 256×256 512×416 Pixel size (x, y) [mm] 0.78×0.78 0.53×0.53 Number of measurements 1 1 Table 2 2D and 3D corpus contents including target phone and reference words (in Portuguese and respective phonetic transcription using IPA phonetic alphabet) used in instructing speaker Phone Word Transcr. 2D 3D Oral vowels [i] pipo [pipu] X X [e] pêca [pekɐ] X X [ε] leva [lεvɐ] X [i] devi [ d i vi ] X [ɐ] cada [kɐdɐ] X X [a] pato [patu] X X [u] buda [budɐ] X X [o] tôpo [topu] X X [ɔ] pote [ p ɔ t i ̶ ] X X Nasal vowels [ĩ] pinta [pĩtɐ] X X [ e ˜ ] pente [ p e ˜ t i ̶ ] X X [ ɐ ˜ ] canto [k ɐ ˜ tu] X X [ũ] punto [pũtu] X X [õ] ponte [ p o ˜ t i ̶ ] X X Stops [p] [apa], [ipi], [upu] X [t] [ata], [iti], [utu] X [k] [aka], [iki], [uku] X [b] [aba], [ibi], [ubu] X [d] [ada], [idi], [udu] X [g] [aga], [igi], [ugu] X Nasal consonants [m] cama [kɐmɐ] X X [n] cana [kɐnɐ] X X [ɲ] canha [kɐɲɐ] X X Fricatives [f] fala [falɐ] X [s] sala [salɐ] X [ʃ] chá [ʃa] X [v] vaca [vakɐ] X [z] zarpa [zarpɐ] X [ʒ] jacto [ʒatu] X [f] [afa], [ifi], [ufu] X X [s] [asa], [isi], [usu] X X [ʃ] [aʃa], [iʃi], [uʃu] X X [v] [ava], [ivi], [uvu] X [z] [aza], [izi], [uzu] X [ʒ] [aʒa], [iʒi], [uʒu] X Laterals [l] laço [lasu] X X [l] pála [palɐ] X [ɫ] mal [maɫ] X X [ʎ] falha [faʎɐ] X [ʎ] palha [paʎɐ] X ☆ Part of the work reported, particularly on nasals, was accepted for presentation at Interspeech 2007. Paper is entitled “An MRI study of European Portuguese nasals. European Portuguese MRI based speech production studies Paula Martins a Inês Carbone b Alda Pinto c Augusto Silva b António Teixeira b ⁎ a Escola Superior de Saúde, Universidade de Aveiro, Portugal b Dep. Electrónica Telec. Informática/IEETA, Universidade de Aveiro, 3810 Aveiro, Portugal c Dep. de Radiologia, Hospital da Universidade de Coimbra, Portugal ⁎ Corresponding author. Tel.: +351 234370500; fax: +351 234370545. Knowledge of the speech production mechanism is essential for the development of speech production models and theories. Magnetic resonance imaging delivers high quality images of soft tissues, has multiplanar capacity and allows for the visualization of the entire vocal tract. To our knowledge, there are no complete and systematic magnetic resonance imaging studies of European Portuguese production. In this study, a recently acquired magnetic resonance imaging database including almost all classes of European Portuguese sounds, excluding taps and trills, is presented and analyzed. Our work contemplated not only image acquisition but also the utilization of image processing techniques to allow the exploration of the entire database in a reasonable time. Contours extracted from 2D images, articulatory measures (2D) and area functions are explored and represent valuable information for articulatory synthesis and articulatory phonetics descriptions. Some European Portuguese distinctive characteristics, such as nasality are addressed in more detail. Results relative to oral vowels, nasal vowels and a comparison between both classes are presented. The more detailed information on tract configuration supports results obtained with other techniques, such as EMMA, and allows the comparison of European Portuguese and French nasal vowels articulation, with differences detected at pharyngeal cavity level and velum port opening quotient. A detailed characterization of the central vowels, particularly the [ ], is presented and compared with classical descriptions. Results for consonants point to the existence of a single positional dark allophone for [l], a more palato-alveolar place of articulation for [ʎ], a more anterior place of articulation for [ʎ] relative to [ɲ], and the use, by our speaker, of a palatal place of articulation for [k]. Some preliminary results concerning coarticulation are also reported. European Portuguese stops revealed less resistant to coarticulatory effects than fricatives. Among all the sounds studied, [ʃ] and [ʒ] present the highest resistance to coarticulation. These results follow the main key features found in other studies performed for different languages. Keywords Speech production European Portuguese Magnetic resonance imaging Nasals Coarticulation Nomenclature ETL echo train length FOV field of view MRI magnetic resonance imaging SSFP steady state free precession TR time to repeat VIBE volume interpolated breath hold examination FLASH fast low angle shot MPRAGE magnetization prepared rapid acquisition gradient echo NEX number of excitations TE time to echo TSE turbo spin echo (sequence) VPOQ velum port opening quotient 1 Introduction Mankind’s knowledge about human speech production and perception is still incomplete. More information is definitely needed. Recently, better techniques for measuring vocal tract configurations have become an increased research interest. Building phonetic information databases has had great relevance in fields such as speech synthesis, speech recognition, speech disorder studies, learning of new languages, etc. An area where production data are very important is articulatory synthesis, where we have been involved for more than a decade (Teixeira et al., 2005). These anthropomorphic synthesizers demand large amounts of detailed anatomic-physiological information, if possible in 3D, and their variation in time (dynamic information). For European Portuguese (EP), not much information is available. To compensate this lack of information, the objectives of the present study are: (1) to provide vocal tract configurations during (sustained) production of all the EP sounds (excluding taps and trills); (2) to perform comparisons between different sound classes; (3) to obtain direct area functions from a great part of the EP sounds; (4) to have a preliminary approach on coarticulation in stops and fricatives and, (5) due to the nature of the research team, to develop acquisition and segmentation techniques with application in the field of speech production. This paper is structured as follows: this first section introduces the problem, presents the most common anatomic-physiological measurement methods for speech production studies, describes the EP relevant specificities, coarticulation and related work in MRI application to speech production studies; Section 2 describes image acquisition and corpus; Section 3 describes image processing; Sections 4 and 5 present our results, separated into vowels and consonants. All the phonetic considerations made in this paper rely on static articulations that might be different from continuous speech articulations. The paper ends with a discussion of the results presented in earlier sections, and with the main conclusions that can be extracted from them. 1.1 Measurement methods Nowadays, the common methods found in the speech research literature to acquire anatomic-physiological information directly are: electromagnetic midsagittal articulography (EMMA), electropalatography (EPG), and magnetic resonance imaging (MRI). EMMA provides valuable kinematic data relative to different articulators (lips, tongue, jaw, velum) with good temporal resolution. However, some drawbacks can be pointed out: the acquired data are, in the majority of available systems, two dimensional and limited to the trajectories of some articulator fleshpoints (Hoole, 1993; Hoole and Nguyen, 1999); the process is invasive and articulation may be affected by the sensors. EPG measures only the linguopalatal contact and its variation on time, being difficult to make well-fitted pseudo-palates, which in turn interfere to some extent with speech production (Stone, 1999). MRI, the technique on which we will focus in this study, has some potential advantages: it provides a good contrast between soft tissues, allows 3D modeling and covers the vocal tract in all of its extension (Baer et al., 1991; Alwan et al., 1997; Narayanan et al., 1997; Narayanan et al., 2004). This last advantage is of special interest in the study of the pharyngeal cavity, as it is not accessible through EMMA or EPG. Moreover, it is non-invasive and considered as safe. Its disadvantages are related to the absence of the teeth in the images, due to their lack of hydrogen protons; the acquisition technique, in which the speaker must be lying down during speech production. This position can have some influence, for instance, on the tongue posture (Tiede et al., 2000; Engwall, 2003), but this drawback can be considered acceptable. The relatively low temporal resolution achieved, even with the fastest acquisition techniques, is a limiting factor (Narayanan et al., 2004). The noisy acquisition environment and the reduced acoustic feedback, due to the use of headphones, are also MRI disadvantages. The MRI technique has already been used for the study of several languages: British English (Baer et al., 1991), American English (Narayanan and Alwan, 1995; Stone et al., 1997; Narayanan et al., 1997; Narayanan et al., 2004), French (Demolin et al., 1996; Badin et al., 1998; Serrurier and Badin, 2005), Swedish (Engwall and Badin, 1999; Ericsdotter, 2005), Japanese (Takemoto et al., 2004), German (Kröger et al., 2000; Hoole et al., 2000; Mathiak et al., 2000), Tamil (Narayanan et al., 2004), and Akan (Tiede, 1996). For EP, one of the authors was involved in the creation of the first and, to the best of our knowledge, unique EMMA database focused on nasals (Teixeira and Vaz, 2001). Also, there are no EPG databases for EP, and there is only one partial MRI study (Rua and Freitas, 2006). For Brazilian Portuguese this information is also scarce. An MRI based study of nasals was performed recently by Gregio (2006). 1.2 European Portuguese “The characteristics which at first hearing distinguish the pronunciation of Portuguese from that of the other Western Romance languages [are]: (a) the very large number of diphthongs (…); (b) the large number of nasal vowels and nasal diphthongs; (c) frequent alveolar and palatal fricatives (…); (d) the extremely ‘dark’ quality of the common variety of l-sound” (Strevens, 1954, p. 6). Despite its similarities to Spanish, both in vocabulary and grammatical structure, Portuguese differs considerably in its pronunciation (Strevens, 1954). In EP there is a maximum of nine oral vowels and 10 oral diphthongs (Cruz-Ferreira, 1999). Oral vowels are usually divided into: anterior ([i], [e], and [ε]); central ([a], [ɐ], and [ ]); and posterior ([u], [o], and [ɔ]). The most problematic vowel is [ ] with descriptions going from the schwa to a high central vowel or even, as proposed by Cruz-Ferreira (1999), a configuration close to [u]. EP has five nasal vowels ([ĩ], [ e ˜ ], [ ɐ ˜ ], [õ], and [ũ]); three nasal consonants ([m], [n], and [ɲ]); and several nasal diphthongs and triphthongs. Despite nasality being present in most of the languages of the world, only about 20% of such languages have nasal vowels (Rossato et al., 2006). There is some uncertainty in the actual configurations assumed by the tongue and other articulators during EP nasals production, namely nasal vowels. This is particularly relevant for mid vowels where the opposition between mid-low and mid-high, present in the oral vowels set, is neutralized (Teixeira et al., 2003). This neutralization allows the oral articulators to rearrange, leading to associate each nasal vowel to several possible oral counterparts (Teixeira et al., 2003): nasal vowel [ e ˜ ] relates to [e] and [ε]; [õ] relates to [o] and [ɔ]; and [ ɐ ˜ ] can be more open than [ɐ] or produced with an oral configuration similar to [a]. Note that [i] and [u] are considered to be the oral counterparts of [ĩ] and [ũ]. Also, some phonetic studies point to the existence of differences related with production of EP nasals relative to French (Teixeira et al., 1999; Teixeira and Vaz, 2001). In this work, we return to the same challenging topic, using MRI as the data acquisition method. In EP six fricative consonants are described (Jesus and Shadle, 2002). Three are produced with vocal fold vibration (voiced fricatives [v], [z] and [ʒ]) and three produced without vibration (unvoiced fricatives [f], [s] and [ʃ]). Sounds [v] and [f] are produced with a constriction point induced by the contact of the lower lip and upper incisor (labiodental), [s] and [z] are fricatives produced with approximation of the tongue tip or blade to the alveolar region. Finally, [ʃ] and [ʒ] are produced in the palato-alveolar area. Phonologically EP has two laterals, /l/ and /ʎ/. The former is produced with contact of tongue tip or blade in the alveolar ridge, the latter produced with a central occlusion between the most anterior tongue dorsum and the anterior palate (palatal consonant). For the apical lateral /l/, in accordance with EP most frequent descriptions, two allophones are considered: one, non-velarized light or clear [l], occurring in syllable onset; the second, occurring in coda or in absolute word-final position, considered a “velarized” [ɫ] and corresponding to the descriptions of the English dark [l]. During the production of this dark allophone, a second and posterior constriction, originated by tongue back raising towards the velum, is considered (Ladefoged and Maddieson, 1996). However, Andrade (1999) found in three Lisbon speakers, evidence that this “velarization” can also occur in syllable onset. This was also described, much earlier, in older EP phonetic descriptions (Strevens, 1954). Also, Recasens and Espinosa (2005), based on acoustic data stated that EP, together with Russian and Leeds British English, belong to a group of sound systems where /l/ presents the same realization in word initially and word finally. 1.3 Coarticulation The term coarticulation has been introduced by Menzerath and Lacerda – a Portuguese Phoneticist – in 1933 (Kühnert and Nolan, 1999). Although it could be simply defined as “the articulatory or acoustic influence of one segment or phone on another” (Magen, 1997) it is a complex and difficult subject. Many theories and models have emerged to explain coarticulation but some doubts still persist. There are, however, some accepted facts: coarticulation was observed in almost all languages, being a universal phenomenon, but coarticulatory effects vary from one language to another Manuel (1999, p. 180). Recent theories of speech production consider that coarticulation plays a central role and that is essential to take coarticulatory effects into account in both speech production models and speech synthesis. Important concepts such as “coarticulation resistance” and “degree of articulatory constraint” (DAC) were introduced to explain why coarticulatory effects are different in different sounds (Recasens et al., 1997). To give a complete picture of coarticulation one should consider lingual, jaw, labial, and laryngeal coarticulation. An extensive review of the subject can be found in (Hardcastle and Hewlett, 1999). Several exploratory techniques are referred as important tools when studying coarticulation, such as EMMA (Hoole, 1993; Hoole and Nguyen, 1999) or EPG (West, 2000). MRI has also been used for the same purpose as described in (Stone et al., 1997; Engwall and Badin, 2000; Stone et al., 2001; Engwall, 2003). We are not aware of any MRI coarticulation study for EP. 1.4 MRI in speech production studies: an overview MRI evaluation of the vocal tract configuration is definitely not a recent issue in the field of speech production. One of the pioneer studies in this field was performed by Baer et al. (1991) for British English. Although it is not the first study that employs MRI as an imaging tool, it was the first that allowed extraction of valuable 3D information related with English vocalic sounds (Engwall, 2002). Traditionally, studies involving MRI were called static (2D and 3D), or dynamic/real-time, although different terminology has been used by different authors, as has been pointed out and explained by Narayanan et al. (2004). From static (2D and 3D) studies, with images acquired during sustained production of sounds, midsagittal profiles and distances, cross sectional areas, articulatory measures, vocal tract area functions, and 3D visualizations were obtained (Baer et al., 1991; Story et al., 1996; Engwall and Badin, 1999). The acquisition time, during which articulation must be sustained, is nowadays substantially shorter in most recent studies, when compared with the first MRI evaluations, which reflect technical advances in the field of MRI technology. This fact leads to a better image quality, since image artifacts, due to movements, contributes negatively to the sharpness and image contrast in a MRI image. For real-time studies, recent improvements in temporal resolution are encouraging, but not yet enough to obtain dynamic information relative to some articulators (e.g. tongue tip or velum opening/closure during nasals sounds), or to study more demanding sounds in terms of temporal resolution as happens with stops (Mathiak et al., 2000). The number of speakers participating in studies with published results is not high, varying between one (Greenwood et al., 1992; Story et al., 1996; Yang, 1999; Engwall and Badin, 1999; Shadle et al., 1999; Kröger et al., 2000; Serrurier and Badin, 2005), two (Baer et al., 1991; Tiede, 1996; Ericsdotter, 2005), four (Narayanan et al., 1995; Narayanan et al., 1997; Alwan et al., 1997; Demolin et al., 1996; Demolin et al., 2003) and five (Dang and Honda, 1994). This fact reflects the high costs of MRI equipment and the access constraints imposed by the use, in the majority of the studies, of hospital diagnostic equipment. There are studies for different languages and for different classes of sounds. In the next paragraphs, one for each class of sounds contemplated in the present study, a brief review of studies, having a phonetical speech production point of view, is made. Oral vowels were studied for American English (Story et al., 1996), British English (Baer et al., 1991), Akan (Tiede, 1996), Japanese (Dang and Honda, 1996), French (Demolin et al., 1996), German (Hoole et al., 2000) and Swedish (Engwall, 1999; Ericsdotter, 2005). Common results are MRI images, distances, segmentations, 3D vocal tract and tongue visualizations, and area functions. Nasal vowels were mainly considered for French (Demolin et al., 1998; Demolin et al., 2003; Engwall et al., 2006). In (Demolin et al., 1998) the results presented are transversal MRI images, cross sectional areas, comparisons between oral and nasal vowels, and 3D reconstructions of the pharynx and of the nasal tract. In 2002, Delvaux et al. (2002), obtained from MRI images the articulatory contours. Recently, Engwall et al. (2006) published MRI images, nasal and oral areas and a relative measure for the velum port opening, VPOQ. Dang and his colleagues (Dang et al., 1994; Dang and Honda, 1994) studied nasal consonants for Japanese (Story et al., 1996) for (American) English, and (Hoole et al., 2000) for German. Japanese studies presented several measurements of the three-dimensional geometry of the vocal tract. In (Story et al., 1996) area functions and vocal tract visualizations are presented. Hoole and coworkers provided tongue contours and respective deformations based on a two-factor tongue model. The study lead by Story et al. (1996), included some investigation on American English stops, through the observation of 3D vocal tract visualizations and respective area functions. Hoole et al. (2000), in 2000, acquired MRI coronal, axial and sagittal volumes of long German vowels and alveolar consonants. Kim (2004) studied Korean coronal stops and affricates. She presented midsagittal MRI images, tongue contours, and some measurements of movements, distances, and widths. Fricatives were studied for a broad number of languages, such as English (British and American), Swedish, German. The oldest study, by Shadle et al. (1996) in 1996, showed only midsagittal MRI images. Mohammad et al. (1997) developed a new method to acquire MRI dynamic images. Jackson (2000), in his work on acoustic modeling, used MRI to draw contours and area functions. Narayanan and Alwan (2000) used vocal tract area functions obtained from MRI images of voiced and unvoiced English fricatives to delineate hybrid source models for fricative consonants. Engwall and Badin (2000) presented midsagittal contours, 3D vocal tract shapes and investigated coarticulatory effects in Swedish fricatives. Hoole and his team (Hoole et al., 2000) focused on the study of the tongue. To gather data on laterals, and to the best of our knowledge (Bangayan et al., 1996; Narayanan et al., 1997; Gick et al., 2002) (for American English) and (Hoole et al., 2000) (for German) used MRI. They presented coronal MRI images, midsagittal segmentations of the vocal tract, area functions, 3D vocal tract and tongue visualizations. 2 Image acquisition 2.1 MRI acquisition The MRI images were acquired using a 1.5T (Magneton Simphony, Maestro Class, Siemens, Erlangen, Germany) scanner equipped with Quantum gradients (maximum amplitude=30mT/m; rise time=240μs; slew rate=125T/m/s; FOV=50cm). Neck and brain phased array coils were used. Two different types of acquisitions were performed, 2D static and 3D static, whose acquisition sequence parameters are shown in Table 1 . For 3D, instead of exciting a series of 2D slices in different planes (coronal, coronal oblique and axial) as reported by other authors in the field (e.g. Badin et al., 1998; Engwall and Badin, 1999) we performed a volumetric acquisition, by exciting a volume of spins in the axial plane (from above hard palate level to C5–C6 level), using a three-dimensional Fourier Transform (3DFT) sequence. This acquisition has some advantages when compared with 2D acquisitions: the possibility of having a reduced slice thickness (in our study we obtained an effective slice thickness of 2mm) contributing to obtain high resolution images with a reduced acquisition time; signal to noise ratio (SNR) is usually high with a 3D excitation; possibility of reslicing in any direction with different slice thickness, a variable number of slices and different orientation with a quality superior that can be obtained with 2D acquisitions. When 3D visualizations are required, this method allows the utilization of faster and direct segmentation tools (e.g. itk-SNAP) to extract tract configuration. Establishing some trade-offs, we obtained at least the same amount of data as reported in the referenced studies, with a reasonable spatial resolution, but decreasing to less than half the acquisition time (18s). Bidimensional acquisitions resulted in images of 256×256pixels and a resolution of 0.78mm/pixel in both directions. For 3D, the volume has 512×416×60 voxels and resolution of 0.53mm/pixel in plane and 2mm resolution in the z-direction. 2.2 Corpus The corpus comprises two subsets, 2D and 3D corpus, acquired using two different acquisition techniques. In both sets, the sounds are artificially sustained (vowels) or holding the articulation (stops) during the period of image acquisition, as already done in a similar way for other languages (Story et al., 1996; Demolin et al., 1996; Engwall and Badin, 1999). Although with some technical differences, our 2D and 3D corpus were inspired by the studies of (Demolin et al., 1996) for French, Badin et al. (1998) also for French, and Engwall and Badin (1999) for Swedish. As in Engwall and Badin (1999), we decided to obtain a large corpus with only one speaker rather than to obtain a small set of items relative to vowels or classes of consonantal sounds with a higher number of speakers. The reason for this option relies on the scarcity of MRI information for EP. Both approaches present advantages and limitations as emphasized by Engwall and Badin (1999). 2.2.1 2D corpus The main goals were: to obtain MRI static images of the vocal tract during the production of all EP vowels and consonants allowing to extract midsagittal contours; to have articulatory measures; and to measure midsagittal distances. Each sound of the 2D corpus (Table 2 ) was pronounced and sustained during the acquisition time (5.6s). To help the speaker, a reference word, containing the target phone, was presented before launching the sequence, using the intercom (e.g. “please say [a] as pronounced on [patu]”). This procedure was used for oral and nasal vowels, nasals, laterals and fricatives with one sample of each sound. For nasal vowels this process does not take into consideration the reported dynamic movement between an oral position towards a nasal position (see for example Teixeira and Vaz, 2001; Teixeira et al., 1999). The acquired image should be considered as more representative of nasal vowels when produced in isolation and of the initial and medial configuration during nasal vowel production. To allow a coarticulation study, stops and fricatives were also acquired on a vowel–consonant–vowel (VCV) symmetric context (non-sense words), with V being one of the cardinal vowels [a, i, u]. Note however that, due to recording duration constraints and the secondary role of coarticulation study in the present paper, only stops and fricatives were considered here. During this recording sequence the speaker was instructed to perform the VC-transition, then to sustain the consonant during acquisition time, and finally perform CV transition. Acquisition was started as soon as the speaker started producing the consonant; the speaker used the acquisition noise to make the final transition. The speaker had the opportunity of having a small training phase before the image acquisition session. 2.2.2 3D corpus For this corpus the main purposes were: (1) to obtain tridimensional information, such as vocal tract area functions, and (2) to complement the 2D information with lateral information. The main challenge with this corpus was to obtain a large volume of data within the smallest acquisition time. As already explained (Section 2.1), instead of choosing a set of directions and acquiring a fixed number of slices, we used a 3D sequence. Despite the reduction in acquisition time, each 3D item takes around 18s. To keep the recording session reasonably short (actual duration was of approximately 90min), in the 3D corpus we only contemplated the sounds for which 3D can provide new important information (as for the laterals) or are reported to be somehow characteristic of Portuguese. This explains the non-inclusion of stops. For oral vowels and fricatives, only a subset of the 2D corpus was considered. The procedures followed in this corpus were similar (excluding acquisition time) to the procedures already detailed for the 2D subset. The corpus actual content, using the IPA phonetic alphabet (International Phonetic Association, 1999) can be found in Table 2. Although Alwan et al. (1997) acquired sustained productions of American English rothics, EP taps and trills were not considered in this static study. We anticipated as particularly problematic to record information on [r,R] due to the several opening/closing movements involved. They have been included in a real-time MRI corpus (not presented in the paper). 3D high resolution sagittal images of the nasal and oral tracts of the speaker at rest (no phonation) were moreover acquired. Finally, as calcified structures such as bone and teeth are not observed on MRI images, dental arches were also obtained, according to the technique described by Takemoto et al. (2004), but using water as an oral MRI contrast agent. These images were however not exploited in this study and are planned to be used in following studies to improve our results (see Section 7.1). 2.3 Speaker For the 2D and 3D corpus subsets, analyzed in the present study, only one speaker was recorded (PAA). The speaker selected was an EP native speaker, male, 25years old, 180cm height, 70kg, from the north of the country, and with both vocal and singing training. The speaker had, at the time of the study, no history of speech or language disorders. During the acquisition of all the sequences involved in the study, the speaker used headphones to respect safety recommendations related with noise levels, and also to allow for better communication. The reduced auditory feedback due to the use of headphones represents a limitation to the study, with possible negative impact on speaker’s articulation. As far as positioning is concerned, the speaker was lying in a comfortable supine position. Head and neck phased array coils were used and the speaker’s head was fixed with regular foams and cushers. The speaker’s head movement was later evaluated, in the 2D corpus, by analysis of the coordinates of one manually marked point supposed to be fixed in the reference coordinate system, the anterior arch of C1. Maximum movement from average (including the error of the manual marking process) was 1pixel (corresponding to 0.78mm) in the anterior–posterior direction and 3pixels (2.34mm) in the other direction. These results support our assumption that speaker’s movements were negligible. 3 Image processing The viability of a large MRI database is determined by the existence of a reliable and fast segmentation method, with low human interaction. This is particularly relevant when using real-time MRI, where the number of images to process is very large. The study of the robustness of the segmentation method is also very important. We need to make sure that the contours generated are truthful enough to represent the vocal tract configuration of the sound being produced. The contours cannot contain errors that may lead to a misinterpretation and/or confusion of the sound with another one. This can be evaluated with a metric called the Pratt Index (Santos et al., 2004). All image analysis operations were performed in Matlab, version 7.0.1. The code used was specially implemented by one of the authors for use in this work. Exception is made for the live wire routine, developed by Chodorowski et al. (2005). We were able to obtain 2D contours, articulatory measures, area functions, quantification of the velum port opening, and 2D/3D visualizations of the vocal tract. To achieve these goals, the image analysis process included mainly: (1) 2D segmentation of the vocal tract, (2) 3D segmentation of the vocal tract and area extraction of the sections, and (3) computation of the velum port opening quotient (VPOQ). 3.1 2D corpus The 2D segmentations were made with the region growing method (Adams and Bischof, 1994). We started by manually placing a seed inside the vocal tract which expanded until it reaches the vocal tract wall. This expansion is based on grey level comparison between the mean grey level value of all the pixels already marked as inside the vocal tract and the neighbour pixels of the contour of the region already delimited. The stop criterion is based on a maximum difference threshold between the pixel being tested and the mean value of all the pixels assumed to belong to the region of interest. To assess reproducibility of the process, 100 contours were generated (each set takes about 35min with the current implementation) with a randomly placed seed inside the vocal tract, for each image. Each contour was compared with the mean contour (chosen as reference contour). Comparisons between contours were made with the Pratt Index (abbreviated as PI) (Santos et al., 2004), a distance between two contours defined by: PI = 1 N ∑ i = 1 N 1 1 + α d i 2 , where N is the number of corresponding points between contours, d i is the distance between two corresponding points, and α is related to the contour size. Based on one of the authors’ previous work on other types of images (Santos et al., 2004), α = 1 / 9 . Corresponding points between contours are obtained as follows: first contour with the smaller number of points is chosen; for each point of this contour, the closest point in the other contour is the correspondent point. This index has its range in the interval [0,1], where 1 means that the two contours are equal. The PI was also used to compare images of different sounds. In this case, we retained 101 PIs for each pair: the PI calculated between the two mean contours (resulting from the process described above) and the 100 PIs resulting from comparison of the contours corresponding to different seeds. As no order effect was anticipated, the 100 contours for each sound were compared with the contours of the other image by their order of calculation. Fig. 1 , presents, separately, the results obtained for oral vowels, nasal vowels and consonants, showing that the region growing segmentation method is robust to changes in the seed (low intra-variability). The corresponding PIs are close to 1, having as a minimum the value 0.84. Also interesting, for validating the process, is the comparison between the PIs calculated for the contours obtained for one sound (intra-variability) and the PIs obtained for different sounds (inter-variability). Fig. 2 presents these results. The 95 % confidence intervals (Sachs, 1984; Bryman and Cramer, 2001), calculated using SPSS, are: CI p [ 0.92 ⩽ Intra ⩽ 0.96 ] = 95 % for the intra-variability, and CI p [ 0.44 ⩽ Inter ⩽ 0.49 ] = 95 % for the inter-variability, resulting in a statistically significant difference between the variability due to the segmentation starting points and the differences due to different sounds. All 2D sagittal images were also manually marked with the following relevant points (Fig. 3 ): highest position of tongue dorsum (TD); tongue tip (TT); tongue root position at the C3–C4 vertebral level (TR); jaw height, using the root of lower incisors (JH); lower lip highest and most anterior position (LL); and upper lip lowest and most anterior position (UL). TR is the intersection with tongue contour of an horizontal line passing through C3–C4 level. Note that all TR measures have therefore the same vertical coordinate value and that the discrepancy observed in Fig. 7b is around 1mm and can be ascribed to the general process accuracy. We used as origin the lower left image point, and assumed that the speaker movement is not relevant. A different reference point could easily be chosen. 3.2 3D Corpus For the volumes, we first segmented the vocal tract in the midsagittal slice using the semiautomatic technique live wire (Chodorowski et al., 2005). Next a (fixed) gridline was applied and its intersections with the contour obtained. Middle points between the intersection in the two contour parts make our first approximation to the centerline. The centerline is then upsampled and smoothed. Then the volume was resliced according to a phoneme-adapted grid with planes oriented normally to the centerline. Each slice was also segmented using the live wire technique. We opted to use a number of slices similar to the used in other studies, 45 slices, covering all the oral tract. Although having a non-isotropic voxel, which is homogenized by a linear interpolation, we believe that with this method we will obtain more realistic data. The live wire segmentation approach is based on optimal search strategies over graphs built upon regional pixel maps defined on the neighbourhood of seed points determined by the user. This is a fully semiautomatic approach taking advantage of the unsurpassed human capacities for object recognition and delineation. Typically, the user starts segmentation by choosing an initial point (seed) on the boundary of the object of interest. Then, the algorithm computes the minimal cost path between the seed and the current position of a pointing device (mouse pointer). The criterion for minimal cost is often the integral of pixel intensities along a path. This minimal cost path is rendered continuously (the live wire paradigm) as a partial contour and if the user considers this partial contour as acceptable then he can proceed and define the next seed point. After a minimum set of seed points the boundary of the target object, not necessarily closed, should be completely delineated. Relying on the user pattern recognition capabilities, the live wire approach offers a sequence of locally optimal contours and it is often the segmentation technique of choice to deal with difficult images with diffuse targets and cluttered backgrounds. This segmentation technique was adopted due to its better performance in the lower image quality of the 3D resliced images, when compared with the region growing technique used for 2D Corpus. As can be observed in Fig. 4 , each resliced plane will have an orientation perpendicular to the centerline of the vocal tract. The bottom part of the vocal tract is usually easy to segment in these resliced planes, but some difficulties were found in the segmentation of the oral cavity. For validation purposes, a sample of the 3D segmentations was visually evaluated by two experts. Difficulties in observing larynx area, due to 3D aliasing, motivated the use of a reference point for our area functions at the basis of C5 vertebral body. Thus, in the obtained area functions, x-axis represents the distance from this reference level towards the lips, representing 0 the basis of C5 and not the larynx position. As the basis of C5 was marked separately from the process of area function determination, it is possible that area function started after this reference point. We also did not put much effort into improving segmentation of this lower part of the pharynx, not forcing the centerline to go as close as possible to the larynx position. We preferred to concentrate on the other parts of the area functions. However, this imprecision around glottis should be improved in the future, leading to more accurate area function lengths. The VPOQ was computed in a similar way to Engwall et al. (2006). In this method, we identified the first slice (from the glottis to the lips) where both the oral and nasal cavities can be seen. We then chose that slice and the next four and measured the area of the oral and nasal passages. Mean VPOQ was calculated as the mean of the quotients between the nasal and oral areas, for the five slices. In Fig. 5 the first oblique slice is shown (counting from the glottis to the lips) where both the oral and the nasal cavities are visible. 4 Results I: vowels We start this study with the analysis of the oral vowels. After we present our results for nasal vowels. At the end of the section a comparative study of nasal and oral vowels is also presented. 4.1 Oral vowels We present the MRI images with superimposed contours for the nine oral vowels in Fig. 6 . Vowels are arranged according to their phonetic description, high vowels at the top and posterior vowels to the right (in agreement with orientation of our images, with lips to the left). The corresponding articulatory measures (TD, TR, TT, JH, UL and LL) are presented in Fig. 7 . The area functions are presented, separately, in Fig. 8 . The following descriptions were based on all the information available, particularly in the parameters presented in Fig. 7. 4.1.1 Anterior oral vowels Regarding the tongue highest point (TD), [ε] is produced with the lowest position of TD; [i] with the most raised and anterior position; [e] in an intermediate position in both dimensions, being closer to [ε] in the anterior–posterior axis. Looking at the [i] and [e] area functions, Fig. 8, (corpus does not include 3D for [ε]) the point of smallest area is more anterior for [i], confirming TD parameter information. In the area functions it is possible to see that for [i] the constricted area is a few centimeters long while in [e] the obstruction zone is much more restricted. It has also been observed, that the most posterior tongue position (TR) is more anterior in [i] than in [ε], contributing to the increase of the pharyngeal cavity and the reduction of the oral cavity. The wide pharyngeal region for [i] is indeed clear on area functions. The JH is lower in [ε] and higher and more anterior in [i]. The TT vertical position increases from [ε] to [i], being [e] closer to the [i]. The distance between [e] and [ε] is almost twice the distance between [e] and [i]. In the horizontal direction differences are smaller: [i] and [ε] present very similar TT horizontal positions; [e] has a slightly posterior position. Regarding lip configuration, the results are different for the upper and lower lip. The three anterior vowels present quasi-identical UL parameter values. For lower lip (LL): [i] presents a higher position; protrusion (x-axis position) is not very different for the three vowels; differences are mainly in the vertical position, being [i]–[e] and [e]–[ε] distances similar. For each one of the three configurations, the velum is raised, not having a significative position alteration among the three vowels. In the region of the glottis there is no evidence, in the sagittal plan and for this speaker, of alterations between the three vowels. In terms of the similarity of contours, with the analysis of PI, [e] is closer to [i] (PI=0.76) than to [ε] (PI=0.72). Despite the very similar values of PI in both cases, non-parametric statistical tests (Mann–Whitney) confirm the difference as significative ( p < 0.001 ) . 4.1.2 Central oral vowels The vowel [ ] (high vowel) is produced with the tongue dorsum (TD) in the highest position inside of the series; followed by [ɐ] and [a] (low vowel). All three have similar x-coordinates for TD. Comparing with the anterior vowels, TD is always lower for central vowels. The highest value for central vowels (10.9) is clearly lower than the lowest position for anterior vowels (11.3). For this series of vowels, TD is not directly related with maximum constriction position, area function provides further insight. Our data show [a] as having its smallest area in the pharyngeal region. The tongue root (TR) is more anterior during the production of [ ] than of [ɐ] or [a]. [a] is also more posterior than all three anterior vowels. In terms of area function, major differences between [a] and [ɐ] are in the pharyngeal region. The jaw position (JH) is lower and posterior for [a] and higher and anterior for [ ]. There is an overlap of the opening values with anterior vowels. Nevertheless, [a] is produced with the lowest position in the combined anterior–central set of vowels. The tongue tip (TT) position follows the same pattern observed for TD, with a correlation between the points. The lower and upper lips positions can be considered as nearly similar for [ɐ] and [ ]. In [a], the lower lip is lower, around 7mm, and, also, more posterior (5mm). This may be related to mandibular position. From contours superimposition, not shown in this paper, the velum presents a more anterior position in the vowels [ɐ] and [ ] than in [a]. Non-parametric statistical tests (Mann–Whitney) showed: as non-significantly different the PIs obtained for the comparisons of [ɐ] with [ ] and [ɐ] with [a]; as significantly ( p < 0.001 ) higher the similarity of these two comparisons than similarity between [ ] and [a]. 4.1.3 Posterior oral vowels It can be observed in Figs. 6 and 7 that vowel [u] is produced with the highest TD position amongst the three posterior vowels, followed by [o] and [ɔ], with the lowest and more posterior position. Compared to anterior and central vowels, posterior vowels are produced with lower TD than the anterior series. Only [a] is produced with lower TD than the lowest posterior ([ɔ]), and only with 3mm difference. When compared with anterior vowels we observe that posterior vowels have, generally, lower TD position, except for [ε], which is slightly lower than [u]. Comparing posterior and central vowels, it can be observed that TD for [u] and [o] is higher than the value for the three central vowels. In the area functions, the point of maximum constriction follows the same tendency of TD parameter to lower from [u] to [ɔ], moving downward in the pharyngeal region. Tongue root position on sagittal images also confirms a more posterior position for [ɔ] than for [u] and [o]. The difference between [u] and [ɔ] is about 1cm. The tongue back position is closer to the velum in [u] and [o], while in [ɔ] is directed towards the pharyngeal wall. This dorsovelar orientation for [o] was an unexpected finding since this oral vowel is generally described as being produced with tongue back oriented towards the pharynx (e.g. Morais Barbosa, 1994, p. 53). From midsagittal profiles, corroborated from area functions values, an increase of oral cavity dimension from [u] to [ɔ] is evident, associated with a decrease of the pharyngeal cavity dimensions. Comparing TR positions for anterior and posterior vowels (Fig. 7b) we can observe a trend for anterior vowels to have more anterior TR positions, but with an overlap of the two classes (e.g. [ε] is more posterior than [o]). The jaw (JH) is lower in the production of [ɔ] than in the production of [o] and [u], these two vowels being produced with JH respectively 5mm and 8mm above. For tongue tip (TT) we notice a similarity between [u] and [o], both with TT more posterior and higher than [ɔ]. When comparing with the two previous series, in posterior vowels the range of values for TT is larger, both in the horizontal and vertical dimensions. While for central and anterior vowels TT has a maximum range of 0.4cm in the horizontal and 1.3cm for vertical, the ranges are 1.0cm and 1.8cm for posterior vowels. Also relevant to this series is the variation of lip position, particularly protrusion. Protrusion is important for [u] and [o]. For [ɔ], lower lip protrusion is smaller and similar to the highest value obtained in previous series (for [ ]). When compared with anterior and central vowels, the difference is marked, as expected, since in EP only posterior vowels are rounded. From the superimposition of contours (not included in the paper), it can be observed that the velum is in a lower position in the production of [ɔ] than in the other two posterior vowels. Area functions for [u] and [o] present a similar pattern, contrary to [ɔ]. Pattern differences are more pronounced at oral cavity level. Analyses of the PI, confirm this tendency, as PI between [u] and [o] mean contours is 0.77, being 0.73 between [o] and [ɔ], and 0.65 between [u] and [ɔ]. Statistical tests (Mann–Whitney) confirm as significantly higher the values of the PI for the pair [u] and [o] when compared with both other two pairs ( p < 0.001 ) . 4.2 Nasal vowels Fig. 9 show the images with superimposed contours and area functions for EP nasal vowels, complementing the information presented in Figs. 7 and 10. Based on these three figures, we can observe that: • Vowels [ĩ] and [e˜] are produced with the tongue (TD) in an anterior and raised position. • Vowel [ ɐ ˜ ] has a low TD position, occupying with [õ] the lowest TD positions measured for the five nasal vowels. • Vowels [õ] and [ũ] are more posterior in terms of TD. • The jaw position, in contrast with what happens in the production of the oral vowels, presents a more restricted range of variation. For the five nasal vowels, higher and lower JH measures differ of 0.7cm while for oral vowels the difference is more than the double, 1.5cm. • The velum is open for all nasal vowels, but its height is variable with the vowel. We will study these differences, below, using 3D information. • Labial protrusion is marked in the production of [ũ] and similar to the protrusion observed in the corresponding oral vowel ([u]). 4.2.1 Nasal vs. Oral vowels In this section comparisons between oral and nasal vowels are presented. They are based on the articulatory measures of Fig. 7, the superimposition of midsagittal contours for EP nasal vowels with their possible oral counterparts (Fig. 10 ) and area functions obtained from 3D acquisitions (Fig. 11 ). For mid and low nasal vowels two oral configurations are considered. With MRI 3D information we can, for the first time for EP, compare the area functions of oral and nasal vowels. Differences between two area functions were obtained as follows: both area functions were resampled at the same positions along the x-axis, resulting in two vectors with the same length; the difference is the result of subtracting the two vectors. The vowels [ĩ] and [i] present similar configurations, the nasal vowel being produced with a higher and posterior position of the tongue body and root when compared with the oral counterpart (Fig. 10a). The TD position is close for the two vowels, being (7.4cm, 11.7cm) for the oral and (7.7cm, 11.8cm) for the nasal (Fig. 7a). The nasal [ũ] is produced with a slightly posterior and lower TD than the oral counterpart [u] (Fig. 7a). Looking at Fig. 10b, comparison of [e], [ε] and [ e ˜ ], we can observe that the contours of the vowels [e] and [ e ˜ ] are closer (PI=0.86) than the contours of [ε] and [ e ˜ ] (PI=0.69). Specifically with respect to TD position, the nasal vowel [ e ˜ ] is produced with the highest TD (Fig. 7a), this difference being however more accentuated for [ε] than for [e]. The oral [e] and the nasal [ e ˜ ] present a similar pattern at pharynx level, which is not valid to [ε], more constricted than [ e ˜ ]. Differences at tongue tip level (TT) are small between [e] and [ e ˜ ] and more pronounced between [ε] and [ e ˜ ]. The velum although opened during the production of the nasal, seems to be in a higher position than in the other nasal vowels. This tendency is observable in contours superimposition not included in the paper. From 3D information (only relative to [e] and [ e ˜ ]), we confirmed that the nasal and the corresponding oral vowel [e], have a very similar pattern on area function. Analyzing Fig. 10c, we can detect some differences. The nasal vowel [ ɐ ˜ ] is produced with a TD in a higher position than for [ɐ] and [a]. In the anterior–posterior axis, [ ɐ ˜ ] has a TD more anterior than all three EP central oral vowels, in a position similar to anterior oral vowel [ε]. The tongue root (TR) is similar for [ ɐ ˜ ] and [ɐ] and more posterior for [a]. Observing Fig. 10d, we detected that, with respect to tongue height, the nasal vowel [õ] is produced between [o] and [ɔ]. In the tip/blade region, and looking at the TT parameter, the configuration of [õ] is closer to [o] than to [ɔ]. Regarding TR, [õ] is between [o] and [ɔ]. In these midsagittal images it is apparent that velum and uvula touch the tongue back during the production of back vowels [õ] and [ũ]. For the other nasal vowels this is not observed. Midsagittal distances in the pharyngeal cavity are different in nasal vowels and their oral counterparts. As an example, [ ɐ ˜ ] has a wider upper pharynx region relative to [ɐ]. During the production of EP oral and nasal vowels, there are not noticeable differences with respect to posterior wall of the pharynx. 4.2.2 VPOQ A particularly interesting parameter to study for the nasals is the VPOQ. The results obtained for EP are presented in Fig. 12 . We can observe that: • for this speaker, the average VPOQ is always higher in the nasal vowels than in the corresponding oral ones; • [ ɐ ˜ ] presents the highest VPOQ, followed by [ũ] and [õ]; • the largest oral/nasal VPOQ difference was observed in the pair [ɐ]/[ ɐ ˜ ]; • the smallest oral/nasal difference is between [u] and [ũ]. 5 Results II: consonants In this section, relative to consonantal sounds, we start with the description of the nasal consonants, to maintain continuity with the anterior section on nasal vowels. Next, stop consonants are briefly described as they are not generally considered as significantly different from other languages. They follow nasal consonants to allow a comparison between these two related classes. Then, we present results concerning fricatives, ending with a class with some EP particularities, the laterals. As the consonants depend on vocalic context, we are limited in the description of articulatory differences. Despite the use of similar vocalic context in the words used to instruct the speaker for the non-VCV parts of the corpus (in general an [a] follows the consonant), we avoided descriptions that could be more related to the production of the vowel than to the consonant we are studying. 5.1 Nasals In Fig. 13 midsagittal MRI images, contours and area functions for the EP nasal consonants are presented. In Fig. 14 a comparison between EP nasal and stop consonants contours is presented. In these images, the different places of articulation and the open position of the velum are clearly visible. The nasal [m] is produced with lip closure, [n] is produced with tongue tip occlusion at the superior incisors, and [ɲ] is clearly produced with tongue touching the hard palate. The tongue dorsum’s highest point (TD) is more anterior for [ɲ] being similar for [m] and [n]; higher, as expected, for [ɲ], followed by [n] and finally [m]. [ɲ] is only 1mm higher than [ĩ] and 2mm higher than [i], the highest vowel TD. The tongue tip (TT), involved in the articulation of [n] and affected in [ɲ] due to the overall raised tongue configuration, obviously presents very different positions. Looking at the contour comparisons for nasal consonants and stops with the same place of articulation, in Fig. 14, the main differences occur in the (upper) pharyngeal region with a more forward position of the tongue root for nasal consonants, associated with a lower position of the velum. EP stops have a narrower pharynx when compared with nasal consonants. This difference is more noticeable in the dentals ([n] vs [t]) than in the bilabials ([m] vs [p]). For the same place of articulation, nasal consonants present a more constricted larynx than stop consonants. VPOQ for nasal consonants was already included in Fig. 12. Nasal consonants present, on average (mean=0.75), intermediate values between the nasal vowels (mean=0.82) and oral vowels (mean=0.19). 5.2 Stops In Fig. 15 , left column, we can verify that in the production of [p] there is lip closure, as expected for a bilabial stop. In the production of [t] (although teeth contour is not visible) we see an approach of the tongue tip to the dental region. In the production of [k], the articulation point does not seem clearly velar, the constriction being in the transition between the palate and the velum. Also in Fig. 15, right column, we can observe that voiced stops present configurations that are close to the unvoiced, sharing the same articulation point. This was confirmed by contour superimposition and calculation of mean differences between contours and PIs, not included. For stops sharing the same place of articulation, the glottis is more constricted for voiced than for unvoiced cognates. Pharyngeal cavity, however, is larger in voiced when compared with unvoiced counterparts. For [p] the effect is observed through the entire pharynx, being for [t] and [k] differences more evident at oro-pharynx level. The effect of coarticulation for stops is evident. For [k] the differences are more significant in the tongue tip region, since this articulator is free for the production of the vowel. For [t], the region with less variation is the one close to the place of articulation (dental), while tongue back is affected by the production of the vowel. In [p], the tongue is free for the production of the vowel, since [p] has a bilabial articulation. 5.3 Fricatives The results for EP fricatives are presented in Fig. 16 . Despite the non-inclusion of the superior incisors in the images, we can infer, through the position of the lips, that the [f] is produced through the approximation of lower lip to the upper incisors (labiodental fricative). Despite the fact that they are quite similar, our results point to an alveolar place of constriction for [s] and [z], being fricatives [ʃ] and [ʒ] produced slightly posterior. The differences for TT horizontal position between these fricatives are of only 6mm, between [s] and [ʃ], and 4mm for the other pair. The [s] production involves the tongue blade while, [ʃ] presents an apical articulation. Other differences between [s] and [ʃ] are: [s] is produced with a slightly lower TD position; the back of the tongue is more posterior in the production of [s]. The same pattern and articulation places can be observed for [z] and [ʒ]. These facts were confirmed using the superimposition of [s,ʃ] and [z,ʒ] midsagittal contours (not included). Through the analysis of the contours (not included) and their PIs, we observed that differences in configuration, for the same place of articulation and vocalic context, are not significant (in the midsagittal plane) in the unvoiced–voiced pairs. However, at the glottis level, there is a higher constriction for voiced fricatives, as already observed for voiced stops. Regarding pharyngeal cavity, there is a tendency for voiced fricatives to have a larger pharynx, but being the difference less evident than for stops. We tested to see if our process was able to distinguish between the fricatives in three different VCV contexts, where V represents one of the vowels [a], [i], or [u]. The 2D results are presented in Fig. 17 and 3D results are shown in Fig. 18 . In Fig. 17, the effect of coarticulation is evident. In [f], a labiodental fricative, we observe differences both in tongue tip and tongue dorsum, the tongue being free for the production of the vowel. In [s], there are only differences in the posterior/back portion of the tongue. We do not observe the vowel effect on tongue tip or blade, used in the production of the consonant (apical alveolar). Relative to [ʃ], the effect of the vowel in the tongue is even less visible. This sound, when compared with others in this study, presents a higher resistance to coarticulation. For the voiced fricatives, the pattern of influence of the vowel in the production of the fricative consonant is similar to that observed for the unvoiced fricatives, being higher for the labiodental [v], smaller in the alveolar [z], and being [ʒ] production practically immune to the vowel effect. Comparing the area functions and the differences between two area functions (average and maximum values), in Fig. 18, coarticulatory effects are smaller for [ʃ]. About the two other unvoiced fricatives, the most affected regions are the pharyngeal region for [s] and the oral cavity for [f]. 5.4 Laterals The EP laterals, [l] and [ʎ], are shown in Fig. 19 . Figure presents 2D information for [ʎ] and the two variants of the l-sound: [l] as in [lasu] and [ɫ] as in [maɫ]. For 3D, a third context is also included, intervocalic position ([palɐ]). In Fig. 20 we compare the three area functions obtained for [l]. The first thing to note in Figs. 19 and 20 are the null areas in the area functions in the zone of partial occlusion. This is a result of the semiautomatic image processing, that was incapable of correctly segmenting the resliced images perpendicular to the centerline. Even with this limitation, 2D contours and area functions provide useful information on EP laterals. Comparing the midsagittal profiles of the lateral [l] and [ɫ], we can verify that the place of articulation is the same for both sounds, in the alveolar/dental region. This can be confirmed both in contour superimposition and at the first point with null area in the area functions, all presented in Fig. 19. It is clear that the active articulator is tongue tip for both sounds. Analyzing the area functions for [l] (Fig. 20), in the three contexts considered, we can observe a similar area variation pattern along the tract, without significant differences. We can report a constriction point beyond the lip region, corresponding to the alveolar area; upward in direction of the glottis an increase of area function is observed. This region corresponds to palatal area. A second constriction point is observed at uvular region, which is similar in the three positions. This second constriction is related with tongue dorsum raising. More detailed analyzes of tongue configurations on resliced coronal cuts, as in (Narayanan et al., 1997; Bangayan et al., 1996), are in progress. The [ʎ] is usually described as a palatal consonant. When compared with the palatal [ɲ], [ʎ] has its occlusion point more anterior. While in the area function the occlusion starts at 11.8cm for [ɲ] (Fig. 13), for [ʎ] occlusion starts at 15.0cm (Fig. 19). This points, at least for this speaker of EP, to a more palato-alveolar place of articulation for [ʎ]. It is produced with the tongue blade, the tongue dorsum not being in contact with the palate. 6 Discussion As our main objective is related to obtaining more data regarding EP production and not to exhaustively compare our results to published descriptions of EP, this discussion will not concentrate on pointing out all the agreements and disagreements between present work and EP common knowledge in articulatory phonetics. The availability of data for only one speaker also supports this option. 6.1 Corpus, MRI acquisition and image processing Our option to address as much as possible of EP sounds with only one speaker allowed us to cover, in a first study, what for other languages was produced incrementally. The existence of data regarding the several classes of EP sounds is particularly valuable to our work in articulatory synthesis. The disadvantage of only one speaker and the unique/reduced number of repetitions are, in our opinion, more than compensated by the advantages of the possibility of making direct comparison between different classes. This was particularly useful in the case of the comparative study of nasal vowels tract configuration relative to oral vowels; comparison of palatals [ɲ] and [ʎ] exact place of articulation and comparison of coarticulatory effects between stops and fricatives. With our option for the (semi)automatic processing, the use of a direct 3D acquisition was possible. As the acquired MRI data are in a volumic layout, image processing techniques were necessary and sufficient means to create the appropriate reformatted planes for further segmentation. This additional flexibility makes it possible to obtain data in planes defined after acquisition and tuned to the objectives of the analyses. Moreover, there was a gain in the acquisition time. With this, our speaker had a much easier task and overall acquisition time was substantially reduced. The choice for a trained speaker with vocal and singing practice also contributed positively to a faster and less error prone acquisition. Some points need however improvement in the acquisition: improvement on the larynx region, sometimes affected by aliasing problems, to allow a better characterization of this zone of the oral tract; improve overall quality of the coronal images for a better study of laterals. Semiautomatic image segmentation proved to be very useful and capable of attaining reproducible results. Nevertheless, there are areas where improvements are needed: segmentation of the images in the zone of partial obstruction for laterals (not completely successful in this first approach); addition to the images of the separately acquired information on speakers’ teeth. 6.2 Oral vowels One of the most relevant results obtained in this study, relative to EP oral vowels, is concerned with central vowels height. Contrary to traditional EP phonetic descriptions (e.g. Viana and Andrade, 1996), in which [ ] is considered as high as [i] (anterior) and [u] (posterior) high vowels, we found that [ ] has, in fact, the highest TD position among the central vowels, but not so high to be considered a high central vowel. Only looking at jaw height (JH) alone we could describe [ ] as a closed vowel, similar to [i]. From an articulatory view point, the differences between the three central vowels are mainly related with tongue dorsum position and shape, jaw height and pharyngeal cavity dimensions (particularly the upper part). Amongst the three central vowels the one that is produced with the highest TD position is the [ ], followed by [ɐ] and [a]. Pharyngeal cavity dimension is also high for [ ] as the tongue dorsum is more raised and advanced in the production of this vowel, when compared with the other central vowels. Important characteristics of [a] are the very low jaw, high lip aperture and posterior position of tongue (TD and TR). The last characteristic goes against its classic classification of [a] as a central vowel, being better described as a low pharyngeal vowel. The [ɐ] is more similar to [a] than [ ] in terms of tongue shape; has an intermediate jaw opening, and presents lip aperture similar to [ ]. The [ ] appears as distinctively different from the other two vowels in the upper pharyngeal region, not presenting the characteristic narrowing of the others. These articulatory differences and characteristics of each of the 3 central vowels can be useful in clarifying their descriptions, a point of discussion in EP Phonetics. However, it is hard to generalize as our data are limited to one speaker. The dorsovelar location of the maximum constriction for the posterior vowel [o] is not in agreement with the usual articulatory description (e.g. Morais Barbosa, 1994), reporting a pharyngeal location for the maximum constriction, as for [ɔ]. Obviously, due to corpus limitation to one speaker, we cannot clarify if this is a speaker characteristic, or a more general phenomenon. 6.3 Nasals As expected, differences between nasal and oral vowels do not only concern velum lowering, but also differences in the position of other articulators (Engwall et al., 2006). The 2D results show that, at least with this speaker of EP, [ ɐ ˜ ] is markedly higher than [a]; [õ] is produced with an articulatory configuration between [o] and [ɔ]; and [ĩ] and [ũ] are produced with a height similar to the oral counterparts. These results agree in general with the ones obtained using EMMA and acoustic inference from first formant values (Teixeira et al., 2003). When compared to French nasal vowels, some differences were detected, particularly at the pharyngeal cavity level. French nasal vowels seem to be produced with a more constricted pharyngeal region (Demolin et al., 1996; Demolin et al., 2003; Engwall et al., 2006; Delvaux et al., 2002). With the exception of [ ɐ ˜ ], a central vowel that presents the highest VPOQ, the posterior vowels ([ũ] and [õ]) have a slightly higher VPOQ than the anterior ones ([ĩ] and [ e ˜ ]). The oral area is always higher than the nasal for all the sounds contemplated in our study, which implies a VPOQ smaller than 1. Although the VPOQ is smaller in orals, in our measures it was always higher than zero due to the existence of a small passage to the nasal cavity even for the production of oral sounds. This is in agreement with the fact that nasal port opening is not sufficient to have a nasal sound. However, the VPOQ is an average value dependent of the sampling process, with possible failures in detecting nasal port closure. Comparing with recent results of (Engwall et al., 2006), we verify that: the average VPOQ follows, in general terms, a similar behaviour: superior in nasal vowels than in the correspondent orals; the VPOQ values for French are significantly higher than the obtained for EP, particularly for the nasal vowels. Relative to EP nasal consonants, the VPOQ results confirmed their relative position of velum aperture, between oral and nasal vowels. New 3D information contributed to validate previous work based on velum position only (Rossato et al., 2006; Teixeira et al., 2003). Also relevant is the close proximity of TD for [ɲ], [i] and [ĩ], consistent with the historic origin of the nasal consonant [ɲ]. 6.4 Stops and fricatives Another fact that also deserves to be mentioned is related to the place of articulation of the so-called “velar” stop [k]. Contrary to the classical descriptions of [k], we observe that [k], at least for this speaker of EP, was produced in the palatal area and does not seem to be dependent on the vocalic context. Although the place of articulation of velar stops could vary with context (Morais Barbosa, 1994), being more anterior when produced in the context of anterior vowels and more posterior in the context of back vowels, this is not observed in our study. In the different contexts studied, the place of articulation is always palatal, only with noticeable differences at tongue tip and blade level. In this area the effect of the vowel is clearly observed, the tip being more anterior in the context of [i] and more posterior in the context of [u]. Further studies are needed to clarify if this context independent point of constriction for [k] is (partially) related to the acquisition procedure, quite different from continuous speech. For fricatives, [ʃ,ʒ] have the point of maximum constriction produced with the tongue tip slightly posterior relative to [s,z], but, in our opinion and using (Ladefoged and Maddieson, 1996, p. 14) information on places of articulation, still in the alveolar region. This is not in accordance with what generally is described for [ʃ], as being produced by an approach of the tongue tip to the palato-alveolar or post-alveolar regions. A more detailed study of [ʃ] articulation point, using complementary techniques as EPG, should be considered. Relative to the stridents, a great similarity in the place of articulation for [s,z] and for [ʃ,ʒ] was evident, the most obvious difference being at the level of sub-laminal cavity which is larger for [ʃ] and [ʒ] than for [s] and [z]. This difference at the level of the sub-laminal cavity can be explained by the more apical articulation for [ʃ,ʒ], as the tongue tip is raised and slightly more posterior. These results are only partially in line with previous results reported for fricatives, but for a different language (Narayanan et al., 1995). The authors reported for [ʃ,ʒ] a high tendency for a laminal articulation rather than apical, and referred to a speaker dependent variability for [s,z] with respect to apical and laminal articulations. Our results regarding a more constricted glottis region together with a larger pharynx for voiced sounds are in line with what was reported by Narayanan et al. (1995), for fricatives: a tendency for larger pharyngeal areas for voiced sounds. This fact was also previously reported by Perkell (1969) for the sibilants [s] and [z] using X-ray techniques. This constriction at glottis level together with a larger pharynx might be explained by the necessity of having muscular adjustments and adequate pressure differences to produce phonation in voiced sounds. 6.5 Laterals In laterals, the differences between [l] and [ɫ] are not significant considering both 2D and 3D information. For American English, as reported by Narayanan et al. (1997) and Bangayan et al. (1996), there are differences in the back region for light and dark versions. For EP, we found /l/ velarization not only in syllable final position, as expected, but also in syllable initial position. EP area functions (for all the contexts considered for /l/) present a similar pattern in front and back regions, which means a second constriction point independent of position in the syllable (onset or coda). These facts point to the existence of only one positional allophone for /l/, a dark, which is in line with Andrade (1999) descriptions for EP: velarization occurs not only in syllable final position but also in initial position. This is also in agreement with older descriptions, see Strevens (1954) and Section 1.2. As far as [ʎ] is concerned, our results point to a more anterior place of articulation (alveolo-palatal) instead of palatal, which is not in line with EP most frequent descriptions, already referred to in the introduction. However, Sá Nogueira (1938) has already pointed to the possibility of this consonant having a more anterior place of articulation. Our finding is also in agreement with what was reported by Recasens and Espinosa (2006). These authors referred the fact that the lateral [ʎ] cannot be exclusively articulated in the palatal area. They pointed out that Romanic Languages also present a closure in the alveolo-palatal area, that could even be alveolar. When compared with the palatal [ɲ], it is evident a more anterior articulation point for [ʎ] and a “closure fronting decreasing in the progression [ʎ]>[ɲ]” as also reported by these authors Recasens and Espinosa (2006). 6.6 Coarticulation In general, EP stops are less resistant to coarticulatory effects than EP fricative. This is in agreement with the less constrained tongue body for stops, when compared to fricatives, reported for other languages by Farnetani (1999) and Recasens (1999). Comparing the labiodental fricative [f] with the bilabial stop [p] it is observed that the effect of the adjacent vowel is greater on the stop than on the fricative of the corresponding class. However, this difference is still sharper when we compare, by e.g., the alveolar fricative [z] with the dental stop [t]. In our study, concerning the tongue blade, for the stops [t,d] and the fricative [s] there is no significant effect of the vowel in this region, although the influence is evident in the production of the stops [k] and [g]. Recasens (1999) reports that the tongue region can present different articulatory behavior as a function of its evolvement in the production of a certain configuration. It is predicted that the blade must be more resistant to coarticulation during the production of alveolar consonants [t,d,s] than on the velar [k,g]. This is also verified in our study. Among all the sounds studied here, and not considering any articulator in particular, the sounds that have the highest resistance to coarticulation are [ʃ] and [ʒ]. This fact was already observed by Farnetani (1999) and can be connected with the complexity involved in the production of these sounds, Hardcastle (1976). Recasens et al. (1997) also refers to the fact that some sounds are more constrained than others. In accordance with Kiritani (1986), we can also consider the tongue-jaw system together. We verified that velar consonants [k] and [g] in [i] context present a more anterior position of tongue blade, but this anteriorization is not evident at jaw level. Tuller et al. (1981), also stated that the height of the jaw does not change in VCV context for [t] and [f], but suffers alterations due to the vowel in [p] and [k]. In our corpus, it was verified that for [t] there is no alteration in the height of the jaw, but this is seen in the production of [f]. 7 Conclusions In this paper we present new MRI data relative to the majority of the EP sounds. Both 2D and 3D MRI data are provided. In line with other studies in the field for other languages, we obtained volumetric MRI but using a different and faster acquisition technique. Unlike other studies in this field, we have used a semiautomatic segmentation method. MRI data obtained for one EP speaker, complemented by the utilization of imaging processing techniques and analyses, was determinant to improve our knowledge on EP oral and nasal sounds, laterals, fricatives and stops. With 2D MRI data, we compared oral and nasal vowels contours, leading to more detailed information than previously possible with other techniques such as EMMA. 3D information and area functions revealed very useful for palatal sounds [ɲ] and [ʎ], characteristic of EP. This is valuable information for evolution of articulatory synthesis of European Portuguese. Also, without claiming generalization due to the single speaker limitation of the data, some interesting findings were reported for palatal consonants, central vowels and laterals. It was possible to verify, for the EP, some facts related to coarticulation already reported for other languages. These results are also interesting due to the reduced use of MRI in coarticulation studies. 7.1 Future With this study, the capacities of MRI in providing useful information on speech production, particularly for EP or in general, is far from being exhausted. After this broad study, we consider as important the following possible continuations: • Perform a formal evaluation of 3D segmentation method, not yet performed due to time limitations; • Improve the area function computation regarding speed, accuracy in the laryngeal region, and taking in consideration the teeth. Only with an improved acquisition and segmentation of the tract near the larynx will be possible to solve the current limitations on area functions length and origin; • Process the nasal tract 3D acquisition to obtain nasal tract area function; • Complement the comparisons between nasal and oral vowels with real-time MRI information. Despite useful for the characterization of EP nasal vowels, the information available for this study suffers from two important limitations: only one speaker was recorded and the variation over time of vocal tract is not available. Real-time MRI, with adequate time resolution and from several speakers, is needed to reduce the remaining doubts regarding the nasal vowels tract configuration; • Conduct specific studies addressing a sound class or set of sounds in detail, with several repetitions and a reasonable number of speakers. This can be started by studying the EP laterals for which we had interesting results, needing more data to enable any generalization; • Repeat acquisition of the present corpus with more speakers. This is necessary to solve the speaker dependent nature of the reported results. Provision to include speakers from different dialects should be considered. With information regarding several speakers and the associated contours and area functions, a search for representative shape descriptors should be investigated; • Complement the study using real-time MRI. Real-time acquisition with a corpus mainly composed of nasal sounds and trills has already been carried out, but not yet fully analysed. In this preliminary and first approach we obtained a temporal resolution close to 200ms (5frames/s). We are particularly interested in improving temporal resolution and obtaining dynamic information on articulators movements, particularly for nasals, during actual production of EP words. Coarticulatory effects will greatly benefit from this line of research. Acknowledgements This work is part of project HERON (POSI/PLP/57680/2004), funded by FCT (Portuguese Research Agency). Authors thank Radiology Department, Coimbra University Hospital (HUC), particularly its Director Professor Filipe Caseiro Alves and its technical staff. We gratefully acknowledge the very important MRI technical support given by João Cunha Pires. We also thank our two speakers for their help and tolerance during the acquisition session. Our thanks to the three anonymous reviewers for their comments and suggestions, contributing to an overall improvement in the paper. References Adams and Bischof, 1994 R. Adams L. Bischof Seeded region growing IEEE Trans. Pattern Anal. Machine Intell. 16 6 1994 641 647 Alwan et al., 1997 A. Alwan S. Narayanan K. Haker Toward articulatory-acoustic models for liquid approximants based on MRI and EPG data. Part II. The rhotics J. Acoust. Soc. Amer. (JASA) 101 2 1997 1078 1089 Andrade, 1999 Andrade, A., 1999. On /l/ velarization in European Portuguese. In: Internat. Conf. of Phonetics (ICPhS), San Francisco. Badin et al., 1998 Badin, P., Bailly, G., Raybaudi, M., Segebarth, C., 1998. A three-dimensional linear articulatory model based on MRI data. In: 5th Internat. Conf. on Spoken Language Processing (ICSLP), pp. 417–420. Baer et al., 1991 T. Baer J.C. Gore L.C. Gracco P.W. Nye Analysis of vocal tract shape and dimensions using magnetic resonance imaging: vowels J. Acoust. Soc. Amer. (JASA) 90 2 1991 799 828 Bangayan et al., 1996 Bangayan, P., Alwan, A., Narayanan, S., 1996. From MRI and acoustic data to articulatory synthesis: a case study of the laterals. In: ICSLP, Philadelphia, pp. 793–796. Bryman and Cramer, 2001 A. Bryman D. Cramer Quantitative Data Analysis with SPSS Release 10 for Windows – A guide for Social Scientists 2001 Routledge Chodorowski et al., 2005 Chodorowski, A., Mattsson, U., Langille, M., Hamarneh, G., 2005. Color lesion boundary detection using live wire. In: SPIE. Cruz-Ferreira, 1999 Cruz-Ferreira, M., 1999. Portuguese (European). In: Handbook of the International Phonetic Association, The International Phonetic Association, Cambridge University Press, pp. 126–130. Dang and Honda, 1994 J. Dang K. Honda MRI measurements and acoustic of the nasal and paranasal cavities J. Acoust. Soc. Amer. (JASA) 94 3, Pt 2 1994 1765 Dang and Honda, 1996 Dang, J., Honda, K., 1996. An improved vocal tract model of vowel production implementing piriform resonance and transvelar nasal coupling. In: ICSLP. Dang et al., 1994 J. Dang K. Honda H. Suzuki Morphological and acoustical analysis of the nasal and the paranasal cavities J. Acoust. Soc. Amer. (JASA) 96 4 1994 2088 2100 Delvaux et al., 2002 Delvaux, V., Metens, T., Soquet, A., 2002. French nasal vowels: acoustic and articulatory properties. In: 7th Internat. Conf. on Spoken Language Processing (ICSLP), Vol. 1, Denver, pp. 53–56. Demolin et al., 1996 Demolin, D., George, M., Lecuit, V., Metens, T., Socquet, A., 1996. Détermination, par IRM, de l’ouverture au velum des voyelles nasales du Français. In: XXIémes Journées d’Etudes sur la Parole, Avignon, France, pp. 83–86. Demolin et al., 1996 Demolin, D., Metens, T., Soquet, A., 1996. Three-dimensional measurement of the vocal tract by MRI. In: 4th Internat. Conf. on Spoken Language Processing (ICSLP), Vol. 1, p. 272. Demolin et al., 1998 Demolin, D., Lecuit, V., Metens, T., Nazarian, B., Soquet, A., 1998. Magnetic resonance measurements of the velum port opening. In: 5th Internat. Conf. on Spoken Language Processing (ICSLP). Demolin et al., 2003 D. Demolin V. Delvaux T. Metens A. Soquet Determination of velum opening for French nasal vowels by magnetic resonance imaging J. Voice 17 4 2003 454 467 Engwall, 1999 Engwall, O., 1999. Modeling of the vocal tract in three dimensions. In: 6th Eur. Conf. on Speech Communication and Technology (Eurospeech), pp. 113–116. Engwall, 2002 Engwall, O., 2002. Tongue talking: studies in intraoral speech synthesis. Doctoral Thesis, KTH – Royal Institute of Technology. Engwall, 2003 Engwall, O., 2003. A revisit to the application of MRI to the analysis of speech production – testing our assumptions. In: 6th Seminar on Speech Production, pp. 43–48. Engwall and Badin, 1999 Engwall, O., Badin, P., 1999. Collecting and analysing two and three dimensional MRI data for Swedish, Speech Transmission Laboratory: Quarterly Progress and Status Report (STL-QPSR), pp. 11–38. Engwall and Badin, 2000 Engwall, O., Badin, P., 2000. An MRI study of Swedish fricatives: coarticulatory effects. In: 5th Seminar on Speech Production, Kloster Seeon, Germany, pp. 297–300. Engwall et al., 2006 Engwall, O., Delvaux, V., Metens, T., 2006. Interspeaker variation in the articulation of nasal vowels. In: 7th Internat. Seminar on Speech Production. Ericsdotter, 2005 Ericsdotter, C., 2005. Articulatory-acoustic relationships in Swedish vowel sounds, Doctoral dissertation, Stockholm University. Farnetani, 1999 E. Farnetani Coarticulation and connected speech processes W.J.H. Laver John The Handbook of Phonetic Sciences 1999 Blackwell Oxford 371 404 Gick et al., 2002 B. Gick A.M. Kang D.H. Whalen MRI evidence for commonality in the post-oral articulations of English vowels and liquids J. Phonetics 30 3 2002 357 371 Greenwood et al., 1992 Greenwood, A.R., Goodyear, C.C., Martin, P.A., 1992. Measurements of vocal tract shapes using magnetic resonance imaging. In: IEE Comm. Speech Vision, Vol. 139, pp. 553–560. Gregio, 2006 Gregio, F.N., 2006. Configuração do trato vocal supraglótico na produção das vogais do português brasileiro: dados de imagens de ressonância magnética, Dissertação de mestrado, Pontificia Universidade Católica de São Paulo. Hardcastle, 1976 W.J. Hardcastle Physiology of Speech Production: An Introduction for Speech Scientists 1976 Academic Press London Hardcastle and Hewlett, 1999 W. Hardcastle N. Hewlett Coarticulation: Theory, Data and Techniques 1999 Cambridge University Press Cambridge Hoole, 1993 P. Hoole Methodological considerations in the use of electromagnetic articulography in phonetic research FIPKM 31 1993 43 64 Hoole and Nguyen, 1999 P. Hoole N. Nguyen Electromagnetic articulography W. Hardcastle N. Hewlett Coarticulation: Theory, Data and Techniques 1999 Cambridge University Press Cambridge 260 269 Hoole et al., 2000 Hoole, P., Wismüller, A., Leinsinger, G., Kroos, C., Geumann, A., Inoue, M., 2000. Analysis of tongue configuration in multi-speaker, multi-volume MRI data. In: 5th Seminar on Speech Production, Kloster Seeon, Germany, pp. 157–160. International Phonetic Association, 1999 International Phonetic Association, 1999. Handbook of the International Phonetic Association: A Guide to the Use of the International Phonetic Alphabet. Cambridge University Press. Jackson, 2000 Jackson, P.J.B., 2000. Characterisation of plosive, fricative and aspiration components in speech production. Ph.D. Thesis, U. Southampton. Jesus and Shadle, 2002 L.M.T. Jesus C.H. Shadle A parametric study of the spectral characteristics of European Portuguese fricatives J. Phonetics 30 2002 437 464 Kim, 2004 H. Kim Stroboscopic-cine MRI data on Korean coronal plosives and affricates: implications for their place of articulation as alveolar Phonetica 61 4 2004 234 251 Kiritani, 1986 S. Kiritani X-ray microbeam method for measurement of articulatory dynamics-techniques and results Speech Comm. 5 2 1986 119 140 Kröger et al., 2000 Kröger, B.J., Winkler, R., Mooshammer, C., Pompino-Marschall, B., 2000. Estimation of vocal tract area function from magnetic resonance imaging: preliminary results. In: 5th Seminar on Speech Production, Kloster Seeon, Germany, pp. 333–336. Kühnert and Nolan, 1999 B. Kühnert F. Nolan The origins of coarticulation W.H. Hewlett Nigel Coarticulation: Theory, Data and Techniques 1999 Cambridge University Press Ladefoged and Maddieson, 1996 P. Ladefoged I. Maddieson The Sounds of the World’s Languages 1996 Blackwell Magen, 1997 H.S. Magen The extent of vowel-to-vowel coarticulation in English J. Phonetics 25 2 1997 187 205 Manuel, 1999 S. Manuel Cross-language studies: relating language-particular coarticulation patterns to other language-particular facts W.J.H. Hewlett Nigel Coarticulation: Theory Data and Techniques 1999 Cambridge University Press Cambridge 179 198 Chapter 8 Mathiak et al., 2000 K. Mathiak I. Hertrich W.E. Kincses U. Klose H. Ackermann W. Grodd Stroboscopic articulography using fast magnetic resonance imaging Internat. J. Lang. Comm. Disorders/Royal College Speech Lang. Therapists 35 3 2000 419 425 Mohammad et al., 1997 Mohammad, M., Moore, E., Carter, J.N., Shadle, C.H., Gunn, S.R., 1997. Using MRI to image the moving vocal tract during speech. In: 5th Eur. Conf. on Speech Communication and Technology (Eurospeech), Vol. 4, pp. 2027–2030. Morais Barbosa, 1994 A. Morais Barbosa Introdução ao Estudo da Fonologia e Morfologia do Português 1994 Almedina Coimbra Narayanan and Alwan, 1995 S. Narayanan A. Alwan A nonlinear dynamical systems analysis of fricative consonants J. Acoust. Soc. Amer. 97 1995 2511 2524 Narayanan and Alwan, 2000 S. Narayanan A. Alwan Noise source models for fricative consonants IEEE Trans. Speech Audio Process. 8 2 2000 328 344 Narayanan et al., 1995 S. Narayanan A. Alwan K. Haker An articulatory study of fricative consonants using MRI J. Acoust. Soc. Amer. (JASA) 98 3 1995 1325 1347 Narayanan et al., 1997 S. Narayanan A. Alwan K. Haker Toward articulatory-acoustic models for liquid approximants based on MRI and EPG data. Part I. The laterals J. Acoust. Soc. Amer. (JASA) 101 2 1997 1064 1077 Narayanan et al., 2004 S. Narayanan K. Nayak S. Lee D. Byrd An approach to real-time magnetic resonance imaging for speech production J. Acoust. Soc. Amer. (JASA) 115 4 2004 1771 1776 Perkell, 1969 J. Perkell Physiology of Speech Production: Results and Implications of a Quantitative Cineradiographic Study 1969 MIT Press Recasens, 1999 D. Recasens Lingual coarticulation W. Hardcastle N. Hewlett Coarticulation 1999 Cambridge University Press Cambridge Recasens and Espinosa, 2005 D. Recasens A. Espinosa Articulatory, positional and coarticulatory characteristics for clear /l/ and dark /l/: evidence from two catalan dialects J. Internat. Phonetic Assoc. 35 1 2005 1 25 Recasens and Espinosa, 2006 D. Recasens A. Espinosa Articulatory, positional and contextual characteristics of palatal consonants: evidence from Majorcan Catalan J. Phonetics 34 3 2006 295 318 Recasens et al., 1997 D. Recasens M.D. Pallarés J. Fontdevila A model of lingual coarticulation based on articulatory constraints J. Acoust. Soc. Amer. (JASA) 102 1997 544 561 Rossato et al., 2006 Rossato, S., Teixeira, A., Ferreira, L., 2006. Les nasales du Portugais et du Français: une étude comparative sur les données EMMA, in: XXVIes Journées détudes sur la parole, Dinard. Rua and Freitas, 2006 Rua, S., Freitas, D., 2006. Morphological dynamic study of human vocal tract. In: CompIMAGE – Computational Modelling of Objects Represented in Images: Fundamentals, Methods and Applications, Coimbra, Portugal. Sachs, 1984 L. Sachs Applied Statistics – A Handbook of Techniques second ed. 1984 Springer-Verlag Sá Nogueira, 1938 R. Sá Nogueira Elementos para um tratado de Fonética Portuguesa 1938 Impressa Nacional Lisbon Santos et al., 2004 B. Santos C. Ferreira J. Silva A. Silva L. Teixeira Quantitative evaluation of a pulmonary contour segmentation algorithm in X-ray computed tomography images Acad. Radiol. 11 8 2004 868 878 Serrurier and Badin, 2005 A. Serrurier P. Badin Towards a 3D articulatory model of the velum based on MRI and CT images ZAS Papers Linguist. 40 2005 195 211 Serrurier and Badin, 2005 Serrurier, A., Badin, P., 2005. A three-dimensional linear articulatory model of velum based on MRI data. In: Interspeech. Shadle et al., 1996 C.H. Shadle M. Tiede S. Masaki Y. Shimada I. Fujimoto An MRI Study of the Effects of Vowel Context on Fricatives Vol. 18 1996 Institute of Acoustics pp. 187–194 Shadle et al., 1999 Shadle, C.H., Mohammad, M., Carter, J.N., Jackson, P.J.B., 1999. Multi-planar dynamic magnetic resonance imaging: new tools for speech research. In: XIVth Internat. Congress of Phonetic Sciences (ICPhS), pp. 623–626. Stone, 1999 M. Stone Laboratory techniques for investigating speech articulation W.H. Laver John The Handbook of Phonetic Sciences 1999 Blackwell 11 32 Stone et al., 1997 Stone, M., Lundberg, A.J., Davis, E.P., Gullipalli, R., NessAiver, M., 1997. Three-dimensional coarticulatory strategies of tongue movement. In: 5th Eur. Conf. on Speech Communication and Technology (Eurospeech), pp. 1–31. Stone et al., 2001 M. Stone E.P. Davis A.S. Douglas M. NessAiver R. Gullipalli W.S. Levine A.J. Lundberg Modeling tongue surface contours from Cine-MRI images J. Speech Lang. Hear. Res. 44 2001 1026 1040 Story et al., 1996 B.H. Story I.R. Titze E.A. Hoffman Vocal tract area functions from magnetic resonance imaging J. Acoust. Soc. Amer. (JASA) 100 1 1996 537 554 Strevens, 1954 P. Strevens Some observations on the phonetics and pronunciation of modern Portuguese Rev. Laboratório Fonética Experimental Coimbra II 1954 5 29 Takemoto et al., 2004 H. Takemoto T. Kitamura H. Nishimoto K. Honda A method of tooth superimposition of MRI data for accurate measurement of vocal tract shape and dimensions Acoust. Sci. Technol. 25 6 2004 468 474 Teixeira and Vaz, 2001 Teixeira, A., Vaz, F., 2001. European Portuguese nasal vowels: an EMMA study. In: 7th Eur. Conf. on Speech Communication and Technology (EuroSpeech), Vol. 2, Scandinavia, pp. 1483–1486. Teixeira et al., 1999 Teixeira, A., Vaz, F., Príncipe, J.C., 1999. Influence of dynamics in the perceived naturalness of Portuguese nasal vowels. In: ICPhS, pp. 2557–2560. Teixeira et al., 2003 Teixeira, A., Castro Moutinho, L., Coimbra, R.L., 2003. Production, acoustic and perceptual studies on European Portuguese nasal vowels height. In: Internat. Congress Phonetic Sciences (ICPhS), pp. 3033–3036. Teixeira et al., 2005 A. Teixeira R. Martinez L.N. Silva L.M.T. Jesus J.C. Príncipe F. Vaz Simulation of human speech production applied to the study and synthesis of European Portuguese EURASIP J. Appl. Signal Process. 2005 9 2005 1435 1448 Tiede, 1996 M. Tiede An MRI-based study of pharyngeal volume contrasts in Akan and English J. Phonetics 24 4 1996 399 421 Tiede et al., 2000 Tiede, M., Masaki, S., Vatikiotis-Bateson, E., 2000. Contrasts in speech articulation observed in sitting and supine conditions. In: 5th Seminar on Speech Production, Kloster Seeon, Germany, pp. 25–28. Tuller et al., 1981 E. Tuller K.S. Harris R. Gross Electromyographic study of the jaw muscles during speech J. Phonetics 9 1981 175 188 Viana and Andrade, 1996 M.d.C. Viana A. Andrade Fonética I. Faria E. Pedro I. Duarte C. Gouveia Introdução à Linguística Geral e Portuguesa 1996 Caminho Lisbon 113 167 West, 2000 West, P., 2000. Long-distance coarticulatory effects of British English /l/ and /r/: an EMA, EPG and acoustic study. In: Speech Production Seminar, Seeon, Germany, pp. 105–108. Yang, 1999 Yang, B., 1999. Measurement and synthesis of the vocal tract of Korean monophthongs by MRI. In: XIVth Internat. Congress of Phonetic Sciences (ICPhS), pp. 2005–2008. "
    },
    {
        "doc_title": "European Portuguese articulatory based text-to-speech: First results",
        "doc_scopus_id": "52949109223",
        "doc_doi": "10.1007/978-3-540-85980-2_11",
        "doc_eid": "2-s2.0-52949109223",
        "doc_date": "2008-10-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Computational processing",
            "European",
            "International conferences",
            "Linguistic modeling",
            "Perceptual evaluation",
            "System capabilities",
            "Text-to-speech",
            "TTS systems"
        ],
        "doc_abstract": "In this paper we present recent work on the development of Linguistic Models, resulting in a first \"complete\" articulatory-based TTS system for Portuguese. The system, based on TADA system, integrates our past work in automatic syllabification and grapheme-phone conversion plus a first gestural specification of European Portuguese sounds. The system was integrated with SAPWindows, an articulatory synthesizer for Portuguese. A demonstration of the system capabilities and a first perceptual evaluation are presented. © 2008 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface",
        "doc_scopus_id": "52949108864",
        "doc_doi": null,
        "doc_eid": "2-s2.0-52949108864",
        "doc_date": "2008-10-06",
        "doc_type": "Editorial",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dynamic language modeling for the European Portuguese",
        "doc_scopus_id": "52949098010",
        "doc_doi": "10.1007/978-3-540-85980-2_35",
        "doc_eid": "2-s2.0-52949098010",
        "doc_date": "2008-10-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Applications.",
            "Broadcast news",
            "Broadcast news transcription",
            "Computational processing",
            "European",
            "International conferences",
            "Language modeling",
            "Language modelling",
            "Multi-pass",
            "Recognition process",
            "Relative reduction",
            "Speech recognizers",
            "Unsupervised adaptation"
        ],
        "doc_abstract": "Up-to-date language modeling is recognized to be a critical aspect of maintaining the level of performance for a speech recognizer over time for most applications. In particular for applications such as transcription of broadcast news and conversations where the occurrence of new words is very frequent, especially for highly inflected languages like the European Portuguese. An unsupervised adaptation approach, which dynamically adapts the active vocabulary and language model during a multi-pass speech recognition process, is presented. Experimental results confirmed the adequacy of the proposed approaches. Experiments were carried out for a European Portuguese Broadcast News transcription system with the best preliminary results yielding a relative reduction of 65.2% in OOV word rate and 6.6% in WER. © 2008 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A dialogue manager for an intelligent mobile robot",
        "doc_scopus_id": "58149121474",
        "doc_doi": null,
        "doc_eid": "2-s2.0-58149121474",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Dialogue managers",
            "Pronoun resolutions"
        ],
        "doc_abstract": "This paper focuses on a dialogue manager developed for Carl, an intelligent mobile robot. It uses the Information State (IS) approach and it is based on a Knowledge Acquisition and Management (KAM) module that integrates information obtained from various interlocutors. This mixed-initiative dialogue manager handles pronoun resolution, it is capable of performing different kinds of clarification questions and to comment information based on the current knowledge acquired.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Vocabulary selection for a broadcast news transcription system using a morpho-syntatic approach",
        "doc_scopus_id": "56149125918",
        "doc_doi": null,
        "doc_eid": "2-s2.0-56149125918",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Asr systems",
            "Baseline systems",
            "Broadcast news",
            "Broadcast news transcriptions",
            "Do-mains",
            "Europeans",
            "OOV words",
            "Relative reductions",
            "Selection strategies",
            "Trading offs",
            "Transcription systems",
            "Vocabulary selection",
            "Vocabulary sizes",
            "Word frequencies"
        ],
        "doc_abstract": "Although the vocabularies of ASR systems are designed to achieve high coverage for the expected domain, out-of-vocabulary (OOV) words cannot be avoided. Particularly, for daily and real-time transcription of Broadcast News (BN) data in highly inflected languages, the rapid vocabulary growth leads to high OOV word rates. To overcome this problem, we present a new morpho-syntatic approach to dynamically select the target vocabulary for this particular domain by trading off between the OOV word rate and vocabulary size. We evaluate this approach against the common selection strategy based on word frequency. Experiments have been carried out for a European Portuguese BN transcription system. Results computed on seven news shows, yields a relative reduction of 37.8% in OOV word rate against the baseline system and 5.5% when compared with the word frequency common approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An information state based dialogue manager for a mobile robot",
        "doc_scopus_id": "56149100697",
        "doc_doi": null,
        "doc_eid": "2-s2.0-56149100697",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Dialogue managers",
            "Dialogue system",
            "Human-robot interaction",
            "Information state system",
            "Information states",
            "Knowledge acquisition and managements",
            "Natural language understanding",
            "Pronoun resolutions"
        ],
        "doc_abstract": "The paper focuses on an Information State (IS) based dialogue manager developed for Carl, an intelligent mobile robot. It uses a Knowledge Acquisition and Management (KAM) module that integrates information obtained from various interlocutors. This mixed-initiative dialogue manager (DM) handles pronoun resolution, is capable of performing different kinds of clarification/ confirmation questions and generates observations based on the current knowledge acquired. An evaluation of the DM on knowledge acquisition tasks is shown.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An MRI study of European Portuguese nasals",
        "doc_scopus_id": "56149084721",
        "doc_doi": null,
        "doc_eid": "2-s2.0-56149084721",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "3d analyses",
            "Dynamic informations",
            "European portuguese",
            "Europeans",
            "MRI",
            "Nasal vowels",
            "Nasals",
            "Research teams"
        ],
        "doc_abstract": "In this work we present a recently acquired MRI database for European Portuguese. As a first example of possible studies, we present results on 2D and 3D analyses of European Portuguese nasals, particularly nasal vowels. This database will enable the extraction of 2D and/or 3D articulatory parameters as well as some dynamic information to include in articulatory synthesizers. It can also be useful to compare the production of European Portuguese with the production of other languages and have further insight on some of the European Portuguese characteristics, as the nasalization and coarticulation. The MRI database and related studies were made possible by the interdisciplinary nature of the research team, comprised of a radiologist, image processing specialists and a speech scientist.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Dynamic language modeling for a daily broadcast news transcription system",
        "doc_scopus_id": "44849113247",
        "doc_doi": "10.1109/asru.2007.4430103",
        "doc_eid": "2-s2.0-44849113247",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Broadcast news",
            "Broadcast news transcriptions",
            "Dynamic languages",
            "Natural language interfaces",
            "Recognition process",
            "Relative reduction",
            "Relevant documents",
            "Unsupervised adaptation"
        ],
        "doc_abstract": "When transcribing Broadcast News data in highly inflected languages, the vocabulary growth leads to high out-of-vocabulary rates. To address this problem, we propose a daily and unsupervised adaptation approach which dynamically adapts the active vocabulary and LM to the topic of the current news segment during a multi-pass speech recognition process. Based on texts daily available on the Web, a story-based vocabulary is selected using a morpho-syntatic technique. Using an Information Retrieval engine, relevant documents are extracted from a large corpus to generate a story-based LM. Experiments were carried out for a European Portuguese BN transcription system. Preliminary results yield a relative reduction of 65.2% in OOV and 6.6% in WER. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information extraction from medical reports",
        "doc_scopus_id": "77954126958",
        "doc_doi": null,
        "doc_eid": "2-s2.0-77954126958",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Extracting information",
            "Human activities",
            "Information Extraction",
            "Information extraction systems",
            "Information extraction technology",
            "Message understanding conferences",
            "Structured information"
        ],
        "doc_abstract": "Information extraction technology, as defined and developed through the U.S. DARPA Message Understanding Conferences (MUCs), has proved successful at extracting information primarily from newswire texts and in domains concerned with human activity. This paper presents an Information Extraction (IE) system, intended to extract structured information from medical reports written in Portuguese. A first evaluation is performed and the results are discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of machine learning and syllable information in European Portuguese grapheme-phone conversion",
        "doc_scopus_id": "33745809812",
        "doc_doi": "10.1007/11751984_24",
        "doc_eid": "2-s2.0-33745809812",
        "doc_date": "2006-07-17",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "European Portuguese grapheme-phone conversion",
            "Parallel combinations",
            "Self-learning methods",
            "Syllable information"
        ],
        "doc_abstract": "In this study evaluation of two self-learning methods (MBL and TBL) on European Portuguese grapheme-to-phone conversion is presented. Combinations (parallel and cascade) of the two systems were also tested. The usefulness of syllable information is also investigated. Systems with good performance were obtained both using a single self-learning method and combinations. Best performance was obtained with MBL and the parallel combination. The use of syllable information contributes to a better performance in all systems tested. © Springer-Verlag Berlin Heidelberg 2006.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A knowledge representation and reasoning module for a dialogue system in a mobile robot",
        "doc_scopus_id": "78651300617",
        "doc_doi": null,
        "doc_eid": "2-s2.0-78651300617",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (miscellaneous)",
                "area_abbreviation": "COMP",
                "area_code": "1701"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Classical semantics",
            "Dialogue systems",
            "Inductive inference",
            "Intelligent mobile robot",
            "Knowledge representation and reasoning",
            "Knowledge representation language",
            "Question Answering"
        ],
        "doc_abstract": "The recent evolution of Carl, an intelligent mobile robot, is presented. The paper focuses on the new knowledge representation and reasoning module, developed to support high-level dialogue. This module supports the integration of information coming from different interlocutors and is capable of handling contradictory facts. The knowledge representation language is based on classical semantic networks, but incorporates some notions from UML. Question answering is based on deductive as well as inductive inference.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On European Portuguese automatic syllabification",
        "doc_scopus_id": "33745210111",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33745210111",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Automatic syllabification",
            "Language processing systems",
            "Syllable boundaries",
            "Syllable structures"
        ],
        "doc_abstract": "This paper presents three methods for dividing European Portuguese (EP) words into syllables, two of them handling graphemes as input, the other processing phone sequences. All three try to incorporate linguistic knowledge about EP syllable structure, but in different degrees. Experimental results showed, for the best method, percentage of correctly recognized syllable boundaries above 99.5 %, and comparable word accuracy. The much simpler finite state transducer based method also achieved a good performance, making it suitable for applications more interested in speed and memory footprint. Being syllabification an essential component of many speech and language processing systems, proposed methods can be useful to researchers working with the EP language.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From robust spoken language understanding to knowledge acquisition and management",
        "doc_scopus_id": "33745207534",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33745207534",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Knowledge representation and reasoning (KRR)",
            "Semantic networks",
            "Spoken language understanding (SLU)"
        ],
        "doc_abstract": "The recent evolution of Carl, an intelligent mobile robot, is presented. The paper focuses on robust spoken language understanding (SLU) and on knowledge representation and reasoning (KRR). Robustness in SLU is achieved through the combination of deep and shallow parsing, tolerating non-grammatical utterances. The KRR module supports the integration of information coming from different interlocutors and is capable of handling contradictory facts. The knowledge representation language is based on semantic networks. Question answering is based on deductive as well as inductive inference. A preliminary evaluation of the efficiency of the SLU/KRR system, for the purpose of knowledge acquisition, is presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Simulation of human speech production applied to the study and synthesis of European Portuguese",
        "doc_scopus_id": "27844573290",
        "doc_doi": "10.1155/ASP.2005.1435",
        "doc_eid": "2-s2.0-27844573290",
        "doc_date": "2005-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Articulatory synthesis",
            "Fricatives",
            "Nasal vowels",
            "Speech production"
        ],
        "doc_abstract": "A new articulatory synthesizer (SAPWindows), with a modular and flexible design, is described. A comprehensive acoustic model and a new interactive glottal source were implemented. Perceptual tests and simulations made possible by the synthesizer contributed to deepening our knowledge of one of the most important characteristics of European Portuguese, the nasal vowels. First attempts at incorporating models of frication into the articulatory synthesizer are presented, demonstrating the potential of performing fricative synthesis based on broad articulatory configurations. Synthesis of nonsense words and Portuguese words with vowels and nasal consonants is also shown. Despite not being capable of competing with mainstream concatenative speech synthesis, the anthropomorphic approach to speech synthesis, known as articulatory synthesis, proved to be a valuable tool for phonetics research and teaching. This was particularly true for the European Portuguese nasal vowels. © 2005 Hindawi Publishing Corporation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An acoustic corpus contemplating regional variation for studies of european Portuguese nasals",
        "doc_scopus_id": "85037167347",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85037167347",
        "doc_date": "2004-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Library and Information Sciences",
                "area_abbreviation": "SOCI",
                "area_code": "3309"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            }
        ],
        "doc_keywords": [
            "Complex sounds",
            "Dynamic nature",
            "Nasal vowels",
            "Regional variation",
            "Speech synthesizer",
            "Speech therapy"
        ],
        "doc_abstract": "Portuguese is one of the two standard Romance varieties having nasal vowels as independent phonemes. These are complex sounds that have a dynamic nature and present several problems for a complete description. In this paper we present a new corpus especially recorded to allow studies of European Portuguese nasal vowels. The main purpose of these studies is the improvement of knowledge about these sounds so that it can be applied to language teaching, speech therapy materials and the articulatory speech synthesizer that is being developed at the University of Aveiro. The corpus described is a valuable resource for such studies due to the regional and contextual coverage and the simultaneous availability of speech and EGG signal. Details about corpus definition, recording, annotation and availability are given.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Towards a personal robot with language interface",
        "doc_scopus_id": "78349285382",
        "doc_doi": null,
        "doc_eid": "2-s2.0-78349285382",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Human robots",
            "Interactivity",
            "Language interface",
            "Multi-modal",
            "Natural language interfaces",
            "Personal robot"
        ],
        "doc_abstract": "The development of robots capable of accepting instructions in terms of familiar concepts to the user is still a challenge. For these robots to emerge it?s essential the development of natural language interfaces, since this is regarded as the only interface acceptable for a machine which expected to have a high level of interactivity with Man. Our group has been involved for several years in the development of a mobile intelligent robot, named Carl, designed having in mind such tasks as serving food in a reception or acting as a host in an organization. The approach that has been followed in the design of Carl is based on an explicit concern with the integration of the major dimensions of intelligence, namely Communication, Action, Reasoning and Learning. This paper focuses on the multi-modal human-robot language communication capabilities of Carl, since these have been significantly improved during the last year.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A robot with natural interaction capabilities",
        "doc_scopus_id": "77954482142",
        "doc_doi": "10.1109/ETFA.2003.1247762",
        "doc_eid": "2-s2.0-77954482142",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Current capability",
            "Human-robot communication",
            "Intelligent Service robots",
            "Multi-modal",
            "Natural interactions"
        ],
        "doc_abstract": "© 2003 IEEE.This paper describes the architecture and current capabilities of Carl, a prototype of an intelligent service robot, designed having in mind such tasks as serving food in a reception or acting as a host in an organization. The approach that has been followed in the design of Carl is based on an explicit concern with the integration of the major dimensions of intelligence, namely communication, action, reasoning and learning. The paper focuses on the multi-modal human-robot communication capabilities of Carl, since these have been significantly improved during the last year.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Experimental phonetics contributions to the Portuguese articulatory synthesizer development",
        "doc_scopus_id": "7044235985",
        "doc_doi": "10.1007/3-540-45011-4_10",
        "doc_eid": "2-s2.0-7044235985",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Acoustic analysis",
            "Glottal source",
            "Regional variation"
        ],
        "doc_abstract": "© Springer-Verlag Berlin Heidelberg 2003.In this paper we present current work and results in two Experimental Phonetics projects motivated by our ongoing development of an articulatory synthesizer for Portuguese. Examples of analyses and results regarding glottal source parameters and from EMMA and acoustic analyses related to the tongue position are presented. In our studies contextual and regional variation is considered.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Adding fricatives to the Portuguese articulatory synthesiser",
        "doc_scopus_id": "27844455306",
        "doc_doi": null,
        "doc_eid": "2-s2.0-27844455306",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Essential characteristic",
            "Flexible designs",
            "nocv1",
            "Source types",
            "Spectral characteristics",
            "Volume velocity source"
        ],
        "doc_abstract": "First attempts at incorporating models of frication into an articulatory synthesizer, with a modular and flexible design, are presented. Although the synthesizer allows the user to choose different combinations of source types, noise volume velocity sources have been used to generate turbulence. Preliminary results indicate that the model is capturing essential characteristics of the transfer functions and spectral characteristics of fricatives. Results also show the potential of performing synthesis based on broad articulatory configurations of fricatives.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "SAPWindows - Towards a versatile modular articulatory synthesizer",
        "doc_scopus_id": "84966340051",
        "doc_doi": "10.1109/WSS.2002.1224366",
        "doc_eid": "2-s2.0-84966340051",
        "doc_date": "2002-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Language and Linguistics",
                "area_abbreviation": "ARTS",
                "area_code": "1203"
            },
            {
                "area_name": "Cultural Studies",
                "area_abbreviation": "SOCI",
                "area_code": "3316"
            }
        ],
        "doc_keywords": [
            "Acoustic model",
            "Generic information",
            "Glottal source",
            "Object oriented design"
        ],
        "doc_abstract": "© 2002 IEEE.A new modular articulatory synthesizer, called SAPWindows, is described. The paper presents generic information regarding the synthesizer structure, currently available models, and a description of the SAPWindows application and its user interface. The implementation of an object-oriented design allows the inclusion of several articulatory models, and the enhancement and creation of new models for improving quality. A comprehensive acoustic model and a new interactive glottal source have already been implemented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "European Portuguese nasal vowels: An EMMA study",
        "doc_scopus_id": "56149118511",
        "doc_doi": null,
        "doc_eid": "2-s2.0-56149118511",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            },
            {
                "area_name": "Linguistics and Language",
                "area_abbreviation": "SOCI",
                "area_code": "3310"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Articulatory synthesis",
            "Nasal vowels"
        ],
        "doc_abstract": "In this paper new EMMA data regarding European Portuguese nasals is presented. Some details about corpus constitution, recording and annotation is given. First results from analysis are presented. Quantitative analysis of velum movement was done for nasal vowels between stops. For the other contexts representative examples are presented and qualitatively analysed. In all contexts nasal vowels are produced with an initial phase having an high velum position. This result supports our previous work conclusions, of nasal vowels viewed as dynamic sounds were beginning must have dominant lips radiation. Obtained knowledge has application in articulatory synthesis, our motivation for this study.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Human-robot interaction through spoken language dialogue",
        "doc_scopus_id": "0034448735",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0034448735",
        "doc_date": "2000-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Body and soul",
            "Communication,Action,Reasoning and Learning in robotics",
            "Human-robot interaction",
            "Sensory motor skill",
            "Spoken language dialogue"
        ],
        "doc_abstract": "The development of robots able to accept, via a friendly interface, instructions in terms of the concepts familiar to the human user remains a challenge. It is argued that designing and building such intelligent robots can be seen as the problem of integrating four main dimensions: human-robot communication, sensory motor skills and perception, decision-making capabilities and learning. Although these dimensions have been thoroughly studied in the past, their integration has seldom been attempted in a systematic way. It is further argued that, for the common user, the only sufficiently practical interface is spoken language. The \"body and soul\" of Carl, a robot currently under construction in our lab, are presented. The spoken language interface is given particular attention.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Partial lack of susceptibility to Schistosoma mansoni infection of Biomphalaria glabrata strains from Itanhomi (Minas Gerais, Brazil) after fourteen years of laboratory maintenance.",
        "doc_scopus_id": "0033126704",
        "doc_doi": "10.1590/S0074-02761999000300026",
        "doc_eid": "2-s2.0-0033126704",
        "doc_date": "1999-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Microbiology (medical)",
                "area_abbreviation": "MEDI",
                "area_code": "2726"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": null,
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Some studies of European Portuguese nasal vowels using an articulatory synthesizer",
        "doc_scopus_id": "0032264008",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0032264008",
        "doc_date": "1998-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Articulatory synthesizers",
            "Dynamic configuration",
            "Nasal vowels"
        ],
        "doc_abstract": "In this paper we present some details of our work in the study of European Portuguese nasal vowels using an articulatory synthesizer. In the first part of the paper we describe some of the articulatory synthesizer characteristics motivated by the particularities of our object of study. In the second part we describe the realization of some experiments using the synthesizer. Experiments address the problems of: (1) static versus dynamic configurations in production of nasal vowels; (2) the relevance of the nostrils radiated flux in the perception of nasality; and (3) the effect of nasal passage blocking in the perception of nasality. Results reported here are preliminary due to the small number of tests performed to the moment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Method to extract articulatory parameters from the speech signal using neural networks",
        "doc_scopus_id": "0031360762",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0031360762",
        "doc_date": "1997-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Articulatory mapping",
            "Kohonen neural networks"
        ],
        "doc_abstract": "In this paper we present a method that uses artificial neural networks for acoustic to articulatory mapping. An assembly of Kohonen neural nets is used, in the first stage a network maps cepstral values, each neuron contains a subnet in a second stage that maps the articulatory space. The method allows both the acoustic to articulatory mapping, ensuring smooth varying vocal tract shapes, and the study of the non uniqueness problem.",
        "available": false,
        "clean_text": ""
    }
]