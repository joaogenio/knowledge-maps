[
    {
        "doc_title": "Lifelog Retrieval From Daily Digital Data: Narrative Review",
        "doc_scopus_id": "85129608434",
        "doc_doi": "10.2196/30517",
        "doc_eid": "2-s2.0-85129608434",
        "doc_date": "2022-05-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Data Collection",
            "Delivery of Health Care",
            "Exercise",
            "Humans",
            "Smartphone",
            "Wearable Electronic Devices"
        ],
        "doc_abstract": "© Ricardo Ribeiro, Alina Trifan, António J R Neves.Background: Over the past decade, the wide availability and small size of different types of sensors, together with the decrease in pricing, have allowed the acquisition of a substantial amount of data about a person's life in real time. These sensors can be incorporated into personal electronic devices available at a reasonable cost, such as smartphones and small wearable devices. They allow the acquisition of images, audio, location, physical activity, and physiological signals among other data. With these data, usually denoted as lifelog data, we can then analyze and understand personal experiences and behaviors. This process is called lifelogging. Objective: The objective of this paper was to present a narrative review of the existing literature about lifelogging over the past decade. To achieve this goal, we analyzed lifelogging applications used to retrieve relevant information from daily digital data, some of them with the purpose of monitoring and assisting people with memory issues and others designed for memory augmentation. We aimed for this review to be used by researchers to obtain a broad idea of the type of data used, methodologies, and applications available in this research field. Methods: We followed a narrative review methodology to conduct a comprehensive search for relevant publications in Google Scholar and Scopus databases using lifelog topic-related. A total of 411 publications were retrieved and screened. Of these 411 publications, 114 (27.7%) publications were fully reviewed. In addition, 30 publications were manually included based on our bibliographical knowledge of this research field. Results: From the 144 reviewed publications, a total of 113 (78.5%) were selected and included in this narrative review based on content analysis. The findings of this narrative review suggest that lifelogs are prone to become powerful tools to retrieve memories or increase knowledge about an individual's experiences or behaviors. Several computational tools are already available for a considerable range of applications. These tools use multimodal data of different natures, with visual lifelogs being one of the most used and rich sources of information. Different approaches and algorithms to process these data are currently in use, as this review will unravel. Moreover, we identified several open questions and possible lines of investigation in lifelogging. Conclusions: The use of personal lifelogs can be beneficial to improve the quality of our life, as they can serve as tools for memory augmentation or for providing support to people with memory issues. Through the acquisition and analysis of lifelog data, lifelogging systems can create digital memories that can be potentially used as surrogate memory. Through this narrative review, we understand that contextual information can be extracted from lifelogs, which provides an understanding of the daily life of a person based on events, experiences, and behaviors.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Use of Robotic Pets by Community-Dwelling Older Adults: A Scoping Review",
        "doc_scopus_id": "85131596559",
        "doc_doi": "10.1007/s12369-022-00892-z",
        "doc_eid": "2-s2.0-85131596559",
        "doc_date": "2022-01-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Aged care",
            "Community-dwelling",
            "Older adults",
            "Robotic pet",
            "Scoping",
            "Scoping review",
            "Scoping study",
            "Social robots",
            "Technology",
            "Well being"
        ],
        "doc_abstract": "© 2022, The Author(s), under exclusive licence to Springer Nature B.V.Robotic pets (RPs) are being increasingly used with older adults to address behavioral and psychological symptoms of dementia, and as a companion in aged care facilities. Main benefits include increased well-being and better social interactions. To date, little is known about the use of RPs by healthy older adults outside institutional settings. The aim of this paper is to map the research about the use of RPs in community-dwelling healthy older adults considering the users’ perspectives. A scoping review was performed using four databases (Academic Search Complete, Web of Science, PubMed, and SCOPUS), searching for papers published until May 2021. A final selection of ten documents was submitted to thematic analysis. The participants in the studies were all potential users (not actual users, since RPs were still in development). Main findings point to three themes concerning the main expected characteristics of RPs from the perspective of community-dwelling healthy older adults: (i) like reals pets, but with less maintenance; (ii) performing multiple functions, with customizability; (iii) facilitators of interactions without promoting social stigma. Community-dwelling older adults seem open to use a RP as long as it promotes their well-being without facilitating ageism. More research is needed regarding the ethics of using robotics companion pets and comprising a person-centered approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Innovative Vision System for Floor-Cleaning Robots Based on YOLOv5",
        "doc_scopus_id": "85129794169",
        "doc_doi": "10.1007/978-3-031-04881-4_30",
        "doc_eid": "2-s2.0-85129794169",
        "doc_date": "2022-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "% reductions",
            "Automotives",
            "Chemical products",
            "Cleaning robot",
            "Deep learning",
            "Floor cleaning",
            "Floor-cleaning robot",
            "Power products",
            "Vision systems",
            "Water products"
        ],
        "doc_abstract": "© 2022, Springer Nature Switzerland AG.The implementation of a robust vision system in floor-cleaning robots enables them to optimize their navigation and analysing the surrounding floor, leading to a reduction on power, water and chemical products’ consumption. In this paper, we propose a novel pipeline of a vision system to be integrated into floor-cleaning robots. This vision system was built upon the YOLOv5 framework, and its role is to detect dirty spots on the floor. The vision system is fed by two cameras: one on the front and the other on the back of the floor-cleaning robot. The goal of the front camera is to save energy and resources of the floor-cleaning robot, controlling its speed and how much water and detergent is spent according to the detected dirt. The goal of the back camera is to act as evaluation and aid the navigation node, since it helps the floor-cleaning robot to understand if the cleaning was effective and if it needs to go back later for a second sweep. A self-calibration algorithm was implemented on both cameras to stabilize image intensity and improve the robustness of the vision system. A YOLOv5 model was trained with carefully prepared training data. A new dataset was obtained in an automotive factory using the floor-cleaning robot. A hybrid training dataset was used, consisting on the Automation and Control Institute dataset (ACIN), the automotive factory dataset, and a synthetic dataset. Data augmentation was applied to increase the dataset and to balance the classes. Finally, our vision system attained a mean average precision (mAP) of 0.7 on the testing set.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Creating a Project track in a 5-year integrated Engineering Curricullum",
        "doc_scopus_id": "85124808238",
        "doc_doi": "10.1109/WEEF/GEDC53299.2021.9657160",
        "doc_eid": "2-s2.0-85124808238",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Engineering (miscellaneous)",
                "area_abbreviation": "ENGI",
                "area_code": "2201"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            }
        ],
        "doc_keywords": [
            "Active Learning",
            "Curricula design",
            "Exposed to",
            "Integrated engineering",
            "Project based learning",
            "Project-based",
            "Review process",
            "Soft skills",
            "Study plans",
            "Transferrable skill"
        ],
        "doc_abstract": "© 2021 IEEE.In 2017, the authors' Department decided to promote a major change in its Electronics and Telecommunications degree. This is a 5-year degree, that corresponds to a 1st and 2nd cycle degrees in the Bologna qualification framework. One of the touchstones in the review process was the introduction of a project-related track across all 5 years of the degree. In this way, students would be permanently exposed to project-based unit courses throughout their degree. We describe the approach followed in this reorganization. The project-based track is progressive and tries to focus on different aspects of what is required for developing a project, from the first year, when students are exposed to two small projects (one per semester) to the fifth year, where the study plan includes the Dissertation, Project or Internship, corresponding to the traditional approach in master's degrees. This course structure has been welcomed by both the students and the companies employing the degree graduates. There is not yet an evidence-based assessment of the full study plan, but some results exist for the individual course units. The informal feedback collected, both from students' representatives and from companies, are of general approval of the changes that have been introduced.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UA.PT Bioinformatics at ImageCLEF 2020: Lifelog Moment Retrieval Web based Tool",
        "doc_scopus_id": "85121844835",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85121844835",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Automatic approaches",
            "Engineering informatics",
            "ImageCLEF",
            "Images processing",
            "Interactive approach",
            "Life log",
            "Moment retrieval",
            "WEB application",
            "Web applications",
            "Web-based tools"
        ],
        "doc_abstract": "Copyright © 2020 for this paper by its authors.This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the ImageCLEF lifelog task, more specifically in the Lifelog Moment Retrieval (LMRT) sub-task. In our first participation last year we tackled the LMRT challenge with an automatic approach. Following the same steps, we improved our results, while introducing a new interactive approach. For the automatic approach, two submissions were made. We started by processing all images in the lifelog dataset using object detection and scene recognition algorithms. Afterwards, we processed the query topics with Natural Language Processing (NLP) algorithms in order to extract relevant words related to the desired moment. Finally, we compared the visual concepts of the image with the textual concepts of the query topic with the goal of computing a confidence score that relates the image to the topic. For the interactive approach, we developed a web application in order to visualize and provide an interactive tool to the users. The application is divided in three stages. In the first one, the user uploads the images from the dataset, as well the textual data annotations. In the second stage, the user interacts with the application assigning the extracted words to the several topics. Consequently, the application retrieves the image associated to the topic with a certain confidence. In the last stage, we provide a visual environment with two different views, in the form of a image gallery or data tables organized into timestamp clusters. Similarly to our previous participation, the results of the automatic approach are still far from being competitive. We conclude that an automatic approach might not be the best solution for the LMRT task since the currently available state-of-the-art technology is still not able to wield better results. However, our interactive approach with relevance feedback obtained better and competitive results, achieving a F1-measure@10 score of 0.52.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The impact of pre-processing algorithms in facial expression recognition",
        "doc_scopus_id": "85113748334",
        "doc_doi": "10.1117/12.2587865",
        "doc_eid": "2-s2.0-85113748334",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Facial expression recognition",
            "Facial Expressions",
            "Facial feature",
            "Pre-processing",
            "Pre-processing algorithms",
            "Pre-processing step",
            "Recognition accuracy",
            "Relevant features"
        ],
        "doc_abstract": "© 2021 SPIE.This paper proposes several pre-processing algorithms to improve facial expression recognition based on Convolutional Neural Networks (CNNs) models. The proposed CNN model was trained on the Extended Cohn-Kanade dataset (CK+) after applying the pre-processing stages and achieved competitive results (93.90% recognition accuracy) despite its simple and light architecture. Using this CNN model, a study on the impact of each pre-processing algorithm when extracting facial features is presented. In the end, it is understood that pre-processing algorithms help CNNs to extract the most relevant features for each facial expression more effectively, reducing the overfitting and increasing the recognition accuracy. Attention maps before and after the pre-processing step are shown in order to visualize its impact when the proposed CNN model makes a prediction.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Understanding public speakers’ performance: First contributions to support a computational approach",
        "doc_scopus_id": "85087533947",
        "doc_doi": "10.1007/978-3-030-50347-5_30",
        "doc_eid": "2-s2.0-85087533947",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Audio-visual data",
            "Computational approach",
            "Professional life",
            "Video database"
        ],
        "doc_abstract": "© Springer Nature Switzerland AG 2020.Communication is part of our everyday life and our ability to communicate can have a significant role in a variety of contexts in our personal, academic, and professional lives. For long, the characterization of what is a good communicator has been subject to research and debate by several areas, particularly in Education, with a focus on improving the performance of teachers. In this context, the literature suggests that the ability to communicate is not only defined by the verbal component, but also by a plethora of non-verbal contributions providing redundant or complementary information, and, sometimes, being the message itself. However, even though we can recognize a good or bad communicator, objectively, little is known about what aspects – and to what extent—define the quality of a presentation. The goal of this work is to create the grounds to support the study of the defining characteristics of a good communicator in a more systematic and objective form. To this end, we conceptualize and provide a first prototype for a computational approach to characterize the different elements that are involved in communication, from audiovisual data, illustrating the outcomes and applicability of the proposed methods on a video database of public speakers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Image selection based on low level properties for lifelog moment retrieval",
        "doc_scopus_id": "85081171286",
        "doc_doi": "10.1117/12.2557073",
        "doc_eid": "2-s2.0-85081171286",
        "doc_date": "2020-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Computational challenges",
            "Computational time",
            "Image selection",
            "Learning curves",
            "Life log",
            "Low-level features",
            "Low-level image features",
            "Wearable devices"
        ],
        "doc_abstract": "© 2020 SPIE.The increasing number of mobile and wearable devices is dramatically changing the way we collect data about person's life. These devices allow recording our daily activities and behavior in several forms, e.g., text, images, bio-signals, or video. However, many times, the collected data includes low quality or irrelevant contents, feeding lifelogging applications with huge amounts of data, and creating computational challenges for patterns' identification. In this paper, we propose a fast image analysis approach to automatically select relevant images from lifelog data. Using images intrinsic information, such as scenes and objects, we have manually curated two datasets, one with relevant content and another with non-relevant information. Then, we applied supervised learning algorithms based on low-level image features, namely blur and focus, to find the binary model that best discriminates between the two classes. The binary models were then compared based on learning curves and f1-scores, achieving a 95.4% of f1-score for the best one. By reducing the amount of images in the lifelog data, we were able to save computational time without losing images with relevant content.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Facial expression recognition using computer vision: A systematic review",
        "doc_scopus_id": "85075218548",
        "doc_doi": "10.3390/app9214678",
        "doc_eid": "2-s2.0-85075218548",
        "doc_date": "2019-11-01",
        "doc_type": "Review",
        "doc_areas": [
            {
                "area_name": "Materials Science (all)",
                "area_abbreviation": "MATE",
                "area_code": "2500"
            },
            {
                "area_name": "Instrumentation",
                "area_abbreviation": "PHYS",
                "area_code": "3105"
            },
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            },
            {
                "area_name": "Process Chemistry and Technology",
                "area_abbreviation": "CENG",
                "area_code": "1508"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Fluid Flow and Transfer Processes",
                "area_abbreviation": "CENG",
                "area_code": "1507"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2019 by the authors. Licensee MDPI, Basel, Switzerland.Emotion recognition has attracted major attention in numerous fields because of its relevant applications in the contemporary world: marketing, psychology, surveillance, and entertainment are some examples. It is possible to recognize an emotion through several ways; however, this paper focuses on facial expressions, presenting a systematic review on the matter. In addition, 112 papers published in ACM, IEEE, BASE and Springer between January 2006 and April 2019 regarding this topic were extensively reviewed. Their most used methods and algorithms will be firstly introduced and summarized for a better understanding, such as face detection, smoothing, Principal Component Analysis (PCA), Local Binary Patterns (LBP), Optical Flow(OF), Gabor filters, among others. This review identified a clear difficulty in translating the high facial expression recognition (FER) accuracy in controlled environments to uncontrolled and pose-variant environments. The future efforts in the FER field should be put into multimodal systems that are robust enough to face the adversities of real world scenarios. A thorough analysis on the research done on FER in Computer Vision based on the selected papers is presented. This review aims to not only become a reference for future research on emotion recognition, but also to provide an overview of the work done in this topic for potential readers.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Focus Estimation in Academic Environments Using Computer Vision",
        "doc_scopus_id": "85076101159",
        "doc_doi": "10.1007/978-3-030-31332-6_54",
        "doc_eid": "2-s2.0-85076101159",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Academic environment",
            "Computer vision algorithms",
            "Face Tracking",
            "Group workshops",
            "System output"
        ],
        "doc_abstract": "© 2019, Springer Nature Switzerland AG.In this paper we propose a system capable of monitoring students’ focus through cameras and using Computer Vision algorithms. Experimental results show that our system is capable of identifying students and tracking their focus during a class. At the end of the class, the system outputs graphical feedback to teachers regarding the average level of students’ focus. Moreover, it can identify lecture periods in which students were less watchful and the corresponding topics that potentially need extra focus. In this paper we start by presenting the architecture of the system, followed by results obtained both during a small-group workshop and a classroom with a large number of attending students. The main goal of this work is to contribute to the transformation of the classroom as a sensing environment, providing information to both teachers and students about their engagement during the class.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UA.Pt bioinformatics at ImageClef 2019: Lifelog moment retrieval based on image annotation and natural language processing",
        "doc_scopus_id": "85070513155",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85070513155",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Engineering informatics",
            "Life log",
            "NAtural language processing",
            "Natural languages",
            "Pre-processing step",
            "Processed images",
            "Processing tools",
            "Visual information"
        ],
        "doc_abstract": "© 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).The increasing number of mobile and wearable devices is dramatically changing the way we can collect data about a person's life. These devices allow recording our daily activities and behavior in the form of images, video, biometric data, location and other data. This paper describes the participation of the Bioinformatics group of the Institute of Electronics and Engineering Informatics of University of Aveiro in the ImageCLEF lifelog task, more specifically in the Lifelog Moment Retrieval sub-task. The approach to solve this sub-task is divided into three stages. The first one is the pre-processing of the lifelog dataset for a selection of the images that contain relevant information in order to reduce the amount of images to be processed and obtain additional visual information and concepts from the ones to be considered. In the second step, the query topics are analyzed using Natural Languages Processing tools to extract relevant words to retrieve the desired moment. This words are compared with the visual concepts words, obtained in the pre-processing step using a pre-trained word2vec model, to compute a confidence score for each processed image. An additional step is used in the last two runs, in order to include the images not processed in the first step and improve the results of our approach. A total of 6 runs were submitted and the results obtained show an evolution with each submission. Although the results are not yet competitive with other teams, this challenge is a good starting point for our research work. We pretend to continue the development of a lifelogging application in the context of a research project, so we expect to participate in the next year in the ImageCLEFlifelog task.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A Personal Robot as an Improvement to the Customers’ In-store Experience",
        "doc_scopus_id": "85057567408",
        "doc_doi": "10.1007/978-3-030-02907-4_15",
        "doc_eid": "2-s2.0-85057567408",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Competitive business",
            "Functionalities",
            "Hardware and software",
            "Integrated architecture",
            "People with reduced mobilities",
            "Personal assistants",
            "Requirements",
            "Retail"
        ],
        "doc_abstract": "© 2019, Springer Nature Switzerland AG.Robotics is a growing industry with applications in numerous markets, including retail, transportation, manufacturing, and even as personal assistants. Consumers have evolved to expect more from the buying experience, and retailers are looking at technology to keep consumers engaged. There are currently many interesting initiatives that explore how robots can be used in retail. In today’s highly competitive business climate, being able to attract, serve, and satisfy more customers is a key to success. A happy customer is more likely to be a loyal one, who comes back and often to the store. It is our belief that smart robots will play a significant role in physical retail in the future. One successful example is wGO, a robotic shopping assistant developed by FollowInspiration. The wGO is an autonomous and self-driven shopping cart, designed to follow people with reduced mobility in commercial environments. With the Retail Robot, the user can control the shopping cart without the need to push it. This brings numerous advantages and a higher level of comfort since the user does not need to worry about carrying the groceries or pushing the shopping cart. The wGO operates under a vision-guided approach based on user-following with no need for any external device. Its integrated architecture of control, navigation, perception, planning, and awareness is designed to enable the robot to successfully perform personal assistance while the user is shopping. This paper presents the wGOs functionalities and requirements to enable the robot to successfully perform personal assistance while the user is shopping in a safe way. It also presents the details about the robot’s behaviour, hardware and software technical characteristics. Experiments conducted in real scenarios were very encouraging and a high user satisfaction was observed. Based on these results, some conclusions and guidelines towards the future full deployment of the wGO in commercial environments are drawn.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of Aruco Markers Using the Quadrilateral Sum Conjuncture",
        "doc_scopus_id": "85049453966",
        "doc_doi": "10.1007/978-3-319-93000-8_41",
        "doc_eid": "2-s2.0-85049453966",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Aruco",
            "Digital image",
            "Fiducial marker",
            "Internal angles",
            "Marker detections",
            "Pose estimation"
        ],
        "doc_abstract": "© 2018, Springer International Publishing AG, part of Springer Nature.Fiducial Markers are heavily used for pose estimation in many applications from robotics to augmented reality. In this paper we present an algorithm for the detection of aruco marker at larger distance. The algorithm uses the quadrilateral sum conjecture and analyzes the sum of the cosine of the internal angles to detect squares at larger distances. Experiments conducted showed that the developed solution was able to improve the detection distance when compared to other methods that use similar marker while keeping similar pose estimation precision.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Monitoring students’ attention in a classroom through computer vision",
        "doc_scopus_id": "85049374546",
        "doc_doi": "10.1007/978-3-319-94779-2_32",
        "doc_eid": "2-s2.0-85049374546",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Computer vision techniques",
            "Face Tracking",
            "Learning environments",
            "Pose estimation",
            "State of the art",
            "State-of-the-art approach"
        ],
        "doc_abstract": "© 2018, Springer International Publishing AG, part of Springer Nature.Monitoring classrooms using cameras is a non-invasive approach of digitizing students’ behaviour. Understanding students’ attention span and what type of behaviours may indicate a lack of attention is fundamental for understanding and consequently improving the dynamics of a lecture. Recent studies show useful information regarding classrooms and their students’ behaviour throughout the lecture. In this paper we start by presenting an overview about the state of the art on this topic, presenting what we consider to be the most robust and efficient Computer Vision techniques for monitoring classrooms. After the analysis of relevant state of the art, we propose an agent that is theoretically capable of tracking the students’ attention and output that data. The main goal of this paper is to contribute to the development of an autonomous agent able to provide information to both teachers and students and we present preliminary results on this topic. We believe this autonomous agent features the best solution for monitoring classrooms since it uses the most suited state of the art approaches for each individual role.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A study on face identification for an outdoor identity verification system",
        "doc_scopus_id": "85032383933",
        "doc_doi": "10.1007/978-3-319-68195-5_75",
        "doc_eid": "2-s2.0-85032383933",
        "doc_date": "2018-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018, Springer International Publishing AG.As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. One of the many uses of face recognition is building access control where a person has one or several photos associated to an Identification Document (also known as identity verification). This paper focuses on the use of face recognition methods in the context of an Identity Verification System to be used under natural light. Experimental results are presented using the most important detection and recognition algorithms taking into consideration several problems: ageing, face rotation, sensor used and illumination. Some pre-processing techniques are proposed using face alignment and auto calibration of camera parameters. The results using these pre-processing algorithms are then compared and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Human-robot interaction based on gestures for service robots",
        "doc_scopus_id": "85032352025",
        "doc_doi": "10.1007/978-3-319-68195-5_76",
        "doc_eid": "2-s2.0-85032352025",
        "doc_date": "2018-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018, Springer International Publishing AG.Gesture recognition is very important for Human-Robot Interfaces. In this paper, we present a novel depth based method for gesture recognition to improve the interaction of a service robot autonomous shopping cart, mostly used by reduced mobility people. In the proposed solution, the identification of the user is already implemented by the software present on the robot where a bounding box focusing on the user is extracted. Based on the analysis of the depth histogram, the distance from the user to the robot is calculated and the user is segmented using from the background. Then, a region growing algorithm is applied to delete all other objects in the image. We apply again a threshold technique to the original image, to obtain all the objects in front of the user. Intercepting the threshold based segmentation result with the region growing resulting image, we obtain candidate objects to be arms of the user. By applying a labelling algorithm to obtain each object individually, a Principal Component Analysis is computed to each one to obtain its center and orientation. Using that information, we intercept the silhouette of the arm with a line obtaining the upper point of the interception which indicates the hand position. A Kalman filter is then applied to track the hand and based on state machines to describe gestures (Start, Stop, Pause) we perform gesture recognition. We tested the proposed approach in a real case scenario with different users and we obtained an accuracy around 89,7%.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Cooperative sensing for 3D ball positioning in the RoboCup middle size league",
        "doc_scopus_id": "85034251915",
        "doc_doi": "10.1007/978-3-319-68792-6_22",
        "doc_eid": "2-s2.0-85034251915",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Catadioptric vision",
            "Communication delays",
            "Cooperative sensing",
            "High resolution",
            "Limited resolution",
            "Omnivision cameras",
            "Performance tests",
            "Perspective cameras"
        ],
        "doc_abstract": "© 2017, Springer International Publishing AG.As soccer in the RoboCup Middle Size League (MSL) starts resembling human soccer more and more, the time the ball is airborne increases. Robots equipped with a single catadioptric vision system will generally not be able to accurately observe depth due to limited resolution. Most teams, therefore, resort to projecting the ball on the field. Within the MSL several methods have already been explored to determine the 3D ball position, e.g., adding a high-resolution perspective camera or adding a Kinect sensor. This paper presents a new method which combines the omnivision camera data from multiple robots through triangulation. Three main challenges have been identified in designing this method: Inaccurate projections, Communication delay and Limited amount of data. An algorithm, considering these main challenges, has been implemented and tested. Performance tests with a non-moving ball (static situation) and two robots show an accuracy of 0.13 m for airborne balls. A dynamic test shows that a ball kicked by a robot could be tracked from the moment of the kick, if enough measurements have been received from two peer robots before the ball exceeds the height of the robots.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Functionalities and requirements of an autonomous shopping vehicle for people with reduced mobility",
        "doc_scopus_id": "85024380664",
        "doc_doi": "10.5220/0006385903730380",
        "doc_eid": "2-s2.0-85024380664",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Automotive Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2203"
            },
            {
                "area_name": "Transportation",
                "area_abbreviation": "SOCI",
                "area_code": "3313"
            }
        ],
        "doc_keywords": [
            "Functionalities",
            "People with reduced mobilities",
            "Requirements",
            "Retail",
            "Shopping carts",
            "User satisfaction"
        ],
        "doc_abstract": "Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.This paper concerns a robot to assist people in retail shopping scenarios, called the wGO. The robot's behaviour is based in a vision-guided approach based on user-following. The wGO brings numerous advantages and a higher level of comfort, since the user does not need to worry about controlling the shopping cart. In addition, this paper introduces the wGOs functionalities and requirements to enable the robot to successfully perform personal assistance while the user is shopping in a safe way. A user satisfaction survey is also presented. Based on the highly encouraging results, some conclusions and guidelines towards the future full deployment of the wGO in commercial environments are drawn.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A ground truth vision system for robotic soccer",
        "doc_scopus_id": "84969962024",
        "doc_doi": "10.5220/0005817506840689",
        "doc_eid": "2-s2.0-84969962024",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Biped walking",
            "Ground truth",
            "Monitoring system",
            "Perception systems",
            "Robotic soccer",
            "Robotic soccer game",
            "Robotic vision",
            "Vision systems"
        ],
        "doc_abstract": "© Copyright 2016 by SCITEPRESS -Science and Technology Publications, Lda. All rights reserved.Robotic soccer represents an innovative and appealing test bed for the most recent advances in multi-agent systems, artificial intelligence, perception and navigation and biped walking. The main sensorial element of a soccer robot must be its perception system, most of the times based on a digital camera, through which the robot analyses the surrounding world and performs accordingly. Up to this date, the validation of the vision system of a soccer robots can only be related to the way the robot and its team mates interpret the surroundings, relative to their owns. In this paper we propose an external monitoring vision system that can act as a ground truth system for the validations of the objects of interest of a robotic soccer game, mainly robots and ball. The system we present is made of two to four digital cameras, strategically positioned above the soccer field. We present preliminary results regarding the accuracy of the detection of a soccer ball, which proves that such a system can indeed be used as a provider for ground truth ball positions on the field during a robotic soccer game.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the use of feature descriptors on raw image data",
        "doc_scopus_id": "84969951371",
        "doc_doi": "10.5220/0005756506550662",
        "doc_eid": "2-s2.0-84969951371",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Bayer pattern",
            "Descriptors",
            "Feature descriptors",
            "Gray-scale images",
            "Object detection and recognition",
            "Raw data",
            "Raw image data",
            "Research efforts"
        ],
        "doc_abstract": "© Copyright 2016 by SCITEPRESS -Science and Technology Publications, Lda. All rights reserved.Local feature descriptors and detectors have been widely used in computer vision in the last years for solving object detection and recognition tasks. Research efforts have been focused on reducing the complexity of these descriptors and improving their accuracy. However, these descriptors have not been tested until now on raw image data. This paper presents a study on the use of two of the most known and used feature descriptors, SURF and SIFT, directly on raw CFA images acquired by a digital camera. We are interested in understanding if the number and quality of the keypoints obtained from a raw image are comparable to the ones obtained in the grayscale images, which are normally used by these transforms. The results that we present show that the number and positions of the keypoints obtained from grayscale images are similar to the ones obtained from CFA images and furthermore to the ones obtained from grayscale images that resulted directly from the interpolation of a CFA image.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Time-constrained detection of colored objects on raw Bayer data",
        "doc_scopus_id": "84959311753",
        "doc_doi": "10.1201/b19241-51",
        "doc_eid": "2-s2.0-84959311753",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Autonomous robotics",
            "Color object detection",
            "Demosaicing algorithm",
            "Full color images",
            "Image processing applications",
            "Object detection algorithms",
            "Perception and actions",
            "Real-time application"
        ],
        "doc_abstract": "© 2016 Taylor & Francis Group, London.In most image processing applications, the raw data acquired by an image sensor passes through a demosaicing algorithm in order to obtain a full color image. The demosaicing step can take time and resources that are of great importance when developing real-time applications. Most of the times, this is done with single purpose of rendering the images into a viewable format for humans. However, most of nowadays sensors allow the retrieval of images in raw format and processing the data as is. In this paper we present a study on the direct usage of raw Bayer data for real-time color object detection, in the scenario of autonomous robot soccer. The experimental results that we provide prove that the efficiency of the chosen object detection algorithm is not lowered and that several improvements of the autonomous robotic vision system used in this study can be noted: the bandwidth needed to transmit the data between the digital camera and the PC is reduced, allowing, in some models, to increase the maximum frame rate. Moreover, a decrease in the delay between the perception and action of the autonomous robot was measured, when using raw image data for processing the environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Image scanning techniques for speeded-up color object detection",
        "doc_scopus_id": "84959285043",
        "doc_doi": "10.1201/b19241-52",
        "doc_eid": "2-s2.0-84959285043",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Color object detection",
            "Color segmentation",
            "Computational costs",
            "Computer vision algorithms",
            "Different resolutions",
            "Low resolution images",
            "Object detection algorithms",
            "Real time capability"
        ],
        "doc_abstract": "© 2016 Taylor & Francis Group, London.Nowadays there are many computer vision algorithms dedicated to solve the problem of object detection, from many different perspectives. Many of these algorithms take a considerable processing time even for low resolution images. Research areas that are more concerned about real-time capabilities of a system, such as the case of robotics for example, are focusing on adapting the existing algorithms or developing new ones that cope with time constraints. Color object detection algorithms are often used in time-constrained application since the color of an object is an important clue for its detection. In addition, color segmentation algorithms have a smaller computational cost than generic object detection algorithms. Most of the color object detection algorithms include a step of image scanning in search of pixels of the color of interest. In this paper we present a study on the influence of an image scanning pattern while searching for pixels of a certain color versus the image resolution in the results of three color object detection algorithms. We present results comparing different scanning approaches versus different resolution images for color object detection algorithms based on blob formation, contours detection and region growing clustering. Our experiments show that using search lines for color segmentation, followed by blob formation provides faster and more accurate results.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of aerial balls in robotic soccer using a mixture of color and depth information",
        "doc_scopus_id": "84933059765",
        "doc_doi": "10.1109/ICARSC.2015.13",
        "doc_eid": "2-s2.0-84933059765",
        "doc_date": "2015-05-04",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "Ball detection",
            "Color classification",
            "League competition",
            "Omni-directional vision",
            "Omnidirectional cameras",
            "Real-time application",
            "Robotic vision",
            "Trajectory calculations"
        ],
        "doc_abstract": "© 2015 IEEE.Detection of aerial objects is a difficult problem to tackle given the dynamics and speed of a flying object. The problem is even more difficult when considering a noncontrolled environment, where the predominance of a given color is not guaranteed, and/or when the vision system is located on a moving platform. Taking as an example the game of robotic soccer promoted by the RoboCup Federation, most of the teams participating in the soccer competitions detect the objects in the environment using an omni directional camera. Omni directional vision systems only detect the ball when it is on the ground, and thus precise information on the ball position when in the air is lost. In this paper we present a novel approach for 3D ball detection in which we use the color information to identify ball candidates and the 3D data for filtering the relevant color information. The main advantage of our approach is the low processing time, being thus suitable for real-time applications. We present experimental results showing the effectiveness of the proposed algorithm. Moreover, this approach was already used in the last official RoboCup Middle Size League competition. The goalkeeper was able to move to a right position in order to defend a goal, in situations where the ball was flying towards the goal.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving the kicking accuracy in a soccer robot",
        "doc_scopus_id": "84955480270",
        "doc_doi": "10.1145/2695664.2695862",
        "doc_eid": "2-s2.0-84955480270",
        "doc_date": "2015-04-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Accuracy",
            "Friction parameters",
            "Heuristic approach",
            "Kicking device",
            "Middle-Size League",
            "Physical characteristics",
            "RoboCu",
            "Target direction"
        ],
        "doc_abstract": "Copyright 2015 ACM.Most of the soccer robots in the Middle Size League of Robo-Cup use electromagnetic kicking devices that allow to kick the ball with adjustable strength. In order to be efficient to score, the kicking strength should be calculated according to several parameters, namely the physical characteristics of the kicking device, the distance to the target, the velocity of the robot and the floor friction parameters. Moreover, the kicking decision should be taken at a moment in which the actual movement of the robot would result in the ball being released to the target direction, even if it is not physically aligned with it. This paper proposes an algorithm to improve the kicking accuracy, taking into account the described parameters, in a heuristic approach. The experimental results presented in this paper show the effectiveness of the proposed solution to improve the efficiency of the kicking device.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Mafco: A compression tool for MAF files",
        "doc_scopus_id": "84929484087",
        "doc_doi": "10.1371/journal.pone.0116082",
        "doc_eid": "2-s2.0-84929484087",
        "doc_date": "2015-03-27",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [
            "Data Compression",
            "Genomics",
            "Sequence Alignment",
            "Time Factors"
        ],
        "doc_abstract": "© 2015 Matos et al.In the last decade, the cost of genomic sequencing has been decreasing so much that researchers all over the world accumulate huge amounts of data for present and future use. These genomic data need to be efficiently stored, because storage cost is not decreasing as fast as the cost of sequencing. In order to overcome this problem, the most popular generalpurpose compression tool, gzip, is usually used. However, these tools were not specifically designed to compress this kind of data, and often fall short when the intention is to reduce the data size as much as possible. There are several compression algorithms available, even for genomic data, but very few have been designed to deal with Whole Genome Alignments, containing alignments between entire genomes of several species. In this paper, we present a lossless compression tool, MAFCO, specifically designed to compress MAF (Multiple Alignment Format) files. Compared to gzip, the proposed tool attains a compression gain from 34% to 57%, depending on the data set. When compared to a recent dedicated method, which is not compatible with some data sets, the compression gain of MAFCO is about 9%. Both source-code and binaries for several operating systems are freely available for non-commercial use at: http://bioinformatics.ua.pt/software/mafco.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Batch Reinforcement Learning for Robotic Soccer Using the Q-Batch Update-Rule",
        "doc_scopus_id": "84945484531",
        "doc_doi": "10.1007/s10846-014-0171-1",
        "doc_eid": "2-s2.0-84945484531",
        "doc_date": "2015-01-09",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Adversarial environments",
            "Amount of interaction",
            "Batch reinforcement learning",
            "Class of methods",
            "Q-batch",
            "Reinforcement learning method",
            "Robotic soccer",
            "Robotics applications"
        ],
        "doc_abstract": "© 2015, Springer Science+Business Media Dordrecht.Reinforcement Learning is increasingly becoming a valuable alternative to tackle many of the challenges existing in a semi-structured, non-deterministic and adversarial environment such as robotic soccer. Batch Reinforcement Learning is a class of Reinforcement Learning methods characterized by processing a batch of interactions. By storing all past interactions, Batch RL methods are extremely data-efficient which makes this class of methods very appealing for robotics applications, specially when learning directly on physical robotic platforms.This paper presents the application of Batch Reinforcement Learning to obtain efficient robotic soccer controllers on physical platforms. To learn the controllers we propose the application of Q-Batch, a novel update-rule that exploits the episodic nature of the interactions in Batch Reinforcement Learning. The approach was validated in three different tasks with increasing difficulty. Results show the proposed approach is able to outperform hand-coded policies, for all the tasks, in a reduced amount of time. Additionally, for one of the tasks, a comparison between Q-Batch and Q-learning is carried out, and results show that, Q-Batch obtains better policies than Q-learning for the same amount of interaction time.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Detection of aerial balls using a Kinect sensor",
        "doc_scopus_id": "84958548108",
        "doc_doi": "10.1007/978-3-319-18615-3_44",
        "doc_eid": "2-s2.0-84958548108",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Detection of objects in the air is a difficult problem to tackle given the dynamics and speed of a flying object. The problem is even more difficult when considering a non-controlled environment where the predominance of a given color is not guaranteed, and/or when the vision system is located on a moving platform. As an example, most of the Middle Size League teams in RoboCup competition detect the objects in the environment using an omni directional camera that only detects the ball when in the ground, and losing any precise information of the ball position when in the air. In this paper we present a first approach towards the detection of a ball flying using a Kinect camera as sensor. The approach only uses 3D data and does not consider, at this time, any additional intensity information. The objective at this stage is to evaluate how useful is the use of 3D information in the Middle Size League context. A simple algorithm to detect a flying ball and evaluate its trajectory was implemented and preliminary results are presented.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UAVision: A modular time-constrained vision library for soccer robots",
        "doc_scopus_id": "84958540813",
        "doc_doi": "10.1007/978-3-319-18615-3_40",
        "doc_eid": "2-s2.0-84958540813",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.The game of soccer is one of the main focuses of the RoboCup competitions, being a fun and entertaining research environment for the development of autonomous multi-agent cooperative systems. For an autonomous robot to be able to play soccer, first it has to perceive the surrounding world and extract only the relevant information in the game context. Therefore, the vision system of a robotic soccer player is probably the most important sensorial element, on which the acting of the robot is fully based. In this paper we present a new modular time-constrained vision library, named UAVision, that allows the use of video sensors up to a frame rate of 50 fps in full resolution and provides accurate results in terms of detection of the objects of interest for a robot playing soccer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new approach for dynamic strategic positioning in RoboCup middle-size league",
        "doc_scopus_id": "84945909472",
        "doc_doi": "10.1007/978-3-319-23485-4_43",
        "doc_eid": "2-s2.0-84945909472",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Best position",
            "Cooperative tasks",
            "Large parts",
            "New approaches",
            "Robotic soccer",
            "Robotic soccer team",
            "Strategic positioning",
            "Team performance"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.Coordination in multi-robot or multi-agent systems has been receiving special attention in the last years and has a prominent role in the field of robotics. In the robotic soccer domain, the way that each team coordinates its robots, individually and together, in order to perform cooperative tasks is the base of its strategy and in large part dictates the success of the team in the game. In this paper we propose the use of Utility Maps to improve the strategic positioning of a robotic soccer team. Utility Maps are designed for different set pieces situations, making them more dynamic and easily adaptable to the different strategies used by the opponent teams. Our approach has been tested and successfully integrated in normal game situations to perform passes in free-play, allowing the robots to choose, in real-time, the best position to receive and pass the ball. The experimental results obtained, as well as the analysis of the team performance during the last RoboCup competition show that the use of Utility Maps increases the efficiency of the team strategy.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Compression of microarray images using a binary tree decomposition",
        "doc_scopus_id": "84911937859",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84911937859",
        "doc_date": "2014-11-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Binary tree decomposition",
            "Compression standards",
            "Context modeling",
            "Hierarchical organizations",
            "Intensity levels",
            "JPEG-LS",
            "Lossless compression",
            "Microarray images"
        ],
        "doc_abstract": "© 2014 EURASIP.This paper proposes a lossless compression method for microarray images, based on a hierarchical organization of the intensity levels followed by finite-context modeling. A similar approach was recently applied to medical images with success. The goal of this work was to further extend, adapt and evaluate this approach to the special case of microarray images. We performed simulations on seven different data sets (total of 254 images). On average, the proposed method attained 9% better results when compared to the best compression standard (JPEG-LS).",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "UAVision: A modular time-constrained vision library for color-coded object detection",
        "doc_scopus_id": "84905852257",
        "doc_doi": "10.1007/978-3-319-09994-1_35",
        "doc_eid": "2-s2.0-84905852257",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Artificial vision system",
            "Computer vision library",
            "Human vision systems",
            "Object Detection",
            "Robotic soccer game",
            "Traffic sign detection",
            "Video sensors",
            "Vision library"
        ],
        "doc_abstract": "The ultimate goal of Computer Vision has been, for more than half of century, to create an artificial vision system that could imitate the human vision. The artificial vision system should have all the capabilities of the human vision system but must not carry the same flaws. Robotics and Automation are just two examples of research areas that use artificial vision systems as the main sensorial element. In these areas, the use of color-coded objects is very common since it relieves the burden of information processing while being an unobtrusive restraint of the environment. We present a novel computer vision library called UAVision that provides support for different video sensors technologies and all the necessary software for implementing an artificial vision system for the detection of color-coded objects. The experimental results that we present, both for the scenario of robotic soccer games and for traffic sign detection, show that our library can work at more than 50fps with images of 1 megapixel. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Self-calibration of colormetric parameters in vision systems for autonomous soccer robots",
        "doc_scopus_id": "84905509813",
        "doc_doi": "10.1007/978-3-662-44468-9_17",
        "doc_eid": "2-s2.0-84905509813",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Extract informations",
            "Humanoid soccer robots",
            "Initial configuration",
            "Intensity histograms",
            "Lighting intensity",
            "Robotic applications",
            "Statistical information",
            "Surrounding environment"
        ],
        "doc_abstract": "Vision is an extremely important sense for both humans and robots, providing detailed information about the environment. In the past few years, the use of digital cameras in robotic applications has been significantly increasing. The use of digital cameras as the main sensor allows the robot to capture the relevant information of the surrounding environment and take decisions. A robust vision system should be able to reliably detect objects and present an accurate representation of the world to higher-level processes, not only under ideal light conditions, but also under changing lighting intensity and color balance. To extract information from the acquired image, shapes or colors, the configuration of the colormetric camera parameters, such as exposure, gain, brightness or white-balance, among others, is very important. In this paper, we propose an algorithm for the self-calibration of the most important parameters of digital cameras for robotic applications. The algorithms extract some statistical information from the acquired images, namely the intensity histogram, saturation histogram and information from a black and a white area of the image, to then estimate the colormetric parameters of the camera. We present experimental results with two robotic platforms, a wheeled robot and a humanoid soccer robot, in challenging environments: soccer fields, both indoor and outdoor, that show the effectiveness of our algorithms. The images acquired after calibration show good properties for further processing, independently of the initial configuration of the camera and the type and amount of light of the environment, both indoor and outdoor. © 2014 Springer-Verlag Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Learning robotic soccer controllers with the Q-Batch update-rule",
        "doc_scopus_id": "84905017754",
        "doc_doi": "10.1109/ICARSC.2014.6849775",
        "doc_eid": "2-s2.0-84905017754",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            }
        ],
        "doc_keywords": [
            "Competitive environment",
            "Data collection",
            "Learning tasks",
            "Parameter-tuning",
            "Physical robots",
            "RoboCup",
            "Robotic soccer",
            "Simulated environment"
        ],
        "doc_abstract": "Robotic soccer provides a rich environment for the development of Reinforcement Learning controllers. The competitive environment imposes strong requirements on performance of the developed controllers. RL offers a valuable alternative for the development of efficient controllers while avoiding the hassle of parameter tuning a hand coded policy. This paper presents the application of a recently proposed Batch RL update-rule to learn robotic soccer controllers in the context of the RoboCup Middle Size League. The Q-Batch update-rule exploits the episodic structure of the data collection phase of Batch RL to efficiently evaluate and improve the learned policy. Three different learning tasks, with increasing difficulty, were developed and applied on a simulated environment and later on the physical robot. The performance of the learned controllers is mostly compared to hand-tuned controllers while some comparisons with other RL methods were performed. Results show that the proposed approach is able to learn the tasks in a reduced amount of time, even outperforming existing hand-coded solutions. © 2014 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Aerial ball perception based on the use of a single perspective camera",
        "doc_scopus_id": "84884705454",
        "doc_doi": "10.1007/978-3-642-40669-0_21",
        "doc_eid": "2-s2.0-84884705454",
        "doc_date": "2013-10-03",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color segmentation",
            "Hybrid process",
            "Perception-based",
            "Perspective cameras",
            "Position estimation",
            "Regions of interest",
            "Size and position",
            "Vision systems"
        ],
        "doc_abstract": "The detection of the ball when it is not on the ground is an important research line within the Middle Size League of RoboCup. A correct detection of airborne balls is particularly important for goal keepers, since shots to goal are usually made that way. To tackle this problem on the CAMBADA team , we installed a perspective camera on the robot. This paper presents an analysis of the scenario and assumptions about the use of a single perspective camera for the purpose of 3D ball perception. The algorithm is based on physical properties of the perspective vision system and an heuristic that relates the size and position of the ball detected in the image and its position in the space relative to the camera. Regarding the ball detection, we attempt an approach based on a hybrid process of color segmentation to select regions of interest and statistical analysis of a global shape context histogram. This analysis attempts to classify the candidates as round or not round. Preliminary results are presented regarding the ball detection approach that confirms its effectiveness in uncontrolled environments. Moreover, experimental results are also presented for the ball position estimation and a sensor fusion proposal is described to merge the information of the ball into the worldstate of the robot. © 2013 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "New telerehabilitation services for the elderly",
        "doc_scopus_id": "84887892291",
        "doc_doi": "10.4018/978-1-4666-3990-4.ch006",
        "doc_eid": "2-s2.0-84887892291",
        "doc_date": "2013-04-30",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            },
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            }
        ],
        "doc_keywords": [
            "Cognitive capability",
            "Health-care system",
            "Multi-Modal Interactions",
            "Older People",
            "Telerehabilitation",
            "Unsolved problems",
            "Vision problems",
            "Work in progress"
        ],
        "doc_abstract": "© 2013 by IGI Global. All rights reserved.The world's population is getting older with the percentage of people over 60 increasing more rapidly than any other age group. Telerehabilitation may help minimise the pressure this puts on the traditional healthcare system, but recent studies showed ease of use, usability, and accessibility as unsolved problems, especially for older people who may have little experience or confidence in using technology. Current migration towards multimodal interaction has benefits for seniors, allowing hearing and vision problems to be addressed by exploring redundancy and complementarity of modalities. This chapter presents and contextualizes work in progress in a new telerehabilitation service targeting the combined needs of the elderly to have professionally monitored exercises without leaving their homes with their need regarding interaction, directly related to age-related effects on, for example, vision, hearing, and cognitive capabilities. After a brief general overview of the service, additional information on its two supporting applications are presented, including information on user interfaces. First results from a preliminary evaluation are also included.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of color spaces for user-supervised color classification in robotic vision",
        "doc_scopus_id": "85072926641",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85072926641",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Autonomous Mobile Robot",
            "Color classification",
            "Color image segmentation",
            "Graphical applications",
            "Image processing applications",
            "Robotic soccer players",
            "Robotic vision",
            "Supervised algorithm"
        ],
        "doc_abstract": "© 2013 CSREA Press. All rights reserved.Autonomous robots are becoming an integrated part of our daily life. The use of a robot for substituting man power in different activities that might be too dangerous, repetitive or time consuming, has become a common procedure nowadays. Robotic soccer is a research branch that focuses on developing autonomous mobile robots, using the game of soccer as a testing platform. The soccer environment in RoboCup competitions is still more controlled than the one of the regular soccer games among human teams. For the vision system of the robotic soccer players, the colors of the objects of interest are still important clues for their detection. Thus, most of the research teams attending the robotic soccer competitions, use manual or user-supervised color classification procedures before each game, in order to guarantee an accurate object detection during the game. This paper intends to be an evaluation of the most common color spaces used in image processing applications that are based on color segmentation. The paper presents a graphical application used for testing both the performance of human users in the classification of different colors, under different color spaces, as well as the performance of user supervised algorithms for color classification. The results acquired prove that the color spaces which separate the luminance information from the chromatic one, mainly the YUV color space, provide a more accurate outcome.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Self-calibration of colormetric parameters in vision systems for autonomous mobile robots",
        "doc_scopus_id": "85072918488",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85072918488",
        "doc_date": "2013-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Autonomous driving agents",
            "Autonomous Mobile Robot",
            "Camera calibration",
            "Colormetric",
            "Initial configuration",
            "Robotic vision",
            "Statistical information",
            "Surrounding environment"
        ],
        "doc_abstract": "© 2013 CSREA Press. All rights reserved.Vision is an extremely important sense for both humans and robots, providing detailed information about the environment. In the past few years, the use of digital cameras in robotic applications has been increasing significantly. The use of digital cameras as the main sensor allows the robot to take the relevant information from the surrounding environment and then take decisions. A robust vision system should be able to detect objects reliably and present an accurate representation of the world to higher-level processes, not only under ideal light conditions, but also under changing light intensity and color balance. In this paper, we propose an algorithm for the self-calibration of the most important parameters of digital cameras for robotic applications. The algorithm extracts statistical information from the acquired images, namely the intensity histogram, saturation histogram and information from a black and a white area of the image, to then estimate the colormetric parameters of the camera. We present experimental results obtained with several autonomous robotic platforms: two wheeled platforms, with different architectures of the vision system, a humanoid robots and an autonomous driving agent. The images acquired after calibration show good properties for further processing, independently of the initial configuration of the camera and the type and amount of light of the environment, both indoor and outdoor.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "From an autonomous soccer robot to a robotic platform for elderly care",
        "doc_scopus_id": "84861984252",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84861984252",
        "doc_date": "2012-06-13",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Developed countries",
            "Elderly care",
            "Hardware and software",
            "Home care",
            "Independent living",
            "Innovative solutions",
            "Number of peoples",
            "Nursing homes",
            "Robotic platforms",
            "Robotic soccer",
            "Soccer robot"
        ],
        "doc_abstract": "Current societies in developed countries face a serious problem of aged population. The growing number of people with reduced health and capabilities, allied with the fact that elders are reluctant to leave their own homes to move to nursing homes, requires innovative solutions since continuous home care can be very expensive and dedicated 24/7 care can only be accomplished by more than one care-giver. This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A modular real-time vision module for humanoid robots",
        "doc_scopus_id": "84857012751",
        "doc_doi": "10.1117/12.911206",
        "doc_eid": "2-s2.0-84857012751",
        "doc_date": "2012-02-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Acquisition modules",
            "Camera calibration",
            "Camera parameter",
            "Client-server models",
            "Color calibration",
            "Color classification",
            "Color-coded environments",
            "Computational capability",
            "Field lines",
            "Hardware architecture",
            "Humanoid robot",
            "Its efficiencies",
            "Modular designs",
            "Modular vision",
            "Object Detection",
            "Processing capability",
            "Processing Time",
            "Real time",
            "Real time vision",
            "RoboCup",
            "Robotic platforms",
            "Robotic soccer",
            "Robotic vision",
            "Self calibration",
            "Self-calibration algorithms",
            "Soccer games",
            "Soccer-playing robots",
            "Vision systems",
            "Worst case scenario"
        ],
        "doc_abstract": "Robotic vision is nowadays one of the most challenging branches of robotics. In the case of a humanoid robot, a robust vision system has to provide an accurate representation of the surrounding world and to cope with all the constraints imposed by the hardware architecture and the locomotion of the robot. Usually humanoid robots have low computational capabilities that limit the complexity of the developed algorithms. Moreover, their vision system should perform in real time, therefore a compromise between complexity and processing times has to be found. This paper presents a reliable implementation of a modular vision system for a humanoid robot to be used in color-coded environments. From image acquisition, to camera calibration and object detection, the system that we propose integrates all the functionalities needed for a humanoid robot to accurately perform given tasks in color-coded environments. The main contributions of this paper are the implementation details that allow the use of the vision system in real-time, even with low processing capabilities, the innovative self-calibration algorithm for the most important parameters of the camera and its modularity that allows its use with different robotic platforms. Experimental results have been obtained with a NAO robot produced by Aldebaran, which is currently the robotic platform used in the RoboCup Standard Platform League, as well as with a humanoid build using the Bioloid Expert Kit from Robotis. As practical examples, our vision system can be efficiently used in real time for the detection of the objects of interest for a soccer playing robot (ball, field lines and goals) as well as for navigating through a maze with the help of color-coded clues. In the worst case scenario, all the objects of interest in a soccer game, using a NAO robot, with a single core 500Mhz processor, are detected in less than 30ms. Our vision system also includes an algorithm for self-calibration of the camera parameters as well as two support applications that can run on an external computer for color calibration and debugging purposes. These applications are built based on a typical client-server model, in which the main vision pipe runs as a server, allowing clients to connect and distantly monitor its performance, without interfering with its efficiency. The experimental results that we acquire prove the efficiency of our approach both in terms of accuracy and processing time. Despite having been developed for the NAO robot, the modular design of the proposed vision system allows it to be easily integrated into other humanoid robots with a minimum number of changes, mostly in the acquisition module. © 2012 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An heuristic for trajectory generation in mobile robotics",
        "doc_scopus_id": "80655142776",
        "doc_doi": "10.1109/ETFA.2011.6059220",
        "doc_eid": "2-s2.0-80655142776",
        "doc_date": "2011-11-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "2-D space",
            "Formal proofs",
            "Holonomic robots",
            "Initial position",
            "Mobile robotic",
            "Numerical experiments",
            "RoboCup",
            "Trajectory generation",
            "Trapezoidal velocity profile",
            "Viable solutions"
        ],
        "doc_abstract": "We present an heuristic to compute the points in a trajectory in a 2D space based on the trapezoidal velocity profile, where the computed trajectory is subject to the constraints of initial position and velocity, final position and velocity (defined as vectors) and the values for acceleration and plateau speed (defined as scalars). The proposed heuristics are directed at omnidirectional holonomic robots, i.e., robots that are capable of, amongst others, manoeuvring without affecting the orientation. These algorithms are currently being applied to the CAMBADA team RoboCup MSL robots. Although without a formal proof, numerical experiments have shown that the algorithms converged to a viable solution when the data fulfils the necessary conditions. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Humanoid behaviors: From simulation to a real robot",
        "doc_scopus_id": "80054807105",
        "doc_doi": "10.1007/978-3-642-24769-9_26",
        "doc_eid": "2-s2.0-80054807105",
        "doc_date": "2011-10-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Aldebaran Nao",
            "behaviors",
            "Humanoid robot",
            "Real robot",
            "simulation"
        ],
        "doc_abstract": "This paper presents the modifications needed to adapt a humanoid agent architecture and behaviors from simulation to a real robot. The experiments were conducted using the Aldebaran Nao robot model. The agent architecture was adapted from the RoboCup 3D Simulation League to the Standard Platform League with as few changes as possible. The reasons for the modifications include small differences in the dimensions and dynamics of the simulated and the real robot and the fact that the simulator does not create an exact copy of a real environment. In addition, the real robot API is different from the simulated robot API and there are a few more restrictions on the allowed joint configurations. The general approach for using behaviors developed for simulation in the real robot was to: first, (if necessary) make the simulated behavior compliant with the real robot restrictions, second, apply the simulated behavior to the real robot reducing its velocity, and finally, increase the velocity, while adapting the behavior parameters, until the behavior gets unstable or inefficient. This paper also presents an algorithm to calculate the three angles of the hip that produce the desired vertical hip rotation, since the Nao robot does not have a vertical hip joint. All simulation behaviors described in this paper were successfully adapted to the real robot. © 2011 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "AdaptO - Adaptive multimodal output",
        "doc_scopus_id": "80052477307",
        "doc_doi": null,
        "doc_eid": "2-s2.0-80052477307",
        "doc_date": "2011-09-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Communication",
                "area_abbreviation": "SOCI",
                "area_code": "3315"
            }
        ],
        "doc_keywords": [
            "Context-awareness",
            "Distributed systems",
            "Ehealth",
            "Multimodal output",
            "Smart environment",
            "Universal access",
            "User"
        ],
        "doc_abstract": "Currently, most multimodal output mechanisms use a very centralized architecture in which the various output modalities are completely devoid of any autonomy. Our proposal, AdaptO, uses an alternative approach, proving output modalities with the capacity to make decisions, thus collaborating with the fission output mechanism towards a more effective, modular, extensible and decentralized solution. In addition, our aim is to provide the mechanisms for a highly adaptable and intelligent multimodal output system, able to adapt itself to changing environment conditions (light, noise, distance, etc.) and to its users needs, limitations and personal choices.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DNA synthetic sequences generation using multiple competing Markov models",
        "doc_scopus_id": "80052209762",
        "doc_doi": "10.1109/SSP.2011.5967639",
        "doc_eid": "2-s2.0-80052209762",
        "doc_date": "2011-09-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Computational model",
            "DNA compression",
            "Markov model",
            "Sequence generation",
            "Statistical generator",
            "Synthetic DNA",
            "Synthetic sequence"
        ],
        "doc_abstract": "The development and implementation of computational models to represent DNA sequences is a great challenge. Markov models, usually known as finite-context models, have been used for a long time in DNA compression. In a previous work, we have shown that finite-context modelling can also be used for sequence generation. Furthermore, it is known that DNA is better represented by multiple finite-context models. However, the previous generator only allowed a single finite-context model to be used for generating a certain sequence. In this paper, we present results regarding a synthetic DNA generator based on multiple competing finite-context models. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A new living lab for usability evaluation of ICT and next generation networks for elderly@home",
        "doc_scopus_id": "79960543375",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960543375",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Conceptual model",
            "General population",
            "Living lab",
            "Next generation network",
            "Universal Design",
            "Usability evaluation",
            "Usability testing",
            "Work in progress"
        ],
        "doc_abstract": "Living Usability Lab for Next Generation Networks (www.livinglab.pt) is a Portuguese industry-academia collaborative R&D project, active in the field of live usability testing, focusing on the development of technologies and services to support healthy, productive and active citizens. The project adopts the principles of universal design and natural user interfaces (speech, gesture) making use of the benefits of next generation networks and distributed computing. Therefore, it will have impact on the general population, including the elderly and citizens with permanent or situational special needs. This paper presents project motivations, conceptual model, architecture and work in progress.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Output matters! Adaptable multimodal output for new telerehabilitation services for the elderly",
        "doc_scopus_id": "79960505802",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960505802",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Ambient assisted living",
            "Intelligent adaptation",
            "Multimodal output",
            "New services",
            "Older adults",
            "Prototype implementations",
            "Telehealth",
            "Telerehabilitation"
        ],
        "doc_abstract": "Technologies capable of being used by the older adults, with their specificities, to provide new services in the areas of telehealth and Ambient Assisted Living are needed. In this paper, we describe a new service, and its first prototype implementation, in the area of elderly health support at home. Special care was taken to improve the usability, adaptability to user and context and inclusiveness capabilities of the output. The basis for intelligent adaptation of the multimodal output - called AdaptO - is proposed and first versions of the needed services and agents were created.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A mobile robotic platform for elderly care",
        "doc_scopus_id": "79960496492",
        "doc_doi": null,
        "doc_eid": "2-s2.0-79960496492",
        "doc_date": "2011-07-25",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computational Theory and Mathematics",
                "area_abbreviation": "COMP",
                "area_code": "1703"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Elderly care",
            "Hardware and software",
            "Independent living",
            "Mobile robotic",
            "Next generation network",
            "Robotic platforms",
            "Robotic soccer"
        ],
        "doc_abstract": "This paper presents the proposal of a robotic platform for elderly care integrated in the Living Usability Lab for Next Generation Networks. The project aims at developing technologies and services tailored to enable the active aging and independent living of the elderly population. The proposed robotic platform is based on the CAMBADA robotic soccer platform, with the necessary modifications, both at hardware and software levels, while simultaneously applying the experiences achieved in the robotic soccer environment.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the representability of complete genomes by multiple competing finite-context (Markov) models",
        "doc_scopus_id": "79959722141",
        "doc_doi": "10.1371/journal.pone.0021588",
        "doc_eid": "2-s2.0-79959722141",
        "doc_date": "2011-07-04",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biochemistry, Genetics and Molecular Biology (all)",
                "area_abbreviation": "BIOC",
                "area_code": "1300"
            },
            {
                "area_name": "Agricultural and Biological Sciences (all)",
                "area_abbreviation": "AGRI",
                "area_code": "1100"
            },
            {
                "area_name": "Multidisciplinary",
                "area_abbreviation": "MULT",
                "area_code": "1000"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "A finite-context (Markov) model of order k yields the probability distribution of the next symbol in a sequence of symbols, given the recent past up to depth k. Markov modeling has long been applied to DNA sequences, for example to find gene-coding regions. With the first studies came the discovery that DNA sequences are non-stationary: distinct regions require distinct model orders. Since then, Markov and hidden Markov models have been extensively used to describe the gene structure of prokaryotes and eukaryotes. However, to our knowledge, a comprehensive study about the potential of Markov models to describe complete genomes is still lacking. We address this gap in this paper. Our approach relies on (i) multiple competing Markov models of different orders (ii) careful programming techniques that allow orders as large as sixteen (iii) adequate inverted repeat handling (iv) probability estimates suited to the wide range of context depths used. To measure how well a model fits the data at a particular position in the sequence we use the negative logarithm of the probability estimate at that position. The measure yields information profiles of the sequence, which are of independent interest. The average over the entire sequence, which amounts to the average number of bits per base needed to describe the sequence, is used as a global performance measure. Our main conclusion is that, from the probabilistic or information theoretic point of view and according to this performance measure, multiple competing Markov models explain entire genomes almost as well or even better than state-of-the-art DNA compression methods, such as XM, which rely on very different statistical models. This is surprising, because Markov models are local (short-range), contrasting with the statistical models underlying other methods, where the extensive data repetitions in DNA sequences is explored, and therefore have a non-local character. © 2011 Pinho et al.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An efficient omnidirectional vision system for soccer robots: From calibration to object detection",
        "doc_scopus_id": "79952628276",
        "doc_doi": "10.1016/j.mechatronics.2010.05.006",
        "doc_eid": "2-s2.0-79952628276",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Multi-robot systems",
            "Omnidirectional vision system",
            "Robotic soccer team",
            "Robotic vision",
            "Self localization",
            "Shape based",
            "Technical challenges",
            "Vision systems"
        ],
        "doc_abstract": "Robotic soccer is nowadays a popular research domain in the area of multi-robot systems. In the context of RoboCup, the Middle Size League is one of the most challenging. This paper presents an efficient omnidirectional vision system for real-time object detection, developed for the robotic soccer team of the University of Aveiro, CAMBADA. The vision system is used to find the ball and white lines, which are used for self-localization, as well as to find the presence of obstacles. Algorithms for detecting these objects and also for calibrating most of the parameters of the vision system are presented in this paper. We also propose an efficient approach for detecting arbitrary FIFA balls, which is an important topic of research in the Middle Size League. The experimental results that we present show the effectiveness of our algorithms, both in terms of accuracy and processing time, as well as the results that the team has been achieving: 1st place in RoboCup 2008, 3rd place in 2009 and 1st place in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271456 291210 291718 291787 291883 31 Mechatronics MECHATRONICS 2010-06-19 2010-06-19 2011-03-08T22:21:19 S0957-4158(10)00086-3 S0957415810000863 10.1016/j.mechatronics.2010.05.006 S300 S300.1 FULL-TEXT 2015-05-15T03:43:39.596909-04:00 0 0 20110301 20110331 2011 2010-06-19T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0957-4158 09574158 21 21 2 2 Volume 21, Issue 2 6 399 410 399 410 201103 March 2011 2011-03-01 2011-03-31 2011 Special Issue on Advances in intelligent robot design for the Robocup Middle Size League M.J.G. van de Molengraft O. Zweigle Special Issue on Advances in intelligent robot design for the Robocup Middle Size League article fla Copyright © 2010 Elsevier Ltd. All rights reserved. EFFICIENTOMNIDIRECTIONALVISIONSYSTEMFORSOCCERROBOTSCALIBRATIONOBJECTDETECTION NEVES A 1 Introduction 2 Architecture of the vision system 3 Calibration of the vision system 3.1 Self-calibration of the digital camera parameters 3.1.1 Proposed algorithm 3.1.2 Experimental results 3.2 Distance map calibration 4 Color-based object detection 4.1 Color extraction 4.2 Object detection 4.3 Experimental results 5 Arbitrary ball detection 5.1 Related work 5.2 Proposed approach 5.3 Experimental results 6 Conclusions Acknowledgment References LIMA 2001 87 102 P ASTROM 1995 K PIDCONTROLLERSTHEORYDESIGNTUNING BAKER 1999 175 196 S BRESENHAM 1965 25 30 J TREPTOW 2004 41 48 A GRIMSON 1990 1255 1274 W NEVESX2011X399 NEVESX2011X399X410 NEVESX2011X399XA NEVESX2011X399X410XA item S0957-4158(10)00086-3 S0957415810000863 10.1016/j.mechatronics.2010.05.006 271456 2011-03-10T12:04:28.24434-05:00 2011-03-01 2011-03-31 true 1828663 MAIN 12 84396 849 656 IMAGE-WEB-PDF 1 si4 1540 53 289 si3 1436 46 293 si2 1250 46 238 si1 1092 46 192 gr10 45233 182 377 gr10 14653 106 219 gr11 18028 138 376 gr11 2966 81 219 gr12 43502 281 757 gr12 6024 81 219 gr13 31126 208 377 gr13 11667 121 219 gr14 7740 60 489 gr14 1994 27 219 gr15 19330 223 373 gr15 3452 131 219 gr16 43365 197 765 gr16 5658 56 219 gr17 11813 182 264 gr17 4751 151 219 gr18 9049 172 358 gr18 2922 105 219 gr19 8089 119 217 gr19 4702 121 219 gr2 27937 199 271 gr2 19723 161 219 gr20 23395 140 484 gr20 5238 63 219 gr21 24408 220 361 gr21 4625 133 219 gr3 57308 455 703 gr3 8598 142 219 gr4 25516 139 381 gr4 10182 80 219 gr5 22239 199 575 gr5 3068 76 219 gr6 26663 137 374 gr6 10562 80 219 gr7 61745 405 533 gr7 15882 164 215 gr8 53286 374 533 gr8 11776 154 219 gr9 33410 179 378 gr9 13228 104 219 gr1 39196 225 378 gr1 14502 130 219 MECH 1160 S0957-4158(10)00086-3 10.1016/j.mechatronics.2010.05.006 Elsevier Ltd Fig. 1 The CAMBADA team playing at RoboCup 2009, Graz, Austria. Fig. 2 On the left, a detailed view of the CAMBADA vision system. On the right, one of the robots. Fig. 3 Some experiments using the automated calibration procedure. At the top, results obtained starting with all the parameters of the camera set to zero. At the bottom, results obtained with all the parameters set to the maximum value. On the left, the initial image acquired. In the middle, the image obtained after applying the automated calibration procedure. On the right, the graphics showing the evolution of the parameters along the time. Fig. 4 On the left, an example of an image acquired with the camera parameters in auto-mode. On the right, an image acquired after applying the automated calibration algorithm. Fig. 5 The histogram of the intensities of the two images presented in Fig. 4. In (a) it is shown the histogram of the image obtained with the camera parameters set to the maximum value. (b) Shows the histogram of the image obtained after applying the automated calibration procedure. Fig. 6 On the left, an image acquired outdoors using the camera in auto-mode. As it is possible to observe, the colors are washed out. This happens because the camera’s auto-exposure algorithm tries to compensate the black region around the mirror. On the right, the same image with the camera calibrated using our algorithm. As can be seen, the colors and the contours of the objects are much more defined. Fig. 7 A screenshot of the tool developed to calibrate some important parameters of the vision system, namely the inverse distance map, the mirror and robot center and the regions of the image to be processed. Fig. 8 A screenshot of the interface to calibrate some important parameters need to obtain the inverse distance map (these parameters are described in [17]). Fig. 9 Acquired image after reverse-mapping into the distance map. On the left, the map was obtained with all misalignment parameters set to zero. On the right, after automatic correction. Fig. 10 A 0.5m grid, superimposed on the original image. On the left, with all correction parameters set to zero. On the right, the same grid after geometrical parameter extraction. Fig. 11 On the left, the position of the radial search lines used in the omnidirectional vision system, after detecting the center of the robot in the image using the tool described in this section. On the right, an example of a robot mask used to select the pixels to be processed, obtained with the same tool. White points represent the area that will be processed. Fig. 12 The software architecture of the omnidirectional vision system. Fig. 13 A screenshot of the application used to calibrate the color ranges for each color class using the HSV color space. Fig. 14 An example of a transition. “G” means green pixel, “W” means white pixel and “X” means pixel with a color different from green or white, for example resulting due to some noise or a not perfect color calibration. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) Fig. 15 Relation between pixels and metric distances. The center of the robot is considered the origin and the metric distances are considered on the ground plane. Fig. 16 On the left, an example of an original image acquired by the omnidirectional vision system. In the center, the corresponding image of labels. On the right, the color blobs detected in the images. Marks over the ball point to the mass center. The several marks near the white lines (magenta) are the position of the white lines. The cyan marks are the position of the obstacles. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 17 Experimental results obtained by the omnidirectional system using the color ball detection. In this experiment, the ball was positioned in the center of the field, position (0,0). The robot performed a predefined trajectory while the position of the ball and the robot was recorded. Both axes in the graphics are in meters. Fig. 18 The circular Hough transform. a and b represent the parameter space that in this application are the radius of the ball and the distance to robot, respectively. Fig. 19 Example of a circle detection through the use of the circular Hough transform. Fig. 20 Example of a captured image using the proposed approach. The cross over the ball points out the detected position. In (b) the image (a), with the Canny edge detector applied. In (c), the image (b) after applying the circular Hough transform. Fig. 21 Experimental results obtained by the omnidirectional system using the morphological ball detection. In this experience, the ball was positioned in the penalty mark of the field. The robot performed a predefined trajectory while the position of the ball was recorded. Both axes in the graphics are in meters. Table 1 Statistical measures obtained for the images presented in Figs. 3 and 4. The initial values refer to the images obtained with the camera before applying the proposed automated calibration procedure. The final values refer to the images acquired with the cameras configured with the proposed algorithm. Experiment – ACM μ E MSV Parameters set to zero Initial 111.00 16.00 0.00 1.00 Final 39.18 101.95 6.88 2.56 Parameters set to maximum Initial 92.29 219.03 2.35 4.74 Final 42.19 98.59 6.85 2.47 Camera in auto-mode Initial 68.22 173.73 6.87 3.88 Final 40.00 101.14 6.85 2.54 An efficient omnidirectional vision system for soccer robots: From calibration to object detection António J.R. Neves ⁎ Armando J. Pinho Daniel A. Martins Bernardo Cunha ATRI, IEETA/DETI, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Robotic soccer is nowadays a popular research domain in the area of multi-robot systems. In the context of RoboCup, the Middle Size League is one of the most challenging. This paper presents an efficient omnidirectional vision system for real-time object detection, developed for the robotic soccer team of the University of Aveiro, CAMBADA. The vision system is used to find the ball and white lines, which are used for self-localization, as well as to find the presence of obstacles. Algorithms for detecting these objects and also for calibrating most of the parameters of the vision system are presented in this paper. We also propose an efficient approach for detecting arbitrary FIFA balls, which is an important topic of research in the Middle Size League. The experimental results that we present show the effectiveness of our algorithms, both in terms of accuracy and processing time, as well as the results that the team has been achieving: 1st place in RoboCup 2008, 3rd place in 2009 and 1st place in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. Keywords Robotic vision Omnidirectional vision systems Color-based object detection Shape-based object detection Vision system calibration 1 Introduction The Middle Size League (MSL) of RoboCup is a forum where several research areas have been challenged for proposing solutions to well-defined practical problems. The robotic vision is one of those areas and, for most of the MSL teams, it has become the only way of sensing the surrounding world. From the point of view of a robot, the playing field during a game provides a fast-changing scenery, where the teammates, the opponents and the ball move quickly and often in an unpredictable way. The robots have to capture these scenes through their cameras and have to discover where the objects of interest are located. There is no time for running complex algorithms. Everything has to be computed and decided in a small fraction of a second, for allowing real-time operation; otherwise, it becomes useless. Real-time is not the only challenge that needs to be addressed. Year after year, the initially well controlled and robot friendly environment where the competition takes place has become increasingly more hostile. Conditions that previously have been taken for granted, such as controlled lighting or easy to recognize color coded objects, have been relaxed or even completely suppressed. Therefore, the vision system of the robots needs to be prepared for adapting to strong lighting changes during a game, as well as, for example, for ball-type changes across games. In this paper, we provide a comprehensive description of the vision system of the MSL CAMBADA team (Fig. 1 ). Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture (CAMBADA) is the RoboCup MSL soccer team of the Institute of Electronics and Telematics Engineering of Aveiro (IEETA) research institute, University of Aveiro, Portugal. The team, which started officially in October 2003, won the 2008 MSL RoboCup World Championship and ranked 3rd in the 2009 edition. We start by presenting and explaining the hardware architecture of the vision system used by the robots of the CAMBADA team, which relies on an omnidirectional vision system (Section 2). Then, we proceed with the description of the approach that we have adopted regarding the calibration of a number of crucial parameters and in the construction of auxiliary data structures (Section 3). Concerning the calibration of the intrinsic parameters of the digital camera, we propose an automated calibration algorithm that is used to configure the most important features of the camera, namely, the saturation, exposure, white-balance, gain and brightness. The proposed algorithm uses the histogram of intensities of the acquired images and a black and a white area, known in advance, to estimate the referred parameters. We also describe a general solution to calculate the robot centered distances map, exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface. The soccer robots need to locate several objects of interest, such as the ball, the opponent robots and the teammates. Moreover, they also need to collect information for self-localization, namely, the position of the field white lines. For these tasks, we have developed fast and efficient algorithms that rely on color information. The color extraction algorithms are based on lookup tables and use a radial model for color object detection. Due to the severe restrictions imposed by the real-time constraint, some of the image processing tasks are implemented using a multi-threading approach and use special data structures to reduce the processing time. Section 4 provides a detailed description of these algorithms. As previously mentioned, the color codes assigned to the objects of interest tend to disappear as the competition evolves. For example, the usual orange ball used in the MSL will soon be replaced by an arbitrary FIFA ball, increasing the difficulty in locating one of the most important objects in the game. Anticipating this scenario, we developed a fast method for detecting soccer balls independently of their colors. In Section 5, we describe a solution based on the morphological analysis of the image. The algorithm relies on edge detection and on the circular Hough transform, attaining a processing time almost constant and complying with the real-time constraint. Its appropriateness has been clearly demonstrated by the results obtained in the mandatory technical challenge of the RoboCup MSL: 2nd place in 2008 and 1st place in 2009. 2 Architecture of the vision system The CAMBADA robots [1] use a catadioptric vision system, often named omnidirectional vision system, based on a digital video camera pointing at a hyperbolic mirror, as presented in Fig. 2 . We are using a digital camera Point Grey Flea 2, 1 Last accessed: 18/02/2010. 1 FL2-08S2C with a 1/3” CCD Sony ICX204 that can deliver images up to 1024×768 pixels in several image formats, namely RGB, YUV 4:1:1, YUV 4:2:2 or YUV 4:4:4. The hyperbolic mirror was developed by IAIS Fraunhofer Gesellschaft 2 Last accessed: 18/02/2010. 2 (FhG-AiS). Although the mirror was designed for the vision system of the FhG Volksbot 3 Last accessed: 18/02/2010. 3 we are achieving also an excellent result with it in our vision system. The use of omnidirectional vision systems have captured much interest in the last years, because it allows a robot to attain a 360° field of view around its central vertical rotation axis, without having to move itself or its camera. In fact, it has been a common solution for the main sensorial element in a significant number of autonomous mobile robot applications, as is the case of the MSL, where most of the teams have adopted this approach [2–9]. A catadioptric vision system ensures an integrated perception of all major target objects in the surrounding area of the robot, allowing a higher degree of maneuverability. However, this also implies higher degradation in the resolution with growing distances away from the robot, when compared to non-isotropic setups. 3 Calibration of the vision system An important task in the MSL is the calibration of the vision system. This includes the calibration of intrinsic parameters of the digital camera, the computation of the inverse distance map, the detection of the mirror and robot center and the definition of the regions of the image that have to be processed. Calibration has to be performed when environmental conditions change, such as playing in a different soccer field or when the lighting conditions vary over time. Therefore, there are adjustments that have to be made almost continuously, for example if the playing field is unevenly illuminated, or less frequently, when the playing field changes. Moreover, a number of adjustments have also to be performed when some of the vision hardware of the robot is replaced, such as the camera or the mirror. All these calibrations and adjustments should be robust, i.e., they should be as much as possible insensitive to small environmental variations, they should be fast to perform and they should be simple to execute, so that no special calibration expert is required to operate them. 3.1 Self-calibration of the digital camera parameters In a near future, it is expected that the MSL robots will have to play under natural lighting conditions and in outdoor fields. This introduces new challenges. In outdoor fields, the illumination may change slowly during the day, due to the movement of the sun, but also may change quickly in short periods of time due to a partial and temporally varying covering of the sun by clouds. In this case, the robots have to adjust, in real-time, both the color segmentation values as well as some of the camera parameters, in order to adapt to new lighting conditions [10]. The common approach regarding the calibration of the robot cameras in the MSL has been based on manual adjustments, that are performed prior to the games, or through some automatic process that runs offline using a pre-acquired video sequence. However, most (or even all) of the parameters remain fixed during the game. We propose an algorithm that does not require human interaction to configure the most important parameters of the camera, namely the exposure, the white-balance, the gain and the brightness. Moreover, this algorithm runs continuously, even during the game, allowing coping with environmental changes that often occur when playing. We use the histogram of intensities of the acquired images and a black and a white area, which location is known in advance, to estimate the referred parameters of the camera. Note that this approach differs from the well known problem of photometric camera calibration (a survey can be found in [11]), since we are not interested in obtaining the camera response values, but only to configure its parameters according to some measures obtained from the acquired images. The self-calibration process for a single robot requires a few seconds, including the time necessary to start the application. This is significantly faster than the usual manual calibration by an expert user, for which several minutes are needed. 3.1.1 Proposed algorithm The proposed calibration algorithm processes the image acquired by the camera and analyzes a white area in the image (a white area in a fixed place on the robot body, near the camera in the center of the image), in order to calibrate the white-balance. A black area (we use a part of the image that represents the robot itself, actually a rectangle in the upper left side of the image) is used to calibrate the brightness of the image. Finally, the histogram of the image intensities is used to calibrate the exposure and gain. The histogram of the intensities of an image is a representation of the number of times that each intensity value appears in the image. For an image represented using 8 bits per pixel, the possible values are between 0 and 255. Image histograms can indicate some aspects of the lighting conditions, particularly the exposure of the image and whether if it is underexposed or overexposed. The assumptions used by the proposed algorithm are the following: (i) The white area should appear white in the acquired image. In the YUV color space, this means that the average value of U and V should be close to 127, that is to say, the chrominance components of the white section should be as close to zero as possible. If the white-balance is not correctly configured, these values are different from 127 and the image does not have the correct colors. The white-balance parameter is composed by two values, WB_BLUE and WB_RED, directly related to the values of U and V, respectively. (ii) The black area should be black. In the RGB color space, this means that the average values of R, G and B should be close to zero. If the brightness parameter is too high, it is observed that the black region becomes blue, resulting in a degradation of the image. (iii) The histogram of intensities should be centered around 127 and should span all intensity values. Dividing the histogram into regions, the left regions represent dark colors, while the right regions represent light colors. An underexposed image will be leaning to the left, while an overexposed image will be leaning to the right in the histogram (for an example, see Fig. 5a). The values of the gain and exposure parameters are adjusted according to the characteristic of the histogram. Statistical measures can be extracted from the images to quantify the image quality [12,13]. A number of typical measures used in the literature can be computed from the image gray level histogram, namely, the mean (1) μ = ∑ i = 0 N - 1 iP i , μ ∈ [ 0 , 255 ] , the entropy (2) E = - ∑ i = 0 N - 1 P i log ( P i ) , E ∈ [ 0 , 8 ] , the absolute central moment (3) ACM = ∑ i = 0 N - 1 | i - μ | P i , ACM ∈ [ 0 - 127 ] and the mean sample value (4) MSV = ∑ j = 0 4 ( j + 1 ) x j ∑ j = 0 4 x j , MSV ∈ [ 0 - 5 ] , where N is the number of possible gray values in the histogram (typically, 256), P i is the relative frequency of each gray value and x j is the sum of the gray values in region j of the histogram (in the proposed approach we divided the histogram into five regions). When the histogram values of an image are uniformly distributed in the possible values, then μ ≈127, E ≈8, ACM ≈60 and MSV ≈2.5. In the experimental results we use these measures to analyze the performance of the proposed calibration algorithm. Moreover, we use the information of MSV to calibrate the exposure and the gain of the camera. The algorithm is depicted next. do do acquire image calculate the histogram of intensities calculate the MSV value if MSV<2.0 OR MSV>3.0 apply the PI controller to adjust exposure else apply the PI controller to adjust gain set the camera with new exposure and gain values while exposure or gain parameters change do acquire image calculate average U and V values of the white area apply the PI controller to adjust WB_BLUE apply the PI controller to adjust WB_RED set the camera with new white-balance parameters while white-balance parameters change do acquire image calculate average R, G and B values of the black area apply the PI controller to adjust brightness set the camera with new brightness value while brightness parameter change while any parameter changed The calibration algorithm configures one parameter at a time, proceeding to the next one when the current one has converged. For each of these parameters, a PI controller was implemented. PI controllers are used instead of proportional controllers as they result in better control, having no stationary error. The coefficients of the controller were obtained experimentally: first, the proportional gain was increased until the camera parameter started to oscillate. Then, it was reduced to about 70% of that value and the integral gain was increased until an acceptable time to reach the desired reference was obtained [14]. The algorithm stops when all the parameters have converged. More details regarding this algorithm can be found in [15]. 3.1.2 Experimental results To measure the performance of this calibration algorithm, tests have been conducted using the camera with different initial configurations. In Fig. 3 , results are presented both when the algorithm starts with the parameters of the camera set to zero, as well as when set to the maximum value. As can be seen, the configuration obtained after running the proposed algorithm is approximately the same, independently of the initial configuration of the camera. Moreover, the algorithm is fast to converge (it takes between 60 and 70 frames). In Fig. 4 , it is presented an image acquired with the camera in auto-mode. As can be seen, the image obtained using the camera with the parameters in auto-mode is overexposed and the white balance is not configured correctly. This is due to the fact that the camera analyzes the entire image and, as can be observed in Fig. 3, there are large black regions corresponding to the robot itself. Our approach uses a mask to select the region of interest, in order to calibrate the camera using exclusively the valid pixels. Moreover, and due to the changes in the environment when the robot is moving, leaving the camera in auto-mode leads to undesirable changes in the parameters of the camera, causing color classification problems. Table 1 presents the values of the statistical measures described in (1)–(4), regarding the experimental results presented in Fig. 3. These results confirm that the camera is correctly configured after applying the automated calibration procedure, since the results obtained are close to the optimal. Moreover, the algorithm converges always to the same set of parameters, independently of the initial configuration. According to the experimental results presented in Table 1, we conclude that the MSV measure is the best one for classifying the quality of an image. This is due to the fact that it is closer to the optimal values when the camera is correctly calibrated. Moreover, this measure can distinguish between two images that have close characteristics, as is the case when the camera is used in auto-mode. The good results of the automated calibration procedure can also be confirmed in the histograms presented in Fig. 5 . The histogram of the image obtained after applying the proposed automated calibration procedure (Fig. 5b) is centered near the intensity 127, which is a desirable property, as shown in Fig. 3 in the middle images. The histogram of the image acquired using the camera with all the parameters set to the maximum value (Fig. 5a) shows that the image is overexposed, leading that the majority of the pixels have bright colors. This algorithm has also been tested outdoors, under natural light. Fig. 6 shows that it works well even when the robot is under very different lighting conditions, showing its robustness. 3.2 Distance map calibration For most practical applications, the setup of the vision system requires the translation of the planar field of view at the camera sensor plane, into real world coordinates at the ground plane, using the robot as the center of this system. In order to simplify this non-linear transformation, most practical solutions adopted in real robots choose to create a mechanical geometric setup that ensures a symmetrical solution for the problem by means of a single viewpoint (SVP) approach. This, on the other hand, calls for a precise alignment of the four major points comprising the vision setup: the mirror focus, the mirror apex, the lens focus and the center of the image sensor. Furthermore, it also demands the sensor plane to be both parallel to the ground field and normal to the mirror axis of revolution, and the mirror foci to be coincident with the effective viewpoint and the camera pinhole respectively [16]. Although tempting, this approach requires a precision mechanical setup. In this section, we briefly present a general solution to calculate the robot centered distances map on non-SVP catadioptric setups, exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface. A detailed description of the algorithms can be found in [17] and a screenshot of the application is presented in Figs. 7 and 8 . This solution effectively compensates for the misalignment that may result either from a simple mechanical setup or from the use of low cost video cameras. The method can also extract most of the required parameters from the acquired image itself, allowing it to be used for self-calibration purposes. In order to allow further trimming of these parameters, two simple image feedback tools have been developed. The first one creates a reverse mapping of the acquired image into the real world distance map. A fill-in algorithm is used to integrate image data in areas outside pixel mapping on the ground plane. This produces a plane vision from above, allowing visual check of line parallelism and circular asymmetries (Fig. 9 ). The second generates a visual grid with 0.5m distances between both lines and columns, which is superimposed on the original image. This provides an immediate visual clue for the need of possible further distance correction (Fig. 10 ). With this tool, it is also possible to determine some other important parameters, namely the mirror center and the area of the image that will be processed by the object detection algorithms (Fig. 11 ). 4 Color-based object detection The algorithms that we propose for object detection can be split into three main modules, namely the Utility Sub-System, the Color Processing Sub-System and the Morphological Processing Sub-System, as shown in Fig. 12 . In the Color Processing Sub-System, proper color classification and extraction processes were developed, along with an object detection process to extract information from the acquired image, through color analysis. The Morphological Processing Sub-System presented in Section 5, is used to detect arbitrary FIFA balls independently of their colors. In order to satisfy the real-time constrains in the proposed image processing system, we implemented efficient data structures to process the image data [18,19]. Moreover, we use a two-thread approach to perform the most time consuming operations in parallel, namely the color classification and the color extraction, taking advantage of the dual core processor used by the laptop computers of our robots. 4.1 Color extraction Image analysis in the MSL is simplified, since objects are color coded. Black robots play with an orange ball on a green field that has white lines. Thus, the color of a pixel is a strong hint for object segmentation. We exploit this fact by defining color classes, using a look-up table (LUT) for fast color classification. The table consists of 16,777,216 entries (224, 8 bits for red, 8 bits for green and 8 bits for blue), each 8 bits wide, occupying a total of 16 MByte. Note that for other color spaces the table size would be the same, changing only the meaning of each component. Each bit expresses whether the color is within the corresponding class or not. This means that a certain color can be assigned to several classes at the same time. To classify a pixel, we first read the pixel’s color and then use the color as an index into the table. The 8-bit value read from the table is called the “color mask” of that pixel. The color calibration is performed in the HSV (Hue, Saturation and Value) color space, since it provides a single, independent, color spectrum variable. In the current setup, the image is acquired in RGB or YUV format and then is converted to an image of labels using the appropriate LUT. Fig. 13 presents a screenshot of the application used to calibrate the color ranges for each color class, using the HSV color space and a histogram based analysis. Certain regions of the image are excluded from analysis. One of them is the part in the image that reflects the robot itself. Other regions are the sticks that hold the mirror and the areas outside the mirror. These regions are found using the algorithm described in Section 3.2. An example is presented on the right of Fig. 11, where the white pixels indicate the area that will be processed. With this approach, we can reduce the time spent in the conversion and searching phases and we also eliminate the problem of finding erroneous objects in those areas. To extract color information from the image we use radial search lines, instead of processing the whole image. A radial search line is a line that starts at the center of the robot, with some angle, and ends at the limits of the image. In an omnidirectional system, the center of the robot is approximately the center of the image (see left of Fig. 11). The search lines are constructed based on the Bresenham line algorithm [20]. They are constructed once, when the application starts, and saved in a structure in order to improve the access to these pixels in the color extraction module. For each search line, we iterate through its pixels to search for transitions between two colors and areas with specific colors. The use of radial search lines accelerates the process of object detection, due to the fact that we only process part of the valid pixels. This approach has a processing time almost constant, independently of the information that is captured by the camera. Moreover, the polar coordinates, inherent to the radial search lines, facilitate the definition of the bounding boxes of the objects in omnidirectional vision systems. We developed an algorithm for detecting areas of a specific color which eliminates the possible noise that could appear in the image. For each radial scanline, it is performed a median filtering operation. Each time a pixel is found with a color of interest, the algorithm analyzes the pixels that follow (a predefined number). If it does not find more pixels of that color, it discards the pixel found and continues. When a predefined number of pixels with that color is found, it considers that the search line has that color. Regarding the ball detection, we created an algorithm to recover lost orange pixels due to the ball shadow cast over itself. As soon as we find a valid orange pixel in the radial sensor, the shadow recovery algorithm tries to search for darker orange pixels previously discarded in the color segmentation analysis. The search is conducted in each radial sensor, starting at the first orange pixel found when searching towards the center of the robot, limited to a maximum number of pixels. For each pixel analyzed, a comparison is performed using a wider region of the color space, in order to being able to accept darker orange pixels. Once a different color is found or the maximum number of pixels is reached, the search along the current sensor is completed and the next sensor is processed. In Fig. 16, we can see the pixels recovered by this algorithm (the orange blobs contain pixels that were not originally classified as orange). To accelerate the process of calculating the position of the objects, we put the color information that was found in each of the search lines into a list of colors. We are interested in the first pixel (in the corresponding search line) where the color was found and with the number of pixels with that color that have been found in the search line. Then, using the previous information, we separate the information of each color into blobs (Fig. 16 shows an example). After this, it is calculated the blob descriptor that will be used for the object detection module, which contains the following information: – Distance to the robot. – Closest pixel to the robot. – Position of the mass center. – Angular width. – Number of pixels. – Number of green and white pixels in the neighborhood of the blob. 4.2 Object detection The objects of interest that are present in a MSL game are: a ball, obstacles and the green field with white lines. Currently, our system detects efficiently all these objects with a set of simple algorithms that, using the color information collected by the radial search lines, calculate the object position and/or its limits in a polar representation (distance and angle). The algorithm that searches for the transitions between green pixels and white pixels is described next. If a non-green pixel is found in a radial scanline, we search for the next green pixel, counting the number of non-green pixels and the number of white pixels that meanwhile appeared. If these values are greater than a predefined threshold, the center of this region is considered a transition point corresponding to a position of a soccer field line. The algorithm is illustrated in Fig. 14 with an example. A similar approach has been described in [21]. The ball is detected using the following algorithm: (i) Separate the orange information into blobs. (ii) For each blob, calculate the information described previously. (iii) Perform a first validation of the orange blobs using the information about the green and white pixels in the neighborhood of the blob, to guarantee that only balls inside the field are detected. (iv) Validate the remaining orange blobs according to the number of pixels. As illustrated in Fig. 15 , it is known the relation between the pixel size at the ground plane and the distance to the center of the robot. Using this knowledge, we estimate the number of pixels that a ball should have according to the distance. (v) Following the same approach, the angular width is also used to validate the blobs. (vi) The ball candidate is the valid blob closest to the robot. The position of the ball is the mass center of the blob. To calculate the position of the obstacles around the robot, we use the following algorithm: (i) Separate the black information into blobs. (ii) Calculate the information for each blob. (iii) Perform a simple validation of the black blobs using the information about the green and white pixels in the neighborhood of the blob, to guarantee that only obstacles inside the field are detected. (iv) The position of the obstacle is given by the distance of the blob relatively to the robot. The limits of the obstacle are obtained using the angular width of the blob. More details regarding the detection and identification of obstacles can be found in [22]. Fig. 16 presents an example of an acquired image, the corresponding segmented image and the detected color blobs. As can be seen, the objects are correctly detected. The position of the white lines, the position of the ball and the information about the obstacles are then sent to the Real-time Database [1,23] and used, afterward, by the high level process responsible for the behaviors of the robots [24,25,22,26]. 4.3 Experimental results To experimentally measure the efficiency of the proposed algorithms, the robot was moved along a predefined path through the robotic soccer field, leaving the ball in a known location. The ball position given by the robot is then compared with the real position of the ball. Note that the results in this test may be affected by errors in the localization algorithm and by some bumps while the robot is moving. The separate study of these sources of error has being left outside this experimental evaluation. However, they should be performed, for better understanding the several factors that influence the correct localization of the ball. The robot path across the field may be seen in Fig. 17 , along with the measured ball position. According to that data, it is possible to notice that the average of the measured positions of the ball is almost centered in the real ball position, showing the effectiveness of the proposed algorithms. Our measures show a very high detection ratio (near 95%), and a good accuracy, with the average measures very close to the real ball position. In our experiments, we verified that the robots are able to detect the ball up to 6m with regular light conditions and a good color calibration, easy to obtain after applying the proposed automated calibration algorithm described in Section 3. The proposed algorithm has an almost constant processing time, independently of the environment around the robot, typically around 6ms. It needs approximately 35MBytes of memory. The experimental results were obtained using a camera resolution of 640×480 pixels and a laptop with an Intel Core 2 duo at 2.0GHz and 1GB of memory. 5 Arbitrary ball detection The color codes tend to disappear as the competition evolves, increasing the difficulty posed to the vision algorithms. The color of the ball, currently orange, is the next color scheduled to become arbitrary. In this section, we propose a solution for overcoming this new challenge, i.e., a method for detecting balls independently of their colors. This solution is based on a morphological analysis of the image, being strictly directed to detect round objects in the field with specific characteristics, in this case the ball. Morphological object recognition through image analysis has became more robust and accurate in the past years, whereas still very time consuming even to modern personal computers. Because RoboCup is a real-time environment, available processing time can become a serious constraint when analyzing large amounts of data or executing complex algorithms. This section presents an arbitrary FIFA ball recognition algorithm, based on the use of image segmentation and the circular Hough transform. The processing time is almost constant and allows real-time processing. As far as we know, this approach has never been proposed. The experimental results obtained, as well as the classifications obtained by the CAMBADA team, seem to be very promising. Regarding the vision system described in Fig. 12, it is possible to specify whether to use the Morphological sub-system to detect the ball or the current color-based approach. Currently, in the MSL, the shape-based detection is only necessary in the mandatory challenge of the competition, although it will be incorporated in the rules in the next years. 5.1 Related work Many of the algorithms proposed during previous research work showed their effectiveness but, unfortunately, their processing time is in some cases over one second per video frame [27]. In [28], the circular Hough transform was presented in the context of colored ball detection as a validation step. However, no details about the implementation and experimental results have been presented. Hanek et al. [29] proposed a Contracting Curve Density algorithm to recognize the ball without color labeling. This algorithm fits parametric curve models to the image data by using local criteria based on local image statistics to separate adjacent regions. This method can extract the contour of the ball even in cluttered environments under different illumination, but the vague position of the ball should be known in advance. The global detection cannot be realized by this method. Treptow et al. [30] proposed a method to detect and track a ball without color information in real-time, by integrating the Adaboost Feature Learning algorithm into a condensation tracking framework. Mitri et al. [31] presented a scheme for color invariant ball detection, in which the edged filtered images serve as the input of an Adaboost learning procedure that constructs a cascade of classification and regression trees. This method can detect different soccer balls in different environments, but the false positive rate is high when there are other round objects in the environment. Coath et al. [32] proposed an edge-based arc fitting algorithm to detect the ball for soccer robots. However, the algorithm is used in a perspective camera vision system in which the field of view is far smaller and the image is also far less complex than that of the omnidirectional vision system used by most of the robotic soccer teams. More recently, Lu et al. [33] considered that the ball on the field can be approximated by an ellipse. They scan the color variation to search for the possible major and minor axes of the ellipse, using radial and rotary scanning, respectively. A ball is considered if the middle points of a possible major axis and a possible minor axis are very close to each other in the image. However, this method has a processing time that can achieve 150ms if the tracking algorithm fails, which might cause problems in real-time applications. 5.2 Proposed approach The proposed approach is presented in the top layer of Fig. 12. The search for potential ball candidates is conducted taking advantage of morphological characteristics of the ball (round shape), using a feature extraction technique known as the Hough transform. This is a technique for identifying the locations and orientations of certain types of features in a digital image [34]. The Hough transform algorithm uses an accumulator and can be described as a transformation of a point in the x, y-plane to the parameter space. The parameter space is defined according to the shape of the object of interest, in this case, the ball presents a rounded shape. First used to identify lines in images, the Hough transform has been generalized through the years to identify positions of arbitrary shapes by a voting procedure [35–37]. Fig. 18 shows an example of a circular Hough transform, for a constant radius, from the x, y-space to the parameter space. In Fig. 19 , we show an example of circle detection through the circular Hough transform. We can see the original image of a dark circle (known radius r) on a bright background (see Fig. 19a). For each dark pixel, a potential circle-center locus is defined by a circle with radius r and center at that pixel (see Fig. 19b). The frequency with which image pixels occur in the circle-center loci is determined (see Fig. 19c). Finally, the highest-frequency pixel represents the center of the circle with radius r. To feed the Hough transform process, it is necessary a binary image with the edge information of the objects. This image, Edges Image, is obtained using an edge detector operator. In the following, we present an explanation of this process and its implementation. To be possible to use this image processing system in real-time, and increase time efficiency, a set of data structures to process the image data has been implemented [18,19]. The proposed algorithm is based on three main operations: (i) Edge detection: this is the first image processing step in the morphological detection. It must be as efficient and accurate as possible in order not to compromise the efficiency of the whole system. Besides being fast to calculate, the intended resulting image must be absent of noise as much as possible, with well defined contours, and be tolerant to the motion blur introduced by the movement of the ball and the robots. Some popular edge detectors were tested, namely Sobel [38,39], Laplace [40,41] and Canny [42]. The tests were conducted under two distinct situations: with the ball standing still and with the ball moving fast through the field. The test with the ball moving fast was performed in order to study the motion blur effect in the edge detectors, on high speed objects captured with a frame rate of 30 frames per second. For choosing the best edge detector for this purpose, the results from the tests were compared taking into account the image of edges and processing time needed by each edge detector. On one hand, the real-time capability must be assured. On the other hand, the algorithm must be able to detect the edges of the ball independently of its motion blur effect. According to our experiments, the Canny edge detector was the most demanding in terms of processing time. Even so, it was fast enough for real-time operation and, because it provided the most effective contours, it was chosen. The parameters of the edge detector were obtained experimentally. (ii) Circular Hough transform: this is the next step in the proposed approach to find points of interest containing eventual circular objects. After finding these points, a validation procedure is used for choosing points containing a ball, according to our characterization. The voting procedure of the Hough transform is carried out in a parameter space. Object candidates are obtained as local maxima of a denoted Intensity Image (Fig. 20 c), that is constructed by the Hough Transform block (Fig. 12). Due to the special features of the Hough circular transform, a circular object in the Edges Image would produce an intense peak in Intensity Image corresponding to the center of the object (as can be seen in Fig. 20c). On the contrary, a non-circular object would produce areas of low intensity in the Intensity Image. However, as the ball moves away, its edge circle size decreases. To solve this problem, information about the distance between the robot center and the ball is used to adjust the Hough transform. We use the inverse mapping of our vision system [17] to estimate the radius of the ball as a function of distance. (iii) Validation: in some situations, particularly when the ball is not present in the field, false positives might be produced. To solve this problem and improve the ball information reliability, we propose a validation algorithm that discards false positives based on information from the Intensity Image and the Acquired Image. This validation algorithm is based on two tests against which each ball candidate is put through. In the first test performed by the validation algorithm, the points with local maximum values in the Intensity Image are considered if they are above a distance-dependent threshold. This threshold depends on the distance of the ball candidate to the robot center, decreasing as this distance increases. This first test removes some false ball candidates, leaving a reduced group of points of interest. Then, a test is made in the Acquired Image over each point of interest selected by the previous test. This test is used to eliminate false balls that usually appear in the intersection of the lines of the field and other robots (regions with several contours). To remove these false balls, we analyze a square region of the image centered in the point of interest. We discard this point of interest if the sum of all green pixels is over a certain percentage of the square area. Note that the area of this square depends on the distance of the point of interest to the robot center, decreasing as this distance increases. Choosing a square where the ball fits tightly makes this test very effective, considering that the ball fills over 90% of the square. In both tests, we use threshold values that were obtained experimentally. Besides the color validation, it is also performed a validation of the morphology of the candidate, more precisely a circularity validation. Here, from the candidate point to the center of the ball, it is performed a search of pixels at a distance r from the center. For each edge found between the expected radius, the number of edges at that distance are determined. By the size of the square which covers the possible ball and the number of edge pixels, it is calculated the edges percentage. If the edges percentage is greater than 70, then the circularity of the candidate is verified. The position of the detect ball is then sent to the Real-time Database, together with the information of the white lines and the information about the obstacles to be used, afterward, by the high level process responsible for the behaviors of the robots. 5.3 Experimental results Fig. 20 presents an example of the Morphological Processing Sub-System. As can be observed, the balls in the Edges Image (Fig. 20b) have almost circular contours. Fig. 20c) shows the resulting image after applying the circular Hough transform. Notice that the center of the balls present a very high peak when compared to the rest of the image. The ball considered was the closest to the robot, due to the fact that it has the high peak in the image. To ensure good results in the RoboCup competition, the system was tested with the algorithms described above. For that purpose, the robot was moved along a predefined path through the robotic soccer field, leaving the ball in a known location. The ball position given by the robot is then compared with the real position of the ball. The results in this test may be affected by the errors in the localization algorithm and by the robot bumps while moving. These external errors are out of the scope of this study. The robot path in the field may be seen in Fig. 21 , along with the measured ball position. It is possible to notice that the average of the measured positions of the ball is almost centered in the real ball position, showing the accuracy of the proposed algorithms. We obtained a very high detection ratio (near 90%) and a false positive rate around 0%, which is a very significant result. With the proposed approach, the omnidirectional vision system can detect the ball within this precision until distances up to 4 meters. The average processing time of the proposed approach was approximately 16ms. It needs approximately 40MBytes of memory. The experimental results have been obtained using a camera resolution of 640×480 pixels and a laptop with an Intel Core 2 duo at 2.0GHz. 6 Conclusions This paper presents the omnidirectional vision system developed for the CAMBADA MSL robotic soccer team, from the calibration to the object detection. We presented several algorithms for the calibration of the most important parameters of the vision system and we proposed efficient color-based algorithms for object detection. Moreover, we proposed a solution for the detection of arbitrary FIFA balls, one of the current challenges in the MSL. The CAMBADA team won the last three editions of the Portuguese Robotics Festival, ranked 5th in RoboCup 2007, won the RoboCup 2008 and ranked 3rd in RoboCup 2009, demonstrating the effectiveness of our vision algorithms in a competition environment. As far as we know, no previous work has been published describing all the steps of the design of an omnidirectional vision system. Moreover, some of the algorithms presented in this paper are state-of-the-art, as demonstrated by the first place obtained in the mandatory technical challenge in RoboCup 2009, where the robots have to play with an arbitrary standard FIFA ball. We are currently working in the automatic calibration of the inverse distance mapping and in efficient algorithms for autonomous color calibration, based on region growing. Regarding the object detection algorithms, as we have reduced the processing time to a few milliseconds, we are working on the acquisition of higher resolution images, capturing only a region of interest. The idea of work with higher image resolutions is to improve the object detection at higher distances. Moreover, we continue the development of algorithms for shape-based object detection, also to incorporate as a validation of the color-based algorithms. Acknowledgment This work was supported in part by the FCT (Fundação para a Ciência e a Tecnologia). References [1] Neves A, Azevedo J, Cunha NLB, Silva J, Santos F, Corrente G, et al. CAMBADA soccer team: from robot architecture to multiagent coordination. In: Vladan Papic editor. Robot soccer. Vienna, Austria: I-Tech Education and Publishing; 2010 [chapter 2]. [2] Zivkovic Z, Booij O. How did we built our hyperbolic mirror omni-directional camera-practical issues and basic geometry. Tech. rep. Intelligent Systems Laboratory, University of Amsterdam; 2006. [3] Wolf J. Omnidirectional vision system for mobile robot localization in the robocup environment. Master’s thesis. Graz University of Technology; 2003. [4] Menegatti E, Nori F, Pagello E, Pellizzari C, Spagnoli D. Designing an omnidirectional vision system for a goalkeeper robot. In: Proc of RoboCup 2001. Lecture notes in computer science, vol. 2377. Springer; 2001. p. 78–87. [5] Menegatti E, Pretto A, Pagello E. Testing omnidirectional vision-based monte carlo localization under occlusion. In: Proc of the IEEE intelligent robots and systems, IROS 2004; 2004. p. 2487–93. [6] P. Lima A. Bonarini C. Machado F. Marchese C. Marques F. Ribeiro Omni-directional catadioptric vision for soccer robots Robot Auton Syst 36 2–3 2001 87 102 [7] Liu F, Lu H, Zheng Z. A robust approach of field features extraction for robot soccer. In: Proc of the 4th IEEE Latin America robotic symposium, Monterry, Mexico; 2007. [8] Lu H, Zheng Z, Liu F, Wang X. A robust object recognition method for soccer robots. In: Proc of the 7th world congress on intelligent control and automation, Chongqing, China; 2008. [9] Voigtlrande A, Lange S, Lauer M, Riedmiller M. Real-time 3D ball recognition using perspective and catadioptric cameras. In: Proc of the 3rd European conference on mobile robots, Freiburg, Germany; 2007. [10] Mayer G, Utz H, Kraetzschmar G. Playing robot soccer under natural light: a case study. In: Proc of the RoboCup 2003. Lecture notes on artificial intelligence, vol. 3020. Springer; 2003. [11] Krawczyk G, Goesele M, Seidel H. Photometric calibration of high dynamic range cameras. Research Report MPI-I-2005-4-005. Max-Planck-Institut für Informatik, Stuhlsatzenhausweg 85, 66123 Saarbrücken, Germany; April 2005. [12] Shirvaikar MV. An optimal measure for camera focus and exposure. In: Proc of the IEEE southeastern symposium on system theory, Atlanta (USA); 2004. [13] Nourani-Vatani N, Roberts J. Automatic camera exposure control. In: Proc of the 2007 Australasian conference on robotics and automation, Brisbane, Australia; 2007. [14] K. Åström T. Hågglund PID controllers: theory, design, and tuning 2nd ed. 1995 Instruments Society of America [15] Neves AJR, Cunha AJPB, Pinheiro I. Autonomous configuration of parameters in robotic digital cameras. In: Proc of the 4th Iberian conference on pattern recognition and image analysis, IbPRIA-2009. Lecture notes in computer science, vol. 5524. Póvoa do Varzim, Portugal: Springer; 2009. p. 80–7. [16] S. Baker S.K. Nayar A theory of single-viewpoint catadioptric image formation Int J Comput Vis 2 1999 175 196 [17] Cunha B, Azevedo JL, Lau N, Almeida L. Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system. In: Proc of the RoboCup 2007. Lecture notes in computer science, vol. 5001. Atlanta (USA): Springer; 2007. p. 417–24. [18] Neves AJR, Martins DA, Pinho AJ. A hybrid vision system for soccer robots using radial search lines. In: Proc of the 8th conference on autonomous robot systems and competitions. Portuguese robotics open – ROBOTICA’2008, Aveiro, Portugal; 2008. p. 51–5. [19] Neves AJR. Corrente G, Pinho AJ. An omnidirectional vision system for soccer robots. In: Proc of the 2nd international workshop on intelligent robotics, IROBOT 2007. Lecture notes in artificial intelligence, vol. 4874. Springer; 2007. p. 499–507. [20] J.E. Bresenham Algorithm for computer control of a digital plotter IBM Syst J 4 1 1965 25 30 [21] Merke A, Welker S, Riedmiller M. Line base robot localisation under natural light conditions. In: Proc of the ECAI workshop on agents in dynamic and real-time environments, Valencia, Spain; 2002. [22] Silva J, Lau N, Rodrigues J, Azevedo JL, Neves AJR. Sensor and information fusion applied to a robotic soccer team. In: RoboCup 2009: robot soccer world cup XIII. Lecture notes in artificial intelligence. Springer; 2009. [23] Almeida L, Santos F, Facchinetti T, Pedreira P, Silva V, Lopes LS. Coordinating distributed autonomous agents with a real-time database: the CAMBADA project. In: Proc of the 19th international symposium on computer and information sciences, ISCIS 2004. Lecture notes in computer science, vol. 3280. Springer; 2004. p. 878–86. [24] Lau N, Lopes LS, Corrente G, Filipe N. Roles, positionings and set plays to coordinate a msl robot team. In: Proc of the 4th international workshop on intelligent robotics, IROBOT’09. Lecture notes in computer science, vol. 5816. Aveiro, Portugal: Springer; 2009. p. 323–37. [25] Lau N, Lopes LS, Corrente G, Filipe N. Multi-robot team coordination through roles, positioning and coordinated procedures. In: Proc of the IEEE/RSJ international conference on intelligent robots and systems, MO, USA: St. Louis; 2009. p. 5841–48. [26] Silva J, Lau N, Neves AJR, Rodrigues J, Azevedo JL. Obstacle detection, identification and sharing on a robotic soccer team. In: Proc of the 4th international workshop on intelligent robotics, IROBOT’09. Lecture notes in computer science, LNAI 5816. Aveiro, Portugal: Springer; 2009. p. 350–60. [27] Mitri S, Frintrop S, Pervolz K, Surmann H, Nuchter A. Robust object detection at regions of interest with an application in ball recognition. In: Proc of the 2005 IEEE international conference on robotics and automation, ICRA 2005, Barcelona, Spain; 2005. p. 125–30. [28] Jonker P, Caarls J, Bokhove W. Fast and accurate robot vision for vision based motion. In: RoboCup 2000: robot soccer world cup IV. Lecture notes in computer science. Springer; 2000. p. 149–58. [29] Hanek R, Schmitt T, Buck S. Fast image-based object localization in natural scenes. In: Proc of the 2002 IEEE/RSJ international conference on intelligent robotics and systems, Lausanne, Switzerland; 2002. p. 116–22. [30] A. Treptow A. Zell Real-time object tracking for soccer-robots without color information Robot Auton Syst 48 1 2004 41 48 [31] Mitri S, Pervolz K, Surmann H, Nuchter A. Fast color independent ball detection for mobile robots. In: Proc of the 2004 IEEE international conference on mechatronics and robotics, Aechen, Germany; 2004. p. 900–5. [32] Coath G, Musumeci P. Adaptive arc fitting for ball detection in RoboCup. In: Proc of the APRS workshop on digital image computing, WDIC 2003, Brisbane, Australia; 2003. p. 63–8. [33] Lu H, Zhang H, Zheng Z. Arbitrary ball recognition based on omni-directional vision for soccer robots. In: Proc of RoboCup 2008; 2008. [34] Nixon M, Aguado A. Feature extraction and image processing. 1st ed. Linacre House, Jordan Hill, Oxford OX2 8DP 225 Wildwood Avenue, Woburn, MA 01801-2041: Reed Educational and Professional Publishing Ltd.; 2002. [35] Ser PK, Siu WC. Invariant hough transform with matching technique for the recognition of non-analytic objects. In: IEEE international conference on acoustics, speech, and signal processing, ICASSP 1993, vol. 5; 1993. p. 9–12. [36] Zhang YJ, Liu ZQ. Curve detection using a new clustering approach in the hough space. In: IEEE international conference on systems, man, and cybernetics, 2000, vol. 4; 2000. p. 2746–51. [37] W.E.L. Grimson D.P. Huttenlocher On the sensitivity of the hough transform for object recognition IEEE Trans Pattern Anal Mach Intell 12 1990 1255 1274 [38] Zou J, Li H, Liu B, Zhang R. Color edge detection based on morphology. In: First international conference on communications and electronics, ICCE 2006; 2006. p. 291–3. [39] Zin TT, Takahashi H, Hama H. Robust person detection using far infrared camera for image fusion. In: Second international conference on innovative computing, information and control, ICICIC 2007; 2007. p. 310. [40] Zou Y, Dunsmuir W. Edge detection using generalized root signals of 2-d median filtering. In: Proc of the international conference on image processing, 1997, vol. 1; 1997. p. 417–9. [41] Blaffert T, Dippel S, Stahl M, Wiemker R. The laplace integral for a watershed segmentation. In: Proc of the international conference on image processing, 2000, vol. 3; 2000. p. 444–7. [42] Canny JF. A computational approach to edge detection. IEEE Trans Pattern Anal Mach Intell 8 (6). "
    },
    {
        "doc_title": "World modeling on an MSL robotic soccer team",
        "doc_scopus_id": "79952617090",
        "doc_doi": "10.1016/j.mechatronics.2010.05.011",
        "doc_eid": "2-s2.0-79952617090",
        "doc_date": "2011-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Coordination and Control",
            "Information fusion techniques",
            "Limited information",
            "Localization algorithm",
            "Obstacle detection",
            "Sensor fusion",
            "Visual matching",
            "World model"
        ],
        "doc_abstract": "When a team of robots is built with the objective of playing soccer, the coordination and control algorithms must reason, decide and actuate based on the current conditions of the robot and its surroundings. This is where sensor and information fusion techniques appear, providing the means to build an accurate model of the world around the robot, based on its own limited sensor information and the also limited information obtained through communication with the team mates. One of the most important elements of the world model is the robot self-localization, as to be able to decide what to do in an effective way, it must know its position in the field of play. In this paper, the team localization algorithm is presented focusing on the integration of visual and compass information. An important element in a soccer game, perhaps the most important, is the ball. To improve the estimations of the ball position and velocity, two different techniques have been developed. A study of the visual sensor noise is presented and, according to this analysis, the resulting noise variation is used to define the parameters of a Kalman filter for ball position estimation. Moreover, linear regression is used for velocity estimation purposes, both for the ball and the robot. This implementation of linear regression has an adaptive buffer size so that, on hard deviations from the path (detected using the Kalman filter), the regression converges faster. A team cooperation method based on sharing the ball position is presented. Other important data during the soccer game is obstacle data. This is an important challenge for cooperation purposes, allowing the improvement of team strategy with ball covering, dribble corridor estimation, pass lines, among other strategic possibilities. Thus, detecting the obstacles is ceasing to be enough and identifying which obstacles are team mates and opponents is becoming a need. An approach for this identification is presented, considering the visual information, the known characteristics of the team robots and shared localization among team members. The described work was implemented on the CAMBADA team and allowed it to achieve particularly good performances in the last two years, with a 1st and a 3rd place in the world championship RoboCup 2008 and RoboCup 2009 editions, respectively, as well as distinctively achieve 1st place in 2008 and 2009 editions of the Portuguese Robotics Open. © 2010 Elsevier Ltd. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271456 291210 291718 291787 291883 31 Mechatronics MECHATRONICS 2010-07-01 2010-07-01 2011-03-08T22:21:19 S0957-4158(10)00102-9 S0957415810001029 10.1016/j.mechatronics.2010.05.011 S300 S300.1 FULL-TEXT 2015-05-15T03:43:39.596909-04:00 0 0 20110301 20110331 2011 2010-07-01T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings vol volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0957-4158 09574158 21 21 2 2 Volume 21, Issue 2 7 411 422 411 422 201103 March 2011 2011-03-01 2011-03-31 2011 Special Issue on Advances in intelligent robot design for the Robocup Middle Size League M.J.G. van de Molengraft O. Zweigle Special Issue on Advances in intelligent robot design for the Robocup Middle Size League article fla Copyright © 2010 Elsevier Ltd. All rights reserved. WORLDMODELINGMSLROBOTICSOCCERTEAM SILVA J 1 Introduction 2 Related work 3 Localization 4 Ball integration 4.1 Ball position 4.2 Ball velocity 4.3 Team ball position sharing 5 Obstacle treatment 5.1 Visual obstacle detection 5.2 Obstacle selection and identification 5.3 Obstacle sharing 6 Conclusion and future work Acknowledgment References METROPOLIS 1949 335 341 N KALMAN 1960 35 45 R LUO 2002 107 119 R LEONARD 1991 376 382 J FOX 1999 391 427 D MOURIKIS 2006 666 681 A DURRANTWHYTE 2008 H SPRINGERHANDBOOKROBOTICS MULTISENSORDATAFUSION BEJCZY 2004 41 42 A ALENYA 2004 23 32 G CHROUST 2004 73 83 S THRUN 2005 W PROBABILISTICROBOTICS SICILIANO 2008 B SPRINGERHANDBOOKROBOTICS LAUER 2005 291 303 M KI2005ADVANCESINARTIFICIALINTELLIGENCE MODELINGMOVINGOBJECTSINADYNAMICALLYCHANGINGROBOTAPPLICATION FERREIN 2006 154 165 A ROBOCUP2005ROBOTSOCCERWORLDCUPIX COMPARINGSENSORFUSIONTECHNIQUESFORBALLPOSITIONESTIMATION LAUER 2006 142 153 M ROBOCUP2005ROBOTSOCCERWORLDCUPIX CALCULATINGPERFECTMATCHEFFICIENTACCURATEAPPROACHFORROBOTSELFLOCALIZATION NEVES 2007 499 507 A PROGRESSINARTIFICIALINTELLIGENCE OMNIDIRECTIONALVISIONSYSTEMFORSOCCERROBOTS CUNHA 2008 417 424 B ROBOCUP2007ROBOTSOCCERWORLDCUPXI OBTAININGINVERSEDISTANCEMAPANONSVPHYPERBOLICCATADIOPTRICROBOTICVISIONSYSTEM SILVAX2011X411 SILVAX2011X411X422 SILVAX2011X411XJ SILVAX2011X411X422XJ item S0957-4158(10)00102-9 S0957415810001029 10.1016/j.mechatronics.2010.05.011 271456 2011-03-10T12:04:28.24434-05:00 2011-03-01 2011-03-31 true 2037530 MAIN 12 77452 849 656 IMAGE-WEB-PDF 1 si1 820 43 146 si4 279 20 19 si3 279 20 19 si2 672 40 84 gr10 40478 301 366 gr10 6601 164 200 gr11 38816 374 635 gr11 3816 129 219 gr12 45526 461 779 gr12 2642 130 219 gr13 25831 160 330 gr13 9938 106 219 gr14 26031 374 628 gr14 2725 130 219 gr15 25951 161 322 gr15 10234 109 219 gr16 11424 220 243 gr16 3895 164 181 gr17 34129 332 324 gr17 9007 164 160 gr18 28869 336 320 gr18 8228 164 156 gr19 31951 292 350 gr19 4895 164 196 gr2 24610 181 349 gr2 9025 114 219 gr20 32518 300 345 gr20 5136 163 188 gr21 11053 222 356 gr21 3093 137 219 gr3 14423 252 223 gr3 4214 164 145 gr4 90061 420 715 gr4 6962 129 219 gr5 31815 170 578 gr5 3190 64 219 gr6 34213 295 372 gr6 5554 164 207 gr7 32017 304 505 gr7 3661 132 219 gr8 11017 117 266 gr8 2052 96 219 gr9 53806 297 357 gr9 9002 164 197 gr1 35942 173 489 gr1 9273 77 219 MECH 1165 S0957-4158(10)00102-9 10.1016/j.mechatronics.2010.05.011 Elsevier Ltd Fig. 1 Picture of the team robots used to obtain the results presented on this paper. Fig. 2 Captures of an image acquired by the robot camera and processed by the vision algorithms. Left (a): The image acquired by the camera. Right (b): The same image after processing with magenta dots over the detected field lines. Fig. 3 Illustration of the compass error angle intervals. Fig. 4 Illustration of two situations where relocation was forced. Dashed line represents the angle given by the compass, solid line represents the angle estimated by the localization algorithm, red lines represent the cycles on which the error between the two angles is greater than the threshold. Left (a): The camera was covered while the robot moved. The estimated orientation error degrades progressively and after getting higher than a threshold, the cycle count starts and forces relocation. Right (b): The robot tilted. The estimated orientation error is immediately affected by more than a threshold and the cycle count starts and forces relocation. Fig. 5 Noisy position of a static ball taken from a rotating robot. Fig. 6 Plot of a robot movement around a fixed ball position. The ball positions measured by the moving robot form a cloud of points (green) in the area of the real ball position (black X). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 7 Plot of a ball movement situation. Fig. 8 Situation where a hard deviation would be detected by the filter. Positions R4,5,6, are the measured positions after the ball hits an obstacle, P4,5,6 are the predicted filtered estimations, which did not consider that something might alter the ball path. Fig. 9 Velocity representation using consecutive measures displacement. Fig. 10 Velocity representation using linear regression over Kalman filtered positions. Fig. 11 Comparison between the velocity estimated by the linear regression (blue solid line, faster convergence) and internally by the Kalman filter (red dashed line, smoother, but of slow convergence). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 12 Diagram of the ball integration algorithm. Fig. 13 Captures of an image acquired by the robot camera and processed by the vision algorithms. The areas of interest were surrounded. (a) The image acquired by the camera. (b) The same image after processing. Obstacles are identified by their center (triangle), left and right limits (squares). It is visible that the two aligned obstacles are detected as a single larger obstacle (top right of the frames). Fig. 14 Relation between pixels and metric distances. The center of the robot is considered the origin and the metric distances are considered on the ground plane. Fig. 15 Example of an image acquired by the robot camera and processed by the vision algorithm. The areas of interest are surrounded. (a) The image acquired by the camera. (b) The same image after processing. It is visible the two possibilities of separation made: angular separation, on the bottom pair of obstacles and length separation, on the top pair of obstacles. Fig. 16 When a CAMBADA robot is on, the estimated centers of the detected obstacles are compared with the known position of the team mates and tested; the left obstacle is within the CAMBADA acceptance radius, the right one is not. Fig. 17 Illustration of single obstacles identification. (a) Image acquired from the robot camera (obstacles for identification are marked). (b) The same image after processing. (c) Image of the control station. Each robot represents itself and robot 6 (the lighter gray) draws all the five obstacles evaluated (squares with the same gray scale as itself). All team mates were correctly identified (marked by its corresponding number over the obstacle square) and the opponent is also represented with no number. Fig. 18 Illustration of multiple obstacles identification. (a) Image acquired from the robot camera (obstacle for identification marked). (b) The same image after processing. Visually, the aligned robots are only one large obstacle. (c) Image of the control station. Each robot represents itself and robot 6 (the darker gray) draws all the five obstacles (squares with the same gray scale as itself). The visual obstacle was successfully separated into the several composing obstacles, and all of them were correctly identified as the correspondent team mate (marked by its corresponding number over the obstacle square) and the opponent is also represented with no number. Fig. 19 Representation of a capture of the obstacle identification algorithm results. The path taken by the observer is represented by blue dots in the rectangular path taken. Near the center, the pivot shared position is represented by the black star and its limits by the black circle. The blob of red is the overlapping positions of the identified obstacle center, represented by a red cross. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 20 Representation of the path taken by the team mate to identify (the red dots represent each communicated position). The observer position is represented by the black star and its limits by the black circle. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.) Fig. 21 Image of the control station showing an obstacle of robot 2 that was not seen by itself (on the center of the field). In this case it assumes the obstacle by confirmation of both robots 5 and 6. Table 1 The mean and standard deviation of the capture perceived obstacle position. Perceived obstacle X Y Mean 0.05 2.01 Std 0.08 0.07 ∣Real−perceived∣=0.16. ∣Std∣=0.10. Table 2 The individual ratio of successful identification of the moving team mate for the several captures performed. Total cycles Successes % Capture 1 1798 1319 73 Capture 2 1065 748 70 Capture 3 1528 1332 87 Capture 4 1162 769 66 Capture 5 1935 1278 66 Capture 6 2152 1411 66 World modeling on an MSL robotic soccer team João Silva ⁎ Nuno Lau António J.R. Neves João Rodrigues José Luís Azevedo ATRI, IEETA/DETI, University of Aveiro, 3810-193 Aveiro, Portugal ⁎ Corresponding author. When a team of robots is built with the objective of playing soccer, the coordination and control algorithms must reason, decide and actuate based on the current conditions of the robot and its surroundings. This is where sensor and information fusion techniques appear, providing the means to build an accurate model of the world around the robot, based on its own limited sensor information and the also limited information obtained through communication with the team mates. One of the most important elements of the world model is the robot self-localization, as to be able to decide what to do in an effective way, it must know its position in the field of play. In this paper, the team localization algorithm is presented focusing on the integration of visual and compass information. An important element in a soccer game, perhaps the most important, is the ball. To improve the estimations of the ball position and velocity, two different techniques have been developed. A study of the visual sensor noise is presented and, according to this analysis, the resulting noise variation is used to define the parameters of a Kalman filter for ball position estimation. Moreover, linear regression is used for velocity estimation purposes, both for the ball and the robot. This implementation of linear regression has an adaptive buffer size so that, on hard deviations from the path (detected using the Kalman filter), the regression converges faster. A team cooperation method based on sharing the ball position is presented. Other important data during the soccer game is obstacle data. This is an important challenge for cooperation purposes, allowing the improvement of team strategy with ball covering, dribble corridor estimation, pass lines, among other strategic possibilities. Thus, detecting the obstacles is ceasing to be enough and identifying which obstacles are team mates and opponents is becoming a need. An approach for this identification is presented, considering the visual information, the known characteristics of the team robots and shared localization among team members. The described work was implemented on the CAMBADA team and allowed it to achieve particularly good performances in the last two years, with a 1st and a 3rd place in the world championship RoboCup 2008 and RoboCup 2009 editions, respectively, as well as distinctively achieve 1st place in 2008 and 2009 editions of the Portuguese Robotics Open. Keywords Sensor fusion World model Kalman filter Linear regression Obstacle detection Visual matching 1 Introduction Nowadays, there are several research domains in the area of multi robot systems. One of the most popular is robotic soccer. RoboCup 1 1 is an international joint project to promote artificial intelligence, robotics and related fields. Most of the RoboCup leagues have soccer as platform for developing technology, either at software or hardware levels, with single or multiple agents, cooperative or competitive [1]. Among RoboCup leagues, the Middle Size League (MSL) is one of the most challenging. In this league, each team is composed of up to five robots with maximum size of 50×50cm base, 80cm height and a maximum weight of 40kg, playing in a field of 18×12m. The rules of the game are similar to the official FIFA rules, with required changes to adapt for the playing robots [2]. Each robot is autonomous and has its own sensorial means. They can communicate with each other, and with an external computer acting as a coach, through a wireless network. This coach computer cannot have any sensor, it only knows what is reported by the playing robots. The agents should be able to evaluate the state of the world and make decisions suitable to fulfill the cooperative team objective. CAMBADA, Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture, is the Middle Size League Robotic Soccer team from the University of Aveiro. The project started in 2003, coordinated by the IEETA 2 Instituto de Engenharia Electrónica e Telemática de Aveiro – Aveiro’s Institute of Electronic and Telematic Engineering. 2 ATRI 3 Actividade Transversal em Robótica Inteligente – Transverse Activity on Intelligent Robotics. 3 group and involves people working on several areas for building the mechanical structure of the robot, its hardware architecture and controllers and the software development in areas such as image analysis and processing, sensor and information fusion, reasoning and control (see Fig. 1 ). This paper provides a description of some sensor and information fusion techniques and algorithms used in the CAMBADA team. The data obtained by these techniques are necessary for building a world model of the robot environment. This paper includes the description of some of the elements of that model necessary for a team of robots to play soccer. In Section 2, a brief overview of some related topics and work in sensor and information fusion for world modeling are presented. Section 3 presents the team self-localization description, introducing it as the first necessary step for all the other information fusion. In Section 4, the ball integration process is presented in all its components, starting with the ball position, its velocity and finally its sharing among team mates. Section 5 presents an overview of obstacle treatment, with some visual detection details, the matching of positions for visual identification and the sharing of information among team mates. Finally, Section 6 concludes the paper. 2 Related work World modeling and sensor and information fusion are tightly related, as the latest provide the means to build the desired model. Sensor and information fusion is the process of combining sensory data, or data derived from sensory data, providing a resulting information that is better than would be possible when the sources were used individually [3]. One of the main areas where sensor fusion techniques are used is position tracking, both for self and object localization/tracking. The integration of information over time in order to filter sensor noise is essential to get better estimates. This type of integration may be performed using Kalman filter based approaches, Monte Carlo methods or Markov approaches. Generally, Monte Carlo [4] approaches have better performance in cases where great discontinuities of the output values are expected, as the assumption of Gaussian probability density functions of the Kalman filter [5] is usually less accurate. However, Kalman filtering is a very effective method if the assumptions of Gaussian noise can be met and the system can be linearized. Other common approaches are the use of the Extended and Unscented Kalman filters [6], which are prepared to deal with non-linear systems at the cost of more computational weight. A general overview of different methods of multi-sensor and information fusion is presented in [7], also with a brief description of application areas, such as robotics, military, biomedical and transportation. Applications in the robotics field include self-localization using either Kalman filter [8], Monte Carlo [9] or Markov [10] methods, or integration of information coming from several robots, to increase the accuracy of each of the robots position estimation [11]. A general recent overview of methods and architectures for multi-sensor data fusion can be found in [12]. Another recurrent problem nowadays is the fusion of visual and inertial sensors [13], where recent results have demonstrated that the visual tracking of objects may work at higher velocities and be more robust if combined with information coming from inertial sensors [14] and also that ego-motion estimation can be more precise and navigation more robust using these approaches [15]. Simultaneous Localization And Mapping (SLAM) is another common application of sensor fusion techniques, as in many cases, autonomous robots have to map the environment rather than simply localize themselves [16,17]. Particularly in RoboCup domain, several teams use this kind of approaches, not only for localization purposes, but also for position estimation and tracking of objects, namely the ball and other robots. Several teams have used Kalman filters for the ball position estimation [18–21]. In [20,21], several information fusion methods are compared for the integration of the ball position using several observers. In [21], the authors conclude that the Kalman reset filter shows the best performance. Although using well known techniques, in this paper we propose practical solutions for an efficient self-localization, ball information treatment and obstacle treatment for an MSL robotic soccer team. As far as we know, no previous work has been published focusing on these several important aspects of developing the world model of an MSL soccer team. 3 Localization Self-localization of the agent is an important issue for a soccer team, as strategic moves and positioning must be defined by positions on the field. In the MSL, the environment is partially known, as every agent knows exactly the layout of the game field but does not know the position of any other elements, either itself, other robots or the ball. Given the known map, the agent has then to locate itself. The CAMBADA team localization algorithm is based on the detected field lines, with fusion of information from the odometry sensors and an electronic compass. It is based on the approach described in [22], with some adaptations. It can be seen as an error minimization task, with a derived measure of reliability of the calculated position so that a stochastic sensor fusion process can be applied to increase the estimation accuracy [22]. From the center of the image (the center of the robot), radial sensors are created around the robot, each one represented by a line with a given angle. These are called scanlines. The image processing, in each cycle, returns a list of positions relative to the robot where the scanlines intercept the field line markings [23]. The idea is to analyze the detected line points, estimating a position, and through an error function describe the fitness of the estimation. This is done by reducing the error of the matching between the detected lines and the known field lines (Fig. 2 ). The error function must be defined considering the substantial amount of noise that affects the detected line points which would distort the representation estimation [22]. In normal operation mode, the localization is done over a limited set of base positions from which tracking is maintained. Since it is an algorithm based on optimization and since there are many local minima, the tracking only works satisfactorily if the estimations are near the solution. In situations where the robot does not possess a valid estimation, a global localization algorithm estimates the robot position on the field using a much wider set of initial estimations over which the already referred error minimization process for optimization is applied. However, this global localization algorithm is computationally heavy and time consuming. For that reason, after having an initial position, the simpler tracking localization handles the cyclic relocation. Although the odometry measurement quality quickly degrades with time, within the reduced cycle times achieved in the application, consecutive readings produce acceptable results and thus, having the visual estimation, it is fused with the odometry values to refine the estimation. This fusion is based on a Kalman filter for the robot position estimated by odometry and the robot position estimated by visual information. This approach allows the agent to estimate its position even if no visual information is available. However, it is not reliable to use only odometry values to estimate the position for more than a few cycles, as slidings and frictions on the wheels produce large errors on the estimations in short time. Due to the nature of the approach, this algorithm works acceptably with a relatively low number of points, like a few tens of points, as long as they are representative of the surroundings. Consider the case of matching a 90degrees corner. If the algorithm had access to 200 points all over the same line, it would not be capable of matching the corner. On the other hand, with only 20 or 30 points scattered over both lines, the algorithm would be capable of detecting the match. Even in situations where the points are over the same line, the merging with odometry and position tracking provide a good robustness to the algorithm [22], as long as the situation is temporary, which is usually the case. The visually estimated orientation can be ambiguous, i.e. each point on the soccer field has a symmetric position, relatively to the field center, where the robot detects exactly the same field lines. To disambiguate the symmetry problem and to detect wrong estimations, an electronic compass is used. The orientation estimated by the robot is compared to the orientation given by the compass and if the error between them is larger than a predefined threshold, actions are taken. If the error is really large (i.e. around ±180degrees), it means that the robot estimated orientation is symmetric to the real one, so it should assume the mirror position. On the other hand, if the error is larger than the acceptance threshold (i.e. a 90degrees acceptable area), a counter is incremented (Fig. 3 ). This counter will be incremented every cycle in which the error is greater than the threshold. If a given number of consecutive cycles with high errors is reached (i.e. the counter reaches a given number, currently 10), the robot considers itself “lost”, meaning that it will not continue to track its position but will instead consider the initial situation, with no a priori knowledge and thus executes the global localization algorithm. Fig. 4 shows situations where the threshold was reached and relocation was forced after some cycles. 4 Ball integration The information of the ball state (position and velocity) is, perhaps, the most important, as it is the main object of the game and it is the base over which most decisions are taken. Thus, its integration has to be as reliable as possible. To accomplish this, a Kalman filter implementation was created to filter the estimated ball position given by the visual information, and a linear regression was applied over filtered positions to estimate its velocity. 4.1 Ball position It is assumed that the ball velocity is constant between cycles. Although that is not true, due to the short time variations between cycles, around 40ms, and given the noisy environment and measurement errors, it is a quite acceptable model for the ball movement. Thus, no friction is considered to affect the ball, and the model does not include any kind of control over the ball. Therefore, given the Kalman filter formulation (described in [24]), the assumed state transition model is given by X k = 1 Δ T 0 1 X k - 1 where X k = Pos Vel is the state vector containing the position and velocity of the ball. Both are composed by the respective (x, y) coordinates. This velocity is only internally estimated by the filter, as the robot sensors can only take measurements on the ball position. After defining the state transition model based on the ball movement assumptions described above and the observation model, the description of the measurements and process noises are important issues to attend. The measurements noise can be statistically estimated by taking measurements of a static ball position at known distances. In practice, measurements of the static ball were taken while the robot was rotating around its vertical axis and this was done with the ball placed at several distances, measured with metric tape. Although real game conditions are probably more adverse, we lack the means to externally know the position of the elements on the field. For that reason, to know the real distance between the robot and the ball, we opted to use the described setup. Some of the results are illustrated in Fig. 5 . The standard deviation of those measurements can be used to calculate the variance and thus define the measurements noise parameter. A relation between the distance of the ball to the robot and the measurements standard deviation can be modeled by a 2nd degree polynomial best fitting the data set in a least-squares sense. Depending on the available data, a polynomial of another degree could be used, but we should always keep in mind the computational weight of increasing complexity. As for the process noise, this is not trivial to estimate, since there is no way to take independent measurements of the process to estimate its standard deviation. The process noise is represented by a matrix containing the covariances correspondent to the state variable vector. Based on the Kalman filter functioning, one can verify that forcing a near null process noise causes the filter to practically ignore the read measures, leading the filter to emphasize the model prediction. This makes it too smooth and therefore inappropriate. On the other hand, if it is too high, the read measures are taken too much into account and the filter returns the measures themselves. To face this situation, one has to find a compromise between stability and reaction. Since we assume an uniform movement for the ball, there are no frictions or other external forces considered. This means that accelerations are not considered in our model and thus, the position and velocity components are quite independent of each other. Since acceleration is the main element of relation between position and velocity, we considered that the errors associated to the process position and velocity estimations do not correlate. Because we assume an uniform movement model that we know is not the true nature of the system, we know that the speed calculation of the model is not very accurate. A process noise covariance matrix was empirically estimated, based on several tests, so that a good smoothness/reactivity relationship was kept. These empirically estimated values were made dependent on the measurement noise so that the Kalman filter predictions are also less accurate when the distance to the ball is too large. This was done so that the filter does not smooth the positions too much. In practice, this approach proved to improve the estimation of the ball position. Since we do not possess the means to externally know the positions of the elements on the field, a capture was made with the ball fixed at a known position on the field (0.0,2.0) (measured with metric tape). The robot was moving around the ball with a speed of 1.3±0.5m/s and the ball position measured at each moment was recorded. The ball position measured by the robot was (−0.01,2.03)±(0.05,0.06)m. Fig. 6 illustrates the capture results. This experiment gives an idea of the noise associated with the ball position detection. Note that during the experiment the distance between the robot and the ball is around 2m. Comparing the ball position cloud with the one obtained at 2m in Fig. 5 one can verify that they are similar, which is consistent with the previous experiment setup to simulate robot movement by rotation on the spot. With the presented setup experiments, the existence of noise in ball measurements became clear. With that existent noise in mind, several tests were made to validate the use of the Kalman filter to reduce it. Fig. 7 represents a capture of one of those tests, a ball movement, where the black dots are the ball positions measured by the robot visual sensors and thus are unfiltered. Red stars 4 For interpretation of color in ‘Figs. 1,2,4-7,9-11,13-15,17-20’ the reader is referred to the web version of this article. 4 represent the position estimations after applying the Kalman filter. The robot position is represented by the black star in its center and its respective radius. The ball was thrown against the robot and deviated accordingly. It is easily perceptible that the unfiltered positions are affected by much noise and the path of the ball after the collision is composed of positions that do not make much physical sense. Although we lack the means to externally provide a ground truth for the ball position during its movements, the filtered positions seem to give a much better approximation to the real path taken by the ball, as they provide a path that physically makes more sense. After producing the a priori estimation of the ball position, this estimation is compared with the read measure to detect if the variation between them is too great. If the difference between them is consistently greater than a given threshold (estimated empirically), the filter can indicate that the ball suffered a hard deviation (Fig. 8 illustrates this concept). Although hard deviations are not a serious problem for the filter (as it quickly converges to the new positions), they are used for velocity convergence (as described in the next subsection). 4.2 Ball velocity The calculation of the ball velocity is a feature becoming more and more important over the time. It allows that better decisions can be implemented based on the ball speed value and direction. Assuming the same ball movement model described before, constant ball velocity between cycles and no friction considered, one could theoretically calculate the ball velocity by simple instantaneous velocity of the ball with the first order derivative of each component Δ D Δ T , being ΔD the displacement on consecutive measures and ΔT the time interval between consecutive measures. However, given the noisy environment, it is also predictable that this approach would be greatly affected by that noise and thus its results would not be satisfactory. Fig. 9 shows a ball movement capture where the ball was moving from left to right, as indicated by the arrow in the top of the figure, and was then deviated into a downward movement near the “1st deviation” tag. While moving downward, the ball was deviated again near the “2nd deviation” tag and started to move from right to left. Finally, in the end of the capture, a new deviation occurred near tag “3rd deviation” where the ball started to move upward. The estimated ball positions are represented by the blue dots. Red lines represent the velocity vectors estimated based on consecutive positions displacement. It is clear that the velocity estimates hardly give an acceptable insight of the ball movement. To keep a calculation of the object velocity consistent with its displacement, an implementation of a linear regression algorithm was chosen. This approach based on linear regression [25] is similar to the velocity estimation described in [18]. By keeping a buffer of the last m measures of the object position and sampling instant (in this case buffers of nine samples were used), one can calculate a regression line to fit the positions of the object. Since the object position is composed by two coordinates (x, y), we actually have two linear regression calculations, one for each dimension. This is made in a transparent way, so the description is presented generally, as if only one dimension was considered. When applied over the positions estimation, the linear regression velocity estimations are much more accurate than the instant velocities calculated by Δ D Δ T , and allow a better insight of the ball movement. The same ball movement capture described earlier is represented in Fig. 10 , this time with the velocity vectors estimated by the linear regression applied over the position estimations provided by the Kalman filter. In order to try to make the regression converge more quickly on deviations of the ball path, a reset feature was implemented. This allows deletion of the older values, keeping only the n most recent ones, and provides control of the buffer size. By keeping the most recent values after a hard deviation, we reduce outliers of the previous path, thus promoting faster convergence. This reset results from the interaction with the Kalman filter described earlier by querying it for the existence of a hard deviation on the ball path. The obtained values were tested to confirm if the linear regression of the ball positions was more precise and would converge faster than the internal velocity estimated by the Kalman filter. Tests showed that the velocity estimated by the Kalman filter has a slower response than the linear regression estimation when deviations occur. Given this, the linear regression was used to estimate the velocity because quickness of convergence was preferred over the slightly smoother approximation of the Kalman filter in the steady state. That is because in the game environment the ball is very dynamic, it constantly changes its direction and thus a convergence in less than half the cycles is much preferred. Fig. 11 shows the results for a theoretical velocity scenario where the ball was moving at a constant speed of 2m/s and suddenly dropped to a constant 1m/s speed. Both the speeds estimated by the Kalman filter and the ones estimated by the linear regression are presented. 4.3 Team ball position sharing Due to the highly important role that the ball has in a soccer game, when a robot cannot detect it by its own visual sensors (omni or frontal camera), it may still know the position of the ball, through sharing of that knowledge by the other team mates. The ball data structure includes a field with the number of cycles it was not visible by the robot, meaning that the ball position given by the vision sensors can be the “last seen” position. When the ball is not visible for more than a given number of cycles, the robot assumes that it cannot detect the ball on its own. When that is the case, it uses the information of the ball communicated by the other running team mates to know where the ball is. This can be done by getting the mean and standard deviation of the positions of the ball seen by team mates. Another approach is to simply use the ball position of the team mate that has more confidence in the detection. Independently of the chosen approach, the robot assumes that ball position as correct. When detecting the ball on its own, there is also the need to validate that information. Currently the seen ball is only considered if it is within a given margin inside the field of play as there would be no point in trying to play with a ball outside the field. For ball position sharing, an approach based on the highest confidence ball position is used. This is due to the fact that the shared positions are updated with 100ms periods, with the possibility of a few more milliseconds of unknown and unpredictable delay in packet transmission. Thus, the lifetime of the information of each team mate is different, and the use of the information of the team mate with higher confidence reduces the probability of the degradation of that information during the respective lifetime. Fig. 12 illustrates the general ball integration activity diagram. 5 Obstacle treatment While playing soccer, the robots have the need to navigate around the field effectively, which means they have to reposition themselves or dribble the ball avoiding the obstacles on the field, that can be either team or opponent robots, or eventually the referee. An increasing necessity felt by the team, to improve its performance, is a better obstacle detection and sharing of obstacle information among team mates. This is important to ensure a global idea of the field occupancy, since the team formation usually keeps the robots spread across the field. Pass lines and dribbling corridors can be estimated more easily with a good coverage of field obstacles, allowing improvements on team strategy and coordination. 5.1 Visual obstacle detection The CAMBADA robots gather their information about the surroundings by means of a robotic vision system. Currently, only the omni directional camera gathers information about obstacles, as no frontal camera is being used at this time. According to RoboCup rules, the robots are mainly black. Since during the game robots play autonomously, all obstacles in the field are the robots themselves (occasionally the referee, which is recommended to wear black/dark pants). The vision algorithm detects the obstacles by evaluating blobs of black color inside the field of play [26]. Through the mapping of image positions to real metric positions [27], obstacles are identified by their center (triangle on the processed image, Fig. 13 b) and left and right limits (squares on the processed image, Fig. 13b). This is done by searching black regions on the scanlines of the vision algorithm [23], already referred in Section 3. The detection of black color on the scanlines is analyzed both in angular intervals and length intervals, to define the limits of each black blob (considering their base points which are represented by the first black pixel in each scanline). Since the vision system is a non-SVP hyperbolic catadioptric system [27], the size of objects on the image varies with the distance to the robot. Due to an inverse distance map calculation, by exploring a back-propagation ray-tracing approach and the geometric properties of the mirror surface, the relation of distances in the image and the real world is known. Fig. 14 is an illustration of how the distance in pixels, from the center of the image, is mapped to the distance in meters, on the ground plane. Through the function represented in Fig. 14, it is possible to create a normalized relation of blobs width and length with the distance. Sometimes an obstacle is separated in several blobs, mainly due to the noise in the image and problems in color classification, which leads to failure in the detection of black regions in the scanlines. To avoid these situations, an offset is considered to decide when the angular space between blobs is considered enough to represent a real obstacle separation. The same principle is considered concerning the position of the black area in consecutive scanlines. The separation offsets of a blob close to the robot are bigger than the ones at a high distance, to maintain coherent precision. The angular separation offset is considered for situations where robots are side-by-side, at the same distance, but there is no visual contact between each blob; the length separation offset is checked for situations where, on consecutive scanlines, there are blobs with visual contact but the robots are actually at different distances. Both situations are depicted in Fig. 15 . For each detected blob, their number of pixels is calculated and an estimation of the obstacles left and right limits, as well as their centers, is made. This information is made available to the integration process for filtering and treatment. 5.2 Obstacle selection and identification With the objective of refining the information of the obstacles, and have more meaningful and human readable information, the obstacles are selected and a matching is attempted, in order to try to identify them as team mates or opponents. Due to the weak precision at long distances, a first selection of the obstacles is made by selecting only the obstacles closer than a given distance as available for identification (currently 5m). Also, obstacles that are smaller than 10cm wide or outside the field of play margin are ignored. This is done because the MSL robots are rather big, and in-game situations small obstacles are not present inside the field. Also, it would be pointless to pay attention to obstacles that are outside the field of play, since the surrounding environment is completely ignorable for the game development. To be able to distinguish obstacles, identifying which of them are team mates and which are opponent robots, a fusion between the own visual information of the obstacles and the shared team mates positions is made. By creating a circle around the team mate positions with the robot radius (considered 22cm), a matching of the estimated center of visible obstacle area is made (Fig. 16 ), and the obstacle is identified as the corresponding team mate in case of a positive matching (Figs. 17 c and 18c). This matching consists on the existence of interception points between the team mate circle and the obstacle circle or if the obstacle center is inside the team mate circle (the obstacle circle can be smaller, and thus no interception points would exist). Since the detected obstacles can be large blobs, the above described identification algorithm cannot be applied directly to the visually detected obstacles. If the detected obstacle fulfills the minimum size requisites already described, it is selected as candidate for being a robot obstacle. Its size is evaluated and classified as robot if it does not exceed the maximum size allowed for MSL robots [2] (Fig. 17a and b). If the obstacle exceeds the maximum size of an MSL robot, a division of the obstacle is made, by analyzing its total size which is used to estimate how many robots are in that obstacle. This may be a common situation, robots clashing together and thus creating a compact black blob, originating a big obstacle if they are sufficiently lined up (Fig. 18 a and b). Although the computations for obstacle identification were in use during RoboCup 2009, their results are yet to be considered in the team strategy. Currently, obstacles are always considered unfriendly and thus to be avoided. Due to this fact, there is currently no data of in-game results for this part of the work. Several captures of the obstacle identification algorithm described earlier were performed and analyzed, to further illustrate the effectiveness of the algorithm. The laboratory used for the tests receives natural light which can affect the vision processing algorithms. The presented results are not treated in any way to diminish the effects of natural light, as we are interested in understanding if the algorithms can cope with those conditions which can be found in real situations. In the first test situation, a robot was positioned on the field at (−0.05,1.88) while broadcasting its position. This robot will be referred to as pivot. Another robot was moving on a rectangular path around the pivot, and a capture of its data was done. This robot will be referred to as observer. This scenario is intended to give some insight about the performance of the identification when the team mates are static or nearly static (as is the case of set plays during the games. In these situations it is important to analyze passing lines). Fig. 19 is a graphic representation of the acquired data, with the pivot represented in black. The blue dots are the positions of the path taken by the observer, which covers the rectangular path for three times. In each cycle, the center of the obstacle perceived by the observer is represented by a red ‘×’. It is visible that, as expected, the obstacle position perceived by the observer is not exactly the pivot position. The capture in question is composed of 677 cycles. The identification of the obstacle as the correspondent team mate failed to succeed in only one cycle, which corresponds to a 99.85% success rate. Considering that the pivot has 22cm radius (although it is slightly bigger), the mean of the centers of the perceived obstacle is within the real area occupied by the pivot, at nearly 16cm with a standard deviation of 10cm (Table 1 ). Another test scenario was considered for evaluation of the algorithm performance for moving obstacles. Several captures were performed to evaluate the performance of the algorithm when identifying a moving team mate. This set of six captures consisted on a robot observing a team mate moving around and registering the data about the obstacles. The path taken by the moving team mate is represented in Fig. 20 . The number of failed identifications was greater when the moving robot was farther from the observer, as expected due to the noisy nature of the measurements. The captures were performed throughout the day, with different lighting conditions but with the same robot calibration. Table 2 summarizes this set of captures, which revealed a total mean identification ratio of approximately 71%. 5.3 Obstacle sharing With the purpose of improving the global perception of the team robots, the sharing of locally known information is an important feature. Obstacle sharing allows the team robots to have a more global perception of the field occupancy, allowing them to estimate, for instance, passing and dribbling corridors more effectively. However, one has to keep in mind that, mainly due to illumination conditions and eventual reflective materials, some of the detected obstacles may not be exactly robots, but dark shadowy areas. If that is the case, the simple sharing of obstacles would propagate an eventually false obstacle among the team. Thus the algorithm for sharing the obstacles makes a fusion of the several team mates information. The fusion of the information is done mate by mate. After building the worldstate by its own means, the agent checks all the available obstacle information provided by team mates, one by one. Their obstacles are matched with the own ones. If the agent does not know an obstacle shared by the team mate, it keeps it in a temporary list of unconfirmed obstacles. This is done to all the team mates obstacles. When another team mate shares a common obstacle, that same obstacle is confirmed and is transferred to the local list of obstacles. In the current cycle, the temporary obstacles that were not confirmed are not considered. A robot does not use negative information from other robots to remove obstacles it actually saw from its local world model. An outline of the algorithm is presented next. for c:=1 to total_number_of_team_mates for o:=1 to total_obstacles_of_team_mate for m:=1 to total_own_obstacles if m matches o I already know this obstacle, do nothing else if previously known by another team mate obstacle confirmed and added else obstacle considered temporarily waits for confirmation by another team mate endif endif endfor endfor endfor The matching of the team mate obstacles with the own obstacles is done in a way similar to the matching of the obstacle identification with the team mate position described earlier. The CAMBADA team mate position in Fig. 16 is replaced by the current team mate obstacle for the matching test. Fig. 21 shows a situation where robot 2, in the goal area was too far to see the obstacle on the middle of the field. Thus, it considered the obstacle in question, only because it is identified by both robots 5 and 6, as visible in the figure. 6 Conclusion and future work The techniques chosen for information and sensor fusion proved to be effective in accomplishing their objectives. The Kalman filter allows to filter the noise on the ball position and provides an important prediction feature which allows fast detection of deviations of the ball path. The linear regression used to estimate the velocity is also effective, and combined with the deviation detection based on the Kalman filter prediction error, provides a faster way to recalculate the velocity in the new trajectory. The improvement on obstacle treatment allows modifications on the overall team strategy, particularly regarding passing possibilities. It also allows the improvement of the robots movement, since team mate obstacles can have a different treatment than the opponents, because team mates have velocities and other information available. The CAMBADA team obtained the 1st place in the last years of the Portuguese robotics open (Robótica 2007, Robótica 2008, Robótica 2009 and Robótica 2010), and internationally achieved 5th place in RoboCup 2007, 1st place in RoboCup 2008, 3rd place in RoboCup 2009 and 2nd place in GermanOpen 2010. Although the described work proved to be effective and helped to achieve good results, improving is always the aim for this kind of project. Thus, improvements on the localization algorithm are desired, as well as a different way to disambiguate symmetric positions to eventually complement or replace the compass. Another path to follow would be the improving of team strategy based on obstacle identification, creating new forms of cooperation and set plays for in-game situations. Acknowledgment This work was partially supported by project ACORD Adaptive Coordination of Robotic Teams, FCT/PTDC/EIA/70695/2006. References [1] Kitano H, Asada M, Kuniyoshi Y, Noda I, Osawa E. RoboCup: the robot world cup initiative. In: Proceedings of the first international conference on autonomous agents. New York (NY, USA): ACM; 1997. p. 340–7. [2] MSL Technical Committee 1997–2009. Middle size robot league rules and regulations for 2009; 2008. [3] Elmenreich W. Sensor fusion in time-triggered systems. Ph.D. thesis. Vienna (Austria): Technische Universitat Wien, Institut fur Technische Informatik; 2002. [4] N. Metropolis S. Ulam The Monte Carlo method J Am Stat Assoc 44 247 1949 335 341 [5] R. Kalman A new approach to linear filtering and prediction problems J Basic Eng 82 1 1960 35 45 [6] Wan E, Merwe RVD. The unscented Kalman filter for nonlinear estimation. In: IEEE adaptive systems for signal processing, communications, and control symposium; 2000. p. 153–8. [7] R. Luo C. Yih K. Su Multisensor fusion integration: approaches, applications, and future research directions IEEE Sens J 2 2 2002 107 119 [8] J. Leonard H. Durrant-Whyte Mobile robot localization by tracking geometric beacons IEEE Trans Robotics Autom 7 3 1991 376 382 [9] Dellaert F, Fox D, Burgard W, Thrun S. Monte Carlo localization for mobile robots. In: IEEE international conference on robotics and automation; 1999. p. 1322–8. [10] D. Fox W. Burgard S. Thrun Markov localization for mobile robots in dynamic environments J Artif Intell Res 11 1999 391 427 [11] A. Mourikis S. Roumeliotis Performance analysis of multirobot cooperative localization IEEE Trans Robotics 22 4 2006 666 681 [12] H. Durrant-Whyte T. Henderson Multisensor data fusion B. Siciliano O. Khatib Springer handbook of robotics 2008 Springer [13] A. Bejczy J. Dias Editorial: integration of visual and inertial sensors J Robotic Syst 21 2 2004 41 42 [14] G. Alenyá E. Martínez C. Torras Fusing visual and inertial sensing to recover robot ego-motion J Robotic Syst 21 1 2004 23 32 [15] S. Chroust M. Vincze Fusion of vision and inertial data for motion and structure estimation J Robotic Syst 21 2 2004 73 83 [16] W.B.S. Thrun D. Fox Probabilistic robotics 2005 The MIT Press [17] B. Siciliano O. Khatib Springer handbook of robotics 2008 Springer [18] M. Lauer S. Lange M. Riedmiller Modeling moving objects in a dynamically changing robot application U. Furbach KI 2005: advances in artificial intelligence Lecture notes in computer science vol. 3698 2005 Springer 291 303 [19] Xu Y, Jiang C, Tan Y. SEU-3D 2006 soccer simulation team description. In: CD proc of RoboCup symposium 2006, Bremen, Germany; 2006. [20] Marcelino P, Nunes P, Lima P, Ribeiro MI. Improving object localization through sensor fusion applied to soccer robots. In: Proc scientific meeting of the portuguese robotics open – Robótica 2003, Lisbon, Portugal; 2003. [21] A. Ferrein L. Hermanns G. Lakemeyer Comparing sensor fusion techniques for ball position estimation A. Bredenfeld A. Jacoff I. Noda Y. Takahashi RoboCup 2005: robot soccer world cup IX Lecture notes in computer science vol. 4020 2006 Springer 154 165 [22] M. Lauer S. Lange M. Riedmiller Calculating the perfect match: an efficient and accurate approach for robot self-localization A. Bredenfeld A. Jacoff I. Noda Y. Takahashi RoboCup 2005: robot soccer world cup IX Lecture notes in computer science vol. 4020 2006 Springer 142 153 [23] Neves A, Martins D, Pinho A. A hybrid vision system for soccer robots using radial search lines. In: Lopes LS, Silva F, Santos V, editors. Proc of the 8th conference on autonomous robot systems and competitions, Portuguese robotics open – Robótica 2008, Aveiro, Portugal; 2008. p. 51–5. [24] Bishop G, Welch G. An introduction to the Kalman filter. In: Proc of SIGGRAPH, Course 8, No. NC 27599-3175. NC (USA): Chapel Hill; 2001. [25] Motulsky H, Christopoulos A. Fitting models to biological data using linear and nonlinear regression. GraphPad Software Inc.; 2003. [26] A. Neves G. Corrente A. Pinho An omnidirectional vision system for soccer robots J. Neves M.F. Santos J.M. Machado Progress in artificial intelligence Lecture notes in artificial intelligence vol. 4874 2007 Springer 499 507 [27] B. Cunha J. Azevedo N. Lau L. Almeida Obtaining the inverse distance map from a non-SVP hyperbolic catadioptric robotic vision system U. Visser F. Ribeiro T. Ohashi F. Dellaert RoboCup 2007: robot soccer world cup XI Lecture notes in artificial intelligence vol. 5001 2008 Springer 417 424 "
    },
    {
        "doc_title": "Exploring homology using the concept of three-state entropy vector",
        "doc_scopus_id": "78049460831",
        "doc_doi": "10.1007/978-3-642-16001-1_14",
        "doc_eid": "2-s2.0-78049460831",
        "doc_date": "2010-11-10",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Data models",
            "DNA coding",
            "Encoding proteins",
            "Entropy value",
            "Markov model",
            "State entropy",
            "DNA coding",
            "Encoding proteins",
            "Entropy value",
            "Markov model",
            "State entropy"
        ],
        "doc_abstract": "The three-base periodicity usually found in exons has been used for several purposes, as for example the prediction of potential genes. In this paper, we use a data model, previously proposed for encoding protein-coding regions of DNA sequences, to build signatures capable of supporting the construction of meaningful dendograms. The model relies on the three-base periodicity and provides an estimate of the entropy associated with each of the three bases of the codons. We observe that the three entropy values vary among themselves and also from species to species. Moreover, we provide evidence that this makes it possible to associate a three-state entropy vector with each species, and we show that similar species are characterized by similar three-state entropy vectors. © 2010 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Sensor and information fusion applied to a robotic soccer team",
        "doc_scopus_id": "77950996272",
        "doc_doi": "10.1007/978-3-642-11876-0_32",
        "doc_eid": "2-s2.0-77950996272",
        "doc_date": "2010-04-22",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Adaptive buffer",
            "Information fusion techniques",
            "Localisation",
            "Multi-agent environment",
            "Noise variations",
            "Obstacle detection",
            "Robotic soccer team",
            "Sensor informations",
            "Team cooperation",
            "Team members",
            "Team performance",
            "Velocity estimation",
            "Visual information",
            "Visual sensor",
            "World model"
        ],
        "doc_abstract": "This paper is focused on the sensor and information fusion techniques used by a robotic soccer team. Due to the fact that the sensor information is affected by noise, and taking into account the multi-agent environment, these techniques can significantly improve the accuracy of the robot world model. One of the most important elements of the world model is the robot self-localisation. Here, the team localisation algorithm is presented focusing on the integration of visual and compass information. To improve the ball position and velocity reliability, two different techniques have been developed. A study of the visual sensor noise is presented and, according to this analysis, the resulting noise variation depending on the distance is used to define a Kalman filter for ball position. Moreover, linear regression is used for velocity estimation purposes, both for the ball and the robot. This implementation of linear regression has an adaptive buffer size so that, on hard deviations from the path (detected using the Kalman filter), the regression converges more quickly. A team cooperation method based on sharing of the ball position is presented. Besides the ball, obstacle detection and identification is also an important challenge for cooperation purposes. Detecting the obstacles is ceasing to be enough and identifying which obstacles are team mates and opponents is becoming a need. An approach for this identification is presented, considering the visual information, the known characteristics of the team robots and shared localisation among team members. The same idea of distance dependent noise, studied before, is used to improve this identification. Some of the described work, already implemented before RoboCup2008, improved the team performance, allowing it to achieve the 1st place in the Portuguese robotics open Robótica2008 and in the RoboCup2008 world championship. © 2010 Springer.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Control and monitoring of a robotic soccer team: The base station application",
        "doc_scopus_id": "71049158832",
        "doc_doi": "10.1007/978-3-642-04686-5_25",
        "doc_eid": "2-s2.0-71049158832",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autonomous robot",
            "Base station applications",
            "Control and monitoring",
            "Efficient architecture",
            "Human interference",
            "Robotic soccer",
            "Robotic soccer team",
            "Software applications"
        ],
        "doc_abstract": "In robotic soccer, teams of autonomous robots play soccer according to rules similar to the official FIFA rules. The game is refereed by a human and his orders are communicated to the teams using an application called \"Referee Box\". No human interference is allowed during the games except for removing malfunctioning robots and re-entering robots in the game. The base station, a software application as described in this paper, has a determinant role during the development of a robotic soccer team and also during a game. This application must control the agents interpreting and sending high level instructions, like Start or Stop, and monitor information of the robots, for example the position and velocity, allowing easily to attest the feasibility of the robots behavior. This paper discusses the importance of the control and monitoring of a robotic soccer team, presenting the main challenges and the approaches that were used by the CAMBADA team in the conception of the base station application. As far as we know, no previous work has been published about the study of these important problems and the discussion of an efficient architecture to a base station application. The results obtained by the team confirms the good performance of this software, both during the games and in the development of the team. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Obstacle detection, identification and sharing on a robotic soccer team",
        "doc_scopus_id": "71049141838",
        "doc_doi": "10.1007/978-3-642-04686-5_29",
        "doc_eid": "2-s2.0-71049141838",
        "doc_date": "2009-11-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Apriori",
            "Multi-agent applications",
            "Obstacle detection",
            "Position information",
            "Robotic soccer",
            "Robotic soccer team",
            "Search lines",
            "Visual detection",
            "World model"
        ],
        "doc_abstract": "When building a representation of the environment for a robot in a multi-agent application, as is the case of robotic soccer, sensor and information fusion of several elements of the environment are an important task. To build an increasingly better world model, one of the aspects that one should consider is the treatment of obstacles. This paper gives an insight of the general steps necessary for a good obstacle representation in the robot world model. A first step is the visual detection of the obstacles in the image acquired by the robot. This is done using an algorithm based on radial search lines and colour-based blobs detection, where each obstacle is identified and delimited. After having the visually detected obstacles, a fusion with a-priori known information about the obstacles characteristics allows the obstacle separation and filtering, so that obstacles that don't fill the criteria are discarded. With the position information shared by team mates, the matching of the obstacles and the team mates positions is also possible, thus identifying each of them. Finally, and with the purpose of having a team world model as coherent as possible, the robots are able to share the obstacle information of each other. The work presented in this paper was developed for the CAMBADA robotic soccer team. After achieving the 1st place in the Portuguese robotics open Robótica2008 and in the Robocup2008 world championship, the correct treatment of obstacles was one of the new challenges proposed among the team to improve the performance for the next competitions. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "DNA coding using finite-context models and arithmetic coding",
        "doc_scopus_id": "70349192866",
        "doc_doi": "10.1109/ICASSP.2009.4959928",
        "doc_eid": "2-s2.0-70349192866",
        "doc_date": "2009-09-23",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Arithmetic coding",
            "Coding methods",
            "Context models",
            "DNA basis",
            "DNA coding",
            "DNA encoding",
            "Finite-context modeling",
            "Genomic database",
            "Lossless compression",
            "Low order",
            "Source coding",
            "Standard compression algorithms"
        ],
        "doc_abstract": "The interest in DNA coding has been growing with the availability of extensive genomic databases. Although only two bits are sufficient to encode the four DNA bases, efficient lossless compression methods are still needed due to the size of DNA sequences and because standard compression algorithms do not perform well on DNA sequences. As a result, several specific coding methods have been proposed. Most of these methods are based on searching procedures for finding exact or approximate repeats. Low order finite-context models have only been used as secondary, fall back mechanisms. In this paper, we show that finite-context models can also be used as main DNA encoding methods. We propose a coding method based on two finite-context models that compete for the encoding of data, on a block by block basis. The experimental results confirm the effectiveness of the proposed method. ©2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Variable order finite-context models in DNA sequence coding",
        "doc_scopus_id": "68749102414",
        "doc_doi": "10.1007/978-3-642-02172-5_59",
        "doc_eid": "2-s2.0-68749102414",
        "doc_date": "2009-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Biological research",
            "Coding methods",
            "Context models",
            "DNA structure",
            "Encoded sequences",
            "Predictive function",
            "Side information",
            "Variable order"
        ],
        "doc_abstract": "Being an essential key in biological research, the DNA sequences are often shared between researchers and digitally stored for future use. As these sequences grow in volume, it also grows the need to encode them, thus saving space for more sequences. Besides this, a better coding method corresponds to a better model of the sequence, allowing new insights about the DNA structure. In this paper, we present an algorithm capable of improving the encoding results of algorithms that depend of low-order finite-context models to encode DNA sequences. To do so, we implemented a variable order finite-context model, supported by a predictive function. The proposed algorithm allows using three finite-context models at once without requiring the inclusion of side information in the encoded sequence. Currently, the proposed method shows small improvements in the encoding results when compared with same order finite-context models. However, we also present results showing that there is space for further improvements regarding the use variable order finite-context models for DNA sequence coding. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Autonomous configuration of parameters in robotic digital cameras",
        "doc_scopus_id": "68749090043",
        "doc_doi": "10.1007/978-3-642-02172-5_12",
        "doc_eid": "2-s2.0-68749090043",
        "doc_date": "2009-08-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Autonomous configuration",
            "Camera parameter",
            "Initial configuration",
            "Making decision",
            "Robotic applications",
            "Surrounding environment"
        ],
        "doc_abstract": "In the past few years, the use of digital cameras in robotic applications has been increasing significantly. The main areas of application of these robots are the industry and military, where these cameras are used as sensors that allow the robot to take the relevant information of the surrounding environment and making decisions. To extract information from the acquired image, such as shapes or colors, the configuration of the camera parameters, such as exposure, gain, brightness or white-balance, is very important. In this paper, we propose an algorithm for the autonomous setup of the most important parameters of digital cameras for robotic applications. The proposed algorithm uses the intensity histogram of the images and a black and a white area, known in advance, to estimate the parameters of the camera. We present experimental results that show the effectiveness of our algorithms. The images acquired after calibration show good properties for further processing, independently of the initial configuration of the camera. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossless compression of microarray images using image-dependent finite-context models",
        "doc_scopus_id": "59449096695",
        "doc_doi": "10.1109/TMI.2008.929095",
        "doc_eid": "2-s2.0-59449096695",
        "doc_date": "2009-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Radiological and Ultrasound Technology",
                "area_abbreviation": "HEAL",
                "area_code": "3614"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Arithmetic coding",
            "Finite-context modeling",
            "Image coding standards",
            "Lossless image compression",
            "Microarray images"
        ],
        "doc_abstract": "The use of microarray expression data in state-of-the-art biology has been well established. The widespread adoption of this technology, coupled with the significant volume of data generated per experiment, in the form of images, has led to significant challenges in storage and query retrieval. In this paper, we present a lossless bitplane-based method for efficient compression of microarray images. This method is based on arithmetic coding driven by image-dependent multibitplane finite-context models. It produces an embedded bitstream that allows progressive, lossy-to-lossless decoding. We compare the compression efficiency of the proposed method with three image compression standards (JPEG2000, JPEG-LS, and JBIG) and also with the two most recent specialized methods for microarray image coding. The proposed method gives better results for all images of the test sets and confirms the effectiveness of bitplane-based methods and finite-context modeling for the lossless compression of microarray images. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Progressive lossless compression of medical images",
        "doc_scopus_id": "70349437371",
        "doc_doi": "10.1109/icassp.2009.4959607",
        "doc_eid": "2-s2.0-70349437371",
        "doc_date": "2009-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Compression methods",
            "Context models",
            "Embedded bitstreams",
            "Lossless compression",
            "Lossless image coding",
            "Lossy-to-lossless",
            "Medical image database",
            "Progressive transmission"
        ],
        "doc_abstract": "This paper describes a lossless compression method for medical images that produces an embedded bit-stream, allowing progressive lossy-to-lossless decoding with L-infinity oriented rate-distortion. The experimental results show that the proposed technique produces better average lossless compression results than several other compression methods, including JPEG2000, JPEG-LS and JBIG, in a publicly available medical image database containing images from several modalities. ©2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A bitplane based algorithm for lossless compression of DNA microarray images",
        "doc_scopus_id": "84863764744",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84863764744",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Arithmetic Coding",
            "Bit stream",
            "Bitplane",
            "Compression efficiency",
            "DNA microarray images",
            "Image compression standards",
            "JPEG 2000",
            "JPEG-LS",
            "Lossless",
            "Lossless compression",
            "Lossy-to-lossless",
            "Microarray images",
            "Microarray technologies",
            "Test sets"
        ],
        "doc_abstract": "During the past years, the development of microarray technology has been remarkable, and it is becoming a daily tool in many genomic research laboratories. The widespread adoption of this technology, coupled with the significant volume of data generated per experiment, in the form of images, have led to significant challenges in storage and query-retrieval. In this paper, we present a lossless bitplane based method for efficient compression of microarray images. This method is based on arithmetic coding driven by image-dependent multi-bitplane finite-context models. It produces an embedded bitstream that allows progressive, lossy-to-lossless decoding. We compare the compression efficiency of the proposed method with three image compression standards (JPEG2000, JPEG-LS and JBIG) and also with the two most recent specialized methods for microarray image coding. The proposed method gives better results for all images of the test sets and confirms the effectiveness of bitplane based methods and finite-context modeling for the lossless compression of microarray images. copyright by EURASIP.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inverted-repeats-aware finite-context models for DNA coding",
        "doc_scopus_id": "84863762123",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84863762123",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "DNA coding",
            "Encoding methods",
            "Inverted repeat",
            "Model order",
            "Model updating",
            "Sequence compression"
        ],
        "doc_abstract": "Finite-context models have been used for DNA sequence compression as secondary, fall back mechanisms, the generalized opinion being that models with order larger than two or three are inappropriate. In this paper we show that finite-context models can also be used as the main encoding method, and that they are effective for model orders at least as higher as thirteen. Moreover, we propose a new model updating scheme that takes into account inverted repeats, a common characteristic in DNA sequences. copyright by EURASIP.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Alternative lossless compression algorithms in X-ray cardiac images",
        "doc_scopus_id": "60749130789",
        "doc_doi": null,
        "doc_eid": "2-s2.0-60749130789",
        "doc_date": "2008-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Cardiac images",
            "Digital imaging and communications in medicines",
            "Digital medical images",
            "Digital medical imaging systems",
            "Healthcare institutions",
            "Image compression algorithms",
            "Image datum",
            "Lossless",
            "Lossless compression algorithms",
            "Lossless compressions",
            "Medical datum",
            "Medical professions",
            "Medical studies",
            "Picture archiving",
            "Remote access",
            "Storage costs",
            "Storage efficiencies",
            "Time redundancies",
            "Transmission performance",
            "Transmission speed",
            "Workgroups"
        ],
        "doc_abstract": "Over the last decade, the use of digital medical imaging systems has increased greatly in healthcare institutions. Today, Picture Archiving and Communication System (PACS) is one of the most valuable tools supporting medical profession in both decision making and treatment procedures. It reduced the costs associated with the storage and management of image data and also increased both the intra and inter-institutional portability of data. One of the most important benefits of the digital medical image is that it allows the widespread sharing and remote access to medical data by outside institutions. PACS presents an opportunity to improve cooperative workgroups taking place either within or with other healthcare institutions. Storage and transmissions costs are continuously decreasing, but, as individual digital medical studies become significantly larger, further improvements on transmission performance and on storage efficiency are critical. Image compression algorithms offer the means to reduce storage cost and to increase transmission speed. Following previous methodologies [1], this paper provides a comparison about the application of DICOM (Digital Imaging and Communications in Medicine) lossless compression standards on Angiography cine acquisition images. A new lossless compression approach that exploits time redundancy between successive frames is also presented. Finally, the standards codecs values are compared with results obtained with the proposed method and with video lossless codecs. © 2008 Taylor & Francis Group,.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "L-infinity progressive image compression",
        "doc_scopus_id": "84898065095",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84898065095",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Constrained decoding",
            "Integer arithmetic",
            "L-infinity",
            "Lossless image coding",
            "Operational rate-distortion",
            "Progressive image compression",
            "Progressive transmission",
            "Reconstructed image"
        ],
        "doc_abstract": "This paper presents a lossless image coding approach that produces an embedded bit-stream optimized for L∞-constrained decoding. The decoder is implementable using only integer arithmetic and is able to deduce from the bit-stream the L∞ error that affects the reconstructed image at an arbitrary point of decoding. The lossless coding performance is compared with JPEG-LS and JPEG2000. Operational rate-distortion curves, in the L∞ sense, are presented and compared with JPEG2000.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An omnidirectional vision system for soccer robots",
        "doc_scopus_id": "38349011052",
        "doc_doi": "10.1007/978-3-540-77002-2_42",
        "doc_eid": "2-s2.0-38349011052",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color classification",
            "Image visualization",
            "Object detection",
            "Omnidirectional vision",
            "Soccer robots"
        ],
        "doc_abstract": "This paper describes a complete and efficient vision system developed for the robotic soccer team of the University of Aveiro, CAMBADA (Cooperative Autonomous Mobile roBots with Advanced Distributed Architecture). The system consists on a firewire camera mounted vertically on the top of the robots. A hyperbolic mirror placed above the camera reflects the 360 degrees of the field around the robot. The omnidirectional system is used to find the ball, the goals, detect the presence of obstacles and the white lines, used by our localization algorithm. In this paper we present a set of algorithms to extract efficiently the color information of the acquired images and, in a second phase, extract the information of all objects of interest. Our vision system architecture uses a distributed paradigm where the main tasks, namely image acquisition, color extraction, object detection and image visualization, are separated in several processes that can run at the same time. We developed an efficient color extraction algorithm based on lookup tables and a radial model for object detection. Our participation in the last national robotic contest, ROBOTICA 2007, where we have obtained the first place in the Medium Size League of robotic soccer, shows the effectiveness of our algorithms. Moreover, our experiments show that the system is fast and accurate having a maximum processing time independently of the robot position and the number of objects found in the field. © Springer-Verlag Berlin Heidelberg 2007.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossy-to-lossless compression of images based on binary tree decomposition",
        "doc_scopus_id": "78649882735",
        "doc_doi": "10.1109/ICIP.2006.312812",
        "doc_eid": "2-s2.0-78649882735",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "Arithmetic Coding",
            "Binary tree decomposition",
            "Compression gain",
            "Compression standards",
            "Context-based",
            "JPEG 2000",
            "JPEG-LS",
            "Lossless compression",
            "Lossy to lossless compression",
            "Lossy-to-lossless"
        ],
        "doc_abstract": "A new lossy-to-lossless quality-progressive method for image coding is presented. This method is based on binary tree decomposition and context-based arithmetic coding. Experimental results obtained with the eighteen 8-bit ISO images show that the proposed method attains an average lossless compression gain of 7.9% with respect to JPEG2000. Compared to JPEG-LS, the compression gain is 3.1%, but this compression standard does not allow lossy-to-lossless decoding. ©2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossless compression of microarray images",
        "doc_scopus_id": "48149100015",
        "doc_doi": "10.1109/ICIP.2006.312802",
        "doc_eid": "2-s2.0-48149100015",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            }
        ],
        "doc_keywords": [
            "3D Context Model",
            "Arithmetic Coding",
            "Bio-medical image processing",
            "Image coding standard",
            "JPEG 2000",
            "JPEG-LS",
            "Lossless",
            "Lossless compression",
            "Microarray experiments",
            "Microarray images",
            "Microarray technologies",
            "Progressive decoding",
            "Test sets"
        ],
        "doc_abstract": "Microarray experiments are characterized by a massive amount of data in the form of images. Since the interest in microarray technology is growing nowadays, a large number of microarray images is currently being produced. In this paper, we present a lossless method for efficiently compress microarray images based on arithmetic coding using a 3D context model. Our method produces an embedded bit-stream that allows progressive decoding. We present the compression results using a large set of images and compare these results with three image coding standards, namely, lossless JPEG2000, JBIG and JPEG-LS. We also compare our results with the two most recent specialized methods, using a common set of images. The proposed method gives better results for all images of the test set. ©2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Exploring three-base periodicity for DNA compression and modeling",
        "doc_scopus_id": "33947690256",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33947690256",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Compression gain",
            "DNA model",
            "Finite-context model"
        ],
        "doc_abstract": "To explore the three-base periodicity often found in proteincoding DNA regions, we introduce a DNA model based on three deterministic states, where each state implements a finite-context model. The results obtained show compression gains in relation to the single finite-context model counterpart. Additionally, and potentially more interesting than the compression gain on its own, is the observation that the entropy associated to each of the three states differs and that this variation is not the same among the organisms analyzed. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A bit-plane approach for lossless compression of color-quantized images",
        "doc_scopus_id": "33947662496",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33947662496",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Bit-plane based encoders",
            "Color-quantized images",
            "Lossless compression"
        ],
        "doc_abstract": "Palette reordering is a well-known and effective approach for improving the compression of color-indexed images. The performance of most of the existing palette reordering methods has been evaluated using the compression rates attained by image coders which are based on predictive or transform coding technology, such as JPEG-LS or JPEG2000. In this paper, we show that image coding technology based on bit-plane coding, such as the one used by JBIG, provides a competitive performance in this class of images. Also, we present a palette reordering algorithm, adapted for bit-plane based encoders, that gives good results and is faster than Mention's or the modified Zeng's methods, the two most effective palette reordering algorithms. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A three-state model for DNA protein-coding regions",
        "doc_scopus_id": "33750311852",
        "doc_doi": "10.1109/TBME.2006.879477",
        "doc_eid": "2-s2.0-33750311852",
        "doc_date": "2006-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Compression gain",
            "DNA compression",
            "DNA sequence modeling",
            "Finite-context models"
        ],
        "doc_abstract": "It is known that the protein-coding regions of DNA are usually characterized by a three-base periodicity. In this paper, we exploit this property, studying a DNA model based on three deterministic states, where each state implements a finite-context model. The experimental results obtained confirm the appropriateness of the proposed approach, showing compression gains in relation to the single finite-context model counterpart. Additionally, and potentially more interesting than the compression gain on its own, is the observation that the entropy associated to each of the three base positions of a codon differs and that this variation is not the same among the organisms analyzed. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the relation between Memon's and the modified Zeng's palette reordering methods",
        "doc_scopus_id": "33646866379",
        "doc_doi": "10.1016/j.imavis.2006.02.005",
        "doc_eid": "2-s2.0-33646866379",
        "doc_date": "2006-05-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Color-indexed images",
            "JPEG-LS",
            "JPEG2000",
            "Lossless image compression",
            "Palette recording"
        ],
        "doc_abstract": "Palette reordering has been shown to be a very effective approach for improving the compression of color-indexed images by general purpose continuous-tone image coding techniques. In this paper, we provide a comparison, both theoretical and experimental, of two of these methods: the pairwise merging heuristic proposed by Memon et al. and the recently proposed modification of Zeng's method. This analysis shows how several parts of the algorithms relate and how their performance is affected by some modifications. Moreover, we show that Memon's method can be viewed as an extension of the modified version of Zeng's technique and, therefore, that the modified Zeng's method can be obtained through some simplifications of Memon's method. © 2006 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271526 291210 291718 291872 291874 31 Image and Vision Computing IMAGEVISIONCOMPUTING 2006-04-17 2006-04-17 2010-03-29T03:54:28 S0262-8856(06)00085-0 S0262885606000850 10.1016/j.imavis.2006.02.005 S300 S300.1 FULL-TEXT 2015-05-14T06:05:42.241066-04:00 0 0 20060501 2006 2006-04-17T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast footnotes primabst ref alllist content subj ssids 0262-8856 02628856 24 24 5 5 Volume 24, Issue 5 12 534 540 534 540 20060501 1 May 2006 2006-05-01 2006 article fla Copyright © 2006 Elsevier B.V. All rights reserved. RELATIONBETWEENMEMONSMODIFIEDZENGSPALETTEREORDERINGMETHODS PINHO A 1 Introduction 2 Palette reordering 2.1 Memon's method 2.2 Modified Zeng's method 3 Analysis of the algorithms 4 Experimental results 5 Conclusions Acknowledgements References WEINBERGER 2000 1309 1324 M SKODRAS 2001 36 58 A TAUBMAN 2002 D JPEG2000IMAGECOMPRESSIONFUNDAMENTALSSTANDARDSPRACTICE AUSBECK 2000 1779 1789 P CHEN 2002 904 908 X PINHO 2004 1411 1418 A MEMON 1996 1522 1527 N PINHO 2004 232 234 A PINHOX2006X534 PINHOX2006X534X540 PINHOX2006X534XA PINHOX2006X534X540XA item S0262-8856(06)00085-0 S0262885606000850 10.1016/j.imavis.2006.02.005 271526 2010-10-08T02:41:52.245775-04:00 2006-05-01 true 132019 MAIN 7 69700 849 656 IMAGE-WEB-PDF 1 si47 1176 35 325 si9 885 46 141 si8 133 15 21 si7 125 15 17 si64 1423 46 293 si63 1195 46 222 si62 219 17 50 si61 224 19 51 si60 450 17 118 si6 1092 35 225 si59 407 17 92 si58 323 18 84 si57 331 18 82 si56 255 16 48 si55 240 15 51 si54 416 17 106 si53 95 10 12 si52 104 12 12 si51 970 35 248 si50 741 35 153 si5 1060 36 267 si49 104 13 12 si48 297 19 76 si46 1074 38 266 si45 745 36 172 si44 700 19 226 si43 249 15 71 si42 322 18 84 si41 362 17 85 si40 338 19 84 si4 630 45 102 si39 771 36 162 si38 1846 85 355 si37 633 36 127 si36 1981 93 286 si35 1516 41 355 si34 1444 47 259 si33 531 23 118 si32 104 13 12 si31 104 12 12 si30 104 13 12 si29 994 35 250 si28 972 35 248 si27 886 35 215 si26 798 35 185 si25 104 12 12 si24 503 21 135 si23 739 36 161 si22 297 19 76 si21 104 12 12 si20 1448 125 112 si19 253 19 64 si18 2072 100 179 si17 354 19 84 si16 426 18 125 si15 443 18 128 si14 130 15 21 si13 125 15 17 si12 130 15 21 si11 125 15 17 si10 875 19 266 gr1 3978 68 266 gr1 776 32 125 IMAVIS 2368 S0262-8856(06)00085-0 10.1016/j.imavis.2006.02.005 Elsevier B.V. Fig. 1 Configurations of neighboring pixels, in relation to the pixel in gray, for constructing function w(i, j). Table 1 Comparison of the coding performance using JPEG-LS and lossless JPEG2000 for different constructions of the w(i, j) function Dither Colors Memon mZeng (a) (b) (c) (d) (a) (b) (c) (d) JPEG-LS No 64 2.641 2.660 2.647 2.663 2.709 2.719 2.709 2.727 128 3.441 3.461 3.461 3.469 3.559 3.569 3.552 3.565 256 4.204 4.253 4.225 4.247 4.473 4.482 4.475 4.505 Yes 64 3.341 3.329 3.307 3.327 3.436 3.445 3.420 3.441 128 4.112 4.150 4.122 4.125 4.239 4.226 4.226 4.238 256 4.870 4.890 4.865 4.887 5.107 5.115 5.120 5.138 Global average 3.768 3.791 3.771 3.786 3.921 3.926 3.917 3.936 JPEG2000 No 64 2.981 2.999 2.985 3.002 3.052 3.061 3.046 3.064 128 3.793 3.816 3.812 3.827 3.925 3.930 3.914 3.925 256 4.576 4.617 4.595 4.611 4.868 4.872 4.870 4.899 Yes 64 3.590 3.568 3.553 3.565 3.672 3.677 3.653 3.671 128 4.354 4.386 4.359 4.355 4.481 4.461 4.458 4.470 256 5.149 5.153 5.138 5.146 5.381 5.378 5.388 5.406 Global average 4.074 4.090 4.074 4.084 4.230 4.230 4.222 4.239 The configurations (a)–(d) are depicted in Fig. 1. Table 2 Comparison of the coding performance using JPEG-LS and lossless JPEG2000 for different initializations of Memon's and mZeng's reordering methods Dither Colors Memon MemonZi mZeng mZengMi bpp bpp % bpp bpp % JPEG-LS No 64 2.641 2.642 0.0 2.709 2.744 −1.3 128 3.441 3.443 −0.1 3.552 3.570 −0.5 256 4.204 4.206 −0.1 4.475 4.494 −0.4 Yes 64 3.341 3.341 0.0 3.420 3.438 −0.5 128 4.112 4.117 −0.1 4.226 4.258 −0.8 256 4.870 4.864 0.1 5.120 5.140 −0.4 JPEG2000 No 64 2.981 2.982 0.0 3.046 3.085 −1.3 128 3.793 3.791 0.1 3.914 3.932 −0.5 256 4.576 4.577 0.0 4.870 4.885 −0.3 Yes 64 3.590 3.590 0.0 3.653 3.671 −0.5 128 4.354 4.358 −0.1 4.458 4.499 −0.9 256 5.149 5.144 0.1 5.388 5.417 −0.5 ‘MemonZi’ refers to Memon's method using the initialization of mZeng's method, whereas ‘mZengMi’ refers to mZeng's method using the initialization of Memon's method. Table 3 Comparison of the coding performance using JPEG-LS and lossless JPEG2000 for several modifications of Memon's and mZeng's methods: ‘-Grp” indicates that the selection phase always comprises the largest set and one set of size one; ‘-INS’ indicates that only the merging options defined in (5) are allowed, whereas ‘+INS’ indicates the inclusion of the merging configurations defined in (6) Dither Colors Memon mZeng -GRP -INS -GRP -INS +INS bpp bpp % bpp % bpp % bpp % bpp % JPEG-LS No 64 2.641 2.655 −0.5 2.748 −4.1 2.722 −3.1 2.709 −2.6 2.660 −0.7 128 3.441 3.450 −0.3 3.616 −5.1 3.574 −3.9 3.552 −3.2 3.463 −0.6 256 4.204 4.256 −1.2 4.519 −7.5 4.481 −6.6 4.475 −6.4 4.322 −2.8 Yes 64 3.341 3.338 0.1 3.489 −4.4 3.460 −3.6 3.420 −2.4 3.321 0.6 128 4.112 4.122 −0.2 4.298 −4.5 4.263 −3.7 4.226 −2.8 4.112 0.0 256 4.870 4.951 −1.7 5.138 −5.5 5.130 −5.3 5.120 −5.1 4.926 −1.2 JPEG2000 No 64 2.981 3.000 −0.6 3.096 −3.9 3.068 −2.9 3.046 −2.2 2.996 −0.5 128 3.793 3.808 −0.4 3.995 −5.3 3.945 −4.0 3.914 −3.2 3.817 −0.6 256 4.576 4.643 −1.5 4.923 −7.6 4.872 −6.5 4.870 −6.4 4.711 −2.9 Yes 64 3.590 3.589 0.0 3.734 −4.0 3.700 −3.1 3.653 −1.8 3.565 0.7 128 4.354 4.364 −0.2 4.549 −4.5 4.511 −3.6 4.458 −2.4 4.352 0.0 256 5.149 5.225 −1.5 5.425 −5.4 5.404 −5.0 5.388 −4.6 5.192 −0.8 Percentages are relative to Memon's method. On the relation between Memon's and the modified Zeng's palette reordering methods Armando J. Pinho ⁎ António J.R. Neves Signal Processing Lab, DET/IEETA, University of Aveiro, Campus Universitario de Santiago, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Fax: +351 23 4370545. Palette reordering has been shown to be a very effective approach for improving the compression of color-indexed images by general purpose continuous-tone image coding techniques. In this paper, we provide a comparison, both theoretical and experimental, of two of these methods: the pairwise merging heuristic proposed by Memon et al. and the recently proposed modification of Zeng's method. This analysis shows how several parts of the algorithms relate and how their performance is affected by some modifications. Moreover, we show that Memon's method can be viewed as an extension of the modified version of Zeng's technique and, therefore, that the modified Zeng's method can be obtained through some simplifications of Memon's method. Keywords Color-indexed images Palette recording Lossless image compression JPEG-LS JPEG2000 1 Introduction A color-indexed image is represented by a matrix of indexes (the index image) and by a color-map or palette. Each index points to a color-map entry, establishing the corresponding color of the pixel. For a particular image, the mapping between index values and colors is not unique—it can be arbitrarily permuted, under the condition that the corresponding index image is changed accordingly. However, although equivalent in terms of representation, for most continuous-tone image coding techniques, such as JPEG-LS [1,2] or lossless JPEG2000 [3–5], different mappings may imply dramatic variations in the compression performance. Moreover, despite the existence of specialized approaches for coding color-indexed images (see, for example [6–9]), it remains an important issue to ensure that standard techniques are able to produce acceptable results within this class of images. Palette reordering is a class of preprocessing methods aiming at finding a permutation of the color palette such that the resulting image of indexes is more amenable for compression (for a survey, see [10]). These preprocessing techniques have the advantage of not requiring post-processing and of being cost-less in terms of side information. However, if the optimal configuration is sought, then the computational complexity involved can be high. In fact, the number of possible configurations for a table of M colors corresponds to the number of permutations of M objects, which equals M! Clearly, exhaustive search is impractical for most of the interesting cases, which motivated several sub-optimal, lower complexity, proposals. In this paper, we address two of the most effective palette reordering methods: Memon's method [11] and the modified Zeng's method [12] (for short, we will refer to the modified Zeng's method as mZeng's method). A comparison of the performance and computational complexity of these two methods is presented in [10]. The main objective of this paper is to provide a detailed analysis and comparison of both techniques, showing how they relate, which are their similarities and fundamental differences. A particularly interesting and potentially useful finding is that mZeng's method can be viewed as a simplified version of Memon's method. The remainder of the paper is organized as follows. In Section 2, both reordering methods are described, using an unifying notation, with the aim of exposing similarities and differences between them. In Section 3, a detailed comparison is performed. Section 4 proceeds with the comparison using experimental data. Finally, in Section 5, some conclusions are drawn. 2 Palette reordering Palette reordering methods can be classified into two main classes. To one of those classes, to which we refer as ‘color-based methods’, belong methods that are characterized by relying only on the information provided by the color palette. The other class characterizes techniques relying only on the statistical information conveyed by the index image, independently of its meaning in terms of color representation. We refer to the techniques in this class as ‘index-based methods’. The main idea behind index-based methods for palette reordering is that colors that occur frequently close to each other should have close indexes. Therefore, based on this principle, the assignment of the indexes is usually guided by some function, w(i, j), measuring the number of occurrences corresponding to pixels with index i that are spatially adjacent to pixels with index j, according to some predefined neighborhood. In the remainder of this section we describe two techniques that fall under the class of index-based methods, and which are among the best known palette reordering methods [10]: Memon's method [11] and the modified Zeng's method [12]. 2.1 Memon's method Memon et al. formulated the problem of palette reordering within the framework of linear predictive coding [11]. In that context, the objective is to minimize the zero-order entropy of the prediction residuals. They noticed that, for image data, the prediction residuals are often well modeled by a Laplacian distribution and that, in this case, minimizing the absolute sum of the prediction residuals leads to the minimization of the zero-order entropy of those residuals. For the case of a first-order prediction scheme, the absolute sum of the prediction residuals reduces to (1) ∑ i = 1 M ∑ j = 1 M c i j | i − j | , where c ij denotes the number of times index i is used as the predicted value for a pixel whose color is indexed by j. The problem of finding a palette reordering that minimizes (1) can be formulated as the optimization version of the linear ordering problem (also known as the minimum linear arrangement), whose decision version is known to be NP-complete [11]. In fact, if we consider a complete non-directed weighted graph G(V, E, w), where each vertex in V={v 1, v 2, …,v M }, corresponds to a palette color, and w(v i , v j )=c ij +c ji , (v i , v j )∈E, corresponds to the weight associated to the edge defined between vertices v i and v j , then the goal is to find a one-to-one mapping (permutation) σ: V→{1, 2,…,M}, among all possible permutations σ n , satisfying: (2) σ = arg min σ n ∑ ( v i , v j ) ∈ E w ( v i , v j ) | σ n ( v i ) − σ n ( v j ) | . With the aim of seeking approximate solutions for (2), Memon et al. proposed two heuristics: one based on simulated annealing, the other, faster to compute, based on a technique called ‘pairwise merging’. Essentially, the pairwise merging heuristic consists on repeatedly merging ordered sets of colors until obtaining a single set. Initially, each color (graph vertex) is assigned to a different set. Then, each iteration consists of two steps: Step 1: From all possible pairs of sets, choose the one satisfying: (3) ( S u , S v ) = arg max ( S a , S b ) ∑ a ∈ S a ∑ b ∈ S b w ( a , b ) . Step 2: Among all possible merging combinations of S u and S v , choose the one minimizing (4) ∑ i = 1 | S m | ∑ j > i | S m | ( j − i ) w ( m i , m j ) , where S m = { m 1 , m 2 , … , m | S m | } , | S m | = | S u | + | S v | , is the set under evaluation, obtained from a particular merging of S u and S v . The m i represent, therefore, the elements of sets S u and S v . To alleviate the computational burden involved in selecting the best way of merging the two ordered sets, Memon et al. proposed a reduced number of configurations [11]. If S u = { u 1 , u 2 , … , u | S u | } and S v = { v 1 , v 2 , … , v | S v | } are the two sets under evaluation, and if | S u | , | S v | > 1 , then the following configurations are considered: (5) { u 1 , u 2 , … , u | S u | , v 1 , v 2 , … , v | S v | } { u | S u | , … , u 2 , u 1 , v 1 , v 2 , … , v | S v | } { v 1 , v 2 , … , v | S v | , u 1 , u 2 , … , u | S u | } { v 1 , v 2 , … , v | S v | , u | S u | , … , u 2 , u 1 } . Alternatively, if one of the sets has size one, then the following configurations are tested (without loss of generality, we consider | S u | = 1 ) : (6) { u 1 , v 1 , v 2 , … , v | S v | } { v 1 , u 1 , v 2 , … , v | S v | } { v 1 , v 2 , u 1 , … , v | S v | } ⋮ { v 1 , v 2 , … , v | S v | , u 1 } . 2.2 Modified Zeng's method The palette reindexing method proposed by Zeng et al. [13] is based on an one-step look-ahead greedy approach, which aims at increasing the lossless compression efficiency of color-indexed images. In [12], a modification of Zeng's algorithm was proposed, relying on a Laplacian model for the distribution of first order prediction residuals, and on the assumption that the entropy of the absolute differences between neighboring pixels is a good indicator of the degree of compressibility of an image. The algorithm starts by finding the index that is most frequently located adjacent to other (different) indexes, and the index that is most frequently found adjacent to it. This pair of indexes is the starting base for an ordered set S that will be constructed, one index at a time, during the operation of the reindexing algorithm. We denote by v i the indexes already assigned to the ordered set (i indicates the position of the index in the ordered set and, therefore, its distance to the left side of the set) and by u those still unassigned. Therefore, just before starting the iterations, S = { v 1 , v 2 } , where (7) v 1 = arg max u j ∑ u i ≠ u j w ( u i , u j ) and (8) v 2 = arg max u w ( v 1 , u ) . The function w(i, j)=w(j, i) denotes the number of occurrences (measured on the initial index image) corresponding to pixels with index i that are spatially adjacent to pixels with index j. New indexes can only be attached to the left or to the right extremity of the ordered set. It is well-known that, for a memoryless source, the number of bits required to represent the occurrence of a given symbol s is given by −log2 P(s), where P(s) denotes the probability of occurrence of s. Therefore, we start by defining the estimated code length implied by placing index u on the left side of S (9) l L ( u ) = − ∑ v i ∈ S w ( u , v i ) log 2 P ( i ) , and by placing it one position farther away (10) l L + ( u ) = − ∑ v i ∈ S w ( u , v i ) log 2 P ( i + 1 ) . We also calculate similar estimates for the right side (11) l R ( u ) = − ∑ v i ∈ S w ( u , v i ) log 2 P ( | S | − i + 1 ) , and (12) l R + ( u ) = − ∑ v i ∈ S w ( u , v i ) log 2 P ( | S | − i + 2 ) . According to the greedy strategy of Zeng's algorithm, the next index, u ¯ , that should integrate S is the one that implies the largest increase in code length if its choice is postponed to the next iteration. Then, the new index, u ¯ , should satisfy (13) u ¯ = arg max u ∉ S Δ l ( u ) , with (14) Δ l ( u ) = { l L + ( u ) − l L ( u ) , if l L ( u ) < l R ( u ) l R + ( u ) − l R ( u ) , otherwise . In words, for each candidate index, u, its best position (left or right) is chosen, i.e. the one that minimizes the code length. Then, among all those indexes, we pick the one producing the largest increase in code length if its choice is postponed to the next iteration. Using (9) and (10), we can write (15) l L + ( u ) − l L ( u ) = ∑ v i ∈ S w ( u , v i ) log 2 P ( i ) P ( i + 1 ) = ∑ v i ∈ S α i w ( u , v i ) , if the best position for index u is the left side, or, using (11) and (12) (16) l R + ( u ) − l R ( u ) = ∑ v i ∈ S w ( u , v i ) log 2 P ( | S | − i + 1 ) P ( | S | − i + 2 ) = ∑ v i ∈ S α | S | − i + 1 w ( u , v i ) , if the best position is the right side, where (17) α k = log 2 P ( k ) P ( k + 1 ) , and where P(k) denotes the probability of occurrence of a difference of k units between two neighboring pixels. Moreover, we can also write (18) l R ( u ) − l L ( u ) = ∑ v i ∈ S w ( u , v i ) ( log 2 P ( i ) − log 2 P ( | S | − i + 1 ) ) = ∑ v i ∈ S β i w ( u , v i ) , where (19) β k = log 2 P ( k ) P ( | S | − k + 1 ) . The new ordered set will be { u ¯ , v 1 , … , v | S | } , if l L ( u ¯ ) < l R ( u ¯ ) , or { v 1 , … , v | S | , u ¯ } , otherwise. This iterative process should continue until assigning all indexes. Finally, the re-indexed image is constructed by applying the mapping v i ↦ ( i − 1 ) to all image pixels, and changing the color-map accordingly. For exponentially distributed residuals, i.e. considering (20) P ( k ) = A θ k , 0 < θ < 1 , 0 ≤ k < M , Eq. (17) reduces to (21) α k = log 2 A θ k A θ k + 1 = − log 2 θ , and (19) reduces to (22) β k = log 2 A θ k A θ | S | − k + 1 = ( 2 k − | S | − 1 ) log 2 θ , i.e. the parameter β k decreases linearly with k (note that log2 θ<0), and α k can be set to an arbitrary positive value, e.g. one. Moreover, note that, in this case: (23) Δ l ( u ) = l L + ( u ) − l L ( u ) = l R + ( u ) − l R ( u ) = ∑ v i ∈ S w ( a , v i ) . Summarizing, the mZeng's algorithm is composed of an initialization phase, and two iterative steps: Initialization: Construct the initial set S = { v 1 , v 2 } , as given by (7) and (8). Step 1: Compute the next index, u ¯ , using (24) u ¯ = arg max a ∉ S ∑ v i ∈ S w ( a , v i ) . Step 2: Compute (25) l R ( u ¯ ) − l L ( u ¯ ) = ∑ v i ∈ S ( | S | − 2 i + 1 ) w ( u ¯ , v i ) and choose the left side of S to attach u ¯ if l R ( u ¯ ) − l L ( u ¯ ) > 0. 3 Analysis of the algorithms In Section 2, we presented Memon's and mZeng's techniques for palette reordering. We now proceed with a comparative discussion of both algorithms, with the aim of identifying similarities and differences among them. To do so, we will focus our analysis mainly in two points, corresponding to steps 1 and 2 of the algorithms, and to which we refer as the selection and merging phases, respectively. However, before we address these two points, we will discuss some issues regarding the construction of the w(i, j) function and also regarding the initialization phase of the algorithms. The construction of the w(i, j) function, responsible for conveying the information of how frequently the pairs of neighboring pixels occur in the image, has typically been left somewhat vague by the authors. Memon referred that this function should reflect the number of times a given index i is used as the predicted value of a pixel having index j [11]. However, he did not mention if that hypothetical first-order predictor uses the left pixel, the upper pixel or even the upper-left pixel. Zeng referred that function w(i, j) should represent the number of occurrences of pixels with index i that are spatially adjacent to pixels with index j [13]. However, the type of neighborhood, 4-connected or 8-connected, was not specified. To better understand how a particular choice of this function affects the performance of the reordering algorithms and, therefore, the compression efficiency of the coding methods applied after reordering, we performed some experiments using the four configurations depicted in Fig. 1 . Detailed results are reported in Table 1 , and show that, according to the experiments performed, the best configuration for Memon's method is that presented in Fig. 1(a), whereas for mZeng's method is the one depicted in Fig. 1(c). Regarding the initialization phase, while it is explicit for mZeng's method, defined by (7) and (8), for Memon's technique it coincides with step 1 of the algorithm. In general, the first pair of indexes generated by the two techniques differs, because the most frequent pair, i.e. the one chosen by Memon's method, does not necessarily contain the most frequent index. Once more, it is not evident of how a particular choice of the first pair of indexes might influence the final outcome of the algorithms. To get some indication on this matter, we ran a simple test consisting on permuting the two initialization approaches between the two algorithms (detailed results are given in Table 2 ). The outcome of this experiment was that for Memon's method the effect was virtually null. However, plugging the initialization approach of Memon's method into mZeng's technique produced an average decrease in lossless compression of around 0.6%. The main conclusion that can be drawn from this simple experiment is that Memon's method seems to be more tolerant to the initialization than mZeng's method. In fact, this is not surprising, since whereas the former has the possibility of splitting apart the first pair at a later time, therefore correcting for poor initializations, the latter does not have this flexibility. Step 1, the selection phase, shows some apparently significant differences. As can be observed from the comparison of (3) and (24), whereas (3) seeks the best pair of sets, ( S u , S v ) , among all possible pairs, (24) seeks the best pair, ( { u ¯ } , S ) , but only among the pairs formed by one particular set (which is always the largest) and all the other sets (which are always of size one). Therefore, Memon's method has a greater freedom for picking up the next couple of sets to be merged. In practice, and based on the experimental data set described in Section 4, we found that, on average, the best pair obtained with (3) contains the largest set together with a set of size one for about 80% of the time. In other words, we found that in four out of five iterations, step 1 of Memon's method provides the same output as step 1 of mZeng's method. Step 2, the merging phase, differs considerably in both algorithms. In fact, whereas for mZeng's technique only two possibilities are considered, i.e. (26) { u ¯ , v 1 , … , v | S | } and (27) { v 1 , … , v | S | , u ¯ } , Memon's method considers, most often, max ( | S u | , | S v | ) merging configurations. This claim is related to the observation already pointed out when step 1 was discussed, i.e. that frequently min ( | S u | , | S v | ) = 1 , which forces the algorithm to use the combinations described in (6). Moreover, if those combinations are removed and only those described in (5) are allowed, then the merging phase of both algorithms becomes equivalent in terms of combinations tested (note that, when | S u | = 1 or | S v | = 1 , then only two of the four configurations described in (5) are different, and those coincide with (26) and (27)). Still regarding step 2 of the algorithms, it remains to be shown that (4) and (25) are, in fact, equivalent. We start by noting that, for configuration (26), Eq. (4) can be written as (28) ∑ i = 1 | S | ∑ j > i | S | ( j − i ) w ( v i , v j ) + ∑ i = 1 | S | i w ( u , v i ) and, for configuration (27), it results in (29) ∑ i = 1 | S | ∑ j > i | S | ( j − i ) w ( v i , v j ) + ∑ i = 1 | S | ( | S | − i + 1 ) w ( u , v i ) . Subtracting (28) from (29) we obtain (25), which shows that, for the two configurations represented in (26) and (27), step 2 of both algorithms are equivalent. The main objective of this section was to show that, although developed using different approaches, the algorithms underlying Memon's and mZeng's techniques share some common points. Effectively, despite the fact of both being based on a Laplacian model of the differences among neighboring pixels, they proposed different heuristic algorithms to address the optimization problem. Moreover, the analysis that we performed showed that Memon's method can be viewed as an extension of mZeng's method. In the remainder of this paper we proceed with the comparison of both methods, but now with a more experimental character. We will provide detailed experimental results regarding the already addressed aspects of the construction of the w(i, j) function and the initialization phase. Also, we will present and discuss results obtained after changing some parts of both algorithms, in order to better understand, in practice, their similarities and differences. 4 Experimental results In this section, we present experimental results based on the set of the 23 ‘kodak’ 768×512 true color images. 1 1 These images can be obtained from Color quantization was applied, both with and without Floyd–Steinberg color dithering, creating images with 256, 128 and 64 colors. Image manipulations have been performed using version 1.2.3 of the ‘Gimp’ program. 2 2 The color-quantized images can be obtained from Compression results are given for JPEG-LS 3 3 Using V2.2 of the SPMG JPEG-LS codec with default parameters and for lossless JPEG2000. 4 4 Using the JasPer 1.700.2 JPEG2000 codec with default parameters The compression figures presented in Tables 1–3 , in bits per pixel (bpp), represent average results over the 23 images, and include the sizes of the uncompressed color-maps. The first set of experiments, presented in Table 1, evaluates the impact of each of the four configurations depicted in Fig. 1, used to build the function w(i, j)=w(j, i), on the compressibility of the reordered images. As can be seen, configuration (a) is the one that globally shows the best results for Memon's technique. For mZeng's method, the experimental results indicate configuration (c) as the best. However, the results also show that the gain provided by these configurations is only marginal. Table 2 presents detailed results regarding the initialization phase of both reordering methods. As can be seen, the impact of using mZeng's initialization on Memon's method (denoted by ‘MemonZi’ in the Table 2) is minimal. On the contrary, the effect of using the initialization procedure of Memon's method in mZeng's technique deserves to be considered. In fact, in this case, the average loss in compression performance is around 0.6%, showing a higher sensibility of mZeng's method to the appropriateness of the first pair of indexes that is created. As mentioned in Section 3, this is due to the reduced flexibility of mZeng's method when compared to Memon's method in terms of the merging capabilities. In Table 3, we show how the methods behave when some modifications are performed in what we denoted as steps 1 and 2 of the algorithms. The first column of results in Table 3 is devoted to Memon's method (using its own initialization procedure and the configuration of Fig. 1(a) for the construction of w(i, j)). The percentages included in Table 3 have been calculated in relation to the compression values displayed in the first column. The results presented under label ‘-GRP’ have been generated by suppressing the capability of Memon's method to select an arbitrary pair of sets, limiting the choice only to the largest set and to a size-one set. In other words, this corresponds of using step 1 of mZeng's method. As can been observed, in general, there is a decrease in compression performance, somewhat more accentuated for images with more colors. The results in the column labeled ‘-INS’ have been obtained by suppressing the ability of Memon's method to insert size-one sets into the other set, during the merging phase. This corresponds to restricting the merging combinations to only those in (5), i.e. corresponds to using step 2 of mZeng's method. As can be seen, in this case the decrease in compression performance is much larger than in the previous case, showing that the possibility of interleaving indexes during the merging phase is a major difference between the two methods. The results of a final modification of Memon's method are reported in the column with label ‘-GRP -INS’, where the two modifications just described have been combined. Surprisingly, in this case the results are better than for the ‘-INS’ case. Our conjecture regarding this behavior is that allowing the selection of arbitrary pairs of sets, as defined by step 1 of Memon's method, without allowing breaking some sets in the future, prevents the correction of some poor selections made in previous iterations. Therefore, from the experimental results obtained, it seems that for step 1 of Memon's method to be effective, then step 2 should include the merging configurations of (6). It is worthwhile to note that if the initialization procedure and the w(i, j) function of mZeng's method are used in Memon's method with the ‘-GRP -INS’ modifications then both algorithms become equivalent. Although, we do not have included those results in Table 3, we verified that, in fact, they are equal. Instead, we include in Table 3 the results of modifying mZeng's method through the inclusion of the capability of putting new indexes not only in the extremities of the set, but also inside it (see column ‘+INS’). This corresponds to using step 2 of Memon's method in mZeng's method. As can be seen, this modification provides a considerable improvement. However, this is also the part of the algorithm that imposes a complexity of o(M 4), instead of the o(M 3) complexity of the original mZeng's method [10]. 5 Conclusions In this paper, we provided a detailed description and analysis of two of the most effective palette reordering methods, used for improving the compression of color-indexed images by general purpose continuous-tone lossless image coding techniques. Our objective was to show how these two methods relate and how different parts of their corresponding algorithms contribute to their performance. The main conclusion of this study was that Memon's method can be viewed as an extension of mZeng's method, the latter being included into the former. To achieve this objective, we performed a step by step comparison of both methods using an unifying notation. Moreover, we provided a discussion on issues related to the construction of the function that conveys the neighboring information and also on the effect of the initialization phase. With this work, we believe having contributed to a better understanding of how state-of-the-art techniques for palette reordering work, easing the task of those seeking further improvements. Acknowledgements This work was supported in part by the Fundação para a Ciência e a Tecnologia (FCT). References [1] ISO/IEC, Information technology—Lossless and near-lossless compression of continuous-tone still images, ISO/IEC 14495-1 and ITU Recommendation T.87, 1999. [2] M.J. Weinberger G. Seroussi G. Sapiro The LOCO-I lossless image compression algorithm: principles and standardization into JPEG-LS IEEE Transactions on Image Processing 9 8 2000 1309 1324 [3] ISO/IEC, Information technology—JPEG 2000 image coding system, ISO/IEC International Standard 15444-1, ITU-T Recommendation T.800, 2000. [4] A. Skodras C. Christopoulos T. Ebrahimi The JPEG 2000 still image compression standard IEEE Signal Processing Magazine 18 5 2001 36 58 [5] D.S. Taubman M.W. Marcellin JPEG 2000: Image Compression Fundamentals, Standards and Practice 2002 Kluwer Dordecht MA [6] P.J. Ausbeck Jr. The piecewise constant image model Proceedings of the IEEE 88 11 2000 1779 1789 [7] Y. Yoo, Y.G. Kwon, A. Ortega, Embedded image-domain compression using context models, in: Proceedings of the Sixth IEEE International Conference on Image Processing, ICIP-99, vol. I, Kobe, Japan, October 1999, pp. 477–481. [8] V. Ratnakar, RAPP: Lossless image compression with runs of adaptive pixel patterns, in: Proceedings of the 32nd Asilomar Conference on Signals, Systems, and Computers, 1998, vol. 2, 1998, pp. 1251–1255. [9] X. Chen S. Kwong J.-F. Feng A new compression scheme for color-quantized images IEEE Transactions on Circuits and Systems for Video Technology 12 10 2002 904 908 [10] A.J. Pinho A.J.R. Neves A survey on palette reordering methods for improving the compression of color-indexed images IEEE Transactions on Image Processing 13 11 2004 1411 1418 [11] N.D. Memon A. Venkateswaran On ordering color maps for lossless predictive coding IEEE Transactions on Image Processing 5 5 1996 1522 1527 [12] A.J. Pinho A.J.R. Neves A note on Zeng's technique for color reindexing of palette-based images IEEE Signal Processing Letters 11 2 2004 232 234 [13] W. Zeng, J. Li, S. Lei, An efficient color re-indexing scheme for palette-based compression, in: Proceedings of the Seveth IEEE International Conference on Image Processing, ICIP-2000, vol. III, Vancouver, Canada, September 2000, pp. 476–479. "
    },
    {
        "doc_title": "On the use of standards for microarray lossless image compression",
        "doc_scopus_id": "33344465888",
        "doc_doi": "10.1109/TBME.2005.869782",
        "doc_eid": "2-s2.0-33344465888",
        "doc_date": "2006-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            }
        ],
        "doc_keywords": [
            "Image coding standards",
            "JBIG",
            "JPEG-LS",
            "JPEG2000",
            "Lossless image compression",
            "Lossy-to-lossless compression",
            "Microarray images"
        ],
        "doc_abstract": "The interest in methods that are able to efficiently compress microarray images is relatively new. This is not surprising, since the appearance and fast growth of the technology responsible for producing these images is also quite recent. In this paper, we present a set of compression results obtained with 49 publicly available images, using three image coding standards: lossless JPEG2000, JBIG, and JPEG-LS. We concluded that the compression technology behind JBIG seems to be the one that offers the best combination of compression efficiency and flexibility for microarray image compression. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossless bit-plane compression of microarray images using 3D context models",
        "doc_scopus_id": "84887250729",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84887250729",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Bitplane coding",
            "Context models",
            "Image coding standard",
            "Lossless image compression",
            "Lossy-to-lossless",
            "Microarray images"
        ],
        "doc_abstract": "With the recent growth of interest in microarray technology and the improvement of the technology responsible for producing these images, massive amounts of microarray images are currently being produced. In this paper, we present a new lossless method for efficiently compress microarray images based on arithmetic coding using a 3D context model. Our method produces an embedded bitstream that allows progressive decoding. We present the compression results using 49 publicly available images and we compare these results with three image coding standards: lossless JPEG2000, JBIG and JPEG-LS. The proposed method gives better results for all images of the test set.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A context adaptation model for the compression of images with a reduced number of colors",
        "doc_scopus_id": "33749587629",
        "doc_doi": "10.1109/ICIP.2005.1530161",
        "doc_eid": "2-s2.0-33749587629",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Binary tree representation",
            "Color quantized images",
            "Context based arithmetic coding"
        ],
        "doc_abstract": "Recently, Chen et al. proposed a method for compressing color-quantized images that is based on a binary tree representation of colors and on context-based arithmetic coding with variable size templates. In this paper, we address the problem of context adaptation and we propose a model that provides improvements in the lossless compression capabilities of Chen's method. © 2005 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Palette reordering under an exponential power distribution model of prediction residuals",
        "doc_scopus_id": "20444459122",
        "doc_doi": "10.1109/ICIP.2004.1418800",
        "doc_eid": "2-s2.0-20444459122",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Color-maps",
            "Exponents",
            "Palette recording",
            "Power distribution models"
        ],
        "doc_abstract": "Palette reordering is one of the most effective approaches for improving the compression of color-indexed images. Recently, a theoretically motivated modification of a reordering technique proposed by Zeng et al. was suggested, based on an exponential distribution model of the prediction residuals. In this paper, we develop this theoretical analysis further, exploiting a broader model based on exponential power distributions. ©2004 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Variable size block-based histogram packing for lossless coding of color-quantized images",
        "doc_scopus_id": "11144267772",
        "doc_doi": null,
        "doc_eid": "2-s2.0-11144267772",
        "doc_date": "2004-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Color-quantized images",
            "Histogram packing",
            "JPEG-LS",
            "Lossless image compression"
        ],
        "doc_abstract": "Previous work has shown that the lossless compression of color-quantized images, by general purpose continuoustone encoders, can be improved if histogram packing is performed on a block by block basis, prior to compression. In this paper, we extend this idea, proposing an algorithm that adaptively finds a more appropriate region size for histogram packing. With this new algorithm, a balance between the effectiveness of the histogram packing operation and the overhead spent in representing the mapping tables is automatically sought, providing a further increase in the lossless compression gains.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A survey on palette reordering methods for improving the compression of color-indexed images",
        "doc_scopus_id": "7444260917",
        "doc_doi": "10.1109/TIP.2004.836168",
        "doc_eid": "2-s2.0-7444260917",
        "doc_date": "2004-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Color-indexed images",
            "Joint Photographic Experts Group (JPEG)",
            "Lossless image compression",
            "Palette reordering"
        ],
        "doc_abstract": "Palette reordering is a well-known and very effective approach for improving the compression of color-indexed images. In this paper, we provide a survey of palette reordering methods, and we give experimental results comparing the ability of seven of them in improving the compression efficiency of JPEG-LS and lossless JPEG 2000. We concluded that the pairwise merging heuristic proposed by Memon et al. is the most effective, but also the most computationally demanding. Moreover, we found that the second most effective method is a modified version of Zeng's reordering technique, which was 3%-5% worse than pairwise merging, but much faster. © 2004 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A note on Zeng's technique for color reindexing of palette-based images",
        "doc_scopus_id": "0442295580",
        "doc_doi": "10.1109/LSP.2003.821758",
        "doc_eid": "2-s2.0-0442295580",
        "doc_date": "2004-02-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            }
        ],
        "doc_keywords": [
            "Color indexed image compression",
            "Color reindexing",
            "Lossless image compression",
            "Palette based images"
        ],
        "doc_abstract": "Palette reindexing is a well-known and very effective approach for improving the compression of color-indexed images. In this letter, we address the reindexing technique proposed by Zeng et al. and we show how its performance can be improved through a theoretically motivated choice of parameters. Experimental results show the practical appropriateness of the proposed modification.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lossless compression of color-quantized images using block-based palette reordering",
        "doc_scopus_id": "35048840166",
        "doc_doi": "10.1007/978-3-540-30125-7_35",
        "doc_eid": "2-s2.0-35048840166",
        "doc_date": "2004-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Block packing",
            "Color indexed images",
            "Color-quantized images",
            "Compression gain",
            "Lossless compression",
            "Lossless image coding",
            "Palette reordering",
            "Reordering techniques"
        ],
        "doc_abstract": "It is well-known that the lossless compression of color-indexed images can be improved if a suitable reordering of the palette is performed before encoding the images. In this paper, we show that, if this reordering is made in a block basis, then further compression gains can be attained. Moreover, we show that the use of block-based palette reordering can outperform a previously proposed block packing procedure. Experimental results using a JPEG-LS encoder are presented, showing how different reordering methods behave. © Springer-Verlag 2004.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "JPEG 2000 coding of color-quantized images",
        "doc_scopus_id": "0345566188",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0345566188",
        "doc_date": "2003-12-17",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Color quantization",
            "Histograms"
        ],
        "doc_abstract": "The efficiency of compressing color-quantized images using general purpose lossless image coding methods depends on the degree of smoothness of the index images. A wellknown and very effective approach for increasing smoothness relies on palette reordering techniques. In this paper, we show that these reordering methods may leave some room for further improvements in the compression performance. More precisely, we provide experimental results showing that the JPEG 2000 lossless compression of palette reordered color-quantized natural images can be further improved if histogram packing is applied on a regional basis.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "On the efficiency of luminance-based palette reordering of color-quantized images",
        "doc_scopus_id": "35248849911",
        "doc_doi": "10.1007/978-3-540-44871-6_89",
        "doc_eid": "2-s2.0-35248849911",
        "doc_date": "2003-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Color indexed images",
            "Color-quantized images",
            "Complex methods",
            "Experimental evidence",
            "Natural images",
            "Palette reordering"
        ],
        "doc_abstract": "Luminance-based palette reordering is often considered less efficient than other more complex approaches, in what concerns improving the compression of color-indexed images. In this paper, we provide experimental evidence that, for color-quantized natural images, this may not be always the case. In fact, we show that, for dithered images with 128 colors or more, luminance-based reordering outperforms other more complex methods. © Springer-Verlag Berlin Heidelberg 2003.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Block-based histogram packing of color-quantized images",
        "doc_scopus_id": "11144291958",
        "doc_doi": "10.1109/ICME.2003.1220924",
        "doc_eid": "2-s2.0-11144291958",
        "doc_date": "2003-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Color tables",
            "Color-quantized images",
            "Compression efficiency",
            "Effective approaches",
            "Histogram packing",
            "Indexed image",
            "Natural images",
            "Preprocessing techniques"
        ],
        "doc_abstract": "© 2003 IEEE.Reordering the color table of color-quantized images is a very effective approach towards the improvement of the compression efficiency of color-indexed images. In this paper, we provide experimental results showing that, for images generated by color-quantization of full color natural images, histogram packing applied on a block basis is able to provide additional compression improvements of around 15%. In other words, we show that the improvements provided by a global color table reordering can be further increased by applying a local preprocessing technique which packs the histograms of regions (blocks) of the index image.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improvement of the lossless compression of images with quasi-sparse histograms",
        "doc_scopus_id": "84960901151",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84960901151",
        "doc_date": "2002-03-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Histogram packing",
            "Lossless compression"
        ],
        "doc_abstract": "© 2002 EUSIPCO.Off-line histogram packing is a known method which is capable of producing improvements if applied prior to the lossless compression of images having sparse histograms. However, this technique becomes useless if the image possesses a quasi-sparse histogram, even if it differs from the strictly sparse case only by a minimal margin. In this paper, we present a technique that is able to overcome this drawback and we present results showing its effectiveness.",
        "available": false,
        "clean_text": ""
    }
]