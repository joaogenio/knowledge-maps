[
    {
        "doc_title": "Comparing augmented reality visualization methods for assembly procedures",
        "doc_scopus_id": "85109364932",
        "doc_doi": "10.1007/s10055-021-00557-8",
        "doc_eid": "2-s2.0-85109364932",
        "doc_date": "2022-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Assembly process",
            "Head mounted displays",
            "Industrial scenarios",
            "Physical workloads",
            "Product demand",
            "Reality visualization",
            "User satisfaction",
            "User study"
        ],
        "doc_abstract": "© 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.Assembly processes require now more than ever a systematic way to improve efficiency complying with increasing product demand. Several industrial scenarios have been using augmented reality (AR) to enhance environments with different types of information and influence the overall user satisfaction and performance. The purpose of this work is to evaluate three different AR-based methods that can be used to support users during the execution of assembly procedures. The AR methods evaluated are handheld mobile AR, indirect AR (showing the augmented scene on a monitor) and see-through head-mounted display. A user study was performed to assess performance, mental and physical workload, as well as acceptance of the aforementioned methods. Results from a thirty participants study did not reveal a best method in terms of performance and user preference, showing that all methods are adequate to support users. However, the study highlights the strengths and weaknesses of each method, which may lead to potential advantages in specific use cases.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Human Factors in Industry 4.0 and Lean Information Management: Remodeling the Instructions in a Shop Floor",
        "doc_scopus_id": "85112159986",
        "doc_doi": "10.1007/978-3-030-77750-0_16",
        "doc_eid": "2-s2.0-85112159986",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Daily tasks",
            "Digital tools",
            "Digital transformation",
            "Human capitals",
            "Information flows",
            "Information sharing",
            "Organizational knowledge",
            "Shop floor"
        ],
        "doc_abstract": "© 2021, Springer Nature Switzerland AG.Industry 4.0 or also called the digital paradigm brings with it an environment where information sharing, and agile flows can improve the results of organizations but also create great challenges for operators. The digital paradigm promotes the globalization of human capital, with an impact on worker turnover, as well as on the potential increase in the loss of organizational knowledge, as workers leave companies. Thus, this work, through the combination of the concepts of digital transformation and information management, joint with the techniques of BPM and Lean, clarifies a methodology (using BPMN) for the creation of a repository of organizational knowledge (enhancing instruction modelling work routines), while promoting the introduction of digital tools in daily tasks, placing the operator in the foreground. This digital introduction promotes the elimination of waste connected with Lean Information Management, creating more fluid information flows in the company. The human factor is then valued and new points of connection between the human workforce and digital tools are established, while organizational knowledge is updated, created, and retained.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Characterization of Portuguese haemophilia patients based on the national registry data",
        "doc_scopus_id": "85105678557",
        "doc_doi": "10.1016/j.procs.2021.01.273",
        "doc_eid": "2-s2.0-85105678557",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder",
            "Characterization of PWH",
            "Clinical conditions",
            "Elsevier",
            "Hemophilia",
            "Hemophilia A",
            "National registry data",
            "Patient record",
            "Portugal",
            "Technological solution"
        ],
        "doc_abstract": "© 2021 The Authors. Published by Elsevier B.V.Haemophilia is a chronic and rare congenital bleeding disorder requiring treatment for life. An updated registry of data on this disease is of paramount importance for documenting prevalence, planning care, and evaluating effectiveness of resources in any country. The study presented in this paper is aimed at characterizing Portuguese patients' clinical condition and demography, as well as the replacement therapy used in their treatment, based on hemo@record data, a technological solution that supports the National haemophilia registry. The analysis of 110 patients records confirmed the high prevalence of haemophilia A (75.5%) compared with haemophilia B (11.8%) and a large incidence in the severe levels (or the existence of people with mild severity without diagnosis and, consequently, without proper treatment) promoting insight about the portrayal of haemophilia in Portugal. This study should be extended as the registry evolves fostering an ever more efficient management of resources and ultimately a better quality of care for people with haemophilia.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2021-02-22 2021-02-22 2021-02-22 2021-02-22 2021-03-08T15:55:40 S1877-0509(21)00322-7 S1877050921003227 10.1016/j.procs.2021.01.273 S300 S300.2 HEAD-AND-TAIL 2021-04-28T06:56:37.333898Z 0 0 20210101 20211231 2021 2021-02-22T17:04:27.749831Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 true 181 181 C Volume 181 124 995 1001 995 1001 2021 2021 2021-01-01 2021-12-31 2021 CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020 Maria Manuela Cruz-Cunha Ricardo Martinho Rui Rijo Nuno Mateus-Coelho Dulce Domingos Emanuel Peres article fla © 2021 The Author(s). Published by Elsevier B.V. CHARACTERIZATIONPORTUGUESEHAEMOPHILIAPATIENTSBASEDNATIONALREGISTRYDATA TEIXEIRA L SRIVASTAVA 2013 e1 A EVATT 2005 B COLVIN 2008 361 374 B TEITEL 2004 118 133 J HARA 2017 1 8 J REITTER 2009 13 15 S IORIO 2008 444 453 A AZNAR 2009 1327 1330 J ZDZIARSKA 2011 e189 J VONDERWEID 2012 S20 S24 N HAY 2004 21 25 C TEIXEIRA 2015 1248 1255 L TEIXEIRA 2015 65 80 L TEIXEIRA 2012 56 62 L TEIXEIRA 2017 131 137 L WINDYGA 2006 52 57 J ALI 2012 851 854 T TEIXEIRAX2021X995 TEIXEIRAX2021X995X1001 TEIXEIRAX2021X995XL TEIXEIRAX2021X995X1001XL Full 2021-01-28T00:18:19Z ElsevierWaived OA-Window This is an open access article under the CC BY-NC-ND license. © 2021 The Author(s). Published by Elsevier B.V. 2021-01-27T11:07:30.475Z FCT Fundação para a Ciência e a Tecnologia item S1877-0509(21)00322-7 S1877050921003227 10.1016/j.procs.2021.01.273 280203 2021-04-28T06:56:37.333898Z 2021-01-01 2021-12-31 true 487363 MAIN 7 46555 849 656 IMAGE-WEB-PDF 1 am 189222 ScienceDirect Available online at www.sciencedirect.com Procedia Computer Science 181 (2021) 995â€“1001 1877-0509 Â© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2020 10.1016/j.procs.2021.01.273 10.1016/j.procs.2021.01.273 1877-0509 Â© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2020 Available online at www.sciencedirect.com ScienceDirect Procedia Computer Science 00 (2019) 000â€“000 www.elsevier.com/locate/procedia 1877-0509 Â© 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN â€“ International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2020 Characterization of Portuguese Haemophilia patients based on the National Registry Data Leonor Teixeiraa,c,*, Carlos Ferreiraa,c, Beatriz Sousa Santosb,c a Department of Economics, Management, Industrial Engineering, and Tourism (DEGEIT),University of Aveiro, 3810-193 Aveiro, Portugal b Department of Electronics, Telecommunications and Informatics (DETI), University of Aveiro, 3810-193 Aveiro, Portugal cI Institute of Electronics and Informatics Engineering of Aveiro (IEETA), University of Aveiro, 3810-193 Aveiro, Portugal Abstract Haemophilia is a chronic and rare congenital bleeding disorder requiring treatment for life. An updated registry of data on this disease is of paramount importance for documenting prevalence, planning care, and evaluating effectiveness of resources in any country. The study presented in this paper is aimed at characterizing Portuguese patientsâ€™ clinical condition and demography, as well as the replacement therapy used in their treatment, based on hemo@record data, a technological solution that supports the National haemophilia registry. The analysis of 110 patients records confirmed the high prevalence of haemophilia A (75.5%) compared with haemophilia B (11.8%) and a large incidence in the severe levels (or the existence of people with mild severity without diagnosis and, consequently, without proper treatment) promoting insight about the portrayal of haemophilia in Portugal. This study should be extended as the registry evolves fostering an ever more efficient management of resources and ultimately a better quality of care for people with haemophilia. Â© 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN â€“ International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies *Corresponding author. Tel.: +351-234-370361; fax: +351-234-370215. E-mail address: lteixeira@ua.pt 996 Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 2 Author name / Procedia Computer Science 00 (2019) 000â€“000 Keywords: Haemophilia; National Registry Data; Characterization of PWH; Portugal 1. Introduction Haemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor VIII (FVIII), in haemophilia A (HA) or factor IX (FIX), in haemophilia B (HB) [1]. It has a low incidence, affecting the population in a ratio of one case per 10000 people, and thus it is classified as a rare disease [2]. This disease impairs the body's ability to make blood clots and may have very negative consequences for the patient. One of its main symptoms is joint haemorrhage, which if repeated can cause destruction of the cartilage, affecting the entire bone and soft tissue of the joint structure. Thus, people not properly treated can become disabled, with reduced quality of life. The treatment of haemophilia is complex involving highly specialized skills of multidisciplinary teams in specialized centres, known as Comprehensive Haemophilia Treatment Centres (HTCs) [3]. Usually, comprehensive care includes several health team members (e.g. haematologist, physical therapist, nurse, psychosocial expert and social worker) working in collaboration to minimize the negative effects of haemophilia and, consequently, maximizing patientsâ€™ quality of life [1]. There are different types of congenital bleeding disorder (HA, HB, von Willebrand, etc.); however, HA and HB are the most prevalent and are usually classified according their severity, namely: (i) severe - clotting factor level lower than 1% of normal; (ii) moderate - clotting factor level between 1% and 5%; and (iii) mild - clotting factor level between 6% and 40%. Clotting factor concentrate (CFC) is the drug administrated to HA and HB patients to compensate the missing clotting factor in their body. These CFC, either plasmatic (plasma-derived), or recombinant [1], contribute to life improvement of people with haemophilia (PWH), allowing the treatment at home with a high level of independence. Most people with severe haemophilia having conditions for self-infusion are, in fact, on home-therapy [4]. This provides benefits, improving not only theirs but also their familiesâ€™ life quality decreasing the economic impact of this disease likewise. Haemophilia treatment can be done either at home or in the hospital, and both types may follow different protocols: (i) on-demand treatment - at the time of clinically evident bleeding; and (ii) prophylactic treatment - to prevent blending episodes. Considering the prophylactic treatment, this can be classified as primary, secondary or tertiary prophylaxis [1]. Although prophylactic treatment can lead to better clinical outcomes, improving the quality of life, some literature reports that this type of treatment is more expensive than the â€œon-demandâ€� therapy [5]. It is important to note that the drugs used in these treatments â€“ CFC - are very expensive and fully financed by governmental entities in developed countries. According to a study involving five European countries, the total annual cost of treating severe haemophilia for 2014 was estimated at EUR 1.4 billion, or just under EUR 200,000 per patient [5]. The same study indicated that CFC represents up to 99% of those costs. Given the characteristics of disease and the impact on PHW, their families, and society in general, institutions as the World Federation of Haemophilia (WFH) and the European Haemophilia Consortium (EHC), recommend establishing National Patient Registries (NPR). This type of tool is defined as a nationwide repository system of demographic, social and clinical data for people suffering from a particular disease [2] [3]. According to Colvin et al. [3] the NPR promotes a more assertive analysis of standards of care, and can be used as a tool for characterizing a particular population, to know its conditions (identifying prevalence and incidence of the disease) and to audit clinical services, supporting the development of better quality of care and resource planning. In fact, the literature identifies a set of advantages associated with the existence of this type of tool. Some countries have already implemented their NPR [6]â€“[11], allowing comparative studies. This paper presents a characterization of Portuguese PWH based on the National Patient Registry, named Hemo@record. Hemo@record is a Web-based solution to manage the complex data associated with PWH, including clinical, social and health status monitoring data [12] [13]. Moreover, it aims to determine the prevalence and incidence of this disease comparing with the results reported in an earlier study conducted in Portugal [14], as well as the ones reported by other countries. The remaining of this paper is organized in three sections: section 2 presents the data and methods used in the study, section 3 presents its main results and finally section 4 draws conclusions and presents ideas for future work. Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 997 Author name / Procedia Computer Science 00 (2019) 000â€“000 3 2. Materials and Methods This is a first study using data from the Portuguese NPR to characterize PWH. The data collected includes, among others, socio-demographic information (e.g. age, sex, academic achievements, occupational activity), diagnosis, severity of haemophilia A and B, type of von Willebrand Disease (vWD), presence of inhibitors, virology status, treatment information, consumed CFC. More detail about the types of variables collected on the Portuguese NPR can be founded in [15]. As reported by the WFH, haemophilia affects individuals at a ratio of approximately 1 case per 10 000 people [1]. Considering that Portugal has about 10 million inhabitants, the expected number of PWH in Portugal is approximately 1000 cases. However, registries about identified cases by the Portuguese Association of Haemophilia (PAH) report about 700 cases. Thus, data considered in this study (110 patients), which were introduced by different HTC in the NPR system, represents 16% of PWH in Portugal. Hemo@record data was exported to an Excel sheet and the data analysis was based on simple descriptive statistics. Main results related to: (i) socio-demographic characteristics; (ii) clinical characteristics, and (iii) replacement therapy are presented in the next section. 3. Results and Discussion 3.1. Socio-demographic characteristics of the patients The Portuguese Haemophilia Register has 95.5% of male and 4.5% of female PWH. The age range is 5â€“82 years old; the mean value is 37 years, the median is 35 years and the first and third quartiles are 25 and 50 years old, respectively. As can be seen in table 1, thirteen patients (11.8%) are children (<15 years old); seven patients (6.4%) are young adults (from 15 to 21 years old), and the remaining patients are adults ([22 â€“ 40] 41.8%, [41 - 65] 35.5%) and seniors (> 66 years old) (4.5%). These results are similar to the obtained in a previous survey conducted in Portugal using a questionnaire [14]. Table 1: Distribution of patients by age range Age range Number Per cent <15 13 11,8% 15 - 21 7 6,4% 22 - 40 46 41,8% 41 - 65 39 35,5% >66 5 4,5% Total 110 100% Concerning academic achievements, the majority of PWH have secondary education (30.9%), 16.4% have higher education, 8.2% elementary education, and 3.6% of patients do not have elementary education. The academic achievement was not applied in 15.5% and with no information in 25.5% of the patients (see table 2). Table 2: Distribution of patients by academic achievement Academic achievement Number Per cent Without elementary education 4 3.6 % Elementary education 9 8.2 % Secondary Education 34 30.9 % Higher Education 18 16.4 % n.a 17 15.5 % unknown 28 25.5 % Total 110 100% 998 Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“10014 Author name / Procedia Computer Science 00 (2019) 000â€“000 In terms of occupational activity, 10% of PWH are students, 46.6 % employed, 11.8% retired, and 2.7% are unemployed. This condition was not applied in 6.4% and with no information in 22.75% of the patients (see table 3). Table 3: Distribution of patients by occupational activity Occupational activity Number Per cent Student 11 10.0 % Active 51 46.4 % Retired 13 11.8 % Unemployed 3 2.7 % n.a 7 6.4 % unknown 25 22.7 % Total 110 100% 3.2. Clinical characteristics of the patients Regarding the type of bleeding disorder, 75.5% suffer with haemophilia A (HA), 11.8% with haemophilia B (HB), 8.2% with vWD and 4.5% with other types of coagulopathies (see Table 4). These results were found to be similar to the previous survey conducted in Portugal through a questionnaire [14]. Table 4: Distribution of patients by inherited coagulopathies Type of coagulopathy Number Per cent Haemophilia A 83 75,5 % Haemophilia B 13 11,8 % von Willebrand 9 8,2 % Other coagulopathies 5 4.5 % Taking into account these data, the observed ratio of HA to HB is 6.2:1, very similar to the data reported in the previous study performed in Portugal (6.6:1) [14], as well as in the studies conducted in other countries, such as Spain (6.5:1) [8] and Poland (6.2:1) [16]. Considering HA severity, 66.3% of the patients present a severe level, 7.2 % a moderate one and 24.1% a mild level. 2.4 % of the patients have no reference related with severity of the disease. In HB, usually not as frequent as HA, 76.9 % present a severe level, 15.4 % a moderate one, and for 7.7% there is no information about severity. Table 5 presents details of the disease severity in both cases, showing a larger incidence of the severe levels. An additional analysis of the data base reveals the existence of patients with mild severity without diagnosis and, consequently, without proper treatment. Table 5 â€“ distribution of patients by severity of ha and HB. Type Severe (<1%) n (%) Moderate (1-5%) n (%) Mild (5-40%) n (%) HA (n=83) 55 (66.3%) 6 (7.2%) 20 (24.1%) HB (n=13) 10 (76.9%) 2 (15.4%) 0 (0%) Inhibitors were detected in 13 (11.8%) patients. The frequency of inhibitor development was 12 in HA patients (11 severe and one moderate) and one in HB patients (severe). The prevalence of inhibitors in HA and HB is in line with the results regarding another country (ex: [17]). Concerning the virology status information, twenty per cent (20%) of the PWH were infected with HCV (Hepatitis C Virus), and about eleven per cent (10.9%) with HIV (Human Immunodeficiency Virus). Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 999 Author name / Procedia Computer Science 00 (2019) 000â€“000 5 3.3. Replacement therapy of the patients Concerning the replacement of the clotting factor in deficit, the registry data indicate that 88.2% (97) of PWH do replacement treatments; of those patients 60,8% (59) are under home treatment (see table 6). Table 6: Distribution of patients with replacement treatment Treatment HA HB dVW Other C. Treatment with clotting factor (n=97) 73 ( 73.2%) 12 (12.4 %) 9 (9.3 %) 5 (5.2%) Home-treatment (n=59) 48 (81.4 %) 9 (15.3%) 2 (3.4%) -- From 97 patients that do replacement treatment, for 94 patients (69 with HA, 12 with HB, 8 with dVW, and 5 with other type of coagulopathies) the mode of substitution or the type of therapy was identified. As can be seen in table 7, among those 94 patients, 51 patients are receiving on-demand treatment, and 43 are receiving prophylactic treatment. Table 7 â€“ Distribution of patients by type of therapy Type of therapy HA HB dVW Other C. On-demand (n=51) 34 ( 66.7%) 8 (15.7 %) 6 (11.8%) 3 (5.9%) Prophylactic (n=43) 35 (81.4%) 4 (9.3%) 2 (4.7%) 2 (4.7%) Table 8 reports the number of patients on-demand and prophylactic treatments considering the type of coagulopathy (HA and HB) and disease severity. The results reveal that, under prophylactic therapy, there are more cases of severe HA and HB, than moderate or mild. Patients with mild severity, usually are under on-demand therapy. Table 8 â€“ Details of distribution of patients by type of therapy. Severe Moderate Mild On-demand (n=42) HA (n=34) 16 (47.1%) 2(5.9 %) 16 (47.1%) HB (n=8) 7 (87.5%) 1 (12.5%) -- Prophylactic (n=39) HA (n=35) 35 (100%) -- -- HB (n=4) 3 (75%) 1 (25%) -- From 97 patients who make treatment based on the replacement of clotting factor, recombinant coagulation factor was administered to 39 patients, plasmatic factor to 38, and 12 patients received both types of factor products (see table 6). For 8 patients no data are available. Table 9 - Distribution of patients by type of clotting factor used. Type of CFC HA HB dVW Other C. Recombinant (n=39) 33 (37.1%) 3 (3.4 %) -- 3 (3.4%) Plasmatic (n=38) 21 (23.6%) 7 (7.9%) 8 (9%) 2 (2.2%) Both types (n=12) 10 (10.2%) 2 (2.2%) -- -- 1000 Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 6 Author name / Procedia Computer Science 00 (2019) 000â€“000 4. Concluding remarks This study analysed 110 records of PWH registered on the Portuguese NPR, 95.5% males and 4.5% females; the results confirmed the high prevalence of haemophilia A (75.5%) compared with haemophilia B (11.8%), as well as a large incidence of severe levels. The results also revealed the presence of inhibitors in 11.8% of patients with a higher incidence in severe HA. These results are comparable to the findings of other similar studies conducted in other countries, as well as a previous study conducted in Portugal using a questionnaire to collect data. Concerning the treatment based on replacement of the CFC, the results revealed that 88.2% of PWH make this kind of treatment (54% following an on-demand protocol, and the remaining 46% on prophylactic treatment), and of those, 60.8% are under home treatment. The recombinant CFC is used in 43% of PWH, plasmatic factor in 44% and both types of CFC in 13% of the cases. This study also confirmed the effortlessness in characterizing the incidence and prevalence of a disease using data collected automatically from a national registry system. As future work, we expect to increase the number of PWH in the Portuguese NPR and, consequently, to carry out a more comprehensive characterization study including a larger and more representative sample. In fact, the results obtained from any NPR in this context promote insight about the portrayal of haemophilia in a country, which allows a more efficient management of resources and a better quality of care. Acknowledgement This work was supported by Portuguese funds through the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) and Foundation for Science and Technology, in the context of the project UIDB/00127/2020. References [1] A. Srivastava, A. K. Brewer, E. P. Mauser-Bunschoten, N. S. Key, S. Kitchen, A. Llinas, C. Ludlam, J. N. Mahlangu, K. Mulder, M. C. Poon, and A. Street, â€œGuidelines for the management of hemophiliaâ€� Haemophilia, vol. 19, no. 1, pp. e1-47, 2013. [2] B. Evatt, â€œWorld Federation of Hemophilia Guide to Developing a National Patient Registryâ€� MontrÃ©al - QuÃ©bec, 2005. [3] B. T. Colvin, J. Astermark, K. Fischer, A. Gringeri, R. Lassila, W. Schramm, A. Thomas, and J. Ingerslev, â€œEuropean principles of haemophilia careâ€� Haemophilia, vol. 14, no. 2, pp. 361â€“74, 2008. [4] J. M. Teitel, D. Barnard, S. Israels, D. Lillicrap, M. Poon, and J. Sek, â€œHome management of haemophiliaâ€� Haemophilia, vol. 10, no. 2, pp. 118â€“133, 2004. [5] J. O. Hara, D. Hughes, C. Camp, T. Burke, L. Carroll, and D. G. Diego, â€œThe cost of severe haemophilia in Europeâ€¯: the CHESS studyâ€� Orphanet J. Rare Dis., vol. 12, no. 106, pp. 1â€“8, 2017. [6] S. Reitter, R. Sturn, W. Streif, T. Schabetsberger, F. Wozak, C. Male, W. Muntean, and I. Pabinger, â€œAustrian Haemophilia Registryâ€� Hamostaseologie, vol. 29, no. 41, pp. 13â€“15, 2009. [7] A. Iorio, E. Oliovecchio, M. Morfini, and P. M. Mannucci, â€œItalian Registry of Haemophilia and Allied Disorders. Objectives, methodology and data analysisâ€� Haemophilia, vol. 14, no. 3, pp. 444â€“53, May 2008. [8] J. A. Aznar, L. Abad-Franch, V. R. Cortina, and P. Marco, â€œThe national registry of haemophilia A and B in Spain: results from a census of patientsâ€� Haemophilia, vol. 15, no. 6, pp. 1327â€“30, Nov. 2009. [9] J. Zdziarska, K. Chojnowski, A. Klukowska, M. Å�Ä™towska, A. Mital, J. MusiaÅ‚, M. Podolak-Dawidziak, J. Windyga, P. Ovesna, P. Brabec, and K. Zawilska, â€œRegistry of inherited bleeding disorders in Poland-current status and potential role of the HemoRec databaseâ€� Haemophilia, vol. 17, no. 1, pp. e189-95, 2011. [10] N. von der Weid, â€œHaemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010â€“2011â€� Hamostaseologie, vol. 32, no. 41, pp. S20â€“S24, 2012. [11] C. Hay, â€œThe UK Haemophilia database: a tool for research, audit and healthcare planning,â€� Haemophilia, vol. 10, no. S3, pp. 21â€“25, 2004. [12] L. Teixeira, V. Saavedra, J. P. SimÃµes, B. Sousa Santos, and C. Ferreira, â€œThe Portuguese National Registry for Hemophilia: Developing of a Web-based Technological Solutionâ€� Procedia Comput. Sci., vol. 64, pp. 1248â€“1255, 2015. [13] L. Teixeira, V. Saavedra, C. Ferreira, and B. Sousa Santos, â€œWeb Platform to Support the Portuguese National Registry of Haemophilia and Other Inherited Blood Disordersâ€� Int. J. Web Portals, vol. 7, no.1, pp. 65â€“80, 2015. [14] L. Teixeira, C. Ferreira, V. Saavedra, B. Sousa Santos, â€œWeb-enabled registry of inherited bleeding disorders in Portugal: conditions and perception of the patientsâ€� Haemophilia, vol. 18, no. 1, pp. 56â€“62, 2012. [15] L. Teixeira, V. Saavedra, B. Sousa Santos, C. Ferreira, â€œPortuguese Haemophilia Registry. Set of variables for a computerized solutionâ€� Hamostaseologie, vol. 37, no. 2, pp. 131â€“137, 2017. Leonor Teixeira et al. / Procedia Computer Science 181 (2021) 995â€“1001 1001 Author name / Procedia Computer Science 00 (2019) 000â€“000 7 [16] J. Windyga, S. Lopaciuk, E. Stefanska, A. Juszynski, D. Wozniak, and O. Strzelecki, â€œHaemophilia in Polandâ€� Haemophilia, vol. 12, no. 1, pp. 52â€“57, 2006. [17] T. Ali and J. F. Schved, â€œRegistry of hemophilia and other bleeding disorders in Syria.,â€� Haemophilia, vol. 18, no. 6, pp. 851â€“4, 2012. tive sample. In fact, the results obtained from any NPR in this context promote insight about the portrayal of haemophilia in a country, which allows a more efficient management of resources and a better quality of care. Acknowledgement This work was supported by Portuguese funds through the Institute of Electronics and Informatics Engineering of Aveiro (IEETA) and Foundation for Science and Technology, in the context of the project UIDB/00127/2020. References [1] A. Srivastava, A. K. Brewer, E. P. Mauser-Bunschoten, N. S. Key, S. Kitchen, A. Llinas, C. Ludlam, J. N. Mahlangu, K. Mulder, M. C. Poon, and A. Street, â€œGuidelines for the management of hemophiliaâ€� Haemophilia, vol. 19, no. 1, pp. e1-47, 2013. [2] B. Evatt, â€œWorld Federation of Hemophilia Guide to Developing a National Patient Registryâ€� MontrÃ©al - QuÃ©bec, 2005. [3] B. T. Colvin, J. Astermark, K. Fischer, A. Gringeri, R. Lassila, W. Schramm, A. Thomas, and J. Ingerslev, â€œEuropean principles of haemophilia careâ€� Haemophilia, vol. 14, no. 2, pp. 361â€“74, 2008. [4] J. M. Teitel, D. Barnard, S. Israels, D. Lillicrap, M. Poon, and J. Sek, â€œHome management of haemophiliaâ€� Haemophilia, vol. 10, no. 2, pp. 118â€“133, 2004. [5] J. O. Hara, D. Hughes, C. Camp, T. Burke, L. Carroll, and D. G. Diego, â€œThe cost of severe haemophilia in Europeâ€¯: the CHESS studyâ€� Orphanet J. Rare Dis., vol. 12, no. 106, pp. 1â€“8, 2017. [6] S. Reitter, R. Sturn, W. Streif, T. Schabetsberger, F. Wozak, C. Male, W. Muntean, and I. Pabinger, â€œAustrian Haemophilia Registryâ€� Hamostaseologie, vol. 29, no. 41, pp. 13â€“15, 2009. [7] A. Iorio, E. Oliovecchio, M. Morfini, and P. M. Mannucci, â€œItalian Registry of Haemophilia PROCS 39844 S1877-0509(21)00322-7 10.1016/j.procs.2021.01.273 Characterization of Portuguese Haemophilia patients based on the National Registry Data Leonor Teixeira a c Carlos Ferreira a c Beatriz Sousa Santos b c a Department of Economics, Management, Industrial Engineering, and Tourism (DEGEIT),University of Aveiro, 3810-193 Aveiro, Portugal Department of Economics, Management, Industrial Engineering, and Tourism (DEGEIT) University of Aveiro Aveiro 3810-193 Portugal b Department of Electronics, Telecommunications and Informatics (DETI), University of Aveiro, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunications and Informatics (DETI) University of Aveiro Aveiro 3810-193 Portugal c I Institute of Electronics and Informatics Engineering of Aveiro (IEETA), University of Aveiro, 3810-193 Aveiro, Portugal I Institute of Electronics and Informatics Engineering of Aveiro (IEETA) University of Aveiro Aveiro 3810-193 Portugal Haemophilia is a chronic and rare congenital bleeding disorder requiring treatment for life. An updated registry of data on this disease is of paramount importance for documenting prevalence, planning care, and evaluating effectiveness of resources in any country. The study presented in this paper is aimed at characterizing Portuguese patients’ clinical condition and demography, as well as the replacement therapy used in their treatment, based on hemo@record data, a technological solution that supports the National haemophilia registry. The analysis of 110 patients records confirmed the high prevalence of haemophilia A (75.5%) compared with haemophilia B (11.8%) and a large incidence in the severe levels (or the existence of people with mild severity without diagnosis and, consequently, without proper treatment) promoting insight about the portrayal of haemophilia in Portugal. This study should be extended as the registry evolves fostering an ever more efficient management of resources and ultimately a better quality of care for people with haemophilia. Keywords Haemophilia National Registry Data Characterization of PWH Portugal References 1 A. Srivastava A.K. Brewer E.P. Mauser-Bunschoten N.S. Key S. Kitchen A. Llinas C. Ludlam J.N. Mahlangu K. Mulder M.C. Poon A. Street “Guidelines for the management of hemophilia” Haemophilia 19 1 2013 e1 2 B. Evatt “World Federation of Hemophilia Guide to Developing a National Patient Registry” Montréal - Québec 2005 3 B.T. Colvin J. Astermark K. Fischer A. Gringeri R. Lassila W. Schramm A. Thomas J. Ingerslev “European principles of haemophilia care” Haemophilia 14 2 2008 361 374 4 J.M. Teitel D. Barnard S. Israels D. Lillicrap M. Poon J. Sek “Home management of haemophilia” Haemophilia 10 2 2004 118 133 5 J.O. Hara D. Hughes C. Camp T. Burke L. Carroll D.G. Diego “The cost of severe haemophilia in Europe: the CHESS study” Orphanet J. Rare Dis. 12 106 2017 1 8 6 S. Reitter R. Sturn W. Streif T. Schabetsberger F. Wozak C. Male W. Muntean I. Pabinger “Austrian Haemophilia Registry” Hamostaseologie 29 41 2009 13 15 7 A. Iorio E. Oliovecchio M. Morfini P.M. Mannucci “Italian Registry of Haemophilia and Allied Disorders. Objectives, methodology and data analysis” Haemophilia 14 3 2008 444 453 May 8 J.A. Aznar L. Abad-Franch V.R. Cortina P. Marco “The national registry of haemophilia A and B in Spain: results from a census of patients” Haemophilia 15 6 2009 1327 1330 Nov. 9 J. Zdziarska K. Chojnowski A. Klukowska M. Łętowska A. Mital J. Musiał M. Podolak-Dawidziak J. Windyga P. Ovesna P. Brabec K. Zawilska “Registry of inherited bleeding disorders in Poland-current status and potential role of the HemoRec database” Haemophilia 17 1 2011 e189 10 N. von der Weid “Haemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010–2011” Hamostaseologie 32 41 2012 S20 S24 11 C. Hay “The UK Haemophilia database: a tool for research, audit and healthcare planning,” Haemophilia 10 S3 2004 21 25 12 L. Teixeira V. Saavedra J.P. Simões B. Sousa Santos C. Ferreira “The Portuguese National Registry for Hemophilia: Developing of a Web-based Technological Solution” Procedia Comput. Sci. 64 2015 1248 1255 13 L. Teixeira V. Saavedra C. Ferreira B. Sousa Santos “Web Platform to Support the Portuguese National Registry of Haemophilia and Other Inherited Blood Disorders” Int. J. Web Portals 7 1 2015 65 80 14 L. Teixeira C. Ferreira V. Saavedra B. Sousa Santos “Web-enabled registry of inherited bleeding disorders in Portugal: conditions and perception of the patients” Haemophilia 18 1 2012 56 62 15 L. Teixeira V. Saavedra B. Sousa Santos C. Ferreira “Portuguese Haemophilia Registry. Set of variables for a computerized solution” Hamostaseologie 37 2 2017 131 137 Author name / Procedia Computer Science 00 (2019) 000-000 16 J. Windyga S. Lopaciuk E. Stefanska A. Juszynski D. Wozniak O. Strzelecki “Haemophilia in Poland” Haemophilia 12 1 2006 52 57 17 T. Ali J.F. Schved “Registry of hemophilia and other bleeding disorders in Syria.,” Haemophilia 18 6 2012 851 854 "
    },
    {
        "doc_title": "People-centered benchmarking of smart school ecosystems: A study with young students from aveiro region",
        "doc_scopus_id": "85091537784",
        "doc_doi": "10.1007/978-981-15-7383-5_11",
        "doc_eid": "2-s2.0-85091537784",
        "doc_date": "2021-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Decision Sciences (all)",
                "area_abbreviation": "DECI",
                "area_code": "1800"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Benchmarking process",
            "Degrees of complexity",
            "Descriptive statistics",
            "Holistic perspectives",
            "Methodological approach",
            "Non-parametric test",
            "Open-ended questions",
            "Relational analysis"
        ],
        "doc_abstract": "© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2021.This paper proposes a smartness benchmarking process capable of comparing the smartness dimensions in different school ecosystems in each stakeholder’s perspective. The process uses a mixed-method approach supported by ASLERD smartness questionnaires used to gather stakeholder opinions with closed and open questions organized into eight different smartness dimensions. All these dimensions inform stakeholder’s motivations and needs considering their relationship with the ecosystem’s territory, its institutions, and the people that share it. Quantitative data from closed questions is used as a valorization indicator of related opinions in open-ended questions. The methodological approach adopted for the benchmarking process flows as a result of the iterative relational analysis of these three modules: (i) descriptive statistics (opinion valorization) calculation; (ii) statistical deeper probing with nonparametric tests; and (iii) qualitative pertinence processing and clustering of issues/problems per smartness dimension. The benchmarking process was tested with three cohorts of seventh-ninth grade students from three different schools in the Aveiro region, Portugal. The data was gathered in May/June 2018 at José Estêvão school (n1 = 156), School No. 2 at São Bernardo (n2 = 60) and at Estarreja school (n3 = 81). Results contain evidence of process validation as a multilevel inspection benchmarking solution and reveal that the process can validate the affordance of the questionnaires. Results depict a misinterpretation of one of the questions in two different data processing phases, situation that was validated with the relational analysis of this data. The mixed-method methodological approach produces different results, shown in this paper with different degrees of complexity, from a holistic perspective to a detailed clustered subjective opinion of a cohort’s specific population.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Interaction with Virtual Content using Augmented Reality: A User Study in Assembly Procedures",
        "doc_scopus_id": "85095832791",
        "doc_doi": "10.1145/3427324",
        "doc_eid": "2-s2.0-85095832791",
        "doc_date": "2020-11-04",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Social Sciences (miscellaneous)",
                "area_abbreviation": "SOCI",
                "area_code": "3301"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "Assembly tasks",
            "Controlled experiment",
            "Interaction design",
            "Interaction methods",
            "Lego block",
            "Limited resolution",
            "Manipulation of virtual objects",
            "See-through HMD"
        ],
        "doc_abstract": "© 2020 ACM.Assembly procedures are a common task in several domains of application. Augmented Reality (AR) has been considered as having great potential in assisting users while performing such tasks. However, poor interaction design and lack of studies, often results in complex and hard to use AR systems. This paper considers three different interaction methods for assembly procedures (Touch gestures in a mobile device; Mobile Device movements; 3D Controllers and See-through HMD). It also describes a controlled experiment aimed at comparing acceptance and usability between these methods in an assembly task using Lego blocks. The main conclusions are that participants were faster using the 3D controllers and Video see-through HMD. Participants also preferred the HMD condition, even though some reported light symptoms of nausea, sickness and/or disorientation, probably due to limited resolution of the HMD cameras used in the video see-through setting and some latency issues. In addition, although some research claims that manipulation of virtual objects with movements of the mobile device can be considered as natural, this condition was the least preferred by the participants.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Lean thinking across the company: Successful cases in the manufacturing industry",
        "doc_scopus_id": "85077836923",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85077836923",
        "doc_date": "2019-01-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2019 Nova Science Publishers, Inc.Lean thinking is a well-known and well-established management philosophy in the production environment. Having had significant success in reducing wastes, its principles and tools can now be found applied in several industrial sectors with good results. Outside the production environment, however, literature is scarce concerning its applicability and outcome. This work presents several case studies, of companies of different sectors, where both inside and outside production areas wastes were identified and acted upon, namely, in Logistics and Production, Information Flow, and Ergonomics. Results suggest that Lean can be equally applicable and beneficial in these areas, potentially making it a viable philosophy across the company.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Investigating different augmented reality approaches in circuit assembly: A user study",
        "doc_scopus_id": "85072281241",
        "doc_doi": "10.2312/egs.20191011",
        "doc_eid": "2-s2.0-85072281241",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Circuit assembly",
            "Complex task",
            "Controlled experiment",
            "Multi-devices",
            "Traditional approaches",
            "User study"
        ],
        "doc_abstract": "© 2019 The Eurographics Association.Augmented Reality (AR) has been considered as having great potential in assisting performance and training of complex tasks. Assembling electronic circuits is such a task, since many errors may occur, as wrong choice or positioning of components or incorrect wiring and thus using AR approaches may be beneficial. This paper describes a controlled experiment aimed at comparing usability and acceptance of two AR-based approaches (one based on a single device and another approach using two interconnected devices), with a traditional approach using a paper manual in the assembly of an electronic circuit. Participants were significantly faster and made fewer errors while using the AR approaches, and most preferred the multi-device approach.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "National Patient Registry: A Web-based Technological Solution for Haemophilia in Portugal",
        "doc_scopus_id": "85055827923",
        "doc_doi": "10.1007/978-3-030-02053-8_148",
        "doc_eid": "2-s2.0-85055827923",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Data centres",
            "Haemophilia",
            "National patient registry",
            "Rare disease",
            "Technological solution",
            "WEB application",
            "Web based",
            "Web-based solutions"
        ],
        "doc_abstract": "© 2019, Springer Nature Switzerland AG.Several national and international entities have recognized the importance of national patient registry systems, as well as their benefits in the treatment of chronic and rare diseases, such as haemophilia. Despite this recognition, due to the lack of motivation to invest in systems that benefit a small proportion of the population – characteristic of rare diseases – there are several barriers to create this type of application. This paper describes a Web-based solution to support the national haemophilia registry in Portugal, the first created in this country for this type of chronic and rare disease. Currently, the technological solution is already developed, tested, and installed at the data centre of a Portuguese University, and clinicians of different Haemophilia Treatment Center around the country have access to the operational version, via web.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An Information Management Framework to Industry 4.0: A Lean Thinking Approach",
        "doc_scopus_id": "85055777410",
        "doc_doi": "10.1007/978-3-030-02053-8_162",
        "doc_eid": "2-s2.0-85055777410",
        "doc_date": "2019-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Added values",
            "Information and Communication Technologies",
            "Information flows",
            "Information management framework",
            "Information process",
            "Lean thinking",
            "Production environments",
            "Unstructured data"
        ],
        "doc_abstract": "© 2019, Springer Nature Switzerland AG.Industry 4.0 is driven by modern information and communication technologies and is associated with smart factories that promote the digital industry with rapid production environments. Despite the benefits expected from the Industry 4.0, the daily volume of unstructured data in that environment may generate noise, as isolated this data do not provide any added value. In this context, Lean Thinking applied to information processes can minimize such waste, helping in the definition of information flows, as well as on the selection of relevant and value-added data. The present paper aims to analyse the relationship between Lean Thinking and Industry 4.0, and propose a framework that promotes the understanding of this relationship, as well as its impact on the information management processes.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluating and enhancing google tango localization in indoor environments using fiducial markers",
        "doc_scopus_id": "85048857208",
        "doc_doi": "10.1109/ICARSC.2018.8374174",
        "doc_eid": "2-s2.0-85048857208",
        "doc_date": "2018-06-06",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            },
            {
                "area_name": "Mechanical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2210"
            },
            {
                "area_name": "Control and Optimization",
                "area_abbreviation": "MATH",
                "area_code": "2606"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018 IEEE.Recent advances in 3D sensing technologies, as well as in inertial measurement technologies, have resulted in significant improvements in the accuracy of the localization of systems that combine all these sensors. Project Tango is one of the most successful examples of such systems. Developed by Google, it integrates in an Android mobile device a set of sensors and software required to provide accurate real-time 3D information when moving the equipment freely in hand. This is making mapping and navigation accessible to the general public, with evident applications in robotics, augmented reality, computer vision and others. The contribution of this paper is towfold: first, we present a thorough evaluation of the localization accuracy of the Tango platform in different conditions; second, we present a fiducial marker-based extension of the Tango localization system, which improves the localization estimates in certain conditions. The paper presents a set of experiments performed to evaluate the position and orientation errors in indoor environments, using Augmented Reality for visualization purposes, with and without area learning, e.g. using a priori information acquired from the environment. In addition, we propose a solution based on the use of additional visual markers, which allows the re-calibration of augmented content in specific locations, to improve tracking accuracy in dynamic environments where spatial and/or illumination changes may occur. A statistical analysis of the results shows that the Tango with area learning and the proposed solution provide a level of accuracy significantly better that the Tango without area learning. Moreover, the proposed solution can overcome some limitations of Tango with area learning when used in dynamic environments.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The perception of tax complexity within local authorities' VAT framework: Evidence from Portugal",
        "doc_scopus_id": "85047335261",
        "doc_doi": "10.4018/978-1-5225-3731-1.ch018",
        "doc_eid": "2-s2.0-85047335261",
        "doc_date": "2018-03-02",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Social Sciences (all)",
                "area_abbreviation": "SOCI",
                "area_code": "3300"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2018, IGI Global.The relevance of the impact of tax complexity on tax noncompliance justifies the study of tax complexity in the scope of several taxpayers, including local authorities. This chapter analyses the case of the value added tax (VAT), a typical example where the misuse of exemptions and no subjections imply the no payment of taxes by citizens. Therefore, this chapter tries to assess, qualitatively and quantitatively, the levels of tax complexity perceived in local authorities' administrations through the amount of binding tax information those organizations request. The results suggest a high degree of perception of tax complexity and uncertainty as well as a lack of transparency associated with the tax framework of the activities of local authorities. It is pointed out that the perceived complexity is essentially legal. Moreover, the findings establish a relation between some more complex legal changes with great tax impact within these entities and the increase of the number of binding tax information requested on their subjects.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "An IT value management capability model for Portuguese universities: A Delphi study",
        "doc_scopus_id": "85061969027",
        "doc_doi": "10.1016/j.procs.2018.10.082",
        "doc_eid": "2-s2.0-85061969027",
        "doc_date": "2018-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "COBIT 5",
            "IT governance",
            "Resource-based view",
            "VAL IT 2.0",
            "Value management"
        ],
        "doc_abstract": "© 2018 The Authors. Published by Elsevier Ltd..One of the most common dilemmas faced today by organizations and their leaders is how to guarantee value from high level IT investments, i.e. how organizations ensure expected benefits from growth in IT investments. Knowledgeable about this reality, organizations seek solutions to solve this problem, either through the adoption of frameworks developed and proposed by the professional community (COBIT5; VAL IT 2.0; IT-CMF), or alternatively, by designing and implementing their own models. The aforementioned, for organizations in general, is not different in the context of Higher Education Institutions (HEIs). This paper adopts a Resource-Based View theory (RBV) to identify a set of competences and resources, which contribute to develop and conceptualize an IT Value Management Capability Model. The identified items were submitted to a panel of experts through a Delphi study in order to validate and propose a baseline to assist academic and practitioners understand essential requirements to implement an IT Value Management Capability Model (ITVMCM) in Portuguese public universities.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2018-10-23 2018-10-23 2018-10-23 2018-10-23 2018-10-31T16:13:02 S1877-0509(18)31728-9 S1877050918317289 10.1016/j.procs.2018.10.082 S300 S300.2 HEAD-AND-TAIL 2019-06-27T08:18:40.177441Z 0 0 20180101 20181231 2018 2018-10-23T23:03:36.452184Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 true 138 138 C Volume 138 83 612 620 612 620 2018 2018 2018-01-01 2018-12-31 2018 CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018 João Eduardo Quintela Varajão Maria Manuela Cruz-Cunha Ricardo Martinho Rui Rijo Dulce Domingos Emanuel Peres CENTERIS 2018 (Main track) article fla © 2018 The Author(s). Published by Elsevier B.V. VALUEMANAGEMENTCAPABILITYMODELFORPORTUGUESEUNIVERSITIESADELPHISTUDY PEREIRA C WILKIN 2013 283 306 C GRAJEK 2018 S NEVO 2011 403 418 S ALI 2015 1 25 S WEILL 2004 P VAN 2004 1 36 W STRATEGIESFORINFORMATIONTECHNOLOGYGOVERNANCE STRUCTURESPROCESSESRELATIONALMECHANISMSFORGOVERNANCE DEHAES 2015 S ENTERPRISEGOVERNANCEINFORMATIONTECHNOLOGYACHIEVINGALIGNMENTVALUEFEATURINGCOBIT5 MAES 2015 82 104 K ASHRAFI 2015 15 38 R BARNEY 1991 99 120 J BARNEY 1995 49 61 J MIKALEF 2016 1 16 P PEREIRA 2017 431 446 C ININFORMATIONSYSTEMS VALUEMANAGEMENTCAPABILITYENABLEDCOBIT5FRAMEWORK ASHURST 2008 352 370 C LINSTONE 1975 H DELPHIMETHODTECHNIQUESAPPLICATIONS OKOLI 2004 15 29 C WORRELL 2013 193 208 J SCHMIDT 1997 763 774 R DHILLON 2017 452 464 G PEREIRAX2018X612 PEREIRAX2018X612X620 PEREIRAX2018X612XC PEREIRAX2018X612X620XC Full 2018-10-06T01:16:47Z ElsevierWaived OA-Window This is an open access article under the CC BY-NC-ND license. © 2018 The Author(s). Published by Elsevier B.V. item S1877-0509(18)31728-9 S1877050918317289 10.1016/j.procs.2018.10.082 280203 2019-06-27T08:18:40.177441Z 2018-01-01 2018-12-31 true 658969 MAIN 9 43501 849 656 IMAGE-WEB-PDF 1 am 577957 ScienceDirect Available online at www.sciencedirect.com Procedia Computer Science 138 (2018) 612â€“620 1877-0509 Â© 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license Selection and peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies. 10.1016/j.procs.2018.10.082 10.1016/j.procs.2018.10.082 Â© 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license Selection and peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies. 1877-0509 Available online at www.sciencedirect.com ScienceDirect Procedia Computer Science 00 (2018) 000â€“000 www.elsevier.com/locate/procedia 1877-0509 Â© 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license Selection and peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies. CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018 An IT Value Management Capability Model for Portuguese Universities: A Delphi Study Cristiano Pereiraa,*, Carlos Ferreiraa, Luis Amaralb aUniversidade Aveiro, Aveiro,Portugal bUniversidade do Minho, GuimarÃ£es, Portugal Abstract One of the most common dilemmas faced today by organizations and their leaders is how to guarantee value from high level IT investments, i.e. how organizations ensure expected benefits from growth in IT investments. Knowledgeable about this reality, organizations seek solutions to solve this problem, either through the adoption of frameworks developed and proposed by the professional community (COBIT5; VAL IT 2.0; IT-CMF), or alternatively, by designing and implementing their own models. The aforementioned, for organizations in general, is not different in the context of Higher Education Institutions (HEIs). This paper adopts a Resource-Based View theory (RBV) to identify a set of competences and resources, which contribute to develop and conceptualize an IT Value Management Capability Model. The identified items were submitted to a panel of experts through a Delphi study in order to validate and propose a baseline to assist academic and practitioners understand essential requirements to implement an IT Value Management Capability Model (ITVMCM) in Portuguese public universities. Â© 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license Selection and peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies Keywords: IT Governance; IT Value Management Capability; Resource-Based View; COBIT 5; VAL IT 2.0 * Corresponding author: E-mail address: cristiano@ua.pt Available online at www.sciencedirect.com ScienceDirect Procedia Computer Science 00 (2018) 000â€“000 www.elsevier.com/locate/procedia 1877-0509 Â© 2018 The Authors. Published by Elsevier Ltd. This is a open access article under the CC BY-NC-ND license Selection a d pe r-review under responsibility of the scientific committee of th CENTERIS - Internatio al o fere ce ENTERprise Information Systems / ProjMAN - I ternational Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies. CENTERIS - International Conference on ENTERprise I formation Systems / ProjMAN - Inter ational Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018 An IT Value Management Capability Model for Portuguese Universities: A Delphi Study Cristiano Pereiraa,*, Carlos Ferreiraa, Luis Amaralb aUniversidade Aveiro, Aveiro,Portugal bUniversidade do Minho, GuimarÃ£es, Portugal Abstract On of the most c mmon dilemmas faced today by organizations and their leaders is how to guarant e value from high leve IT investments, i.e. how orga izations nsure expected benefits from growth in IT invest nts. Knowledg able about this reality, o ganizations seek solutions to solve this problem, either through the adoption of frameworks dev loped and proposed by the professional community (COBIT5; VAL IT 2.0; IT-CMF), or alter a ively, by designing and impleme ting heir own models. The aforementioned, for organizations in general, is not different in the context of Higher Education Institutions (HEIs). This paper ado s R source-Bas d View theory (RBV) to identify a set of compe nces and resourc s, which contribut to develop nd conceptualize an IT Value Management Capa ility Model. The identified items were submitted to a panel of exp rts through a Del hi study in order to validat and pro ose a baseline to assist academic and practitioners understand essential requirements to implement an IT Value Management Capability Model (ITVMCM) in Portuguese public universities. Â© 2018 The Authors. Published by Elsevier Ltd. This is a open access article under the CC BY-NC-ND license ivecommons.org/licenses/by-nc-nd/4.0/) Selection and pee -rev ew und r responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - I ternational Confere ce on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies Keywords: IT Governance; IT Value Management Capability; Resource-Based View; COBIT 5; VAL IT 2.0 * Corresponding author: E-mail address: cristiano@ua.pt 2 Author name / Procedia Computer Science 00 (2018) 000â€“000 1. Introduction Evolution in IT investments increases concern of organizations to ensure expected benefits, several studies point to cases of failure, or little evidence suggesting productivity gains from these type of investments [1]. This is a common dilemma faced by organizations and their leaders, how to guarantee value from high IT investments [2]. IT governance (ITG) is a fundamental influence on IT Value Management (ITVM), knowledgeable about this reality, organizations (public or private) seek solutions in ITG frameworks proposed by practitioners [3,4] or design their own models, adjusted to their particular reality. The aforementioned for organizations in general is not of minor importance in the context of Higher Education Institutions (HEIs). To corroborate this statement, a study carried out by EDUCASE [5] highlights some issues related to the subject: i) Institution-wide IT strategy; ii) Balancing and rightsizing IT priorities and budget to support IT enabled institutional efficiencies and innovations in the context of institutional funding realities; and iii) IT staffing and organizational models. Organizations with decentralized structures, like Portuguese universities, need to regularly assess and renew their ITG approach to adapt business change. In a context of continuous organizational transformations, we suggested an IT Value Management Capability Model (ITVMCM) to support institutional leaders, providing guidance on main competences and organizational resources to improve IT benefits. Most research on IT business value was guided by the Resource-Based View theory (RBV) which focuses on organizational resources as a support to institutions achieve competitive advantage [6]. Sustained on RBV, this paper aims to identify and propose an ITVMCM for public Portuguese universities. Two research questions emerge: 1) what are the competences and organizational resources that enable the development of an ITVM capability in public Portuguese universities, 2) what is the relative level of importance of each of those components? Since ITVM models and implementations guidelines in HEIs are scarcely available, we opted for an exploratory study rather than hypothesis testing. Exploratory research was built on literature review and a Delphi method research, involving a panel of experts from the IS/IT industry, academy (professors and researchers) and a third group from university IS/IT managers. The remainder of this paper is organized as follows: Section 2 outlines the background of the study, presenting a brief review related to ITG, IT value delivery and a brief outlook to RBV theory. Section 3 describes the research methodology, including a description on how Delphi was designed and executed. Section 4 presents a summary of results and finally, Section 5 states the main conclusions, and limitations, along with future research proposals. 2. Theoretical foundations 2.1. The nature of IT value delivery: Evolution from governance to enterprise governance of IT Governance denotes regulating or controlling â€œsomethingâ€�. That â€œsomethingâ€� depends on a specific governance perspective, it might be the organization as a whole (Enterprise or Corporate Governance) or it can be IT (IT Governance). Initially, ITG describes how the board of directors and executive management consider IT in their supervision, monitor, control and direction of organizations [7]. Later, Weill and Ross [8], proposed one of the most referenced definitions of ITG, â€œeffective IT governance is the single most important predictor of the value an organization generates from ITâ€�. Simultaneously, Van Grembergen et al. [9] denote ITG transversal characteristics across all organization (Business and IT), this vision, points to a broader concept called Enterprise Governance of IT (EGIT) that encompasses organizational capacity and enabled outcomes, specifically business/IT alignment and in the end value creation out of IT-enabled investments [10]. Parallel to academic evolution, practitioners offer new approaches to ITG, developing new frameworks and standards. Two of the most known are: COBIT 5, released by ISACA in 2012 [4], and the international standard ISO/IEC 38500 [11]. From previous studies, a question emerges â€“ what constitutes value creation from IT investments? By itself, IT investments will not generate value for business, only linking IT and non-IT resources can competitive advantage be attained [12]. The Ambiguity to identify what is value for business is one of the main reasons that makes IT value hard to obtain. Cristiano Pereira et al. / Procedia Computer Science 138 (2018) 612â€“620 613 Available online at www.sciencedirect.com ScienceDirect Procedia Computer Science 00 (2018) 000â€“000 www.elsevier.com/locate/procedia 1877-0509 Â© 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license Selection and peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies. CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018 An IT Value Management Capability Model for Portuguese Universities: A Delphi Study Cristiano Pereiraa,*, Carlos Ferreiraa, Luis Amaralb aUniversidade Aveiro, Aveiro,Portugal bUniversidade do Minho, GuimarÃ£es, Portugal Abstract One of the most common dilemmas faced today by organizations and their leaders is how to guarantee value from high level IT investments, i.e. how organizations ensure expected benefits from growth in IT investments. Knowledgeable about this reality, organizations seek solutions to solve this problem, either through the adoption of frameworks developed and proposed by the professional community (COBIT5; VAL IT 2.0; IT-CMF), or alternatively, by designing and implementing their own models. The aforementioned, for organizations in general, is not different in the context of Higher Education Institutions (HEIs). This paper adopts a Resource-Based View theory (RBV) to identify a set of competences and resources, which contribute to develop and conceptualize an IT Value Management Capability Model. The identified items were submitted to a panel of experts through a Delphi study in order to validate and propose a baseline to assist academic and practitioners understand essential requirements to implement an IT Value Management Capability Model (ITVMCM) in Portuguese public universities. Â© 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license Selection and peer-review under responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies Keywords: IT Governance; IT Value Management Capability; Resource-Based View; COBIT 5; VAL IT 2.0 * Corresponding author: E-mail address: cristiano@ua.pt Available online at www.sciencedirect.com ScienceDirect Procedia Computer Science 00 (2018) 000â€“000 www.elsevier.com/locate/procedia 1877-0509 Â© 2018 The Authors. Published by Elsevier Ltd. This is a open access article under the CC BY-NC-ND license Selection a d pe r-review under responsibility of the scientific committee of th CENTERIS - Internatio al o fere ce ENTERprise Information Systems / ProjMAN - I ternational Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies. CENTERIS - International Conference on ENTERprise I formation Systems / ProjMAN - Inter ational Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018 An IT Value Management Capability Model for Portuguese Universities: A Delphi Study Cristiano Pereiraa,*, Carlos Ferreiraa, Luis Amaralb aUniversidade Aveiro, Aveiro,Portugal bUniversidade do Minho, GuimarÃ£es, Portugal Abstract On of the most c mmon dilemmas faced today by organizations and their leaders is how to guarant e value from high leve IT investments, i.e. how orga izations nsure expected benefits from growth in IT invest nts. Knowledg able about this reality, o ganizations seek solutions to solve this problem, either through the adoption of frameworks dev loped and proposed by the professional community (COBIT5; VAL IT 2.0; IT-CMF), or alter a ively, by designing and impleme ting heir own models. The aforementioned, for organizations in general, is not different in the context of Higher Education Institutions (HEIs). This paper ado s R source-Bas d View theory (RBV) to identify a set of compe nces and resourc s, which contribut to develop nd conceptualize an IT Value Management Capa ility Model. The identified items were submitted to a panel of exp rts through a Del hi study in order to validat and pro ose a baseline to assist academic and practitioners understand essential requirements to implement an IT Value Management Capability Model (ITVMCM) in Portuguese public universities. Â© 2018 The Authors. Published by Elsevier Ltd. This is a open access article under the CC BY-NC-ND license ivecommons.org/licenses/by-nc-nd/4.0/) Selection and pee -rev ew und r responsibility of the scientific committee of the CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - I ternational Confere ce on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies Keywords: IT Governance; IT Value Management Capability; Resource-Based View; COBIT 5; VAL IT 2.0 * Corresponding author: E-mail address: cristiano@ua.pt 2 Author name / Procedia Computer Science 00 (2018) 000â€“000 1. Introduction Evolution in IT investments increases concern of organizations to ensure expected benefits, several studies point to cases of failure, or little evidence suggesting productivity gains from these type of investments [1]. This is a common dilemma faced by organizations and their leaders, how to guarantee value from high IT investments [2]. IT governance (ITG) is a fundamental influence on IT Value Management (ITVM), knowledgeable about this reality, organizations (public or private) seek solutions in ITG frameworks proposed by practitioners [3,4] or design their own models, adjusted to their particular reality. The aforementioned for organizations in general is not of minor importance in the context of Higher Education Institutions (HEIs). To corroborate this statement, a study carried out by EDUCASE [5] highlights some issues related to the subject: i) Institution-wide IT strategy; ii) Balancing and rightsizing IT priorities and budget to support IT enabled institutional efficiencies and innovations in the context of institutional funding realities; and iii) IT staffing and organizational models. Organizations with decentralized structures, like Portuguese universities, need to regularly assess and renew their ITG approach to adapt business change. In a context of continuous organizational transformations, we suggested an IT Value Management Capability Model (ITVMCM) to support institutional leaders, providing guidance on main competences and organizational resources to improve IT benefits. Most research on IT business value was guided by the Resource-Based View theory (RBV) which focuses on organizational resources as a support to institutions achieve competitive advantage [6]. Sustained on RBV, this paper aims to identify and propose an ITVMCM for public Portuguese universities. Two research questions emerge: 1) what are the competences and organizational resources that enable the development of an ITVM capability in public Portuguese universities, 2) what is the relative level of importance of each of those components? Since ITVM models and implementations guidelines in HEIs are scarcely available, we opted for an exploratory study rather than hypothesis testing. Exploratory research was built on literature review and a Delphi method research, involving a panel of experts from the IS/IT industry, academy (professors and researchers) and a third group from university IS/IT managers. The remainder of this paper is organized as follows: Section 2 outlines the background of the study, presenting a brief review related to ITG, IT value delivery and a brief outlook to RBV theory. Section 3 describes the research methodology, including a description on how Delphi was designed and executed. Section 4 presents a summary of results and finally, Section 5 states the main conclusions, and limitations, along with future research proposals. 2. Theoretical foundations 2.1. The nature of IT value delivery: Evolution from governance to enterprise governance of IT Governance denotes regulating or controlling â€œsomethingâ€�. That â€œsomethingâ€� depends on a specific governance perspective, it might be the organization as a whole (Enterprise or Corporate Governance) or it can be IT (IT Governance). Initially, ITG describes how the board of directors and executive management consider IT in their supervision, monitor, control and direction of organizations [7]. Later, Weill and Ross [8], proposed one of the most referenced definitions of ITG, â€œeffective IT governance is the single most important predictor of the value an organization generates from ITâ€�. Simultaneously, Van Grembergen et al. [9] denote ITG transversal characteristics across all organization (Business and IT), this vision, points to a broader concept called Enterprise Governance of IT (EGIT) that encompasses organizational capacity and enabled outcomes, specifically business/IT alignment and in the end value creation out of IT-enabled investments [10]. Parallel to academic evolution, practitioners offer new approaches to ITG, developing new frameworks and standards. Two of the most known are: COBIT 5, released by ISACA in 2012 [4], and the international standard ISO/IEC 38500 [11]. From previous studies, a question emerges â€“ what constitutes value creation from IT investments? By itself, IT investments will not generate value for business, only linking IT and non-IT resources can competitive advantage be attained [12]. The Ambiguity to identify what is value for business is one of the main reasons that makes IT value hard to obtain. 614 Cristiano Pereira et al. / Procedia Computer Science 138 (2018) 612â€“620 Author name / Procedia Computer Science 00 (2018) 000â€“000 3 2.2. Resource-Based View RBV theory has been one of the most well-known and powerful theories for understanding organizations over the past two decades [13]. Resource Based View claims that resources (including IT resources) enable organizations to achieve competitive advantage, but not all, only a subset known as VRIN (valuable, rare, imperfectly imitable and non-substitutable) resources [14]. A firmâ€™s resource includes not only physical assets, such as plant and location (tangible resources), but also human, knowledge and relationship resources (intangible ones). These intangible resources are unique and cannot be easily imitated, thus satisfying VRIN conditions of RBV theory [13]. In contrast to resources, Barney [15] defines capabilities as â€œthe firmâ€™s capacity to deploy resources, usually in combination, using organizational processes, to effect a desired endâ€�. Organizational capability has attracted great interest from researchers, namely in the IS field, they state that capabilities can be a major source of firmâ€™s performance [16]. Organizational capability is what you can do with what you know. According to C. Pereira et al. [17], what you know should be seen as a new concept that emerges from RBV literature, known as competences. Ashurst et al. [18] indicate that competences consist of a combination of personal skills, knowledge and experience, organizational roles and processes called practices. The concept of practice is align with how people actually work. To create sustainable competitive advantage (value delivery), a firm needs to deploy resources, like: physical capital resources (e.g. IT investments); human capital resources (e.g. skills, knowledge, experience); and organizational capital resources (e.g. boards of directors and top management), through a set of practices, logically orchestrated to develop competences that can be integrated in IT value management capability [17]. 3. Research Design 3.1. The Delphi Method The Delphi method can be characterized â€œas a method for structuring a group communication process so that the process is effective in allowing a group of individuals, as a whole, to deal with a complex problemâ€� [19]. Multiple applications and variations on classical Delphi method can be found in the IS field [20;21]. Key features of Delphi studies are anonymity between participants with controlled feedback. Besides anonymity and controlled feedback, other characteristics of the Delphi method are: iteration, consensus and statistical group response aggregation. Two of these stand out, the possibility of anonymity and the concept of consensus. The first allows panel members to express their views and perceptions in a more honest and genuine way, without being subject to social pressures that may result from the presence of experts with more dominant personalities or status. The second because a Delphi study usually seeks to elicit consensus on the topic under research, through a series of statistical measures. The level of consensus among experts can be evaluated at the end of each round and a decision can be achieved at the end of the study. 3.2. Design of the Delphi Study According specificities of Delphi is essential plan and prior design study, ensuring aspects on which is important reflect during design phase are not forgot. Absence of planning, decisions and options taken are negative aspects, pointed out by several authors [20;21]. Considering these criticisms, we describe the main phases in the next sub- sections. 3.2.1. Items List Generation There are usually three alternatives envisaged at this level to generate a first item list: i) from literature review; ii) from group discussions and iii) from a mixed strategy that starts with a literature review process, followed by group evaluation. We opted for a literature review process to identify the initial list, resulting in 14 ITVM competences and 25 enablers. This Initial list of items and correspondent description are provided in Appendix A. To mitigate or reduce bias in responses that can be introduced and eventual incompleteness of the generated list, two decisions were taken. First, in Round 1 experts could propose new items, not included before and second sort alphabetically competences and enablerâ€™s items that constitute initial list. 4 Author name / Procedia Computer Science 00 (2018) 000â€“000 3.2.2. Structure of the Questionnaire To meet the study goals, we separate the questionnaire in two parts. In part I, we list organizational competences, part II refers to enablers that contribute to the development of ITVM competences. Apart from these, a third step was included, (free text area), where experts could identify and describe new potential relevant items not included in the initial list. The Ranking technique used to evaluate the submitted items follows the Q-Sort method, through structured procedures that make respondents analyze the importance of items in an aggregated form and not isolated. 3.2.3. Panel Constitution Particular attention to the composition and panel size are aspects to consider while defining the expert panel. Worrell et al. [21], consider panel size between 10 and 30 experts as a usual number used in multiple Delphi studies. In effect, size depends on the requirements identified in the panel composition process, as well as on the characteristics of each individual member. To generate the panel of experts we based on the procedure suggested by Okoli and Pawlowskiâ€™s [20], which resulted in 93 potential experts to whom an invitation message was sent, of these, 33 (35%) belong to IS/IT Industry, 37 (40%) to academy (professors and researchers) and 23 (25%) to university IS/IT managers. 3.2.4. Mode of Execution Decision on how data collection would take place fell on web-based option. Advantages of this option, over a traditional mail, is obvious, namely, efficiency of the process and minimization of costs. We used the tool E-Delphi in order to interact with experts and execute all phases of the Delphi study. 3.2.5. Stopping Criterion As stated before, at the end of each round an assessment of consensus among experts should be carried out. If consensus is not significant, a new iteration occurs, if consensus is satisfactory, the study ends. Despite the apparent simplicity of this method, it is not always easy to achieve the desired level of consensus. To support evaluation two statistical measures were selected: to evaluate the level of agreement of expertÂ´s opinion in each round the KendallÂ´s W coefficient was selected; to evaluate the level of stability of global opinion between rounds the SpearmanÂ´s Rank correlation coefficient (Spearman rho) was selected. The level of agreement varies between zero (no agreement) and one (perfect agreement). A high value of W means experts are judging the level of importance of items in similar way. Spearman rho correlation coefficient varies between zero (no stability) and one (perfect stability). Besides typical statistical measures to evaluate consensus, Diamond et al. [22] refer a maximum number of rounds as stop criteria. Usually three rounds, before expert panel fatigue becomes an issue [20]. 3.3. Execution of the Delphi Study This section briefly describes the implementation of the Delphi study. It aims to characterize central moments carried out within this research. Table 1 summarizes the main figures of the study. Table 1 â€“ Characterization of rounds Round 1 Round 2 Round 3 Response rate 42 experts (47%) from a total of 90 experts initially invited 25 experts (60%) from a total of 42 experts from previous round 21 experts (84%) from a total of 25 experts from previous round Total questionnaire items 39 39 39 Global level consensus Very Low Low Moderated Level of importance competences KendallÂ´s W 0,18 (p<0.001) low consensus 0,24 (p<0.001) low consensus 0,34 (p<0.001) moderated consensus Spearman rho not Applicable not evaluated 0,91 (p<0.001) very high stability 0,95 (p<0.001) very high stability Level of importance enablers KendallÂ´s W 0,23 (p<0.001) low consensus 0,30 (p<0.001) moderated consensus 0,30 (p<0.001) moderated consensus Spearman rho not Applicable not evaluated 0,92 (p<0.001) very high stability 0,93 (p<0.001) very high stability Cristiano Pereira et al. / Procedia Computer Science 138 (2018) 612â€“620 615 Author name / Procedia Computer Science 00 (2018) 000â€“000 3 2.2. Resource-Based View RBV theory has been one of the most well-known and powerful theories for understanding organizations over the past two decades [13]. Resource Based View claims that resources (including IT resources) enable organizations to achieve competitive advantage, but not all, only a subset known as VRIN (valuable, rare, imperfectly imitable and non-substitutable) resources [14]. A firmâ€™s resource includes not only physical assets, such as plant and location (tangible resources), but also human, knowledge and relationship resources (intangible ones). These intangible resources are unique and cannot be easily imitated, thus satisfying VRIN conditions of RBV theory [13]. In contrast to resources, Barney [15] defines capabilities as â€œthe firmâ€™s capacity to deploy resources, usually in combination, using organizational processes, to effect a desired endâ€�. Organizational capability has attracted great interest from researchers, namely in the IS field, they state that capabilities can be a major source of firmâ€™s performance [16]. Organizational capability is what you can do with what you know. According to C. Pereira et al. [17], what you know should be seen as a new concept that emerges from RBV literature, known as competences. Ashurst et al. [18] indicate that competences consist of a combination of personal skills, knowledge and experience, organizational roles and processes called practices. The concept of practice is align with how people actually work. To create sustainable competitive advantage (value delivery), a firm needs to deploy resources, like: physical capital resources (e.g. IT investments); human capital resources (e.g. skills, knowledge, experience); and organizational capital resources (e.g. boards of directors and top management), through a set of practices, logically orchestrated to develop competences that can be integrated in IT value management capability [17]. 3. Research Design 3.1. The Delphi Method The Delphi method can be characterized â€œas a method for structuring a group communication process so that the process is effective in allowing a group of individuals, as a whole, to deal with a complex problemâ€� [19]. Multiple applications and variations on classical Delphi method can be found in the IS field [20;21]. Key features of Delphi studies are anonymity between participants with controlled feedback. Besides anonymity and controlled feedback, other characteristics of the Delphi method are: iteration, consensus and statistical group response aggregation. Two of these stand out, the possibility of anonymity and the concept of consensus. The first allows panel members to express their views and perceptions in a more honest and genuine way, without being subject to social pressures that may result from the presence of experts with more dominant personalities or status. The second because a Delphi study usually seeks to elicit consensus on the topic under research, through a series of statistical measures. The level of consensus among experts can be evaluated at the end of each round and a decision can be achieved at the end of the study. 3.2. Design of the Delphi Study According specificities of Delphi is essential plan and prior design study, ensuring aspects on which is important reflect during design phase are not forgot. Absence of planning, decisions and options taken are negative aspects, pointed out by several authors [20;21]. Considering these criticisms, we describe the main phases in the next sub- sections. 3.2.1. Items List Generation There are usually three alternatives envisaged at this level to generate a first item list: i) from literature review; ii) from group discussions and iii) from a mixed strategy that starts with a literature review process, followed by group evaluation. We opted for a literature review process to identify the initial list, resulting in 14 ITVM competences and 25 enablers. This Initial list of items and correspondent description are provided in Appendix A. To mitigate or reduce bias in responses that can be introduced and eventual incompleteness of the generated list, two decisions were taken. First, in Round 1 experts could propose new items, not included before and second sort alphabetically competences and enablerâ€™s items that constitute initial list. 4 Author name / Procedia Computer Science 00 (2018) 000â€“000 3.2.2. Structure of the Questionnaire To meet the study goals, we separate the questionnaire in two parts. In part I, we list organizational competences, part II refers to enablers that contribute to the development of ITVM competences. Apart from these, a third step was included, (free text area), where experts could identify and describe new potential relevant items not included in the initial list. The Ranking technique used to evaluate the submitted items follows the Q-Sort method, through structured procedures that make respondents analyze the importance of items in an aggregated form and not isolated. 3.2.3. Panel Constitution Particular attention to the composition and panel size are aspects to consider while defining the expert panel. Worrell et al. [21], consider panel size between 10 and 30 experts as a usual number used in multiple Delphi studies. In effect, size depends on the requirements identified in the panel composition process, as well as on the characteristics of each individual member. To generate the panel of experts we based on the procedure suggested by Okoli and Pawlowskiâ€™s [20], which resulted in 93 potential experts to whom an invitation message was sent, of these, 33 (35%) belong to IS/IT Industry, 37 (40%) to academy (professors and researchers) and 23 (25%) to university IS/IT managers. 3.2.4. Mode of Execution Decision on how data collection would take place fell on web-based option. Advantages of this option, over a traditional mail, is obvious, namely, efficiency of the process and minimization of costs. We used the tool E-Delphi in order to interact with experts and execute all phases of the Delphi study. 3.2.5. Stopping Criterion As stated before, at the end of each round an assessment of consensus among experts should be carried out. If consensus is not significant, a new iteration occurs, if consensus is satisfactory, the study ends. Despite the apparent simplicity of this method, it is not always easy to achieve the desired level of consensus. To support evaluation two statistical measures were selected: to evaluate the level of agreement of expertÂ´s opinion in each round the KendallÂ´s W coefficient was selected; to evaluate the level of stability of global opinion between rounds the SpearmanÂ´s Rank correlation coefficient (Spearman rho) was selected. The level of agreement varies between zero (no agreement) and one (perfect agreement). A high value of W means experts are judging the level of importance of items in similar way. Spearman rho correlation coefficient varies between zero (no stability) and one (perfect stability). Besides typical statistical measures to evaluate consensus, Diamond et al. [22] refer a maximum number of rounds as stop criteria. Usually three rounds, before expert panel fatigue becomes an issue [20]. 3.3. Execution of the Delphi Study This section briefly describes the implementation of the Delphi study. It aims to characterize central moments carried out within this research. Table 1 summarizes the main figures of the study. Table 1 â€“ Characterization of rounds Round 1 Round 2 Round 3 Response rate 42 experts (47%) from a total of 90 experts initially invited 25 experts (60%) from a total of 42 experts from previous round 21 experts (84%) from a total of 25 experts from previous round Total questionnaire items 39 39 39 Global level consensus Very Low Low Moderated Level of importance competences KendallÂ´s W 0,18 (p<0.001) low consensus 0,24 (p<0.001) low consensus 0,34 (p<0.001) moderated consensus Spearman rho not Applicable not evaluated 0,91 (p<0.001) very high stability 0,95 (p<0.001) very high stability Level of importance enablers KendallÂ´s W 0,23 (p<0.001) low consensus 0,30 (p<0.001) moderated consensus 0,30 (p<0.001) moderated consensus Spearman rho not Applicable not evaluated 0,92 (p<0.001) very high stability 0,93 (p<0.001) very high stability 616 Cristiano Pereira et al. / Procedia Computer Science 138 (2018) 612â€“620 Author name / Procedia Computer Science 00 (2018) 000â€“000 5 In temporal terms, the study took place over twelve weeks (December 2017 to March of 2018). The first moment of interaction was the sending of invitations to participate in the study to 93 experts by e-mail, three of them declined their participation. In Round 1, the questionnaire was send to 90 experts. In subsequent rounds, the questionnaire was only sent to experts who completed the previous round. Response rates obtained were of 47% in Round 1, 60% in Round 2 and 84% in Round 3. After Round 1, only one new item proposal emerged. After examining the existence of similarities or overlaps between the initial list and the proposed item, we assumed that the new proposed item was already considered, therefore, we did not include it. In conclusion, the initial list, stated in Appendix A, turned out to be the final list evaluated by the experts. As stated in section 3.2.5, we have evaluated the consensus level at the end of each round using two statistical measures, KendallÂ´s W and Spearmanâ€™s rho. Supported by Schmidt, [23] to interpret KendallÂ´s W, the following consensus categories were considered: (1) very low or low for 0,01<W<0,29; (2) moderated for 0,30<W<0,50; (3) high or very high for 0,51<W<0,99. Stability of global opinion between rounds is considered very high for Spearman rho >0,90 [24]. Taking into account the values of the statistical tests performed, global level of consensus evaluated in Round 1 and 2 were very low and low, respectively. Evaluation at the end of Round 3 revealed a moderated global level consensus. Bearing in mind the high number of items involved (39 items), the very high stability in global panel rank between rounds and the substantial decrease in participants at the end of Round 3, we considered this a tolerable level of consensus and, therefore, considered it a positive panel convergence, thus leading to the conclusion of the Q-Sort Delphi study. 4. Research Results and Discussion Results reflect the opinion and experience of 21 experts who participated in all three rounds, 20% belong to the IS/IT Industry, 66% to the academy group and 14% to the university IS/IT managers group. The evolution in items rankings are summarized in Table 2. We used the exploratory cluster analysis technique (â€œWard's Method\" with the measure of similarity \"Square Euclidean Distance\") to organize data in significant groups and support our analysis. DendogramÂ´s results allows us to identify four clusters for competences and five clusters for enablers. Table 2 â€“ Ranking of importance of items ITVM Enabler Items Ranking ITVM Competence Items Ranking Round 1 Round 2 Round 3 Enabler Cluster Round 1 Round 2 Round 3 Competences Cluster 2 1 1 E22 Most Significant 1 2 1 C4 Most Significant 1 2 2 E14 7 4 2 C5 4 3 3 E19 Significant 2 1 3 C3 5 8 4 E3 3 3 4 C14 Significant 3 4 5 E10 8 5 5 C1 10 6 6 E23 4 7 6 C7 Neutral 6 7 7 E9 9 9 7 C13 8 5 8 E25 Moderated 6 6 8 C9 7 9 9 E2 5 8 9 C8 16 14 10 E16 10 10 10 C10 Less Significant 18 12 11 E8 11 12 11 C11 11 11 12 E21 12 11 12 C2 14 16 13 E17 Neutral 13 13 13 C12 19 17 14 E18 14 14 14 C6 13 15 15 E6 22 21 16 E7 9 10 17 E5 17 13 18 E1 12 20 19 E12 15 19 20 E11 Less Significant 20 18 21 E15 21 22 22 E4 25 25 23 E13 23 24 24 E24 24 23 25 E20 6 Author name / Procedia Computer Science 00 (2018) 000â€“000 In addition, we requested experts to rank competences according to the level of importance they considered that best contributes to ITVM organizational capability in Portuguese HEIs. After the first phase, a second request, related to the first, was made, in order to rank the enablers according to what experts assumed as the best in the development of competences ranked previously. From Table 2, some relevant aspects should be highlight. First, the importance recognized to competence C4, which, in all rounds, occupied the top positions, emphasizing the importance of an ITVM model framed with ITG. This corroborates the importance pointed out by Maes et al. [12], regarding the adoption of a model to implement such type of practices. Secondly, the consistency in all rounds associated to the first five positions in the ranking of competences. Only two exceptions: the exclusion in Round 2 of competence C8, which in Round 1 ranked in the 5th position and finished, in Round 3, in position 9, as well as competence C7, initially in position 4, that finished the study in position 6. In the opposite direction, C5 and C1 started outside the top five and ended in Round 3 in the 2nd and 5th position respectively. More evident are the results regarding the items placed in less significant positions of the ranking, which in the three rounds are occupied by the same set of competences (C10, C11, C2, C12 and C6). What concerns the enabler items ranking evolution, the most significant enablers are E22 and E14, permanently in the first two positions in all the three Rounds. These are the two most important organizational resources to develop ITVM competences. Finally, we can highlight the strong consistency across the three rounds regarding the less significant enablers (E4, E13, E24 and E20). The same stability is verified in the top five positions in all rounds. An exception is E3, which in the 2nd Round, ranked in the 8th position, returning to the Top 5 in Round 3. With the purpose of setting up a minimum baseline to assist academic and practitioners understanding the essential, requirements or items, to implement our ITVM capability model in Portuguese universities, we based on C. Pereira et al. (6), and describe, in the next paragraph, the main relationships between the greatest significant enablers that contribute to deploy the most important competences. Following our results, we conclude that E22 and E14 contribute to deploy C4 and C3. The 3rd ranking enabler E19 contributes to deploy C14 and C1. Enabler E3 also helps in the deployment of competences C14 and C5. Like E22 and E14, enabler E10, contributes to deploy competences C4 and C3. The sixth ranking enabler, E23, contributes to deploy C5 and C1, ITVM competences. The enabler of the cluster, classified as significant, E9, contributes to the four first competences (C4, C5, C3 and C14). Two categories of enablers are not presented in the proposed minimum baseline: i) Culture, Ethics and Behavior and ii) Infrastructures and Services, which does not mean that they are not important. From this results we verify that enabler E2 (Culture, Ethics and Behavior category) is categorized by experts as moderated, while enabler E7 (Infrastructures & Services) with a neutral impact. The lack of enablers from the category Culture, Ethics and Behavior, in the most significant or significant cluster was a surprise to us. According to C. Pereira et al. [17] this category is important to maximize the benefits of IT adoption, to reevaluate the investment portfolio. Several studies have seen organizational culture as an important factor that may explain significant variations in IT business value and is essential to change organizational culture, once this change involves peopleÂ´s value, attitudes and behaviors. 5. Conclusions From an academic point of view, this study gives an exploratory light on the issue of IT value management to support Portuguese HEIs, providing guidance about â€œwhatâ€� improves value from IT investments. This study revealed that, according to the experts panel HEIs could leverage a wide range of ITVM competences and enablers to support the development of an organizational ITVM capability, important to achieve value creation of IT-enabled investments. As stated in section 1, ITG has a great influence on ITVM, to corroborate this, we verify that E22 and E14 contribute to deploy competence C4, in other words, knowledge and the implementation of ITG frameworks and processes help institutions to gain competences to create ITVM practices. C4 is in line with issues acknowledged by international HEIs (7), especially institutional adaptiveness related to repositioning the role of IT leadership as a strategic partner of institutional leadership. It was a surprise for us that C12, was considered one of the less significant competences, which opposes results and conclusions from [25]. Concluding, each HEI has to select its own set of ITG practices, suitable for their dimension, culture and level of ITG/ITVM maturity. Of course, this research has limitations. First, the study is limited to the Portuguese context, to extend the study to international HEIs would be a plus. Second, to identify which resources and/or competences would be appropriate for different organizational contexts: public, private, profit or non-profit, could be interesting to explore in future research. Cristiano Pereira et al. / Procedia Computer Science 138 (2018) 612â€“620 617 Author name / Procedia Computer Science 00 (2018) 000â€“000 5 In temporal terms, the study took place over twelve weeks (December 2017 to March of 2018). The first moment of interaction was the sending of invitations to participate in the study to 93 experts by e-mail, three of them declined their participation. In Round 1, the questionnaire was send to 90 experts. In subsequent rounds, the questionnaire was only sent to experts who completed the previous round. Response rates obtained were of 47% in Round 1, 60% in Round 2 and 84% in Round 3. After Round 1, only one new item proposal emerged. After examining the existence of similarities or overlaps between the initial list and the proposed item, we assumed that the new proposed item was already considered, therefore, we did not include it. In conclusion, the initial list, stated in Appendix A, turned out to be the final list evaluated by the experts. As stated in section 3.2.5, we have evaluated the consensus level at the end of each round using two statistical measures, KendallÂ´s W and Spearmanâ€™s rho. Supported by Schmidt, [23] to interpret KendallÂ´s W, the following consensus categories were considered: (1) very low or low for 0,01<W<0,29; (2) moderated for 0,30<W<0,50; (3) high or very high for 0,51<W<0,99. Stability of global opinion between rounds is considered very high for Spearman rho >0,90 [24]. Taking into account the values of the statistical tests performed, global level of consensus evaluated in Round 1 and 2 were very low and low, respectively. Evaluation at the end of Round 3 revealed a moderated global level consensus. Bearing in mind the high number of items involved (39 items), the very high stability in global panel rank between rounds and the substantial decrease in participants at the end of Round 3, we considered this a tolerable level of consensus and, therefore, considered it a positive panel convergence, thus leading to the conclusion of the Q-Sort Delphi study. 4. Research Results and Discussion Results reflect the opinion and experience of 21 experts who participated in all three rounds, 20% belong to the IS/IT Industry, 66% to the academy group and 14% to the university IS/IT managers group. The evolution in items rankings are summarized in Table 2. We used the exploratory cluster analysis technique (â€œWard's Method\" with the measure of similarity \"Square Euclidean Distance\") to organize data in significant groups and support our analysis. DendogramÂ´s results allows us to identify four clusters for competences and five clusters for enablers. Table 2 â€“ Ranking of importance of items ITVM Enabler Items Ranking ITVM Competence Items Ranking Round 1 Round 2 Round 3 Enabler Cluster Round 1 Round 2 Round 3 Competences Cluster 2 1 1 E22 Most Significant 1 2 1 C4 Most Significant 1 2 2 E14 7 4 2 C5 4 3 3 E19 Significant 2 1 3 C3 5 8 4 E3 3 3 4 C14 Significant 3 4 5 E10 8 5 5 C1 10 6 6 E23 4 7 6 C7 Neutral 6 7 7 E9 9 9 7 C13 8 5 8 E25 Moderated 6 6 8 C9 7 9 9 E2 5 8 9 C8 16 14 10 E16 10 10 10 C10 Less Significant 18 12 11 E8 11 12 11 C11 11 11 12 E21 12 11 12 C2 14 16 13 E17 Neutral 13 13 13 C12 19 17 14 E18 14 14 14 C6 13 15 15 E6 22 21 16 E7 9 10 17 E5 17 13 18 E1 12 20 19 E12 15 19 20 E11 Less Significant 20 18 21 E15 21 22 22 E4 25 25 23 E13 23 24 24 E24 24 23 25 E20 6 Author name / Procedia Computer Science 00 (2018) 000â€“000 In addition, we requested experts to rank competences according to the level of importance they considered that best contributes to ITVM organizational capability in Portuguese HEIs. After the first phase, a second request, related to the first, was made, in order to rank the enablers according to what experts assumed as the best in the development of competences ranked previously. From Table 2, some relevant aspects should be highlight. First, the importance recognized to competence C4, which, in all rounds, occupied the top positions, emphasizing the importance of an ITVM model framed with ITG. This corroborates the importance pointed out by Maes et al. [12], regarding the adoption of a model to implement such type of practices. Secondly, the consistency in all rounds associated to the first five positions in the ranking of competences. Only two exceptions: the exclusion in Round 2 of competence C8, which in Round 1 ranked in the 5th position and finished, in Round 3, in position 9, as well as competence C7, initially in position 4, that finished the study in position 6. In the opposite direction, C5 and C1 started outside the top five and ended in Round 3 in the 2nd and 5th position respectively. More evident are the results regarding the items placed in less significant positions of the ranking, which in the three rounds are occupied by the same set of competences (C10, C11, C2, C12 and C6). What concerns the enabler items ranking evolution, the most significant enablers are E22 and E14, permanently in the first two positions in all the three Rounds. These are the two most important organizational resources to develop ITVM competences. Finally, we can highlight the strong consistency across the three rounds regarding the less significant enablers (E4, E13, E24 and E20). The same stability is verified in the top five positions in all rounds. An exception is E3, which in the 2nd Round, ranked in the 8th position, returning to the Top 5 in Round 3. With the purpose of setting up a minimum baseline to assist academic and practitioners understanding the essential, requirements or items, to implement our ITVM capability model in Portuguese universities, we based on C. Pereira et al. (6), and describe, in the next paragraph, the main relationships between the greatest significant enablers that contribute to deploy the most important competences. Following our results, we conclude that E22 and E14 contribute to deploy C4 and C3. The 3rd ranking enabler E19 contributes to deploy C14 and C1. Enabler E3 also helps in the deployment of competences C14 and C5. Like E22 and E14, enabler E10, contributes to deploy competences C4 and C3. The sixth ranking enabler, E23, contributes to deploy C5 and C1, ITVM competences. The enabler of the cluster, classified as significant, E9, contributes to the four first competences (C4, C5, C3 and C14). Two categories of enablers are not presented in the proposed minimum baseline: i) Culture, Ethics and Behavior and ii) Infrastructures and Services, which does not mean that they are not important. From this results we verify that enabler E2 (Culture, Ethics and Behavior category) is categorized by experts as moderated, while enabler E7 (Infrastructures & Services) with a neutral impact. The lack of enablers from the category Culture, Ethics and Behavior, in the most significant or significant cluster was a surprise to us. According to C. Pereira et al. [17] this category is important to maximize the benefits of IT adoption, to reevaluate the investment portfolio. Several studies have seen organizational culture as an important factor that may explain significant variations in IT business value and is essential to change organizational culture, once this change involves peopleÂ´s value, attitudes and behaviors. 5. Conclusions From an academic point of view, this study gives an exploratory light on the issue of IT value management to support Portuguese HEIs, providing guidance about â€œwhatâ€� improves value from IT investments. This study revealed that, according to the experts panel HEIs could leverage a wide range of ITVM competences and enablers to support the development of an organizational ITVM capability, important to achieve value creation of IT-enabled investments. As stated in section 1, ITG has a great influence on ITVM, to corroborate this, we verify that E22 and E14 contribute to deploy competence C4, in other words, knowledge and the implementation of ITG frameworks and processes help institutions to gain competences to create ITVM practices. C4 is in line with issues acknowledged by international HEIs (7), especially institutional adaptiveness related to repositioning the role of IT leadership as a strategic partner of institutional leadership. It was a surprise for us that C12, was considered one of the less significant competences, which opposes results and conclusions from [25]. Concluding, each HEI has to select its own set of ITG practices, suitable for their dimension, culture and level of ITG/ITVM maturity. Of course, this research has limitations. First, the study is limited to the Portuguese context, to extend the study to international HEIs would be a plus. Second, to identify which resources and/or competences would be appropriate for different organizational contexts: public, private, profit or non-profit, could be interesting to explore in future research. 618 Cristiano Pereira et al. / Procedia Computer Science 138 (2018) 612â€“620 Author name / Procedia Computer Science 00 (2018) 000â€“000 7 Acknowledgements This work has been supported by COMPETE: POCI-01-0145-FEDER-007043 and FCT â€“ FundaÃ§Ã£o para a CiÃªncia e Tecnologia within the Project Scope: UID/CEC/00319/2013. Appendix A. List of items evaluated by the experts panel ID Competences Items â€“ Part I Description C1 Benefits planning and delivery Competence to identify, plan and ensure realization of benefits. Ability to understand expected benefits and manage their performance throughout the life cycle of the investment. C2 Business performance improvement Competence to establish a philosophy of continuous improvement of ITVM practices. Ability to identify, implement and control corrective actions required to optimize value of IT initiatives. C3 Establish an enlightened IT relationship with stakeholders Competence to manage and align IT function with stakeholders. Ability to establish a culture of collaboration, partnerships, with internal and external entities. C4 Establish an IT value management model Competence to create an ITVM model, within institution's governance context. Ability to define policies, roles and responsibilities, ensure IT decision-making is in line with institution's strategy. C5 Establish business strategic direction Competence to ensure that institution's strategy incorporates new opportunities triggered by IT. Ability to align institution's strategies with IT strategies (e.g., budgets, resources) C6 Financial planning Competence to define and establish financial planning practices for effective management of IT assets. Ability to manage and make available financial information (e.g. acquisition and ownership costs) to prepare business cases for new IT investments. C7 Identify IT insights and innovations Competence to recognize ideas and improvement opportunities triggered by emerging technologies. Ability to capture, collect, classify and understand new ideas. C8 IS/IT staff development Competence to optimize skills and knowledge of IT collaborators. Ability to train, develop, and certify human resource skills, ensure technical, organizational, and personal skills. C9 Managing change Competence to keep up organizational changes. Ability to evaluate, adjust and produce changes resulting from evolution and correctness during the implementation of business cases. C10 Post implementation evaluation Competence to monitor performance of investment mix. Ability to set goals, metrics, monitor and report evolution and implementation of projects results (e.g. expected value vs. realized value). C11 Prioritize Competence to prioritize proposed investments. Ability to analyze and prioritize investment mix based on criteria, such financial cost, associated risk, alignment with strategy and potential value. C12 Structure a business case Competence to draw a business case document. Ability to define content that must be present in a business case (e.g., expected benefits, costs and risks) to enable efficient decision-making. C13 Structure IT services and projects Competence to define and manage IT services and projects. Ability to specify project management model, scope, resources, indicate project managers and teams, funding, timelines and interdependencies between projects, plan and control deviations from initial planning. C14 Structuring IT portfolio and IT investment criteria Competence to constitute and characterize portfolio or \"collection\" of IT investments. Ability to establish benchmarks and investment categories based on size and relative weight. ID Enabler Items â€“ Part II Description E1 Culture, Ethics and Behavior - Incentives and rewards Promotes incentive mechanisms and rewards to encourage employees adopt appropriate behavior, through incentive schemes and rewards. E2 Culture, Ethics and Behavior - Communication and awareness Directly related to individual and collective behaviors. Adapt communication of desired behaviors and values to the culture of institution, communicating effectively, regulations, norms and rules. Leadership should raise awareness of desired behavior through example. E3 Information - Budget and IT investment criteria Information and disclosure of IT budget as well criteria for classification of investment proposals. E4 Information - Constitution/bylaws/statutes Information or documentation about laws and institution statutes, which must be met when making decisions and executing investments. E5 Information - Investment performance appraisal and reporting Information produced and disclosed on evolution of execution, performance and conclusion of investments and information of realized benefits (realized value). Cristiano Pereira et al. / Procedia Computer Science 138 (2018) 612â€“620 619 Author name / Procedia Computer Science 00 (2018) 000â€“000 7 Acknowledgements This work has been supported by COMPETE: POCI-01-0145-FEDER-007043 and FCT â€“ FundaÃ§Ã£o para a CiÃªncia e Tecnologia within the Project Scope: UID/CEC/00319/2013. Appendix A. List of items evaluated by the experts panel ID Competences Items â€“ Part I Description C1 Benefits planning and delivery Competence to identify, plan and ensure realization of benefits. Ability to understand expected benefits and manage their performance throughout the life cycle of the investment. C2 Business performance improvement Competence to establish a philosophy of continuous improvement of ITVM practices. Ability to identify, implement and control corrective actions required to optimize value of IT initiatives. C3 Establish an enlightened IT relationship with stakeholders Competence to manage and align IT function with stakeholders. Ability to establish a culture of collaboration, partnerships, with internal and external entities. C4 Establish an IT value management model Competence to create an ITVM model, within institution's governance context. Ability to define policies, roles and responsibilities, ensure IT decision-making is in line with institution's strategy. C5 Establish business strategic direction Competence to ensure that institution's strategy incorporates new opportunities triggered by IT. Ability to align institution's strategies with IT strategies (e.g., budgets, resources) C6 Financial planning Competence to define and establish financial planning practices for effective management of IT assets. Ability to manage and make available financial information (e.g. acquisition and ownership costs) to prepare business cases for new IT investments. C7 Identify IT insights and innovations Competence to recognize ideas and improvement opportunities triggered by emerging technologies. Ability to capture, collect, classify and understand new ideas. C8 IS/IT staff development Competence to optimize skills and knowledge of IT collaborators. Ability to train, develop, and certify human resource skills, ensure technical, organizational, and personal skills. C9 Managing change Competence to keep up organizational changes. Ability to evaluate, adjust and produce changes resulting from evolution and correctness during the implementation of business cases. C10 Post implementation evaluation Competence to monitor performance of investment mix. Ability to set goals, metrics, monitor and report evolution and implementation of projects results (e.g. expected value vs. realized value). C11 Prioritize Competence to prioritize proposed investments. Ability to analyze and prioritize investment mix based on criteria, such financial cost, associated risk, alignment with strategy and potential value. C12 Structure a business case Competence to draw a business case document. Ability to define content that must be present in a business case (e.g., expected benefits, costs and risks) to enable efficient decision-making. C13 Structure IT services and projects Competence to define and manage IT services and projects. Ability to specify project management model, scope, resources, indicate project managers and teams, funding, timelines and interdependencies between projects, plan and control deviations from initial planning. C14 Structuring IT portfolio and IT investment criteria Competence to constitute and characterize portfolio or \"collection\" of IT investments. Ability to establish benchmarks and investment categories based on size and relative weight. ID Enabler Items â€“ Part II Description E1 Culture, Ethics and Behavior - Incentives and rewards Promotes incentive mechanisms and rewards to encourage employees adopt appropriate behavior, through incentive schemes and rewards. E2 Culture, Ethics and Behavior - Communication and awareness Directly related to individual and collective behaviors. Adapt communication of desired behaviors and values to the culture of institution, communicating effectively, regulations, norms and rules. Leadership should raise awareness of desired behavior through example. E3 Information - Budget and IT investment criteria Information and disclosure of IT budget as well criteria for classification of investment proposals. E4 Information - Constitution/bylaws/statutes Information or documentation about laws and institution statutes, which must be met when making decisions and executing investments. E5 Information - Investment performance appraisal and reporting Information produced and disclosed on evolution of execution, performance and conclusion of investments and information of realized benefits (realized value). 8 Author name / Procedia Computer Science 00 (2018) 000â€“000 E6 Information - Service Level Agreements (SLA) Information and advertising of requirements and service levels agreed between suppliers (internal / external) and customers or users of IT services. E7 Infrastructures & Services - Sourcing Decision Related to decision making about model services, systems and IT infrastructures, namely with internal resources or outsourcing of solutions and services. E8 Organizational Structures - Board Group of executives responsible for governance and general control of resources. In context of public universities in Portugal, is equivalent to governing body, general council of the university. E9 Organizational Structures - C-suite executive Organizational structures of executive level composed of directors for each area of activity. In the context of HEIs, the service director structures (administrator and service directors) and /or the rector team (rector / vice-rector / pro-dean) with executive responsibilities. E10 Organizational Structures - IT strategic committee Group of senior executives appointed by board of directors/general council, ensures general council informed of IT issues. Responsible for executive management of services, assets and portfolio of IT investments. Committee usually chaired by a board member rather than IT director. E11 Organizational Structures - Project Management Office Responsible for support project managers, evaluates and reports project execution (costs, human resources, quality, time), define methodologies, standards and tools used in project management. E12 Organizational Structures - Value Management Office Act in management of IT investment portfolio, assesses and advises on investment opportunities. Support IT strategic committee evaluating business cases, track investments, report progress and value generated. E13 People and Skills - Knowledge of financial management Related to skills and competences in the area of financial management in public institutions. E14 People and Skills - Knowledge of frameworks for governance of IT Related to skills and competences, individual and collective, in IT governance and management frameworks (e.g. project management, innovation management, IT vendor management) E15 People and Skills - Knowledge of HR management Related to skills and competences in the area of HR management in public institutions. E16 Principles, Policies & frameworks - Enterprise Governance Principles Practical guidelines for governance and day-to-day management, apply across business and IT (e.g. ISO / IEC 38500; OECD Principles of Governance), provide instructions from board of directors and executive officers that clarify objectives and institution values. E17 Principles, Policies & frameworks - Management and resource allocation policies Establishes criteria and terms for allocating resources to investment projects, financial or human resources, defining practices to develop improvement of individual performance. E18 Principles, Policies & frameworks - Monitoring and Reporting Policy Establishes and communicates practices for monitoring and reporting benefits realized during investments life cycle. E19 Principles, Policies & frameworks - Portfolio and IT Investments Policy Formulates institution policies (IT / business), identification acceptable level of risk and level of costs versus expected benefits, categorizes and evaluates risks and investments to select according defined criteria. E20 Principles, Policies & frameworks - Vendor Management Policy Establishes terms and good practices of \"sourcing\" and relationship management with IT suppliers. E21 Process - Ensure Benefits Delivery Optimize potential value of IT processes, services and assets resulting from IT investments. Manage realized value, in face of costs and benefits, guarantee delivery of services and solutions with value for institution. E22 Process - Governance and IT Management Clarify and maintain mission and vision of IT in institution. Defines organizational structures (roles and responsibilities) of governance, management and use of IT, specifies IT management activities, procedures, respective skills, and competencies. Ensure that IT-related decision-making aligned with institution's objectives, IT management processes supervised and regulatory and legal requirements fulfilled. E23 Process - Manage Portfolio and Projects Manage project portfolio in a coordinated way, considering each category of investment, resources and financial constraints. Evaluate, prioritize, plan, control and execute projects and close them with a post-implementation review. Manage demand for resources and funding in entire portfolio involving and communicating with all stakeholders. E24 Process - Manage Suppliers Manage relationship with IT suppliers. Manage relationships, contracts, supplier performance review and monitoring, minimize risk and ensure competitive pricing for IT goods and services. E25 Process - Managing Innovation and Organizational Change Manage adoption of innovation and organizational changes triggered by IT. Analyze, validate and implement innovation opportunities in business processes and services, maximizing probability of success of organizational changes. Prepare and commit all stakeholders to change. 620 Cristiano Pereira et al. / Procedia Computer Science 138 (2018) 612â€“620 Author name / Procedia Computer Science 00 (2018) 000â€“000 9 References [1] C. Wilkin, J. Campbell, S. Moore, and W. Van Grembergen, â€œCo-Creating Value from IT in a Contracted Public Sector Service Environment: Perspectives on COBIT and Val IT,â€� J. Inf. Syst., vol. 27, no. 1, pp. 283â€“306, Jun. 2013. [2] C. Pereira, C. Ferreira, and L. Amaral, â€œEstruturas Organizacionais para GestÃ£o do Valor das TI: Um Modelo Conceptual para as Universidades PÃºblicas em Portugal,â€� in 15a ConferÃªncia da AssociaÃ§Ã£o Portuguesa de Sistemas de InformaÃ§Ã£o - CAPSI 2015, 2015. [3] ITGI, Enterprise Value: Governance of IT Investments, The Val IT Framework 2.0. 2008. [4] ISACA, COBIT 5 - A Business Framework for the Governance and Management of Enterprise IT. 2012. [5] S. Grajek, â€œTop 10 IT Issues, 2018: The Remaking of Higher Education,â€� EDUCAUSE review, 2018. [6] S. Nevo and M. Wade, â€œFirm-level benefits of IT-enabled resources: A conceptual extension and an empirical assessment,â€� J. Strateg. Inf. Syst., vol. 20, no. 4, pp. 403â€“418, Dec. 2011. [7] S. Ali, P. Green, and A. Robb, â€œInformation technology investment governance: What is it and does it matter?,â€� Int. J. Account. Inf. Syst., vol. 18, pp. 1â€“25, Sep. 2015. [8] P. Weill and J. W. Ross, IT Governance - How Top Performers Manage IT Decision Rights for Superior Results. Harvard Business School Press, 2004. [9] W. Van Grembergen, S. De Haes, and E. Guldentops, â€œStructures, Processes and Relational Mechanisms for IT Governance,â€� in Strategies for Information Technology Governance, IGI Global, 2004, pp. 1â€“36. [10] S. De Haes and W. Van Grembergen, Enterprise Governance of Information Technology - Achieving Alignment and Value, Featuring COBIT 5. Springer International Publishing, 2015. [11] ISO/IEC, â€œISO/IEC 38500:2015 - Information technology - Governance of IT for the organization.â€� 2015. [12] K. Maes, S. De Haes, and W. Van Grembergen, â€œDeveloping a Value Management Capability: A Literature Study and Exploratory Case Study,â€� Inf. Syst. Manag., vol. 32, no. 2, pp. 82â€“104, Apr. 2015. [13] R. Ashrafi and J. Mueller, â€œDelineating IT Resources and Capabilities to Obtain Competitive Advantage and Improve Firm Performance,â€� Inf. Syst. Manag., vol. 32, no. 1, pp. 15â€“38, Jan. 2015. [14] J. Barney, â€œFirm Resources and Sustained Competitive Advantage,â€� J. Manage., vol. 17, no. 1, pp. 99â€“120, 1991. [15] J. B. Barney, â€œLooking inside for Competitive Advantage,â€� Acad. Manag. Exec., vol. 9, no. 4, pp. 49â€“61, 1995. [16] P. Mikalef and A. Pateli, â€œInformation technology-enabled dynamic capabilities and their indirect effect on competitive performance: Findings from PLS-SEM and fsQCA,â€� J. Bus. Res., vol. 70, no. October 2016, pp. 1â€“16, Jan. 2017. [17] C. Pereira, C. Ferreira, and L. Amaral, â€œIT Value Management Capability Enabled with COBIT 5 Framework,â€� in Information Systems, vol. 299, M. Themistocleous and V. Morabito, Eds. Springer International Publishing, 2017, pp. 431â€“446. [18] C. Ashurst, E. Doherty, and J. Peppard, â€œImproving the impact of IT development projectsâ€¯: the Benefits Realization Capability Model,â€� Eur. J. Inf. Syst., vol. 17, no. 4, pp. 352â€“370, 2008. [19] H. A. Linstone and M. Turoff, The Delphi Method: Techniques and Applications. Reading, Mass.â€¯: Addison-wesley, 1975. [20] C. Okoli and S. D. Pawlowski, â€œThe Delphi method as a research tool: an example, design considerations and applications,â€� Inf. Manag., vol. 42, no. 1, pp. 15â€“29, Dec. 2004. [21] J. L. Worrell, P. M. Di Gangi, and A. A. Bush, â€œExploring the use of the Delphi method in accounting information systems research,â€� Int. J. Account. Inf. Syst., vol. 14, no. 3, pp. 193â€“208, Sep. 2013. [22] I. R. Diamond, R. C. Grant, B. M. Feldman, P. B. Pencharz, S. C. Ling, A. M. Moore, and P. W. Wales, â€œDefining consensus: A systematic review recommends methodologic criteria for reporting of Delphi studies,â€� J. Clin. Epidemiol., vol. 67, no. 4, pp. 401â€“409, Apr. 2014. [23] R. Schmidt, â€œManaging Delphi Surveys Using Nonparametric Statistical Techniques,â€� Decis. Scienses, vol. 28, no. 3, pp. 763â€“774, 1997. [24] G. Dhillon, R. Syed, and F. de SÃ¡-Soares, â€œInformation security concerns in IT outsourcing: Identifying (in) congruence between clients and vendors,â€� Inf. Manag., vol. 54, no. 4, pp. 452â€“464, 2017. [25] C. Pereira, C. Ferreira, and L. Amaral, â€œShape a Business Case Process: An IT Governance and IT Value Management Practices Viewpoint with COBIT 5.0,â€� in 17a ConferÃªncia da AssociaÃ§Ã£o Portuguesa de Sistemas de InformaÃ§Ã£o, 2017, pp. 60â€“75. TGI, Enterprise Value: Governance of IT Investments, The Val IT Framework 2.0. 2008. [4] ISACA, COBIT 5 - A Business Framework for the Governance and Management of Enterprise IT. 2012. [5] S. Grajek, â€œTop 10 IT Issues, 2018: The Remaking of Higher Education,â€� EDUCAUSE review, 2018. [6] S. Nevo and M. Wade, â€œFi PROCS 34888 S1877-0509(18)31728-9 10.1016/j.procs.2018.10.082 An IT Value Management Capability Model for Portuguese Universities: A Delphi Study Cristiano Pereira a Carlos Ferreira a Luis Amaral b a Universidade Aveiro, Aveiro, Portugal Universidade Aveiro Aveiro Portugal b Universidade do Minho, Guimarães, Portugal Universidade do Minho Guimarães Portugal One of the most common dilemmas faced today by organizations and their leaders is how to guarantee value from high level IT investments, i.e. how organizations ensure expected benefits from growth in IT investments. Knowledgeable about this reality, organizations seek solutions to solve this problem, either through the adoption of frameworks developed and proposed by the professional community (COBIT5; VAL IT 2.0; IT-CMF), or alternatively, by designing and implementing their own models. The aforementioned, for organizations in general, is not different in the context of Higher Education Institutions (HEIs). This paper adopts a Resource-Based View theory (RBV) to identify a set of competences and resources, which contribute to develop and conceptualize an IT Value Management Capability Model. The identified items were submitted to a panel of experts through a Delphi study in order to validate and propose a baseline to assist academic and practitioners understand essential requirements to implement an IT Value Management Capability Model (ITVMCM) in Portuguese public universities. Keywords IT Governance IT Value Management Capability Resource-Based View COBIT 5 VAL IT 2.0 References 1 C. Wilkin J. Campbell S. Moore W. Van Grembergen Co-Creating Value from IT in a Contracted Public Sector Service Environment: Perspectives on COBIT and Val IT J. Inf. Syst. 27 1 2013 283 306 Jun. 2 C. Pereira, C. Ferreira, and L. Amaral, “Estruturas Organizacionais para Gestão do Valor das TI: Um Modelo Conceptual para as Universidades Públicas em Portugal,” in 15a Conferência da Associação Portuguesa de Sistemas de Informação - CAPSI 2015, 2015. 3 ITGI, Enterprise Value: Governance of IT Investments, The Val IT Framework2.0. 2008. 4 ISACA, COBIT5 - A Business Framework for the Governance and Management of Enterprise IT. 2012. 5 S. Grajek Top 10 IT Issues, 2018: The Remaking of Higher Education EDUCAUSE review 2018 6 S. Nevo M. Wade Firm-level benefits of IT-enabled resources: A conceptual extension and an empirical assessment J. Strateg. Inf. Syst. 20 4 2011 403 418 Dec. 7 S. Ali P. Green A. Robb Information technology investment governance: What is it and does it matter? Int. J. Account. Inf. Syst. 18 2015 1 25 Sep. 8 P. Weill J.W. Ross IT Governance - How Top Performers Manage IT Decision Rights for Superior Results Harvard Business School Press 2004 9 W. Van Grembergen S. De Haes E. Guldentops Structures, Processes and Relational Mechanisms for IT Governance Strategies for Information Technology Governance 2004 IGI Global 1 36 10 S. De Haes W. Van Grembergen Enterprise Governance of Information Technology - Achieving Alignment and Value, Featuring COBIT5 2015 Springer International Publishing 11 ISO/IEC, “ISO/IEC 38500:2015 - Information technology - Governance of IT for the organization.” 2015. 12 K. Maes S. De Haes W. Van Grembergen Developing a Value Management Capability: A Literature Study and Exploratory Case Study Inf. Syst. Manag. 32 2 2015 82 104 Apr. 13 R. Ashrafi J. Mueller Delineating IT Resources and Capabilities to Obtain Competitive Advantage and Improve Firm Performance Inf. Syst. Manag. 32 1 2015 15 38 Jan. 14 J. Barney Firm Resources and Sustained Competitive Advantage J. Manage. 17 1 1991 99 120 15 J.B. Barney Looking inside for Competitive Advantage Acad. Manag. Exec. 9 4 1995 49 61 16 P. Mikalef A. Pateli Information technology-enabled dynamic capabilities and their indirect effect on competitive performance: Findings from PLS-SEM and fsQCA J. Bus. Res. 70 2017 2016 1 16 October, Jan. 17 C. Pereira C. Ferreira L. Amaral IT Value Management Capability Enabled with COBIT 5 Framework M. Themistocleous V. Morabito in Information Systems 2017 Springer International Publishing 431 446 vol. 299 18 C. Ashurst E. Doherty J. Peppard Improving the impact of IT development projects : the Benefits Realization Capability Model Eur. J. Inf. Syst. 17 4 2008 352 370 19 H.A. Linstone M. Turoff The Delphi Method: Techniques and Applications 1975 Addison-wesley Reading, Mass.  20 C. Okoli S.D. Pawlowski The Delphi method as a research tool: an example, design considerations and applications Inf. Manag. 42 1 2004 15 29 Dec. 21 J.L. Worrell P.M. Di Gangi A.A. Bush Exploring the use of the Delphi method in accounting information systems research Int. J. Account. Inf. Syst. 14 3 2013 193 208 Sep. 22 I. R. Diamond, R. C. Grant, B. M. Feldman, P. B. Pencharz, S. C. Ling, A. M. Moore, and P. W. Wales, “Defining consensus: A systematic review recommends methodologic criteria for reporting of Delphi studies,” J. Clin. Epidemiol., vol. 67, no. 4, pp. 401–409, Apr. 2014. 23 R. Schmidt Managing Delphi Surveys Using Nonparametric Statistical Techniques Decis. Scienses 28 3 1997 763 774 24 G. Dhillon R. Syed F. de Sá-Soares Information security concerns in IT outsourcing: Identifying (in) congruence between clients and vendors Inf. Manag. 54 4 2017 452 464 25 C. Pereira, C. Ferreira, and L. Amaral, “Shape a Business Case Process: An IT Governance and IT Value Management Practices Viewpoint with COBIT 5.0,” in 17a Conferência da Associação Portuguesa de Sistemas de Informação, 2017, pp. 60–75. "
    },
    {
        "doc_title": "Effect of hand-avatar in a selection task using a tablet as input device in an immersive virtual environment",
        "doc_scopus_id": "85018939288",
        "doc_doi": "10.1109/3DUI.2017.7893364",
        "doc_eid": "2-s2.0-85018939288",
        "doc_date": "2017-04-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            }
        ],
        "doc_keywords": [
            "button selection task",
            "hand-avatar",
            "Immersive virtual environments",
            "Input devices",
            "User study"
        ],
        "doc_abstract": "© 2017 IEEE.How does the virtual representation of the user's hands influence the performance on a button selection task performed in a tablet-based interaction within an immersive virtual environment? To answer this question, we asked 55 participants to use three conditions: no-hand avatar, realistic avatar and translucent avatar. The participants were faster but made slightly more errors while using the no-avatar condition, and considered easier to perform the task with the translucent avatar.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Covering-based rural road network methodology for hilly regions of developing countries: Application in Nepal",
        "doc_scopus_id": "85013128277",
        "doc_doi": "10.1061/JTEPBS.0000013",
        "doc_eid": "2-s2.0-85013128277",
        "doc_date": "2017-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Civil and Structural Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2205"
            },
            {
                "area_name": "Transportation",
                "area_abbreviation": "SOCI",
                "area_code": "3313"
            }
        ],
        "doc_keywords": [
            "Covering",
            "Minimum spanning trees",
            "Nodal points",
            "Real world road networks",
            "Road network",
            "User friendly"
        ],
        "doc_abstract": "© 2016 American Society of Civil Engineers.This work provides a practical approach for the definition of rural road networks in hilly regions of developing countries. The proposed methodology enables the determination of obligatory points for the road network, which provides basic accessibility to settlements within a specified maximum walking time. The proposed methodology, based on a geographic information system (GIS), takes into account the regions' main characteristics (i.e., trails slope and availability), and does not require significant data. Moreover, simplifications to the method are proposed herein, allowing it to be used even without a GIS; thus making it a more practical, realistic, and user-friendly approach for obtaining real-world road networks of hilly regions in developing countries.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Tax professionals' profiles concerning tax noncompliance and tax complexity: Empirical contributions from Portugal",
        "doc_scopus_id": "85042020362",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85042020362",
        "doc_date": "2017-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Accounting",
                "area_abbreviation": "BUSI",
                "area_code": "1402"
            },
            {
                "area_name": "Finance",
                "area_abbreviation": "ECON",
                "area_code": "2003"
            },
            {
                "area_name": "Economics and Econometrics",
                "area_abbreviation": "ECON",
                "area_code": "2002"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© School of Taxation and Business Law (Atax).This paper analyses the profiles of tax professionals with the greatest propensity for tax noncompliant behaviour. Data were collected in 2013 using a questionnaire applied to Portuguese tax professionals. From the profiles we argue that the propensity for noncompliance is greater among professionals responsible for medium-sized enterprises in contexts of high tax complexity. In particular, in terms of voluntary tax noncompliance, we noted greater vulnerability to the pressures exerted by employers/clients for participation in aggressive tax planning schemes among young women, as well as professionals working in in-house departments of accounting and taxation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Addressing a multi-objective capacitated location-routing problem using evolutionary algorithms",
        "doc_scopus_id": "85040930529",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85040930529",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            },
            {
                "area_name": "Industrial and Manufacturing Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2209"
            },
            {
                "area_name": "Safety, Risk, Reliability and Quality",
                "area_abbreviation": "ENGI",
                "area_code": "2213"
            }
        ],
        "doc_keywords": [
            "Capacitated location",
            "Computational experiment",
            "Distribution route",
            "Location routing problem",
            "Location-routing",
            "Multi objective",
            "Obnoxious facilities",
            "Semi-obnoxious"
        ],
        "doc_abstract": "In location-routing problems (LRP) the location of facilities and the corresponding distribution routes are addressed simultaneously, reflecting a common concern when designing logistics systems. The LRP considering capacitated routes and depots has been called capacitated location-routing problem (CLRP), where current studies focus on a single objective: Cost minimization. Although this approach is suitable for determining the location-routing of most (desirable) facilities, for (semi-)obnoxious facilities, as other objectives also gain relevance, multi-objective approaches should be used. This paper proposes two evolutionary algorithms for a multi-objective CLRP. The algorithms are based on two well-known frameworks, which are hybridized with local search procedures. Computational experiments were carried out on a set of benchmark instances adapted from the single-objective literature. Results show that local search procedures allow obtaining a better approximation of the Pareto front and reinforce the belief that multi-objective approaches should be used in LRPs when other objectives besides cost have to be considered.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "It value management capability enabled with COBIT 5 framework",
        "doc_scopus_id": "85028683876",
        "doc_doi": "10.1007/978-3-319-65930-5_35",
        "doc_eid": "2-s2.0-85028683876",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Control and Systems Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2207"
            },
            {
                "area_name": "Management Information Systems",
                "area_abbreviation": "BUSI",
                "area_code": "1404"
            },
            {
                "area_name": "Business and International Management",
                "area_abbreviation": "BUSI",
                "area_code": "1403"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "COBIT 5",
            "Competitive advantage",
            "Financial returns",
            "IT governance",
            "Resource-based theory",
            "Resource-based view",
            "Strategic factors",
            "Value management"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.One of the most common dilemmas faced today by organizations and their leaders is how to guarantee value from the high level IT investments, i.e. how organizations ensure expected benefits from this growth in IT investments. A superior understanding of how to deliver value to the business from IT initiatives is critical. Value should not be view only as a financial return, but also as other strategic factors that affect the business. This paper adopts a resource-based theory perspective to identify and propose a set of competences, resources, and practices, which contribute to develop and conceptualize an IT Value Management Capability Model, with an oriented practical perspective to existing IT Value Management professional frameworks, namely; COBIT 5 and Val IT 2.0. Based on literature findings, our model supports managers on developing resources, practices, and competences that contribute to business value and competitive advantage.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Shape a business case process: An IT governance and IT value management practices viewpoint with COBIT 5.0",
        "doc_scopus_id": "85028677724",
        "doc_doi": "10.18803/capsi.v17.60-75",
        "doc_eid": "2-s2.0-85028677724",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Management Information Systems",
                "area_abbreviation": "BUSI",
                "area_code": "1404"
            },
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Business case",
            "COBIT5",
            "Enterprise IT",
            "VAL IT 2.0",
            "Value management"
        ],
        "doc_abstract": "The growth in IT investments increases the concern of organizations and their leaders to ensure the expected business value from these investments. Business value derived only emerges through business changes and innovations, i.e., organizational transformation. Organizational change through digital transformation is a fundamental component for business value resulting from IT investments and a driver of further change. Organizations perceive the Business Case as a critical instrument to realize the potential value from IT. A well-developed and intelligently used business case is one of the most valuable tools available to management. This paper embraces an oriented practical perspective to existing Enterprise Governance of IT and IT Value Management professional frameworks. A literature review methodology is performed in academic and practitioner literature. Based on literature findings, we propose a based model that supports organizations in development a maintenance of a Business Case Process grounded on COBIT 5.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Freehand gesture-based 3D manipulation methods for interaction with large displays",
        "doc_scopus_id": "85025152989",
        "doc_doi": "10.1007/978-3-319-58697-7_10",
        "doc_eid": "2-s2.0-85025152989",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D object manipulations",
            "3D user interface",
            "Hand gesture",
            "Large displays",
            "User study"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Gesture-based 3D interaction is a research topic with application in numerous scenarios which gained relevance with the recent advances in low-cost tracking systems. Yet, it poses many challenges due to its novelty and consequent lack of systematic development methodologies. Developing easy to use and learn gesture-based 3D interfaces is particularly difficult since the most adequate and intuitive gestures are not always obvious and there is often a variety of different gestures used to perform similar actions. This paper presents the development and evaluation of interaction methods to manipulate 3D virtual objects in a large display set-up using freehand gestures detected by a Kinect depth sensor. We describe the implementation of these methods and the user studies conducted to improve them and assess their usability as manipulation methods. Based on the results of these studies we also propose a method that overcomes the lack of roll movement detection by the Kinect and makes simpler the scaling and rotation in all degrees-of-freedom using hand gestures.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation methods to support health information systems development: A framework supported in literature and practical experience",
        "doc_scopus_id": "85025117076",
        "doc_doi": "10.1007/978-3-319-58466-9_8",
        "doc_eid": "2-s2.0-85025117076",
        "doc_date": "2017-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Evaluation methods",
            "Formative",
            "Health information systems",
            "SDLC",
            "Summative",
            "User-centred"
        ],
        "doc_abstract": "© Springer International Publishing AG 2017.Given the diversity and complexity of the Health Information Systems (HIS), and taking into account the impact of this type of systems in the clinical performance and patient outcome, a rigorous evaluation process in the system development life cycle (SDLC) is extremely important. An effective evaluation during development not only promotes the quality of the final solution, but also ensures motivated users, error-free systems, and can even establish good practices to minimize costs in future developments. However, the HIS evaluation is a difficult process due to the complex nature of the health care domain, the objects being evaluated, as well as the comprehensiveness of the concept of the evaluation itself. The present work intends to explore, based on a literature review, the main methods of HIS evaluation to support the development, identifying in which stage of the SDLC these methods can be applied. Additionally, this work discusses the reasons for the evaluation of such systems, illustrating these issues with two real case studies of HIS implementations, in which some of the methods were successfully applied.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Portuguese haemophilia registry: Set of variables for a computerized solution",
        "doc_scopus_id": "85019495052",
        "doc_doi": "10.5482/HAMO-15-09-0027",
        "doc_eid": "2-s2.0-85019495052",
        "doc_date": "2017-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Hematology",
                "area_abbreviation": "MEDI",
                "area_code": "2720"
            }
        ],
        "doc_keywords": [
            "Confidentiality",
            "Database Management Systems",
            "Databases, Factual",
            "Hemophilia A",
            "Humans",
            "Information Storage and Retrieval",
            "Medical Record Linkage",
            "Medical Records Systems, Computerized",
            "Portugal",
            "Registries",
            "Data Collection",
            "Databases, Factual",
            "Hemophilia A",
            "Portugal",
            "Software Design"
        ],
        "doc_abstract": "© Schattauer 2017.National Patient Registries (NPR) have an important role in the management of haemophilia and other inherited bleeding disorders, representing powerful instruments to support healthcare and research. Computer software to assist the NPR is crucial, as it facilitates the introduction of the data from a national universe that will be centralized and merged into a unique location, thus ensuring a greater reliability and accuracy of the collected data, avoiding duplication of patients. In Portugal, despite the efforts and recognition of the need of a NPR, just recently the protocol for the establishment of the computer software to support the Portuguese National Registry of Haemophilia and other Congenital Coagulopathies (PorR H&CC) was approved. This paper aims to present this newly developed computerized solution, as well as to report the main variables and information that will be available. The development of this application, which includes a set of socio-demographic, clinical and treatment data, was based on the principles of WFH, and the database that supports the NPR, with anonymized data, is operated and maintained in accordance with the Data Protection Law. Currently, the first data are available on the application. Our focus now is to ensure more registries and continuous data entry in order to have complete information on the characterization of the haemophilia patient population in Portugal.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Goodwill and Mandatory Disclosure Compliance: A Critical Review of the Literature",
        "doc_scopus_id": "85010378552",
        "doc_doi": "10.1111/auar.12129",
        "doc_eid": "2-s2.0-85010378552",
        "doc_date": "2016-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Accounting",
                "area_abbreviation": "BUSI",
                "area_code": "1402"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2016 CPA AustraliaThe aim of this paper is to critically analyse the literature published from 2002 to mid-2015 on disclosures of goodwill and their respective impairment tests; identify the main contributions of the literature, as well as its limitations; and suggest new approaches for future research. We also present a summary of the main determinants of disclosures on goodwill in the literature as well as the still scarce studies’ conclusions of those disclosures in the market. After a review of the literature, we discuss the need to reinforce enforcement mechanisms, so as to improve the level of compliance of disclosures on goodwill and their impairment tests. The majority of the analysed literature reveals that the information disclosed about goodwill is incomplete and largely heterogeneous, indicating a reduced level of compliance with the disclosures required by accounting standards. The paper may support the development of future empirical studies about goodwill's disclosures, which will bridge the identified gaps in the literature.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Developing 3D freehand gesture-based interaction methods for virtual walkthroughs: Using an iterative approach",
        "doc_scopus_id": "85014148227",
        "doc_doi": "10.4018/978-1-5225-0435-1.ch003",
        "doc_eid": "2-s2.0-85014148227",
        "doc_date": "2016-06-29",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3D interactions",
            "Controlled experiment",
            "Development methodology",
            "Gesture tracking",
            "Gesture-based interaction",
            "Iterative approach",
            "Navigation methods",
            "Research topics"
        ],
        "doc_abstract": "© 2016 by IGI Global. All rights reserved.Gesture-based 3D interaction has been considered a relevant research topic as it has a natural application in several scenarios. Yet, it presents several challenges due to its novelty and consequential lack of systematic development methodologies, as well as to inherent usability related problems. Moreover, it is not always obvious which are the most adequate and intuitive gestures, and users may use a variety of different gestures to perform similar actions. This chapter describes how spatial freehand gesture based navigation methods were developed to be used in virtual walkthroughs meant to be experienced in large displays using a depth sensor for gesture tracking. Several iterations of design, implementation, user tests, and controlled experiments performed as formative and summative evaluation to improve, validate, and compare the methods are presented and discussed.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A simple and effective evolutionary algorithm for the capacitated location-routing problem",
        "doc_scopus_id": "84955451591",
        "doc_doi": "10.1016/j.cor.2016.01.006",
        "doc_eid": "2-s2.0-84955451591",
        "doc_date": "2016-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            }
        ],
        "doc_keywords": [
            "Capacitated location",
            "Computational evaluation",
            "Computing time",
            "Hybrid genetic algorithms",
            "Local search",
            "Location-routing"
        ],
        "doc_abstract": "© 2016 Elsevier Ltd. All rights reserved.This paper proposes a hybrid genetic algorithm (GA) to solve the capacitated location-routing problem. The proposed algorithm follows the standard GA framework using local search procedures in the mutation phase. Computational evaluation was carried out on three sets of benchmark instances from the literature. Results show that, although relatively simple, the proposed algorithm is effective, providing competitive results for benchmark instances within reasonable computing time.",
        "available": true,
        "clean_text": "serial JL 271709 291210 291692 291715 291772 291813 291817 291871 31 Computers & Operations Research COMPUTERSOPERATIONSRESEARCH 2016-01-12 2016-01-12 2016-01-25 2016-01-25 2016-02-22T12:57:38 S0305-0548(16)00008-3 S0305054816000083 10.1016/j.cor.2016.01.006 S300 S300.1 FULL-TEXT 2016-09-02T17:19:40.919789-04:00 0 0 20160601 20160630 2016 2016-01-12T07:20:26.473949Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist yearnav figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast grantnumber grantsponsor highlightsabst primabst ref 0305-0548 03050548 true 70 70 C Volume 70 14 155 162 155 162 201606 June 2016 2016-06-01 2016-06-30 2016 article fla Copyright © 2016 Elsevier Ltd. All rights reserved. ASIMPLEEFFECTIVEEVOLUTIONARYALGORITHMFORCAPACITATEDLOCATIONROUTINGPROBLEM LOPES R 1 Introduction 2 Literature review 3 Evolutionary algorithm 3.1 Chromosome representation 3.2 Crossover operator 3.3 Mutation operators 3.3.1 Add and swap operators 3.3.2 Local search procedures 3.4 General structure 4 Computational evaluation 4.1 Implementation 4.2 Benchmark instances 4.3 Results 5 Conclusions Acknowledgments References BALDACCI 2011 1284 1296 R BARRETO 2007 968 977 S BELENGUER 2011 931 941 J BRANCO 1990 86 95 I CONTARDO 2013 263 295 C CONTARDO 2014 1 38 C CONTARDO 2014 88 102 C DREXL 2015 283 308 M DUHAMEL 2010 1912 1923 C ESCOBAR 2013 70 79 J ESCOBAR 2014 344 356 J FILHO 1998 189 209 V HANSEN 2006 802 817 P HEMMELMAYR 2012 3215 3228 V LAPORTE 1989 471 482 G LAPORTE 2000 285 300 G LIN 1973 498 516 S LOPES 2013 795 822 R MARINAKIS 2008 49 65 Y OSMAN 1993 421 451 I PRINS 2004 1985 2002 C PRINS 2006 183 194 C EVOLUTIONARYCOMPUTATIONINCOMBINATORIALOPTIMIZATIONLECTURENOTESINCOMPUTERSCIENCE AMEMETICALGORITHMPOPULATIONMANAGEMENTMAPMFORCAPACITATEDLOCATIONROUTINGPROBLEM PRINS 2006 221 238 C PRINS 2007 470 483 C PRODHON 2014 1 17 C SALHI 1999 3 19 S SAVELSBERGH 1992 146 154 M TING 2013 34 44 C TOTH 2003 333 346 P TUZUN 1999 87 99 D YU 2010 288 299 V LOPESX2016X155 LOPESX2016X155X162 LOPESX2016X155XR LOPESX2016X155X162XR 2019-01-25T00:00:00Z UnderEmbargo item S0305-0548(16)00008-3 S0305054816000083 10.1016/j.cor.2016.01.006 271709 2016-02-22T09:59:04.029387-05:00 2016-06-01 2016-06-30 true 403383 MAIN 8 57904 849 656 IMAGE-WEB-PDF 1 gr1 11275 83 219 gr2 10422 115 219 gr3 11292 107 219 gr4 10914 164 203 gr1 26364 86 226 gr2 45473 267 508 gr3 47717 247 508 gr4 51542 409 508 si0001 295 13 57 si0002 131 10 12 si0003 128 10 10 si0004 120 13 9 si0005 138 8 13 si0006 224 13 42 si0007 117 8 9 si0008 166 10 24 si0009 149 14 12 si0010 178 13 26 si0011 157 13 15 si0012 148 13 13 si0013 145 17 12 si0014 123 10 10 si0015 142 13 13 si0016 280 13 45 si0017 364 13 56 si0018 124 10 9 si0019 280 13 47 si0020 263 15 45 si0021 126 10 10 si0022 401 13 70 si0023 188 9 28 si0024 452 11 79 si0025 499 13 87 si0026 243 12 42 si0027 246 13 40 si0028 428 11 73 si0029 404 26 67 si0030 826 32 148 si0031 122 8 9 si0032 258 11 48 si0033 394 11 86 si0034 271 11 48 si0035 147 8 19 si0036 902 143 1925 si0037 165 13 25 si0038 139 10 22 si0039 165 8 23 si0040 177 13 22 CAOR 3915 S0305-0548(16)00008-3 10.1016/j.cor.2016.01.006 Elsevier Ltd Fig. 1 Chromosome representation of a solution for the CLRP. Fig. 1. Fig. 2 Example of the RCX operator. Fig. 2. Fig. 3 Examples of the add and swap mutation operators. Fig. 3. Fig. 4 Overview of the proposed hybrid GA. Fig. 4. Table 1 Results for the instances by Tuzun and Burke [31]. Table 1. Instance BKR HybridGA HybridGA+ CPU Average Best CPU Average Best Result Std dev Gap BKR Result Gap BKR Result Std dev Gap BKR Result Gap BKR 1 P111112 1467.68 19.0 1478.57 0.39 0.74 1470.05 0.16 78.3 1474.73 0.22 0.48 1469.54 0.13 2 P111122 1449.20 31.8 1460.62 0.40 0.79 1449.20 0.00 92.0 1453.42 0.19 0.29 1449.20 0.00 3 P111212 1394.80 18.1 1404.43 0.29 0.69 1396.46 0.12 70.5 1401.79 0.25 0.50 1395.80 0.07 4 P111222 1432.29 30.9 1433.09 0.04 0.06 1432.29 0.00 88.1 1432.63 0.03 0.02 1432.29 0.00 5 P112112 1167.16 21.2 1171.53 0.24 0.37 1167.53 0.03 78.5 1169.95 0.13 0.24 1167.53 0.03 6 P112122 1102.24 27.5 1106.10 0.17 0.35 1102.49 0.02 101.7 1103.64 0.06 0.13 1102.38 0.01 7 P112212 791.66 17.3 796.33 0.37 0.59 792.52 0.11 74.3 794.10 0.17 0.31 791.91 0.03 8 P112222 728.30 37.7 734.05 0.30 0.79 730.15 0.25 107.5 732.37 0.16 0.56 730.27 0.27 9 P113112 1238.24 17.1 1244.49 0.21 0.50 1239.61 0.11 72.9 1241.66 0.14 0.28 1238.49 0.02 10 P113122 1245.31 27.2 1247.15 0.04 0.15 1246.06 0.06 103.7 1246.45 0.03 0.09 1246.06 0.06 11 P113212 902.26 19.4 910.04 0.57 0.86 903.16 0.10 88.2 907.12 0.27 0.54 903.50 0.14 12 P113222 1018.29 28.1 1025.26 0.28 0.68 1019.72 0.14 99.1 1023.08 0.21 0.47 1019.39 0.11 13 P131112 1866.75 63.0 1936.10 0.34 3.71 1924.44 3.09 288.0 1930.80 0.28 3.43 1921.30 2.92 14 P131122 1823.20 81.0 1853.82 0.54 1.68 1837.67 0.79 316.7 1841.24 0.37 0.99 1830.75 0.41 15 P131212 1964.30 58.0 2007.21 0.38 2.18 1992.13 1.42 288.4 1989.59 0.20 1.29 1981.26 0.86 16 P131222 1792.80 81.2 1834.70 0.53 2.34 1818.80 1.45 322.1 1822.49 0.36 1.66 1813.72 1.17 17 P132112 1443.33 63.1 1475.33 0.50 2.22 1461.75 1.28 261.5 1465.21 0.34 1.52 1457.01 0.95 18 P132122 1434.60 78.1 1452.51 0.18 1.25 1447.88 0.93 389.9 1449.42 0.15 1.03 1446.46 0.83 19 P132212 1204.42 81.8 1219.07 0.35 1.22 1212.55 0.68 269.7 1214.56 0.17 0.84 1211.83 0.62 20 P132222 930.99 73.6 937.64 0.18 0.71 934.51 0.38 317.7 935.78 0.07 0.51 934.37 0.36 21 P133112 1694.18 53.9 1724.64 0.54 1.80 1709.67 0.91 356.8 1718.35 0.37 1.43 1708.63 0.85 22 P133122 1392.00 83.6 1409.31 0.26 1.24 1402.25 0.74 339.6 1406.15 0.22 1.02 1400.01 0.58 23 P133212 1198.20 59.5 1223.15 0.36 2.08 1214.87 1.39 234.9 1220.26 0.31 1.84 1213.93 1.31 24 P133222 1151.80 77.5 1161.55 0.20 0.85 1157.50 0.49 347.2 1159.92 0.15 0.71 1157.11 0.46 25 P121112 2243.40 121.7 2297.33 0.36 2.40 2280.97 1.67 623.0 2284.78 0.44 1.84 2270.18 1.19 26 P121122 2138.40 184.7 2213.79 0.54 3.53 2195.06 2.65 700.3 2199.78 0.63 2.87 2177.32 1.82 27 P121212 2209.30 125.8 2251.74 0.33 1.92 2237.80 1.29 615.8 2241.53 0.27 1.46 2230.37 0.95 28 P121222 2222.90 190.7 2272.27 0.48 2.22 2255.03 1.45 659.0 2261.76 0.32 1.75 2250.38 1.24 29 P122112 2073.70 137.5 2111.75 0.31 1.83 2100.21 1.28 593.3 2108.40 0.23 1.67 2101.75 1.35 30 P122122 1692.17 188.4 1723.88 0.37 1.87 1711.58 1.15 726.0 1722.06 0.31 1.77 1712.60 1.21 31 P122212 1453.18 138.8 1478.06 0.25 1.71 1472.38 1.32 689.6 1471.76 0.25 1.28 1466.64 0.93 32 P122222 1082.46 229.5 1094.81 0.11 1.14 1092.14 0.89 749.1 1090.44 0.17 0.74 1087.91 0.50 33 P123112 1954.70 137.4 1985.98 0.30 1.60 1977.48 1.17 700.6 1981.53 0.15 1.37 1976.97 1.14 34 P123122 1918.93 161.5 1976.13 0.41 2.98 1960.24 2.15 812.9 1963.24 0.35 2.31 1951.47 1.70 35 P123212 1762.00 139.6 1781.02 0.20 1.08 1774.48 0.71 630.4 1777.37 0.17 0.87 1772.91 0.62 36 P123222 1390.87 191.5 1418.24 0.59 1.97 1401.98 0.80 802.6 1406.67 0.32 1.14 1399.82 0.64 Average 86.0 0.33 1.45 0.87 363.6 0.23 1.09 0.71 Median 75.6 0.34 1.25 0.80 317.2 0.22 1.00 0.62 Table 2 Results for the instances by Barreto et al. [2]. Table 2 Instance LB BKR HybridGA HybridGA+ CPU Average Best CPU Average Best Result Std dev Gap BKR Result Gap BKR Result Std dev Gap BKR Result Gap BKR 1 Gaskell67 424.9 424.9 0.8 424.9 0.00 0.00 424.9 0.00 1.8 424.9 0.00 0.00 424.9 0.00 2 Gaskell67 585.1 585.1 0.6 585.1 0.00 0.00 585.1 0.00 1.5 585.1 0.00 0.00 585.1 0.00 3 Gaskell67 512.1 512.1 0.9 512.1 0.00 0.00 512.1 0.00 2.8 512.1 0.00 0.00 512.1 0.00 4 Gaskell67 562.2 562.2 1.1 562.2 0.00 0.00 562.2 0.00 3.5 562.2 0.00 0.00 562.2 0.00 5 Gaskell67 504.3 504.3 0.9 504.3 0.00 0.00 504.3 0.00 3.2 504.3 0.00 0.00 504.3 0.00 6 Gaskell67 460.4 460.4 1.4 460.4 0.00 0.00 460.4 0.00 4.7 460.4 0.00 0.00 460.4 0.00 7 Christofides69 565.6 565.6 4.2 565.6 0.00 0.00 565.6 0.00 16.1 565.6 0.00 0.00 565.6 0.00 8 Christofides69 810.9 844.4 23.1 853.7 0.69 1.10 846.5 0.25 62.4 849.2 0.41 0.57 844.4 0.00 9 Christofides69 833.4 833.4 17.2 839.0 0.36 0.67 833.4 0.00 82.4 835.3 0.24 0.23 833.4 0.00 10 Perl83 204.0 204.0 0.1 204.0 0.00 0.00 204.0 0.00 0.2 204.0 0.00 0.00 204.0 0.00 11 Perl83 1074.8 1112.1 20.9 1112.8 0.03 0.07 1112.1 0.00 48.5 1112.5 0.03 0.04 1112.1 0.00 12 Perl83 1568.1 1622.5 33.7 1623.6 0.04 0.07 1622.5 0.00 94.2 1622.8 0.02 0.02 1622.5 0.00 13 Perl83 557,275.2 256.7 580,397.2 0.92 4.15 572,038.3 2.65 4746.5 561,495.7 0.43 0.76 557,249.3 0.00 14 Perl83 670,118.5 215.0 685,951.5 0.55 2.36 680,246.9 1.51 1771.3 668,878.0 0.44 -0.19 663,070.0 −1.05 15 Min92 3062.0 3062.0 1.0 3062.0 0.00 0.00 3062.0 0.00 2.5 3062.0 0.00 0.00 3062.0 0.00 16 Min92 5423.0 5709.0 62.9 5794.7 0.62 1.50 5730.5 0.38 343.1 5732.8 0.25 0.42 5709.0 0.00 17 Daskin95 351.5 355.8 32.4 356.0 0.04 0.05 355.8 0.00 160.6 355.8 0.00 0.00 355.8 0.00 18 Daskin95 43,406.0 43,919.9 96.0 44,179.6 0.30 0.59 43,993.4 0.17 529.9 43,982.7 0.12 0.14 43,919.9 0.00 19 Or76 12,048.4 12,290.3 36.6 12,324.8 0.25 0.28 12,290.3 0.00 286.5 12,293.6 0.03 0.03 12,290.3 0.00 Average 42.4 0.20 0.57 0.26 429.6 0.10 0.11 −0.06 Median 17.2 0.03 0.05 0.00 48.5 0.00 0.00 0.00 Table 3 Results for the instances by Prins et al. [24]. Table 3 Instance LB BKR HybridGA HybridGA+ CPU Average Best CPU Average Best Result Std dev Gap BKR Result Gap BKR Result Std dev Gap BKR Result Gap BKR 1 20-5-1a 54,793 54,793 0.7 54,793 0.00 0.00 54,793 0.00 1.6 54,793 0.00 0.00 54,793 0.00 2 20-5-1b 39,104 39,104 0.6 39,104 0.00 0.00 39,104 0.00 1.5 39,104 0.00 0.00 39,104 0.00 3 20-5-2a 48,908 48,908 0.7 48,908 0.00 0.00 48,908 0.00 1.5 48,908 0.00 0.00 48,908 0.00 4 20-5-2b 37,542 37,542 0.5 37,542 0.00 0.00 37,542 0.00 1.4 37,542 0.00 0.00 37,542 0.00 5 50-5-1a 90,111 90,111 7.1 90,111 0.00 0.00 90,111 0.00 17.4 90,111 0.00 0.00 90,111 0.00 6 50-5-1b 63,242 63,242 4.0 63,242 0.00 0.00 63,242 0.00 14.5 63,242 0.00 0.00 63,242 0.00 7 50-5-2a 88,298 88,298 7.0 88,506 0.19 0.24 88,298 0.00 17.8 88,401 0.08 0.12 88,298 0.00 8 50-5-2b 67,308 67,308 4.1 67,402 0.12 0.14 67,308 0.00 16.8 67,315 0.01 0.01 67,308 0.00 9 50-5-2bis 84,055 84,055 6.4 84,055 0.00 0.00 84,055 0.00 15.1 84,055 0.00 0.00 84,055 0.00 10 50-5-2bbis 51,822 51,822 4.7 51,822 0.00 0.00 51,822 0.00 14.0 51,822 0.00 0.00 51,822 0.00 11 50-5-3a 86,203 86,203 7.1 86,203 0.00 0.00 86,203 0.00 16.2 86,203 0.00 0.00 86,203 0.00 12 50-5-3b 61,830 61,830 3.9 61,830 0.00 0.00 61,830 0.00 14.1 61,830 0.00 0.00 61,830 0.00 13 100-5-1a 274,814 274,814 28.4 277,376 0.23 0.93 276,490 0.61 73.2 277,320 0.20 0.91 276,467 0.60 14 100-5-1b 207,037 213,615 16.1 215,878 0.25 1.06 214,865 0.59 78.6 215,405 0.22 0.84 214,686 0.50 15 100-5-2a 193,671 193,671 29.3 195,076 0.17 0.73 194,534 0.45 73.1 194,725 0.18 0.54 194,013 0.18 16 100-5-2b 157,095 157,095 15.9 157,516 0.10 0.27 157,220 0.08 66.6 157,340 0.07 0.16 157,172 0.05 17 100-5-3a 200,079 200,079 30.7 201,145 0.20 0.53 200,354 0.14 74.8 201,062 0.18 0.49 200,327 0.12 18 100-5-3b 152,441 152,441 21.1 153,408 0.26 0.63 152,672 0.15 61.5 153,127 0.20 0.45 152,653 0.14 19 100-10-1a 258,243 287,695 39.6 295,818 0.69 2.82 291,961 1.48 95.0 292,917 0.62 1.82 289,649 0.68 20 100-10-1b 218,826 230,989 28.2 236,982 0.95 2.59 233,512 1.09 87.4 236,233 0.64 2.27 233,453 1.07 21 100-10-2a 243,590 243,590 49.3 244,959 0.25 0.56 243,855 0.11 110.3 244,712 0.10 0.46 244,075 0.20 22 100-10-2b 203,988 203,988 28.2 204,313 0.09 0.16 203,988 0.00 97.6 204,060 0.03 0.04 203,988 0.00 23 100-10-3a 222,353 250,882 52.9 254,732 0.21 1.53 253,756 1.15 109.3 254,242 0.19 1.34 253,510 1.05 24 100-10-3b 189,309 204,317 25.1 205,256 0.20 0.46 204,759 0.22 84.9 205,131 0.11 0.40 204,755 0.21 25 200-10-1a 475,294 403.7 482,532 0.22 1.52 480,593 1.11 895.5 481,511 0.13 1.31 480,435 1.08 26 200-10-1b 377,043 241.4 382,136 0.15 1.35 381,116 1.08 776.5 381,338 0.14 1.14 380,505 0.92 27 200-10-2a 449,006 336.5 452,710 0.14 0.82 451,560 0.57 835.5 451,994 0.10 0.67 451,293 0.51 28 200-10-2b 374,280 236.4 376,081 0.20 0.48 374,696 0.11 764.5 375,507 0.11 0.33 374,943 0.18 29 200-10-3a 469,433 346.3 477,577 0.19 1.73 475,855 1.37 834.8 476,462 0.17 1.50 475,276 1.24 30 200-10-3b 362,653 216.2 366,893 0.21 1.17 365,597 0.81 720.5 366,343 0.13 1.02 365,592 0.81 Average 73.1 0.17 0.66 0.37 199.1 0.12 0.53 0.32 Median 23.1 0.16 0.47 0.11 73.2 0.10 0.36 0.13 Table 4 Most recent and effective methods in the literature. Table 4 Algorithm Publication # Runs CPU GRASP+ELS [9] 5 Core2 Quad, 2.83GHz SALRP [32] 1 Core2 Quad, 2.66GHz ALNS-500K/ALNS-5000K [14] 5 Opteron 275, 2.20GHz 2-Phase HGTS [10] 1 Core2 Duo, 2.00GHz MACO [29] 10 Athlon XP 2500+, 2.00GHz GRASP+ILP [7] 10 Xeon E5462 Quad Core, 3.00GHz GVTNS [11] 1 Core2 Duo, 2.00GHz HybridGA/HybridGA+ 10 Core i7-4790, 3.60GHz Table 5 Average results for the three benchmark sets. Table 5 Algorithm Tuzun and Burke Barreto et al. Prins et al. CPU Avg Gap BKR Best Gap BKR CPU Avg Gap BKR Best Gap BKR CPU Avg Gap BKR Best Gap BKR GRASP+ELS 606.6 1.30 187.6 a 0.08 a 258.2 1.11 SALRP 826.4 1.49 464.1 0.43 422.4 0.46 ALNS-500K 829.6 0.90 0.44 177.2 a 0.25 a 0.16 a 451.1 0.74 0.44 ALNS-5000K 8103.0 0.18 1772.0 a 0.06 a 4221.0 0.27 2-Phase HGTS 392.3 1.15 105.2 a 0.78 a 176.4 0.57 MACO 201.9 1.24 191.7 0.11 191.3 0.40 GRASP+ILP 2589.5 0.54 0.18 264.3 a 0.64 a 0.14 a 1163.0 0.32 0.12 GVTNS 201.2 0.76 53.0 a 0.67 a 91.2 0.37 HybridGA 86.0 1.45 0.87 18.7 a 0.30 a 0.06 a 73.1 0.66 0.37 42.4 0.57 0.26 HybridGA+ 363.6 1.09 0.71 93.4 a 0.10 a 0.00 199.1 0.53 0.32 429.6 0.11 −0.06 a a Considering only (the same) 13 out of the 19 instances. A simple and effective evolutionary algorithm for the capacitated location–routing problem Rui Borges Lopes a ⁎ Carlos Ferreira b Beatriz Sousa Santos c a Department of Economics, Management and Industrial Engineering/CIDMA, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Economics, Management and Industrial Engineering/CIDMA, University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal b Department of Economics, Management and Industrial Engineering/IEETA, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Economics, Management and Industrial Engineering/CIO, University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal c Department of Electronics, Telecommunications and Informatics/IEETA, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunications and Informatics/IEETA, University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal ⁎ Corresponding author. This paper proposes a hybrid genetic algorithm (GA) to solve the capacitated location–routing problem. The proposed algorithm follows the standard GA framework using local search procedures in the mutation phase. Computational evaluation was carried out on three sets of benchmark instances from the literature. Results show that, although relatively simple, the proposed algorithm is effective, providing competitive results for benchmark instances within reasonable computing time. Keywords Location Location–routing Genetic algorithm 1 Introduction Location–routing problems (LRP) deal with the combination of two types of decisions that often arise in logistics: the location of facilities and the design of the distribution routes. Several LRP variants have appeared in the literature [19,8], among which the capacitated location–routing problem (CLRP) has recently emerged as one of the most addressed. The CLRP can be defined on a complete and undirected graph G = ( V , E ) with a vertex set V and an edge set E . V consists of a subset J of m potential depots and a subset I = V ∖ J of n clients. Each client i ∈ I has a non-negative demand d i , to be satisfied only once, and is to be assigned to a single depot j ∈ J with capacity w j . The shipment of clients’ demand from the assigned depot is carried out by an unlimited fleet of homogeneous vehicles with capacity Q ; each vehicle returning to the departure depot at the end of the route. The total demand of the clients assigned to each depot must not exceed its capacity and the total demand satisfied by any vehicle must not exceed Q . The following non-negative costs are incurred: fixed cost f j when depot j ∈ J has clients assigned and must be opened; fixed cost F for each vehicle used; and traveling cost c ij each time edge ( i , j ) ∈ E is in a vehicle route. The goal is to determine the set of depots to open and the tracing of the routes in order to minimize total costs. The CLRP is NP-hard and only few instances with more than 100 clients have been solved to proven optimality [1,3,5,7] making heuristic approaches often more suitable for solving real-life instances. This paper proposes a simple but effective heuristic algorithm for solving the CLRP, namely, a hybrid genetic algorithm (GA) where local search procedures are used as mutation operators. The remainder of this paper is organized as follows. Section 2 provides a brief review on the most recent heuristic algorithms for the CLRP. The proposed evolutionary algorithm is detailed in Section 3 and evaluated in Section 4. Finally, conclusions are drawn in Section 5. 2 Literature review Recent surveys on the LRP are the works by Lopes et al. [19], Prodhon and Prins [26] and Drexl and Schneider [8]. In Lopes et al. [19] heuristic approaches are classified according to the adopted framework (how location and routing phases interact) and the used method(s), compiling results of several heuristics for the CLRP. Prodhon and Prins [26] and Drexl and Schneider [8] can be seen as complementary surveys: the former emphasizing the CLRP, detailing and comparing recent algorithms, and the latter focusing on other LRP variants. Although most methods in the literature follow a hierarchical framework and use tour construction and improvement typically within metaheuristics, no clear conclusions could be drawn on the best performing frameworks and methods. The most recent and relevant approaches are mentioned hereafter. Barreto et al. [2] presents a clustering based heuristic for tackling the CLRP with no vehicle acquisition cost. Several clustering methods are used to obtain the routing data and then a facility location problem is solved with the collapsed routes. Marinakis and Marinaki [20] solved the same problem using a bilevel GA. Addressing the CLRP strictly as defined previously are the following works. Prins et al. [24] propose a constructive algorithm for the CLRP: extended savings heuristic. The heuristic is randomized and used in a greedy randomized adaptive search procedure (GRASP). The performance of this dedicated constructive algorithm is worth noting, which motivated further development, presented in Duhamel et al. [9], where a similar GRASP is hybridized with evolutionary local search. In Prins et al. [25] facility location (through Lagrangean relaxation) and vehicle routing (using a granular tabu search) are performed iteratively. The time required to obtain good solutions is remarkable, mostly due to the effectiveness of granular tabu search (GTS) in the routing phase [30]. Also using GTS are the works by Escobar et al. [10] and Escobar et al. [11]: firstly in the improvement phase of its hybrid heuristic; then within a variable neighborhood search algorithm. Other recent methods are by Yu et al. [32], with a simulated annealing (SA) based heuristic; Hemmelmayr et al. [14], using an adaptive large neighborhood search (ALNS) heuristic; Ting and Chen [29] with an ant colony optimization (ACO) algorithm; and Contardo et al. [6] slightly changing the GRASP by Prins et al. [24] and combining it with an integer-linear program. Looking at the methods’ performance, Escobar et al. [10] and Ting and Chen [29] show similar performance concerning both results and CPU times. These methods have been slightly improved by Escobar et al. [11], which provides a good trade-off of results and time to obtain them. The method by Contardo et al. [6] presents the best overall results for the benchmark instances from the literature at the expense of significantly higher computing times. Concerning genetic algorithms for the CLRP two approaches can be found in the literature: a memetic algorithm with population management [23] and a bilevel GA [20]. The memetic algorithm by Prins et al. [23] uses a fixed length chromosome composed of a sequence of genes for the depots and another for the clients, requiring a dedicated procedure for fitness evaluation. As crossover may produce unfeasible offspring a repair procedure is used. Depot configuration is only changed by crossover and local search is used for improving routing. The method works on a small population of high quality solutions using a population management procedure for ensuring diversity. The bilevel GA [20] solves the CLRP in two levels: in the first, solving the capacitated facility location problem; in the second, a vehicle routing problem (VRP) is solved for each of the individuals. Each individual in the population is a solution for the location problem and crossover and mutation only occurs at the first level. For obtaining the CLRP solution a VRP is solved in the second level using expanding neighborhood search (ENS). A large population and very few generations are used. Both methods require efficient constructive algorithms for obtaining the initial population. 3 Evolutionary algorithm The metaheuristic presented here follows the standard GA framework hybridized with local search procedures. The proposed algorithm shares some core principles with the hybrid GA by Prins [22], with good results for the VRP; the main being the use of local search as mutation operators. However, several new implementation aspects were developed or adapted to effectively address the CLRP. The most relevant are the chromosome representation and the crossover and mutation operators. The main components are detailed and the general structure of the algorithm is presented in the following sections. The similarities of this framework with memetic algorithms and scatter search are discussed in Prins [22]. Compared with other genetic algorithms in the CLRP literature, the main advantages of the proposed approach are: an intuitive chromosome representation, also allowing an easy fitness evaluation; an efficient constructive algorithm is not necessary; feasible offspring provided by crossover; and a simple framework. 3.1 Chromosome representation The chromosome in the proposed GA represents a complete solution, i.e., the collection of routes. Both the route (gene) length and the chromosome length are variable and depend on the number of clients serviced and the number of routes in the solution. For example, given a CLRP with 15 clients I={1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15} and 4 possible depot locations J={16, 17, 18, 19}, the chromosome representation of a solution is provided in Fig. 1. The solution in the figure represents installing facilities 16 and 18, and servicing the clients in the given order by four routes (vehicles). The adopted representation, assuming solution feasibility, together with the crossover operator (detailed in the following section), allows obtaining feasible children solutions. Thus, the use of repair methods to restore feasibility is avoided. Moreover, it allows a fast evaluation of its fitness value Fitness ( S ) : the total cost of solution S . Note that higher quality solutions have smaller fitness values. 3.2 Crossover operator The proposed crossover operator (inspired by an operator proposed by Hosny and Mumford [15], for a VRP) tries to copy complete routes from the parent to the child, thus will be named route copy crossover (RCX). It operates by copying to the child a random number of routes (between 1/3 and 2/3) from one of the parents, and the remaining unvisited clients are placed in a relocation pool following the original order in the other parent. The clients in the relocation pool are then inserted in the child, in new routes, and using the currently open depots (as long as capacity is obeyed, randomly opening a new one otherwise). This preempts the use of repair methods as child solutions are always feasible. An example of the RCX is illustrated in Fig. 2. In the example, assuming the first and third routes of Parent 1 are selected, both are copied to Child 1. The remaining clients not yet included in Child 1 (shown underlined in Parent 2) are copied, following their order of appearance, to the relocation pool. The clients in the relocation pool are then used to form new routes in Child 1, using the currently open depots (opening more when depot capacity constraints are violated) and following the sequence as long as vehicle (route) capacity is obeyed. The second child is created similarly, using the parents in reverse roles. The RCX allows inheriting some of the routes from one parent while at the same time randomizing the building of the child solution routes (yet still partially inheriting the structure of the route from the other parent). Moreover, the operator promotes solutions with few open depots and routes with little unused capacity, two features often found in good solutions. 3.3 Mutation operators One of the main components of genetic algorithms is mutation operators, often performing simple random moves. Although useful for providing randomization in the search for the global optimum they are usually insufficient for obtaining competitive results. Here, improvement procedures are also used. Both simple operators and local search procedures are detailed as follows. Note that the simple operators (“add” and “swap”) are important for providing additional randomness, while the local search procedures allow quickly obtaining much better results. 3.3.1 Add and swap operators The first mutation operator, named “add”, seeks to avoid a fast convergence to solutions with few depots (prone to happen due to the RCX operator). At the same time it diversifies the open depots, by opening a new one and randomly reassigning between 1 and 2/3 of the routes to it, depot capacity allowing (see example in Fig. 3, top, with changes underlined). With this operator depot configuration is randomly changed. The second operator randomly swaps the position of two clients in the tracing of the routes (possibly between two routes, depot and vehicle capacity allowing), hence named “swap”. This operator aims at providing a small randomization to the building of the routes. In Fig. 3, bottom, the swap mutation operator is applied to the underlined clients. 3.3.2 Local search procedures Two local search procedures are proposed: one for improving depot location and subsequent allocation of the routes ( LS location ), and another for route improvement ( LS routing ). LS location is a facility location algorithm, namely, the tabu search developed by Filho and Galvão [12]; chosen due to its reduced CPU time and near-optimal results. However, since solutions have routes instead of individual clients, in order to apply the heuristic, each route is collapsed into a single client and the considered distance is the smallest insertion cost of the depot in the original route, similarly to Barreto et al. [2]. LS routing is composed of a relocate algorithm [28], 1-interchange algorithm [21], and a 2-opt algorithm [24], performing intra- and inter-route moves applied sequentially and cyclically until no improvement can be found. Additionally, for the more thorough search, the HybridGA+ variant, a 3-opt algorithm [4] is used. The relocate algorithm performs in the same way as the one used in vehicle routing problems [28], where a single client is reinserted in another position inside the current route or in another route provided there is an improvement to the solution. In the latter case, an adaptation was made for the CLRP in order to account for depot capacity and vehicle constraints. The 1-interchange algorithm [21] tests exchanging each client from one route with every other customer in another route even if not sharing the same depot. If improvements are found and depot and vehicle capacities obeyed, the move is implemented. The 2-opt algorithm [24] inside the routes is equivalent to the well-known 2-opt move [18] whereas the moves between different routes have to consider depot and vehicle capacity constraints as well as depot reassignment. Similarly applying intra- and inter-route moves a 3-opt algorithm [4] is used in variant HybridGA+. All the methods in LS routing implement the first found improvement rather than the best and stop when no additional improvement is detected – for some of these procedures this has been empirically found to be better [13]. 3.4 General structure The proposed algorithm starts out by creating an initial population P made of feasible random chromosomes: Generate ( P ) . Afterwards, a main loop is performed, stopping when a maximum number α max of iterations without improving the best solution is reached. In each iteration of the main loop two parents are chosen to crossover using binary tournament selection: two individuals (chromosomes) are randomly selected from the population and the fittest becomes a parent. RCX is then applied and the two obtained children are added to the population. For maintaining the population size, the two worst solutions are removed. The mutation phase is applied afterwards, where the operators “add” ( AddMutation ) and “swap” ( SwapMutation ), followed by the local search procedures LS location and LS routing are applied to all the population. The location related operators have a mutation rate p location ; for routing related operators mutation rate p routing is used. The best solution is searched and stored (if found) at each iteration. Although increasing the computational burden, it prevents the loss of good solutions during the course of the algorithm. The algorithm ends when the main loop stops, returning the best solution ( BestSolution ). An overview of the pseudo-code of the proposed hybrid GA is given in Fig. 4. Other features, common to GA algorithms, were also tested. These will be addressed in Section 4.1. 4 Computational evaluation Implementation aspects and an evaluation of the proposed GA (using benchmark instances) are presented and discussed in the following sections. 4.1 Implementation The algorithm was coded in C# and results obtained using a 3.60GHz Intel Core i7-4790 CPU with 8GB of RAM and running Windows 7; it is embedded in the LORE tool, available at An extensive study was made testing different features, common to GA algorithms. The tested features which did not consistently improve results (in some cases incurring in additional complexity) were • half of the initial population generated with the randomized extended savings heuristic [24], • changing the order in which mutation operators are used, and • partial replacement procedures. Features typically providing worse results were • all individuals becoming parents and producing the same amount of children, sorting and removing the worst half, • using binary tournament for removing individuals in each iteration, • dynamically changing the mutation rates, and • removing “add” and “swap” mutation operators. When testing the different features one of the components that proved crucial for the method’s performance was the location related operators (“add” and LS location ), as removing them degraded results on average 12.5%. Concerning parameters, based on the testing of the algorithm, the following were found to be the most suitable. The initial population size depends directly on the number of clients and inversely on the number of depots: (1) | P | = ⌈ a n m ⌉ . As the number of depots increases less solutions are required for providing randomness. However, for better exploring the tracing of bigger routes (i.e. when more clients exist) more individuals are required in the population. Also, smaller populations tend to provide faster results, while too small fail to efficiently explore the solution space. For the maximum number of iterations without improving the best solution ( α max ) it was found that the larger the population, the less iterations were required. Thus, the following equation was used: (2) α max = ⌈ a 250 n + 500 m | P | ⌉ . The multiplier a , used in both parameters, depends on the size of the instance, taking on the value 10 for smaller instances ( n < 100 ), 5 for mid-size instances ( 100 ≤ n < 250 ), and 1 for larger instances ( n ≥ 250 ). The reasoning is: the higher the value, the more intensified is the search for good solutions (more individuals and iterations), and more time-consuming it becomes (possibly too much in larger instances). Finally, mutation rates p location and p rou ting were set respectively at 0.02 and 0.06, providing an overall 16% probability of occurring mutations. Note that the tested features as well as the proposed parameters were evaluated using the benchmark instances detailed in the following section. For newer instances, the most important parameter requiring tuning is the size of the population (using similar equations for the remaining). Results reported in this paper concern two variants of the algorithm: HybridGA and HybridGA+. The only difference between them is that in the latter variant a more thorough search is performed by running a 3-opt algorithm in LS routing and doubling α max . 4.2 Benchmark instances For evaluating the algorithm (as well as tested features and parameters) three sets of CLRP benchmark instances were used. These are the sets by Tuzun and Burke [31], Barreto et al. [2] and Prins et al. [24] with, respectively, 36, 19 and 30 instances. In the randomly generated instances by Tuzun and Burke [31] the number of clients is n ∈ {100, 150, 200} and the number m of depots is 10 and 20. Vehicle capacity Q = 150, vehicle fixed cost F = 10 and clients demand follow a uniform distribution in the range [1, 20]. These instances can be found in The benchmark instances proposed by Barreto et al. [2] are available from and were either obtained from the literature or adapted from VRP instances. The number of clients n ranges from 12 to 318, the number of depots m from 2 to 15 and there is no vehicle fixed cost. The last set by Prins et al. [24] was randomly generated (available at with n ∈ {20, 50, 100, 200}, m ∈ {5, 10}, Q ∈ {70, 150} and F = 1000. Euclidean distances are used in all the instances, multiplied by 100 and rounded up to the next integer in the third set. In the randomly generated sets clients are spatially distributed in clusters. 4.3 Results Two variants of the proposed method were tested performing ten runs on each instance, from which average solution values, best solutions and average computing times were obtained. Results for the three benchmark sets can be seen in Tables 1–3, and full solution data is available at The first column displays the name of the instance, followed by the best known lower bound (LB), when available, and currently best known result (BKR). Then, data concerning the two variants of the proposed hybrid GA is shown: average CPU time in seconds, Average and Best Results concerning cost values, and standard deviation (Std dev) of the cost over the ten runs in percentage. Finally, GapBKR concerns the gap to best known result, which is provided in percentage for both Average and Best results. Note that few lower bounds are available for the first set, and therefore only BKR is provided in Table 1. Proven optimal values are italicized and average and median values are provided (as data showed skewed distributions and/or outlying data points for CPU time and Gaps). For HybridGA, in the benchmark set by Tuzun and Burke CPU times are on average lower than 90s and, concerning best obtained solutions, gaps to best known results are on average 0.87% with highest gap at 3.09%. In the HybridGA+ variant average CPU times ascend to 6min and average gaps lower to 0.71%. In both variants median values are similar to average values. In the second set (Barreto et al.), computing times are on average less than 43 and 430s and median values are 17 and 49s (this difference is mostly due to the two outliers: instances 13 and 14). Average GapBKR of best results are for both variants respectively 0.26% and −0.06% and median values are 0% for both. Two new BKR were found, one of them reaching an improvement of over 1% to previous results. Gaps to best known results are very low for all instances pointing to low-quality lower bounds for instances 8, 11, 12 and 16. For the Prins et al. benchmark set CPU times are on average 73 and 199s and median are 23 and 73s, due to the larger instances (instances 25–30). Concerning GapBKR of best found solutions, average and median values are respectively 0.37% and 0.11% for HybridGA, 0.32% and 0.13% for HybridGA+. In this set larger gaps can be found in instances 19, 20, 23, 25, 26 and 29; in these first three instances and instance 24 results suggest corresponding low-quality lower bounds. Overall, among the two variants, the running time of HybridGA+ increases twofold for smaller instances and can go up to ten times more in larger instances. Best results are the same for smaller instances (although the second variant reaches them more consistently) and are increased in larger instances (only marginally in some cases). For all instances containing 50 or less clients, the two variants succeeded to find the optimal solution and an interesting feature is that the method seems faster when larger routes are found. This is noticeable when comparing CPU times of equal sized instances but with different vehicle capacities (e.g., instances with suffix “a” and “b” in the last benchmark set, where instances “b” have more than double the vehicle capacity of instances “a”, being the only difference between them). This behavior is not shared by most other methods in the literature. Characteristics of most recent and effective methods in the literature are presented in Table 4 where, for each algorithm, source of publication, number of runs and CPU times are provided. In Table 5 average results of the proposed approach (both HybridGA and HybridGA+) are compared with results from those other methods. CPU refers to the average computing time in seconds, Avg GapBKR and Best GapBKR to the average gaps between the obtained average or best results and the currently best known results (in percentage). In the case of the proposed variants two values are provided in the second benchmark set for both CPU and gaps for a direct comparison with algorithms only considering the same 13 out of the 19 instances. As CPU times of the remaining algorithms were acquired from the corresponding publications, therefore depending on many factors (such as computers and programming languages used), they are mostly indicative. However, the proposed method seems to be among the fastest (if not the faster for the case of HybridGA). Concerning Best GapBKR, the proposed algorithm is able to outperform all of the remaining in the benchmark set by Barreto et al. considering either the subset of 13 instances or the complete set; only MACO surpasses the faster variant (HybridGA) in the complete set. On the other two benchmark sets average results are also competitive, having the fourth best average gap for the Tuzun and Burke set and the third best for the Prins et al. set. 5 Conclusions In this paper an evolutionary algorithm (hybrid GA) is presented for solving the CLRP. It follows the standard GA framework hybridized with local search procedures in the mutation phase. Two variants of the algorithm are tested on three sets of instances from the literature. A comparative analysis is also performed with other published approaches and results suggest that the method, although relatively simple, is competitive. The algorithm provides the best results and CPU times on the Barreto et al. instances, and is among the best performing methods on the other two benchmark sets having good trade-offs of CPU times and gaps. Moreover, it was able to detect the optimal solution on all instances containing 50 or less customers. It is worth noting the ease of implementation of the proposed algorithm. As advocated by Laporte et al. [17] for the VRP, similarly extendable to the LRP, it is probably time to develop faster and simpler algorithms, even if causing a small loss in solution quality. This work is a step in that direction. The proposed hybrid GA has already been implemented in a decision-support tool available at Future work may include further fine-tuning of parameters or including other local search procedures (such as methods with destroy-and-repair operators) in the mutation phase. Concerning the problem addressed herein, it may be extended to address several time periods, making heuristic developments for the CLRP even more relevant, as such methods may be used within myopic or panoramic frameworks for tackling dynamic LRPs [16,27]. Acknowledgments This work was supported by Portuguese funds through the CIDMA – Center for Research and Development in Mathematics and Applications, and the Portuguese Foundation for Science and Technology (“FCT – Fundação para a Ciência e a Tecnologia”), within project UID/MAT/04106/2013. References [1] R. Baldacci A. Mingozzi R. Wolfler Calvo An exact method for the capacitated location–routing problem Oper Res 59 5 2011 1284 1296 [2] S. Barreto C. Ferreira J. Paixão B.S. Santos Using clustering analysis in a capacitated location–routing problem Eur J Oper Res 179 3 2007 968 977 [3] J.M. Belenguer E. Benavent C. Prins C. Prodhon R. Wolfler Calvo A branch-and-cut method for the capacitated location-routing problem Comput Oper Res 38 6 2011 931 941 [4] I.M. Branco J.D. Coelho The Hamiltonian p-median problem Eur J Oper Res 47 1 1990 86 95 [5] C. Contardo J.-F. Cordeau B. Gendron A computational comparison of flow formulations for the capacitated location–routing problem Discret Optim 10 4 2013 263 295 [6] C. Contardo J.-F. Cordeau B. Gendron A GRASP+ILP-based metaheuristic for the capacitated location–routing problem J Heuristics 20 1 2014 1 38 [7] C. Contardo J.-F. Cordeau B. Gendron An exact algorithm based on cut-and-column generation for the capacitated location–routing problem INFORMS J Comput 26 1 2014 88 102 [8] M. Drexl M. Schneider A survey of variants and extensions of the location–routing problem Eur J Oper Res 241 2 2015 283 308 [9] C. Duhamel P. Lacomme C. Prins C. Prodhon A GRASPxELS approach for the capacitated location–routing problem Comput Oper Res 37 11 2010 1912 1923 [10] J.W. Escobar R. Linfati P. Toth A two-phase hybrid heuristic algorithm for the capacitated location–routing problem Comput Oper Res 40 1 2013 70 79 [11] J.W. Escobar R. Linfati M.G. Baldoquin P. Toth A granular variable tabu neighborhood search for the capacitated location–routing problem Transp Res Part B 67 2014 344 356 [12] V.J. Filho R.D. Galvão A tabu search heuristic for the concentrator location problem Locat Sci 6 1 1998 189 209 [13] P. Hansen N. Mladenović First vs. best improvement: an empirical study Discret Appl Math 154 5 2006 802 817 [14] V.C. Hemmelmayr J.-F. Cordeau T.G. Crainic An adaptive large neighborhood search heuristic for two-echelon vehicle routing problems arising in city logistics Comput Oper Res 39 12 2012 3215 3228 [15] Hosny M, Mumford C. Investigating genetic algorithms for solving the multiple vehicle pickup and delivery problem with time windows. In: Proceedings of the VIII Metaheuristic International Conference (MIC 2009). Hamburg; 2009. [16] G. Laporte P.J. Dejax Dynamic location–routing problems J Oper Res Soc 40 5 1989 471 482 [17] G. Laporte M. Gendreau J.-Y. Potvin F. Semet Classical and modern heuristics for the vehicle routing problem Int Trans Oper Res 7 4–5 2000 285 300 [18] S. Lin B.W. Kernighan An effective heuristic algorithm for the traveling salesman problem Oper Res 21 2 1973 498 516 [19] R.B. Lopes C. Ferreira B.S. Santos S. Barreto A taxonomical analysis, current methods and objectives on location-routing problems Int Trans Oper Res 20 6 2013 795 822 [20] Y. Marinakis M. Marinaki A bilevel genetic algorithm for a real location routing problem Int J Logist Res Appl 11 1 2008 49 65 [21] I.H. Osman Metastrategy simulated annealing and tabu search algorithms for the vehicle routing problem Ann Oper Res 41 4 1993 421 451 [22] C. Prins A simple and effective evolutionary algorithm for the vehicle routing problem Comput Oper Res 31 12 2004 1985 2002 [23] C. Prins C. Prodhon R. Wolfler Calvo A memetic algorithm with population management (MA|PM) for the capacitated location–routing problem Gottlieb Jens Raidl R. Günther Evolutionary computation in combinatorial optimization – lecture notes in computer science vol. 3906 2006 Springer Berlin 183 194 [24] C. Prins C. Prodhon R. Wolfler Calvo Solving the capacitated location–routing problem by a GRASP complemented by a learning process and a path relinking 4OR 4 3 2006 221 238 [25] C. Prins C. Prodhon A. Ruiz P. Soriano R. Wolfler Calvo Solving the capacitated location–routing problem by a cooperative Lagrangean relaxation-granular tabu search heuristic Transp Sci 41 4 2007 470 483 [26] C. Prodhon C. Prins A survey of recent research on location–routing problems Eur J Oper Res 238 1 2014 1 17 [27] S. Salhi G. Nagy Consistency and robustness in location–routing Stud Locat Anal 13 1999 3 19 [28] M. Savelsbergh The vehicle routing problem with time windows: minimizing route duration INFORMS J Comput 4 2 1992 146 154 [29] C.J. Ting C.H. Chen A multiple ant colony optimization algorithm for the capacitated location routing problem Int J Prod Econ 141 1 2013 34 44 [30] P. Toth D. Vigo The granular tabu search and its application to the vehicle–routing problem INFORMS J Comput 15 4 2003 333 346 [31] D. Tuzun L.I. Burke A two-phase tabu search approach to the location routing problem Eur J Oper Res 116 1 1999 87 99 [32] V.F. Yu S.W. Lin W. Lee C.J. Ting A simulated annealing heuristic for the capacitated location routing problem Comput Ind Eng 58 2 2010 288 299 "
    },
    {
        "doc_title": "The Recognition of Goodwill and Other Intangible Assets in Business Combinations - The Portuguese Case",
        "doc_scopus_id": "84961239885",
        "doc_doi": "10.1111/auar.12073",
        "doc_eid": "2-s2.0-84961239885",
        "doc_date": "2016-03-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Accounting",
                "area_abbreviation": "BUSI",
                "area_code": "1402"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2016 CPA Australia Ltd.In this study, we investigate the magnitude of goodwill recognised in business combinations during the years 2005 to 2009 by the Portuguese companies listed on Euronext Lisbon, and characterise the amount of the other intangible assets recognised separately from goodwill. We also analyse the level of compliance of those companies with the main disclosure requirements of International Financial Reporting Standard (IFRS) 3 - Business Combinations. Our study, which involves the analysis of 197 business combinations, reveals that the amounts of goodwill continue to be highly material, while conversely, the value of identifiable intangible assets in those acquisitions is very low. The results suggest that Portuguese companies do not undertake sufficient efforts to individually identify and disclose intangibles acquired in business combinations. This fact is reinforced by the reduced level of compliance with the disclosures required by IFRS 3, particularly the factors that contribute to the recognition of goodwill. Our findings provide feedback to standard setters in an effort to improve practice in the application of IFRS 3. Moreover, they reinforce their recent concerns regarding the post-implementation review of business combinations, as well as the ongoing project of the IASB, whose objective is to improve disclosures in existing standards.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Hemo@record: The Portuguese system for registration of patients with haemophilia and other congenital coagulopathies",
        "doc_scopus_id": "85047082103",
        "doc_doi": "10.18803/capsi.v16.124-138",
        "doc_eid": "2-s2.0-85047082103",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Management Information Systems",
                "area_abbreviation": "BUSI",
                "area_code": "1404"
            },
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Congenital coagulopathies",
            "Haemophilia",
            "Hemo@record",
            "National registry",
            "WEB application"
        ],
        "doc_abstract": "© 2016 Atas da Conferencia da Associacao Portuguesa de Sistemas de Informacao. All rights reserved.This paper describes a web application designed to support the first national registry of patients with haemophilia and other congenital coagulopathies in Portugal. The importance of national patient registry systems (nPRS), in the scope of the treatment of the chronic and rare diseases such as haemophilia, has been recognized by several institutions at an international level and by national government entities. However, there are several barriers to the creation of such systems, in particular due to the lack of motivation to invest in order to benefit only a small proportion of the population, a characteristic of rare diseases. The conditions for the creation of the first Portuguese nPRS in the field of haemophilia were recently created through a joint project between a group of medical professionals belonging to the Portuguese Association of Coagulopathies Congenital (APCC) and a group of researchers from the University of Aveiro. Currently, the technological solution is already developed, tested, and installed at the data centre of the University of Aveiro. Its operational version is being used by clinicians of different Haemophilia Treatment Centres (HTCs) around the country. The data inserted accounts for about 10% of the Portuguese population with haemophilia and all entities involved are working towards achieving 100% of this population. The data inserted accounts for about 10% of the Portuguese population with haemophilia and all entities involved are working towards achieving 100% of this population.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "LORE, a decision support tool for location, routing and location-routing problems",
        "doc_scopus_id": "84984797969",
        "doc_doi": "10.1007/978-3-319-44896-1_17",
        "doc_eid": "2-s2.0-84984797969",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Data input",
            "Decision support tools",
            "Graphical user interfaces (GUI)",
            "Location routing problem",
            "Location-routing",
            "Solution-finding process"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.LOcation Routing Exploration (LORE) is a decision support tool for addressing location, routing and location-routing problems. In this paper the LORE tool will be presented, and its main characteristics addressed. Among the main features of the tool is the ability to support a variety of problems currently being studied in the location and routing literature (due to the proposed data structure), and the graphical user interface (GUI). The data structure will be presented being provided an explanation on how it can support related problems. The GUI main goal is not only to aid the solution-finding process but also to foster greater insight into the problem(s) at hand. To that extent, the GUI, developed to fit the target user’s profile and intended tasks, is presented, namely data input and visualization features.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating human factors in information systems development: User centred and agile development approaches",
        "doc_scopus_id": "84978877756",
        "doc_doi": "10.1007/978-3-319-40247-5_35",
        "doc_eid": "2-s2.0-84978877756",
        "doc_date": "2016-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Agile software development",
            "Clinical information",
            "Complex environments",
            "Information system development",
            "Information systems development",
            "Interactive software",
            "Software development process",
            "Web-based applications"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2016.This paper presents an overview and discussion based on the literature review of recent research of some practices that incorporate human factors, emphasizing the user-centred design (UCD) and agile software development (ASD) approaches. Additionally, this article presents an experience of the development of a web-based application that aims to manage the clinical information in haemophilia care, which benefited from these practices, making use of some methods to support the collaboration and communication between designers, users, and developers. The results of our experience show that the hybrid approach, that combines the principles of UCD with values of ASD can help to integrate human factors into the software development process in a highly complex environment, characterized by missing information, shifting goals and a great deal of uncertainty, such as the healthcare field.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "National Patient Registry with Hemophilia and other Congenital Coagulopathies: The Portuguese system",
        "doc_scopus_id": "84943339580",
        "doc_doi": "10.1109/CISTI.2015.7170480",
        "doc_eid": "2-s2.0-84943339580",
        "doc_date": "2015-07-28",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Education",
                "area_abbreviation": "SOCI",
                "area_code": "3304"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            }
        ],
        "doc_keywords": [
            "congenital coagulopathies",
            "hemophilia",
            "national registry",
            "Portugal",
            "WEB application"
        ],
        "doc_abstract": "© 2015 AISTI.This work presents the recently developed Portuguese Patient Registry with Hemophilia and other Congenital Coagulopathies. It is a web-based application that aims to collect a set demographic, social and clinical data (categorized by diagnostic, treatment and follow-up data) of people with hemophilia and other congenital coagulopathies (H&oCC) in a national universe. This project was developed in collaboration with a group of clinicians belonging to the Portuguese Association of Congenital Coagulopathies (APCC) and the technological solution is currently installed in the data center of the University of Aveiro, to be used by clinicians of different Hemophilia Treatment Centers (HTC) located across the country.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Identification of IT Value Management Practices and Resources in COBIT 5",
        "doc_scopus_id": "85034751772",
        "doc_doi": "10.17013/risti.15.17-33",
        "doc_eid": "2-s2.0-85034751772",
        "doc_date": "2015-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "COBIT 5",
            "Enablers",
            "IT governance",
            "Val IT 2.0",
            "Value management"
        ],
        "doc_abstract": "Enterprise Governance of Information Technology is a primary theme recognized by the academic and professional community. The need for value to the organization, whether strategic or financial, resulting from investments made in IT implies an appropriate IT Value Management. The difficulties or the little knowhow in the implementation of practical Frameworks proposed by the professional community like Val IT 2.0 or COBIT 5 lead organizations opt for the development of their own models. This paper identifies a set of practices and resources of IT Value Management resulting from the integration of VAL IT 2.0 in COBIT 5. A research methodology based on a literature review, will support the design of a conceptual model for identifying a set of enablers of IT Value Management present in COBIT 5.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Web platform to support the Portuguese national registry of haemophilia and other inherited blood disorders",
        "doc_scopus_id": "84973569046",
        "doc_doi": "10.4018/IJWP.2015010104",
        "doc_eid": "2-s2.0-84973569046",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            }
        ],
        "doc_keywords": [
            "Haemophilia",
            "Inherited Blood Disorders",
            "National Patients Registry",
            "Portugal",
            "Rare disease",
            "WEB application"
        ],
        "doc_abstract": "Copyright © 2015, IGI Global.Patient registries are essential tools for identifying and tracking people with a particular disease and for collecting epidemiological information, having a special role in rare and chronic diseases, where haemophilia and other inherited blood disorders (HoIBD) are classified. Web-based technologies represent an excellent solution to support different types of registries, due to the benefits that they can promote in the management of disease data. This work presents the web platform developed in a joint initiative between the Portuguese Association of Congenital Coagulopathies (PACC) and the University of Aveiro (UA), with the purpose of creating the first National Patients Registry (NPR) with HoIBD in Portugal. This application is hosted in the data centre of the UA, and at this moment it is already used by clinicians of the different Haemophilia Treatment Centres (HTC) located in Portugal, with the next challenge being the increase in the number of users.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The Portuguese National Registry for Hemophilia: Developing of a Web-based Technological Solution",
        "doc_scopus_id": "84962809050",
        "doc_doi": "10.1016/j.procs.2015.08.507",
        "doc_eid": "2-s2.0-84962809050",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder",
            "Coagulation factor",
            "Congenital Coagulopathies",
            "Hemophilia",
            "National Pacients Registry",
            "Technological platform",
            "Technological solution",
            "WEB application"
        ],
        "doc_abstract": "© 2015 The Authors. Published by Elsevier B.V.The crucial role that patient records have in the management of the rare and chronic diseases greatly increases the need to create mechanisms to facilitate the identification and management of the patient's data. Hemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor that affects the population on a ratio of 1 case for 10,000 people born. Currently, there are several countries with technological platforms to support the National Patients' Registries (NPR) of Hemophilia and other Congenital Coagulopathies (HoCC), due to its benefits in the management of the disease. This work presents the technological platform developed in a joint initiative between the University of Aveiro (UA) and the Portuguese Association of Congenital Coagulopathies (PACC), with the purpose of creating the first NPR with HoCC in Portugal. This web application is hosted in the data center of the University of Aveiro, and is being used by the clinicians of the different Hemophilia Treatment Centers (HTC) across the country.",
        "available": true,
        "clean_text": "serial JL 280203 291210 291871 31 90 Procedia Computer Science PROCEDIACOMPUTERSCIENCE 2015-09-15 2015-09-15 2015-09-15 2015-09-15 2016-03-16T16:55:20 S1877-0509(15)02642-3 S1877050915026423 10.1016/j.procs.2015.08.507 S300 S300.5 HEAD-AND-TAIL 2016-03-16T13:28:00.410768-04:00 0 0 20150101 20151231 2015 2015-09-15T04:43:01.025327Z rawtext articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight oauserlicense openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content oa subj suppl tomb vol volfirst volissue volumelist yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 1877-0509 18770509 true 64 64 C Volume 64 159 1248 1255 1248 1255 2015 2015 2015-01-01 2015-12-31 2015 Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015 Maria Manuela Cruz-Cunha João Varajão Rui Rijo Ricardo Martinho Petra Schubert Albert Boonstra Ricardo Correia Alexander Berler article fla Copyright © 2015 The Authors. Published by Elsevier B.V. PORTUGUESENATIONALREGISTRYFORHEMOPHILIADEVELOPINGAWEBBASEDTECHNOLOGICALSOLUTION TEIXEIRA L SRIVASTAVA 2013 e1 e47 A VIVIANI 2014 81 L LARA 2011 389 396 B AZNARJA 2009 1327 1330 ZDZIARSKA 2011 e189 e195 J HAY 2004 21 25 C HESSE 2013 S15 S21 J WALKER 1995 548 551 I TEIXEIRAX2015X1248 TEIXEIRAX2015X1248X1255 TEIXEIRAX2015X1248XL TEIXEIRAX2015X1248X1255XL Full 2015-08-21T00:47:13Z ElsevierWaived OA-Window item S1877-0509(15)02642-3 S1877050915026423 10.1016/j.procs.2015.08.507 280203 2016-03-16T13:28:00.410768-04:00 2015-01-01 2015-12-31 true 829119 MAIN 8 51617 849 656 IMAGE-WEB-PDF 1 Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 1877-0509 Â© 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of SciKA - Association for Promotion and Dissemination of Scientific Knowledge doi: 10.1016/j.procs.2015.08.507 ScienceDirect Available online at www.sciencedirect.com Conference on ENTERprise Information Systems / International Conference on Project MANagement / Conference on Health and Social Care Information Systems and Technologies, CENTERIS / ProjMAN / HCist 2015 October 7-9, 2015 The Portuguese National Registry for Hemophilia: Developing of a web-based technological solution Leonor Teixeira a,c,*, Vasco Saavedra a , JoÃ£o Pedro SimÃµes d, Beatriz Sousa Santos b,c, Carlos Ferreira a,c aDepartment of Economics, Management and Industrial Engineering, University of Aveiro, 3810-193 Aveiro, Portugal bDepartment of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal cIEETA - Institute of Telematics and Electronic Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal dTruphone, Lisbon, 2740-120 Oeiras, Portugal Abstract The crucial role that patient records have in the management of the rare and chronic diseases greatly increases the need to create mechanisms to facilitate the identification and management of the patientâ€™s data. Hemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor that affects the population on a ratio of 1 case for 10,000 people born. Currently, there are several countries with technological platforms to support the National Patientsâ€™ Registries (NPR) of Hemophilia and other Congenital Coagulopathies (HoCC), due to its benefits in the management of the disease. This work presents the technological platform developed in a joint initiative between the University of Aveiro (UA) and the Portuguese Association of Congenital Coagulopathies (PACC), with the purpose of creating the first NPR with HoCC in Portugal. This web application is hosted in the data center of the University of Aveiro, and is being used by the clinicians of the different Hemophilia Treatment Centers (HTC) across the country. Â© 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of SciKA - Association for Promotion and Dissemination of Scientific Knowledge. Keywords: National Pacients Registry, Web-application, Hemophilia, Congenital Coagulopathies *Corresponding author. Tel.: +351-234-370361; fax: +351-234-370215. E-mail address: lteixeira@ua.pt Â© 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license Peer-review under responsibility of SciKA - Association for Promotion and Dissemination of Scientific Knowledge 1249 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 1. Introduction According to the report of the World Federation of Hemophilia (WFH), an important measure for improving the management of Hemophilia and other Congenital Coagulopathies (HoCC) is the existence of patientsâ€™ registries, which, in a national context, could lead to the National Patientsâ€™ Registries (NPR) with HoCC [1]. The vision of the WFH with 'Treatment for All' is that all people that suffer with HoCC must have access to medical care and appropriate treatments. For this, the identification of all patients who suffer with this disease represents the first step, being the NPR the fundamental tool for such identification [2]. Registries of patients have the invaluable potential to provide an understanding of the disorder, to provide useful information for planning health care services and to identify suitable groups of patients for clinical trials enrolment [3]. This kind of registries are particularly useful for people suffering with rare and chronic diseases, such as HoCC, where important research questions cannot be answered without a set of information about the prevalence and evolution of the disease. Registries for rare diseases are broadly accepted for their usefulness in monitoring the identification and diagnosis of people with hemophilia (PWH) and evaluating their health. The purpose of a Registry in the context of HoCC is to define the population demographics and collect observational data on specific hemophilia health concerns such as the prevalence of viral infections, factor inhibitors, implementation of prophylaxis, and track the usage of the products (drugs). In addition to the demographic characterization of the population with hemophilia, the existence of those records allows the collection of data for statistical analysis related to the specificity of the disease, such as the prevalence of viral infections, existence of inhibitor factors, implementation of prophylactic treatments, joint evaluation, among other mechanisms to assist the decision making process. Given the importance of these data for disease management, the consistent definition of this data, its correct storage and possibility of subsequent extraction of information are important factors for a more objective view of the practice of hemophilia care, with a profound impact on the health and in the quality of life of these patients. The lack of a National Patients Registry for Hemophilia and other Congenital Coagulopatias in Portugal, associated with the difficulty that clinicians of this area faced in order to manage the specific patient information, motivated a group of physicians to seek for a technological solution that facilitated and optimized the information management process. This article presents the newly developed web-based solution to support the Portuguese National Hemophilia Registry, whose project arose from a joint initiative between the hemophilia healthcare professionals, represented by the Portuguese Association of Congenital Coagulopathies (PACC) and a group of researchers from the University of Aveiro (UA) responsible for analyzing, developing and implementing the technological solution. 2. Registry and its role in chronic and rare diseases Registry is defined in epidemiology as a data file with information efficiently recoverable, related with elements that are important for health in a defined population in such a way that the registered elements can be extrapolated to a base population [4]. In the scope of congenital bleeding disorders, the registry is a database of people identified with this disease including information on personal details, diagnosis, treatment and complications [1], [5]. The registry is thus a key tool for identifying and tracking individuals with a particular disease, and consequently for quality control and quality assurance in treatment, and for studying the impact of new developments on prevention, diagnosis and treatments [6]. These registries contain demographical, social and clinical data, categorized in diagnose, treatment and follow-up data. The WFH recommends that this data be stored in a centralized unique national repository, as, among other benefits, avoids patient duplication, ensuring a greater reliability of registries in order to portray a faithful reality of the disease in a country [1]. Given the usefulness of these systems, there are several countries that have already implemented NPR in the context of Hemophilia and other Congenital Coagulopathies. Considering the results the results of a study published by Oâ€™Mahony et al. in 2012 [7], that was based in an inquiry made in 35 European countries, there are 27 countries with NPR and 8 without a NPR, a group where Portugal was included. In the countries with a NPR, not all have the information openly available, and some of them are reported in the literature, as the NPR from Austria [6], Italy [6], Spain [8], Poland [9], Switzerland [10], United Kingdom [11], 1250 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 Germany [12]. Other NPR are referred in the work of other authors as is the cases of NPR from Bulgaria, Czech Republic, Slovenia, Finland, Greece, Russia, and Slovakia [7]. Outside European continent, there are other reference NPR in this area, as is the case as the NPR from Canada [13]. 3. The Web-application to support the Portuguese National Registry for Hemophilia This work aims to present the web-application to support a National Registry of Hemophilia and other Congenital Coagulopathies in Portugal. The analysis, conception and development of the platform that would support NPR with HoCC in Portugal was a joint initiative between the Portuguese Association of Congenital Coagulopathies (PACC) and a group of researchers from the University of Aveiro (UA). The PACC is a non-profit organization represented by a group of health care providers in the area of HoCC, and the entity responsible for defining the rules, guidelines and setting goals for the NPR. The University of Aveiro was the part responsible for the technology leadership, more specifically for the analysis, conception, implementation and maintenance and support of the NPR of HoCC platform. 3.1. Disease characterization and problem contextualization Hemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor [2]. According to the WFHâ€™s annual global surveys, the number of PWH in the world is approximately 400 000, affecting the population in a ratio of 1 case for 10,000 people born [5]. Portugal is a country with 10 million inhabitants and about 1000 PWH are expected. However, the number of the other cases with Congenital Coagulopathies, including the vonWillebrand disease (vWB) and other type of deficiencies of factors, does increase this value. To treat these patients, there are different treatment centers scattered through Portugal, and despite the efforts and recognition of the need of a NPR, just recently the protocol for the creation of the system to support the Registry was approved. For this project, the establishment of an expert working group was endorsed, involving a group of physicians and a group of researchers in the field of computer science, aiming to develop the first Web-based platform to support the NPR for HoCC in Portugal. 3.2. Methodology of Development The development process of the web-platform (named hemo@record) followed an iterative and incremental process, which allowed the requirement elicitation and its validation with a group of clinicians, future users of the system [14]. Information collected in this phase was incorporated into a detailed specifications document to guide development, using an object-oriented (OO) environment. After this phase, a detailed description of the registry was submitted to the National Committee for Data Protection in March 2013 and consent for the registry was obtained two months later. The guidelines of the National Data Protection Agency on confidentiality were carefully followed along the development process of the technological solution and the web-based application that supports the NPR for HoCC in Portugal is operated and maintained in accordance with the Data Protection Law. The data will be collected in the application after consent of the patient. 3.3. Brief Description of the technological solution developed The hemo@record was developed to store and manage a set of anonymous data from patients with HoCC, and to be used by clinicians that treat people with this disease scattered over the country. There are two different profiles of users (actors) with access credentials to use the system, namely: Âƒ Physician - it includes the doctors that treat PWH and have permission to perform a set of tasks, including introduction, update, deletion of their patient's data, as well as, query and visualize information (with some critical data hidden), from all the patients at national level; Âƒ Coordinator â€“ it is also a doctor with administration permissions, having access to all the functionalities of the system. 1251 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 Each patient has his/her personal data hidden and is identified by an anonymous code, having associated a set of records that reflects his/her clinical history in terms of health through time. To provide an efficient set of functionalities to the users, the system is organized through a set of 8 modules, managing demographic, clinical and treatment data (see Fig.1). Fig.1 - Snapshot of modules present in hemo@care (user interfaces in Portuguese). Âƒ Two of those modules are statistical modules, presenting a set of statistical indicators, at national and local level; Âƒ Six modules providing data management functionalities (insert, update, delete, export): Modules for administration and audit purposes (available to the coordinator only), and modules for patient management, registry management, treatments management and death management. Modules to visualize national and local statistics The national statistics module was designed to present a set of national indicators, reflecting the current situation, encompassing the data from all the patients introduced in the system by their doctors. These indicators provide a complete analysis of the national situation, namely: (i) distribution of patients per area of residence; (ii) distribution of patients per Hemophilia Treatment Center (HTC) or Hospital; (iii) distribution of patients per type of coagulopathy; (iv) distribution of patients per type of treatment; (v) distribution of patients per severity of disease; (vi) distribution of patients per treatment regimen; (vii) evolution in the number of occurrences; and, (viii) evolution in the number of deaths. The local statistics module presents the same indicators, with the exception of those that identify the geographic location of the patient, such as: â€˜distribution of patients by place of residenceâ€™ and â€˜distribution of patients per HTC or Hospital (see Fig.2) Fig.2 - Snapshot of user interface with indicators of local statistics module (interfaces in Portuguese). 1252 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 In a nutshell, the difference between these two modules is that the former present all the national data, while the later only presents the local data associated with the user HTC. Each statistics module is presented in a specific chart (pie chart, bar chart, etc.) that is appropriate to the type of data that is provided by the module (see examples in Fig.3). Distribution of patients by type of coagulopathy Distribution of patients by HTC/ Hospital Evolution in the number of occurrences Fig.3 - Snapshots with examples of some indicators (in Portuguese with fictitious data). Modules to manage patientsâ€™ data The remaining six modules provide the information to manage all patient data. The first step to use the web-application is the introduction of the patient basic data, i.e., the anonymous and unique patient code, complemented with a set of demographical data as gender, HTC or Hospital, data of birth, place and country of birth, and place and country of residence, (Figure 4 - left side). The patient is introduced only once, the system having an automatic process that identifies possible duplications, triggering automatic mechanisms for alerting the Coordinator. After registering the patient, it is possible to introduce registries for that patient. Each registry stores the patient's health condition at a specific instant of time, more specifically the clinical condition or/and the social and life quality information. To be able to assess the evaluation through time, each time a change on a given situation or medical condition occurs; the system automatically creates a new record. As such, each patient can have several registries, and through the analysis off all the records, is possible to determine the evolution of the patient's health condition, social and quality of life evolution. The system presents patients listings in a proprietary grid view, using a color representation in order to promptly identify different situations, like patients without associated registries, patients that belong to the same HTC as the physician, patients already diseased, etc. (Figure 4 - right side). Fig.4 - Snapshot of some aspects of the user interface (in Portuguese with fictitious data). 1253 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 Due to the laws of privacy and data protection, the identifying data of patients from other HTC's (anonymous code and hospital name), are hidden. As can be seen in Fig.4 (right side), there are several functionalities that can be performed after selecting a patient, such as: create or update patient's registries, create or update patient treatment details (treatment date, drug name and consumed dosage). To facilitate the statistical analysis and detect trends, the data can be exported to a Microsoft Excel format for posterior analysis. All the grids provide powerful filter and sorting mechanisms, having each field a specific filter system adapted to the type of data hosted in that field (for example, a date field allows to filter dates within a chosen interval, etc.). The system also provides a complete solution to manage the details of the patient treatments, according to the rules imposed by the WFH such as: create or update data about Coagulation Factor Concentrate (CFC) products (drugs used in Hemophilia treatments), create or update suppliers, create or update patient consumption details, etc. Fig.5 (left side) presents the example of interface that allows to record a patient's treatment details, while Fig.5 (right side) shows all the treatments presented on the system proprietary grid, providing the same powerful filtering and sorting mechanisms. Fig.5 - Snapshot of some aspects of the user nterfaces (in Portuguese with fictitious data). 3.4 . Technological Solution From a technological standpoint, the hemo@record web-application was developed using the Spring development Framework, a Java platform that provides comprehensive infrastructure support for developing Enterprise Java applications [15]. The advantage of using the Spring framework is that it handles the infrastructure support details, allowing developers to focus on the business logic of the system. The Web frontend was developed using Sencha GXT, a Java-based framework for building feature-rich web-applications based on GWT. â€œGXT features high- performance user interfaces (UI) widgets that are interoperable with native GWT components, templates, and layout managerâ€� [16]. GWT is a development toolkit for building and optimizing complex browser-based applications. Its goal is to enable productive development of high-performance web- applications without the need to be an expert on web technologies such as HTML, JavaScript, CSS, etc. Using GWT, the cross-browser inconsistency is eliminated, as GWT compiles a specific version optimized for a specific browser. The Data Base Management System (DBMS) used was Oracle MySQL. In order to avoid the complexities of the underlying Data Base Management System (DBMS), and, if needed, to easily swap the DBMS used, an Object/Relational Mapping (ORM) framework was used. Hibernate is an ORM framework that allows the use of natural Object-oriented idioms including inheritance, polymorphism, association, composition, and the Java collections framework to manipulate database data [17]. The hemo@record runs on an Apache Tomcat Web server, an open source software implementation of the Java Servlet and JavaServer Pages technologies. In a nutshell, each use-case of the web-application generates a request that is processed by the server that runs in specific business logic, retrieves the requested data from the hemo@record database, transforming the data in Java objects by the use of the ORM framework. These objects transport the data extracted from the database, and are returned to the web-application. The browser that renders the web-application 1254 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 interprets the data, using the compiled GXT widgets to present it as useful and relevant information for the end user. 4. Conclusions The use of Information and Communication Technologies (ICT) in rare and chronic disease have shown numerous advantages in terms of data management and information retrieval, with great impact on the quality of health care provision, research and public health monitoring trends. In the context of hemophilia, several countries had already taken advantage of ICT, implementing technological solutions to support the National Registry of Patients. Portugal is a small country with about 10 million inhabitants and 1000 expected PWH. Despite the fact that the national patient registry is a desire project from several years, the protocol for the creation of an online national registry was only recently signed, involving a group of healthcare professionals, represented by the PACC and a group of researchers from the University of Aveiro. This paper described a web platform that supports the National Patientsâ€™ Registries (NPR) of Haemophilia and other Congenital Coagulopathies (HoCC) in Portugal. This recently created platform (hemo@record) allows a complete solution for managing the complexity of data associated to Haemophilia and Congenital Coagulopathies patients, including clinical, social and health status monitoring data, thus reflecting in real time the national reality in terms of HoCC data. As such, the hemo@record can give answer in real time to questions such as: 'How many?', 'Where are they?' and 'How are they?â€™ assuring a proper and informed treatment, provided by clinicians of the several HTC from all over the country. Acknowledgments We would also like to acknowledge the valuable contribution of the clinical professionals of Portuguese Association of Congenital Coagulopathies (PACC) for their contribution towards this project. This work is funded by National Funds through FCT - Foundation for Science and Technology, in the context of the project PEst-OE/EEI/UI0127/2014. References 1. Evatt B: World Federation of Hemophilia Guide to Developing a National Patient Registry. MontrÃ©al - QuÃ©bec, 2005. 2. Srivastava A, Brewer AK, Mauser-Bunschoten EP, Key NS, Kitchen S, Llinas A, et al.: Guidelines for the management of hemophilia. Haemophilia 2013; 19:e1â€“47. 3. Viviani L, Zolin A, Mehta A, Olesen HV: The European Cystic Fibrosis Society Patient Registry: valuable lessons learned on how to sustain a disease registry. Orphanet J Rare Dis 2014 ; 9:81. 4. Lara B, Morales P, Blanco I, Vendrell M, de Gracia RoldÃ¡n J, Monreal M, et al.: Respiratory Disease Registries in Spain: Fundamentals and Organization [Internet]. Arch Bronconeumol (English Version) 2011;47:389â€“396. 5. World Federation of Hemophilia: World Federation of Hemophilia Report on the ANNUAL GLOBAL SURVEY 2011. MontrÃ©al - QuÃ©bec, 2012. 6. Reitter S, Streif W, Schabetsberger T, Wozak F, Hartl H, Male C, et al.: Austrian Hemophilia Registry: design, development and set of variables. Wien Klin Wochenschr 2009; 121:196â€“201. 7. Oâ€™Mahony B, Noone D, Giangrande PLF, Prihodova L: Haemophilia care in Europe - a survey of 35 countries. Haemophilia 201; 19:e239â€“ 47. 8. Aznar Ja, Abad-Franch L, Cortina VR, Marco P: The national registry of haemophilia A and B in Spain: results from a census of patients. Haemophilia 2009; 15:1327â€“30. 9. Zdziarska J, Chojnowski K, Klukowska a, Ã ÄŠtowska M, Mital a, MusiaÃ¡ J, et al.: Registry of inherited bleeding disorders in Poland--current status and potential role of the HemoRec database. Haemophilia 2011; 17:e189â€“95. 10. Von der Weid N: Haemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010â€“2011 [Internet]. . Hamostaseologie 2012; 32:S20â€“S24. 11. Hay C: The UK Haemophilia database: a tool for research, audit and healthcare planning. Haemophilia 2004; 10:21â€“25. 12. Hesse J, Haschberger B, Heiden M, Seitz R, Schramm W: Neue Daten aus dem Deutschen HÃ¤mophilieregister [Internet]. . Hamostaseologie 2013; 33:S15â€“S21. 13. Walker I, Pai M, Akabutu J, Ritchie B, Growe G, Poon MC, et al.: The Canadian Hemophilia Registry as the basis for a national system for monitoring the use of factor concentrates. Transfusion 1995; 35:548â€“51. 1255 Leonor Teixeira et al. / Procedia Computer Science 64 ( 2015 ) 1248 â€“ 1255 14. Teixeira L, Saavedra V, Ferreira C, SimÃµes J, Sousa Santos B: Requirements engineering using mockups and prototyping tools: developing an healthcare web-application; in Yamamoto S (ed): HCI International. Creta, Springer International Publishing Switzerland, 2014, pp 652â€“ 663. 15. Rod Johnson, Juergen Hoeller et al.: Spring Framework Reference Documentation [Internet] 2014 [cited 2015 Apr 1];Available from: 16. Sencha GXT: Create feature-rich HTML5 applications using Java [Internet] 2015 [cited 2015 Apr 1];Available from: 17. Hibernate: Hibernate ORM [Internet] [cited 2015 Apr 10];Available from: pathies (HoCC) in Portugal. This recently created platform (hemo@record) allows a complete solution for managing the complexity of data associated to Haemophilia and Congenital Coagulopathies patients, including clinical, social and health status monitoring data, thus reflecting in real time the national reality in terms of HoCC data. As such, the hemo@record can give answer in real time to questions such as: 'How many?', 'Where are they?' and 'How are they?â€™ assuring a proper and informed treatment, provided by clinicians of the several HTC from all over the country. Acknowledgments We would also like to acknowledge the valuable contribution of the clinical professionals of Portuguese Association of Congenital Coagulopathies (PACC) for their contribution towards this project. This work is funded by National Funds through FCT - Foundation for Science and Technology, in the context of the project PEst-OE/EEI/UI0127/2014. References 1. Evatt B: World Federation of Hemophilia Guide to Developing a National Patient Registry. MontrÃ©al - QuÃ©bec, 2005. 2. Srivastava A, Brewer AK, Mauser-Bunschoten EP, Key NS, Kitchen S, Llinas A, et al.: Guidelines for the management of hemophilia. Haemophilia 2013; 19:e1â€“47. 3. Viviani L, Zolin A, Mehta A, Olesen HV: The European Cystic Fibrosis Society Patient Registry: valuable lessons learned on how to sustain a disease registry. Orphanet J Rare Dis 2014 ; 9:81. 4. Lara B, Morales P, Blanco I, Vendrell M, de Gracia RoldÃ¡n J, Monreal M, et al.: Respiratory Disease Registries in Spain: Fundamentals and Organization [Internet]. Arch Bronconeumol (English Version) 2011;47:389â€“396. 5. World Federation of Hemophilia: World Federation of Hemophilia Report on the ANNUAL GLOBAL SURVEY 2011. MontrÃ©al - QuÃ©bec, 2012. 6. Reitter S, Streif W, Schabetsberger T, Wozak F, Hartl H, Male C, et al.: Austrian Hemophilia Registry: design, development and set of variables. Wien Klin Wochenschr 2009; 121:196â€“201. 7. Oâ€™Mahony B, Noone D, Giangrande PLF, Prihodova L: Haemophilia care in Europe - a survey of 35 countries. Haemophilia 201; 19:e239â€“ 47. 8. Aznar Ja, Abad-Franch L, Cortina VR, Marco P: The national registry of haemophilia A and B in Spain: results from a census of patients. Haemophilia 2009; 15:1327â€“30. 9. Zdziarska J, Chojnowski K, Klukowska a, Ã ÄŠtowska M, Mital a, MusiaÃ¡ J, et al.: Registry of inherited bleeding disorders in Poland--current status and potential role of the HemoRec database. Haemophilia 2011; 17:e189â€“95. 10. Von der Weid N: Haemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010â€“2011 [Internet]. . Hamostaseologie 2012; 32:S20â€“S24. 11. Hay C: The UK Haemophilia database: a tool for research, audit and healthcare planning. Haemophilia 2004; 10:21â€“25. 12. Hesse J, Haschberger B, Heid PROCS 6859 S1877-0509(15)02642-3 10.1016/j.procs.2015.08.507 The Authors ☆ Peer-review under responsibility of SciKA - Association for Promotion and Dissemination of Scientific Knowledge. The Portuguese National Registry for Hemophilia: Developing of a Web-based Technological Solution Leonor Teixeira a c ⁎ Vasco Saavedra a João Pedro Simões d Beatriz Sousa Santos b c Carlos Ferreira a c a Department of Economics, Management and Industrial Engineering, University of Aveiro, 3810-193 Aveiro, Portugal Department of Economics, Management and Industrial Engineering, University of Aveiro, 3810-193 Aveiro Portugal b Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunications and Informatics, University of Aveiro, 3810-193 Aveiro Portugal c IEETA - Institute of Telematics and Electronic Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro, Portugal IEETA - Institute of Telematics and Electronic Engineering of Aveiro, University of Aveiro, 3810-193 Aveiro Portugal d Truphone, Lisbon, 2740-120 Oeiras, Portugal Truphone, Lisbon, 2740-120 Oeiras Portugal ⁎ Corresponding author. Tel.: +351-234-370361; fax: +351-234-370215. The crucial role that patient records have in the management of the rare and chronic diseases greatly increases the need to create mechanisms to facilitate the identification and management of the patient's data. Hemophilia is an X-linked congenital bleeding disorder caused by a deficiency of coagulation factor that affects the population on a ratio of 1 case for 10,000 people born. Currently, there are several countries with technological platforms to support the National Patients’ Registries (NPR) of Hemophilia and other Congenital Coagulopathies (HoCC), due to its benefits in the management of the disease. This work presents the technological platform developed in a joint initiative between the University of Aveiro (UA) and the Portuguese Association of Congenital Coagulopathies (PACC), with the purpose of creating the first NPR with HoCC in Portugal. This web application is hosted in the data center of the University of Aveiro, and is being used by the clinicians of the different Hemophilia Treatment Centers (HTC) across the country. Keywords National Pacients Registry Web-application Hemophilia Congenital Coagulopathies References [1] Evatt B: World Federation of Hemophilia Guide to Developing a National Patient Registry. Montréal - Québec, 2005. [2] A. Srivastava A.K. Brewer E.P. Mauser-Bunschoten N.S. Key S. Kitchen A. Llinas Guidelines for the management of hemophilia Haemophilia 19 2013 e1 e47 [3] L. Viviani A. Zolin A. Mehta Olesen HV: The European Cystic Fibrosis Society Patient Registry: valuable lessons learned on how to sustain a disease registry Orphanet J Rare Dis 9 2014 81 [4] B. Lara P. Morales I. Blanco M. Vendrell J. de Gracia Roldán M. Monreal Respiratory Disease Registries in Spain: Fundamentals and Organization [Internet] Arch Bronconeumol (English Version) 47 2011 389 396 [5] World Federation of Hemophilia: World Federation of Hemophilia Report on the ANNUAL GLOBAL SURVEY 2011. Montréal - Québec, 2012. [6] Reitter S, Streif W, Schabetsberger T, Wozak F, Hartl H, Male C, et al.: Austrian Hemophilia Registry: design, development and set of variables. Wien Klin Wochenschr 2009; 121:196-201. [7] O’Mahony B, Noone D, Giangrande PLF, Prihodova L: Haemophilia care in Europe - a survey of 35 countries. Haemophilia 201; 19:e239-47. [8] Aznar Ja L. Abad-Franch V.R. Cortina Marco P: The national registry of haemophilia A and B in Spain: results from a census of patients. Haemophilia 15 2009 1327 1330 [9] J. Zdziarska K. Chojnowski Klukowska a M. Łętowska a Mital J. Musiał Registry of inherited bleeding disorders in Poland--current status and potential role of the HemoRec database Haemophilia 17 2011 e189 e195 [10] Von der Weid N: Haemophilia registry of the Medical Committee of the Swiss Hemophilia Society. Update and annual survey 2010-2011 [Internet]. . Hamostaseologie 2012; 32:S20-S24. [11] C. Hay The UK Haemophilia database: a tool for research, audit and healthcare planning Haemophilia 10 2004 21 25 [12] J. Hesse B. Haschberger M. Heiden R. Seitz Schramm W: Neue Daten aus dem Deutschen Hämophilieregister [Internet] Hamostaseologie 33 2013 S15 S21 [13] I. Walker M. Pai J. Akabutu B. Ritchie G. Growe M.C. Poon The Canadian Hemophilia Registry as the basis for a national system for monitoring the use of factor concentrates Transfusion 35 1995 548 551 [14] Teixeira L, Saavedra V, Ferreira C, Simões J, Sousa Santos B: Requirements engineering using mockups and prototyping tools: developing an healthcare web-application; in Yamamoto S (ed): HCI International. Creta, Springer International Publishing Switzerland, 2014, pp 652-663. [15] Rod Johnson, Juergen Hoeller et al.: Spring Framework Reference Documentation [Internet] 2014 [cited 2015 Apr 1];Available from: [16] Sencha GXT: Create feature-rich HTML5 applications using Java [Internet] 2015 [cited 2015 Apr 1];Available from: [17] Hibernate: Hibernate ORM [Internet] [cited 2015 Apr 10];Available from: "
    },
    {
        "doc_title": "Developing and evaluating two gestural-based virtual environment navigation methods for large displays",
        "doc_scopus_id": "84947297363",
        "doc_doi": "10.1007/978-3-319-20804-6_13",
        "doc_eid": "2-s2.0-84947297363",
        "doc_date": "2015-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "3DUIs",
            "Depth sensors",
            "Gestural interaction",
            "Large displays",
            "Navigation in virtual en-vironments",
            "Navigation methods",
            "User study"
        ],
        "doc_abstract": "© Springer International Publishing Switzerland 2015.In this paper we present two methods to navigate in virtual environments displayed in a large display using gestures detected by a depth sensor. We describe the rationale behind the development of these methods and a user study to compare their usability performed with the collaboration of 17 participants. The results suggest the users have a better performance and prefer one of them, while considering both as suitable and natural navigation methods.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Tax professionals' perception of tax system complexity: Some preliminary empirical evidence from Portugal",
        "doc_scopus_id": "84930358014",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84930358014",
        "doc_date": "2015-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Accounting",
                "area_abbreviation": "BUSI",
                "area_code": "1402"
            },
            {
                "area_name": "Finance",
                "area_abbreviation": "ECON",
                "area_code": "2003"
            },
            {
                "area_name": "Economics and Econometrics",
                "area_abbreviation": "ECON",
                "area_code": "2002"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This paper analyses tax professionals' (TOCs) perception of tax complexity within the Portuguese fiscal system. This study is relevant to the international tax literature research for two reasons. Firstly, its intention is to determine the dimensions of the endogenous causes of tax complexity, creating indices of these causes using the Principal Component Analysis (PCA) method. Secondly, it aims to identify the factors that could influence the level of tax complexity perceived by TOCs. In 2013, a survey was conducted in Portugal to evaluate TOCs' perception of tax complexity. This paper presents the results collected from 994 questionnaires responded to by TOCs. The survey findings concluded that TOCs perceived three dimensions of causes of tax complexity: «Legal Complexity»; «Complexity of Preparation of Information and Record Keeping»; and «Complexity of Tax Forms». The exogenous factors include tax knowledge, with a negative effect, and size of companies, with a positive effect on TOCs' perception of tax complexity. Understanding these relationships can be a key issue for tax policy makers, in order to reduce their negative effects on the perception of tax complexity. Therefore, this paper contributes to the international tax literature by presenting empirical evidence concerning the dimensions of tax complexity.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A tool for visually exploring multi-objective mixed-integer optimization models",
        "doc_scopus_id": "84912080897",
        "doc_doi": "10.1109/IV.2014.29",
        "doc_eid": "2-s2.0-84912080897",
        "doc_date": "2014-09-18",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Conflicting objectives",
            "Interactive visualization tool",
            "Mixed integer optimization",
            "Multi-objective optimization models",
            "Multi-objective optimization problem",
            "Optimal solutions",
            "Preferred solutions",
            "Visual exploration"
        ],
        "doc_abstract": "© 2014 IEEE.Multi-objective optimization models have been increasingly used as optimal decisions are searched in settings considering several conflicting objectives. In these cases compromises must be made and often a large number of non-dominated optimal solutions exist. From these solutions decision-makers must find the preferred one. This is a difficult task both from a computational and cognitive point of views, as it requires several solutions to be obtained and compared. An interactive visualization tool for fully understanding the best trade-offs is therefore becoming increasingly important. This paper proposes visualization solutions, implemented in a tool, for aiding decision-makers in finding the preferred solution in multi-objective optimization problems.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The role of ICTs in the management of rare chronic diseases: The case of hemophilia",
        "doc_scopus_id": "84945379874",
        "doc_doi": "10.4018/978-1-4666-6339-8.ch067",
        "doc_eid": "2-s2.0-84945379874",
        "doc_date": "2014-08-31",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "© 2015 by IGI Global. All rights reserved.This chapter outlines a study that examines the role of Information and Communication Technologies (ICTs) in management of a rare and chronic disease, hemophilia. Evidence in literature shows how the adoption of ICTs can improve the management of chronic conditions. Furthermore, these tools may also give response to rare diseases' needs, while greatly improving the quality of life of those patients. A Web-based application that was developed to facilitate communication between Healthcare Professionals (HCPs) and patients in a specific Hemophilia Treatment Center (HTC), to improve the utility and quality of clinical data and treatment information, as well as to help the management of resources involved in a specific rare chronic disease, represents a practical case presented in this chapter. This technological solution allows the management of inherited bleeding disorders, integrating, diffusing, and archiving large sets of data relating to the clinical practice of hemophilia care, more specifically the clinical practice at the Hematology Service of Coimbra Hospital Center.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Requirements engineering using mockups and prototyping tools: Developing a healthcare web-application",
        "doc_scopus_id": "84904115337",
        "doc_doi": "10.1007/978-3-319-07731-4_64",
        "doc_eid": "2-s2.0-84904115337",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Information transfers",
            "Levels of abstraction",
            "Rapid prototyping model",
            "Requirement elicitation",
            "Requirement engineering",
            "Requirements engineering process",
            "Requirements specifications",
            "Web applications"
        ],
        "doc_abstract": "Healthcare web-application development teams involve non-computer experts working (clinicians) on the requirements specification that is later processed by software engineers/analysts (conceptual model) and coded by software programmers (software project). The management of this process, which involves different levels of abstraction and professionals with different backgrounds, is often complex. As such, mediators and facilitator's mechanisms for the requirements-gathering process and information transfer are needed. The main purpose of this work is to minimize the problems associated with this complex process, supporting the requirements engineering process of a healthcare web-application in a rapid prototyping model. The results proved that a rapid and functional prototyping model can improve the effectiveness of the requirement elicitation of any software development. © 2014 Springer International Publishing.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing two input devices for virtual walkthroughs using a head mounted display (HMD)",
        "doc_scopus_id": "84901800305",
        "doc_doi": "10.1117/12.2036486",
        "doc_eid": "2-s2.0-84901800305",
        "doc_date": "2014-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Ease-of-use",
            "Head mounted displays",
            "Input and outputs",
            "Input devices",
            "User performance",
            "User study",
            "Walkthroughs"
        ],
        "doc_abstract": "Selecting input and output devices to be used in virtual walkthroughs is an important issue as it may have significant impact in usability and comfort. This paper presents a user study meant to compare the usability of two input devices used for walkthroughs in a virtual environment with a Head-Mounted Display. User performance, satisfaction, ease of use and comfort, were compared with two different input devices: a two button mouse and a joystick from a gamepad. Participants also used a desktop to perform the same tasks in order to assess if the participant groups had similar profiles. The results obtained by 45 participants suggest that both input devices have a comparable usability in the used conditions and show that participants generally performed better with the desktop; a discussion of possible causes is presented. © 2014 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Location-arc routing problem: Heuristic approaches and test instances",
        "doc_scopus_id": "84887109658",
        "doc_doi": "10.1016/j.cor.2013.10.003",
        "doc_eid": "2-s2.0-84887109658",
        "doc_date": "2014-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            }
        ],
        "doc_keywords": [
            "Arc routing",
            "Capacitated arc routing problem",
            "Garbage collection",
            "Heuristic approach",
            "Heuristics",
            "Improvement methods",
            "Location-routing",
            "Test instances"
        ],
        "doc_abstract": "Location-routing is a branch of locational analysis that takes into account distribution aspects. The location-arc routing problem (LARP) considers scenarios where the demand is on the edges rather than being on the nodes of a network (usually a road network is assumed). Examples of such scenarios include locating facilities for postal delivery, garbage collection, road maintenance, winter gritting and street sweeping. This paper presents some heuristic approaches to tackle the LARP, as well as some proposals for benchmark instances (and corresponding results). New constructive and improvement methods are presented and used within different metaheuristic frameworks. Test instances were obtained from the capacitated arc routing problem (CARP) literature and adapted to address the LARP. © 2013 Elsevier Ltd.",
        "available": true,
        "clean_text": "serial JL 271709 291210 291692 291715 291772 291813 291817 291871 31 Computers & Operations Research COMPUTERSOPERATIONSRESEARCH 2013-10-21 2013-10-21 2013-12-03T05:28:33 S0305-0548(13)00298-0 S0305054813002980 10.1016/j.cor.2013.10.003 S300 S300.1 FULL-TEXT 2015-05-14T07:00:29.889601-04:00 0 0 20140301 20140331 2014 2013-10-21T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings suppl volfirst volissue figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0305-0548 03050548 true 43 43 C Volume 43 30 309 317 309 317 201403 March 2014 2014-03-01 2014-03-31 2014 article fla Copyright © 2013 Elsevier Ltd. All rights reserved. LOCATIONARCROUTINGPROBLEMHEURISTICAPPROACHESTESTINSTANCES LOPES R 1 Introduction 2 Problem definition 3 Constructive methods and improvement heuristics 3.1 Extended augment-merge 3.2 Extended merge 3.3 Reverse 3.4 Relocate 4 Metaheuristics 4.1 Tabu search-variable neighborhood search 4.1.1 Variable neighborhood search 4.1.2 Tabu search 4.2 Greedy randomized adaptive search procedure 4.3 Tabu search-greedy randomized adaptive search procedure 4.3.1 Greedy randomized adaptive search procedure 4.3.2 Tabu search 5 Computational results 5.1 Implementation and benchmark instances 5.2 Comparative analysis 6 Summary and conclusions References NAGY 2007 649 672 G LOPES 2013 795 822 R GHIANI 2001 151 159 G GOLDEN 1981 305 315 B PEARN 1987 285 288 W BALDACCI 2006 52 60 R LONGO 2006 1823 1837 H WOHLK 2008 29 48 S VEHICLEROUTINGPROBLEMLATESTADVANCESNEWCHALLENGES ADECADECAPACITATEDARCROUTING LEVY 1989 74 94 L LAPORTE 1988 163 198 G VEHICLEROUTINGMETHODSSTUDIES LOCATIONROUTINGPROBLEMS GHIANI 1999 291 302 G PIA 2006 125 141 A AMAYA 2007 45 53 A 1990 DISCRETELOCATIONTHEORY HERTZ 2003 247 252 A CLARKE 1964 568 581 G BELENGUER 2006 3363 3383 J PRINS 2006 221 238 C BEULLENS 2003 629 643 P LIN 1973 498 516 S SAVELSBERGH 1992 146 154 M 2003 HANDBOOKMETAHEURISTICS TALBI 2009 E METAHEURISTICSDESIGNIMPLEMENTATION MLADENOVIC 1997 1097 1100 N HANSEN 2001 449 467 P POLACEK 2008 405 423 M TAILLARD 1997 170 186 E FILHO 1998 189 209 V GLOVER 1986 533 549 F BARRETO 2007 968 977 S RESENDE 2003 219 249 M HANDBOOKMETAHEURISTICS GREEDYRANDOMIZEDADAPTIVESEARCHPROCEDURES DIJKSTRA 1959 269 271 E GOLDEN 1983 47 59 B BENAVENT 1992 669 690 E EGLESE 1994 231 244 R LOPESX2014X309 LOPESX2014X309X317 LOPESX2014X309XR LOPESX2014X309X317XR item S0305-0548(13)00298-0 S0305054813002980 10.1016/j.cor.2013.10.003 271709 2013-12-03T04:43:41.701828-05:00 2014-03-01 2014-03-31 true 1355243 MAIN 9 62354 849 656 IMAGE-WEB-PDF 1 si0183 123 10 10 si0182 148 13 13 si0181 131 15 10 si0180 138 8 13 si0179 133 10 10 si0178 128 10 10 si0177 131 10 12 si0176 151 13 14 si0175 376 11 67 si0174 167 10 17 si0173 540 14 91 si0172 167 10 17 si0171 248 10 35 si0170 425 14 67 si0169 300 11 44 si0168 352 13 53 si0167 763 17 165 si0166 248 10 35 si0165 167 10 17 si0164 358 11 72 si0163 208 11 29 si0162 132 10 10 si0161 158 13 15 si0160 246 13 38 si0159 167 10 17 si0158 503 13 105 si0157 409 12 70 si0156 166 10 18 si0155 186 11 20 si0154 300 11 44 si0153 167 10 17 si0152 316 10 69 si0151 352 11 71 si0150 332 10 69 si0149 322 11 51 si0148 405 14 65 si0147 167 10 17 si0146 120 13 9 si0145 246 13 38 si0144 167 10 17 si0143 158 13 15 si0142 132 10 10 si0141 266 14 56 si0140 208 11 29 si0139 285 14 53 si0138 144 11 12 si0137 144 11 12 si0136 144 11 12 si0135 431 16 73 si0134 409 16 63 si0133 455 14 76 si0132 317 16 47 si0131 300 11 44 si0130 261 10 40 si0129 167 10 17 si0128 272 16 38 si0127 422 16 74 si0126 167 10 17 si0125 158 13 15 si0124 132 10 10 si0123 156 12 15 si0122 193 13 36 si0121 216 13 43 si0120 122 10 10 si0119 156 12 15 si0118 156 12 15 si0117 111 11 6 si0116 158 13 15 si0115 288 8 59 si0114 274 8 46 si0113 162 13 14 si0112 246 13 38 si0111 167 10 17 si0110 319 9 57 si0109 513 15 94 si0108 132 10 11 si0107 296 15 42 si0106 513 15 94 si0105 111 11 6 si0104 128 11 9 si0103 118 13 8 si0102 111 10 6 si0101 122 10 10 si0100 111 9 7 si0099 147 12 13 si0098 124 10 9 si0097 133 10 10 si0096 112 8 7 si0095 114 8 8 si0094 154 13 14 si0093 155 13 14 si0092 1135 16 361 si0091 111 6 9 si0090 138 8 13 si0089 122 10 10 si0088 206 13 32 si0087 175 13 20 si0086 634 15 163 si0085 461 15 108 si0084 661 17 170 si0083 703 23 139 si0082 1272 27 327 si0081 877 25 213 si0080 633 27 136 si0079 1039 29 249 si0078 656 23 134 si0077 628 22 160 si0076 1516 27 334 si0075 118 13 8 si0074 202 10 30 si0073 175 13 20 si0072 118 13 8 si0071 150 13 12 si0070 206 11 31 si0069 209 14 30 si0068 163 11 19 si0067 159 12 16 si0066 156 14 17 si0065 117 8 9 si0064 156 14 17 si0063 117 8 9 si0062 124 10 9 si0061 124 10 9 si0060 124 10 9 si0059 118 10 9 si0058 124 10 9 si0057 124 10 9 si0056 147 12 17 si0055 124 10 9 si0054 156 14 17 si0053 141 13 12 si0052 124 10 9 si0051 151 13 14 si0050 202 10 30 si0049 134 10 13 si0048 146 13 13 si0047 143 14 11 si0046 267 13 49 si0045 143 14 11 si0044 133 10 10 si0043 242 16 43 si0042 142 14 11 si0041 296 16 65 si0040 120 13 9 si0039 225 12 43 si0038 133 10 10 si0037 158 12 17 si0036 141 13 12 si0035 332 17 61 si0034 130 10 11 si0033 133 10 10 si0032 120 13 9 si0031 118 13 8 si0030 111 10 6 si0029 130 10 11 si0028 151 13 14 si0027 118 13 8 si0026 111 10 6 si0025 123 10 10 si0024 148 13 13 si0023 135 10 11 si0022 133 10 10 si0021 155 13 14 si0020 134 10 13 si0019 132 10 11 si0018 267 13 49 si0017 178 13 26 si0016 152 16 12 si0015 145 17 12 si0014 205 13 34 si0013 138 8 13 si0012 120 13 9 si0011 131 10 12 si0010 132 10 11 si0009 131 10 12 si0008 300 13 61 si0007 123 10 10 si0006 148 13 13 si0005 131 15 10 si0004 138 8 13 si0003 159 13 15 si0002 157 13 15 si0001 159 13 17 gr8 49353 397 376 gr7 29076 326 263 gr6 29735 426 226 gr5 23201 241 263 gr4 32351 207 509 gr3 13456 163 378 gr2 23380 265 550 gr1 23191 165 532 gr8 5355 164 155 gr7 2991 164 132 gr6 2250 164 87 gr5 3610 164 179 gr4 2854 89 219 gr3 1716 94 219 gr2 1952 105 219 gr1 2053 68 219 CAOR 3436 S0305-0548(13)00298-0 10.1016/j.cor.2013.10.003 Elsevier Ltd © 2013 Elsevier Ltd. All rights reserved. Fig. 1 Augment (left) and merge (right) moves in augment-merge [3]. Fig. 2 Some merges in the merge phase of the EAM and EM algorithms. Fig. 3 The reverse move applied to a route. Fig. 4 Steps of the basic VNS [26]. Fig. 5 Overview of the proposed TS–VNS metaheuristic for the LARP. Fig. 6 Overview of the proposed GRASP metaheuristic for the LARP. Fig. 7 Overview of the proposed TS–GRASP metaheuristic for the LARP. Fig. 8 Graphical representation of an optimal solution for instance “gdb-1”. Table 1 Data concerning the proposed instances for the LARP. Instance | V | | E | | R | m f ¯ Q F 1 gdb-20 11 22 22 3 10 27 1 2 gdb-21 11 33 33 3 8 27 1 3 gdb-22 11 44 44 3 5 27 1 4 gdb-23 11 55 55 3 10 27 1 5 gdb-1 12 22 22 3 20 5 5 6 gdb-2 12 26 26 3 25 5 5 7 gdb-12 13 23 23 3 50 35 5 8 gdb-5 13 26 26 3 20 5 2 9 bccm-2A 24 34 34 5 20 180 5 10 bccm-2C 24 34 34 5 50 40 2 11 bccm-3A 24 35 35 5 15 80 2 12 bccm-3C 24 35 35 5 25 20 5 13 bccm-1B 24 39 39 5 15 120 2 14 bccm-1C 24 39 39 5 25 45 8 15 bccm-8A 30 63 63 5 10 200 5 16 bccm-6B 31 50 50 5 40 120 5 17 bccm-5A 34 65 65 5 20 220 5 18 bccm-5C 34 65 65 5 30 130 2 19 bccm-7A 40 66 66 5 5 200 2 20 bccm-4C 41 69 69 5 25 130 2 21 bccm-9B 50 92 92 5 30 175 5 22 bccm-10D 50 97 97 5 20 75 5 23 eglese-E1-A 77 98 51 10 250 305 20 24 eglese-E2-B 77 98 72 10 750 200 20 25 eglese-E3-C 77 98 87 10 1000 135 50 26 eglese-E4-B 77 98 98 10 650 180 75 27 eglese-S1-A 140 190 75 10 400 210 50 28 eglese-S2-B 140 190 147 10 2500 160 100 29 eglese-S3-C 140 190 159 10 1500 120 100 30 eglese-S4-A 140 190 190 10 1000 230 100 Table 2 Average and median results for the EAM and EM constructive methods with and without LS. Method CPU (s) GapLB (%) Average Median Average Median EAM 0.02 0.01 41.48 37.91 EAM+LS 0.02 0.01 38.18 35.93 EM 0.12 0.02 39.74 39.94 EM+LS 0.12 0.02 38.48 37.22 Table 3 Results for the TS–VNS, the GRASP (using EAM and EM) and the TS–GRASP metaheuristic approaches. Instance LB TS–VNS GapLB GRASP–EAM GapLB GRASP–EM GapLB TS–GRASP GapLB Best CPU Best CPU Best CPU Best CPU 1 gdb-20 135 139 0.36 2.96 135 0.03 0.00 135 0.10 0.00 135 0.04 0.00 2 gdb-21 170 176 0.77 3.53 173 0.08 1.76 173 0.28 1.76 171 0.13 0.59 3 gdb-22 210 216 1.38 2.86 215 0.29 2.38 213 0.74 1.43 213 0.29 1.43 4 gdb-23 247 264 1.23 6.88 258 0.57 4.45 254 1.22 2.83 254 0.52 2.83 5 gdb-1 353 359 0.61 1.70 353 0.02 0.00 359 0.10 1.70 353 0.05 0.00 6 gdb-2 379 400 0.47 5.54 400 0.04 5.54 390 0.17 2.90 390 0.11 2.90 7 gdb-12 532 607 0.35 14.10 607 0.16 14.10 543 0.09 2.07 537 0.05 0.94 8 gdb-5 397 419 0.43 5.54 419 0.03 5.54 407 0.16 2.52 405 0.13 2.02 9 bccm-2A 247 249 2.02 0.81 255 0.05 3.24 247 0.40 0.00 247 0.18 0.00 10 bccm-2C 358 384 0.77 7.26 384 0.04 7.26 370 0.35 3.35 370 0.17 3.35 11 bccm-3A 96 99 1.17 3.13 97 0.06 1.04 96 0.45 0.00 96 0.20 0.00 12 bccm-3C 158 174 0.98 10.13 172 0.04 8.86 166 0.44 5.06 162 0.16 2.53 13 bccm-1B 209 220 1.19 5.26 216 0.09 3.35 211 0.59 0.96 211 0.39 0.96 14 bccm-1C 310 348 1.40 12.26 340 0.07 9.68 332 0.53 7.10 325 0.24 4.84 15 bccm-8A 404 433 5.41 7.18 427 0.56 5.69 425 3.07 5.20 424 0.97 4.95 16 bccm-6B 320 335 1.62 4.69 336 0.14 5.00 329 1.38 2.81 329 0.64 2.81 17 bccm-5A 441 464 2.95 5.22 459 0.56 4.08 456 3.02 3.40 450 1.41 2.04 18 bccm-5C 448 488 2.68 8.93 475 0.53 6.03 462 2.88 3.13 463 1.04 3.35 19 bccm-7A 290 304 4.69 4.83 302 0.44 4.14 297 3.01 2.41 298 1.18 2.76 20 bccm-4C 454 467 5.51 2.86 458 0.52 0.88 458 3.35 0.88 458 1.36 0.88 21 bccm-9B 392 415 5.60 5.87 413 1.30 5.36 406 9.52 3.57 408 4.78 4.08 22 bccm-10D 521 553 6.51 6.14 549 1.63 5.37 552 11.17 5.95 544 3.83 4.41 23 eglese-E1-A 2716 3256 6.16 19.88 3231 0.43 18.96 3014 2.07 10.97 2964 2.14 9.13 24 eglese-E2-B 5070 5811 4.97 14.62 5856 0.87 15.50 5584 5.29 10.14 5469 2.20 7.87 25 eglese-E3-C 7727 9147 5.07 18.38 9123 1.42 18.07 8566 10.31 10.86 8665 4.13 12.14 26 eglese-E4-B 7148 8017 7.90 12.16 8100 1.52 13.32 7751 13.40 8.44 7669 6.52 7.29 27 eglese-S1-A 4586 8.06 4404 1.10 4378 8.16 4315 2.78 28 eglese-S2-B 15,820 33.21 15,592 6.76 15,307 55.04 15,069 22.64 29 eglese-S3-C 17,090 40.96 16,933 7.88 16,109 65.01 16,029 24.21 30 eglese-S4-A 12,722 24.29 12,491 10.76 11,977 97.13 11,879 52.35 Average 5.96 7.41 1.27 6.52 9.98 3.82 4.49 3.23 Median 2.35 5.70 0.44 5.37 1.73 2.87 0.81 2.79 Location-arc routing problem: Heuristic approaches and test instances Rui Borges Lopes a ⁎ Frank Plastria b Carlos Ferreira a Beatriz Sousa Santos c a Department of Economics, Management and Industrial Engineering – C.I.O., University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Economics, Management and Industrial Engineering – C.I.O., University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal b Department of Mathematics, Operational Research, Statistics and Information Systems for Management, Vrije Universiteit Brussel, Pleinlaan 2, B-1050 Brussel, Belgium Department of Mathematics, Operational Research, Statistics and Information Systems for Management, Vrije Universiteit Brussel Pleinlaan 2 Brussel B-1050 Belgium c Department of Electronics, Telecommunications and Informatics – I.E.E.T.A., University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal Department of Electronics, Telecommunications and Informatics – I.E.E.T.A., University of Aveiro, Campus Universitário de Santiago Aveiro 3810-193 Portugal ⁎ Corresponding author. Tel.: +351 234370361. Location-routing is a branch of locational analysis that takes into account distribution aspects. The location-arc routing problem (LARP) considers scenarios where the demand is on the edges rather than being on the nodes of a network (usually a road network is assumed). Examples of such scenarios include locating facilities for postal delivery, garbage collection, road maintenance, winter gritting and street sweeping. This paper presents some heuristic approaches to tackle the LARP, as well as some proposals for benchmark instances (and corresponding results). New constructive and improvement methods are presented and used within different metaheuristic frameworks. Test instances were obtained from the capacitated arc routing problem (CARP) literature and adapted to address the LARP. Keywords Location-routing Arc routing Heuristics Test instances 1 Introduction Location-routing problems (LRP) deal with the combination of two types of decisions that often arise: the location of facilities and the design of the distribution routes. While most LRP papers address node routing (see for example [1,2]), one may consider several scenarios where the demand is on the edges rather than being on the nodes of a network (usually a road network is assumed). These problems are referred to in the literature as location-arc routing problems (LARPs), and are derived from the capacitated arc routing problem (CARP) [3]. The LARP is typically overlooked in the literature. It has been shown that node routing problems can be converted into arc routing problems (the capacitated vehicle routing problem – CVRP – can be transformed into the CARP [4]), and that the reverse is also possible, replacing each arc by three [5] or two vertices [6,7], making the two classes of problems equivalent (the same holds true for their location counterparts: the capacitated LRP and the LARP). Still, for the three transformations of the CARP into the CVRP, the resulting instance requires either fixing of variables or the use of edges with infinite cost. Moreover, the resulting CVRP graph is a complete graph of larger size. Hence, the transformation increases the problem size and the planar structure of a usual CARP graph is lost [8] dramatically changing the number of edges from linear to quadratic. The same can be extrapolated to the LARP, motivating its study using dedicated methods and algorithms. The first work on the LARP, by Levy and Bodin [9], intended to tackle a practical problem arising in the scheduling of postal carriers in the United States postal service. The developed algorithm used the location–allocation–routing (L–A–R) concept described by [10] for the LRP, which includes three steps: firstly, depots are to be located using a depot selection procedure; secondly, arcs with demand are to be allocated to depots; thirdly, an Euler tour route of minimum traverse cost is determined for each set of arcs allocated to depots. Ghiani and Laporte [11] addressed an undirected LARP, called the location rural postman problem, in which depots are to be located and routes to be drawn (serving edges with demand), at minimum cost, in an undirected graph. The authors show that the problem can be transformed into a rural postman problem if there is a single depot to open or no bounds on the number of depots. Using an exact branch-and-cut approach they solve the transformed problem. In subsequent work by Ghiani and Laporte [3] a set of common applications for the LARP is mentioned (mail delivery, garbage collection and road maintenance). Furthermore the authors define the LARP as an extension of one of the three classical arc routing problems: the Chinese postman problem, the rural postman problem, and the CARP. The authors also present some insight into heuristic approaches using the decomposition of the problem into location (L), allocation (A) and routing (R) [10]: location–allocation–routing (L–A–R) and allocation–routing–location (A–R–L). Muyldermans [12] presents a variant of the LARP: the p dead-mileage problem. In this problem, unlike the previously addressed LARPs, splitting of the demand is allowed, that is, the client can be serviced more than once. The objective is to minimize dead-mileage (deadheading) and the problem is solved exactly. Finally, the works by Pia and Filippi [13] and Amaya et al. [14] address variants of the CARP with a structure similar to the LARP, respectively, the CARP with mobile depots and the CARP with refill points. In the first, two different types of vehicles are considered: compactors and satellites. Compactors can be seen as mobile depots for the satellites. The second problem considers servicing of arcs by vehicles that must be refilled at certain nodes (to be determined) in order to complete the service. From the previously mentioned variants, the LARP addressed here is the one studied by Ghiani and Laporte [11] which can be seen as the arc routing equivalent to the capacitated LRP, and thus an extension to the CARP. In this paper some heuristic approaches are presented to tackle the LARP, as well as some proposals for benchmark instances (and corresponding results). Regarding the heuristic approaches new constructive and improvement methods are developed and used within different metaheuristic frameworks. The test instances were obtained from the CARP literature and adapted to address the LARP. The remainder of this paper is outlined as follows. In Section 2 a formal definition of the problem is given. Constructive methods and improvement heuristics are presented in Section 3, and used within different metaheuristic frameworks proposed in Section 4. The developed test instances are addressed in Section 5 as well as the corresponding computational results. Finally, conclusions and future research directions are presented in Section 6. 2 Problem definition The LARP consists of determining simultaneously depot location and routes in a graph in order to serve a specified set of required arcs under given operational constraints. Muyldermans [12] has shown that, for this problem, an optimal solution exists with the facilities located on the vertices of the graph. Formally, the LARP can be described on a weighted and directed graph G = ( V , A ) with vertex set V and set of arcs A . The vertex set V contains a non-empty subset J of m potential depot locations ( J ⊆ V ) with a fixed cost f j and an associated capacity b j ( j ∈ J ). Every arc a = ( i , j ) in the arc set A has a non-negative traversal cost c a and a non-negative demand for service d a . The arcs with positive demand form the subset R of the arcs required to be serviced, only once, by a fleet K of identical vehicles with capacity Q . Vehicles start and end their route in the same depot, and each new vehicle (or route, as it is assumed that each vehicle performs a single route) involves a fixed cost F . The movement from the end i of one required arc to the start j of another required arc without servicing the traversed arcs (either required or not) is known as “deadheading”, and has an associated cost denoted by z i j (usually the cost of the shortest path in G from i to j ). The problem aims to determine the set of depots to be opened in J and the tracing of the distribution routes assigned to each open depot in such a way that the sum of fixed and traversal costs to serve all arcs in R is minimized. Assuming G to be connected, it is possible to transform it into a complete graph G ^ = ( V ^ , A ^ ) where V ^ is composed of the set V R of vertices containing the extremities of the arcs in R ( V R ⊆ V ), and J ( V ^ = V R ∪ J ). As G ^ is a complete graph and V R ⊆ V ^ , R is a subset of A ^ . Each arc a = ( i , j ) in the arc set A ^ has a non-negative cost c ^ a which takes on the value c a if a ∈ R , z i j otherwise. For any subset S of vertices in V ^ , let δ + ( S ) ( δ − ( S )), be the set of arcs leaving (entering) S , and L ( S ) the set of arcs with both extremities in S . When S contains a single vertex v , δ + ( v ) is a simplification for δ + ( { v } ). The following binary variables are used: x a k , equal to 1 if and only if arc a ∈ A ^ is used in the route performed by vehicle k ∈ K ; y j , equal to 1 if and only if depot j is to be opened; and w a j , equal to 1 if and only if the arc a ∈ R is assigned to depot j . The LARP can be formulated as: (1) ( LARP ) min Z = ∑ j ∈ J f j y j + ∑ a ∈ A ^ ∑ k ϵ K c ^ a x a k + ∑ k ∈ K ∑ a ∈ δ + ( J ) F x a k (2) s . t . : ∑ k ∈ K x a k = 1 ∀ a ∈ R , (3) ∑ a ∈ R d a x a k ≤ Q ∀ k ∈ K , (4) ∑ a ∈ δ + ( i ) x a k − ∑ a ∈ δ − ( i ) x a k = 0 ∀ i ∈ V ^ , ∀ k ∈ K , (5) ∑ a ∈ δ + ( J ) x a k ≤ 1 ∀ k ∈ K , (6) ∑ a ∈ L ( S ) x a k ≤ | S | − 1 ∀ k ∈ K , ∀ S ⊆ V R , (7) ∑ b ∈ δ + ( j ) ∩ δ − ( V R ) x b k + x a k ≤ 1 + w a j ∀ a ∈ R , ∀ j ∈ J , ∀ k ∈ K , (8) ∑ a ∈ R d a w a j ≤ b j y j ∀ j ∈ J , (9) x a k ∈ { 0 , 1 } ∀ a ∈ A ^ , ∀ k ∈ K , (10) y j ∈ { 0 , 1 } ∀ j ∈ J , (11) w a j ∈ { 0 , 1 } ∀ a ∈ R , ∀ j ∈ J . The objective function (1) minimizes the sum of, respectively, the fixed costs of opening the depots, the costs of all traversed arcs, and the cost of acquiring vehicles. Constraints (2) ensure that each required arc is serviced once by exactly one vehicle. Capacity constraints are satisfied thanks to inequalities (3) and (8). Equalities (4) are the flow conservation constraints which, coupled with constraints (5), ensure the routes return to the departure depot. Constraints (6) are subtour elimination constraints while the set of constraints (7) specify that a required arc must be assigned to a depot in case there is a route linking them. Finally, constraints (9)–(11) define the variables. Note that integrality of w a j can be relaxed to [ 0 , 1 ] because if not pushed to 1 by (7) minimization will choose for 0 due to (8). It can be noted that the LARP considered here can be seen as an extension of the CARP, where multiple depots are considered and an additional level of decision is for locating the depots. 3 Constructive methods and improvement heuristics As the LARP results from the combination of a facility location problem and the CARP, both NP-hard problems [15,4], it is NP-hard. As a consequence, large instances can hardly be solved using exact methods; moreover, sharp bounds on the optimal value are typically hard to obtain. The best way to tackle these problems is then to use heuristic approaches [16] such as constructive methods and improvement heuristics. Constructive methods are commonly used to obtain initial solutions from which improvement heuristics seek to attain better ones. Furthermore, these approaches are often used as the first step to many metaheuristics. In this section constructive methods (extended augment-merge and extended merge) and improvement heuristics (reverse and relocate with both intra- and inter-route moves) are proposed to tackle the LARP. 3.1 Extended augment-merge The augment-merge algorithm (illustrated in Fig. 1) was proposed for the CARP by Golden and Wong [4]. It starts with a trivial solution in which each arc with demand is serviced by a separate route. Then, after sorting the obtained routes in decreasing cost order and starting with the route with highest cost, it proceeds to the augment phase testing if the route already goes through demand arcs on less costly routes. If so, and provided vehicle capacity is preserved, the latter route is augmented into the former (the route with highest cost). Afterwards the algorithm proceeds to the merge phase, where every feasible merge is evaluated for any two routes, merging the routes that provide the highest saving. This is done until no feasible saving exists. This last step is closely related to the well-known “savings” or Clarke and Wright algorithm [17]. The extended version (extended augment-merge – EAM) for the LARP obtains the initial solution by assigning, sequentially, each required arc to the closest depot in which it can fit, thus building a dedicated route. When all required arcs are assigned, the depots without demand to supply are closed. The augment phase is similar to the augment-merge algorithm, increasingly considering depot capacity constraints. In the merge phase of the EAM, the resulting route T , which may result from four different merges, is evaluated for reassignment to all depots (totaling 4 m possible merges for each pair of routes). The resulting saving σ can be calculated as follows: (12) σ = F + z r i + z j r + z s k + z l s − z j k − z t i − z l t + f r θ r + f s θ s − f t ( 1 − y t ) . θ r ( θ s ) is binary and equal to 1 if depot r ( s ), the depot of route R ( S ), supplies no more routes after the merger, and thus can be closed. y t is a binary value (defined earlier for the formulation) equal to 1 if depot t (the depot currently evaluated to be assigned to T ) is already open before the merge, and i , j , k , and l are the vertices of the arcs with demand which are connected to the depots in each route. This saving can be seen depicted in Fig. 2. The EAM ends when there is no more feasible merge with a positive saving. The time complexity of this method is O ( m | E | 2 log | E | ) due to the merge phase, where the list of ( m | E | 2 ) possible merges can be sorted using heapsort (the actual merging of the two routes can be done in linear time). 3.2 Extended merge In the augment-merge algorithm the use of the augment phase has been contested [18]. If all the arcs in A are required to be serviced, it performs well, as the arcs absorbed by the higher cost routes are often contiguous. However, if this is not the case, the deadheading distance created between absorbed arcs cannot be recovered in the merge phase, leading to degraded results. Belenguer et al. [18] further support this statement by presenting overall better results for the algorithm without the augment phase. As the EAM derives from the augment-merge, a similar situation may occur. This suggests the development of an extended merge (EM) algorithm for the LARP, similar to the EAM, but without performing the augment phase. The merge phase is used in both the EAM and the EM algorithms. The extension of the merge phase (in both algorithms) uses some concepts from the extension to the savings algorithm proposed by Prins et al. [19] for the capacitated LRP. Similarly to the EAM method, EM can be executed in O ( m | E | 2 log | E | ) time. 3.3 Reverse The reverse improvement heuristic [20] performs inside the routes and the corresponding move can be seen as the arc routing equivalent of the 2-opt move [21]. The reverse move is identical to the one used in the CARP ( Fig. 3). The algorithm replaces a subsequence of arcs by the reverse, always ensuring the required arcs are serviced. This may lead to other shortest paths (in the deadheading distance) linking the required arcs. The algorithm implements the first found feasible move that improves the solution. This is done sequentially until no more feasible moves can be found. 3.4 Relocate In the relocate improvement heuristic (well-known from the node routing context) two possible variations are considered: relocate inside the routes and relocate between two routes. In both variations the concept is to relocate a given arc (or subsequence of arcs) that requires service to another position in the route, or in another route. The relocate algorithm inside the routes is based on the CVRP algorithm by Savelsbergh [22], while the inter-route algorithm is based on the work by Beullens et al. [20] for the CARP. In the latter, not only vehicle capacity, but also depot capacity constraints have to be taken into account. In both algorithms the (subsequence of) required arc(s) or its reversal is reinserted, depending on which of the two provides the largest improvement. 4 Metaheuristics Metaheuristics are general combinatorial optimization techniques most useful and efficient in solving hard problems [23,24]. While in theory they can handle any combinatorial optimization problem, it is often the case that an important effort must be put on finding the right way to adapt the general parameters of these methods to a particular problem [16]. This is the case in this section, where three general metaheuristics (tabu search combined with a variable neighborhood search – TS–VNS; greedy randomized adaptive search procedure – GRASP; and tabu search combined with a greedy randomized adaptive search procedure – TS–GRASP) are adapted and parameters tuned in order to tackle the LARP. The metaheuristics use the previously developed constructive and improvement methods in their specific framework. 4.1 Tabu search-variable neighborhood search This approach (TS–VNS) is an iterative framework composed of a tabu search (TS) and a variable neighborhood search (VNS), respectively, for the location and (arc) routing phases. These two algorithms are performed iteratively until a stopping criterion is met, namely, a number m a x t s v n s of iterations without improvements to the solution (empirically found to be equal to 10). The TS–VNS approach starts by obtaining a solution using the VNS without constraints on the subset of depots S D to use ( S D ⊆ J ). 4.1.1 Variable neighborhood search VNS is a metaheuristic proposed by Mladenović and Hansen [25] in which the main concept is to perform a systematic change of neighborhood within the local search, exploring increasing neighborhoods of the current solution. If an improvement is found, the search proceeds to the new solution and restarts. The steps of the basic VNS can be seen in Fig. 4. In the adopted VNS: N l is a finite set of pre-selected neighborhood structures inspired in the work by Polacek et al. [27] for the CARP; the initial solution is obtained by performing the EAM (preferred to EM since it is faster to obtain solutions, as the following steps behave similarly in both methods); and the stopping condition is a given number m a x v n s of iterations reached (equal to the number of arcs in the problem, multiplied by ten: m a x v n s = 10 | A | ). The shaking step uses the CROSS-exchange operator proposed by Taillard et al. [28] for the VRP with time windows. It starts by randomly selecting two different routes, to which the CROSS-exchange operator is applied, swapping the sequence of consecutive required arcs. The number of required arcs which get swapped on each route is randomly obtained from an uniform distribution in the range [1, min( l , R T )], R T being the total number of required arcs for route T . When l = l m a x ( l m a x = 6, found empirically) the upper bound on the range is substituted by R T . The described shaking step is biased to exchange smaller sequences of required arcs, while still allowing to perform large swaps. The local search is applied on the two changed routes only, and is composed of the formerly proposed reverse and relocate improvement heuristics, performing intra-route (reverse and relocate) and inter-route (relocate) improvements, sequentially, until no additional improvement can be found. The newly obtained local optimum is only moved to when a cost reduction is obtained, thus making this a descent first improvement procedure which, according to Hansen and Mladenović [26], can be easily transformed into a descent–ascent method (although experimental trials with this variant proved unfruitful for the present approach). At each iteration i t , after obtaining the best solution S i t from the VNS using the subset of depots S D ( V N S ( S D , S i t ) ), the TS–VNS approach proceeds to the location phase, performed by the TS algorithm. 4.1.2 Tabu search The used TS algorithm is the one presented by Filho and Galvão [29], for the concentrator location problem, which provides near-optimal results in low CPU time, validating its use. TS was proposed by Glover [30] and has become one of the most widespread local search methods for combinatorial optimization. It uses a working memory called tabu list in which some attributes are stored (and forbidden to be used) for a number of moves. In the used TS algorithm some adaptations were made to handle the LARP. Each route in the current best solution is collapsed into a single client (regarding demand), and the distance to the several feasible depots is the smallest insertion cost of the depot in the route, as in Barreto et al. [31]. The problem thus becomes a facility location problem and the algorithm tries to obtain the best depot location for the current routes ( T S ( S i t ) ). Using the best obtained depot configuration (excluding the remaining depots from the problem), the TS–VNS approach proceeds to constructing the routes (routing phase) using the abovementioned VNS algorithm. If no improvement was found in the last (five) iteration(s), the approach provides some diversification by randomly opening one (two) depot(s) and closing another from S D , always observing depot capacity constraints. An overview of the pseudo code of the TS–VNS approach can be seen in Fig. 5. i t t s v n s , B e s t S o l , c o s t ( S i t ) , c o s t ( B e s t S o l ) , d e p o t s ( S i t ) respectively refers to the counter of successive iterations without improvement of the best solution, the best solution found, the cost of the current solution, the cost of the best solution found, and the depots to be opened in the current solution. 4.2 Greedy randomized adaptive search procedure The EAM and EM are randomized to provide the greedy randomized search procedure producing the GRASP–EAM and GRASP–EM. The following features are valid for both variations and apply some concepts of the GRASP developed by Prins et al. [19] for the capacitated LRP. For the randomized version of the constructive algorithms ( R C A ( S D , S i t ) ) a restricted candidate list (RCL) of size φ is created from the savings calculated during the merge evaluations. The RCL contains the pairs of routes providing the best φ savings, from which one is randomly chosen to be performed. Changing the size of the RCL during the GRASP has been shown to often give better results [32]. So, at each merge, φ is randomly selected in [ 1 , φ m a x ] , where φ m a x is the maximum RCL size allowed (found empirically: φ m a x = 7 ). The local search (LS), typically used in GRASP metaheuristics, is based on the reverse and relocate improvement heuristics previously presented, which performs intra-route (reverse and relocate) and inter-route (relocate) improvements, sequentially, until no additional improvement can be found. Moreover, a learning process was included in the GRASP which reduces the computational time and improves the final solution [19]. The constructive algorithms, at each iteration i t , provide a solution S i t which often has many open depots and, although the merges can close some, it may not be enough. In order to obtain better solutions, a subset S D of available depots is chosen to be used in the constructive algorithms ( S D ⊆ J ). In the first iteration of the GRASP all depots are used. Then, at each iteration, one of them is iteratively picked from J and the remaining depots in S D are randomly chosen (as well as their number) – c h o o s e ( S D ) – always ensuring there is enough capacity to serve all clients. This can be seen as a diversification. Adding a memorization during the diversification mode enables the GRASP to learn about the good subset to open, and to possibly find better solutions. An intensification mode using this learning process was implemented, varying the GRASP iterations (using the boolean value d i v m o d e and reaching the maximum of m a x i t = 75 ) between: • Diversification mode – applied for m a x d i v = 8 iterations, in which the solution space is explored by varying the subset of open depots (as explained previously). • Intensification mode – performed for m a x i n t = 7 iterations, where an attempt is made to improve the routing for the selected depots ( S D ), obtained from the currently best found solution. The parameters were set after a preliminary experimenting phase and allow five complete runs of the two modes returning, in the end, the best found solution ( B e s t S o l ). Fig. 6 provides an overview of the GRASP approach, where d i v and i n t are the counters of successive iterations in, respectively, diversification mode and intensification mode. 4.3 Tabu search-greedy randomized adaptive search procedure Attempting to integrate the best features of the previously described TS and GRASP, a TS–GRASP approach was developed. The TS handles the location phase while the GRASP addresses the (arc) routing phase. An iterative framework is used, in which the algorithms for both phases are performed iteratively until a stopping criterion is met (a number m a x t s g r a s p of iterations without improvements to the solution, empirically found: m a x t s g r a s p = 10 ). The required adaptations will be described below. Similarly to the TS–VNS approach, the TS–GRASP starts by obtaining a solution for the routing phase without constraints on the subset of depots S D to use ( S D ⊆ J ), however, instead of the VNS, the following GRASP is used. 4.3.1 Greedy randomized adaptive search procedure This GRASP uses the EM for the randomized constructive algorithm, as it provides bigger diversity and better results, returning a solution S i t at each iteration i t . The randomization performs similarly, using the RCL (with the same value of φ m a x ), as does the LS, with the reverse and relocate improvement heuristics (applied sequentially as long as there is improvement to the solution). Unlike the previous GRASP however, there is no diversification mode ( m a x d i v = 0 ), as the TS handles the location phase. Hence, this GRASP tries to obtain the best results (always in intensification mode) for the fixed depot configuration S D . The m a x i t parameter also differs from the previous approach, varying it at each iteration according to: (13) m a x i t = ⌈ 0.25 i t t s g r a s p ⌉ + 1 where i t t s g r a s p is the number of iterations without improvement. This further intensifies the search when fewer improvements are found (ensuring the GRASP is performed at least once). The best obtained solution ( B e s t S o l ) is used in the location phase (TS algorithm: T S ( B e s t S o l ) ) with which the TS–GRASP approach proceeds once the maximum number of iterations m a x i t is reached. 4.3.2 Tabu search The TS algorithm in this approach works exactly as in the TS–VNS. Routes are collapsed into a single client and distances to depots are updated. Using the algorithm by Filho and Galvão [29], the best subset S D of depots to open is obtained ( d e p o t s ( B e s t S o l ) ), to which a small diversification is then added: observing capacity constraints, if no improvement was found in the last (five) iteration(s), one (two) depot(s) is (are) randomly chosen to be opened and one to be closed. The TS–GRASP approach then continues to the routing phase, restarting the GRASP and using only the new subset of depots S D , as seen in Fig. 7. In the figure, n e w b e s t s o l is a boolean value indicating whether a new best solution was found in the routing phase. 5 Computational results In order to ascertain the performance of the proposed methods and approaches for the LARP, experimental results were obtained. First, we discuss implementation and propose a new set of benchmark instances. This is followed by a comparative analysis on the algorithmic proposals using the newly devised instances. 5.1 Implementation and benchmark instances All the aforementioned approaches were implemented in C♯ and the results obtained using a 3.00GHz Intel Xeon E5450 Quad Core CPU with 8 GB of RAM and Windows XP (without parallel processing). To obtain the deadheading distances ( z i j ), in both the constructive methods and improvement heuristics (and consequently in the metaheuristic approaches), the well-known Dijkstra algorithm [33] was used. A new set of instances is proposed in order to compare results and times (justified by the absence of benchmark instances in the literature). The instances were drawn from the CARP literature (the original sets can be found in and adapted to the LARP. These are the well-known and widely used instances from: Golden et al. [34], named “gdb”; Benavent et al. [35], called “bccm”; and Eglese [36], named “eglese”. The first two sets were generated on a graph, with all edges being required, while the latter originates from a real-world (winter gritting) problem for the road network of Lancashire (UK). In the latter set of instances two graphs were obtained and different instances created by changing the set of required edges and the capacities of the vehicles. From the previously mentioned CARP instances, some were chosen (promoting diversity) and adapted to support more than one depot and a cost structure in which location costs range from 30% to 70% of the total cost (with deadheading distance used as distribution costs). Table 1 displays basic data about the proposed instances named after the original CARP set followed by the instance number/name in the original set (e.g. eglese-E1-A is the instance “E1-A” from [36]). The first columns of the table display the name of the instance and the number of the vertices ( V ), edges ( E ), and required edges ( R ) (all instances use undirected graphs). Then follows the number of potential depot locations ( m ) and the average depot fixed cost ( f ¯ ). Finally, columns “ Q ” and “ F ” refer, respectively, to the vehicles capacity and fixed cost. The proposed instances are available at 5.2 Comparative analysis Results for the constructive methods (with and without the local search – LS – consisting of the improvement heuristics applied sequentially until no more improvements can be found) and for the metaheuristic approaches were obtained for the newly devised instances. For the constructive methods, a single run for each instance is performed (as there is no randomization), and the corresponding result and computing time found. For the metaheuristic approaches, twenty runs were made for each instance, from which the average and best result (and the time to obtain it) were acquired. Average and median results for the EAM and EM constructive methods (with and without LS) can be seen in Table 2: the first column shows the method name, followed by average and median values of CPU times (in seconds) and gap to lower bounds GapLB (in percentage). The lower bounds were obtained by running the optimization solver CPLEX 12 where constraints (6) (see Section 2) were relaxed, including them iteratively, as they would get violated, until running out of memory (typically running for a few days). Table 3 shows the results regarding the metaheuristic approaches (TS–VNS, GRASP–EAM, GRASP–EM, and TS–GRASP): the first columns display the instances name, followed by the lower bounds (LB). For each algorithm are also shown: the best results – Best; CPU time, in seconds; and the gap (GapLB), in percentage, between the lower bound and the algorithm best result. Average and median values are provided for CPU time and GapLB, as skewed distributions and/or outlying data points were obtained. Looking at the results for the constructive methods, it can be concluded that, overall, EM performs better than EAM. This may lead to conclude that the claim by Belenger et al. [18] for the CARP, suggesting the augment phase generally leads to poorer results, is valid for the LARP. However, as the EAM obtains faster results (and with somewhat similar final results) it may be an interesting constructive method to be used in metaheuristics. Regarding the metaheuristic approaches, computing times are, on average, less than 10s, the GRASP–EAM being the fastest, followed by the TS–GRASP, the TS–VNS, and the GRASP–EM (although the difference among them is negligible as this is a strategic problem). The TS–VNS approach presents the worst results which, looking at the results of the TS–GRASP, suggest that this is due to the VNS (used in the routing phase) not performing as well as the GRASP on the route building/improvement. Comparing the results of the two GRASP (GRASP–EAM and GRASP–EM), GRASP–EM performs better by providing an average as well as a median improvement of around 2.5%. This may be due to the approach allowing a greater diversity in the possible merges by not having an augment phase, thus easing the finding of better solutions. Looking at the TS–GRASP, when compared with the GRASP–EM results, it can be concluded that the TS, used in the location phase, performs better than the diversity mechanism used in the choosing of open depots in the GRASP (as the route building works similarly). Moreover, this difference is further stressed in the last eight instances, which have more possible depot locations. Thus, as the TS–GRASP has a better location algorithm, it does not waste so much time trying to obtain the best depot configuration, resulting in smaller computing times and allowing further intensification of the routing phase (enabling to find better results). A graphical representation of an optimal solution for instance “gdb-1” can be seen in Fig. 8, where routes are displayed using different colors and a single depot is to be installed (in node 12). 6 Summary and conclusions In this paper heuristic approaches to tackle the LARP, as well as some proposals for benchmark instances (and corresponding results) are presented. Regarding the heuristic approaches new constructive and improvement methods are proposed and used within different metaheuristic frameworks (two constructive methods, three heuristic improvements and, using these, four metaheuristic approaches). Due to the lack of benchmark instances in the literature a new set of instances was devised, adapted from the CARP literature. Concerning the proposed heuristics, the TS–GRASP outperformed the other approaches results wise, and was extremely competitive regarding computing times. A limitation of this work is that conclusions on the proposed approaches were based on the results obtained for the newly devised set of benchmark instances which, in some cases, have high gap values; yet, the computational analysis of the results allowed to validate the proposed instances as they appear to be balanced (regarding location and routing costs) and representative of several different cost configurations. Notice that the proposed approaches use the same local search methods (although within different frameworks), and thus the obtained upper bounds may be not entirely independent. On the other hand, the quality assessment of the presented results is based upon the lower bound values obtained using a commercial software package. As future work, the development of new sets of benchmark instances may prove useful, either based on real-world problems, or generated using a structure that allows to draw conclusions on which real-world situations may be tackled with a specific approach. Finally, new methods for obtaining lower and upper bounds to tackle the LARP should be developed as this is a highly applicable problem which requires dedicated methods and algorithms. References [1] G. Nagy S. Salhi Location-routing: issues, models and methods Eur J Oper Res 177 2 2007 649 672 [2] R.B. Lopes C. Ferreira B.S. Santos S. Barreto A taxonomical analysis, current methods and objectives on location-routing problems Intl Trans Oper Res 20 6 2013 795 822 10.1111/itor.12032 [3] G. Ghiani G. Laporte Location-arc routing problems Opsearch 38 2 2001 151 159 [4] B.L. Golden R.T. Wong Capacitated arc routing problems Networks 11 3 1981 305 315 [5] W.L. Pearn A. Assad B.L. Golden Transforming arc routing into node routing problems Comput Oper Res 14 4 1987 285 288 [6] R. Baldacci V. Maniezzo Exact methods based on node-routing formulations for undirected arc-routing problems Networks 47 1 2006 52 60 [7] H. Longo M.Pd. Aragão E. Uchoa Solving capacitated arc routing problems using a transformation to the CVRP Comput Oper Res 33 6 2006 1823 1837 [8] S. Wøhlk A decade of capacitated arc routing B.L. Golden S. Raghavan E. Wasil The vehicle routing problem: latest advances and new challenges 2008 Springer New York 29 48 [9] L. Levy L. Bodin The arc oriented location routing problem INFOR 27 1 1989 74 94 [10] G. Laporte Location-routing problems B.L. Golden A.A. Assad Vehicle routing: methods and studies 1988 North-Holland Amsterdam 163 198 [11] G. Ghiani G. Laporte Eulerian location problems Networks 34 4 1999 291 302 [12] Muyldermans L. Routing, districting and location for arc traversal problems [Ph.D. thesis]. Leuven: Katholieke Universiteit Leuven; 2003. [13] A.D. Pia C. Filippi A variable neighborhood descent algorithm for a real waste collection problem with mobile depots Intl Trans Oper Res 13 2 2006 125 141 [14] A. Amaya A. Langevin M. Trépanier The capacitated arc routing problem with refill points Oper Res Lett 35 1 2007 45 53 [15] P.B. Mirchandani R.L. Francis Discrete location theory 1990 Wiley New York [16] A. Hertz M. Widmer Guidelines for the use of meta-heuristics in combinatorial optimization Eur J Oper Res 151 2 2003 247 252 [17] G. Clarke J. Wright Scheduling of vehicles from a central depot to a number of delivery points Oper Res 12 4 1964 568 581 [18] J.M. Belenguer E. Benavent P. Lacomme C. Prins Lower and upper bounds for the mixed capacitated arc routing problem Comput Oper Res 33 12 2006 3363 3383 [19] C. Prins C. Prodhon R. Wolfler Calvo Solving the capacitated location-routing problem by a GRASP complemented by a learning process and a path relinking 4OR 4 3 2006 221 238 [20] P. Beullens L. Muyldermans D. Cattrysse D. Van Oudheusden A guided local search heuristic for the capacitated arc routing problem Eur J Oper Res 147 3 2003 629 643 [21] S. Lin B.W. Kernighan An effective heuristic algorithm for the traveling salesman problem Oper Res 21 2 1973 498 516 [22] M. Savelsbergh The vehicle routing problem with time windows: minimizing route duration INFORMS J Comput 4 2 1992 146 154 [23] F.W. Glover G.A. Kochenberger Handbook of metaheuristics 2003 Kluwer Academic Publishers Norwell [24] E.G. Talbi Metaheuristics: from design to implementation 2009 Wiley Hoboken [25] N. Mladenović P. Hansen Variable neighborhood search Comput Oper Res 24 11 1997 1097 1100 [26] P. Hansen N. Mladenović Variable neighborhood search: principles and applications Eur J Oper Res 130 3 2001 449 467 [27] M. Polacek K.F. Doerner R.F. Hartl V. Maniezzo A variable neighborhood search for the capacitated arc routing problem with intermediate facilities J Heuristics 14 5 2008 405 423 [28] É. Taillard P. Badeau M. Gendreau F. Guertin J.Y. Potvin A tabu search heuristic for the vehicle routing problem with soft time windows Transp Sci 31 2 1997 170 186 [29] V.J.M.F. Filho R.D. Galvão A tabu search heuristic for the concentrator location problem Locat Sci 6 1 1998 189 209 [30] F. Glover Future paths for integer programming and links to artificial intelligence Comput Oper Res 13 5 1986 533 549 [31] S. Barreto C. Ferreira J. Paixão B.S. Santos Using clustering analysis in a capacitated location-routing problem Eur J Oper Res 179 3 2007 968 977 [32] M.G.C. Resende C.C. Ribeiro Greedy randomized adaptive search procedures F.W. Glover G.A. Kochenberger Handbook of metaheuristics 2003 Kluwer Academic Publishers Norwell 219 249 [33] E.W. Dijkstra A note on two problems in connexion with graphs Numerische Math 1 1 1959 269 271 [34] B.L. Golden J.S. DeArmon E.K. Baker Computational experiments with algorithms for a class of routing problems Comput Oper Res 10 1 1983 47 59 [35] E. Benavent V. Campos Á. Corberán E. Mota The capacitated chinese postman problem: lower bounds Networks 22 7 1992 669 690 [36] R.W. Eglese Routeing winter gritting vehicles Discrete Appl Math 48 3 1994 231 244 "
    },
    {
        "doc_title": "Extending the H-tree layout pedigree: An evaluation",
        "doc_scopus_id": "84893273871",
        "doc_doi": "10.1109/IV.2013.56",
        "doc_eid": "2-s2.0-84893273871",
        "doc_date": "2013-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Evaluation",
            "Family structure",
            "Genealogy",
            "H-tree layout",
            "Information visualization",
            "Space-filling",
            "User experience",
            "User performance"
        ],
        "doc_abstract": "Visualizing large family structures is becoming increasingly important, as more genealogical data becomes available. A space-filling h-tree layout pedigree has been recently proposed to make better use of the available space than traditional representations. In a previous paper we applauded the technique's usage of available space but remarked that it makes generation identification difficult and does not allow navigating to descendants of represented individuals. A set of extensions was proposed to help overcome these limitations and a preliminary evaluation suggested that those extensions enhance the original technique. This paper presents a more thorough evaluation carried out to assess if and how the proposed extensions improve the original h-tree layout pedigree technique. Results suggest that these extensions improve user performance on some tasks, effectively provide new functionality, and generally enhance user experience. © 2013 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A taxonomical analysis, current methods and objectives on location-routing problems",
        "doc_scopus_id": "84885471121",
        "doc_doi": "10.1111/itor.12032",
        "doc_eid": "2-s2.0-84885471121",
        "doc_date": "2013-11-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Business and International Management",
                "area_abbreviation": "BUSI",
                "area_code": "1403"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Strategy and Management",
                "area_abbreviation": "BUSI",
                "area_code": "1408"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            },
            {
                "area_name": "Management of Technology and Innovation",
                "area_abbreviation": "BUSI",
                "area_code": "1405"
            }
        ],
        "doc_keywords": [
            "Algorithmic approach",
            "Location routing problem",
            "Location-routing",
            "Multi objective",
            "Second level",
            "Solution methods",
            "Structural characteristics"
        ],
        "doc_abstract": "Location-routing is a branch of locational analysis that takes into account distribution aspects. This paper proposes a taxonomy, with two levels, for location-routing problems. The first level focuses on the structural characteristics of the problems. The second level branches into the different algorithmic approaches and objective perspectives. An introduction to the previously defined problems is presented, categorising the papers in the literature (a total of 149 references) according to the proposed classification. Moreover, an overview of the most significant aspects of the different solution methods and main objectives, with special emphasis on multi-objective approaches, is provided. Some data providing a better insight into the publication progress are also included. Finally, several promising research directions are identified. © 2013 International Federation of Operational Research Societies Published by John Wiley & Sons Ltd, 9600 Garsington Road, Oxford, OX4 2DQ, UK and 350 Main St, Malden, MA02148, USA.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The role of ICTs in the management of rare chronic diseases: The case of hemophilia",
        "doc_scopus_id": "84898365353",
        "doc_doi": "10.4018/978-1-4666-3990-4.ch033",
        "doc_eid": "2-s2.0-84898365353",
        "doc_date": "2013-04-30",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Medicine (all)",
                "area_abbreviation": "MEDI",
                "area_code": "2700"
            },
            {
                "area_name": "Health Professions (all)",
                "area_abbreviation": "HEAL",
                "area_code": "3600"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder",
            "Chronic conditions",
            "Chronic disease",
            "Clinical practices",
            "Health care professionals",
            "Information and Communication Technologies",
            "Technological solution",
            "Web-based applications"
        ],
        "doc_abstract": "© 2013 by IGI Global. All rights reserved.This chapter outlines a study that examines the role of Information and Communication Technologies (ICTs) in management of a rare and chronic disease, hemophilia. Evidence in literature shows how the adoption of ICTs can improve the management of chronic conditions. Furthermore, these tools may also give response to rare diseases' needs, while greatly improving the quality of life of those patients. A Web-based application that was developed to facilitate communication between Healthcare Professionals (HCPs) and patients in a specific Hemophilia Treatment Center (HTC), to improve the utility and quality of clinical data and treatment information, as well as to help the management of resources involved in a specific rare chronic disease, represents a practical case presented in this chapter. This technological solution allows the management of inherited bleeding disorders, integrating, diffusing, and archiving large sets of data relating to the clinical practice of hemophilia care, more specifically the clinical practice at the Hematology Service of Coimbra Hospital Center.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The recognition of the intangible assets in business combinations: The Portuguese case",
        "doc_scopus_id": "84871115457",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84871115457",
        "doc_date": "2012-12-20",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            }
        ],
        "doc_keywords": [
            "Accounting standards",
            "Business combination",
            "Goodwill",
            "Intangible assets",
            "Intangibles",
            "Portuguese Groups",
            "Recognition"
        ],
        "doc_abstract": "This study assesses to analyze the business combinations during the years 2005 to 2009 by the Portuguese groups with securities traded in Euronext Lisbon, and the relevance of the respective recognized goodwill value, as to characterize and identify the amount of the other type of intangible assets accounted separately from the goodwill in those groups, which since 2005 applies IFRS 3 in the accounting of its business combinations. The current accounting standards governing the treatment of intangible assets are unanimous on the need for recognition of identifiable intangibles acquired in connection with a business combination. This is not, however, the trend in the most recent studies on this issue, where it is concluded that companies do not undertake sufficient efforts to measure autonomously its intangible assets acquired in a business combination. Our results confirm those obtained in previous studies. The goodwill continues to be recognized by high magnitudes and, conversely, the value of identifiable intangible assets in the acquisition is very low. Theresults of the study conclude that there is not an effort of separate identification and disclosure of intangibles acquired in the business combinations. This fact is evidenced by the reduced number of operations in which were released factors that contributed to the recognition of the goodwill and the identification of intangibles which are part of it.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improvement of surveillance of hemophilia treatment through ICTs",
        "doc_scopus_id": "84880819568",
        "doc_doi": "10.1109/EMBC.2012.6347332",
        "doc_eid": "2-s2.0-84880819568",
        "doc_date": "2012-12-14",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Chronic disease",
            "Coordination and management",
            "Effective management",
            "National health services",
            "Rare disease",
            "Specific component",
            "Technological solution"
        ],
        "doc_abstract": "Hemophilia, in addition of being a chronic disease, is also a rare disease, and as such, quite expensive for the National Health Service (NHS) due to the cost associated with the drugs used in treatments (Clotting Factor Concentrate - CFC). On the other hand, due to the specific characteristics of this type of disorder, it is necessary to ensure that data generated during the treatments are quickly communicated to the clinicians responsible for monitoring those patients. As such, an effective management of this disease, with maximum safety for patients, involves not only an efficient information management process, but also the coordination and management of all the associated resources. This article aims to present one specific component of a technological solution that can help in coordinate actions of patients, physicians and nurses, as well as improve the surveillance of hemophilia treatment, within a specific Comprehensive Hemophilia Diagnostic and Treatment Center (HTC). © 2012 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "End-user attitude in ERP post-implementation: A study in a multinational enterprise",
        "doc_scopus_id": "84898372428",
        "doc_doi": "10.4018/978-1-4666-1764-3.ch013",
        "doc_eid": "2-s2.0-84898372428",
        "doc_date": "2012-12-01",
        "doc_type": "Book Chapter",
        "doc_areas": [
            {
                "area_name": "Chemical Engineering (all)",
                "area_abbreviation": "CENG",
                "area_code": "1500"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This chapter analyzes the impact of the implementation of SAP R/3 in a Multinational Portuguese Organization (MPO), defining some Critical Success Factors (CSFs). In order to understand the motivations of end-users prior to implementation and to analyze the behavior after a change (post-implementation), a study based on a questionnaire was carried out. The sample included 67 users of SAP R/3 that were present throughout the process. Considering the results, the authors conclude that the implementation of SAP R/3 in MPO was successful, and the respondents consider their work more productive and achieve easier access to information. The existence of a solid team to support the project was established as a major facilitator in the whole process, as opposed to the limited time and lack of training that emerged as barriers to the implementation. It was also found that the learning period assumes a high importance in the success of the implementation, since increasing the training time reduces the need for support to the end-users. © 2012, IGI Global.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "User-centered requirements engineering in health information systems: A study in the hemophilia field",
        "doc_scopus_id": "84860229517",
        "doc_doi": "10.1016/j.cmpb.2010.10.007",
        "doc_eid": "2-s2.0-84860229517",
        "doc_date": "2012-06-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Data collection",
            "Design process",
            "Domain model",
            "Engineering systems",
            "Evolutionary design",
            "Grounded theory",
            "Health information systems",
            "Hemophilia",
            "Implementation process",
            "Information and Communication Technologies",
            "Methodological approach",
            "Object-oriented system",
            "Requirements engineering process",
            "Task analysis",
            "User centered designs",
            "User-centered",
            "Web based information systems"
        ],
        "doc_abstract": "The use of sophisticated information and communication technologies (ICTs) in the health care domain is a way to improve the quality of services. However, there are also hazards associated with the introduction of ICTs in this domain and a great number of projects have failed due to the lack of systematic consideration of human and other non-technology issues throughout the design or implementation process, particularly in the requirements engineering process. This paper presents the methodological approach followed in the design process of a web-based information system (WbIS) for managing the clinical information in hemophilia care, which integrates the values and practices of user-centered design (UCD) activities into the principles of software engineering, particularly in the phase of requirements engineering (RE). This process followed a paradigm that combines a grounded theory for data collection with an evolutionary design based on constant development and refinement of the generic domain model using three well-known methodological approaches: (a) object-oriented system analysis; (b) task analysis; and, (c) prototyping, in a triangulation work. This approach seems to be a good solution for the requirements engineering process in this particular case of the health care domain, since the inherent weaknesses of individual methods are reduced, and emergent requirements are easier to elicit. Moreover, the requirements triangulation matrix gives the opportunity to look across the results of all used methods and decide what requirements are critical for the system success. © 2010 Elsevier Ireland Ltd.",
        "available": true,
        "clean_text": "serial JL 271322 291210 291791 291871 291901 31 Computer Methods and Programs in Biomedicine COMPUTERMETHODSPROGRAMSINBIOMEDICINE 2010-11-13 2010-11-13 2014-09-30T23:42:41 S0169-2607(10)00269-5 S0169260710002695 10.1016/j.cmpb.2010.10.007 S300 S300.2 FULL-TEXT 2015-05-14T05:25:39.534626-04:00 0 0 20120601 20120630 2012 2010-11-13T00:00:00Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issfirst issn issnnorm issuelist itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb volfirst volissue volumelist yearnav figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref 0169-2607 01692607 false 106 106 3 3 Volume 106, Issue 3 5 160 174 160 174 201206 June 2012 2012-06-01 2012-06-30 2012 Section I: Methodology article fla Copyright © 2010 Elsevier Ireland Ltd. All rights reserved. USERCENTEREDREQUIREMENTSENGINEERINGINHEALTHINFORMATIONSYSTEMSASTUDYINHEMOPHILIAFIELD TEIXEIRA L 1 Introduction 2 Background 2.1 Requirements engineering process 2.2 User-centered design approach 3 A web-based information system for managing clinical information in hemophilia care 3.1 Study contextualization 3.2 Methods and study design 3.2.1 Grounded theory: the research methods for data collection 3.2.2 Evolutionary design 3.2.2.1 First method: object-oriented system analysis 3.2.2.2 Second method: task analysis 3.2.2.3 Third method: prototyping 4 Results 4.1.1 High-level requirements identified 4.1.2 The general domain model 5 Discussion 6 Conclusion Conflict of interest Acknowledgements References AMMENWERTH 2005 1 3 E HORSKY 2005 264 266 J GARDE 2006 265 278 S MAGUIRE 2001 587 634 M HAN 2005 1506 1512 Y KOPPEL 2005 1197 1203 R AMMENWERTH 2003 237 248 E AARTS 2007 S1 S3 J RINKUS 2005 4 17 S ZHANG 2005 1 3 J SAMARAS 2005 61 74 G BOEHM 1991 32 41 B SEFFAH 2005 3 14 A HUMANCENTEREDSOFTWAREENGINEERINGINTEGRATINGUSABILITYINSOFTWAREDEVELOPMENTLIFECYCLE INTRODUCTIONHUMANCENTEREDSOFTWAREENGINEERINGINTEGRATINGUSABILITYINDEVELOPMENTPROCESS SOUTHON 1999 33 46 G STUMPF 2002 45 48 S HEEKS 2006 125 137 R JOHNSON 2000 394 398 C PROCEEDINGSAMIASYMPOSIUM INCREASINGPRODUCTIVITYREDUCINGERRORSTHROUGHUSABILITYANALYSISACASESTUDYRECOMMENDATIONS VIMARLUND 2002 76 80 V BEUSCARTZEPHIR 1997 19 28 M CARROLL 2002 123 135 C IRESTIG 2008 82 94 M JIYE 2005 105 109 M NUSEIBEH 2000 35 46 B PROCEEDINGSIEEEINTERNATIONALCONFERENCESOFTWAREENGINEERING REQUIREMENTSENGINEERINGAROADMAP CHENG 2007 285 303 B PROCEEDINGSIEEEINTERNATIONALCONFERENCESOFTWAREENGINEERINGFUTURESOFTWAREENGINEERING RESEARCHDIRECTIONSINREQUIREMENTSENGINEERING SAWYER 2001 30 55 P SWEBOKGUIDESOFTWAREENGINEERINGBODYKNOWLEDGE SOFTWAREREQUIREMENTS ABRAN 2004 A SWEBOKGUIDESOFTWAREENGINEERINGBODYKNOWLEDGE SOMMERVILLE 2007 I SOFTWAREENGINEERING CARROLL 1991 J TASKARTIFACTCYCLEDESIGNINGINTERACTIONPSYCHOLOGYHUMANCOMPUTERINTERFACE FAIRLEY 1997 417 432 R JOHNSON 2005 75 87 C ZHANG 2002 42 47 J BEUSCARTZEPHIR 2005 179 189 M KUSHNIRUK 2002 141 149 A ZHANG 2005 173 175 J DEROUCK 2008 589 601 S BERG 2003 297 301 M GENNARI 2005 797 807 J CHAUNCEY 2006 E KAULIO 1998 103 112 M GREENE 1985 523 545 J GLASER 1967 B DISCOVERYGROUNDEDTHEORYSTRATEGIESFORQUALITATIVERESEARCH DENZIN 2005 N SAGEHANDBOOKQUALITATIVERESEARCH STRAUSS 1998 A BASICSQUALITATIVERESEARCHTECHNIQUESPROCEDURESFORDEVELOPINGGROUNDEDTHEORY GALAL 1999 92 102 G KUZIEMSKY 2007 S141 S148 C AGGARWAL 2002 383 397 V BANHART 2000 622 626 F EGYHAZY 1998 48 65 C KUIKKA 1999 838 841 E KROL 1999 145 158 M HAKMAN 1999 1 17 M ZHU 2006 258 267 Y PARK 2007 735 746 H BOOCH 1999 G UNIFIEDMODELINGLANGUAGEUSERGUIDE WHITTEN 2004 J SYSTEMSANALYSISDESIGNMETHODS TEIXEIRA 2006 2610 2613 L PROCEEDINGS28THANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBS MODELINGAWEBBASEDINFORMATIONSYSTEMFORMANAGINGCLINICALINFORMATIONINHEMOPHILIACARE PATRICIA 1991 106 112 S PROCEEDINGS9THANNUALINTERNATIONALCONFERENCESYSTEMSDOCUMENTATION MULTIPLEMETHODSUSABILITYINTERFACEPROTOTYPESCOMPLEMENTARITYLABORATORYOBSERVATIONFOCUSGROUPS DIAPER 2004 D HANDBOOKTASKANALYSISFORHUMANCOMPUTERINTERACTION ANNETT 1971 J TASKANALYSIS STUART 2004 145 154 J PROCEEDINGS3RDANNUALCONFERENCETASKMODELSDIAGRAMS TASKARCHITECTTAKINGWORKOUTTASKANALYSIS NIELSEN 2000 J DESIGNINGWEBUSABILITYPRACTICESIMPLICITY DIX 1998 A HUMANCOMPUTERINTERACTION SHNEIDERMAN 2005 B DESIGNINGUSERINTERFACESTRATEGIESFOREFFECTIVEHUMANCOMPUTERINTERACTION TEIXEIRA 2007 3669 3672 L PROCEEDINGS29THANNUALINTERNATIONALCONFERENCEIEEEENGINEERINGINMEDICINEBIOLOGYSOCIETYEMBS USINGTASKANALYSISIMPROVEREQUIREMENTSELICITATIONINHEALTHINFORMATIONSYSTEM CORTES 2004 207 212 T TEIXEIRAX2012X160 TEIXEIRAX2012X160X174 TEIXEIRAX2012X160XL TEIXEIRAX2012X160X174XL item S0169-2607(10)00269-5 S0169260710002695 10.1016/j.cmpb.2010.10.007 271322 2014-10-02T02:09:58.634896-04:00 2012-06-01 2012-06-30 true 2076110 MAIN 15 48739 849 656 IMAGE-WEB-PDF 1 gr7 425114 1689 2665 gr6 1411211 2140 4333 gr5 448527 1508 2667 gr4 466177 2009 2333 gr3 465991 1272 3000 gr2 689754 2317 3167 gr1 243984 1353 1667 gr7 71867 382 602 gr6 169078 484 979 gr5 65219 340 602 gr4 62286 454 527 gr3 76106 287 678 gr2 101300 523 715 gr1 46153 305 376 gr7 7307 139 219 gr6 8969 108 219 gr5 8793 124 219 gr4 11837 164 190 gr3 7872 93 219 gr2 12621 160 219 gr1 12974 164 202 COMM 3137 S0169-2607(10)00269-5 10.1016/j.cmpb.2010.10.007 Elsevier Ireland Ltd Fig. 1 Software, system and user requirements. Fig. 2 Overview of the research approach: evolutionary process of the user-centered requirements engineering (on the left side) compared to the multiple levels of analysis of the HCC (on the right side). Fig. 3 Example of high-level use-case diagram including some insertion tasks: human actors – nurse, patient and physician; non-human actors – ClinidataXXI, IHIS and MIS [60]. Fig. 4 Part of HTA for “Register Treatment” functionality: designed with the TaskArchitect ® tool [64]. Fig. 5 Appearance of prototype interface. Fig. 6 UML class diagram giving general overview of domain model. Fig. 7 Iterative process with methods triangulation in requirements engineering. Table 1 Triangulation matrix based on three different analysis and validation methods with final list of high-level requirements of the WbIS for managing clinical information in hemophilia care. Actors/requirements identified Method OOSA HTA Prototyping Patient P1/N6: Register infusion treatment (home treatment) x + + P2/N7: Register bleeding episode x + + P3/N8: Register product administered x + + P4/D25: View treatment evolution x NA + P5/N9/D2: View treatment protocol x NA + P6: View particular stock of CFC − − x Subtotal 5 0 1 Nurse N1: Register product (CFC: Stock management) x + + N2: Assign product to patient (CFC: Stock management) x + + N3: Change product location (CFC: Stock management) − x + N4: Register safety stock (CFC: Stock management) − x + N5: View stock level (CFC: Stock management) x NA + N6/P1: Register infusion treatment (hospital treatment) x + + N7/P2: Register bleeding episode x + + N8/P3: Register product administered x + + N9/P5/D2: View treatment protocol x NA + Subtotal 7 2 0 Physician/hematologist Doctor D1: Register treatment protocol x + + D2/P5/N9: View treatment protocol x + + D3, D4: Register/view family-tree x/x +/NA +/+ D5, D6: Register/view external hospitalization x/x +/NA +/+ D7, D8: Register/view joint-exam x/x +/NA +/+ D9, D10: Register/view data surgery x/x +/NA +/+ D11, D12: Register/view bleeding diagnosis x/x +/NA +/+ D13, D14: Register/view life quality x/x +/NA +/+ D15, D16: Register/view vaccination plan −/− −/− x/x D17, D18: Register/view pathology record −/− −/− x/x D19, D20: Register/view general exam x/x +/NA +/+ D21: View medical appointment x + + D22: View inhibitor exam − − x D23: View virology exam x NA + D24: View pathological exam x NA + D25/P4: View treatment evolution x NA + D26: View different statistical data (graphs and tables) x NA + Subtotal 21 0 5 MIS M1: Register medical appointment x NA + M2: Register patient medication − NA x Clinidata XXI C1: Register inhibitor exam − NA x C2: Register virology exam x NA + C3: Register pathological exam x NA + IHIS I1: Register patient biographic data x NA + I2: Register patient contact x NA + Subtotal 5 0 2 Total identified functionalities 38 2 8 Total confirmed functionalities − 17 40 x, identify; +, confirm; −, nothing; NA, no-applicable; CFC, coagulation-factor concentrate. User-centered requirements engineering in health information systems: A study in the hemophilia field Leonor Teixeira a b e ⁎ Carlos Ferreira a c Beatriz Sousa Santos d e a Department of Economics, Management and Industrial Engineering, University of Aveiro, Portugal b Governance, Competitiveness and Public Politics (GOVCOPP), University of Aveiro, Portugal c Operational Research Centre (CIO), University of Lisbon, Lisbon, Portugal d Department of Electronics, Telecommunications and Informatics, University of Aveiro, Aveiro, Portugal e Institute of Electronics and Telematics Engineering of Aveiro (IEETA), Aveiro, Portugal ⁎ Corresponding author at: University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal. Tel.: +351 234 370 361; fax: +351 234 370 215. The use of sophisticated information and communication technologies (ICTs) in the health care domain is a way to improve the quality of services. However, there are also hazards associated with the introduction of ICTs in this domain and a great number of projects have failed due to the lack of systematic consideration of human and other non-technology issues throughout the design or implementation process, particularly in the requirements engineering process. This paper presents the methodological approach followed in the design process of a web-based information system (WbIS) for managing the clinical information in hemophilia care, which integrates the values and practices of user-centered design (UCD) activities into the principles of software engineering, particularly in the phase of requirements engineering (RE). This process followed a paradigm that combines a grounded theory for data collection with an evolutionary design based on constant development and refinement of the generic domain model using three well-known methodological approaches: (a) object-oriented system analysis; (b) task analysis; and, (c) prototyping, in a triangulation work. This approach seems to be a good solution for the requirements engineering process in this particular case of the health care domain, since the inherent weaknesses of individual methods are reduced, and emergent requirements are easier to elicit. Moreover, the requirements triangulation matrix gives the opportunity to look across the results of all used methods and decide what requirements are critical for the system success. Keywords Health information systems Requirements engineering User-centered design Human-factor engineering system Triangulation matrix Hemophilia 1 Introduction The use of sophisticated information and communication technologies (ICTs) in the health care domain is a way to improve the quality of services. The literature points to a general consensus that health information systems (HISs) are thought to have the potential to improve patient care. However, there are also hazards associated with the introduction of ICTs in health care, and some works report how difficult it is the successful introduction of ICTs in this domain [1,2]. In fact, health care is a unique and complex domain and HISs have human safety implications and profound effects on individual patient care [3]. Successful development of HISs can increase efficiency and productivity, ease of use and learning, adoption, retention and satisfaction of the users [2], and simultaneously, can help to decrease medical errors as well as to reduce support and training cost [4]. On the other hand, HISs are usually complex systems and their failures may cause negative effects on patients [5–7] and possibly, when insufficiently designed, they may result in spending more time with the computer than with the patient [8]. According to [9] ICTs have been hailed as a solution to reduce errors in health care, but there is also evidence that they can be part of the problem. There are a large number of HISs projects that have failed, and most of these failures are not due to flawed technology, but rather due to the lack of systematic consideration of human and other non-technology issues throughout the design or implementation process [10–12]. Also, several studies have shown that 80% of total maintenance costs with information systems (ISs) are related to users’ problems and not technical bugs, and among them 64% are related with usability problems [13]. A survey of over 8000 projects made by The Standish Group and undertaken by 350 US different companies revealed that one third of the projects were never completed and one half succeeded with partial functionalities. The major source of such failures resided on poor requirements, specifically: the lack of user involvement (13%), incomplete requirements (12%), changing requirements (11%), unrealistic expectations (6%), and unclear objectives (5%) [14]. These problems are mainly due to the fact that, in developing interactive software, most software engineering methodologies do not propose any mechanisms to: (i) explicitly and empirically identify and specify user needs and usability requirements; and (ii) test and validate requirements with end-users before and during the development process [15]. The health care domain has been particularly prone to such problems in recent years, and there are numerous examples of potentially useful systems that have failed or have been abandoned due to unanticipated human or organizational issues [16–18]. Since the design of systems that are used by people is a complex endeavor, the systems that cannot be used intuitively often lead to an increase in error rate and a decrease in user acceptance [19]. Involving end-users in the design process has been suggested to be a key marginal investment for being able to transform the cost related to the implementation of HISs into future benefits [20–22]. Moreover, the participation of end-users and the involvement of relevant stakeholders in early steps of the design process, not only prevents post-implementation problems, but also gives them the chance to address and resolve potential conflicts concerning the future system [23]. However, involving end-users in development of HISs is often complicated, especially when they have limited computer skills or due to the fact that users’ knowledge is tacit and consequently task description is very difficult. Particularly in healthcare, effective research of user requirements is often discussed in terms of which method to use or “which method is better”, since the dominant culture in this industry is still train people to adapt to poorly designed technology, rather than designing technology to fit to people's characteristics [11]. The principles of user-centered design (UCD) [4,24] combined with ethnographic practices [25] can improve synergies among technology, people and work environment (tasks). In order to perform a user needs analysis and to write requirements specification for integrated care in the hemophilia field, we followed a user-centered requirement engineering process involving the end-users through different techniques of requirements elicitation and validation. This work describes the application of UCD principles in the requirements engineering process to generate the requirements document of a web-based information system (WbIS) for managing the clinical information in hemophilia care, as well as some results. The research was based on three well-known methodological approaches: the first one consisted of a classical object-oriented systems analysis (OOSA) method based on Unified Modeling Language (UML) notation; the second consisted in a task analysis (TA) method based on hierarchical task analysis (HTA) notation; and the third consisted in prototyping based on the construction of an executable system model. These three methods were performed through an iterative development process combined with grounded theory (GT) in a triangulation work. 2 Background 2.1 Requirements engineering process The success of any software system depends on how well it fits the needs of its users and its environment [26]. Software requirements comprise these needs, and requirements engineering (RE) is the process by which the requirements are determined [27]. A requirement is a property that a system must exhibit in order to meet the system's motivation need; and software requirements are a property which must be exhibited by software developed to solve a particular problem within one organizational context. Therefore the software requirements are a complex combination of requirements from different people at different levels of an organization and from the environment in which the system must execute [28]. They express the needs and constraints placed on a software product that contribute to the solution of some real-world problem and normally result of an arrangement between “user requirements” and “system requirements”, Fig. 1 . User requirements denote the requisites of the people who will be the system client or end-users. System requirements add requirements of other stakeholders (such as regulatory authorities) and requirements that do not have an identifiable human source and that normally result from the intersection among technical, cultural and social environments [28,29]. The information system (IS) is never used on its own but always as part of some broader system including hardware, people and, often, organizations [30]. Successful RE involves understanding the needs of users, clients and other stakeholders, as well as understanding the context in which the software will be used [27]. Thus, identifying all the users and other stakeholders who may be impacted by the system is very important and this will help to ensure that the needs of all those involved are taken into account and, if required, the system is tested by them. In the literature, RE is presented as a knowledge area of software engineering related with elicitation, analysis, specification and validation of software requirements [29]. However, and according to [30], this process of RE is difficult for several reasons: (i) stakeholders often do not know what they want from the system, except in general terms; (ii) stakeholders express requirements in their own terms and with implicit knowledge of their work, and requirements engineers, without experience in the user domain, must understand these requirements; (iii) different stakeholders have different requirements, which they may express in different ways; (iv) political factors may influence the system requirements; (v) the business environment in which the analysis takes place is dynamic. The resulting requirements document has to be understood and usable by domain experts and other stakeholders, who may not be knowledgeable about computing [27]. Hence, requirements notation used in models that involve users’ validation must use a balanced language adequate for stakeholders having computing and non-computing backgrounds. On the other hand, it should be noted that the analyses of activities to elicit the user requirements, which normally focus on present activities, can change in time. When the new artifact was implemented, the activity also changed, thus, the analysis of the user needs might be outdated. This ‘vicious circle’ was presented by Carroll et al. [31] as the task–artifact cycle. In other words, the task–artifact cycle is an iterative process of continuous, mutually dependent development between task and artifact, a process that will never reach an optimum state. This cycle suggests a revision of the original task for which the artifact was made, and consequently the redesign of the artifact according to the new identified requirements. To avoid this trap, and to aid the communication among different stakeholders with different background, some authors proposed several techniques and methods. For example, the so-called “Concept of Operations (ConOps)” was proposed by Fairley and Thayer [32], as a way of dealing with that problematic issue. In our case, we used evolutionary design techniques combined with UCD principles to minimize the impact that may arise from the basic assumption of the task–artifact cycle. 2.2 User-centered design approach Within the field of software development, there are several approaches for designing software applications. User-centered design (UCD) is one of them concerned with incorporating the perspective of the users into the software development process in order to achieve a desirable and usable system [10,33,34]. UCD is a complement to software engineering traditional methods rather than a substitute. Maguire [4] and Ji-Ye et al. [24] describe UCD as a multidisciplinary design approach based on the active involvement of users to improve the understanding of user and task requirements, and the iteration of design and evaluation. Much has been written in the research literature about UCD and as further evidence of internationally endorsed best practice, this approach is defined in ISO documents, including ISO 13407 [35]. It states that there are four design activities that need to start at the earliest stages of a project: (i) understand and specify the context of use; (ii) specify the user and organizational requirements; (iii) produce design solutions; and, (iv) evaluate design against requirements. In the healthcare domain the application of the UCD principles is more recent, namely some UCD approaches, such as the human centered distributed information design methodology (HCDID) by Zhang et al. [34] and Rinkus et al. [10] and the user-centered framework proposed by Johnson et al. [33], which are based on four types of analyses: (i) user analysis – identifying the characteristics of existing and potential users; (ii) functional analysis – identifying a system's abstract structures of a given domain model; (iii) representational analysis – identifying an appropriate information display format for a given task performed by a specific type of users; and (iv) task analysis – identifying the procedures and actions to be carried out as well as the information to be processed to achieve task goals by using specific representations. Beuscart-Zéphir et al. [36] present another UCD approach, an user-centered assessment method, that includes three main dimensions of analysis: qualitative management, usability assessment, and performance analysis. Beuscart-Zéphir et al. [21], Carroll et al. [22] and Kushniruk [37] also advocate the importance of user involvement during early phases of the design process in the development of HISs, based on formative evaluation. Health information systems (HISs) are highly complex systems and, we believe that a proper UCD approach has the potential to improve the quality of the final solution. The lack of minimum considerations of design principles centered in human factors in many HISs makes them very difficult to learn and use, and this difficulty leads to strong resistance by users, in some cases it leads to abandoning the HIS altogether or increasing “human error” resulting from an incorrect usage [34]. The paradigm of UCD in health care domain [10–12,33,34,36,38,39] offers a new look at system design including human factors, covering more than usability engineering, and human factors. Basically, the goal of a UCD approach consists in creating systems that are modeled in conformity with the characteristics and tasks of the potential users. UCD has given rise to many forms of design practices in which various characteristics of the context of use are considered. Particularly, in the medical informatics domain, several authors emphasize the need for UCD development and argue for an early user involvement [9,12,40]. This development process paradigm can be improved using ethnographic practices. Myers [25], Sommerville [30] and Gennari et al. [41] present ethnographic method as observational techniques that can be used to understand social and organizational requirements having a strong contribution to a complex set of communication behaviors and needs. In this context, the main value of this technique is that it helps requirements engineers to discover implicit requirements that reflect the actual rather than the formal process in which people are involved. Sometimes users may have difficulties expressing their tasks, therefore we need to apply ethnographic methods to observe and analyze them in order to be able to get some evidence to help deducting the requirements. 3 A web-based information system for managing clinical information in hemophilia care In this section, we will present a study where we applied the user-centered requirements engineering methodology. Firstly, we will describe the domain problem and the critical aspects that justify the application of an UCD approach. Then we will explain the study design, namely the methods and procedures of applying the principles of UCD in RE process. After that, we present the results of the methodology process. Finally, we discuss and explain the impact of the method on the requirement engineering process in this particular problem and in health care IS, in general. 3.1 Study contextualization This study was requested by the hematology service of a regional hospital (HS_CHC) in order to evaluate the feasibility and usefulness of a WbIS to assist the hemophilia patient care at this hemophilia treatment center (HTC) located in the center region of Portugal. This HTC provides assistance to about 200 patients (≈30% of the registered patients in Portugal) in three integrated hospitals and provides clinical and laboratorial support to other hospitals all over the country. The main goal of this application is to support the management of inherited bleeding disorders, which involves the integration, distribution and archive of large sets of information coming from heterogeneous sources in the scope of hemophilia care, as well as to facilitate communication between health care professionals (HCPs) and patients at this HTC. Currently, in order to support the information flow and communication between different HCPs, this HTC uses several different computer-based and paper-based ISs. The computer-based ISs are basically: (i) IHIS (Integrated Hospital Information System) – which allows to visualize, manage and archive the administrative information while opening a clinical process; (ii) MIS (Medical Support Information System) – which allows to visualize, manage and archive clinical information during the medical appointments; (iii) NIS (Nursing Support Information System) – which allows to visualize, manage and archive nursing information; (iv) and, ClinidataXXI – to archive clinical analysis results and laboratorial information providing on-line service at the three afore mentioned hospitals. These computer-based ISs represent generic solutions, since they were developed to support generic requirements of Hospitals and Healthcare Centers, and consequently cannot respond to the specific needs of hemophilia care. In fact, HCPs of hemophilia care generate a lot of information when they see their patients; part of this information is in electronic format and is stored in computer-based ISs; but another part (i.e. home treatment records, joint exam records, life quality records) is on paper format and is stored in paper files. At present, patients record the result of home infusion treatment in paper diaries and send them to the HTC by post, fax, or bring them when they attend routine review appointments. This system has a number of weaknesses as often paper records are incomplete or not returned. Moreover, the period between individual infusions of treatments and receipt data may be long, which is undesirable, since that data is very important for clinical decision about treatments. In this pathology, the information is more than a resource, as further clinical decisions about patients’ treatment depend on it. The fact that the information concerning hemophilia patients is spread throughout different ISs is an obstacle for quality of data, fast retrieval of information and, consequently, healthcare quality. The need of an integrated IS for this specific pathology is obvious, and new ICTs have an important role in this field. Portugal, in spite of having about 700 patients with hemophilia, does not have a hemophilia national patient registry and most HTCs do not have a specific system to store and manage information concerning this pathology. This problem emerges in the scope of a highly complex environment, characterized by missing information, shifting goals and a great deal of uncertainty [37]. Healthcare decisions are complex processes strongly based on tacit knowledge that contributes for a difficult process of requirement elicitation using traditional methodologies. Therefore we believe that the combination of a traditional system development methodology with UCD actively involving users in the design process can improve the elicitation requirements process in this domain, taking into account the natural impact that may arise from the basic assumption of the task–artifact cycle, proposed by Carroll et al. [31]. 3.2 Methods and study design To conduct this study along the requirement engineering process, we followed an approach that combines a grounded theory (GT) with an evolutionary design based on constant development and refinement of the generic domain model. All users, i.e., physicians, nurses, laboratorial staff, administrative staff and patients were, directly or indirectly, involved in all steps of this study. To avoid a technical focus, which too often has been the case of similar projects, different analysis and validation methods using the UCD principles were chosen to accomplish the RE process: (i) the first one consisted of a classical OOSA method represented by UML notation; (ii) the second consisted in a model-based TA represented by HTA notation; (iii) the third consisted in the prototyping technique based on developing an executable system model. The research based on three well-known methodological approaches was developed through an iterative process under two conditions: (i) the users were active and showed about the same level of participation in all phases of the design; and, (ii) the context of use was taken into account in all phases of the design, Fig. 2 (on the left side). According to the representation in Fig. 2, the different methods of our methodology (on the left side of the Fig. 2) have a match with the levels of analysis of the human-centered approach (HCC) proposed by Zhang et al. [11,34] and Rinkus et al. [10] and used by Johnson et al. in their user-centered framework [33] (on the right side of the Fig. 2). Basically with the first method (OOSA) we are trying to capture the same information as the user and functional analysis level; our second method (TA) has the same principle of the task analysis level; and, the prototyping methods have a correspondence with the representation analysis level in the HCC methodology. Nevertheless, our methodology differs from the HCC methodology as we applied the different methods using an iterative design approach based on triangulation work, in order to validate and improve the domain model. The main objective of our methodology is to support the initial part of the process (i.e. the elicitation and transformation of users’ needs into detailed requirements specifications) in the development of a new IS. Triangulation is an approach to data collection and analysis that uses multiple methods, measures, or approaches to look for convergence on product requirements or other problem areas [42,43]. In general it refers to the application of different sources of data, methods and theories to investigate the same phenomenon [8,44]. This approach has two main objectives: (i) validation of results in order to confirm results with data from other sources or methods; and (ii) completeness of results in order to complement the data with new results, thus to find new information to get additional pieces to the overall system [8]. Therefore, these objectives were the two main reasons by which we chose and applied a triangulation approach in the present study. According to the types of triangulation mentioned by [8,44], we used (i) ‘data triangulation’ during data collection activities, where various data sources were used; and, (ii) ‘methods triangulation’ during the evolutionary design activities, where several methods for analysis and validation were applied; according to [43], a ‘between-methods triangulation’ in a ‘sequential combination of methods’. Basically, the ‘between-methods triangulation’ refers to the process of combining different methods in order to test external validity and ‘sequential combination of methods’ refers to the sequential application of the methods [43]. 3.2.1 Grounded theory: the research methods for data collection The study outlined in this article was developed along two years. During the first nine months, an exploratory study was made based on a grounded theory (GT) approach. The GT approach was published in 1967 by Glaser and Strauss as a qualitative method in social research [45] and it supports the inductive development of theories which are grounded in data to explain a phenomenon [46,47]. The theory emerges of the process from systematically recording and analyzing data related to the phenomenon, normally called ‘constant comparison’. In the present context, we used a GT approach to explain the clinical practice in the HS_CHC and consequently to support the process of RE in a complex environment on the basis of a comprehensive domain model. The use of GT to aid the requirements engineering activities is not very common, however is not novel too. For instance, the works of Galal and Paul [48], Garde and Knaup [3] and Kuziemsky et al. [49] describe the application of this qualitative approach in the RE process, and the last two were applied in the health care domain. In the present study and in order to stimulate basic research as well as to understand the information flow and the management process of clinical data, we started to analyze the clinical practice of the HS_CHC using qualitative methods. Direct observation (ethnography) of the current work practices, analysis of documentation and unstructured interviews based on open-ended questions with users were used. A preliminary specification of the new solution was drafted using information obtained from direct observation and documentation analysis. These techniques were used during the first three months, and the data were collected by the researcher using a specific report. After this phase, qualitative interviews and several focus groups with HCPs and hemophiliac patients, to explore their perceptions about the new system and to capture their needs, were conducted. Three nurses, two physicians and three patients were involved in six focus groups carried out during the following six months. The data were used to complement the preliminary specification and was converted in a preliminary domain model by the researcher. Specific unstructured interviews with patients were also conducted in an informal setting, while they waited for their appointments, as well as with the administrative staff, in their work place. Ethnographic methods were used in all exploratory study phases, since they produce results with greater accuracy when combined with other methods [46]. Basically we used a different source of data (‘data triangulation’) in order to ensure richness in the working material, reducing at the same time the inherent weaknesses of individual methods. After this exploratory study analysis we concluded that although departmental IS have sometimes been interfaced in order to transfer some data among a patient registration system, a laboratory system and a medical system, this was usually based on ad-hoc activities without the implementation of a global communication concept. 3.2.2 Evolutionary design Based on the results of the exploratory study supported by the GT approach, we outlined the preliminary requirements document and the domain model. In order to validate and consequently complement the domain model we applied the three different analysis and validation methods (‘methods triangulation’), Fig. 2. The combination of these three methods was a deliberate choice to ensure richness in the working material and a possibility to collect as many aspects as possible. The order by which they were applied was a planned choice too and will be explained in the next subsections. 3.2.2.1 First method: object-oriented system analysis First, we chose an OOSA method as an object-oriented (OO) environment is an appropriate representation method of a real-life entity or abstraction [50] and would allow the incorporation of new components based, for instance, on incremental solution and evolutionary design. Thinking about the environment in terms of objects comes naturally to people and OOSA has been also successfully applied in different HISs projects [50–57]. Among this, the UML was the notation chosen since it is an OO design technique representing a standard language for specifying, visualizing, constructing, and documenting the artifacts of systems [58,59]. One example of this phase results is the high-level use-case diagram (including insertion tasks) that is depicted in Fig. 3 . After drawing these representations, users were invited to evaluate the use-case models. For this evaluation, open-ended focus group interviews [46,61] were conducted with different groups of users (patients, nurses and physicians). The process was open-ended since the interviews were not guided by pre-defined questions; nevertheless they were supported and oriented by models to stimulate discussion. In order to simplify and help the users in this process, during the discussion, some functionalities were converted in small stories (scenarios) using natural language. We are sensible to the fact that use-case diagrams represent abstract models and consequently they could be unintelligible for people without computing science background. In order to workaround this problem, we introduced additional activities to complement a process of RE, based on TA methods. 3.2.2.2 Second method: task analysis Based on results obtained with first method, we applied another to complement the domain model, in order to validate the functionalities and consequently to identify the sequences of sub-functionality within a specific functionality. Leading the users through a small number of scenarios representing sequences of events in the application domain can help to explore the key aspects affecting the requirements. We believe that the task analysis (TA) approach has the power to help in this aspect. There are many types of TA and diverse techniques provide different types of data for analysis [62]. We applied a hierarchical task analysis (HTA), developed by Annett et al. [63], that has the benefits of being readily understandable by a wide people background that normally think about their work as a structured hierarchy of tasks. While the lack of syntax can be said to limit its expressiveness because it does not include the constructs of more formal meddling languages, HTA can also be seen as a more powerful medium for communication in that the analyst is free to express the user's task without syntactical constraints (Ormerod et al., 1998 in [64]). In our particular case, the TA was performed on user data, gathered during the first phase of the requirements engineering process and represented by use-case diagrams, Fig. 2. This analysis began with a statement of the user definition of each task. A complete list of actions needed to perform each task was subsequently produced. Users were then asked to first explain their primary goal when accomplishing a specific task in their work. After that, and based on the data resulting from the system analysis process in the first phase, a TA was performed and represented through HTA notation. We applied this technique (HTA) to tasks performed by our three human actors – nurse, patient and physician – choosing more error prone functionalities, i.e. data insertion tasks. One example of the obtained HTA model about “register treatment” is illustrated in Fig. 4 . The task models resulting from this phase were then reviewed by users: three nurses, two physicians and three patients were directly involved in this review process. They were asked to “think aloud” and verbalize how they would perform each task whilst they look at the models. The resulting information was registered and used to refine the task model, in combination with information obtained from observation and discussion with users. This cycle was repeated until we noticed that the model was not evolving anymore, which happened after three iterations. The use of this type of analysis proved to be a practical way of involving users allowing a better understanding of the human factors involved in our system and fostered an easier incorporation of the notion of usability engineering in the process of requirement engineering. Subsequently, this method assumes particular importance on the discovering process of the users’ tacit knowledge. 3.2.2.3 Third method: prototyping In order to provide a contextual framework that serves to focus the discussion, to help users verify their understanding, as well as to test the overall interaction metaphor, we decided to apply a prototyping technique in the last phase of the RE process. There are different types of prototypes and we applied a horizontal prototype (or user interface prototype) since it represents a model of the outer shell of an entire system, with many features but with very little processing behind them. This prototype, implemented in HTML and JavaScript, showed all items described in the domain model and how these changed as a result of user interaction. Besides, in prototype development, we referred to principles of Human Computer Interaction (HCI) design [65–67] and structured the information on the screen in a logical and consistent manner, corresponding to the order in which this information appear in paper-based files. Moreover, to help the users in building a mental model of the new system, the information is presented in a known and understandable manner in accordance with sequence information captured in the TA phase [68]. The appearance of one prototype interface screen is demonstrated in Fig. 5 . To evaluate the result of this phase, we discussed the prototype in an open-ended focus group [61,69]. First of all, and in order to stimulate discussion, the prototype was presented to users. Based on this, the users suggested and debated changes, and recommended other new functionalities. The resulting information was integrated in the prototype and, after that, a new discussion took place. This cycle was repeated until we noticed that the prototype was not evolving anymore, which happened after two iterations. Modeling the external features of a HIS with a horizontal prototype greatly enhances communication between the requirements engineer and the users. This opportune interaction sometimes results in more concrete outputs, and users normally find it easier to respond to a suggested approach demonstrated by a prototype, than to define their requirements with the system represented by an abstract model. With this experiment, we concluded that the prototype is the easiest tool for HCPs participation in a requirements engineering process of a HIS. 4 Results The results of the evolutionary design process of a WbIS for managing the clinical information in hemophilia care may be divided into two distinct parts: the list of high-level requirements identified and the conceptual model of domain. 4.1.1 High-level requirements identified A list of 48 high-level requirements obtained from multiple methods was presented and grouped by actor: 38 requirements were identified or validated in the first step, 2 were identified in the second step, and 8 were identified in the last step. Table 1 presents a resultant triangulation matrix that lists high-level requirements on one dimension and the three different analysis and validation methods on the other dimension. This (triangulation matrix) indicates which of the methods was responsible for the identification of the requirement (x); and after its identification, if it had confirmation by one of the subsequent methods (+) or if it was not considered (NA). For example, requirements P1, P2 and P3 were identified in the first step and were confirmed in the second and third steps. This might indicate that they are core requirements. Requirements P4 and P5 were identified in the first step; they were not considered in the second step and had confirmation in the third step. Thus, they might also be core requirements. In the second method they were not considered since we applied the TA method only to insertion tasks performed by human actors as they are more error prone functionalities. The requirement P6 was discovered only in the third step, which could indicate that it is a dependent and/or emergent requirement. This type of requirements which emerge with the users’ understanding of the system development during the design process are denominated ‘emergent requirements’ [30]. Different actors were identified: physician, nurse and patient represent the human actors; IHIS, MIS and ClinidataXXI, represent non-human actors, i.e. other existing ISs that will be integrated with WbIS. Patients have an individualized password protected home page, which can only be accessed by themselves or, in case of children, aged people or illiterates, by a caregiver. For example, when a patient receives a treatment with an infusion of coagulation-factor concentrate (CFC), he/she can access his/her personal page and enter details about that treatment (P1), associated bleeding episodes (P2) and the amount and batch number of CFC administered (P3). If the treatment takes place in HTC, these data could be registered by a nurse (N6:N8) who also has an individualized password. The patient's page shows a recommended treatment regimen for prophylaxis or immune-tolerance as well as information on bleeding episodes management (P5). These treatment protocols are entered by a physician and can be up-dated at any time (D1). The physician, normally a hematologist doctor, registers and views a lot of information specific of this pathology and related with the patient (D1:D26). The HIS, MIS and ClinidataXXI represent non-human actors and they register some data (M1, M2; C1:C3; I1, I2). 4.1.2 The general domain model The obtained general domain model was represented through a class diagram of UML notation. This diagram represents the static structure showing object classes, attributes, as well as relationships between those object classes. We identified different kinds of information that were grouped in three big components: patient information, treatment information and CFC-Stock information, Fig. 6 . In the patient information component, besides patient personal data (coming from IHIS), it is also important to collect the data obtained from the doctor appointments and prescribed medication (coming from MIS); as well as all historical information about pathological, virology, and inhibitor exams (coming from Clinidata XXI). These data will be registered in the WbIS automatically by non-human actors. Furthermore, regarding the patient, it is also necessary to store additional data as: bleeding diagnosis, life quality, family-tree, pathologic record, joint condition, and so on. Patients’ data are stored with a unique identification; and each patient can undergo many treatments. The treatments can be on-demand, prophylactic or immune-tolerance. On-demand treatments have associated bleeding episodes (in a joint, muscle or mucosa). On the other hand, treatments can be made at home or at the HTC. The registration of this data was made by the patient or his/her caregiver (in case of home treatment) or by a nurse (in case of hospital treatment). Usually, treatments consist of infusion of blood products, the coagulation-factor concentrates (CFC), which are in stock; thus, functionality concerning the management of blood products inventory should also be integrated. When a blood product arrives at the HTC, a nurse registers its data into the system (name of product, data of reception, batch number, number of units per batch, validity); on the other hand, a blood product can be in different places, as it can be given to patients for home treatment (patient stock) or stayed in the hospital (hospital stock). When a blood product is administered in the sequence of a treatment, the system should register this occurrence and automatically update the stock level. When the stock level reaches the safety stock, the system will send an alert so that a new order is placed. 5 Discussion In this study, we used an evolutionary design process in order to specify the requirements document of a WbIS for managing the clinical information in hemophilia care, and to define a more accurate and realistic domain model in this field. The user-centered requirements engineering methodology based on three different analysis and validation methods was used in an iterative design approach based on the triangulation work. As stated before, the triangulation approach has two main objectives: to confirm results with data from other sources, and to find new data to obtain a more complete solution. Validation of results is achieved when results from one method are confirmed by results from other method. Completeness of results is achieved when one method presents results which have not been found with other method. We involved the same group of users using different analysis and validation methods to confirm the previous results (validation) and consequently to complement the data with new results (completeness). Each method was applied with the same users, in the same environment but in different times. As shown in Fig. 7 , and according to the triangulation matrix presented in Table 1, the results of some parts of the study were validated by the results from other parts, and all parts combined contributed to improve and complete the results about the domain model. With this methodological approach, the requirements triangulation not only contributed for the process of elicitation and validation of the system requirements but also allowed to look across all methods used and decide what requirements are critical for the system success. For example, all requirements identified in the first step and confirmed in the second and third steps should be considered as core requirements. The requirements that were discovered only in the third step probably are dependent and/or emergent requirements. Usually, this type of requirements which emerge with the users’ understanding of the system development during the design process is associated with tacit knowledge and of difficult elicitation. With this study, we also concluded that the HCPs had severe difficulties in predicting their expectations using situations based on abstract models, as UML notation. Indeed, design representation principles emphasize the importance of using representations that are easy to understand for all the stakeholders, and UML notation is obviously not appropriate in this respect. On the other hand, the prototype proved to be the most adequate tool for HCPs participation in a requirements engineering process of a HIS, greatly enhancing communication between the requirements engineer and the users. The exploratory study supported by GT combined with requirements triangulation based on three well-known methods provides a practical framework for understanding and considering user perspectives in the initial part of the development, and transforming these needs into detailed requirements specifications, i.e., the conceptual domain model. However, this approach is time-consuming, but we believe that this disadvantage is largely balanced by the good results, since the errors or ambiguities in this initial step turn out to be costly in the future. For this reason, the application of this methodology in its complete form should be circumscribed to a complex problem characterized by unclear requirements and to support the development of new IS, and not the redesign of existing ones. Furthermore, this approach can help dealing with the basic assumption that results from the “task–artifact cycle” defended by Carroll et al. [31]. 6 Conclusion In the medical informatics literature, one of the identified reasons for information system failures and unsuccessful results is a lack of systematic consideration of human and other non-technology issues throughout design or implementation process [10–12]. Several authors emphasize the need for UCD development of health information systems and argue for an early user involvement [10,12,33,34]. The UCD approach consists in creating systems that are modeled in conformity with the characteristics and tasks of the potential users. UCD has given rise to many forms of design practices in which various characteristics of the context of use are considered. In this work, we illustrated the use of user-centered requirements engineering framework that supports the development process of a WbIS to assist the hemophilia patient care in the hematology service of a Portuguese regional hospital. This study was developed along two years, and during the first nine months, we made an exploratory study based on a GT approach, using several qualitative methods. These qualitative methods, such as ethnography and focus groups, have the ability to capture experiences, emotions, and human interaction processes through the inductive collection of one subsequent rigorous analysis of information from the individuals’ perspective [34]. In this stage the ethnographic study revealed important process details that were missed by other requirements elicitation techniques, specifically analysis of documentation and unstructured interviews. In the health care domain, ethnography is a very important method for requirements elicitation, however it is not a complete approach on its own and it should be used in complement to other approaches. After this exploratory study, we performed an evolutionary design based on constant development and refinement of the generic domain model and requirements document. Different methods based on triangulation work using the UCD principles were used. First, we chose an OOSA method since it is an appropriate method to represent the real-life entity or abstraction and use-case diagram of UML notation to identify and represent the individual interactions with systems through actor and use-case notation. As a second method we chose a model-based TA as it is a good method to capture and understand the sequences of the sub-tasks (sub-functionality) within one specific task (functionality) and thus enabled us to capture the details about the task. In a complex domain such as healthcare, it is necessary to provide a contextual framework that serves to focus the discussion and analysis. Hence, we chose in the last phase a prototyping technique in order to validate the achieved solution and consequently analyze users’ preferences concerning user interfaces. The ‘between-methods triangulation’ in a sequential combination helped us to achieve those goals and the three used different types of analysis and validation methods contributed for the domain model design that will support the development of the WbIS in hemophilia care. These three methods together proved to have great value for the design of our system allowing different views of the same problem. Although the first method based on UML notation represents the standard language for specifying, visualizing, constructing and documenting the artifacts of any system, its models must be avoided in the validation process with users without computing knowledge as it is in general the case of HCPs. In fact, we verified that the usage of abstract terminologies (like UML) in the validation process can confuse and intimidate users and little feedback is obtained, since users do not react, and sometimes just agree when they do not understand our language. The second method turned out to be a practical way in the process of understanding the human factors involved in the tasks. It is a more powerful medium for communication in that the requirement engineer is free to express users’ tasks without syntactical constraints. Little new functionalities were added in this step, yet very important information details were collected with this method. Finally, prototyping proved to be the easiest tool to involve the user in a requirement engineering process contributing to the identification of new high-level functionalities and definition of user interfaces. The result of this study, the conceptual model, was implemented using open source technologies, and followed the principles of an agile development approach, more specifically the eXtreme Programming (XP). This approach was chosen by its proximity to the UCD practices, getting a quick feedback from the user through the testing of the small software versions obtained in different iterations. The technological solution is a user-friendly web application that allows communication among hemophilia patients and HCPs, and makes information on treatment's effectiveness, and patient clinical data, as well as other disease related information available. On one hand, it offers HCPs analytical tools to transform data into information, and mine relevant information through data queries. On the other hand, using a user-friendly web application, patients can have direct access to relevant information, allowing them to view their clinical history, as well as introduce home-therapy data. Thus, the new system has improved the process of information managing associated with the clinical practice, since it allows clinicians quickly and easily access a set of meaningful data. Additionally the system provides a set of alerts to abnormal situations, or to those that require a check by a clinician. This happens, for example, when the level of safety stock is reached, or when a patient introduces a treatment registry. For the patients, the system also brought additional benefits, as it allows recording and sending their treatments data, when previously they would have to be recorded on paper and delivered at the hospital. This helped to improve the quality of life for patients, putting them in constant contact with the hospital. Finally, there is the possibility to automatically manage the stocks of CFCs used in treatments, which relieves the nurses from those manual, time consuming and error prone tasks. Based on the experience gained with this project, we conclude that requirements engineering in health care domain is a really complex task; however, we believe that using a UCD in a triangulation work in such a critical step is a good design practice. Conflict of interest None declared. Acknowledgements We would like to gratefully acknowledge the contribution of the clinical professionals of HS_CHC, specifically to Doctor Natália Martins and Doctor Ramon Salvado for providing us access to data and information systems, as well as for all the help in the requirements analysis and discussion of the proposed model. We are also gratefully to Eng. Vasco Saavedra, Enga Ana Luisa Ramos, Eng. Igor Carreira and Doctor José Moreira for the fruitful discussion about this issue. References [1] E. Ammenwerth N. Shaw Bad health informatics can kill – is evaluation the answer? Methods of Information in Medicine 44 2005 1 3 [2] J. Horsky J. Zhang V.L. Patel To err is not entirely human: complex technology and user cognition Journal of Biomedical Informatics 38 2005 264 266 [3] S. Garde P. Knaup Requirements engineering in health care: the example of chemotherapy planning in paediatric oncology Requirements Engineering 11 2006 265 278 [4] M. Maguire Methods to support human-centred design International Journal of Human–Computer Studies 55 2001 587 634 [5] M. Del-Beccaro, H. Jeffries, M. Eisenberg, E. Harry, Computerized provider order entry implementation: no association with increased mortality rates in an intensive care unit, Pediatrics 118 (2006) 290–295. [6] Y. Han J. Carcillo S. Venkataraman R. Clark S. Watson T. Nguyen H. Bayir R. Orr Unexpected increased mortality after implementation of a commercially sold computerized physician order entry system Pediatrics 116 2005 1506 1512 [7] R. Koppel J.P. Metlay A. Cohen B. Abaluck A.R. Localio S.E. Kimmel B.L. Strom Role of computerized physician order entry systems in facilitating medication errors Journal of the American Medical Association 293 2005 1197 1203 [8] E. Ammenwerth C. Iller U. Mansmann Can evaluation studies benefit from triangulation? A case study International Journal of Medical Informatics 70 2003 237 248 [9] J. Aarts P. Gorman IT in health care: sociotechnical approaches “To Err is System” International Journal of Medical Informatics 76 2007 S1 S3 [10] S. Rinkus M. Walji K.A. Johnson-Throop J.T. Malin J.P. Turley J.W. Smith J. Zhang Human-centered design of a distributed knowledge management system Journal of Biomedical Informatics 38 2005 4 17 [11] J. Zhang Human-centered computing in health information systems. Part 1. Analysis and design Journal of Biomedical Informatics 38 2005 1 3 [12] G.M. Samaras R.L. Horst A systems engineering perspective on the human-centered design of health information systems Journal of Biomedical Informatics 38 2005 61 74 [13] B.W. Boehm Software risk management: principles and practices Software, IEEE 8 1991 32 41 [14] Standish_Group, The CHAOS report. Technical report, Standish Group, 1995. [15] A. Seffah J. Gulliksen M.C. Desmarais An introduction to human-centered software engineering: integrating usability in the development process Human-Centered Software Engineering – Integrating Usability in the Software Development Lifecycle 2005 Springer Netherlands 3 14 [16] G. Southon C. Sauer K. Dampney Lessons from a failed information systems initiative: issues for complex organisations International Journal of Medical Informatics 55 1999 33 46 [17] S.H. Stumpf R.R. Zalunardo R.J. Chen Barriers to telemedicine implementation: usually it's not technology issues that undermine a project – it's everything else Healthcare Informatics 19 2002 45 48 [18] R. Heeks Health information systems: failure, success and improvisation International Journal of Medical Informatics 75 2006 125 137 [19] C. Johnson T. Johnson J. Zhang Increasing productivity and reducing errors through usability analysis: a case study and recommendations Proceedings AMIA Symposium 2000 394 398 [20] V Vimarlund T. Timpka Design participation as insurance: risk- management and end-users participation in the development of information systems in healthcare organizations Methods of Information in Medicine 41 2002 76 80 [21] M.C. Beuscart-Zéphir J. Brender R. Beuscart I. Ménager-Depriester Cognitive evaluation: how to assess the usability of information technology in healthcare Computer Methods and Programs in Biomedicine 54 1997 19 28 [22] C. Carroll P. Marsden P. Soden E. Naylor J. New T. Dornan Involving users in the design and usability evaluation of a clinical decision support system Computer Methods and Programs in Biomedicine 69 2002 123 135 [23] M. Irestig T. Timpka Politics and technology in health information systems development: a discourse analysis of conflicts addressed in a systems design group Journal of Biomedical Informatics 41 2008 82 94 [24] M. Ji-Ye V. Karel W.S. Paul C. Tom The state of user-centered design practice Communications of the ACM 48 2005 105 109 [25] M. Myers, Investigating information systems with ethnographic research, Communications of the AIS 2 (1999) 18 [26] B. Nuseibeh S. Easterbrook Requirements engineering: a roadmap Proceedings of the IEEE International Conference on Software Engineering 2000 ICM Press Limerick, Ireland 35 46 [27] B.H.C. Cheng J.M. Atlee Research directions in requirements engineering Proceedings of the IEEE International Conference on Software Engineering: Future of Software Engineering 2007 IEEE Computer Society Minneapolis (MN), USA 285 303 [28] P. Sawyer G. Kotonya Software requirements A. Abran J.W. Moore P. Bourque R. Dupuis SWEBOK – Guide to the Software Engineering Body of Knowledge 2001 IEEE Computer Society California 30 55 [29] A. Abran J.W. Moore P. Bourque R. Dupuis SWEBOK – Guide to the Software Engineering Body of Knowledge 2004 ed. 2004 IEEE Computer Society California [30] I. Sommerville Software Engineering 8th ed. 2007 Addison-Wesley Harlow [31] J.M. Carroll W.A. Kellogg M.B. Rosson The Task–Artifact Cycle, Designing Interaction: Psychology at the Human–Computer Interface 1991 Cambridge University Press 74–102 [32] R. Fairley R. Thayer The concept of operations: the bridge from operational requirements to technical specifications Annals of Software Engineering 3 1997 417 432 [33] C.M. Johnson T.R. Johnson J. Zhang A user-centered framework for redesigning health care interfaces Journal of Biomedical Informatics 38 2005 75 87 [34] J. Zhang V.L. Patel K.A. Johnson J.W. Smith Designing human-centered distributed information systems IEEE Intelligent Systems 17 2002 42 47 [35] ISO-13407, International Organization for Standardization /ISO 13407:1999, Human-centred design processes for interactive systems, 1999 [36] M.-C. Beuscart-Zéphir F. Anceaux H. Menu S. Guerlinger L. Watbled F. Evrard User-centred, multidimensional assessment method of clinical information systems: a case-study in anaesthesiology International Journal of Medical Informatics 74 2005 179 189 [37] A. Kushniruk Evaluation in the design of health information systems: application of approaches emerging from usability engineering Computers in Biology and Medicine 32 2002 141 149 [38] J. Zhang Human-centered computing in health information systems. Part 2. Evaluation Journal of Biomedical Informatics 38 2005 173 175 [39] S. De Rouck A. Jacobs M. Leys A methodology for shifting the focus of e-health support design onto user needs: a case in the homecare field International Journal of Medical Informatics 77 2008 589 601 [40] M. Berg J. Aarts J. Van Der Lei ICT in health care: sociotechnical approaches Methods of Information in Medicine 42 2003 297 301 [41] J.H. Gennari C. Weng J. Benedetti D.W. McDonald Asynchronous communication among clinical researchers: a study for systems design International Journal of Medical Informatics 74 2005 797 807 [42] E.W. Chauncey Triangulation: the explicit use of multiple methods, measures, and approaches for determining core issues in product development Interactions 13 2006 46–ff [43] M.A Kaulio I.C.M. Karlsson Triangulation strategies in user requirements investigations: a case study on the development of an IT-mediated service Behaviour & Information Technology 17 1998 103 112 [44] J. Greene C. McClintock Triangulation in evaluation: design and analysis issues Evaluation Review 9 1985 523 545 [45] B. Glaser A. Strauss The Discovery of Grounded Theory: Strategies for Qualitative Research 1967 Aldine Publishing Company Chicago, Illinois [46] N.K. Denzin Y.S. Lincoln The SAGE Handbook of Qualitative Research 3rd ed. 2005 Sage Publications Thousand Oaks, CA [47] A. Strauss J. Corbin Basics of Qualitative Research: Techniques and Procedures for Developing Grounded Theory 2nd ed. 1998 Sage Thousand Oaks, CA [48] G.H. Galal R.J. Paul A qualitative scenario approach to managing evolving requirements Requirements Engineering 4 1999 92 102 [49] C.E. Kuziemsky G.M. Downing F.M. Black F. Lau A grounded theory guided approach to palliative care systems design International Journal of Medical Informatics 76 2007 S141 S148 [50] V. Aggarwal The application of the unified modeling language in object-oriented analysis of healthcare information systems Journal of Medical Systems 26 2002 383 397 [51] F. Banhart R. Lohmann An object-oriented approach for structuring the electronic medical record Studies in Health Technology and Informatics 77 2000 622 626 [52] C. Egyhazy S. Eyestone J. Martino C. Hodgson Object-oriented analysis and design: a methodology for modeling the computer-based patient record Topics in Health Information Management 19 1998 48 65 [53] E. Kuikka A. Eerola J. Porrasmaa A. Miettinen J. Komulainen Design of the SGML-based electronic patient record system with the use of object-oriented analysis methods Studies in Health Technology and Informatics 68 1999 838 841 [54] M. Krol D.L. Reich Object-oriented analysis and design of a health care management information system Journal of Medical Systems 23 1999 145 158 [55] M. Hakman T. Groth Object-oriented biomedical system modeling – the rationale Computer Methods and Programs in Biomedicine 59 1999 1 17 [56] Y.-M. Zhu S.M. Cochoff An object-oriented framework for medical image registration, fusion, and visualization Computer Methods and Programs in Biomedicine 82 2006 258 267 [57] H.-A. Park I. Cho N. Byeun Modeling a terminology-based electronic nursing record system: an object-oriented approach International Journal of Medical Informatics 76 2007 735 746 [58] G. Booch J. Rumbaugh I. Jacobson The Unified Modeling Language User Guide 1999 Addison-Wisley Reading, MA [59] J.L. Whitten L.D. Bentley K.C. Dittman Systems Analysis and Design Methods 6th ed. 2004 McGraw-Hill/Irwin New York [60] L. Teixeira C. Ferreira B.S. Santos N. Martins Modeling a web-based information system for managing clinical information in hemophilia care Proceedings of 28th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS) 2006 IEEE CNF NY, USA 2610 2613 [61] S. Patricia Multiple methods and the usability of interface prototypes: the complementarity of laboratory observation and focus groups Proceedings of the 9th Annual International Conference on Systems Documentation Chicago 1991 106 112 [62] D. Diaper N.A. Stanton The Handbook of Task Analysis for Human–Computer Interaction 2004 Lawrence Erlbaum Associates Mahwah, New Jersey [63] J. Annett K.D. Duncan R.B. Stammers M.J. Gray Task Analysis 1971 Her Majesty's Stationery Office – HMSO London [64] J. Stuart R. Penn TaskArchitect: taking the work out of task analysis Proceedings of the 3rd Annual Conference on Task Models and Diagrams Prague, Czech Republic 2004 ACM 145 154 [65] J. Nielsen Designing Web Usability: The Practice of Simplicity 2000 New Riders Indianapolis [66] A. Dix J. Finley G. Abowd B. Russell Human–Computer Interaction 2nd ed. 1998 Prentice Hall London [67] B. Shneiderman C. Plaisant Designing the User Interface: Strategies for Effective Human–Computer Interaction 4th ed. 2005 Pearson Boston, MA [68] L. Teixeira C. Ferreira B.S. Santos Using task analysis to improve the requirements elicitation in health information system Proceedings of 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBS) 2007 IEEE CNF Lyon, France 3669 3672 [69] T. Cortes A. Lee J. Boal L. Mion A. Butler Using focus groups to identify asthma care and education issues for elderly urban-dwelling minority individuals Applied Nursing Research 17 2004 207 212 "
    },
    {
        "doc_title": "Web-enabled registry of inherited bleeding disorders in Portugal: Conditions and perception of the patients",
        "doc_scopus_id": "84355166743",
        "doc_doi": "10.1111/j.1365-2516.2011.02574.x",
        "doc_eid": "2-s2.0-84355166743",
        "doc_date": "2012-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Hematology",
                "area_abbreviation": "MEDI",
                "area_code": "2720"
            },
            {
                "area_name": "Genetics (clinical)",
                "area_abbreviation": "MEDI",
                "area_code": "2716"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Local and national haemophilia registries are powerful instruments to support the healthcare and researchers and improve the communication between Comprehensive Haemophilia Diagnostic and Treatment Centres (HTCs) and patients. Hemo@care is an example of a Local Haemophilia Registry Systems (LHR-Sys) based on the Web, developed in collaboration with a HTC located in Portugal, to support the haemophilia treatments registry, collect and manage the clinical information and provide mechanisms to control the clotting factor concentrates (CFC) stock. To extend this solution (the hemo@care) to other Portuguese HTCs and consequently to meet the preconditions to create a National Haemophilia Registry Systems (NHR-Sys), a study based on a questionnaire was carried out at nationwide. This study aims to assess the conditions and motivations of people with haemophilia (PWH) geographically scattered throughout the country, to use a potential Web-enabled registry with the purpose of replacing the traditional paper-diaries, to understand their judgment about a potential NHR-Sys currently non-existent in Portugal, and at the same time, to characterize demographically and pathologically those people at the nationwide. The results based on the analysis of 168 responses (response rate of 31%) confirmed the high prevalence of the disease in haemophilia A (75%) compared with haemophilia B (11.3%) and a large incidence in the severe levels, or the existence of people with mild severity without diagnosis and treatment. Furthermore, the results also revealed the need, conditions and motivation for using a registry system by PWH; thus it is deemed to justify the extension of the hemo@care to other HTCs in Portugal and consequently to create the NHR-Sys. © 2011 Blackwell Publishing Ltd.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inter-observer variability assessment of a left ventricle segmentation tool applied to 4D MDCT images of the heart",
        "doc_scopus_id": "84862625754",
        "doc_doi": "10.1109/IEMBS.2011.6090923",
        "doc_eid": "2-s2.0-84862625754",
        "doc_date": "2011-12-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Cardiac angiography",
            "Cardiac phasis",
            "Computed Tomography",
            "Interobserver variability",
            "Left ventricles",
            "MDCT images",
            "Multiple detectors",
            "Myocardial perfusion",
            "Segmentation tool",
            "Semi-automatics"
        ],
        "doc_abstract": "Multiple detector row computed tomography (MDCT) cardiac angiography provides a large amount of data concerning multiple cardiac phases which are not often considered. Segmentation is a first step towards exploring how this additional data can be used to perform left ventricle functional analysis or myocardial perfusion assessment. We present preliminary results regarding the assessment of inter-observer variability for a semi-automatic (multi-phase) segmentation tool for the left-ventricle. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using participatory design in a health information system",
        "doc_scopus_id": "84862293238",
        "doc_doi": "10.1109/IEMBS.2011.6091321",
        "doc_eid": "2-s2.0-84862293238",
        "doc_date": "2011-12-26",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Health information systems",
            "Participatory design",
            "Project managers",
            "Requirement analysis",
            "User centred design"
        ],
        "doc_abstract": "This article describes the experience of developing an interactive Health Information System (iHIS) currently under test in a hospital, which benefited from the practices of the User-Centred Design (UCD), in a Participatory Design (PD) approach. Techniques from the Human-Computer Interaction (HCI) and/or Usability Engineering (UE), combined with traditional Software Engineering (SE), allowed an effective and usable solution from the user's point of view. The good results usually achieved with this approach were confirmed. Despite these good results, we deem that if there is not some control of the procedure by the project manager, it may be difficult to end the requirement analysis, since requirement reformulation is fostered. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using Participatory Design in a Health Information System.",
        "doc_scopus_id": "84055190037",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84055190037",
        "doc_date": "2011-12-01",
        "doc_type": "Article",
        "doc_areas": [],
        "doc_keywords": [],
        "doc_abstract": "This article describes the experience of developing an interactive Health Information System (iHIS) currently under test in a hospital, which benefited from the practices of the User-Centred Design (UCD), in a Participatory Design (PD) approach. Techniques from the Human-Computer Interaction (HCI) and/or Usability Engineering (UE), combined with traditional Software Engineering (SE), allowed an effective and usable solution from the user's point of view. The good results usually achieved with this approach were confirmed. Despite these good results, we deem that if there is not some control of the procedure by the project manager, it may be difficult to end the requirement analysis, since requirement reformulation is fostered. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Inter-observer variability assessment of a left ventricle segmentation tool applied to 4D MDCT images of the heart.",
        "doc_scopus_id": "84055176594",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84055176594",
        "doc_date": "2011-12-01",
        "doc_type": "Article",
        "doc_areas": [],
        "doc_keywords": [],
        "doc_abstract": "This article describes the experience of developing an interactive Health Information System (iHIS) currently under test in a hospital, which benefited from the practices of the User-Centred Design (UCD), in a Participatory Design (PD) approach. Techniques from the Human-Computer Interaction (HCI) and/or Usability Engineering (UE), combined with traditional Software Engineering (SE), allowed an effective and usable solution from the user's point of view. The good results usually achieved with this approach were confirmed. Despite these good results, we deem that if there is not some control of the procedure by the project manager, it may be difficult to end the requirement analysis, since requirement reformulation is fostered. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Integrating user studies into computer graphics-related courses",
        "doc_scopus_id": "80052317465",
        "doc_doi": "10.1109/MCG.2011.78",
        "doc_eid": "2-s2.0-80052317465",
        "doc_date": "2011-09-07",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "computer graphics education",
            "graphics and multimedia",
            "Human-computer",
            "Information visualization",
            "Student collaboration",
            "User study"
        ],
        "doc_abstract": "The authors argue in favor of introducing user studies into computer graphics, human-computer interaction, and information visualization courses. They discuss two sets of user studies they developed and performed over several years, with student collaboration, and the different aspects of the studies they had to consider. They also discuss a user study they designed for an information visualization course. © 2011 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A preparatory study to choose similarity metrics for left-ventricle segmentations comparison",
        "doc_scopus_id": "79955785537",
        "doc_doi": "10.1117/12.878294",
        "doc_eid": "2-s2.0-79955785537",
        "doc_date": "2011-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Computational costs",
            "Ground truth",
            "Image processing and analysis",
            "Left ventricles",
            "preparatory studies",
            "Quantitative measures",
            "Segmentation methods",
            "Similarity metrics"
        ],
        "doc_abstract": "In medical image processing and analysis it is often required to perform segmentation for quantitative measures of extent, volume and shape. The validation of new segmentation methods and tools usually implies comparing their various outputs among themselves (or with a ground truth), using similarity metrics. Several such metrics are proposed in the literature but it is important to select those which are relevant for a particular task as opposed to using all metrics and therefore avoiding additional computational cost and redundancy. A methodology is proposed which enables the assessment of how different similarity and discrepancy metrics behave for a particular comparison and the selection of those which provide relevant data. © 2011 SPIE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Enterprise resource planning system in a multinational enterprise: Users' attitude post implementation",
        "doc_scopus_id": "78650924687",
        "doc_doi": "10.1007/978-3-642-16419-4_27",
        "doc_eid": "2-s2.0-78650924687",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Mathematics (all)",
                "area_abbreviation": "MATH",
                "area_code": "2600"
            }
        ],
        "doc_keywords": [
            "Critical success factor",
            "ERP",
            "Implementation",
            "SAP R/3",
            "Training"
        ],
        "doc_abstract": "This paper analyzes the impact of the implementation of SAP R/3 in a Multinational Portuguese Organization (MPO), defining some Critical Success Factors (CSFs). In order to understand the motivations of end-users prior to implementation and to analyze the behavior after a change (post-implementation), a study based on a questionnaire was carried out. The sample included 67 users of SAP R/3 that were present throughout the process. Considering the results, we can conclude that the implementation of SAP R/3 in MPO was successful, and the respondents consider their work more productive and achieve easier access to information. The existence of a solid team to support the project was established as a major facilitator in the whole process, in opposing with the limited time and lack of training that emerged as barriers to the implementation. It was also found that the learning period assumes a high importance in the success of the implementation, once increasing the training time reduces the need for support to the end-users. © 2010 Springer-Verlag.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Improving the management of chronic diseases using web-based technologies: An application in hemophilia care",
        "doc_scopus_id": "78650813584",
        "doc_doi": "10.1109/IEMBS.2010.5626026",
        "doc_eid": "2-s2.0-78650813584",
        "doc_date": "2010-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder",
            "Chronic disease",
            "Clinical practices",
            "Health care providers",
            "Information and communication",
            "Portugal",
            "Technological solution",
            "Web technologies",
            "Web-based technologies"
        ],
        "doc_abstract": "Modern methods of information and communication that use web technologies provide an opportunity to facilitate closer communication between patients and healthcare providers, allowing a joint management of chronic diseases. This paper describes a web-based technological solution to support the management of inherited bleeding disorders integrating, diffusing and archiving large sets of data relating to the clinical practice of hemophilia care, more specifically the clinical practice at the Hematology Service of Coimbra Hospital Center (a Hemophilia Treatment Center located in Portugal). © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Wiimote as an input device in google earth visualization and navigation: A user study comparing two alternatives",
        "doc_scopus_id": "78449283947",
        "doc_doi": "10.1109/IV.2010.72",
        "doc_eid": "2-s2.0-78449283947",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Google earth",
            "Input devices",
            "Usability",
            "User study",
            "Wiimote"
        ],
        "doc_abstract": "This paper presents a user study performed to compare the usability of the Wiimote as an input device to visualize information and navigate in Google Earth using two different configurations. This study had the collaboration of 15 participants which performed a set of tasks using the Wiimote as an input device while the image was projected on a common projection screen, as well as a mouse on a desktop. Results show that most users clearly preferred one of the Wiimote configurations over the other, and over the mouse; moreover, they had better performances using the preferred configuration, and found it easier to use. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information visualization in facility location and vehicle routing decisions",
        "doc_scopus_id": "78449271134",
        "doc_doi": "10.1109/IV.2010.25",
        "doc_eid": "2-s2.0-78449271134",
        "doc_date": "2010-11-24",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Decision support tools",
            "Facility location",
            "Facility locations",
            "General publics",
            "Information visualization",
            "Location-routing",
            "Routing decisions",
            "Usability evaluation methods"
        ],
        "doc_abstract": "Facility location and vehicle routing are amongst the most important logistic decisions in today's organizations. These aspects are intertwined and, in some cases, should be addressed in an integrated way (giving rise to the location-routing approach). A decision support tool that can make easier the visualization (and editing) of information regarding these problems is becoming increasingly important as: it enables to further understand the problem at hand; and, at the same time, it fosters better communication of the decisions in a way easier to understand by the general public. This paper presents some concepts for information visualization on the problems arisen by the aforementioned decisions, which have been incorporated in a decision support tool and tested using usability evaluation methods. © 2010 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A perceptual data repository for polygonal meshes",
        "doc_scopus_id": "70350543996",
        "doc_doi": "10.1109/VIZ.2009.41",
        "doc_eid": "2-s2.0-70350543996",
        "doc_date": "2009-11-05",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            }
        ],
        "doc_keywords": [
            "Data repositories",
            "Experimental methodology",
            "Observer studies",
            "Perceived quality",
            "Polygonal meshes",
            "Preliminary assessment"
        ],
        "doc_abstract": "A repository containing perceived quality data for polygonal meshes, obtained through observer studies, is presented. It includes information regarding the experimental methodology, protocol and models used, with the purpose of allowing researchers to use it, e.g., for a faster preliminary assessment of their perceived quality metrics without the overhead of designing and performing an observer study. © 2009 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A preliminary usability evaluation of Hemo@Care: A web-based application for managing clinical information in hemophilia care",
        "doc_scopus_id": "70350238791",
        "doc_doi": "10.1007/978-3-642-02806-9_91",
        "doc_eid": "2-s2.0-70350238791",
        "doc_date": "2009-10-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Health information system",
            "Hemo@care",
            "Nursing staff",
            "Usability evaluation",
            "WEB application",
            "Web-based applications"
        ],
        "doc_abstract": "In this work, an overall description of the methods used and the results obtained in the on-going evaluation of hemo@care is presented. To help understanding the methods and results, we first give an overview of the main functionalities of hemo@care, which is a web application to manage the clinical information in hemophilia care, developed to be used by hematologists, nursing staff and patients suffering from hemophilia. Following we described the methods used in this particular evaluation, and finally we present the main results and general conclusions of these preliminary usability evaluation. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "The user's role in the development process of a clinical information system: An example in hemophilia care",
        "doc_scopus_id": "70350214967",
        "doc_doi": "10.1007/978-3-642-02806-9_106",
        "doc_eid": "2-s2.0-70350214967",
        "doc_date": "2009-10-27",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Theoretical Computer Science",
                "area_abbreviation": "MATH",
                "area_code": "2614"
            },
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Clinical information system",
            "Critical phasis",
            "Development process",
            "Health information system",
            "Human-centered designs",
            "User-centered design",
            "WEB application",
            "Web based information systems"
        ],
        "doc_abstract": "This work describes the development process of a Web-based Information System for managing clinical information in hemophilia care, emphasizing the role of the users around a human-centered development. To help understanding all this process, we first present the relevant concepts concerning human-centered design; next we describe the web application for managing the clinical information in hemophilia care, as well as, the development process followed in its development; and finally we illustrate the importance of the user's involvement in critical phases through the demonstration of some results. © 2009 Springer Berlin Heidelberg.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Determinants of executive stock options: Portuguese evidence",
        "doc_scopus_id": "84897382426",
        "doc_doi": "10.22495/cocv7i2c2p2",
        "doc_eid": "2-s2.0-84897382426",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Business, Management and Accounting (all)",
                "area_abbreviation": "BUSI",
                "area_code": "1400"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "The main purpose of this study is to investigate the determinants of stock options attribution. We hypothesize that the equity held by managers, the ownership concentration, the liquidity constraints, the firm risk and the firm size are related to the firm's attribution of stock options. Using a sample of 44 companies listed in Euronext Lisbon, we find that only equity held by managers and firm size are related to the firm's attribution of stock options, documenting a positive relationship for both.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Head-mounted display versus desktop for 3D navigation in virtual reality: A user study",
        "doc_scopus_id": "58049208308",
        "doc_doi": "10.1007/s11042-008-0223-2",
        "doc_eid": "2-s2.0-58049208308",
        "doc_date": "2009-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Media Technology",
                "area_abbreviation": "ENGI",
                "area_code": "2214"
            },
            {
                "area_name": "Hardware and Architecture",
                "area_abbreviation": "COMP",
                "area_code": "1708"
            },
            {
                "area_name": "Computer Networks and Communications",
                "area_abbreviation": "COMP",
                "area_code": "1705"
            }
        ],
        "doc_keywords": [
            "3d navigations",
            "Application areas",
            "Design and developments",
            "Game scenarios",
            "Head mounted display (HMD)",
            "Navigation tasks",
            "Set-ups",
            "Short periods",
            "Usability evaluations",
            "User evaluations",
            "User performances",
            "User study",
            "Virtual environments (VE)",
            "Vr systems"
        ],
        "doc_abstract": "Virtual Reality (VR) has been constantly evolving since its early days, and is now a fundamental technology in different application areas. User evaluation is a crucial step in the design and development of VR systems that do respond to users' needs, as well as for identifying applications that indeed gain from the use of such technology. Yet, there is not much work reported concerning usability evaluation and validation of VR systems, when compared with the traditional desktop setup. The paper presents a user study performed, as a first step, for the evaluation of a low-cost VR system using a Head-Mounted Display (HMD). That system was compared to a traditional desktop setup through an experiment that assessed user performance, when carrying out navigation tasks in a game scenario for a short period. The results show that, although users were generally satisfied with the VR system, and found the HMD interaction intuitive and natural, most performed better with the desktop setup. © 2008 Springer Science+Business Media, LLC.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "A decision-support tool for a capacitated location-routing problem",
        "doc_scopus_id": "56049110500",
        "doc_doi": "10.1016/j.dss.2008.07.007",
        "doc_eid": "2-s2.0-56049110500",
        "doc_date": "2008-12-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Management Information Systems",
                "area_abbreviation": "BUSI",
                "area_code": "1404"
            },
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Developmental and Educational Psychology",
                "area_abbreviation": "PSYC",
                "area_code": "3204"
            },
            {
                "area_name": "Arts and Humanities (miscellaneous)",
                "area_abbreviation": "ARTS",
                "area_code": "1201"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Decision-support tool",
            "Geographic datums",
            "Routing problems",
            "Support tools",
            "Vehicle fleets",
            "Web map servers",
            "Windows platforms"
        ],
        "doc_abstract": "In this paper we present a decision-support tool (DST) that implements a capacitated location-routing problem (CLRP) with two levels (depots and customers) and a capacitated and homogeneous vehicle fleet. It allows the exploration of the solution finding process in a way easily understandable by the user, and enables access to online geographic data through web map servers (WMS). This tool was developed for Windows platforms having an architecture that easily allows the integration of new functionality. © 2008 Elsevier B.V. All rights reserved.",
        "available": true,
        "clean_text": "serial JL 271653 291210 291773 291813 291820 31 Decision Support Systems DECISIONSUPPORTSYSTEMS 2008-07-18 2008-07-18 2010-04-18T12:12:23 S0167-9236(08)00133-4 S0167923608001334 10.1016/j.dss.2008.07.007 S300 S300.1 FULL-TEXT 2015-05-14T05:08:13.335849-04:00 0 0 20081201 20081231 2008 2008-07-18T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure body affil articletitle auth authfirstini authfull authkeywords authlast primabst ref vitae alllist content subj ssids 0167-9236 01679236 46 46 1 1 Volume 46, Issue 1 30 366 375 366 375 200812 December 2008 2008-12-01 2008-12-31 2008 article fla Copyright © 2008 Elsevier B.V. All rights reserved. ADECISIONSUPPORTTOOLFORACAPACITATEDLOCATIONROUTINGPROBLEM LOPES R 1 Introduction 2 A capacitated location-routing problem (CLRP) 3 Solving the capacitated location-routing problem 3.1 Cluster analysis 3.2 Travelling salesman problem 3.3 Improvement of client routes 3.4 Capacitated location-allocation problem 4 A decision-support tool for the CLRP 4.1 Graphical user interface (GUI) 4.2 Information flow 4.2.1 Data input 4.2.2 Web map server integration 4.2.3 Graphical representation 4.2.4 Visualization of the solution 4.2.5 Data output 4.2.6 Other characteristics 5 Conclusion References ALBAREDASAMBOLA 2005 407 428 M ALTER 2004 319 327 S AMBROSINO 2005 610 624 D BARRETO 2007 968 977 S BOOKBINDER 2005 461 466 J BRANCO 1990 86 95 I BRUNS 1995 1 6 A PROCEEDINGSSECONDINTERNATIONALWORKSHOPDISTRIBUTIONLOGISTICS ITERATIVEHEURISTICFORLOCATIONROUTINGPROBLEMSBASEDCLUSTERING CARLSSON 2002 105 110 C COMMISSIONOFTHEEUROPEANCOMMUNITIES 2001 EUROPEANTRANSPORTPOLICYFOR2010TIMEDECIDECOM2001370BRUSSELSWHITEPAPER DIX 2004 A HUMANCOMPUTERINTERACTION EOM 1990 333 342 H EOM 1997 18 32 S EVERITT 1993 B CLUSTERANALYSIS LAPORTE 1988 163 197 G VEHICLEROUTINGMETHODSSTUDIES LOCATIONROUTINGPROBLEMS LIN 2006 1833 1849 C MALCZEWSKI 1999 J GISMULTICRITERIADECISIONANALYSIS MELECHOVSKY 2005 375 391 J MELO 1999 471 476 A MIN 1996 259 288 H MIN 1998 1 15 H MURTY 1999 175 182 K NAGY 1996 1166 1174 G NAGY 2007 649 672 G PERL 1985 381 396 J SALHI 1989 150 156 S SHIM 2002 111 126 J SRIVASTAVA 1990 427 435 R TUZUN 1999 87 99 D LOPESX2008X366 LOPESX2008X366X375 LOPESX2008X366XR LOPESX2008X366X375XR item S0167-9236(08)00133-4 S0167923608001334 10.1016/j.dss.2008.07.007 271653 2010-10-11T17:08:56.215647-04:00 2008-12-01 2008-12-31 true 2789438 MAIN 10 90054 849 656 IMAGE-WEB-PDF 1 fx1 10806 11 11 fx1 2181 49 50 fx10 10851 11 11 fx10 2196 49 50 fx11 11175 11 11 fx11 2508 49 50 fx12 11213 11 11 fx12 2542 49 50 fx13 11257 11 11 fx13 2598 49 49 fx14 11188 11 11 fx14 2244 49 49 fx15 11100 11 11 fx15 2458 49 49 fx2 10886 11 11 fx2 2201 50 50 fx3 11063 11 11 fx3 2200 50 50 fx4 11182 11 11 fx4 2465 50 50 fx5 12649 11 24 fx5 4037 50 110 fx6 13182 11 38 fx6 3563 37 125 fx8 11280 11 11 fx8 2044 49 50 fx9 11313 11 11 fx9 2159 49 50 fx7 4383 11 24 fx7 3715 50 110 gr1 31161 425 347 gr1 1300 93 76 gr10 84385 414 493 gr10 5362 93 111 gr11 30749 351 346 gr11 1775 93 92 gr12 46073 381 329 gr12 3169 93 80 gr13 34824 388 346 gr13 1868 93 83 gr2 108611 464 444 gr2 4461 93 89 gr3 90952 451 623 gr3 5853 91 125 gr4 26887 178 274 gr4 4741 81 125 gr5 78351 270 489 gr5 5935 69 125 gr6 159012 902 623 gr6 3998 93 64 gr7 65906 388 489 gr7 4917 94 118 gr8 245962 902 623 gr8 4169 93 64 gr9 61094 450 623 gr9 3673 90 125 DECSUP 11520 S0167-9236(08)00133-4 10.1016/j.dss.2008.07.007 Elsevier B.V. Fig. 1 Input and output data concerning each phase of the sequential heuristic for the CLRP. Fig. 2 Some instances and corresponding results of the sequential heuristic for the CLRP. Fig. 3 The GUI and the four different visualization areas. Fig. 4 Clients data with the Import and Export options. Fig. 5 Graphical data input (directly into the map area). Fig. 6 Currently featured server integration (Demis® and Google Maps®). Fig. 7 Objects graphical representation according to their position on the map. Fig. 8 Changing the map scale (from 1:1098 to 1:612). Fig. 9 Visualization of clients demand. Fig. 10 Control panel displaying the results associated to each grouping method. Fig. 11 Graphical representation of the solution. Fig. 12 Data window with the numerical information of the solution. Fig. 13 Visualization of the VRP (after the third step of the heuristic) on the DST. A decision-support tool for a capacitated location-routing problem Rui Borges Lopes a ⁎ Sérgio Barreto b Carlos Ferreira a Beatriz Sousa Santos c a Department of Economics, Management and Industrial Engineering/CIO, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal b ISCA-Higher Institute of Accounting and Administration/CIO, University of Aveiro, R. Associação Humanitária dos Bombeiros de Aveiro, 3811-902 Aveiro, Portugal c Department of Electronics, Telecommunications and Informatics/IEETA, University of Aveiro, Campus Universitário de Santiago, 3810-193 Aveiro, Portugal ⁎ Corresponding author. Tel.: +351 234370361; fax: +351 234370215. In this paper we present a decision-support tool (DST) that implements a capacitated location-routing problem (CLRP) with two levels (depots and customers) and a capacitated and homogeneous vehicle fleet. It allows the exploration of the solution finding process in a way easily understandable by the user, and enables access to online geographic data through web map servers (WMS). This tool was developed for Windows platforms having an architecture that easily allows the integration of new functionality. Keywords Decision-support tool Location-routing Web map servers 1 Introduction Location and routing have been some of the major concerns in logistics, having implications on the complete supply chain [7]. It is generally accepted that the success of many enterprises may depend on the location-distribution decisions [3,11] and that, nowadays, many managers tend to support their decisions on acquired experience [10]. This attitude may be caused by both the overall complexity of the problem and the specificities of different cases [22,25]. The development of an effective tool to support this kind of decisions is similarly complex due to the same reasons. Moreover, in the case of a tool to support decisions for location-routing problems (LRP), there are other key aspects beyond optimization issues; one such aspect is the presentation of the solution and the exploration of the process in a way easily understandable by the decision-maker, allowing better judgments. One can find in the literature a significant number of contributions concerning LRPs [16,22,25], as well as contributions regarding decision-support systems (DSS) [2,13,14,28]. On the other hand there is a lack of studies involving both areas. For that reason, an integrated approach to location and routing, and an application that can support the decision, may represent an important competitive advantage. With this paper we will try to help filling that gap, presenting a decision tool for a LRP integrating the essential features of this type of problem. 2 A capacitated location-routing problem (CLRP) The location-routing problem (LRP) appears as a combination of two (difficult) problems: the facility location problem (FLP) and the vehicle routing problem (VRP); both of them can be shown to be NP-hard. Moreover, the multiplicity of characteristics of those models also leads to a wide diversity of LRPs [22,25]. The LRP is a better model in contexts involving the simultaneous location of facilities and the design of distribution routes between the facilities and the users, given that solving separately these two aspects will most likely produce a suboptimal solution [27]. In this paper we consider a discrete capacitated location-routing problem (CLRP) with two layers (depots and customers) and a capacitated and homogeneous vehicle fleet. Each customer has a certain demand and the potential facilities (depots) have a certain capacity; the location cost of each depot is known, as well as the unitary cost of distribution (function of covered distance). Euclidean distances are considered. The model seeks to determine which depots must be opened (established) and to draw the distribution routes from these depots to the customers (vehicles start and end their routes at the same depot) minimizing the total cost (location and distribution costs) [6]. While the multiplicity of real cases and the integration of their characteristics in LRP models lead to a great diversity of problems [22,25], the above described CLRP integrates the essential LRP features and considers appropriate constraints to guarantee routes and depots with balanced capacity. 3 Solving the capacitated location-routing problem The capacitated location-routing problem (CLRP) is NP-hard; thus, most practical (large dimension) problems call for heuristic algorithms in order to obtain “good solutions” in reasonable time [1,17]; moreover, heuristic algorithms are (usually) easy to understand, modify and implement, providing several solutions allowing the user the flexibility to choose the preferred one. Heuristic methods for LRPs included sequential [23], iterative [26] and hierarchical [24] approaches. Recently, metaheuristics such as tabu search, simulated annealing and genetic algorithms were proposed [19,30]. Due to the characteristics of our model (vehicles with small capacity and significant fixed costs for the facilities), we used a sequential method as it has some advantages from a computational point of view [21,29]. More precisely we used a sequential distribution-first location-second heuristic [5], consisting of four main steps: (i) construct groups of customers with a capacity limit; (ii) determine the distribution in each customer group; (iii) improve the routes and (iv) locate the depots and assign the routes to them. Fig. 1 illustrates the schema of the heuristic and some details of the steps are presented in this section. This heuristic has been submitted to several computational tests with promising results, validating its use in a DSS (Fig. 2 ). The interested reader is referred to the work of [5] for a complete presentation and evaluation of this heuristic. The instances presented in Fig. 2 are available from [4]. 3.1 Cluster analysis Several authors have integrated cluster analysis procedures in heuristics for LRPs [9,21]. A detailed description of the clustering phase used in this heuristic can be found in [6]. The first step of the heuristic groups clients based on the capacity of the used vehicles. For this purpose, twenty different possibilities are used, combining four clustering methods (two hierarchical and two non-hierarchical) with six proximity measures (single linkage or nearest neighbour, complete linkage or farthest neighbour, group average, centroid, ward and saving) [15]. 3.2 Travelling salesman problem After the definition of the different groups of clients, an optimal distribution route for each cluster is determined, solving a travelling salesman problem (TSP). The method is based on the relaxation of the sub-routes constraints, in a two-index integer linear programming TSP formulation, whenever the group has 40 customers or less. The obtained integer problem is solved using the CPLEX® software package. Whenever sub-routes are detected, adequate constraints are introduced and the iterative process continues until the determination of the TSP optimal solution. If the group integrates more than 40 customers, a two-step heuristic is used for solving the TSP. In the first step, a feasible TSP route is obtained using the farthest-type neighbourhood criteria, to choose the next client to be included in the route, and saving criteria, to determine where to include it. In the second step, the feasible TSP solution is improved through a 3-optimal local search procedure. 3.3 Improvement of client routes This step provides an improvement of the routes found so far, using a 3-optimal local search as the one proposed by [8] for the Hamiltonian p-median problem. Besides providing a decrease of the routes cost, this step contributes to the elimination of the disagreeable biased effect, which emerges when the capacitated groups are formed. This phase provides the final routes, their capacity and the distance covered by them. 3.4 Capacitated location-allocation problem In the last step of the heuristic a capacitated location-allocation problem (CLAP) is solved. In this problem the heuristic allocates the routes to the depots (if it is advisable to install the depot). The routes are collapsed into a single client with an associated cost of allocation (to each depot) obtained by a saving function. The resulting single source CLAP is solved exactly using the CPLEX® software package. 4 A decision-support tool for the CLRP A very important issue of this tool is the target audience; we do not expect users (decision-makers) to have any background on modelling and optimization aspects concerning LRPs. Thus, the information provided to users is neither technical data regarding the heuristic nor its validation; we focused instead on providing a usable interface. To implement the application, we used an object-oriented (OO) methodology (unified modelling language—UML) and Visual Basic®, version 6.0. The developed application is organized around a main window, with all the functionalities accessible in this window, through the toolbar, or the menu (in a way easily understandable by users). In this context, the target user of this application is typically a decision-maker, with higher education and moderate computer literacy, but having much experience in the subject (professional experience in real installations of depots and designing logistic systems) and that will use the tool infrequently. This user profile suggests that the main usability goal should be ease of learning; therefore, the user interface should be extremely intuitive. The profile of the end-users, as well as the task they intend to perform using this DST, and the usability principles (e.g. consistency, compatibility, familiarity, feedback, robustness, etc.) [12] were taken into consideration during the design of the tool. The tool offers the following possibilities: • Input (or edit) new (or existing) data in order to define the problem; • Obtain solutions for the CLRP; • Visualize the results either through numeric or graphical representation; • Export the data to other applications; • Export the graphical solution. The developed tool incorporates two main parts: the solution algorithms (previously presented) and the graphical user interface (GUI). It was implemented for Windows platforms and has an open architecture which allows an easy integration of new functionality. The DST can also incorporate any other solution method for the CLRP that uses commercial software, due to its integration with the CPLEX® which allows solving the provided formulation. 4.1 Graphical user interface (GUI) The main purpose of the GUI was to allow an easy and efficient access to solutions for the CLRP. Thus, according to [18], the following characteristics are fundamental: • Easy to learn: allowing the intuitive use of the tool by any user; • Robustness: allowing the user to recover from unintended situations; • Interactivity: allowing the information to effectively flow between the user and the system; • Based on events: allowing the user to always be aware of the tasks he is performing. The GUI comprises four distinct sections as shown in Fig. 3 : • A toolbar with buttons allowing a quick access to the functionalities (Area A); • An alphanumerical component to display all the information regarding the problem (clients and depots data, vehicle capacity) (Area B); • A visualization area displaying the information regarding the maps (Area C); • A status bar showing the used hierarchic method and proximity measure (clustering phase), an icon indicating if the location of the objects in the map is provided by the application (Yes: ; No: ) and if the obtained solution is exact or approximate (Area D). Regarding the toolbar, there is a set of buttons corresponding to different functionalities, besides the standard ones (New, Open, Save, Print, etc.): Language: allows changing the language of the tool (currently supports English, Portuguese and Spanish); Import Map: allows importing an image to the map (bmp, or format files); Import Client/Depot Picture: allows importing an image (standard format: ico, bmp, etc.) to represent the client/depot; Client/Depot/Route Colour: allows modifying the client/depot/route icons colour in order to easily highlight the objects and identify the different routes on the map; Display Labels/Pictures: allows activating or deactivating the visualization of icons corresponding to clients and depots; View Demand: allows visualizing the clients based on their demand; Recalculate Scale: allows recalculating scale, hence adjusting the objects to the visualization area; Zoom In: allows zooming in on a specific area of the map. The interaction with web map servers (WMS) provides new imagery and detailed data of the selected region; Move Map: allows moving the map inside the visualization area. Through this option it is possible to adjust the map image to the remaining objects; Lock Relation: allows maintaining the relation between the map and the objects. This enables the use of different visualization scales for the map and the objects; Hide Map: allows hiding the image inserted in the map area, allowing a better view of the objects; Import Solution: allows importing a solution file obtained with other software; Save Map: allows saving to an image file all the data in the map area (map, clients, depots and routes). Most of these options are also available through the main menu, allowing a greater flexibility to the user. 4.2 Information flow In this section the information flow of the proposed DST will be presented. Firstly, the data input options, needed to insert the necessary information for characterizing and solving the problem, are presented. After the data input it is possible to obtain online geographic data from the web map servers (WMS). It is also possible to proceed to a set of changes, from a graphical point of view, namely associating maps and images, changing the visualization scale, etc., thus making the interpretation of the information easier. Further on, users can choose several ways to run the algorithm, enabling them to visualize and analyse the obtained solution. Finally, users may export the obtained solution as well as the inserted data. The following subsections describe in more detail how to work with the proposed DST, as well as its main functionalities. 4.2.1 Data input The information needed to obtain the solution using the tool is the following: • Clients location; • Demand associated with each client; • Possible locations for the depots; • Capacity associated with each depot; • Vehicles capacity. Our tool allows the required data to be inserted either globally or one client or depot at a time. In order to insert the data globally, there is an Open file option (allowing the user to recover all data regarding a problem previously saved) as well as an Import data option (allowing to import several clients and depots, even if they where originally inserted in other applications). Through this set of options the user can quickly import a large quantity of data. The input (or update) of new (or existing) data individually can be done either numerically (as shown in Fig. 4 ) or graphically (directly on the map). The option to insert the data directly on the map area makes easier the identification of the location on the map (X and Y coordinates) where the user intends to insert the client and/or depot (Fig. 5 ). In order to edit the inserted data it is possible to drag the clients and/or depots across the map area (allowing the corresponding update of its coordinates). Finally, the capacity of the fleet is inserted, thus fully characterizing the capacitated location-routing problem (CLRP). 4.2.2 Web map server integration The OpenGIS web map server (WMS) protocol defines an interface for web based mapping applications; it is based on a query syntax for posting a request for the desired layers and zoom window to the server, which returns a map as a standard picture. Our decision-support tool enables the user to obtain real online geographic information using WMS. Currently, the application supports the integration with the following servers (Fig. 6 ): • Demis®; • Google Maps®. When these options are enabled the user can zoom in on any point of the world map, thus obtaining real online geographic information on the selected area. All the options regarding the layers of both servers are fully integrated in the application. Demis® is an online server that complies with the OpenGIS WMS protocol. Google Maps® is an online server that provides the same satellite imagery than Google Earth®, although using a different projection. 4.2.3 Graphical representation Most of the graphical representation functionalities are concerned with the map and its interaction with the remaining objects (clients, depots and routes). In the map area there is a set of options that allow the introduction, editing and visualization of the inserted data. In order to ensure that the objects (clients and depots) and associated information are always visible, different icons were used according to the objects location on the map and the distance to the border (Fig. 7 ) [20]. There are different ways to define the map scale, and the user has the possibility to change the representation scale. The application allows the user to zoom in or out (by increasing or decreasing the representation scale) on a specific area of the map, in order to have a better view of the data in that area (Fig. 8 ). There is also the possibility to lock the relation between both representation scales (map and objects); moreover, combined with the “Move map” option, allows the user to easily adjust the map to the remaining objects (clients and depots). When working with the WMS the zoom option provides new imagery and geographic data regarding the selected area. It is also possible to visualize the map based on the clients demand. A circle where the radius represents the demand is drawn (clients with higher demand values correspond to circles with bigger radius). Through this option the user can easily identify clients with higher demand values, thus providing a useful view of the map (Fig. 9 ). Finally, regarding the routes, they are displayed when the user obtains or imports the solution. When one of these options is used, the application draws a line connecting the clients to the depots (or to other clients). A different colour is associated to each route, allowing a better identification of the solution proposed. Information regarding the used grouping method can be visualized in the status bar; there is also an indication about the solution: optimal or approximate. 4.2.4 Visualization of the solution In order to obtain the solution, the user can choose one of the following options: • Import the solution file; • Run the algorithm, choosing a specific grouping method; • Run the algorithm for all the grouping methods; • Run the algorithm step-by-step. By allowing to import a solution file, it becomes possible to easily interact with other applications. With this option it is also possible to get solutions from algorithms currently not included in this tool; the user only has to guarantee the solution file follows a specific structure. If the user runs the algorithm for all the grouping methods [6], the application displays a window where the user can visualize the different grouping methods. The total cost of the solution associated to each method, a bar indicating the percentage of improvement (as compared to the worst obtained result) as well as an image where the user can easily identify the best solutions found (Fig. 10 ). Afterwards, the user must choose the solution to visualize, either graphically on the map or numerically. In the former case, routes are displayed using different colours and the depots that will not be installed will have a new graphical representation, making it easier to understand the solution (Fig. 11 ). All the information regarding the solution can also be visualized numerically (Fig. 12 ). The user can see the total cost of the solution, the depots to be installed, and the data of the different routes (capacity, path and cost). Finally, it is also possible to run the algorithm step-by-step, which provides the application with a strong pedagogical component, allowing the user to have a detailed view of the development of the algorithm. This option enables the user to additionally handle the vehicle routing problem (VRP) and the capacitated location-allocation problem (CLAP) by running only a subset of the original heuristic (the first, second and third steps to obtain the VRP solution and the fourth step for the CLAP), as shown in Fig. 13 . 4.2.5 Data output Regarding the data output the following options are available: • Data export; • Save the data in a file; • Save the map image (containing all the information). In order to export data, there is the possibility of exporting the clients and the depots data to text format files, facilitating the integration with other software. There is also the possibility of saving the data in a file. Through this option the user can save all the data into a single file, with a specific format. Finally, there is the “Save Map” option, which allows the user to save the map he is currently visualizing to an image file. 4.2.6 Other characteristics Besides the previously mentioned options, there is a set of characteristics that have been added to the application in order to improve its usefulness. For instance: an option to change the language; a help file and a set of options regarding the map, the visualization scale and the algorithm used. The integration of other algorithms is made through the “Solver Options”, available in the menu. 5 Conclusion A decision-support tool that implements a capacitated location-routing problem (CLRP) with two levels (depots and customers) and a capacitated and homogeneous vehicle fleet was presented. It allows the exploration of the solution finding process in a way easily understandable by the user, and enables access to online geographic data through web map servers (WMS). A usable user interface was a great concern throughout the development of this tool, which was designed to allow decision-makers with a moderate computer literacy to be able to obtain good quality solutions without much learning effort. Although the best decision may not be the most cost efficient, this type of tool can help managers make a more scientifically supported decision, by providing the total estimated costs of a set of different solutions. From this point it is up to the manager to make the decision, taking into consideration the estimated cost, the service level or even the company strategy and motivation. A decision-support tool assists, but does not replace, the decision-maker. It does not try to provide the ‘answer’, nor does it impose a predefined sequence of analysis. It supports semi-structured decisions where parts of the analysis can be systematized by the tool, improving the decision-maker's insight and judgement. Although this decision-support tool has been developed for the CLRP, due to its modular architecture, it could also include, Network CLRP, LRP with paths and even more specific cases such as the CLAP and the VRP. Moreover, future developments could lead to further integration with other WMS or geographic information systems (GIS). References [1] M. Albareda-Sambola J.A. Díaz E. Fernández A compact model and tight bounds for a combined location-routing problem Computers & Operations Research 32 2005 407 428 [2] S. Alter A work system view of DSS in its fourth decade Decision Support Systems 38 2004 319 327 [3] D. Ambrosino M.G. Scutellà Distribution network design: new problems and related models European Journal of Operational Research 165 2005 610 624 [4] S.S. Barreto, 2003. [5] S.S. Barreto, Análise e Modelização de Problemas de Localização-Distribuição [Analysis and Modelization of Location-Routing Problems] (Ph.D. dissertation, University of Aveiro, 2004) (in Portuguese). [6] S. Barreto C. Ferreira J. Paixão B.S. Santos Using clustering analysis in a capacitated location-routing problem European Journal of Operational Research 179 2007 968 977 [7] J.H. Bookbinder Global Logistics Transportation Research E 41 2005 461 466 [8] I.M. Branco J.D. Coelho The Hamiltonian p-median problem European Journal of Operational Research 47 1990 86 95 [9] A. Bruns A. Klose An iterative heuristic for location-routing problems based on clustering Proceedings of the Second International Workshop on Distribution Logistics 1995 1 6 [10] C. Carlsson E. Turban DSS: directions for the next decade Decision Support Systems 33 2002 105 110 [11] Commission of the European Communities European transport policy for 2010: time to decide, COM (2001) 370, Brussels, White Paper 2001 [12] A. Dix J. Finlay G. Abowd R. Beale Human–Computer Interaction 3rd ed. 2004 Prentice Hall New Jersey [13] H.B. Eom S.M. Lee Decision support systems applications research: a bibliography (1971–1988) European Journal of Operational Research 46 3 1990 333 342 [14] S.B. Eom S.M. Lee C. Somarajan E.B. Kim Decision support systems applications — a bibliography (1988–1994) OR Insight 10 2 1997 18 32 [15] B.S. Everitt Cluster Analysis 3rd ed. 1993 Arnold London [16] G. Laporte Location-routing problems B.L. Golden A.A. Assad Vehicle Routing: Methods and Studies 1988 North-Holland Amsterdam 163 197 [17] C. Lin C. Kwok Multi-objective metaheuristics for a location-routing problem with multiple use of vehicles on real data and simulated data European Journal of Operational Research 175 2006 1833 1849 [18] J. Malczewski GIS and Multicriteria Decision Analysis 1999 John Wiley and Sons New York [19] J. Melechovský C. Prins C.R. Wolfler-Calvo A metaheuristic to solve a location-routing problem with non-linear costs Journal of Heuristics 11 2005 375 391 [20] A. Melo B.S. Santos C. Ferreira J.S. Pinto Software application for data visualization and interaction in a location routing problem Revista do Departamento de Electrónica e Telecomunicações da Universidade de Aveiro 2 4 1999 471 476 [21] H. Min Consolidation terminal location-allocation and consolidated routing problems Journal of Business Logistics 17 2 1996 259 288 [22] H. Min V. Jayaraman R. Srivastava Combined location-routing problems: a synthesis and future research directions European Journal of Operational Research 108 1998 1 15 [23] K.G. Murty P.A. Djang The U.S. army national guard’s mobile training simulators location and routing problem Operations Research 47 1999 175 182 [24] G. Nagy S. Salhi Nested heuristic methods for the location-routing problem Journal of the Operational Research Society 47 1996 1166 1174 [25] G. Nagy S. Salhi Location-routing: issues, models and methods European Journal of Operational Research 177 2007 649 672 [26] J. Perl M.S. Daskin A warehouse location-routing problem Transportation Research 19B 5 1985 381 396 [27] S. Salhi G.K. Rand The effect of ignoring routes when locating depots European Journal of Operational Research 39 1989 150 156 [28] J.P. Shim M. Warkentin J.F. Courtney D.J. Power R. Sharda C. Carlsson Past, present, and future of decision support technology Decision Support Systems 33 2002 111 126 [29] R. Srivastava W.C. Benton The location-routing problem: considerations in physical distribution system design Computers & Operations Research 17 5 1990 427 435 [30] D. Tuzun L.I. Burke A two-phase tabu search approach to the location routing problem European Journal of operational Research 116 1999 87 99 Rui Borges Lopes received his MSc degree in Operations Management from the University of Aveiro in 2005. He teaches at the Department of Economics, Management and Industrial Engineering of the University of Aveiro and is a researcher in the R&D unit CIO of the University of Lisbon. His current research interests lie primarily in Operations Research (location-routing models), Spatial Decision-Support Systems and Multiple Criteria Programming. Sérgio Barreto is an Associate Professor at the Accounting and Administration Institute of the University of Aveiro where he is a member of the Marketing and Data Analysis Research Center and teacher of the Mathematics group. He has a PhD in Industrial Management from the University of Aveiro. He is also a researcher of the CIO (Operations Research Center) of the University of Lisbon and author and co-author of several scientific papers presented at conferences and published in journals. His research area is related with combinatorial optimization with a special interest in location and routing problems. Carlos Ferreira received a MSc degree in Statistics and Operational Research from the University of Lisbon and a PhD in Mathematics from the University of Aveiro in 1998. He currently is an Associate Professor with the Department of Economics, Management and Industrial Engineering at the University of Aveiro, Portugal, where he is director of the Information Management MSc. His teaching and research interests are in Operations Research, Data Analysis and Information Management. Beatriz Sousa Santos received her PhD in Electrical Engineering in 1989 and is currently Associate Professor with the Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal. She lectures Human–Computer Interaction and Computer Graphics and her main research interests are in the areas of Data and Information Visualization. "
    },
    {
        "doc_title": "Usability Evaluation in Virtual Reality: A User Study Comparing Three Different Setups",
        "doc_scopus_id": "85119835829",
        "doc_doi": null,
        "doc_eid": "2-s2.0-85119835829",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Head-mounted-displays",
            "Low-costs",
            "Performance",
            "Usability evaluation",
            "User performance",
            "User study",
            "VR systems"
        ],
        "doc_abstract": "© The Eurographics Association 2008.We describe a user study comparing a low cost VR system using a Head-Mounted-Display (HMD) to a desktop and another setup where the image is projected on a screen. Eighteen participants played the same game in the three platforms. Results show that users generally did not like the setup using a screen and the best performances were obtained with the desktop configuration. This result could be due to the fact that most users were gamers used to the interaction through keyboard/mouse. Still, we noticed that user performance in the HMD setup was not dramatically worse and that users do not collide as often with walls.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Perceived quality assessment of polygonal meshes using observer studies: A new extended protocol",
        "doc_scopus_id": "41149133313",
        "doc_doi": "10.1117/12.766527",
        "doc_eid": "2-s2.0-41149133313",
        "doc_date": "2008-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Mesh quality",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "The complexity of a polygonal mesh is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, only a few observer studies are reported comparing the perceived quality of the simplified meshes, and it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting mesh, as perceived by the final users. Similar issues occur regarding other mesh processing methods such as smoothing. Mesh quality indices are the obvious less costly alternative to user studies, but it is also not clear how they relate to perceived quality, and which indices best describe the users behavior. This paper describes on going work concerning the evaluation of perceived quality of polygonal meshes using observer studies, while looking for a quality index which estimates user performance. In particular, given some results obtained in previous studies, a new experimental protocol was designed and a study involving 55 users was carried out, which allowed their validation, as well as further insight regarding mesh quality, as perceived by human observers. © 2008 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using task analysis to improve the requirements elicitation in health information system",
        "doc_scopus_id": "57649224764",
        "doc_doi": "10.1109/IEMBS.2007.4353127",
        "doc_eid": "2-s2.0-57649224764",
        "doc_date": "2007-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Clinical information",
            "Health information system",
            "Task analysis"
        ],
        "doc_abstract": "This paper describes the application of task analysis within the design process of a Web-based information system for managing clinical information in hemophilia care, in order to improve the requirements elicitation and, consequently, to validate the domain model obtained in a previous phase of the design process (system analysis). The use of task analysis in this case proved to be a practical and efficient way to improve the requirements engineering process by involving users in the design process. ©2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Preliminary usability evaluation of PolyMeCo: A visualization based tool for mesh analysis and comparison",
        "doc_scopus_id": "35048822035",
        "doc_doi": "10.1109/GMAI.2007.27",
        "doc_eid": "2-s2.0-35048822035",
        "doc_date": "2007-10-16",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Polygonal meshes",
            "Usability evaluation",
            "Visualization based tool"
        ],
        "doc_abstract": "An overall description of the methods used and the results obtained in the on-going evaluation of PolyMeCo - a mesh analysis and comparison tool - is presented. We are trying to evaluate some aspects of both the user interface and the visualization techniques implemented. Heuristic evaluation, observation and querying techniques were used and produced encouraging preliminary results, which provided new ideas, as well as information, that will inform the development of a more usable version of PolyMeCo, including new functionality. © 2007 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Usability in virtual and augmented environments: A qualitative and quantitative study",
        "doc_scopus_id": "34548209132",
        "doc_doi": "10.1117/12.703878",
        "doc_eid": "2-s2.0-34548209132",
        "doc_date": "2007-08-31",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Augmented Reality systems",
            "Head-Mounted Display",
            "HMD visualization",
            "Usability evaluation"
        ],
        "doc_abstract": "Virtual and Augmented Reality are developing rapidly: there is a multitude of environments and experiments in several laboratories using from simple HMD (Head-Mounted Display) visualization to more complex and expensive 6-wall projection CAVEs, and other systems. Still, there is not yet a clear emerging technology in this area, nor commercial applications based on such a technology are used in large scale. In addition to the fact that this is a relatively recent technology, there is little work to validate the utility and usability of Virtual and Augmented Reality environments when compared with the traditional desktop set-up. However, usability evaluation is crucial in order to design better systems that respond to the users' needs, as well as for identifying applications that might really gain from the use of such technologies. This paper presents a preliminary usability evaluation of a low-cost Virtual and Augmented Reality environment under development at the University of Aveiro, Portugal. The objective is to assess the difference between a traditional desktop set-up and a Virtual/Augmented Reality system based on a stereo HMD. Two different studies were performed: the first one was qualitative and some feedback was obtained from domain experts who used an Augmented Reality set-up as well as a desktop in different data visualization scenarios. The second study consisted in a controlled experiment meant to compare users' performances in a gaming scenario in a Virtual Reality environment and a desktop. The overall conclusion is that these technologies still have to overcome some hardware problems. However, for short periods of time and specific applications, Virtual and Augmented Reality seems to be a valid alternative since HMD interaction is intuitive and natural. © 2007 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Using clustering analysis in a capacitated location-routing problem",
        "doc_scopus_id": "33845918053",
        "doc_doi": "10.1016/j.ejor.2005.06.074",
        "doc_eid": "2-s2.0-33845918053",
        "doc_date": "2007-06-16",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            },
            {
                "area_name": "Modeling and Simulation",
                "area_abbreviation": "MATH",
                "area_code": "2611"
            },
            {
                "area_name": "Management Science and Operations Research",
                "area_abbreviation": "DECI",
                "area_code": "1803"
            },
            {
                "area_name": "Information Systems and Management",
                "area_abbreviation": "DECI",
                "area_code": "1802"
            }
        ],
        "doc_keywords": [
            "Cluster analysis",
            "Combinatorial optimization",
            "Location routing"
        ],
        "doc_abstract": "The location routing problem (LRP) appears as a combination of two difficult problems: the facility location problem (FLP) and the vehicle routing problem (VRP). In this work, we consider a discrete LRP with two levels: a set of potential capacitated distribution centres (DC) and a set of ordered customers. In our problem we intend to determine the set of installed DCs as well as the distribution routes (starting and ending at the DC). The problem is also constrained with capacities on the vehicles. Moreover, there is a homogeneous fleet of vehicles, carrying a single product and each customer is visited just once. As an objective we intend to minimize the routing and location costs. Several authors have integrated cluster analysis procedures in heuristics for LRPs. As a contribution to this direction, in this work several hierarchical and non-hierarchical clustering techniques (with several proximity functions) are integrated in a sequential heuristic algorithm for the above mentioned LRP model. All the versions obtained using different grouping procedures were tested on a large number of instances (adapted from data in the literature) and the results were compared so as to obtain some guidelines concerning the choice of a suitable clustering technique. © 2006.",
        "available": true,
        "clean_text": "serial JL 271700 291210 291692 291715 291813 291814 291817 291871 31 European Journal of Operational Research EUROPEANJOURNALOPERATIONALRESEARCH 2006-05-03 2011-01-13 2010-11-17T20:28:36 S0377-2217(06)00078-6 S0377221706000786 10.1016/j.ejor.2005.06.074 S300 S300.1 FULL-TEXT 2015-05-14T07:48:30.363566-04:00 0 0 20070616 2007 2006-05-03T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdatestart pubdatetxt pubyr sectiontitle sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body mmlmath acknowledge affil articletitle auth authfirstini authfull authkeywords authlast primabst ref alllist content subj ssids 0377-2217 03772217 179 179 3 3 Volume 179, Issue 3 27 968 977 968 977 20070616 16 June 2007 2007-06-16 2007 Feature Cluster: Advances in Location Analysis Guest Editors: M. Dimopoulou and I. Giannikos article fla Copyright © 2006 Published by Elsevier B.V. USINGCLUSTERINGANALYSISINACAPACITATEDLOCATIONROUTINGPROBLEM BARRETO S 1 Introduction 2 A capacitated location-routing problem (CLRP) 3 A cluster analysis approach 4 Proximity measures among groups 5 A cluster analysis based heuristic 5.1 One phase hierarchical method (V1) 5.2 Two phase hierarchical method (V2) 5.2.1 Relative proximity measure 5.3 Direct assignment non-hierarchical method (V3) 5.4 Sequential assignment non-hierarchical method (V4) 6 Computational tests 7 Conclusions Acknowledgement References ANDERBERG 1973 M CLUSTERANALYSISFORAPPLICATIONS BALAKRISHNAN 1987 35 61 A BAYNE 1980 51 62 C BODIN 1975 11 29 L BRAMEL 1997 J LOGICLOGISTICS BRANCO 1990 86 95 I CHRISTOFIDES 1969 309 318 N CLARK 1964 568 581 G CULLEN 1981 125 143 F DANTZIG 1959 80 91 G DASKIN 1995 M NETWORKDISCRETELOCATIONMODELSALGORITHMSAPPLICATIONS EDELBROCK 1980 299 318 C EVERITT 1993 B CLUSTERANALYSIS FUKUNAGA 1978 176 181 K GASKELL 1967 281 295 T GOLDEN 1980 475 496 R GOWER 1985 397 405 J GOWER 1969 54 64 J JAIN 1988 A ALGORITHMSFORCLUSTERINGDATA JOHNSON 1967 241 254 S KAUFMAN 1990 L FINDINGGOUPSINDATAINTRODUCTIONCLUSTERANALYSIS KLOSE 1995 A USINGCLUSTERINGMETHODSINPROBLEMSCOMBINEDLOCATIONROUTINGOPERATIONSRESEARCHPROCEEDINGS KUEHN 1963 643 666 A MADSEN 1983 295 301 O MARANZANA 1963 129 135 F MILLIGAN 1985 97 109 G MIN 1989 377 386 H MIN 1996 235 263 H MIN 1992 259 288 H ROMESBURG 1984 H CLUSTERANALYSISFORRESEARCHERS SRIVASTAVA 1993 497 506 R SRIVASTAVA 1990 427 435 R BARRETOX2007X968 BARRETOX2007X968X977 BARRETOX2007X968XS BARRETOX2007X968X977XS item S0377-2217(06)00078-6 S0377221706000786 10.1016/j.ejor.2005.06.074 271700 2010-12-25T07:24:51.810909-05:00 2007-06-16 true 444383 MAIN 10 59864 849 656 IMAGE-WEB-PDF 1 si5 1430 41 219 si4 644 40 98 si3 999 22 184 si2 2040 41 360 si1 1032 30 227 fx1 13779 59 338 fx1 1794 22 125 gr7 22969 158 326 gr7 3432 61 125 gr4 32774 288 437 gr4 2597 82 125 gr6 24152 177 328 gr6 3098 67 125 gr5 27320 201 333 gr5 3281 75 125 gr1 19709 266 317 gr1 1994 93 111 gr2 45455 474 338 gr2 2145 93 66 gr3a 21113 275 265 gr3a 1976 93 90 gr3b 25224 275 266 gr3b 2285 93 90 gr3c 24586 274 265 gr3c 2222 93 90 gr3d 26568 274 265 gr3d 2250 93 90 EOR 7357 S0377-2217(06)00078-6 10.1016/j.ejor.2005.06.074 Fig. 1 Location of the 150 biggest European cities, where the squares represent potential DCs. Fig. 2 Graphical and analytical representation of some proximity measures. Fig. 3a Step 1. Capacity limited group construction. Fig. 3b Step 2. Route design in each group. Cost=526. Fig. 3c Step 3. Route improvement. Cost=463. Fig. 3d Step 4. DC location and route assignment to the open DCs. Cost=614. Fig. 4 Heuristic versions and proximity measures. Fig. 5 Success rates obtained for the four versions of the heuristic (V1, V2, V3, V4) and the six proximity measures (SL, CL, GA, CT, WA, SA). Fig. 6 Average results after steps 2, 3 and 4. Fig. 7 CLRP average results (after step 4). Table 1 Instances lower and upper bounds CLRP instance Vehicles capacity LB UB Gap 1 Gaskell67—21×5 6000 424.9∗ 435.9 2.59 2 Gaskell67—22×5 4500 585.1∗ 591.5 1.09 3 Gaskell67—29×5 4500 512.1∗ 512.1∗ 0.00 4 Gaskell67—32×5 8000 556.5 571.7 2.73 5 Gaskell67—32×5 11,000 504.3∗ 511.4 1.41 6 Gaskell67—36×5 250 460.4∗ 470.7 2.24 7 Christofides69—50×5 160 549.4 582.7 6.06 8 Christofides69—75×10 140 744.7 886.3 19.01 9 Christofides69—100×10 200 788.6 889.4 12.78 10 Perl83—12×2 140 204.0∗ 204.0∗ 0.00 11 Perl83—55×15 120 1074.8 1136.2 5.71 12 Perl83—85×7 160 1568.1 1656.9 5.66 13 Perl83—318×4 25,000 – 580,680.2 – 14 Perl83—318×4 8000 – 747,619.0 – 15 Min92—27×5 2500 3062.0∗ 3062.0∗ 0.00 16 Min92—134×8 850 – 6238.0 – 17 Daskin95—88×8 9,000,000 356.4 384.9 8.00 18 Daskin95—150×10 8,000,000 43,938.6 46,642.7 6.15 19 Or76—117×14 150 12,048.4 12,474.2 3.53 Average gap 4.81 Median 3.13 Using clustering analysis in a capacitated location-routing problem Sérgio Barreto a e ⁎ Carlos Ferreira b e José Paixão c e Beatriz Sousa Santos d a Higher Institute of Accounting and Administration/ISCA, University of Aveiro, Aveiro, Portugal b Department of Economics, Management and Industrial Engineering, University of Aveiro, Aveiro, Portugal c Department of Statistics and Operations Research, University of Lisbon, Lisbon, Portugal d Department of Electronics and Telecommunications/IEETA, University of Aveiro, Aveiro, Portugal e Operational Research Centre, University of Lisbon, Lisbon, Portugal ⁎ Corresponding author. Tel.: +351 234 380 110; fax: +351 234 380 111. The location routing problem (LRP) appears as a combination of two difficult problems: the facility location problem (FLP) and the vehicle routing problem (VRP). In this work, we consider a discrete LRP with two levels: a set of potential capacitated distribution centres (DC) and a set of ordered customers. In our problem we intend to determine the set of installed DCs as well as the distribution routes (starting and ending at the DC). The problem is also constrained with capacities on the vehicles. Moreover, there is a homogeneous fleet of vehicles, carrying a single product and each customer is visited just once. As an objective we intend to minimize the routing and location costs. Several authors have integrated cluster analysis procedures in heuristics for LRPs. As a contribution to this direction, in this work several hierarchical and non-hierarchical clustering techniques (with several proximity functions) are integrated in a sequential heuristic algorithm for the above mentioned LRP model. All the versions obtained using different grouping procedures were tested on a large number of instances (adapted from data in the literature) and the results were compared so as to obtain some guidelines concerning the choice of a suitable clustering technique. Keywords Combinatorial optimization Distribution Heuristics Cluster analysis Location-routing 1 Introduction In an ever more demanding society, having customers less and less willing to wait for the products they want to acquire, decisions concerning the location of distribution centres (DC) and tracing of distribution routes are a central problem, having implications on the complete supply chain (Bramel and Simchi-Levi, 1997). Nowadays, even small and medium enterprises should be aware that their future success may depend on the location–distribution decisions and recognise the need for flexible and efficient, as well as reliable, decision methods (White Paper, 2001). In the last four decades (Maranzana, 1963), the investigation on modelling and resolution of location-routing problems (LRP) has advanced and produced a large body of literature (Barreto et al., 2003b), allowing a complete and detailed view of this problem and its characteristics. However, a deficit in theoretical investigation at the simplest LRP level still exists. This deficit hinders a better understanding of its properties and the development of new approaches; furthermore, it holds back the creation of the solid foundations needed to support complex applications. As an attempt to contribute to this investigation, we present in this paper a study on a location routing problem, common in many organizations, which has two levels (customers and distribution centres) and vehicles with a limited capacity; we will call it a capacitated location-routing problem (CLRP). 2 A capacitated location-routing problem (CLRP) Let a set of customers and potential distribution centres (DCs) be represented by points on the plane. Each customer has a certain demand (units of load); the location (installation) cost of each DC is known, as well as the unitary cost of distribution (function of covered distance). The vehicles (routes) and the potential DCs have a certain capacity (units of load). The purpose of this CLRP is, then, to choose the DCs that must be opened (installed) and to draw the routes from these DCs to the customers, having as an objective the minimization of the total cost (location and distribution costs). Solving exactly a CLRP is a difficult task, since this type of problem is NP-complete (Srivastava, 1986). Thus, one way to “solve efficiently” large problems is to look for heuristics, which have some advantages such as: (i) getting “good solutions” in acceptable time; (ii) producing several “good solutions” allowing the user to choose the most suitable according to the scenario; (iii) being easy to understand, modify and implement, they allow to deal with larger problems. 3 A cluster analysis approach Cluster analysis (Anderberg, 1973) studies the division of entities (as objects or individuals) in groups based in one or several of their characteristics. An important issue is the notion of group; according to Jain and Dubes (1988): “Cluster may be described as connected regions of a multi-dimensional space containing a relatively high density of points, separated from other such regions by a region containing a relatively low density of points”. This definition of group is an excellent reason to use cluster analysis in the resolution of LRPs. Recognizing groups of customers can be a good start to obtain good LRP solutions. Let us consider, as an example, Fig. 1 showing the location of the 150 biggest European cities (Daskin, 1995), where the squares represent potential DCs. Observing this figure, it seems natural that a CLRP related to these cities will tend to construct routes encompassing high density regions (population agglomerations). The identification by visual inspection of some groups of cities that can form good bases for the distribution routes (for example, the Canaries Islands, the Iberian Peninsula, the British Islands, the north of the Europe or Italy and Greece) is not difficult. However, what is relatively easy to visualize is more difficult to carry out. Nevertheless, cluster analysis provides a vast set of grouping methodologies that can be used in heuristic approaches to the CLRP. The potential of cluster analysis for the resolution of LRPs (or problems directly related, as the facility location problem (FLP) and vehicle routing problem (VRP)) has also been recognized by other authors. Dantzig and Ramser (1959) were of the first ones to mention the identification of groups of points in the multiple travelling salesman problem (MTSP), stating that “One would look for “clusters of points” and determine by trial and error the order in which they should be traversed, taking care that no loop crosses itself”. Concerning location problems, Kuehn and Hamburger (1963), also recognized the benefits of grouping (nearby customers) when they say that “… in many cases a priori judgements can be made that customers in certain geographical regions will not be serviced from potential warehouses in other regions (…) customers can frequently be aggregated into concentrations of demand (for example, metropolitan chain grocery and wholesaler warehouses) because of geographical proximity”. This is probably one of the first explicit references on the interest of grouping customers. Fukunaga and Short (1978) used grouping algorithms in location problems as well. The well known Clarke–Wright algorithm (Clark and Wright, 1964) uses grouping, with excellent results, to implement an original saving procedure to determine the proximity matrix. Bodin (1975) mentions the construction of groups as a technique to approach Routing and Scheduling Vehicles Problems. According to Madsen (1983), also Bednar and Strohmeier solve VRPs supported by grouping procedures. Cullen et al. (1981) make use of simple grouping procedures in the development of a heuristic for VRP. Branco and Coelho (1990) also use grouping processes in algorithms for a specific LRP (Hamiltonian p-median problem). Min (1989) explicitly includes cluster analysis in algorithms for VRP with distribution and collection. He uses the group average proximity measure which, according to Romesburg (1984), has advantages over the single linkage or complete linkage measures. In another paper, Min et al. (1992) use the ward measure of proximity due to its tendency to form groups of equal dimension and the good results reported in some studies. Srivastava (1993) also considers a LRP heuristic based on grouping procedures, using the minimum spanning tree to determine a set of groups, which he refines later by means of a 1-optimal method. Bruns and Klose (1995) and Klose (1995) make explicit reference to the integration of the cluster analysis in a heuristic procedure for the LRP. They employ hierarchical grouping techniques with single and complete linkage, group average and ward proximity measures. Min (1996) considers as well a sequential method for a LRP with capacity that starts by grouping customers through a hierarchical method and uses the ward proximity measure. Actually, although one can find in the literature a significant number of attempts to integrate grouping techniques in algorithms for the LRP, the same is not true when looking for comparative studies among the several grouping techniques in order to evaluate their real capacities. 4 Proximity measures among groups Cluster analysis considers different grouping methods accordingly to the type of variables (qualitative, quantitative, binary or mixed). In this paper we have quantitative variables (co-ordinates of customers and DCs on the plane), and the classification will use hierarchical and non-hierarchical methods (or partition methods). Several measures have been proposed to determine the proximity between points on the plane (Anderberg, 1973; Gower, 1985), however the most common for quantitative data is the Euclidean metric that determines the proximity between the points I =(x i , y i ) and J =(x j , y j ) as d ( I , J ) = ( x i - x j ) 2 + ( y i - y j ) 2 . Based on this concept of proximity between two elements, some measures of proximity among groups have been proposed: single linkage (nearest neighbour), complete linkage (farthest neighbour), group average, centroid, ward and saving. With the exception of the saving measure, proposed in this study, these are some of the most popular in cluster analysis literature (Anderberg, 1973; Jain and Dubes, 1988; Kaufman and Rousseeum, 1990; Everitt, 1993). Fig. 2 shows a graphical and analytical representation of these proximity measures. According to Jain and Dubes (1988), Jardine and Sibson considered that the single linkage is the only measure satisfying all the mathematical criteria they had defined. However, they concluded that the use of this measure usually produces worse results, when compared to the other measures. An important element on the centroid and ward proximity measures is the centroid (gravity centre) of each group. In our case for two groups, A and B, we will have m A and m B defined as m A = ∑ I ∈ A x i | A | , ∑ I ∈ A y i | A | , m B = ∑ J ∈ B x j | B | , ∑ J ∈ B y j | B | . The ward proximity measure (Kaufman and Rousseeum, 1990) also includes the sum square error of a group quantified as the sum of the square deviation of the elements of the group to the centroid, SEQ ( A ) = ∑ I ∈ A [ d ( I , m A ) ] 2 . While this measure is widely used, the mean square error may prove inadequate to investigate the data structure, due to its tendency to form groups of equal size (Milligan and Schilling, 1985; Min, 1996). On the other hand, this can be an advantage to solve the LRP. Golden and Meehl (1980) claim that group average, complete linkage and ward produce better results than single linkage and centroid for a specific data set. Bayne et al. (1980) affirm that ward and complete linkage are preferable to the centroid and group average measures. Some authors have attempted to demonstrate the superiority of one or another proximity measure. Despite these efforts, we still do not have a clear idea on the general or specific potentialities of each one. The lack of agreement among the several studies reinforces the idea that there is not a measure which is adequate for all the applications and cases. In each case, the measure must be carefully chosen and, probably, only after tests with several measures, definitive conclusions can be drawn (Edelbrock and McLaughlin, 1980). 5 A cluster analysis based heuristic Heuristic methods for resolution of LRPs can be classified as sequential and iterative. The former solve sequentially the location and the vehicle routing problems. The latter solve the same problems iteratively, while it is possible to improve the solution. According to Min (1996) and Balakrishnan et al. (1987), for limited capacity of vehicles and significant fixed cost of distribution centres, the sequential methods are preferable from a computational point of view. Moreover, Srivastava and Benton (1990) conclude that the error associated to the obtained solutions is perfectly acceptable. As a consequence, we propose for the CLRP a sequential heuristic of the type distribution-first, location-second, which we present below. A sequential heuristic for the CLRP Input: Co-ordinates of N ={1,2,…, n} customers on the plane with demand e i : i ∈ N. Co-ordinates of P ={n +1, n +2,…, n + p} potential DCs with capacity u k and location cost f k : k ∈ P. w =vehicle capacity. Output: Vehicles routes based in the DCs. Step 1. Construct groups of customers with a capacity limit. Step 2. Determine the distribution route in each customer group. Step 3. Improve the routes. Step 4. Locate the DCs and assign the routes to them. Fig. 3 shows the results of each step of the heuristic applied to an instance of a CLRP with 50 customers and 5 potential DC (adapted from Christofides and Eilon (1969)). In step 2, whenever the group has 40 customers or less, the TSP routes are determined by an exact algorithm which solves the relaxation of the sub-cycles constraints with more than three customers. These constraints are introduced later if they are violated in this step. If the route integrates more than 40 customers, a two stages heuristic procedure is used. In the first stage, a feasible solution is obtained using a choice criterion of the farthest type and an insertion criterion of the saving type. In the second stage, the solution is improved through a 3-optimal local search procedure. In the improvement of the routes (step 3) a 3-optimal local search procedure is used, as the one proposed by Branco and Coelho (1990) for the Hamiltonian p-median problem. After step 3, each route collapses into one customer with a saving type DC assignment cost. Then, the single source capacitated location problem is solved, leading to a feasible solution of the CLRP. To implement step 1, four grouping methods are considered (two hierarchical and two non-hierarchical): 1. One-phase hierarchical method. 2. Two-phase hierarchical method. 3. Direct-assignment, non-hierarchical method. 4. Sequential-assignment, non-hierarchical method. For each of these methods, the six proximity measures presented in Fig. 2 are evaluated. The integration of these methods in step 1 leads to four versions of the proposed heuristic, V1, V2, V3 and V4 (Fig. 4 ). 5.1 One phase hierarchical method (V1) This is an agglomerative hierarchical method that performs an iterative merging of the nearest groups. There are several ways to implement agglomerative hierarchical methods, the most common ones being the spanning tree and the Johnson methods (Johnson, 1967). The former considers the formation of groups from the minimum spanning tree of a graph (Gower and Ross, 1969) having the disadvantage of supporting only the single linkage and complete linkage proximity measures. The later needs to store the triangular matrix of proximity between the groups; however it allows using all proximity measures showed in Fig. 2 and a similar algorithmic construction for the hierarchical and non-hierarchical cases. Beginning with groups consisting of only one customer, the hierarchical algorithm leads to the formation of one single group. In CLRPs the capacity limit avoids the consecutive joining of groups, acting as a natural stopping criterion on their final number. However, the construction of groups with limited capacity leads to a difficulty to avoid drawback related to the final part of the grouping process: the groups reaching their capacity limits prevent the merging of the near groups since this will exceed their capacity. For this reason the merging of far away groups often occurs (as it can be observed in Figs. 3a and 3b) producing the biased effect (Klose, 1995). 5.2 Two phase hierarchical method (V2) The two phase hierarchical method starts by applying the hierarchical method without capacity constraints, freely constructing a number of groups, thus preventing the undesirable biased effect. Knowing the demand of the customers (e i ) and the maximum vehicle capacity (w), the minimum number of vehicles (r) is determined as r = ∑ i ∈ N e i w . Next, the hierarchical grouping method is applied until there are only r groups. The lack of capacity constraints probably leads to the formation of groups that violate these constraints. In this case, the second phase is performed using a procedure that allows transferring customers from the groups exceeding capacity limits to other groups that can receive them without exceeding their capacity. Customer transfer is based on a relative proximity measure defined as follows. 5.2.1 Relative proximity measure Let a set G ={G 1, G 2,…, G r } include r groups on the plane. Let i ∈ G l ∈ G be a customer and Prox(i, G j ) be the proximity between customer i and group G j ∈ G. The proximity coefficient of customer i is defined as ρ prox ( i ) = min G j ∈ G ⧹ G l Prox ( i , G j ) Prox ( i , G l ) . This proximity coefficient is a relative measure, independent of the customer space distribution, which represents the degree of proximity to the next group. The numerator of the expression represents the external proximity and the denominator the internal proximity. In this second phase, customers having the least proximity coefficient are transferred, as long as the receiving group does not exceed its capacity. If, however, no customers can be transferred (and the violation of capacity still exists), the number of vehicles (r) is increased by one unit and the procedure restarts. 5.3 Direct assignment non-hierarchical method (V3) While the hierarchical methods begin with a set of groups with one element and, through a nested process, converge to one group, the non-hierarchical methods are devised to construct r groups, where r is known a priori or is determined as part of the method. In the direct assignment non-hierarchical method the minimum number of groups (r) is determined as in the two phase hierarchical method. Then, r customer sources (who will serve of first customer in each group) are established. To prevent the biased effect, the customer sources must be located on the boundary; therefore, they are chosen using a farthest neighbour proximity measure. The remaining customers are directly assigned to the group whose vertex source is next. 5.4 Sequential assignment non-hierarchical method (V4) In the previous method, capturing new customers to the group depends entirely on the customer source; the remaining elements of the group do not contribute for this gravitational action. In the sequential assignment non-hierarchical algorithm, the responsibility of capturing not yet assigned customers is shared by the entire group. That is why it is necessary to calculate, in all iterations, the proximity between the free customers and the modified groups. Customer sources are determined as in the previous method. 6 Computational tests To evaluate the four versions of the heuristic (V1, V2, V3, V4) and the six proximity measures (SL, CL, GA, CT, WA, SA) computational tests were carried out on 19 CLRP instances (see Table 1 ) obtained from the literature (Or, 1976; Perl, 1983) or adapted from data related with VRP (Gaskell, 1967; Christofides and Eilon, 1969; Min et al., 1992; Daskin, 1995). Data relative to the used instances are available in Barreto (2003) and the complete tables of results are available in Barreto (2004). To evaluate the performance of each version of the heuristic, it is essential to define “good performance” and decide which measures should be used. Naturally, a good heuristic produces good results (in this case low cost CLRP solutions). However, a heuristic may provide a very good solution for a certain instance and have a poor performance in others. In this case we cannot say that it’s a “good” heuristic. Thus, the evaluation of a heuristic must have into account its capacity to generate frequently “good” solutions. In short, a good heuristic must generate, for most of the instances, “good” solutions; still, it may not be able to find the best solution. Three types of success rates were considered: (i) number of generated solutions equal to the best known solutions (BKS); (ii) number of solutions within a 2% tolerance from the BKS; (iii) number of solutions within a 5% tolerance from the BKS. Fig. 5 shows the success rates for all the versions using all proximity measures. V2 heuristic, using the complete linkage proximity measure, yielded the best success rates. With a tolerance of 2%, the best results were obtained also by V2, but now for the group average proximity measure. V1 and V3 produced more solutions within a 5% tolerance. V3 version using SL, CL, GA, CT and SA proximity measures produced equivalent success rates. A standardization of the data (customers and DC) for the square [0,500]2 was performed in order to get comparable results for all the instances used as well as to allow the use of some statistical measures. Fig. 6 shows the average results for all the versions and proximity measures: CAR represents the average cost after a routing process (after step 2), CAI the average cost after the improvement step (step 3) and CAL the average cost after the location procedure (after step 4) or the average CLRP cost. V1 and V3 produced the best average results. V2 and V4 produced the worst results. The average standard deviation in each version is 108.2, 217.0, 142.6 and 221.7, respectively, confirming the good performance of V1. The route improvement procedure allowed a decrease of 16% in the route costs and, as shown in Fig. 6, had a significant impact in the cases of (V2,SL), (V2,SA), (V3,WA) and (V4,CL), attenuating the consequences of a poor initialization. Moreover, in Barreto et al. (2003a) we have shown that CLRP solutions obtained without the route improvement step are 25% worst. Furthermore, this step has an important role in the elimination of the biased effect as shown in Figs. 3a, 3b and 3c. Fig. 7 shows the CLRP average results and confirms the good performance of V1 version. The group average (GA) proximity measure produces the most balanced results. Generally, the analysis of the success rates and average results allows ordering the versions by decreasing performance: V1, V3, V2 and V4. Concerning the proximity measures, the best performance was obtained by GA followed by CT, SA, CL, SL and WA measures. Table 1 shows, for the 19 instances of the LRPC, a lower bound (LB) and an upper bound (UB) of the optimal solution cost. The running time was less than one second, except for instances 11 (202seconds maximum), 13 and 14 (115seconds). The CLRP instance column contains information about the author, the publication year and the number of customers and potential DCs. The lower bound was obtained with a relaxed 2-index integer linear programming formulation (Barreto, 2004) and the upper bound is the best known solution (BKS) obtained using the heuristic. The values followed by an asterisk correspond to the optimal solution. It was not possible to get LBs for instances 13, 14 and 16. For all the instances, the Gap falls between a minimum of 0% and a maximum of 19.01% with an average of 4.81% and a median of 3.13%. 7 Conclusions Due to the complexity of the capacitated location-routing problem (CLRP), the heuristic approach is a promising way to find good solutions for medium and large problems. In this paper, a cluster analysis based sequential heuristic that uses simple procedures was presented. Moreover, four grouping techniques (hierarchical and non-hierarchical) and six proximity measures (single linkage, complete linkage, group average, centroid, ward and saving) were used to obtain several versions of the heuristic. Computational tests on 19 CLRP instances adapted from the literature were performed in order to compare their performance. The obtained results seem to indicate that version V1 had a slightly better performance than the other versions; version V3 was in second place followed by version V2, while version V4 had the worst performance. Concerning proximity measures, the group average measure has produced the most balanced results, followed by the measures centroid, saving, complete linkage, single linkage and ward. However, all the measures obtained good results for some instances; thus, it seems an advisable strategy to use several versions of the proposed heuristic and several proximity measures and then choose the best solution. In absolute terms, the average Gap was 4.81% with a median of 3.13% and, in some instances, optimality was reached. Several authors used clustering techniques in algorithms to solve the CLRP but never justified its use nor evaluated its performance. In this work the good performance of the clustering procedures is demonstrated; the average gap is narrow (less than 5%) and the result improves when the median (≈3%) is considered, attenuating the previous influence of the two outliers (19.01 and 12.78) which are due to the LB solution and not to (UB) heuristic solution. Despite these encouraging results, there are yet many opportunities ahead. The work is in progress, and eventually improvements are expected. Not all the clustering methods neither the proximity measures were tested and this is an open opportunity. In this process the construction of groups (cluster) with limited capacity is a crucial moment and it is necessary to improve the methods to perform it. For instance, further improvement could be obtained in the sequential heuristic for the CLRP using the following additional steps: Step 5. Determine the solution of the TSP in each route (including the DC). Step 6. Apply a 1-optimal procedure to the customers, subjected to the capacity constraints. Return to step 5 if any route was changed. Moreover, in the non-hierarchical case, another possibility to obtain better solutions is to run the same version several times using different customer sources. All the above ideas attempt to improve the solution concerning cost; nevertheless, the choice of a final solution does not solely depend on its cost. Other objectives and aspirations of managers can also be relevant. In this context, the possibility to generate several “good” solutions in an acceptable time can be very important in order to support decisions. This study confirms the potentialities of using clustering techniques in the CLRP approach. The grouping procedures are very fast and allow obtaining good alternative solutions corresponding to different configurations. This is a good example of an investigation opportunity, with promising results, using two distinct scientific areas, cluster analysis and operational research. Acknowledgement This research was partially financially supported by the European Social Fund. References Anderberg, 1973 M.R. Anderberg Cluster Analysis for Applications 1973 Academic Press New York Balakrishnan et al., 1987 A. Balakrishnan J.E. Ward R.T. Wong Integrated facility location and vehicle routing models: Recent work and future prospects American Journal of Mathematical Sciences 7 1&2 1987 35 61 Barreto, 2003 Barreto, S.S., 2003. Available from: Barreto, 2004 Barreto, S.S., 2004. Análise e modelização de problemas de localização-distribuição [Analysis and modelling of location-routing problems]. PhD Dissertation, University of Aveiro, Aveiro, Portugal (in Portuguese). Barreto et al., 2003a Barreto, S.S., Ferreira, C.M., Paixão, J.M., 2003a. Using clustering analysis in a capacitated location-routing problem. Communication presented at XIV Meeting of the European Working Group on Locational Analysis, September 11–13, Corfu, Greece. Barreto et al., 2003b Barreto, S.S., Ferreira, C.M., Paixão, J.M., 2003b. Problemas de localização-distribuição: uma revisão bibliográfica [Location-routing problems: A bibliographical review]. In: Proceedings of the VI Congreso Galego de Estatı´stica e Investigación de Operacións, November 5–7, University of Vigo, Vigo, Spain, pp. 93–100 (in Portuguese). Bayne et al., 1980 C.K. Bayne J.J. Beauchamp C.L. Begovich V.E. Kane Monte carlo comparisons of selected clustering procedures Pattern Recognition 12 1980 51 62 Bodin, 1975 L.D. Bodin A taxonomic structure for vehicle routing and scheduling problems Computers and Urban Society 1 1975 11 29 Bramel and Simchi-Levi, 1997 J. Bramel D. Simchi-Levi The Logic of Logistics 1997 Springer-Verlag New York Branco and Coelho, 1990 I.M. Branco J.D. Coelho The hamiltonian p-median problem European Journal of Operational Research 47 1990 86 95 Bruns and Klose, 1995 Bruns, A., Klose, A., 1995. An iterative heuristic for location-routing problems based on clustering. In: Proceedings of the Second International Workshop on Distribution Logistics, The Netherlands, pp. 1–6. Christofides and Eilon, 1969 N. Christofides S. Eilon An algorithm for the vehicle-dispatching problem Operational Research Quarterly 20 3 1969 309 318 Clark and Wright, 1964 G. Clark J.W. Wright Scheduling of vehicles from a central depot to a number of delivery points Operations Research 14 1964 568 581 Cullen et al., 1981 F.H. Cullen J.J. Jarvis H.D. Ratliff Set partitioning based heuristics for interactive routing Networks 11 1981 125 143 Dantzig and Ramser, 1959 G.B. Dantzig J.H. Ramser The truck dispatching problem Management Science 6 1 1959 80 91 Daskin, 1995 M.S. Daskin Network and Discrete Location: Models, Algorithms and Applications 1995 John Wiley & Sons, Inc. New York Edelbrock and McLaughlin, 1980 C. Edelbrock B. McLaughlin Hierarchical cluster analysis of intraclass correlations: A mixture model study Multivariate Behavioral Research 15 1980 299 318 Everitt, 1993 B.S. Everitt Cluster Analysis 1993 Arnold London Fukunaga and Short, 1978 K. Fukunaga R.D. Short Generalized clustering for problem localization IEEE Transactions on Computers C 27 2 1978 176 181 Gaskell, 1967 T.J. Gaskell Bases for vehicle fleet scheduling Operational Research Quarterly 18 3 1967 281 295 Golden and Meehl, 1980 R.R. Golden P.E. Meehl Detection of biological sex – an empirical test of cluster methods Multivariate Behavioural Research 15 1980 475 496 Gower, 1985 J.C. Gower Measures of similarity, dissimilarity and distance Encyclopedia of Statistical Sciences 5 1985 397 405 Gower and Ross, 1969 J.C. Gower G.J.S. Ross Minimum spanning trees and single linkage cluster analysis Applied Statistics 18 1969 54 64 Jain and Dubes, 1988 A.K. Jain R.C. Dubes Algorithms for Clustering Data 1988 Prentice Hall Englewood Clifs, New Jersey Johnson, 1967 S.C. Johnson Hierarchical clustering schemes Psychometrika 32 3 1967 241 254 Kaufman and Rousseeum, 1990 L. Kaufman P.J. Rousseeum Finding Goups in Data: An Introduction to Cluster Analysis 1990 John Wiley & Sons Inc. New York Klose, 1995 A. Klose Using clustering methods in problems of combined location and routing. Operations Research Proceedings 1995 Springer pp. 411–416 Kuehn and Hamburger, 1963 A.A. Kuehn M.J. Hamburger A heuristic program for locating warehouses Management Science 9 4 1963 643 666 Madsen, 1983 O.B.G. Madsen Methods for solving combined two level location-routing problems of realistic dimensions European Journal of Operational Research 12 1983 295 301 Maranzana, 1963 F.E. Maranzana On the location of supply points to minimize transportation costs IBM Systems Journal 2 1963 129 135 Milligan and Schilling, 1985 G.W. Milligan D.A. Schilling Asymptotic and finite-sample characteristics of four external criterion measures Multivariate Behavioral Research 20 1985 97 109 Min, 1989 H. Min The multiple vehicle routing problem with simultaneous delivering and pick-up points Transportation Research A 23 5 1989 377 386 Min, 1996 H. Min Consolidation terminal location–allocation and consolidated routing problems Journal of Business Logistics 17 2 1996 235 263 Min et al., 1992 H. Min J. Current D. Schilling The multiple depot vehicle routing problem with backhauling Journal of Business Logistics 13 1 1992 259 288 Or, 1976 Or, I., 1976. Traveling salesman–type combinatorial problems and their relation to the logistics of regional blood banking. PhD Dissertation, Northwestern University, Evanston, Illinois, USA. Perl, 1983 Perl, J., 1983. A unified warehouse location-routing analysis. PhD Dissertation, Northwestern University, Evanston, Illinois, USA. Romesburg, 1984 H.C. Romesburg Cluster Analysis for Researchers 1984 Lifetime Learning Publications Belmont, Canada Srivastava, 1986 Srivastava, R., 1986. Algorithms for solving the location-routing problem. PhD Dissertation, The Ohio State University. Srivastava, 1993 R. Srivastava Alternate solution procedures for the location-routing problem OMEGA The International Journal of Management Sciences 21 4 1993 497 506 Srivastava and Benton, 1990 R. Srivastava W.C. Benton The location-routing problem: Considerations in physical distribution system design Computers and Operations Research 17 5 1990 427 435 White Paper, 2001 White Paper, 2001. European transport policy for 2010: Time to decide. The European Commission, COM(2001)370. "
    },
    {
        "doc_title": "Using task analysis to improve the requirements elicitation in health information system.",
        "doc_scopus_id": "84903811794",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84903811794",
        "doc_date": "2007-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "This paper describes the application of task analysis within the design process of a Web-based information system for managing clinical information in hemophilia care, in order to improve the requirements elicitation and, consequently, to validate the domain model obtained in a previous phase of the design process (system analysis). The use of task analysis in this case proved to be a practical and efficient way to improve the requirements engineering process by involving users in the design process.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of methods for the simplification of mesh models using quality indices and an observer study",
        "doc_scopus_id": "34548273966",
        "doc_doi": "10.1117/12.704098",
        "doc_eid": "2-s2.0-34548273966",
        "doc_date": "2007-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Mesh models",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "The complexity of a polygonal mesh model is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, only a few observer studies are reported comparing them regarding the perceived quality of the obtained simplified meshes, and it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting model, as perceived by the final users. Mesh quality indices are the obvious less costly alternative to user studies, but it is also not clear how they relate to perceived quality, and which indices best describe the users behavior. Following on earlier work carried out by the authors, but only for mesh models of the lungs, a comparison among the results of three simplification methods was performed through (1) quality indices and (2) a controlled experiment involving 65 observers, for a set of five reference mesh models of different kinds. These were simplified using two methods provided by the OpenMesh library - one using error quadrics, the other additionally using a normal flipping criterion - and also by the widely used QSlim method, for two simplification levels: 50% and 20% of the original number of faces. The main goal was to ascertain whether the findings previously obtained for lung models, through quality indices and a study with 32 observers, could be generalized to other types of models and confirmed for a larger number of observers. Data obtained using the quality indices and the results of the controlled experiment were compared and do confirm that some quality indices (e.g., geometric distance and normal deviation, as well as a new proposed weighted index) can be used, in specific circumstances, as reasonable estimators of the user perceived quality of mesh models. © 2007 SPIE-IS&T.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Modeling a Web-based Information System for managing clinical information in hemophilia care",
        "doc_scopus_id": "34047122804",
        "doc_doi": "10.1109/IEMBS.2006.259679",
        "doc_eid": "2-s2.0-34047122804",
        "doc_date": "2006-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Bleeding disorder integration",
            "Data storage",
            "Hemophiliacs"
        ],
        "doc_abstract": "Nowadays, Information Systems combined with the Internet, have a significant role in data storage, as in the efficiency and promptness of data transfer and can offer a large contribute in managing and manipulating the information resulting from treatment and attendance of chronic patients, as hemophiliacs [1, 2]. On the other hand, the Internet also created the opportunity of patients to insert data concerning home treatments. This paper briefly describes the modeling process of a Web-based information system to help the management of inherited bleeding disorders integrating, diffusing and archiving large sets of information from heterogeneous sources in scope of the hemophilia care at the Hematology Service of Coimbra Hospital Center, in Portugal. © 2006 IEEE.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Perceived Quality of Simplified Polygonal Meshes: Evaluation using Observer Studies",
        "doc_scopus_id": "34249048115",
        "doc_doi": null,
        "doc_eid": "2-s2.0-34249048115",
        "doc_date": "2006-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Computer Graphics and Computer-Aided Design",
                "area_abbreviation": "COMP",
                "area_code": "1704"
            }
        ],
        "doc_keywords": [
            "Controlled experiment",
            "Generic modeling",
            "Lung model",
            "Mesh modeling",
            "Mesh simplifications",
            "Observer studies",
            "Perceived quality",
            "Polygonal mesh models",
            "Polygonal meshes",
            "Simplification method"
        ],
        "doc_abstract": "© The Eurographics Association 2006.The complexity of a polygonal mesh model is usually reduced by applying a simplification method, resulting in a similar mesh having less vertices and faces. Although several such methods have been developed, it is not yet clear how the choice of a given method, and the level of simplification achieved, influence the quality of the resulting mesh, as perceived by the final users. Following on work carried out by the authors, but only for mesh models of the lungs [SSSMF05, SSFM05], a comparison among the results of three mesh simplification methods, for a few generic models and two simplification levels, was performed through a controlled experiment involving 65 observers. The goal was to ascertain whether the main findings previously obtained for lung models, through a study with 32 subjects, could be generalized to other types of models and confirmed for a larger number of observers. This was verified through the analysis of the data collected from the experiment, which shows that, regarding perceived quality, users are indeed sensitive to the mesh simplification method used and that this sensitivity varies with the simplification level.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of methods for the simplification of mesh models of the lungs using quality indices and an observer study",
        "doc_scopus_id": "33746134806",
        "doc_doi": null,
        "doc_eid": "2-s2.0-33746134806",
        "doc_date": "2005-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Engineering (all)",
                "area_abbreviation": "ENGI",
                "area_code": "2200"
            }
        ],
        "doc_keywords": [
            "Lungs",
            "Mesh models",
            "Normal flipping criterion",
            "Quality indices"
        ],
        "doc_abstract": "This paper presents a comparison among mesh simplification methods performed through quality indices and a controlled experiment involving 32 observers. The simplification methods: OpenMesh, OpenMesh with a normal flipping criterion and QSlim, were compared at two simplification levels: 50% and 20% of the original number of faces. Results obtained using the quality indices and the controlled experiment were compared and show that some quality indices can be used as reasonable estimators of the observers' performance in specific circumstances.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparing three methods for simplifying mesh models of the lungs: An observer test to assess perceived quality",
        "doc_scopus_id": "24644483667",
        "doc_doi": "10.1117/12.593906",
        "doc_eid": "2-s2.0-24644483667",
        "doc_date": "2005-09-19",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Atomic and Molecular Physics, and Optics",
                "area_abbreviation": "PHYS",
                "area_code": "3107"
            },
            {
                "area_name": "Biomaterials",
                "area_abbreviation": "MATE",
                "area_code": "2502"
            },
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [
            "Evaluation",
            "Interaction",
            "Lungs",
            "Mesh simplification",
            "Perceived quality",
            "User study"
        ],
        "doc_abstract": "Meshes are currently used to model objects, namely human organs and other structures. However, if they have a large number of triangles, their rendering times may not be adequate to allow interactive visualization, a mostly desirable feature in some diagnosis (or, more generally, decision) scenarios, where the choice of adequate views is important. In this case, a possible solution consists in showing a simplified version while the user interactively chooses the viewpoint and, then, a fully detailed version of the model to support its analysis. To tackle this problem, simplification methods can be used to generate less complex versions of meshes. While several simplification methods have been developed and reported in the literature, only a few studies compare them concerning the perceived quality of the obtained simplified meshes. This work describes an experiment conducted with human observers in order to compare three different simplification methods used to simplify mesh models of the lungs. We intended to study if any of these methods allows a better-perceived quality for the same simplification rate. A protocol was developed in order to measure these aspects. The results presented were obtained from 32 human observers. The comparison between the three mesh simplification methods was first performed through an Exploratory Data Analysis and the significance of this comparison was then established using other statistical methods. Moreover, the influence on the observers' performances of some other factors was also investigated.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Visual application for the analysis of web-based information systems usage: A preliminary usability evaluation",
        "doc_scopus_id": "4644357302",
        "doc_doi": null,
        "doc_eid": "2-s2.0-4644357302",
        "doc_date": "2004-10-11",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Information visualization",
            "Usability evaluation",
            "Web site structure"
        ],
        "doc_abstract": "This paper presents a general description of the methods used in the on-going evaluation of a Visualizer, which is a sub-component of the Web Log Visual Analysis System. We are trying to evaluate some aspects of the user interface and visualization techniques implemented as part of the prototype. Observation and querying techniques were used with two types of users. A general description of those users and methods is presented. Preliminary results were encouraging and provided new ideas and information that will, eventually, allow a more complete and formal evaluation of our application.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantitative evaluation of a pulmonary contour segmentation algorithm in x-ray computed tomography images",
        "doc_scopus_id": "3442887729",
        "doc_doi": "10.1016/j.acra.2004.05.004",
        "doc_eid": "2-s2.0-3442887729",
        "doc_date": "2004-08-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Radiology, Nuclear Medicine and Imaging",
                "area_abbreviation": "MEDI",
                "area_code": "2741"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Rationale and objectives Pulmonary contour extraction from thoracic x-ray computed tomography images is a mandatory preprocessing step in many automated or semiautomated analysis tasks. This study was conducted to quantitatively assess the performance of a method for pulmonary contour extraction and region identification. Materials and methods The automatically extracted contours were statistically compared with manually drawn pulmonary contours detected by six radiologists on a set of 30 images. Exploratory data analysis, nonparametric statistical tests, and multivariate analysis were used, on the data obtained using several figures of merit, to perform a study of the interobserver variability among the six radiologists and the contour extraction method. The intraobserver variability of two human observers was also studied. Results In addition to a strong consistency among all of the quality indexes used, a wider interobserver variability was found among the radiologists than the variability of the contour extraction method when compared with each radiologist. The extraction method exhibits a similar behavior (as a pulmonary contour detector), to the six radiologists, for the used image set. Conclusion As an overall result of the application of this evaluation methodology, the consistency and accuracy of the contour extraction method was confirmed to be adequate for most of the quantitative requirements of radiologists. This evaluation methodology could be applied to other scenarios. © AUR, 2004.",
        "available": true,
        "clean_text": "serial JL 272938 291210 291703 31 Academic Radiology ACADEMICRADIOLOGY 2004-07-28 2004-07-28 2010-11-13T22:12:05 S1076-6332(04)00281-8 S1076633204002818 10.1016/j.acra.2004.05.004 S300 S300.2 FULL-TEXT 2015-05-15T05:02:15.579713-04:00 0 0 20040801 20040831 2004 2004-07-28T00:00:00Z articleinfo crossmark dco dateupdated tomb dateloaded datesearch indexeddate issuelist volumelist yearnav articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright dateloadedtxt docsubtype doctype doi eid ewtransactionid hubeid issfirst issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype subheadings volfirst volissue figure table body acknowledge affil articletitle auth authfirstini authfull authkeywords authlast doctopic primabst pubtype ref alllist content subj ssids 1076-6332 10766332 11 11 8 8 Volume 11, Issue 8 10 868 878 868 878 200408 August 2004 2004-08-01 2004-08-31 2004 converted-article fla Copyright © 2004 AUR. Published by Elsevier Inc. All rights reserved. QUANTITATIVEEVALUATIONAPULMONARYCONTOURSEGMENTATIONALGORITHMINXRAYCOMPUTEDTOMOGRAPHYIMAGES1 SOUSASANTOS B Materials and methods Quality assessment strategies Quantitative evaluation of the performance Interobserver variability Intraobserver variability Test dataset Hand-outlined contours References contour obtained from the hand-drawn contours Comparing contours Figures of merit Statistical methods Results Study of the interobserver and intraobserver variability involving our algorithm and two expert radiologists Study of the interobserver variability through direct comparison among the algorithm and six expert radiologists Study of the interobserver variability using a reference contour Conclusions Acknowledgements References ROBB 2003 756 760 W BRINK 1994 887 893 J LI 2003 255 265 B BROWN 1997 828 839 M SONKA 1996 314 326 M DURYEA 1995 183 191 J PARKER 1980 291 295 R PROCEEDINGSPHYSICALASPECTSMEDICALIMAGING MEASUREMENTBASICCTDATA HU 2001 490 498 S HASEGAWA 1998 241 250 A SILVA 2000 583 598 J PROCEEDINGSVIBEROAMERICANSYMPOSIUMPATTERNRECOGNITIONSIARP2000 LUNGSEGMENTATIONMETHODSINXRAYCTIMAGES SILVA 2001 216 224 A SPIEMEDICALIMAGING2001 FASTPULMONARYCONTOUREXTRACTIONINXRAYCTIMAGES CHALANA 1997 642 652 V BOWYER 1999 567 606 K HANDBOOKMEDICALIMAGINGVOL2MEDICALIMAGEPROCESSINGANALYSISCAPX VALIDATIONMEDICALIMAGEANALYSISTECHNIQUES BLAKE 1998 A ACTIVECONTOURS GUNN 1997 63 68 S KASS 1988 321 331 M LIANG 1999 933 940 J INTERNATIONALCONFERENCECOMPUTERVISIONVOLUME2SEPTEMBER2025 UNITEDSNAKES YEZZI 1997 199 209 A SIVARAMAKRISHNA 2001 250 256 R FERREIRA 2003 347 358 C SPIEMEDICALIMAGING2003 COMPARISONASEGMENTATIONALGORITHMSIXEXPERTIMAGIOLOGISTSINDETECTINGPULMONARYCONTOURSXRAYCTIMAGES WAGNER 2003 213 224 R SPIEMEDICALIMAGING2003 CONTEMPORARYISSUESFOREXPERIMENTALDESIGNINASSESSMENTMEDICALIMAGINGCOMPUTERASSISTSYSTEMS ALTMAN 1999 D PRACTICALSTATISTICSFORMEDICALRESEARCH ABDOU 1979 753 763 I SACHS 1984 L APPLIEDSTATISTICSAHANDBOOKTECHNIQUES HOAGLIN 1983 D UNDERSTANDINGROBUSTEXPLORATORYDATAANALYSIS 1999 STATSOFTSTATISTICARELEASE55FORWINDOWSSTATSOFTINC GIBBONS 1997 J NONPARAMETRICMETHODSFORQUANTITATIVEANALYSIS HAIR 1995 J MULTIVARIATEDATAANALYSISREADINGS BUVAT 1999 1466 1477 I SPIEMEDICALIMAGING1999 NEEDDEVELOPGUIDELINESFOREVALUATIONMEDICALIMAGEPROCESSINGPROCEDURES SOUSASANTOSX2004X868 SOUSASANTOSX2004X868X878 SOUSASANTOSX2004X868XB SOUSASANTOSX2004X868X878XB item S1076-6332(04)00281-8 S1076633204002818 10.1016/j.acra.2004.05.004 272938 2010-12-23T00:07:40.847864-05:00 2004-08-01 2004-08-31 true 292261 MAIN 11 55545 849 656 IMAGE-WEB-PDF 1 si5 1177 40 193 si4 722 39 153 si3 793 17 188 si2 837 48 140 si1 1095 48 201 gr1 2340 93 87 gr1 68862 534 499 gr2 882 34 125 gr2 6262 137 499 gr3 1968 93 121 gr3 17616 385 499 gr4 3273 93 122 gr4 19776 288 376 gr5 1504 72 125 gr5 7590 218 376 gr6 1705 72 125 gr6 8884 218 376 gr7 1345 73 125 gr7 6598 220 376 gr8 1574 78 125 gr8 14367 310 498 XACRA 586 S1076-6332(04)00281-8 10.1016/j.acra.2004.05.004 AUR Original investigations Figure 1 Images (a, c) and corresponding contours detected by six radiologists (b, d). Figure 2 Definition of the auxiliary contour to obtain pairs of corresponding points on two contours A and B. P and Q are matching points on the contours under comparison. Figure 3 Box-plots for the comparison of the contours using (a) FPratt, (b) mean distance, and (c) angle θ, involving our algorithm and two radiologists. Figure 4 Box-plots comparing contours detected by the algorithm (a) and all the radiologists (dr) using FPratt. Figure 5 Box-plots corresponding to the comparison (to the reference) of the contours detected by each detector (dr1 to dr6 and the algorithm AlG) using the maximum distance figure of merit. Figure 6 Box-plots corresponding to the comparison (to the reference) of the contours detected by each radiologist, in the first time (dr1_T1 to dr6_T1), two radiologists in the second time (dr1_T2 and dr2_T2) and the algorithm (A) using FPratt. Figure 7 Dendogram plot (clustering analysis) showing all the radiologists in the first time (dr1_T1 to dr6_T1), two radiologists in the second time (dr1_T2 and dr2_T2), and the algorithm (A) using FPratt. Figure 8 Correspondence analysis plot showing all the radiologists in the first time (dr1_T1 to dr6_T1), two radiologists in the second time (dr1_T2 and dr2_T2), and the algorithm (alg) using FPratt. Table 1 Friedman ANOVA for the Comparison of the Contours using FPratt- H = 157.67, (P < .000001) and the Null Hypothesis is Rejected Variable Sum of Ranks DR2T1 144 DR1DR2T2 186 DR2T2 203 DR1T12 272 DR1DR2T1 284 DR1T2 293 DR1T1 377 DR2T12 401 Original investigations Quantitative evaluation of a pulmonary contour segmentation algorithm in x-ray computed tomography images1 Beatriz Sousa Santos PhD a b * Carlos Ferreira PhD c d José Silvestre Silva MSc a e Augusto Silva PhD a b Luı́sa Teixeira MD f a Departamento de Electrónica e Telecomunicações, Portugal b Instituto de Engenharia Electrónica e Telemática de Aveiro, (B.S.S., A.S.), Portugal c Departamento de Economia, Gestão e Engenharia Industrial, Universidade de Aveiro, Portugal d Centro de Investigação Operacional, Universidade de Lisboa, Portugal e Departamento de Fı́sica, Faculdade de Ciências e Tecnologia, Universidade de Coimbra, Portugal f Serviço de Imagiologia, Hospitais da Universidade de Coimbra, Portugal * Address correspondence to B.S.S. DET Departamento de Electronica e Telecomunicaçoes, Universidade de Aveiro, 3810 Aveiro, Portugal Rationale and objectives Pulmonary contour extraction from thoracic x-ray computed tomography images is a mandatory preprocessing step in many automated or semiautomated analysis tasks. This study was conducted to quantitatively assess the performance of a method for pulmonary contour extraction and region identification. Materials and methods The automatically extracted contours were statistically compared with manually drawn pulmonary contours detected by six radiologists on a set of 30 images. Exploratory data analysis, nonparametric statistical tests, and multivariate analysis were used, on the data obtained using several figures of merit, to perform a study of the interobserver variability among the six radiologists and the contour extraction method. The intraobserver variability of two human observers was also studied. Results In addition to a strong consistency among all of the quality indexes used, a wider interobserver variability was found among the radiologists than the variability of the contour extraction method when compared with each radiologist. The extraction method exhibits a similar behavior (as a pulmonary contour detector), to the six radiologists, for the used image set. Conclusion As an overall result of the application of this evaluation methodology, the consistency and accuracy of the contour extraction method was confirmed to be adequate for most of the quantitative requirements of radiologists. This evaluation methodology could be applied to other scenarios. Keywords Quantitative evaluation computed tomography (CT) pulmonary segmentation interobserver and intraobserver variability We have reached a point at which computed tomography (CT) images can be reconstructed faster than they can be read. This fact encourages software developers to design programs that will aid radiologists in the reading of CT images and in diagnosing conditions on the basis of CT findings (1). Segmentation often occurs as a preprocessing step of more global image analysis tasks, as is the case of computer-aided analysis of pulmonary x-ray tomograms (2), where many analytic procedures start by correctly identifying the pulmonary regions (3–6). Most algorithms for the segmentation of pulmonary regions are based on intensity discrimination within the Hounsfield scale (7–9); however this task may become very complex because of the presence of spurious structures within the same scale range or the visual merging of the pulmonary regions themselves. In previous works (10,11) we presented algorithms designed to cope with these difficulties, which generate contours with a variable degree of similarity to those provided by radiologists. A quantitative evaluation of the performance of these algorithms is crucial before their clinical use can be considered. Yet, the performance evaluation of segmentation algorithms in medical imaging is recognized as a difficult problem; actually, if one can find in the literature a significant number of contributions concerning the overall segmentation problem by itself, the same is not true when looking for quality and effectiveness assessments performed in some systematic way (12) and having a practical value (13). This evaluation encounters the first great obstacle: the fact that the ground truth is unknown (13) (ie, it is not possible to identify the real contour corresponding to a given image). This problem is often circumvented using the contour resulting from manually tracing the object boundary by a knowledgeable human as a surrogate of that truth. However, not only will contours drawn by two radiologists be different (interobserver variability), but there will also not be agreement between contours drawn by the same radiologist at different occasions (intraobserver variability). These two types of variability have to be taken into account in the performance evaluation of segmentation algorithms; we will have to compare this performance with the performance of several radiologists in some statistically supported manner. In an earlier work (11) we verified that a greater similarity existed between the contours produced by our algorithm and the contours drawn by two expert radiologists, than between the contours drawn by the same two radiologists. This meant that the interobserver variability between our algorithm and any of the two radiologists was less than the interobserver variability between the two radiologists. To investigate if this was specific for those two radiologists, or if it was more general, we have performed a study including six radiologists from different hospitals. To further investigate this issue, we have considered the study of the intraobserver variability relevant; in this respect our algorithm has a clear advantage because its intraobserver variability is zero. Still, the comparison of the interobserver variability between our algorithm and each radiologist to hers/his intraobserver variability could provide interesting additional information on the performance of the algorithm. While other authors have proposed pulmonary segmentation algorithms and have evaluated them (4,8), they have not compared their performance as contour detectors with as many radiologists, nor have they used such a statistically based method as we have used in this study. Materials and methods Quality assessment strategies It is common to treat the physician ground truth as unquestionable, and assume it as a relatively error-free gold standard; however, there is some level of variability in the specification of the ground truth and it is important to have an estimate of this level. This type of variability is an important concern in determining the appropriate criteria for matching a detected contour to a ground truth contour (13). Quantitative evaluation of the performance of segmentation algorithms in medical imaging has been recognized as an important problem. However, many of the evaluation studies that have been carried out did not use a large enough dataset, real images, convenient performance metrics, appropriate statistical methods, or a suitable ground truth. Thus, they cannot be considered correct or complete. Several methodologies have been proposed to perform this evaluation appropriately. The Handbook of Medical Imaging (13) presents a thorough overview of the field. Chalana and Kim (12) also present a concrete approach to segmentation performance assessment through contour comparison. Quantitative evaluation of the performance As mentioned previously, the ideal way of evaluating the performance of our segmentation algorithm would be to compare the contours detected on a valid test dataset with the “real contours” corresponding to each image. However, as we have seen, there are no such real contours. Several expert radiologists will detect different contours on the same image (see Fig 1b); also, each expert radiologist will detect on the same image, at different times, slightly different contours, unlike our algorithm, which always detects the same contours on the same image (its variability is 0 and it does not depend on any seed points introduced by a human observer, as other pulmonary segmentation algorithms (14–18)). This intraobserver variability can be used as a “variability quantum”; it gives an idea of the level of variability that has to be expected and thus can be acceptable to exist between any two contour detectors (algorithm or human). Therefore, comparing the variability between our algorithm and each radiologist with the intraobserver variability of any expert could work as an “acceptability measure” of that variability. As a consequence of the intraobserver and interobserver variability, the manually drawn contours can be considered as a collection of ground truths, all of them equally acceptable. To circumvent this problem we have performed two studies (using different methods) to compare the behavior of our algorithm, as a contour detector, with the behavior of a reasonable number of expert radiologists. These studies involve the assessment of the interobserver variability among a number of “contour detectors”: several humans and one automated (our algorithm). The rationale for this study was that, if the interobserver variability between the algorithm and any of the radiologists is similar in magnitude to the interobserver variability between any two radiologists, then the difference between the algorithm and the radiologists, as contour detectors, could be considered not significant. This rational is similar to the one behind the study by Sivaramakrishna et al (19) to validate a segmentation algorithm of mammographic images, which also seems comparable to our case. Interobserver variability In our first study concerning interobserver variability we directly compared the contours produced by all detectors (algorithm vs all radiologists and every radiologist vs all the others and algorithm). In a subsequent study we compared each detector with a reference contour (surrogate ground truth) obtained from the hand-drawn contours, as described by Ferreira et al (20). To perform these studies we asked six experienced radiologists, from three different hospitals, to draw contours of the pulmonary regions on the chosen images. This number of radiologists seemed reasonable for such a study, taking into consideration that they have been trained and work at three different hospitals. Moreover, it would be difficult to obtain the collaboration of more radiologists. Intraobserver variability To assess intraobserver variability, we asked two radiologists to hand-draw the contours on the same set of images twice, without telling them that they had already drawn contours on those images. We chose the youngest radiologist and the head of the CT department who was responsible for thoracic radiology at the University Hospital, because these radiologists have a significant difference in years of experience. This choice was made in the hope of obtaining two significantly different values of intraobserver variability (which would probably not be the case if the two radiologists had approximately the same experience). The time elapsed between the delineation of the two contours on the same image by the same radiologist was at least 1 month (which agrees with the proposal of Wagner et al (21)]) to minimize the effect of the recollection of having drawn the previous contours. Test dataset The proper choice of the used dataset is very important; a poor selection of either the number of images or the method to select these images can jeopardize the validity of the evaluation procedure. We used 30 512 × 512 images (N = 60 contours) selected using a pseudorandom generator from a set of 253 images that had not been used to develop the algorithm. These images were all the images that could be used to support diagnosis corresponding to exams of eight patients collected at the Radiology Department of the University Hospital in Coimbra, independently of their pathologies. While the used dataset contained images corresponding to different pulmonary levels, which increased variability, using images from a greater number of patients would probably increase case variability. We used the power of a hypothesis test to calculate the sample size, N, of the test dataset, specifying the smallest difference that would be worthwhile to detect. This means, according to Altman (22), trying to make “clinical” importance and statistical significance agree. As a first approach, we hoped to be able to detect a difference of 1 standard deviation. We set the power (1-β) at 90% and chose a 1% significance level (α); using the nomogram for calculating sample size (22), this gives a total sample of N = 60. Hand-outlined contours Our radiologists manually outlined all the contours on transparent sheets superimposed on quality printings of the test images working independently from each other and (as much as possible) in the same way and on the same conditions. The obtained contours were digitized and processed to identify the contours of left and right lungs. This identification is performed computing the image Radon transforms for 0° and 90°, estimating the center of each lung from the maximum values of these two transforms. Applying a morphologic filling starting from the center of one lung and a second filling starting from any point external to the lungs, we obtained an image containing the filled area of the other lung. The contour of the lung was then easily obtained. Erosion was applied to obtain a thinner version of each of the contours (20). We have chosen this method as a compromise between feasibility to the radiologists and acceptable accuracy. References contour obtained from the hand-drawn contours In the last study of interobserver variability, we compared all contours to reference contours obtained from the six contours detected by the radiologists on each image, as described by Ferreira et al (20). For most images having diagnostic value, the contours detected by all the radiologists are only slightly different and thus using a kind of “average” contour seemed an acceptable surrogate to “ground truth” (Fig 1b); however, in particular regions of a few images of the data set, affected by partial volume effect or motion artifacts, the six radiologists detected contours that seem to correspond to the use of different segmentation criteria (Fig 1d); in this case an “average” contour does not make sense as “ground truth” and a different approach should be used. This needs further investigation; however, the impact on the results of this study is not expected to be significant because of the small number of images and reduced zones where this fact was observed in the used data set. Comparing contours The comparison between any two contours was accomplished in two different ways: one based on the local distances between contours and the other exploring a similarity measure between the image masks (binary images containing the pulmonary areas defined by the contours). The computation of distances between contours implies defining pairs of matching points on both contours. To find these pairs of points we used an auxiliary contour as shown in Figure 2. Differences between the contours were quantified using the Euclidean distances measured between corresponding points (11). Figures of merit The values of the computed distances between the contours allow a localized and accurate quantification of their differences, easily assessable through simple visualization techniques. However, we consider it fundamental to use global quality figures of merit, which facilitate a comprehensive comparison. Thus, several figures of merit, based on the computed distances, were used as performance measures: the Pratt figure of merit F Pratt (23): (1) F Pratt = 1 N ∑ i=1 N 1 1+α×di 2 the Mean Distance: (2) dmean= 1 N ∑ i=1 N di the Maximum Distance: (3) d max =max(di)1≤i≤N and the number of distances greater than 5 pixels (approximately 1% error for a resolution of 512 × 512): (4) n>1%= ∑i=1 N mi N (where mi = 0 if di ≤ 5 and mi = 1 if di > 5). The Pratt figure of merit gives a general impression of the distances between contours; it is a relative measure and varies in the interval [0,1] where “1” means a complete match of the contours. In our case, α, which is a normalization parameter related to the size of the contours, was chosen to be 1/9 so that if all the distances di are equal to 3 pixels, F Pratt will have a value of 0.5. The value of 3 pixels was chosen to produce a scale that allows enough discrimination among the contours drawn by the radiologists. The mean distance also gives an integrated view of the distances between contours, while the maximum distance gives a worst case view. Finally, the number of distances greater than 1% (5 pixels in our case) provides information on the number of relevant errors and thus complements the information obtained from the previous indexes. Another figure of merit was computed based on the similarity between the two binary images, A and B, including the areas defined by the pulmonary contours. This simple measure of similarity may be defined by: (5) θ=cos −1 A·B ‖A‖·‖B‖ where “.” and “‖‖” denote the usual inner product and norm of vectors. In a Hilbert space context, θ is the angle between two binary image vectors A and B. The correct identification of the measurement scale (24) is an important issue concerning the information provided by these figures of merit and the statistical methods that can be used. In this respect, the Pratt and θ figures of merit are measured on an ordinal scale whereas the mean error and the number of distances greater than 5 pixels are measured on a ratio scale. Statistical methods As a first step in the analysis of the data obtained from the comparison among contours using all figures of merit, we performed an exploratory data analysis (25); this analysis provided an overview of the structure of the data (showing the amplitudes, asymmetries, location, possible outliers, etc) and also some clues to the type of statistical tests to be used to test our hypothesis. The software used was Statistica (Statsoft, Tulsa, OK) (26). Because the sample set did not correspond to independent experiments, nor did the data have a normal distribution, a nonparametric test was used (27). We also used multivariate data analysis (28) to assess if our algorithm is generally comparable to the six human observers as a contour detector on the used image data set. Results Study of the interobserver and intraobserver variability involving our algorithm and two expert radiologists The two radiologists were called dr1 and dr2, and the two contour drawing moments were called t1 and t2. Figure 3 shows the box-plots and corresponding median and quartile values for the comparison between the contours detected by our algorithm and the two radiologists in the two moments, using different figures of merit. The box-plots can be interpreted in the following way: drit1-comparison between the contours detected by dri at moment t1 to the contours detected by our algorithm, on the selected set of images; drit12-comparison between the contours detected by dri at moments t1 and t2, on the same images. According to this notation: dr1t12 and dr2t12 represent the intraobserver variability of experts dr1 and dr2; dr1t1, dr1t2, dr2t1 and dr2t2 represent the interobserver variability between our algorithm and each radiologist in each moment; dr1dr2t1 and dr1dr2t2 represent the interobserver variability between both radiologists at moments t1 and t2, respectively. All these can be compared in Figure 3 through several figures of merit: FPratt, mean distance, and angle θ. Observing the box-plots corresponding to FPratt we note that: 1. At moment t1, dr1 is more similar to our algorithm than to dr2, because the median value of dr1t1 (median, 0.81) is higher (ie, better) than the median value of dr1dr2t1 (median, 0.75); this was confirmed using a nonparametric test for the equality of the median, the Wilcoxon test (28), which rejected the null hypothesis (P < .00004). Also the range of the values is smaller for dr1t1 than for dr1dr2t1. Both results suggest that the interobserver variability between the two radiologists is higher than the variability between dr1 and our algorithm; 2. At moment t2, both dr1 and dr2 are more similar to our algorithm than to each other; for instance, the median value of dr1t2 (median, 0.78) is higher than the median value of dr1dr2t2 (median, 0.70), confirmed using the Wilcoxon test (P < .00009). 3. dr1 is more similar to our algorithm than to himself because the median value of dr1t1 (median, 0.81) is higher (better) than the median value of dr1t12 (median, 0.76), according to the Wilcoxon test (P < .00007). On the other hand, the median value of dr1t2 (median, 0.78) was not considered significantly different of the median value of dr1t12, according to the Wilcoxon test (P < .4). The above results suggest that the interobserver variability between dr1 and our algorithm is ≤ the intraobserver variability of dr1. These findings are not contradicted by the observation of the information obtained using the other figures of merit and were confirmed using a nonparametric method, the Friedman’s two-way analysis of variance (27). The calculated H = 157.67 (with N = 60 and k = 8); under the null hypothesis (equality of medians), H has a χ2 distribution with (k-1) degrees of freedom. In our case, for a 1% significance level (α), χ2 (7);0.01 = 18.48; thus H = 157.67⪢χ2 (7);0.01 = 18.48 (P < .000001) and the null hypothesis is rejected. Table 1 presents the sum of ranks in ascending order. This means that the medians are in fact significantly different, which reinforces the three observations presented above. Moreover, these observations can be confirmed through Table 1, where we can see, for instance, that the sum of ranks corresponding to dr1t1 and dr1t2 are both higher than the sum of ranks corresponding to dr1t12 (377, 299, and 272, respectively). Taking the observation of Table 1 further, we notice that all (except dr2t1) variabilities between our algorithm and each radiologist are less than (at least) the interobserver variability between the two radiologists at moment t2 (dr1dr2t2). These and other findings that can be extracted from these results seem to indicate that, as a detector of pulmonary contours on the used set of images, our algorithm behaves as a third human observer. Study of the interobserver variability through direct comparison among the algorithm and six expert radiologists Let us generalize the previous comparison to six radiologists. dr1 … dr6 stand for the six radiologists and A for the algorithm. Figure 4 shows the box-plots and corresponding median and quartile values for the comparison between the contours detected by our algorithm and the six radiologists in all possible combinations using FPratt (considering that, for instance, dri_drj is equal to drj_dri, we only show one). Thus, in Figure 4, the meaning is: a_dri-comparison between the contours detected by our algorithm and the contours detected by dri, on the selected set of images; it represents the interobserver variability between our algorithm and this radiologist; dri_drj-comparison between the contours detected by dri and the contours detected by drj, on the selected set of images; it represents the interobserver variability between these two radiologists. Observing Figure 4 we note that the median values corresponding to situations of the type a_dri are generally higher and more similar among them than the ones corresponding to dri_drj. Performing a correspondence analysis (28) and observing the plane defined by the first two axis (which represents approximately 46% of the total inertia), we notice that our algorithm is clearly included in the main groups formed by the comparisons among all the radiologists and the algorithm. Comparisons between dr5, dr6 and dr2 seem to be isolated. This could be because dr2 had just finished his training as a radiologist and dr5 and dr6 both work in the same hospital (different from dr2). Study of the interobserver variability using a reference contour We primarily show results obtained using the Pratt figure of merit because we have concluded in previous studies, and confirmed through this one, that the figures of merit (except for the maximum distance) produce consistent results, conveying the same type of information. As a first approach, we studied the interobserver variability among all radiologists and the algorithm in a worst-case scenario. This was performed using the maximum distance figure of merit and exploratory data analysis. Figure 5 shows the box-plots of the data resulting from the comparison of the contours obtained by each detector (humans and algorithm) to the reference contours using the maximum distance. On these plots we observe a concentration of the smaller values, some outliers for all detectors (corresponding to images that should be analyzed) and median values for all detectors between 5.4 and 9.9 pixels; these values can be considered low for images of 512 × 512 pixels. Thus, even in this case all the detectors (including our algorithm) seem to have a good performance for the used image data set. As a second approach, we studied the variability between the reference and all radiologists as well as the algorithm using the Pratt figure of merit and exploratory data analysis. In this study, we included the contours drawn by all the radiologists (dr1 to dr6) in first time, the contours drawn by dr1 and dr2 the second time (as dr1_T2 and dr2_T2), as well as the contours obtained using our algorithm (a). Observing Figure 6, which shows the box corresponding to these data, we notice that the median value obtained for our algorithm is quite similar to the value for radiologist dr4_t1, higher than the values for radiologists dr1_T2, dr3_t1, dr5_t1, dr6_T1 and lower than the values for radiologists dr1_T1, dr2_t1, dr2_T2. This indicates that our algorithm produced, for the used image set, contours more similar to the reference than a significant part of the radiologists. The above result suggested that we should further explore the relation among the performance of our algorithm as a detector to the performance of all the radiologists. Thus, we used clustering analysis (28), which closely associated our algorithm with dr1_t1 as shown by the dendogram plot of Figure 7; this means that, in this context, our algorithm is more similar to radiologist dr1 than he is to himself in different moments, namely dr1_t1 and dr1_t2. This conclusion was already obtained in the previous study through direct comparison among radiologists and algorithm. A confirmation of this result was obtained through the use of another method of multivariate data analysis. Figure 8 shows the projection on the plane defined by the first two axes (approximately 66% of the total inertia) of a correspondence analysis. Observing this figure, we notice that our algorithm is clearly included in a group of four radiologists (dr1_t1, dr1_t2, dr3_ t1, dr4_t1), radiologists dr5 and dr6 form another group and dr2 is isolated between the two groups. Note that the same conclusion could be drawn from the dendogram of Figure 7. This could be related, as observed in the previous study, to the facts that radiologists dr5 and dr6 work in the same department, (different from the others) and perhaps use different segmentation criteria, radiologist dr2 has just finished his training as a radiologist and all the others have a much larger experience. To obtain a global average view of the distance between detected contours and the reference, we used the mean-distance figure of merit and angle θ, and we obtained a confirmation of the results previously found through the Pratt figure of merit (20). Conclusions In this article we propose a methodology to the quantitative evaluation of the performance of a pulmonary contour segmentation algorithm involving the study of interobserver and intraobserver variability. Making accurate, unbiased estimates or comparisons of performance is, in general, a very difficult task. However, some guidelines are known to facilitate it (13,22,29). For our case, we considered the following guidelines useful: • Report results on common test datasets; • Use test datasets different from those used to train the segmentation method; • Use an adequate methodology to choose the test datasets and clearly state it (eg, the inclusion and exclusion criteria and the determination of the sample size); • Choose carefully and define clearly the observers and methods used to obtain the ground-truth; • Let the observers operate in the same conditions; • Clearly specify the performance metric (figures of merit) used; • Correctly identify the measurement scales, which determine the kind of statistical methods that could be used; • Choose hypothesis tests compatible with the quality indexes used and clearly justify it (as the chosen α and β and if the test is one- or two-tailed); • Use nonparametric tests if the data is categorical, the statistical distribution of the data is unknown (or known and not suitable for parametric methods) or the sample size is small; • Use paired test if possible (if all the methods can be applied to the same image). We present results concerning the interobserver variability among six radiologists and the algorithm, using two different approaches: Through the direct comparison of the contours detected by this algorithm to the contours hand-drawn by six radiologists; Through a comparison to a reference contour (obtained from the hand-drawn contours) used as a surrogate ground truth. This last approach is easier to generalize to a greater number of radiologists; however, it is necessary to further investigate what is the most correct way of computing the reference contour when radiologists use different segmentation criteria. All the comparisons were made using several figures of merit. While the Pratt figure of merit, the mean distance, and the angle θ produced consistent results conveying the same type of information, an integrated view of the distances between contours, maximum distance is useful for worst-case scenarios. We also assessed the intraobserver variability of two radiologists to have a measure of the level of interobserver variability that is expected and has to be accepted. We believe this methodology is general enough to be applicable to many other problems of segmentation on medical images, in spite of the fact that it was developed for this specific application. Concerning the performance of our segmentation algorithm, the results presented allow us to conclude that it is possibly as good a lung contour detector, in most thoracic CT images with diagnostic value, as any of the six radiologists. This assertion is mainly based on the fact that it exhibits a greater “agreement” to any of the radiologists than the radiologists among them, in the used image set. This is true, with a few exceptions, for images with complex vascular patterns crossing the interface between the mediastinic and pulmonary fields. Acknowledgements The authors express their gratitude to the following radiologists for drawing contours: Dr Pedro Agostinho from University Hospital of Coimbra, Dr Rui Pinho e Melo and Dr Jorge Pinho e Melo from CENTAC-Center of Computed Tomography, Aveiro; Dr Anabela Fidalgo and Dr Fernando Figueiredo from the Imagiology Department at the Hospital Infante D. Pedro, Aveiro. The authors are also grateful to an anonymous reviewer for his pertinent comments and suggestions. References 1 W.L. Robb Perspective on the first 10 years of the CT scanner industry Acad Radiol 10 2003 756 760 2 J. Brink J.P. Heiken G. Wang K.W. McEnery F.J. Schlueter M.W. Vannier Helical CT principles and technical considerations Radiographics 14 1994 887 893 3 B. Li G. Christensen E. Hoffmann G. McLeannan J. Reinhardt Establishing a normative atlas of the human lung intersubject warping and registration of volumetric CT images Acad Radiol 10 2003 255 265 4 M.S. Brown M.F. McNitt-Gray N.J. Mankovich Method for segmentation chest CT image data using an anatomical model preliminary results IEEE Trans Med Imaging 16 1997 828 839 5 M. Sonka W. Park E.A. Hoffman Rule-based detection of intrathoracic airways trees IEEE Trans Med Imaging 15 1996 314 326 6 J. Duryea J.M. Boone A fully automated algorithm for the segmentation of lung fields on digital chest radiographic images Med Phys 22 1995 183 191 7 R.P. Parker Measurement of basic CT data B.M. Moores R.P. Parker B.R. Pullan Proceedings of physical aspects of medical imaging 1980 Wiley & Sons Manchester, UK 291 295 8 S. Hu E.A. Hoffman J.M. Reinhardt Automatic lung segmentation for accurate quantization of volume x-ray CT images IEEE Trans Med Imaging 20 2001 490 498 9 A. Hasegawa S.-.C.B Lo J.-.S Lin M.T. Freedman S.K. Mun A shift-invariant neural network for the lung field segmentation in chest radiography J VLSI Signal Process 18 1998 241 250 10 J.S. Silva A. Silva B.S. Santos Lung segmentation methods in x-ray CT images Proceedings of V Ibero-American Symposium On Pattern Recognition-SIARP’2000 2000 APRP—Portuguese Association for Pattern Recognition Lisbon, Portugal 583 598 11 A. Silva J.S. Silva B.S. Santos C. Ferreira Fast pulmonary contour extraction in x-ray CT images a methodology and quality assessment C.-.T Chen A.V. Clough SPIE-Medical Imaging 2001 Physiology and Function from Multidimensional Images 4321 2001 SPIE Bellingham, WA 216 224 12 V. Chalana Y. Kim A methodology for evaluation of boundary detection algorithms on medical images IEEE Trans Pattern Anal Machine Intell 16 1997 642 652 13 K.W. Bowyer Validation of medical image analysis techniques J. Fitzpatrick M. Sonka Handbook of medical imaging. Vol 2. Medical image processing and analysis (cap. X) 1999 SPIE-The International Society for Optical Engineering Bellingham, WA 567 606 14 A. Blake M. Isard Active contours 1998 Springer Verlag London 15 S.R. Gunn M.S. Nixon A robust snake implementation; a dual active contour IEEE Trans Pattern Anal Machine Intell 19 1997 63 68 16 M. Kass A. Witkin D. Terzopoulos Snakes active contour models Int J Comput Vision 1 1988 321 331 17 J. Liang T. McInerney D. Terzopoulos United snakes International Conference of Computer Vision—Volume 2, September 20–25 1999 Kerkyra Greece 933 940 18 A. Yezzi S. Kichenassamy A. Kumar P. Olver A. Tannenbaum A geometric snake model for segmentation of medical imagery IEEE Trans Med Imaging 16 1997 199 209 19 R. Sivaramakrishna N. Obuchowski W. Chilcote K. Powell Automatic segmentation of mammographic density Acad Radiol 8 2001 250 256 20 C. Ferreira B.S. Santos J.S. Silva A. Silva Comparison of a segmentation algorithm to six expert imagiologists in detecting pulmonary contours on x-ray CT images SPIE Medical Imaging 2003 Image Perception, Observer Performance and Technology Assessment 2003 SPIE Bellingham, WA 347 358 21 R. Wagner S. Beiden G. Campbell C. Metz W. Sacks Contemporary issues for experimental design in assessment of medical imaging and computer-assist systems SPIE Medical Imaging 2003 Image Perception, Observer Performance and Technology Assessment 5034 2003 SPIE Bellingham, WA 213 224 22 D.G. Altman Practical statistics for medical research 1999 CRC Press London, UK 23 I.E. Abdou W.K. Pratt Quantitative design and evaluation of enhancement/thresholding edge detectors Proceedings IEEE 67 1979 753 763 24 L. Sachs Applied statistics-a handbook of techniques 1984 Springer-Verlag New York, NY 25 D. Hoaglin F. Mosteller J. Tukey Understanding robust and exploratory data analysis 1983 Wiley & Sons 26 Statsoft. Statistica-release 5.5 for Windows. Statsoft Inc 1999 27 J.D. Gibbons Nonparametric methods for quantitative analysis 1997 American Sciences Press Syracuse, NY 28 J.F. Hair R.E. Anderson R.L. Tatham W.C. Black Multivariate data analysis with readings 1995 Prentice-Hall Upper Saddle River, NJ 29 I. Buvat The need to develop guidelines for the evaluation of medical image processing procedures K.M. Hanson SPIE-Medical Imaging 1999 Image Processing 1999 SPIE Bellingham, WA 1466 1477 "
    },
    {
        "doc_title": "Comparison of a segmentation algorithm to six expert imagiologists in detecting pulmonary contours on X-ray CT images",
        "doc_scopus_id": "0041357174",
        "doc_doi": "10.1117/12.480072",
        "doc_eid": "2-s2.0-0041357174",
        "doc_date": "2003-09-12",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Quantitative evaluation of the performance of segmentation algorithms on medical images is crucial before their clinical use can be considered. We have quantitatively compared the contours obtained by a pulmonary segmentation algorithm to contours manually-drawn by six expert imaiologists on the same set of images, since the ground truth is unknown. Two types of variability (inter-observer and intra-observer) should be taken into account in the performance evaluation of segmentation algorithms and several methods to do it have been proposed. This paper describes the quantitative evaluation of the performance of our segmentation algorithm using several figures of merit, exploratory and multivariate data analysis and non parametric tests, based on the assessment of the inter-observer variability of six expert imagiologists from three different hospitals and the intra-observer variability of two expert imagiologists from the same hospital. As an overall result of this comparison we were able to claim that the consistency and accuracy of our pulmonary segmentation algorithm is adequate for most of the quantitative requirements mentioned by the imagiologists. We also believe that the methodology used to evaluate the performance of our algorithm is general enough to be applicable to many other segmentation problems on medical images.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Experimental methodology for the evaluation of the 3D visualization of quantitative information: A case study concerning SEEG information",
        "doc_scopus_id": "84943393171",
        "doc_doi": "10.2498/cit.2002.02.04",
        "doc_eid": "2-s2.0-84943393171",
        "doc_date": "2002-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Computer Science (all)",
                "area_abbreviation": "COMP",
                "area_code": "1700"
            }
        ],
        "doc_keywords": [
            "Empirical evaluations",
            "Epilepsy",
            "Evaluation",
            "Experimental methodology",
            "Exploratory data analysis",
            "Quantitative information",
            "Quantitative visualization",
            "Spatio-temporal dynamics"
        ],
        "doc_abstract": "The visual analysis of Stereoeletroencephalographic (SEEG) signals in their anatomical context is aimed at understanding the spatio-temporal dynamics of epileptic processes. The magnitude of these signals may be encoded by graphical glyphs, having a direct impact on the perception of the values. This problem has motivated an evaluation of the quantitative visualization of these signals, specifically with regard to the influence of the coding scheme of the glyphs on the understanding and analysis of the signals. This work describes an experiment conducted with human observers in order to evaluate three different coding schemes used to visualize the magnitude of SEEG signals in their 3D anatomical context. Before the experiment we had no clue to which of these schemes would provide better performance to the human observers, while the literature offered theories supporting different answers. Through our experiment we intended to find out if any of these coding schemes allows better performance in two aspects: accuracy and speed. A protocol has been developed to measure these aspects. The results presented in this work were obtained from 40 human observers. Comparison between the three coding schemes was first performed through an Exploratory Data Analysis (EDA). Statistical significance of this comparison was then established using nonparametric methods. Influence of some other factors on the observers' performance was also investigated.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "WebMaster - An internet information support system for academic services using ASP technology: Modelling and implementation",
        "doc_scopus_id": "84973890941",
        "doc_doi": null,
        "doc_eid": "2-s2.0-84973890941",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Information Systems",
                "area_abbreviation": "COMP",
                "area_code": "1710"
            },
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Human-Computer Interaction",
                "area_abbreviation": "COMP",
                "area_code": "1709"
            },
            {
                "area_name": "Artificial Intelligence",
                "area_abbreviation": "COMP",
                "area_code": "1702"
            }
        ],
        "doc_keywords": [
            "Academic department",
            "Design and implementations",
            "Dynamic information",
            "Information concerning",
            "Internet information",
            "OOHDM",
            "Repetitive task",
            "Web information systems"
        ],
        "doc_abstract": "Nowadays the management and publication of academic and administrative information should not ignore the new information technologies. This paper describes the WebMaster application, a Web Information System for computerizing the academic information concerning a Master degree within an academic department. It performs, as well, the automation of all administrative jobs and the inherent procedures for the distribution of this information, increasing its accessibility, reliability, up-to-dateness and decreasing the bureaucratic burden. With this application it is possible to speed up some routine activities, decreasing the need for manual, exhaustive and repetitive tasks. The general goal of this work is to determine how and to what extent different groups of users within an academic department can interchange information using the Web; in order to attain this goal, we have developed a framework with three essential aspects, that will be briefly presented: contents, design and implementation.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of the 3D visualization of quantitative stereoelectroencephalographic information. New results",
        "doc_scopus_id": "0034867503",
        "doc_doi": "10.1117/12.431194",
        "doc_eid": "2-s2.0-0034867503",
        "doc_date": "2001-01-01",
        "doc_type": "Article",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [
            "Epilepsy",
            "Exploratory data analysis",
            "Graphical glyphs",
            "Spatio temporal dynamics",
            "Stereoelectroencephalographic signals",
            "Three dimensional visualization"
        ],
        "doc_abstract": "The visual analysis of Stereoelectroencephalographic (SEEG) signals in their anatomical context is aimed at the understanding of the spatio-temporal dynamics of epileptic processes. The magnitude of these signals may be encoded by graphical glyphs, having a direct impact on the perception of the values. Our study is devoted to the evaluation of the quantitative visualization of these signals, specifically to the influence of the coding scheme of the glyphs on the understanding and the analysis of the signals. This work describes an experiment conducted with human observers in order to evaluate three different coding schemes used to visualize the magnitude of SEEG signals in their 3D anatomical context. We intended to study if any of these coding schemes allows better performances for the human observers in two aspects: accuracy and speed. A protocol has been developed in order to measure these aspects. The results that will be presented in this work were obtained from 40 human observers. The comparison between the three coding schemes has first been performed through an Exploratory Data Analysis (EDA). The statistical significance of this comparison has then been established using nonparametric methods. The influence on the observers' performance of some other factors has also been investigated.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Fast pulmonary contour extraction in X-ray CT images: A methodology and quality assessment",
        "doc_scopus_id": "0034866059",
        "doc_doi": "10.1117/12.428139",
        "doc_eid": "2-s2.0-0034866059",
        "doc_date": "2001-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Electronic, Optical and Magnetic Materials",
                "area_abbreviation": "MATE",
                "area_code": "2504"
            },
            {
                "area_name": "Condensed Matter Physics",
                "area_abbreviation": "PHYS",
                "area_code": "3104"
            },
            {
                "area_name": "Computer Science Applications",
                "area_abbreviation": "COMP",
                "area_code": "1706"
            },
            {
                "area_name": "Applied Mathematics",
                "area_abbreviation": "MATH",
                "area_code": "2604"
            },
            {
                "area_name": "Electrical and Electronic Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2208"
            }
        ],
        "doc_keywords": [],
        "doc_abstract": "Segmentation of thoracic X-Ray Computed Tomography images is a mandatory pre-processing step in many automated or semi-automated analysis tasks such us region identification, densitometric analysis, or even for 3D visualization purposes when a stack of slices has to be prepared for surface or volume rendering. In this work, we present a fully automated and fast method for pulmonary contour extraction and region identification. Our method combines adaptive intensity discrimination, geometrical feature estimation and morphological processing resulting into a fast and flexible algorithm. A complementary but not less important objective of this work consisted on a quality assessment study of the developed contour detection technique. The automatically extracted contours were statistically compared to manually drawn pulmonary outlines provided by two radiologists. Exploratory data analysis and non-parametric statistical tests were performed on the results obtained using several figures of merit. Results indicate that, besides a strong consistence among all the quality indexes, there is a wider inter-observer variability concerning both radiologists than the variability of our algorithm when compared to each one of the radiologists. As an overall conclusion we claim that the consistence and accuracy of our detection method is more than acceptable for most of the quantitative requirements mentioned by the radiologists.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Evaluation of the 3D visualization of quantitative Stereoeletroencephalographic information",
        "doc_scopus_id": "0034444451",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0034444451",
        "doc_date": "2000-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Epilepsy",
            "Quantitative evaluation",
            "Stereoelectroencephalography"
        ],
        "doc_abstract": "Steoreoelectroencephalography (SEEG) has been used to define and understand the organization of epileptogenic zones of the brain. The fusion of the SEEG signals and the anatomy on a common referential is a possible method for the analysis of these signals. This work describes an experiment conducted with human observers in order to evaluate three different coding schemes used to visualize the magnitude of SEEG signal in the anatomical context.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Information visualization in locating obnoxious facility: Some examples with mono and bicriteria models",
        "doc_scopus_id": "78449286955",
        "doc_doi": "10.1109/IV.2000.859794",
        "doc_eid": "2-s2.0-78449286955",
        "doc_date": "2000-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Bi-criteria",
            "Decision supports",
            "Information visualization",
            "Obnoxious facilities",
            "Operation research models",
            "Research method",
            "Social concerns"
        ],
        "doc_abstract": "© 2000 IEEE.The problem of Iocating obnoxious facitities has became a major social concern. Operation Research models and methods have been used in can be contexl, houever their use can be jeopardised lie liuide demanded ejforr. In consequence, tlw use of visuaíizíilion ¡md inleraclion methods becomes oj great importance. Tìu^ paper addresses specific issues ami slutws examples of developed solutions for the visualization and Interacilon problems related to the use of mono and multicriterui Obnoxious l-acility Location in Decision Support scenarios.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Quantifying the objective quality of voxel based data visualizations produced by a ray caster: A proposal",
        "doc_scopus_id": "0030721073",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030721073",
        "doc_date": "1997-01-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Software",
                "area_abbreviation": "COMP",
                "area_code": "1712"
            },
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            }
        ],
        "doc_keywords": [
            "Volume rendering (VR) method",
            "Voxel based data set"
        ],
        "doc_abstract": "A set of parameters to assess the objective quality of visualizations of a voxel based data set, produced using a ray caster, is proposed as a first step toward the evaluation of the overall quality of these visualizations. Results obtained using synthetic data and a simple implementation of a ray caster are presented. The final goal of this evaluation will be the computation of `confidence indices' that could offer the user a `guided visualization', i.e. allow him/her to decide what are the `best' visualizations of a data set.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Decision support system for the location of hospital facilities: a prototype",
        "doc_scopus_id": "0030314140",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0030314140",
        "doc_date": "1996-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Bicriteria model",
            "Visual basic",
            "Windows platform"
        ],
        "doc_abstract": "A prototype of a Decision Support System (DSS) to the location of hospital facilities, which uses a bicriteria model is presented. This DSS is being developed for a Windows platform using Visual Basic and having an open architecture.",
        "available": false,
        "clean_text": ""
    },
    {
        "doc_title": "Comparison of two models for the quantification of left ventricle function",
        "doc_scopus_id": "0028746481",
        "doc_doi": null,
        "doc_eid": "2-s2.0-0028746481",
        "doc_date": "1994-12-01",
        "doc_type": "Conference Paper",
        "doc_areas": [
            {
                "area_name": "Signal Processing",
                "area_abbreviation": "COMP",
                "area_code": "1711"
            },
            {
                "area_name": "Biomedical Engineering",
                "area_abbreviation": "ENGI",
                "area_code": "2204"
            },
            {
                "area_name": "Computer Vision and Pattern Recognition",
                "area_abbreviation": "COMP",
                "area_code": "1707"
            },
            {
                "area_name": "Health Informatics",
                "area_abbreviation": "MEDI",
                "area_code": "2718"
            }
        ],
        "doc_keywords": [
            "Contractibility",
            "Left ventricle function",
            "Principal component analysis"
        ],
        "doc_abstract": "A comparison of two models for the quantification of the Left Ventricle (LV) wall motion is presented. This comparison was made using Principal Component Analysis (PCA) as an attempt to investigate if two of the most commonly used models produce comparable results.",
        "available": false,
        "clean_text": ""
    }
]